Title,Description,Tags,Accepted_Answer,View_Count,Creation_Date
How to implement query searching in a specific cluster after document clustering?,"<p>I have two clusters as a class which has </p>

<pre><code>Cluster : class

DocumentList : List&lt;Document&gt;
centroidVector : Map&lt;String,Double&gt;
</code></pre>

<p>Now the problem is that when the query is searched it is parsed as a file and then made into a document object , added to documentIndex and its index is constructed along with other documents . I did that because it had to go through the same procedure i.e tokenizing ,stemming etc. But now i want to implement query search in a specific cluster with which the query vector is most similar with , i.e dot product ~ 0.5 -1 . So i would have to take a dot product between the query vector and the cluster vector to do that. But i dont know how to implement it because the index is created in memory and is not stored in the database. Still in the process of doing that .</p>

<p>Thank you </p>
","machine-learning, nlp, cluster-analysis","<p>Clustering is not meant for <em>searching</em> (i.e. indexing etc.). It is an analysis step meant to <em>find</em> possible unknown structure within your data set, not to retrieve information faster.
You can exploit the structure <em>sometimes</em> for faster search, but then you need an index that can make use of this.</p>

<p>Just do an index right away if you want to do similarity search! Then try to improve the index by doing some clustering before.</p>
",106,1342720641
"can NLTK/pyNLTK work &quot;per language&quot; (i.e. non-english), and how?","<p>How can I tell NLTK to treat the text in a particular language?</p>

<p>Once in a while I write a specialized NLP routine to do POS tagging, tokenizing and etc. on a non-english (but still hindo-European) text domain.</p>

<p>This question seem to address only different corpora, not the change in code/settings:
<a href=""https://stackoverflow.com/questions/1639855/nltk-tagging-in-german"">POS tagging in German</a></p>

<p>Alternatively,are there any specialized Hebrew/Spanish/Polish NLP modules for python?</p>
","python, nlp, nltk","<p>I'm not sure what you're referring to as the changes in code/settings. NLTK mostly relies on machine learning and the ""settings"" are usually extracted from the training data.</p>

<p>When it comes to POS tagging the results and tagging will be dependant on the tagger you use/train. Should you train your own you'll of course need some spanish / polish training data. The reason these might be hard to find is the lack of gold standard material publicly available. There are tools out there to do that do this, but this one isn't for python (<a href=""http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/"" rel=""noreferrer"">http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/</a>). </p>

<p>The nltk.tokenize.punkt.PunktSentenceTokenizer tokenizer will tokenize sentences according to multilingual sentence boundaries the details of which can be found in this paper (<a href=""http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485"" rel=""noreferrer"">http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485</a>).</p>
",10303,1259137052
Extract terminology from sentences quickly,"<p>I am working in Text Mining and my work is focused on biomedical entities (genes, proteins, drugs and diseases). I would like to share with you some questions.</p>

<p>Now, my goal is to find biomedical entities in biomedical text (from Medline) and through of dictionaries of terms, I can identify each entity found with its unique identifier.</p>

<p>To store text, dicitionaries and results, I am using MongoDB (a nonSQL database). Each abstract is splitted in sentences, and each sentence is store in a new record (with list of tokens, chunks and Part-of-Speech tags). To find entities, I get all senteces and for each one I create a regular expresion for each term in the dictionary (in Python):</p>

<pre><code>for term in dicitonary:
     matches = re.finditer(r'(' + term + ')', sentence)
     for m in matches:
          ini = m.start()
          end = m.end()
          result.append(ini, end, dictionary.get_identification[term])
</code></pre>

<p>But it is really slow, I have several subsets of 150,000 abstracts (>1,000,000 of sentences).</p>

<p>For me, it is really interesting soft-matching to extract more entities where their terminology is not exactly in my dictionary, but it can increase my running time.</p>

<p>I think that my problem is to do a lot of regular expressions (I have dictionary with 300,000 entries) for each sentence, because I have to find the terms in sentence. Without Machine Learning algorithm, how could you resolve this problem? And with ML algorithms? Now, I am flexible to change my programming language, databases...</p>

<p>Thank you very much!!!</p>

<p>Regards,</p>

<p>àlex.</p>
","python, mongodb, nlp, text-mining","<p>Instead of building one RE per <code>term</code>, build a single, disjunctive one that can catch all of them:</p>

<pre><code>pattern = re.compile(""(%s)"" % ""|"".join(re.escape(term) for term in dictionary))
</code></pre>

<p>then use <code>pattern.finditer</code>.</p>

<p>As for ""how to use machine learning"", that's far too broad a question, IMHO. Start out by googling for ""biomedical named entity recognition"" -- there's a vast amount of literature about that problem and assorted tools.</p>
",608,1342690306
Topic Modeling using Mallet Api for Java,"<p>Hi i have to do topic modeling using Mallet Java API but i am new to Mallet so i am finding it real difficult to understand the Mallet libraries and use them. Does anyone know any place where there might be some source code for topic modeling to understand the libraries?</p>
","java, nlp, mallet",,3494,1308938478
Financial news headers classification to positive/negative classes,"<p>I'm doing a small research project where I should try to split financial news articles headers to positive and negative classes.For classification I'm using SVM approach.The main problem which I see now it that not a lot of features can be produced for ML. News articles contains a lot of Named Entities and other ""garbage"" elements (from my point of view of course).</p>

<p>Could you please suggest ML features which can be used for ML training? Current results are: precision =0.6, recall=0.8</p>

<p>Thanks</p>
","nlp, classification, svm, sentiment-analysis",,1224,1342502693
How to parse text too variable for formal grammars but too constrained for NLP?,"<p>I'm dealing with a corpus of text that is written informally, but generally conforms to a very standard format by convention (think something like <em>Froyo Frozen Yogurt</em>, <em>Smucker's Peanut Butter</em>) and occasionally requires recursion (<em>Froyo Frozen Yogurt with Smucker's Peanut Butter</em>). </p>

<p>With regexes, the complexity quickly grows out of hand (<em>Frozen Yogurt by Froyo</em>, <em>Froyo Frozen Yogurt with Peanut Butter by Smucker's</em>, etc).</p>

<p>I'm having trouble finding resources to help me write an EBNF for this, and NLP method are too complex (plus my ""parts of speech"" don't really correspond to normal english). Is there an intermediate approach, aimed at semi-formalized text? </p>
","parsing, nlp, linguistics",,150,1339541837
NLTK stem words produces odd results,"<p>After running <code>nltk.stem.porter.PorterStemmer().stem_word(word)</code> I get many words with 'ing' cut off or 'y' swapped with 'i' . e.g. 'Quality' becomes 'Qualiti' and (even stranger) 'value' becomes 'valu'?</p>

<p>As the resulting words are not actual english words, I am not sure how am I meant to use them? My best guess is that I am meant to put the stem words into another function which will give me all the derived/child words from this stem (e.g. 'valu' would return <code>['valuing','valued', 'values', ...]</code>. Is there such a function?</p>
","python, nlp, nltk","<p>Stemming extracts the stem of the word by going through a series of transformation rules which strip off common suffixes and prefixes. Hence the result produced may not be an actual English word.
The general use of stemming is to normalize words so that they are considered the same. For example:</p>

<pre><code>stem_word('value') == stem_word('valuing')
</code></pre>

<p>The stemmed words can then be indexed for searching. Same stemming is done with the incoming query so that query words match the stemmed words in the index when doing the lookup.</p>
",2141,1342493031
Change a synonym lemma according to a POS Tag value,"<p>I need to replace a a word with its synonym in a sentence.
For that, I POS Tag each word in the sentence and for the word I wish to replace I find the best matching synonym Wordnet synset.
Now, what I am missing is the ability to take the synonym lemma and change it according to the original word POS Tag value.
I was wondering whether anyone knows of a library in C++ or Python that I can take a lemma and the POS Tag value and change the lemma according to the input POS Tag...
For instance, the sentence:
""The grand jurry commented""</p>

<p>The word 'commented' was pos tagged as 'VBD' (past tense)
I can take the synonym: 'remark' and need to change it to 'remarked' -> as 'VBD' is a past tense of the verb</p>
","c++, python, nlp, wordnet",,574,1342282811
Stanford NLP: Loading the parse model fails,"<p>I've got a problem loading the Parse models from Stanford NLP tools.
I deserialize the model file like i should, but it fails when it comes to the binary grammar part, with this exception:</p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException
at edu.stanford.nlp.parser.lexparser.BinaryGrammar.init(BinaryGrammar.java:216)
at edu.stanford.nlp.parser.lexparser.BinaryGrammar.readObject(BinaryGrammar.java:203)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351)
...
</code></pre>

<p>Did anyone experience similar errors? What could be the cause?</p>

<p><strong>EDIT:</strong></p>

<p>Here the code (nothing surprising, just plain deserialization).</p>

<pre><code>        ObjectInputStream in;
    InputStream is = null;
    try {
        is = aUrl.openStream();

        in = new ObjectInputStream(new BufferedInputStream(new GZIPInputStream(is)));

        ParserData parserdata = (ParserData) in.readObject();
        in.close();
        return parserdata;
    } catch (ClassNotFoundException e) {
        throw new IOException(e);
    } finally {
        is.close();
    }
</code></pre>

<p>I am not sure about the version i use, as i get the library from a private repository (as i do the model), but i will check which versions are in there.</p>

<p><strong>EDIT2:</strong></p>

<p>After checking versions, i found out i used a model from 2011 with a parser version from 2012 (still not exactly sure which versions were used here). Using a newer model file resolved the problem, sorry to bother you.</p>
","java, nlp, stanford-nlp",,435,1342291882
Lowest Common Ancestor of multiple nodes in a n-ary tree,"<p>I am trying to implement LCA of multiple nodes in an n-ary tree in java. I am working with parse trees of sentences, So its reasonable to assume that number of children of a node &lt;= 6. Multiple nodes here are two phrases(continuous word sequence) in a sentence. Let k be the number of nodes involved.</p>

<p>One way is to find the LCA of two nodes for k/2 pairs and we will get k/2 nodes. Now recurse on these k/2 nodes. The order will be O(nlog k), where O(n) is the complexity of linear LCA finding algorithms. Can I do it more efficiently ?</p>
","java, tree, nlp, text-processing, graph-algorithm","<p>I solved the problem using the fact that the nodes of the phrases are continuous i.e. have continuous indices in the list of leaf nodes of a parse tree. </p>

<p>Let <code>segment1</code> have indices from <code>start1</code> to <code>end1</code>. Same be the case for <code>segment2 = (start2,end2)</code>.</p>

<p>The Required Common Ancestor of <code>(start1, end1)</code> and <code>(start2, end2)</code> is the common ancestor of nodes with indices <code>min(start1,start2)</code> and <code>max(end1,end2)</code>.</p>
",2129,1337844023
What area deals with the extraction of words with similar characteristics?,"<p>I have a dataset with annotations in the form: <code>&lt;Word/Phrase, Ontology Class&gt;</code>, where Ontology Class can be one of the following <code>{Physical Object, Action, Quantity}</code>. I have created this dataset manually for my particular ontology model from a large corpus of text. </p>

<p>Because this process was manual, I am sure that I may have missed some words/phrases from my corpus. If such is the case, I am looking at ways to automatically extract other words from the same corpus that have the ""characteristics"" as these words in the labeled dataset. Therefore, the first task is to define ""characteristics"" before I even go with the task of extracting other words.</p>

<p>Are there any standard techniques that I can use to achieve this?</p>

<p><strong>EDIT:</strong> Sorry. I should have mentioned that these are domain-specific words not found in WordNet.</p>
","python, machine-learning, nlp, nltk","<p>Take a look at <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html"" rel=""nofollow"">chapter 6</a> of the NLTK book. From what you have described, it sounds like a supervised classification technique based on feature (""characteristic"") extraction might be a good choice. From the book:</p>

<blockquote>
  <p>A classifier is called supervised if it is built based on training
  corpora containing the correct label for each input.</p>
</blockquote>

<p>You can use some of the data that you have manually encoded to train your classifier. It might look like this:</p>

<pre><code>def word_features(name):
    features = {}
    features[""firstletter""] = name[0].lower()
    features[""lastletter""] = name[-1].lower()
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        features[""count(%s)"" % letter] = name.lower().count(letter)
        features[""has(%s)"" % letter] = (letter in name.lower())
    return features
</code></pre>

<p>Next you can train your classifier on some of the data you have already tagged:</p>

<pre><code>&gt;&gt; words = [('Rock', 'Physical Object'), ('Play', 'Action'), ... ]
&gt;&gt;&gt; featuresets = [(word_features(n), g) for (n,g) in words]
&gt;&gt;&gt; train_set, test_set = featuresets[500:], featuresets[:500]
&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)
</code></pre>

<p>You should probably train on half of the data you already tagged. That way you can test the accuracy of the classifier with the other half. Keep working on the features until the accuracy of the classifier is as you desire.</p>

<pre><code>nltk.classify.accuracy(classifier, test_set)
</code></pre>

<p>You can check individual classifications as follows:</p>

<pre><code>classifier.classify(word_features('Gold'))
</code></pre>

<p>If you are not familiar with NLTK, then you can read the previous chapters as well.</p>
",467,1342225498
Using WordNet to determine semantic similarity between two texts?,"<p>How can you determine the semantic similarity between two texts in python using WordNet? </p>

<p>The obvious preproccessing would be removing stop words and stemming, but then what?</p>

<p>The only way I can think of would be to calculate the WordNet path distance between each word in the two texts. This is standard for unigrams. But these are large (400 word) texts, that are natural language documents, with words that are not in any particular order or structure (other than those imposed by English grammar). So, which words would you compare between texts? How would you do this in python? </p>
","python, nlp, nltk, wordnet, semantic-analysis",,7569,1342146952
NLTK multiple feature sets in one classifier?,"<p>In NLTK, using a naive bayes classifier, I know from examples its very simply to use a ""bag of words"" approach and look for unigrams or bigrams or both. Could you do the same using two completely different sets of features?</p>

<p>For instance, could I use unigrams and length of the training set (I know this has been mentioned once on here)? But of more interest to me would be something like bigrams and ""bigrams"" or combinations of the POS that appear in the document?</p>

<p>Is this beyond the power of the basic NLTK classifier?</p>

<p>Thanks
Alex</p>
","python, nlp, nltk","<p>NLTK classifiers can work with any key-value dictionary. I use <code>{""word"": True}</code> for text classification, but you could also use <code>{""contains(word)"": 1}</code> to achieve the same effect. You can also combine many features together, so you could have <code>{""word"": True, ""something something"": 1, ""something else"": ""a""}</code>. What matters most is that your features are consistent, so you always have the same kind of keys and a fixed set of possible values. Numeric values can be used, but the classifier isn't smart about them - it will treat numbers as discrete values, so that 99 and 100 are just as different as 1 and 100. If you want numbers to be handled in a smarter way, then I recommend using scikit-learn classifiers.</p>
",1635,1342124886
Algorithms/theory behind predictive autocomplete?,"<p>Simple word autocomplete just displays a list of words that match the characters that were already typed. But I would like to order the words in the autocomplete list according to the probability of the words occuring, depending on the words that were typed before, relying on a statistical model of a text corpus. What algorithms and data structures do I need for this? Can you give me links for good tutorials? </p>
","algorithm, text, autocomplete, nlp, probability","<p>Peter norvig had an article <a href=""http://norvig.com/spell-correct.html"" rel=""noreferrer"">How to Write a Spelling Corrector</a> that explains how Google's <em>Did you mean...?</em> feature works that uses Bayesian inference to make it effective. It is a very good read, and should be adaptable to an autocomplete feature.</p>
",12876,1342086198
NLTK named entity recognition in dutch,"<p>I am trying to extract named entities from dutch text. I used <a href=""https://github.com/japerk/nltk-trainer/"" rel=""noreferrer"">nltk-trainer</a> to train a tagger and a chunker on the conll2002 dutch corpus. However, the parse method from the chunker is not detecting any named entities. Here is my code:</p>

<pre><code>str = 'Christiane heeft een lam.'

tagger = nltk.data.load('taggers/dutch.pickle')
chunker = nltk.data.load('chunkers/dutch.pickle')

str_tags = tagger.tag(nltk.word_tokenize(str))
print str_tags

str_chunks = chunker.parse(str_tags)
print str_chunks
</code></pre>

<p>And the output of this program:</p>

<pre><code>[('Christiane', u'N'), ('heeft', u'V'), ('een', u'Art'), ('lam', u'Adj'), ('.', u'Punc')]
(S Christiane/N heeft/V een/Art lam/Adj ./Punc)
</code></pre>

<p>I was expecting Christiane to be detected as a named entity.
Any help?</p>
","python, nlp, nltk, named-entity-recognition","<p>The <code>conll2002</code> corpus has both spanish and dutch text, so you should make sure to use the <code>fileids</code> parameter, as in <code>python train_chunker.py conll2002 --fileids ned.train</code>. Training on both spanish and dutch will have poor results.</p>

<p>The default algorithm is a Tagger based Chunker, which does not work well on conll2002. Instead, use a classifier based chunker like NaiveBayes, so the full command might look like this (and I've confirmed that the resulting chunker does recognize ""Christiane"" as a ""PER""):</p>

<p><code>python train_chunker.py conll2002 --fileids ned.train --classifier NaiveBayes --filename ~/nltk_data/chunkers/conll2002_ned_NaiveBayes.pickle</code></p>
",5989,1341230050
Reconstruct original sentence from smaller phrases?,"<p>I have an original sentence </p>

<pre><code>sent = ""For 15 years photographer Kari Greer has been documenting wildfires and the men and women who battle them.""
</code></pre>

<p>and phrases:</p>

<pre><code>phrases = [
  ""For 15 years"",
  ""wildfires and the men and women who battle them"",
  ""has been documenting wildfires"",
  ""been documenting wildfires and the men and women who battle them"",
  ""documenting wildfires and the men and women who battle them"",
  ""them"",
  ""and the men and women who battle them"",
  ""battle them"",
  ""wildfires"",
  ""the men and women"",
  ""the men and women who battle them"",
  ""15 years"",
  ""photographer Kari Greer""
]
</code></pre>

<p>I want to reconstruct the original sentence from the phrases (without loosing any words) and store selected phrases in the new array keeping the order so that I get: </p>

<pre><code> result = [
   ""For 15 years"",
   ""photographer Kari Greer"",
   ""has been documenting wildfires"",
   ""and the men and women who battle them""
]
</code></pre>

<p><strong>Edit</strong>: It is important that the <code>result</code> has the minimum number of elements.</p>

<p><strong>Edit</strong>: Here is the version of the answer code that works for the more complex case:</p>

<pre><code> sent =""Shes got six teeth Pink says of her 13-month-old daughter but shes not a biter""      
 phrases = [""her 13-month-old daughter"", ""she"", ""says of her 13-month-old daughter"", ""a biter"", ""got six teeth"", ""Pink"", ""of her 13-month-old daughter"", ""s not a biter"", ""She"", ""six teeth"", ""s got six teeth"", ""Shes got six""] 

def shortest(string, phrases)
 string = string.gsub(/\.|\n|\'|,|\?|!|:|;|'|""|`|\n|,|\?|!/, '')
 best_result = nil
 phrases.each do |phrase|
  if string.match(/#{phrase}/)
    result = [phrase] + shortest(string.sub(/#{phrase}/, """").strip, phrases)
        best_result = result  if (best_result.nil? || result.size &lt; best_result.size) # &amp;&amp; string == result.join("" "")
      end
    end
  best_result || []
end
</code></pre>
","ruby, string, nlp","<pre><code>def shortest(string, phrases)
  best_result = nil
  phrases.each do |phrase|
    if string.match(/\A#{phrase}/)
      result = [phrase] + shortest(string.sub(/\A#{phrase}/, """").strip, phrases)
      best_result = result if (best_result.nil? || result.size &lt; best_result.size) &amp;&amp; string.match(Regexp.new(""\\A#{result.join(""\\s?"")}\\Z""))
    end
  end
  best_result || []
end
result = shortest(sent.gsub(/\./, """"), phrases)
</code></pre>

<p><strong>Edit:</strong> Updated the algorithm to allow some phrases to not have spaces between them.</p>
",520,1341262034
NLP Parser in Haskell,"<p>Does Haskell have a good 
(a) natural language parser
(b) part of speech tagger
(c) nlp library (a la python's nltk)</p>
","haskell, nlp","<p>Have a look at Hackage:</p>

<ul>
<li><a href=""http://hackage.haskell.org/packages/archive/pkg-list.html#cat:natural%20language%20processing"" rel=""noreferrer"">Hackage#Natural Language Processing</a></li>
</ul>

<p>and</p>

<ul>
<li><a href=""http://hackage.haskell.org/package/Dao"" rel=""noreferrer"">Dao program: An interactive knowledge base, natural language interpreter</a>.</li>
</ul>
",2988,1341995581
What approch for simple text processing in Haskell?,"<p>I am trying to do some simple text processing in Haskell, and I am wondering what might me the best way to go about this in an FP language. I looked at the parsec module, but this seems much more sophisticated than I am looking for as a new Haskeller. What would be the best way to strip all the punctuation from a corpus of text? My naive approach was to make a function like this:</p>

<pre><code>removePunc str = [c | c &lt;- str, c /= '.',
                                 c /= '?',
                                 c /= '.',
                                 c /= '!',
                                 c /= '-',
                                 c /= ';',
                                 c /= '\'',
                                 c /= '\""',]
</code></pre>
","haskell, nlp","<p>You can simply write your code:</p>

<pre><code>removePunc = filter (`notElem` "".?!-;\'\"""")
</code></pre>

<p>or</p>

<pre><code>removePunc = filter (flip notElem "".?!-;\'\"""")
</code></pre>
",1342,1341968783
what kind of an approach should I use to extract semantics from a sentence,"<p>Say for an example there is a sentence ""Create a customer in the CRM and set his age to 25 and gender to male."". I would like to extract information like gender is male, age is 25, command is create a customer. How can I do this?</p>

<p>I've tried Semantic role labelers like mate-tools and it cannot extract things like age is 25, gender is male.
Is there a particular way or method or a pipeline I can do this successfully and what kind of an accuracy can I expect?</p>
",nlp,"<p>Is your input constrained or are you expecting to parse standard everyday English? If you have to recognise just a few phrases you can get away with a rule-based system backed by a dependency parser (i.e. find the keyword ""create"", use the parser to work out it is a customer that the user wants created; then match the known attributes of the customer by following the dependencies in a similar way). If the input is unconstrained, you can forget about it.</p>
",284,1341994643
"Determine whether a romanized name is Japanese or not, preferably in Ruby","<p>How can I determine whether a romanized name is likely, or unlikely, to be a Japanese name?</p>

<pre><code>""Yukihiro Matsumoto"".likely_to_be_japanese? # =&gt; true
""John Smith"".likely_to_be_japanese? # =&gt; false
</code></pre>

<p>Ideally, I'd also like to feed in free-form text, and detect whether the text has a name likely to be Japanese, has a name unlikely to be Japanese, or doesn't have any names in it. Preferably ignoring false positives.</p>

<pre><code>""call Koichi on (02) 5550 5555"".has_japanese_name_in_it? # =&gt; true
""call John on (02) 5550 5556"".has_non_japanese_name_in_it? # =&gt; true
""utility bill to be shared equally"".has_non_japanese_name_in_it? =&gt; false
</code></pre>

<p>Are there any libraries that can help me do this, preferably in Ruby? Or would I have to find a corpus of Japanese, and non-Japanese, names and build my own solution?</p>
","ruby, nlp, cjk",,413,1341962180
Set_Weights in NLTK Maxent?,"<p>I'm wondering what the set_weights method of the Maxent class in NLTK is used for (or more specifically how to use it). As I understand, it allows you to manually assign weights to certain features? Could somebody provide a basic example of the type of parameter that would be passed into it?</p>

<p>Thanks
Alex</p>
","python, machine-learning, nlp, nltk","<p>It apparently allows you to set the coefficient matrix of the classifier. This may be useful if you have an external MaxEnt/logistic regression learning package from which you can export the coefficients. The <code>train_maxent_classifier_with_gis</code> and <code>train_maxent_classifier_with_iis</code> learning algorithms call this function.</p>

<p>If you don't know what a coefficient matrix is; it's the β mentioned in <a href=""https://en.wikipedia.org/wiki/Multinomial_logit#Model"" rel=""nofollow"">Wikipedia's treatment of MaxEnt</a>.</p>

<p>(To be honest, it looks like NLTK is either leaking implementation details here, or has a very poorly documented API.)</p>
",445,1341865799
How to search a String in Class in c#,"<p>I am developing an app in which i have some data fetched from net into a class.
Class is</p>

<pre><code>public class Detail
{
        public string name { get; set; }
        public List&lt;Education&gt; education { get; set; }
        public City city { get; set; }
        public List&lt;Work&gt; work { get; set; }
}

public class Education
{
        public string DegreeName { get; set; }
}

public class City 
    {
        public string name { get; set; }
    }
public class Work
    {
        public string name { get; set; }
    }
</code></pre>

<p>Data is stored for a person in the above class.</p>

<p>Now i want to search for a string say <code>q="" Which Manager Graduated From USA ?""</code></p>

<p>So i want it to search for the above query...</p>

<p>Based on how much words matched, i want to give the Name of user. So searching for person if he is <em>a Manager Graduated From USA ?</em> (may be less words, for search like some <em>Director from India</em>)</p>

<p>The approach i am trying to look for words like <em>Manager</em> in <code>Work</code> and <em>Graduate</em> in <code>Education</code> and <em>Location</em> for <code>USA</code> </p>

<p>I am making an array of search string</p>

<pre><code>string[] qList = q.Split(' ');
</code></pre>

<p>and then traverse through the class. But i don't have any idea of how to (efficiently) look for data in the class.</p>

<p>And is my approach good enough for search or is there any better option ?</p>
","c#, asp.net, asp.net-mvc, string, nlp",,2532,1341905431
How to tokenize italian input?,"<p>I'm trying to tokenize italian text for further processing, in Java.
Is there any tool that for tokenizing italian input?
A <a href=""http://code.google.com/p/tt4j/wiki/SimpleTokenizer"" rel=""nofollow"">SimpleTokenizer</a> works fine a certain extent, but then in case like Italian family names like ""De Marchi"" I get it as 2 tokens.</p>
","java, nlp, tokenize",,684,1341830143
Text simplification using machine learning,"<p>I'm through a project which is about text simplification, there are several open sources which provide the parser of text such as Stanford parser. wondering if there any parser which is able to parse a text using machine learning!</p>
","java, parsing, machine-learning, nlp, stanford-nlp",,696,1341718566
English Word Declension and Conjugation,"<p>I am currently working on a Natural Language Parser and I need to be able to conjugate English Verbs and Nouns.</p>

<p>I already have a list of Verb Irregulars but I am struggling to find a set of ""rules"" if you will, for conjugating regular English verbs. I know there are a few, various rules such as: ""if the word ends in 'X', then the plural form would end in 'Y'"" with the most basic being add an s to the end.</p>

<p>I am looking for the rules for finding the: Base form, Past simple, Past participle, 3rd person singular, Present participle Gerund</p>

<p>Also, I would be looking to do the same for finding the plurals and possessive forms of any given noun, along with a list of regulars. I have had no source of results or luck in my searching in this area and any help with conjugating (for lack of a better word) nouns would be very helpful.</p>

<p>[edit]
A link to a list of irregular nouns and a list of rules like: if the word ends in a consonant, then ad ""s"" (or whatever) would be awesome!!!
[/edit]</p>

<p>One more thing... for my english ver irregulars, I am using <a href=""http://www.online-languages.info/english/irregular_verbs.php?l=1"" rel=""nofollow"">this site</a></p>

<p>Sorry for what appears to be the lack of searching, trust me I have looked.</p>
",nlp,,744,1341472553
Find two phrases from the larger sentence with the least overlap,"<p>I have:</p>

<pre><code>phrase = ""will have to buy online pass from EA to play online but its in perfect condition"" 

phrases = [""its"",
""perfect condition"",
""but its"",
""in perfect condition"",
""from EA"",
""buy online pass from EA"",
""to play online but its in perfect condition"",
""online"",
""online pass"",
""play online but its in perfect condition"",
""online but its"",
""EA"",
""will have to buy online pass from EA to play online but its in perfect condition"",
""have to buy online pass from EA to play online but its in perfect condition"",
""u"",
""pass"",
""to buy online pass from EA""]
</code></pre>

<p>I would like to find two phrases from the array that are within 6-10 words limit and have least overlap word-wise... </p>

<p>Something like:</p>

<pre><code>result = [""to buy online pass from EA"", ""play online but its in perfect condition""]
</code></pre>

<p>would be perfect.. What is the best way to do it?</p>
","ruby, string, nlp","<pre><code>split_phrases = phrases.map {|phrase| phrase.split }

# find number of words of overlap between two word vectors
def overlap(p1,p2)
  s1 = p1.size
  s2 = p2.size

  # make p1 the longer phrase
  if s2 &gt; s1
    s1,s2 = s2,s1
    p1,p2 = p2,p1
  end

  # check if p2 is entirely contained in p1
  return s2 if p1.each_cons(s2).any? {|p| p == p2}

  longest_prefix = (s2-1).downto(0).find { |len| p1.first(len) == p2.last(len) }
  longest_suffix = (s2-1).downto(0).find { |len| p2.first(len) == p1.last(len) }

  [longest_prefix, longest_suffix].max
end

def best_two_phrases_with_minimal_overlap(wphrases, minlen=6, maxlen=10)
  # reject too small or large phrases, evaluate every combination, order by word overlap
  scored_pairs = wphrases.
    select {|p| (minlen..maxlen).include? p.size}.
    combination(2).
    map { |pair| [ overlap(*pair), pair ] }.
    sort_by { |tuple| tuple.first }

  # consider all pairs with least word overlap
  least_overlap = scored_pairs.first.first
  least_overlap_pairs = scored_pairs.
    take_while {|tuple| tuple.first == least_overlap }.
    map {|tuple| tuple.last }

  # return longest pair with minimal overlap
  least_overlap_pairs.sort_by {|pair| pair.first.size + pair.last.size }.last
end

puts best_two_phrases_with_minimal_overlap(split_phrases).map{|p| p.join ' '}

# to play online but its in perfect condition
# to buy online pass from EA
</code></pre>
",230,1341615067
regarding using lucene in other languages and tools,"<p>Is that possible to import Lucene index using R or Python? In terms of the text mining purposes, does Lucene index perform much better than traditional text feature construction approaches, e.g., bag-of-words?</p>
","python, r, lucene, full-text-search, nlp",,101,1341580825
is MaltParser the &quot;Nivre&quot; parser mentioned in Parsing to Stanford Dependencies Trade-offs between speed and accuracy?,"<p>hard to find rich info about Nivre parser, google lead to Maltparser, is it the same?</p>

<p>is it the one mentioned in the paper Parsing to Stanford Dependencies Trade-offs between speed and accuracy?</p>

<p>it performs quite well in the paper, why is it so unpopular?</p>
","parsing, nlp","<p>The MaltParser is the implementation of all the algorithms published by Nivre et al. So whenever experimental data is reported for the Nivre algorithm(s), it is probably the result of running the MaltParser.</p>
",411,1340146635
Python: Clustering Search Engine Keywords,"<p>Python: Clustering Search Engine Keywords </p>

<p>Hi,
I have a CSV, up to 20,000 rows (I have had 100,000+ for different websites), each row containing a referring keyword (i.e.  a keyword someone typed into a search engine to find the website in question), and a number of visits.</p>

<p>What I'm looking to do is cluster these keywords into clusters of ""similar meaning"", and create a hierarchy of the clusters (structured in order of summed total number of searches per cluster).</p>

<p>An example cluster - ""womens clothing"" - would ideally contain keywords along these lines: 
womens clothing, 1000 
ladies wear, 300 
womens clothes, 50 
ladies clothing, 6 
womens wear, 2 </p>

<p>I could look to use something like the Python Natural Language Toolkit: <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">http://www.nltk.org/</a> and WordNet, but, I'm guessing that for some websites the referring keywords will be words/phrases that WordNet knows nothing about. For example, if the website is a celebrity website WordNet is unlikely to know anything about ""Lady Gaga"", worse situation if the website is a news website.</p>

<p>So, I'm also guessing therefore that the solution has to be one that looks to use just the source data itself.</p>

<p>My query is very similar to the one raised at <a href=""https://stackoverflow.com/questions/4617023/how-to-cluster-search-engine-keywords"">How to cluster search engine keywords?</a>, only I'm looking for somewhere to start but using Python instead of Java.</p>

<p>I did also wonder whether Google Predict and/or Google Refine might be of any use.</p>

<p>Anyway, any thoughts/suggestions most welcome,</p>

<p>Thanks,
C</p>
","python, text, nlp, cluster-analysis, keyword",,6655,1301309928
Clustering Data,"<p>I have millions of names stored in my database these names are nothing but customer names,<br>
I Have to cluster names which are phonetically similar to each other internally,<br>
one approach that i am using is matching each name with some selective similar names fetched from database based on sound-ex,meta-phone,initials..etc<br>
But it is very slow ,<br>
now i am thinking about generating unique id for each names and clustering similar unique ids,
but i am not able to generate unique ids.
there Names are Indian names and written using English Alphabet.<br>
Is there any algorithm for clustering similar names.
please help </p>
","java, machine-learning, nlp, cluster-analysis",,212,1337179083
extracting English verbs from a given text,"<p>I need to extract all English verbs from a given text and I was wondering how I could do it...
At first glance, my idea is to use regular expressions because all English verb tenses follow patterns but maybe there is another way to do it. What I've thought is simply:</p>

<ol>
<li>Create a pattern for every verb tense. I have to distinguish between regular verbs (http://en.wikipedia.org/wiki/English_verbs) and irregular verbs (http://www.chompchomp.com/rules/irregularrules01.htm) in some way.</li>
<li>Iterate over these patterns and split the text using them (the last word of each substring is supposed to be the verb that gives complete meaning to the sentence, which I need for other purposes -> nominalization)</li>
</ol>

<p>What do you think? I guess this isn't an efficient way to do it but I can't imagine another one.</p>

<p>Thank you in advance!</p>

<p>PS: </p>

<ol>
<li>I have two dictionaries, one for all English Verbs and the other one for all English nouns</li>
<li>The main problem of all this is that the project consists on verb nominalization (is just a uni project), so all the ""effort"" is supposed to be focused in this part, nominalization. In concrete, I follow this model: acl.ldc.upenn.edu/P/P00/P00-1037.pdf). The project consists on given a text, find all the verbs in that text and propose multiple nominalizations for each verb. So the first step (finding verbs), should be as simple as possible... but I can't use any parser, it's not allowed</li>
</ol>
","java, regex, nlp",,15522,1300878259
"Can you programmatically detect pluralizations of English words, and derive the singular form?","<p><strong>Given some (English) word that we shall assume is a plural</strong>, is it possible to derive the singular form? I'd like to avoid lookup/dictionary tables if possible.</p>

<p>Some examples:</p>

<pre>
Examples  -> Example    a simple 's' suffix
Glitch    -> Glitches   'es' suffix, as opposed to above
Countries -> Country    'ies' suffix.
Sheep     -> Sheep      no change: possible fallback for indeterminate values
</pre>

<p>Or, <a href=""http://en.wiktionary.org/wiki/Appendix:Irregular_plurals:English"" rel=""nofollow noreferrer"">this seems to be a fairly exhaustive list.</a></p>

<p>Suggestions of libraries in language <code>x</code> are fine, as long as they are open-source (ie, so that someone can examine them to determine how to do it in language <code>y</code>)</p>
","language-agnostic, nlp, stemming, lemmatization","<p>It really depends on what you mean by 'programmatically'.  Part of English works on easy to understand rules, and part doesn't. It has to do mainly with frequency. For a brief overview, you can read Pinker's ""Words and Rules"", but do yourself a favor and don't take the whole generative theory of linguistics entirely to heart. There's a lot more empiricism there than that school of thought really lends to the pursuit.</p>

<p>A lot of English can be statistically lemmatized.  By the way, stemming or lemmatization is the term you're looking for.  One of the most effective lemmatizers which work off of statistical rules bootstrapped with frequency-based exceptions is the <a href=""http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html"" rel=""noreferrer"">Morpha Lemmatizer</a>.  You can give this a shot if you have a project that requires this type of simplification of strings which represent specific terms in English.</p>

<p>There are even more naive approaches that accomplish much with respect to normalizing related terms. Take a look at the <a href=""http://tartarus.org/~martin/PorterStemmer/"" rel=""noreferrer"">Porter Stemmer</a>, which is effective enough to cluster together <em>most</em> terms in English.</p>
",4956,1252033792
Item matching with domain knowlege,"<p>I have various product items that I need to decide if they are the same. A quick example:</p>

<p><code>Microsoft RS400 mouse with middle button</code> should match <code>Microsoft Red Style 400 three buttoned mouse</code> but not <code>Microsoft Red Style 500 mouse</code></p>

<p>There isn't anything else nice that I can match with apart from the name and just doing it on the ratio of matching words isn't good enough (The error rate is far too high)</p>

<p>I do know about the domain and so I can (for example) hand write the fact that a three buttoned mouse is probably the same as a mouse with a middle button. I also know the manufacturers (or can take a very good guess at them).</p>

<p>The only thought I have had so far is matching them by trying to use hand written rules to reduce the size of the string and then checking the matching words, but I wondered if anyone had any ideas best way of doing this matching was with a better accuracy and precision (or where to start looking) and if anyone knew of any work that had been done in this area? (papers, examples etc).</p>
","language-agnostic, nlp, string-matching",,127,1340699794
Guess tags of a paragraph programmatically using python,"<p>I've trying to read about NLP in general and nltk in specific to use with python. I don't know for sure if what am looking for exists out there, or if I perhaps need to develop it. </p>

<p>I have a program that collect text from different files, the text is extremely random and talks about different things. Each file contains a paragraph or 3 maximum, my program opens the files and store them into a table. </p>

<p>My question is, can i guess tags of what the paragraph is about? if anyone knows of an existing technology or approach, I would really appreciate it. </p>

<p>Thanks,</p>
","python, nlp, nltk","<p>Your task is called ""document classification"", and the <a href=""https://sites.google.com/site/naturallanguagetoolkit/book"" rel=""nofollow"">nltk book</a> has a whole chapter on it. I'd start with that. </p>

<p>It all depends on your criteria for assigning tags. Are you interested in matching your documents against a pre-existing set of tags, or perhaps in topic extraction (select the N most important words or phrases in the text)? </p>
",396,1339862416
Unconventional named-entity recognition,"<p>I'm trying to design a somewhat unconventional NER system that marks certain multiword strings as single units/tokens. </p>

<p>There are a lot of cool NER tools out there, but I have a few special needs that make it pretty much impossible to use something straight out of the box:</p>

<p>First, the entities can't just be extracted and printed out in a list--they need to be marked in some way and consolidated into tokens.</p>

<p>Second, categorization is not important--Person/Organization/Location doesn't matter (at least in the output).</p>

<p>Third, these aren't just your typical ENAMEX named entities we're looking for.  We want companies and organizations, but also concepts like 'climate change' and 'gay marriage.'  I've seen tags like these on some tools out there, but all of them were 'extraction-style'.</p>

<p>How would I got about getting this type of functionality?  Would training the Stanford tagger on my own, hand-annotated dataset do the job (where 'climate change'-esque phrases are labeled MISC or something)?  Or am I better off just making a shortlist of the 'weird' entities and checking the text against that after it's been run through a regular NER system?</p>

<p>Thanks so much!</p>
","python, nlp, nltk, stanford-nlp","<p>The underlying CRF model of a named entity tagger such as Stanford NER can actually be used to recognize anything, not just named entities. There are certainly people who have used them quite successfully to pick out various kinds of terminological phrases. The software can certainly give you marked up token sequences in context.</p>

<p>There is, however, a choice as to whether to approach this in a ""more unsupervised"" way, where something like NP chunking and collocation statistics are used, or the fully supervised way of a straightforward CRF, where you're providing lots of annotated data of the kind of phrases you'd like to get out.</p>
",1576,1340221732
Javascript regex retrieve variables from sentence,"<p>I'm wondering if it's possible to extract a number of variables from a predefined sentence with regex or similar.</p>

<p>e.g.</p>

<p>If this was the pattern...</p>

<pre><code>""How many * awards did * win in *?""
</code></pre>

<p>And someone typed...</p>

<pre><code>""How many gold awards did johnny win in 2008?""
</code></pre>

<p>How can I somehow return... </p>

<pre><code>[""gold"",""johnny"",""2008""]
</code></pre>

<p>I'd also like to return the fact that it matches the pattern before retrieving the variables as there will be many different patterns. Note: It will also be possible for someone to type multiple words in place of a * e.g. <em>johnny english</em> instead of just <em>johnny</em></p>

<p>Thanks</p>
","javascript, regex, nlp","<p>Building on Derek's answer and SimpleCoder's comment, here would be the full function:</p>

<pre><code>// This function escapes a regex string
// http://simonwillison.net/2006/jan/20/escape/
function escapeRegex(text) {
    return text.replace(/[-[\]{}()*+?.,\\^$|#\s]/g, ""\\$&amp;"");
}

function match(pattern, text) {
    var regex = '^' + escapeRegex(pattern).replace(/\\\*/g, '(.+)') + '$';
    var query = text.match(new RegExp(regex, 'i'));
    if (!query)
        return false;

    query.shift(); // remove first element
    return query;
}

match(""How many * awards did * win in *?"", ""How many gold awards did johnny win in 2008?"");
</code></pre>
",124,1340822546
Natural Language Generation in PHP,"<p>I woke up last night with a thought in my head: Can PHP be used to generate random words that sound natural? (Like the Lorem ipsum verses).</p>

<ol>
<li>Words being single letter: 'a,e,i,o,u'</li>
<li>Words being double letter: any combination of vowel and consonant.</li>
<li>Maximum word length would be I think six letters.</li>
</ol>

<p>The purpose would be to fill space on website templates with this instead of 'Lorem ipsum', or send test emails for certain PHP scripts to make sure mail() works.</p>

<p>But my thoughts on how it would work are that PHP would generate random length words, 1-6 letters each, with a few ""don't do this"" rules like ""no two single-letter words next to each other"" or ""no three-vowels in a row"" or ""no three-consonants in a row"" and automatically add punctuation and capitalization after between 4 and 8 words to a sentence.</p>

<p>Would this be at all possible, and if so, are there any pre-existing classes or functions I could implement?</p>
","php, nlp",,1086,1340810699
Is there an algorithm for temporal characteristics of verbs (Zeno Vendler&#39;s paper)?,"<p>Is there an algorithm for finding temporal characteristics of verbs? Meaning if it's an ""event"", ""accomplishment"", ""achievement"" or ""state""? As described in Zeno Vendler's paper ""Verbs and Times""?</p>

<p><a href=""http://semantics.uchicago.edu/kennedy/classes/s07/events/vendler57.pdf"" rel=""nofollow"">http://semantics.uchicago.edu/kennedy/classes/s07/events/vendler57.pdf</a></p>

<p>Or maybe someone has an idea of what would be the best way to implement such thing?</p>

<p>Thanks!</p>
","algorithm, nlp, computer-science",,252,1337981695
Discover user behind multiple different user accounts according to words he uses,"<p>I would like to create algorithm to distinguish the persons writing on forum under different nicknames.</p>

<p>The goal is to discover people registring new account to flame forum anonymously, not under their main account.</p>

<p>Basicaly I was thinking about stemming words they use and compare users according to similarities or these words.</p>

<p><img src=""https://i.sstatic.net/ggqW0.png"" alt=""Users using words""></p>

<p>As shown on the picture there is user3 and user4 who uses same words. It means there is probably one person behind the computer.</p>

<p>Its clear that there are lot of common words which are being used by all users. So I should focus on ""user specific"" words.</p>

<p>Input is (related to the image above):</p>

<pre><code>&lt;word1, user1&gt;
&lt;word2, user1&gt;
&lt;word2, user2&gt;
&lt;word3, user2&gt;
&lt;word4, user2&gt;
&lt;word5, user3&gt;
&lt;word5, user4&gt;
... etc. The order doesnt matter
</code></pre>

<p>Output should be:</p>

<pre><code>user1
user2
user3 = user4
</code></pre>

<p>I am doing this in Java but I want this question to be language independent.</p>

<p><strong>Any ideas how to do it?</strong></p>

<p>1) how to store words/users? What data structures?</p>

<p>2) how to get rid of common words everybody use? I have to somehow ignore them among user specific words. Maybe I could just ignore them because they get lost. I am afraid that they will hide significant difference of ""user specific words""</p>

<p>3) how to recognize same users? - somehow count same words between each user?</p>

<p>I am very thankful for every advice in advance.</p>
","algorithm, language-agnostic, nlp","<p>I recommend a language modelling approach. You can train a <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow"">language model</a> (unigram, bigram, <a href=""http://eprints.eemcs.utwente.nl/7256/01/p178-hiemstra.pdf"" rel=""nofollow"">parsimonious</a>, ...) on each of your user accounts' words. That gives you a mapping from words to probabilities, i.e. numbers between 0 and 1 (inclusive) expressing how likely it is that a user uses each of the words you encountered in the complete training set. Language models can be stored as arrays of pairs, hash tables or <a href=""https://en.wikipedia.org/wiki/Sparse_matrix"" rel=""nofollow"">sparse vectors</a>. There are plenty of libraries on the web for fitting LMs.</p>

<p>Such a mapping can be considered a high-dimensional vector, in the same way documents are considered as vector in the <a href=""https://en.wikipedia.org/wiki/Vector_space_model"" rel=""nofollow"">vector space model</a> of information retrieval. You can then compare these vectors by using <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow"">KL-divergence</a> or any of the popular distance metrics: <a href=""https://en.wikipedia.org/wiki/Euclidean_distance"" rel=""nofollow"">Euclidean distance</a>, <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine distance</a>, etc. A strong similarity/small distance between two users' vectors might then indicate that they belong to one and the same user.</p>
",522,1332069961
Check English grammar,"<p>I am looking for a simple C# library that does the following: Takes a string representing a single sentence, and returns a boolean saying if it's grammatically correct.</p>

<p>I have not been able to find a single, self-contained library that does this after extensive searching.</p>
","c#, nlp, grammar","<p>If you have MS Word installed you can include a reference to the COM library ""Microsoft Word 12.0 Object Library"" in your project. (I think there may also be a .NET interop library, but I haven't tested it).</p>

<p>Then in your code you can simply invoke:</p>

<pre><code>Microsoft.Office.Interop.Word.Application myWord = new Microsoft.Office.Interop.Word.Application();

...

return myWord.CheckGrammar(""Your string here"");
</code></pre>

<p>And that should do it.</p>
",5034,1340588220
How to build a 4language dictionary from bilingual dictionaries,"<p>3 days ago I asked a question about building a parallel dictionary which has 3 languages: <a href=""https://stackoverflow.com/questions/11135928/removing-differencies-of-some-text-pairs"">removing differencies of some text pairs</a></p>

<p>The question was as follows:
 I have 2 pair of files by the following characteristics: pair1: (File1.txt , File2.txt) pair2: (File3.txt , File4.txt)</p>

<p>There is a line by line correspondence between each files in these pairs. say that File1.txt and File3.txt are some English words, and File2.txt and File4.txt are their Arabic and French translations respectively. In addition, File1.txt and File3.txt are very similar (and in some cases the same).</p>

<pre><code>
    File1.txt       File2.txt
    EnWord1         ArTrans1
    EnWord2         ArTrans2
    EnWord3         ArTrans3
    Enword4         ArTrans4

    File3.txt       File4.txt
    EnWord1         FrTrans1
    EnWord3         FrTrans3
    Enword4         FrTrans4
    Enword5         FrTrans5
</code></pre>

<p>What I wanted to do then, was to compare English sides of the pairs, find the common words that appear in both files (EnWord1,EnWord3, and EnWord4) and filter out their corresponding translations.
In short, I can say that using two bilingual English-Arabic and English French dictionaries, I am trying to build a 3-lingual English-Arabic-French dictionary.</p>

<p>Steve answered me and wrote a nice code to find duplicated English words and remove others and their translations: <a href=""https://stackoverflow.com/a/11141345/1265960"">The answer could be found here</a></p>

<p>But I still have a bit more complicated question:
What should I do, if I want to add another language? I mean that I have another English-Russian dictionary (Say File5.txt contains English entities, and File6.txt contains Russian entities) and I want to build a 4-language dictionary instead of a 3-language one.</p>

<p>one way is to build a 3-language dictionary using the current code, and then by rerunning it on a new language pair, build a 4-language dictionary. but I think it is not efficient enough, and it would be better solution to this problem. It also may bring in some inconsistencies in other languages.
My main challenge is checking the duplications: when have just 2 language pairs, it would be very easy to check the duplications. But what should I do if I want to check the duplications in 3 pairs?
How can I change the code to be able to extract 4language dictionary in just one pass?</p>
","linux, perl, text, nlp","<p>I'll describe a generic approach that I would use in this task.</p>

<p>1) define <code>%dictionary</code> hash. Each key of this hash would be an English word, and each value would be, in turn, a reference to another hash containing that word' translations. Something like this:</p>

<pre><code>my %dictionary = ( 
  'EnWord1' =&gt; { 
     arabic =&gt; 'Arabic EnWord1', 
     french =&gt; 'French EnWord1',
     ...
  },
  ...
);
</code></pre>

<p>(this hash will be empty before we start processing files, I'm just showing a structure here).</p>

<p>2) Scan each pair of files simultaneously, add a corresponding record to this hash. There's a simple approach:</p>

<pre><code>my %filenames = (
  'arabic' =&gt; ['File1.txt', 'File2.txt'],
  'french' =&gt; ['File3.txt', 'File4.txt'],
  ...
);

for my $lang (keys %filenames) {
  open my $efh, '&lt;', $filenames{$lang}[0] or die $!, ""\n"";
  open my $tfh, '&lt;', $filenames{$lang}[1] or die $!, ""\n"";
  while (&lt;$efh&gt;) {
    chomp(my $enLine = $_);
    chomp(my $trLine = &lt;$tfh&gt;);
    $dictionary{$enLine}{$lang} = $trLine;
  }
}
</code></pre>

<p>3) Refine <code>%dictionary</code>: leave only those elements, which have translations defined in all languages scanned...</p>

<pre><code>my $proper_translations_count = scalar keys %filenames;
for my $word (keys %dictionary) {
  my $translations = $dictionary{$word};
  if (scalar keys %$translations != $translations_count) {
    delete $dictionary{$word};
  }
}
</code></pre>

<p>4) Output <code>%dictionary</code> any way suitable.</p>
",425,1340423799
JAVA Separate the a sentence contains Part of Speech(POS) tag into free of POS tag sentence and just POS tag sentence?,"<p>Assume the sentences is:</p>

<blockquote>
  <p>It/pps urged/vbd that/cs the/at next/ap Legislature/nn-tl <code>/</code> provide/vb enabling/vbg funds/nns and/cc re-set/vb the/at effective/jj date/nn so/cs that/cs an/at orderly/jj implementation/nn of/in the/at law/nn may/md be/be effected/vbn ''/'' ./.</p>
</blockquote>

<p>The sentence above is take from brown corpus. How can I get the sentence free of all these POS tag and print and another sentence is just POS tag.</p>

<p>The sentence free of POS tag result like as below:</p>

<blockquote>
  <p>It urged that the next Legislature `` provide enabling funds and re-set the effective date so that an orderly implementation of the law may be effected '' .</p>
</blockquote>

<p>The sentence just POS tag result like as below:</p>

<blockquote>
  <p>pps vbd cs at ap nn-tl `` vb vbg nns cc vb at jj nn cs cs at jj nn in at nn md be vbn '' .</p>
</blockquote>
","java, string, nlp, stanford-nlp, pos-tagger",,475,1340359917
English query generation through machine translation systems,"<p>I'm working on a project to generate questions from sentences. Right now, I'm at a point where I can generate questions like:
""Angela Merkel is the chancelor of Germany."" -> ""Angela Merkel is who?""</p>

<p>Now, of course, I want the questions to look like ""Who is...?"" instead. Is there any easy way to do this that I haven't thought of yet?</p>

<p>My current idea would be to train an English(not quite question) -> English(question) translator, maybe using existing machine translation engines like moses. Is this overkill? How much data would I need? Are there corpora that address this or a similar problem? Is using a general translation engine even appropriate for this task?</p>
","machine-learning, nlp, translation","<p>Check out Michael Heilman's dissertation <a href=""http://www.ark.cs.cmu.edu/mheilman/questions/papers/heilman-question-generation-dissertation.pdf"" rel=""nofollow"">Automatic Factual Question Generation from Text</a> for background on question generation and to see what his approach to this problem looks like.  You can find more by searching for research on ""question generation"".  He mentions a corpus from Microsoft: the <a href=""http://research.microsoft.com/en-us/downloads/88c0021c-328a-4148-a158-a42d7331c6cf/"" rel=""nofollow"">Microsoft Research Question-Answering Corpus</a>.</p>

<p>I don't think that an approach based solely on (current) statistical machine translation approaches is going to work that well, since you're usually going to need a deeper syntactic analysis of the source sentence to do a good job of generating an appropriate question.  For simple questions like your example, it's pretty easy to design syntactic tree transformations to generate the question, but it gets much trickier as soon as the sentences get a little more complicated.</p>
",156,1340204730
Dictionary words for download,"<p>Can someone offer a suggestion on where to find a dictionary word list with frequency information?</p>

<p>Ideally, the source would be English words of the North American variety.</p>
","nlp, document-classification","<p>How about <a href=""http://en.wiktionary.org/wiki/Wiktionary%3aFrequency_lists"" rel=""nofollow"">this</a>?</p>
",7456,1290278812
Phrase corpus for sentimental analysis,"<p>Good day,
I'm attempting to write a sentimental analysis application in python (Using naive-bayes classifier) with the aim to categorize phrases from news as being positive or negative.
And I'm having a bit of trouble finding an appropriate corpus for that.
I tried using ""General Inquirer"" (http://www.wjh.harvard.edu/~inquirer/homecat.htm) which works OK but I have one big problem there.
Since it is a word list, not a phrase list I observe the following problem when trying to label the following sentence:</p>

<blockquote>
  <p>He is not expected to win.</p>
</blockquote>

<p>This sentence is categorized as being positive, which is wrong. The reason for that is that ""win"" is positive, but ""not"" does not carry any meaning since ""not win"" is a phrase.
Can anyone suggest either a corpus or a work around for that issue?
Your help and insight is greatly appriciated. </p>
","python, nlp, nltk","<p>See for example: ""What's great and what's not: learning to classify the scope of negation for improved sentiment analysis"" by Councill, McDonald, and Velikovich</p>

<p><a href=""http://dl.acm.org/citation.cfm?id=1858959.1858969"" rel=""nofollow"">http://dl.acm.org/citation.cfm?id=1858959.1858969</a></p>

<p>and followups, </p>

<p><a href=""http://scholar.google.com/scholar?cites=3029019835762139237&amp;as_sdt=5,33&amp;sciodt=0,33&amp;hl=en"" rel=""nofollow"">http://scholar.google.com/scholar?cites=3029019835762139237&amp;as_sdt=5,33&amp;sciodt=0,33&amp;hl=en</a></p>

<p>e.g. by Morante et al 2011</p>

<p><a href=""http://eprints.pascal-network.org/archive/00007634/"" rel=""nofollow"">http://eprints.pascal-network.org/archive/00007634/</a></p>
",1539,1338234961
Jython: ImportError: No module named multiarray,"<p>When I try to call file and its method using Jython it shows the following error, while my Numpy, Python and NLTK is correctly installed and it works properly if I directly run directly from the Python shell</p>

<pre><code>File ""C:\Python26\Lib\site-packages\numpy\core\__init__.py"", line 5, in &lt;module&gt;
import multiarray
ImportError: No module named multiarray
</code></pre>

<p>The code that I am using is simple one:</p>

<pre><code>PyInstance hello = ie.createClass(""PreProcessing"", ""None"");  
PyString str = new PyString(""my name is abcd"");
PyObject po = hello.invoke(""preprocess"", str);
System.out.println(po);
</code></pre>

<p>When I run only the file of python containing class <code>PreProcessing</code> and calling method  preprocess it works fine, but with Jython it throws error.</p>

<p>Jython is unable to import all the libraries that have only compiled version kept in the folder not the class code itself. Like instead of <code>multiarray.py</code> it only has <code>multiarray.pyd</code> that is the compiled version so it is not getting detected in Jython.</p>

<p>Why is it showing this behaviour? How to resolve it?</p>

<p>Please help!</p>
","python, numpy, nlp, jython, nltk",,2584,1312547520
Agreement feature extraction from a text,"<p>I'm going through a task where i have to extract the agreement feature of the nouns in the text...
The agreement feature such as:</p>

<pre><code>number = singular, plural
person = first, second, third
gender = male, female, neuter
animacy = animate, inanimate
</code></pre>

<p>is there anyway to extract these features from the text ....</p>
","java, nlp, stanford-nlp, opennlp",,743,1340001283
Detect a pronoun and its noun?,"<p>Wondering if there is any tool that can help me to detect a pronoun's name in a text.</p>

<p>Example</p>

<pre><code>Jone is Spanish. He can speak German.
</code></pre>

<p>How can I tag <code>He</code> to <code>Jone</code>?</p>
","java, nlp, weka, stanford-nlp","<p>The problem you are trying to solve is known as <a href=""http://en.wikipedia.org/wiki/Anaphora_%28linguistics%29#Anaphor_resolution"" rel=""noreferrer"">anaphora resolution</a>. A java tool to perform this task is available <a href=""http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html"" rel=""noreferrer"">here</a>. The source code is made available on that site. For a theoretical look at how it works, check out <a href=""http://acl.ldc.upenn.edu/J/J94/J94-4002.pdf"" rel=""noreferrer"">this paper</a> by Lappin and Leass from 1994.</p>
",2482,1333265949
Algorithm to compare similarity of ideas (as strings),"<p>Consider an arbitrary text box that records the answer to the question, what do you want to do before you die?</p>

<p>Using a collection of response strings (max length 240), I'd like to somehow sort and group them and count them by idea (which may be just string similarity as described in <a href=""https://stackoverflow.com/questions/653157/a-better-similarity-ranking-algorithm-for-variable-length-strings"">this question</a>).</p>

<ol>
<li>Is there another or better way to do something like this?</li>
<li><em>Is this any different</em> than string similarity?</li>
<li>Is this the right question to be asking?</li>
</ol>

<p>The idea here is to have people write in a text box over and over again, and me to provide a number that describes, generally speaking, that 802 people <em>wrote approximately the same thing</em></p>
","algorithm, nlp, artificial-intelligence","<p>It is much more difficult than string similarity. This is what you need to do at a minimum:</p>

<ul>
<li>Perform some text formatting/cleaning tasks like removing punctuations characters and common ""stop words""</li>
<li>Construct a corpus (collection of words with their usage statistics) from the terms that occur answers.</li>
<li>Calculate a weight for every term.</li>
<li>Construct a document vector from every answer (each term corresponds to a dimension in a very high dimensional Euclidian space)</li>
<li>Run a clustering algorithm on document vectors.</li>
</ul>

<p>Read a good statistical natural language processing book, or search google for good introductions / tutorials (likely terms: <strong>statistical nlp</strong>, <strong>text categorization</strong>, <strong>clustering</strong>) You can probably find some libraries (<strong>weka</strong> or <strong>nltk</strong> comes to mind) depending on the language of your choice but you need to understand the concepts to use the library anyway.</p>
",1247,1333401711
How can I find if there is comparison between two objects in a sentence or not?,"<p>Good day.
First I would like to apologies for not being very specific in the title, please if anyone knows the correct term for what I need, do let me know.
Here is my question. There is a person X and a person Y and I'm doing sentiment analysis on each person. Here are the two case that are baffling me (have in mind that the cases here are simplified) . </p>

<ul>
<li>Case 1: ""X is outperforming Y""</li>
<li>Case 2: ""X's performance is outstanding while Y's performance is poor""</li>
</ul>

<p>When doing sentiment analysis in Case 2 you can just split the sentence in two and do the analysis separately for the part for X and the part for Y while this would not work for Case 1. There is a relationship between X and Y in #1 which means that in sentences in that type if X is good than Y is bad. 
So here is my question: Is there any way to recognize sentence structures like in case 2. I was thinking of POS tagging but since I'm not a native speaker (and my grammar is bad) it's a bit hard for me to see how this would work.</p>

<p>Thank you very much for your help.</p>
","python, nlp, nltk",,286,1339692426
"Given a huge set of street names, what is the most efficient way to test whether a text contains one of the street names from the set?","<p>I have an interesting problem that I need help with. I am currently working on a feature of my program and stumbled into this issues</p>

<ol>
<li><p>I have a huge list of street names in Indonesia ( > 100k rows ) stored in database,
Each street name may have more than 1 word. For example : ""Sudirman"", ""Gatot Subroto"", or ""Jalan Asia Afrika"" are all legit street names</p></li>
<li><p>have a bunch of texts ( > 1 Million rows ) in databases, that I split into sentences. Now, the features  ( function to be exact ) that I need to do , is to test whether there are street names inside the sentences or no, so just a  true / false test</p>

<p>I have tried to solve it by doing these steps:</p></li>
</ol>

<p>a. Putting the street names into a Key,Value Hash</p>

<p>b. Split each sentences into words</p>

<p>c. Test whether words are in the hash</p>

<p>This is fast, but will not work with multiple words</p>

<p>Another alternatives that I thought of is to do these steps:</p>

<p>a. Split each sentences into words</p>

<p>b. Query the database with LIKE statement ( i,e. SELECT #### FROM street_table WHERE name like '%word%' )</p>

<p>c. If query returned a row, it means that the sentence contains street names</p>

<p>Now, this solution is going to be a very IO intensive. </p>

<p>So my question is ""What is the most efficient way to do this test"" ? regardless of the programming language. I do this in python mainly, but any language will do as long as I can grasp the concepts</p>

<p>============EDIT 1 =================</p>

<p>Will this be periodical ?</p>

<p>Yes, I will call this feature / function with an interval of 1 minute. Each call will take 100 row of texts at least and test them against the street name database</p>
","algorithm, nlp","<p>A simple solution would be to create a dictionary/multimap with first-word-of-street-name=>full-street-name(s). When you iterate each word in your sentence you'll look up potential street names, and check if you have a match (by looking at the next words). </p>

<p>This algorithm should be fairly easy to implement and should perform pretty good too.</p>
",437,1339415142
Choose or generate canonical variant from multiple sentences,"<p>I'm working with an API that maps my GTIN/EAN queries to product data.</p>

<p>Since the data returned originates from merchant product feeds, the following is almost universally the case:</p>

<ul>
<li>Multiple results per GTIN</li>
<li>Products' titles are pretty much unstructured</li>
<li>Products' titles are ""polluted"" with
<ul>
<li>SEO-related stuff,</li>
<li>information about the quantity contained,</li>
<li>""buy two, get one free"" offers,</li>
<li>etc.</li>
</ul></li>
</ul>

<p><strong>I'm looking for a programmatic way to either</strong></p>

<ul>
<li><em><strong>choose</em> the ""cleanest""/most canonical version available</strong></li>
<li><strong>or <em>generate</em> a new one that represents the ""lowest common denominator"".</strong></li>
</ul>

<p>Consider the following example results for a single EAN query:</p>

<ul>
<li>Nivea Deo Roll-On Dry Impact for Men</li>
<li>NIVEA DEO Roll on Dry/blau</li>
<li>Nivea Deo Roll-On Dry Impact for Men, 50 ml, 3er Pack (3 x 50 ml)</li>
<li>Nivea Deo Roll on Dry/blau 50 ml</li>
<li>Nivea Deoroller 50ml dry for Men blau   Mindestabnahme: 6 Stück (1 VE)</li>
<li>NIVEA Deoroller, Dry Impact for Men</li>
<li>NIVEA DEO Roll on Dry/blau_50 ml</li>
</ul>

<p><strong>My homebrew approach looks like this:</strong></p>

<ul>
<li>Basic cleanup:
<ul>
<li>Lowercase the titles,</li>
<li>strip excessive whitespace,</li>
<li>throw out apparent stopwords such as ""buy"" and ""click""</li>
</ul></li>
<li>Build an array for <code>word =&gt; global occurence</code>
<ul>
<li><code>""Nivea"" =&gt; 7</code></li>
<li><code>""Deo"" =&gt; 5</code></li>
<li><code>""Deoroller"" =&gt; 2</code></li>
<li><code>…</code></li>
<li><code>""VE"" =&gt; 1</code></li>
</ul></li>
<li>Calculate the ""cumulative word value"" for each of the titles
<ul>
<li><code>""Nivea Deo"" =&gt; 12</code></li>
<li><code>""Nivea Deoroller VE"" =&gt; 10</code></li>
</ul></li>
<li>Divide the cumulative value by the length of the title, resulting in a score
<ul>
<li><code>""Nivea Deo"" =&gt; 6</code></li>
<li><code>""Nivea Deoroller VE"" =&gt; 3.34</code></li>
</ul></li>
</ul>

<p>Obviously, my approach is pretty basic, error-prone and biased towards short sentences with frequently used words – yielding more or less satisfactory results.</p>

<ul>
<li><strong>Would you choose a different approach?</strong></li>
<li><strong>Is there some NLP magic way to take care of the problem that I don't know of?</strong></li>
</ul>
","php, text-mining, information-extraction, nlp","<p>Since your existing metric seems to bias towards shorter phrases, you should consider factoring in bigrams into the mix. So instead of considering scores for just individual words, additionally consider the score for consecutive pairs of words as well (e.g. 'nivea deo', deo roll-on', 'roll-on dry', etc). When computing the score for each title, factor in the scores for every unigram and bigram you can generate out of the title together, but maybe give the bigrams more weight, and this should encourage your algorithm to prefer longer phrases.</p>

<p>If you have large existing corpus of lots of names like these at your disposal, consider using something like <a href=""http://en.wikipedia.org/wiki/Tf%2aidf"">TF-IDF</a><br>
What you are doing right can be likened to just using TF. Using your global corpus, you can compute the idf of each unigram and bigram, which is basically a measure of unique or rare a word or phrase is across the entire corpus.<br>
tf = the number of times you have seen an ngram within these results<br>
idf = a global measure of how unique an ngram might be across all results (or atleast a very large number of them)<br>
So when computing the score for a title, instead of simply adding up the tf's of each ngram in it, you add up the tf*idf of each ngram instead. Rarer ngrams (which possibly do a better job at distinguishing this item from all other items) have a higher idf, so your algorithm should give higher weight to them. A lot of junk terms (like Mindestabnahme) would have really high idf, but they would have a really small tf, so they might not make a big difference. Alternatively prune off tokens you see fewer than k times, to get rid of noise.  </p>

<p>Another NLP trick to know about is <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"">Levenshtein distance</a> .. which is a way to quantify how similar two strings are. You can compute the levenshtein distance between every pair of strings within your results, and then try preferring the result which has the lowest average distance from all the other strings. This might not work well by itself... but factoring this score in with your existing approach might help you navigate some tricky cases.  </p>
",867,1338582302
Search string for numbers,"<p>I have a javascript chat bot where a person can type into an input box any question they like and hope to get an accurate answer. I can do this but I know I'm going about this all wrong because I don't know what position the number will appear in the sentence. If a person types in exactly:</p>

<p>what's the square root of 5 this works fine. </p>

<p>If he types in things like this it doesn't. </p>

<p>what is the square root of the 5</p>

<p>the square root of 5 is what</p>

<p>do you know what the square root of 5 is</p>

<p>etc</p>

<p>I need to be able to determine where the number appears in the sentence then do the calculation from there. Note the line below is part of a bigger working chatbot. In the line below I'm just trying to be able to answer any square root question regardless of where the number appears in the sentence. I also know there are many pitfalls with an open ended input box where a person can type anything such as spelling errors etc. This is just for entertainment not a serious scientific project. :)   </p>

<pre><code>if(
    (word[0]==""what's"") &amp;&amp;
    (word[1]==""the"") &amp;&amp;
    (word[2]==""square"") &amp;&amp;
    (word[3]==""root"") &amp;&amp;
    (word [4]==""of"") &amp;&amp;
    (input.search(/\d{1,10}/)!=-1) &amp;&amp;
    (num_of_words==6)
){        
    var root= word[5];
    if(root&lt;0){ 
        document.result.result.value = ""The square root of a negative number is not possible."";
    }else{
         word[5] = Math.sqrt(root);
         word[5] = Math.round(word[5]*100)/100 
         document.result.result.value = ""The square root of ""+ root +"" is ""+ word[5] +"".""; 
    }
    return true;
}
</code></pre>

<p>Just to be clear the bot is written using ""If statemments"" for a reason. If the input in this case doesn't include the words ""what"" and ""square root"" and ""some number"" the line doesn't trigger and is answered further down by the bot with a generic ""I don't know type of response"". So I'm hoping any answer will fit the format I am using. Be kind, I'm new here. I like making bots but I'm not much of a programmer. Thanks.</p>
","javascript, math, nlp, string-parsing, square-root","<p>You can do this using a regular expression.</p>

<pre><code>""What is the sqrt of 5?"".match(/\d+/g);
[""5""]
</code></pre>

<p>The output is an array containing all of the numbers found in the string. If you have more than one, like ""Is the square root of 100 10?"" then it will be</p>

<pre><code>""Is the square root of 100 10?"".match(/\d+/g);
[""100"", ""10""]
</code></pre>

<p>so you can pick what number you want to use in your script.</p>
",763,1339112717
"Translation of Single Words, Taking into Account Context, using Computer Language Processing Tools","<p>I would like to automatically annotate texts for learners of foreign languages with translations of difficult words.</p>

<p>For instance, if the original text is:</p>

<blockquote>
  <p>El gato esta en la casa de mis vecinos</p>
</blockquote>

<p>Becomes</p>

<blockquote>
  <p>El gato esta en la casa de mis <strong>vecinos</strong> (<em>neighbours</em>)</p>
</blockquote>

<p>The first step is to identify which words are the difficult ones. This could be done by lemmatization of the words in the original text and comparing them with a list of 'easy words' (a basic vocabulary of 1500-2000 words). Those not found in this list will be designated as 'hard words.' This process seems straightforward enough using the Natural Language Tool Kit (NLTK) for Python.</p>

<p>There is some difficulty in words that must be translated as a pair, such as 'newly weds,' or phrasal verbs 'he <strong>called</strong> me <strong>up</strong>' or the German 'er <strong>ruft</strong> mich <strong>an</strong>' (anrufen). Here words can't be treated individually. For phrasal verbs and the like perhaps some understanding of grammer is needed.</p>

<p>The second step involves obtaining a correct translation of the difficult words according to context in which they appear. As I understand, this is effectively applying the first half of a statistical machine translation system like google translate. I believe this problem could solved using the Google Translate Research API, that lets you send text to be translated, and the response includes information about which word in the translation corresponds to which word in the original text. So you could feed in the whole sentence and then fish out the word you wanted from the response. You have to apply to use this API however, and they have usage limits, which would likely be a problem for my application. I would rather find another solution. I expect no solution will give 100% correct translations and they will have to be checked by hand, but this should still speed things up.</p>

<p>Thanks for your comments.</p>

<p>David</p>
","python, text, nltk, google-translate, nlp",,947,1301156047
NLP libraries for simple POS tagging,"<p>I'm a student who's working on a summer project in NLP.  I'm fairly new to the field, so I apologize if there's a really obvious solution.  The project is in C, both due to my familiarity with it, and the computationally intensive nature of the project (my corpus is a plaintext dump of wikipedia).  </p>

<p>I'm working on an approach to relationship extraction, exploiting the consistency principle to try to learn (to within some error threshold) a set of rules dictating which clusters of grammar objects imply a connection between those objects.  </p>

<p>One of the first steps in the algorithm involves finding the set of all possible grammar objects a given word can refer to (POS disambiguation is done implicitly by the algorithm at a later step).  I've looked at several parsers, but they all seem to do the disambiguation step themselves, which (from my end) is counterproductive.  I'm looking for something off the shelf that (ideally) gives me a one-command way to turn up this information.  </p>

<p>Does such a thing exist?  If not, is there an existent dictionary containing this information that's trivially machine parseable?    </p>

<p>Thank you for your help.</p>
","c, nlp, artificial-intelligence, tagging",,508,1339048598
Java: How to Integrate Other Software,"<p>I've always been impressed by the StackOverflow hive mind, and was hoping you could point me in the right direction here.</p>

<p>I've taken some courses in Java programming, and understand how to write a fairly complex Java program. However, I've never learned how to integrate others' software into my own programs.</p>

<p>For a new project, I'd like to integrate a <a href=""http://cogcomp.cs.illinois.edu/page/download_view/POS"" rel=""nofollow"">part-pf-speech tagger</a> and <a href=""http://cogcomp.cs.illinois.edu/page/download_view/Chunker"" rel=""nofollow"">chunker</a> into my code, but have no idea how to ""load"" these programs (if load is the correct term).</p>

<p>I'm certainly not looking for step-by-step instructions, but rather a guide for how to go about this sort of issue. If anyone could get me started in the right direction, I would greatly appreciate it.</p>

<p>Thanks,
Adam</p>
","java, nlp, integration","<p>It looks like the externals you want to use are themselves in Java. This means you're in luck - you can use pure java language features to make it work.</p>

<p>There are two things to it:</p>

<p>1) your source files that interact directly with the external libraries have to be imported, or otherwise you'll have to refer to them using the fully qualified classname. 
Importing is done with the <code>import</code> statement. These statements should appear right before your class declaration, like so:</p>

<pre><code>import foo.*;       //import all classes from the package foo
import foo.bar.Baz; //import only the Baz class from the package foo.bar

public class MyClass {
    Baz myBaz = null;               //declare a member of type Baz class from package foo.bar
    foo.bar.BazBaz myBazBaz = null; //by using a fully qualified classname, I didn't need to write an import statement for foo.bar.BazBaz  
}
</code></pre>

<p>2) when you compile your sources, the java compiler needs to know where to look for classes you referenced in your source. This is done via the classpath. </p>

<p>The classpath can be a list of just .class files (compiled java classes), but also .jar files (java archives) and .zip files. Typically a project will package all classes it needs in one or more .jar files. </p>

<p>The location of these classes have no bearing on the way you interact with them in java code. It's the compiler's job to read these jars and class files and locate the classes you referred to in your code. If the compiler can't locate the classes you're referring to, you will get a compile time error and you can't compile your program.</p>

<p>You can specify the classpath as an argument to the java compiler command line (http://docs.oracle.com/javase/6/docs/technotes/tools/windows/javac.html#options). However, this becomes unwieldy very rapidly. </p>

<p>Instead, you should use a build tool like <code>ant</code> to do this work for you. The best way to get started is to read this page: <a href=""http://ant.apache.org/manual/index.html"" rel=""nofollow"">http://ant.apache.org/manual/index.html</a>. </p>

<p>From there, go to ""Using apache ant"" and then to ""Writing a simple build file"" in its entirety, they explain how to set up the classpath very well there.</p>
",2534,1339447123
How can I remove duplicate letters in strings?,"<p>How can I 'normalise'</p>

<pre><code>word = 'yeeeessssssss'
</code></pre>

<p>to</p>

<pre><code>word = 'yes'
</code></pre>
","python, nlp","<p>IF by normalizing, you mean remove repeated characters, this should work:</p>

<pre><code>re.sub(r'(\w)\1+', r'\1', 'yeeeesssss')  // yes
</code></pre>
",3875,1339425432
Resources on generating equivalent phrases (same language translation)?,"<p>I'm interested in building a program that takes some text (an article, for example) and then generates a new text with equivalent meaning, but I'm not sure how to get started on such a problem. </p>

<p>Can anyone recommend some code/books/papers/techniques that would help me tackle this?</p>
","language-agnostic, nlp",,175,1320183715
How to do random embedded bracketing of elements,"<p>I'm writing a learning algorithm for automatic constituent bracketing. Since the algorithm starts from scratch, the bracketing (embedded) should be random at first. It is then improved through iterations. I'm stuck with how to do random bracketing. Can you please suggest a code in R or Python or give some programming idea (pseudo code)? I also need ideas on how to check a random bracketing against a proper one for correctness.</p>

<p>This is what I'm trying to finally arrive at, through the learning process, starting from random bracketing.</p>

<p>This is a sentence. 
'He' 'chased' 'the' 'dog.'</p>

<p>Replacing each element with grammatical elements,
N, V, D, N.</p>

<p>Bracketing (first phase) (D, N are constituents):
(N) (V) (D N)</p>

<p>Bracketing (second phase):
(N) ((V) (D N))</p>

<p>Bracketing (third phase):
((N) ((V) (D N)))</p>

<p>Please help. Thank you.</p>
","parsing, nlp, machine-learning","<p>Here's all I can say with the information provided:</p>

<p>A naive way for the bracketing would be to <a href=""https://stackoverflow.com/questions/1160160/generating-all-possible-trees-of-depth-n"">generate some trees</a> (generating <em>all</em> can quickly get very space consuming), having as many leaves as there are words (or components), then selecting a suitable one (at random or according to proper partitioning) and apply it as bracketing pattern. For more efficiency, look for a true random tree generation algorithm (I couldn't find one at the moment).</p>

<p>Additionally, I'd recommend reading about <a href=""http://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic algos</a>/evolutionary programming, especially <a href=""http://en.wikipedia.org/wiki/Fitness_function"" rel=""nofollow noreferrer"">fitness fucnctions</a> (which are the ""check random results for correctness"" part). As far as I understood you, you want the program to detect ways of parsing and then keep them in memory as ""learned"". That quite matches a genetic algorithm with memorization of ""fittest"" patterns (and only mutation as changing factor).</p>

<p>An awesome, very elaborate (if working), but probably extremely difficult approach would be to use <a href=""http://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow noreferrer"">genetic programming</a>. But that's probably too different from what you want.</p>

<p>And last, the easiest way to check correctness of bracketing imo would be to keep a table with the grammar/syntax rules and compare with them. You also could improve this to a better fitness function by keeping them in a tree and measuring the distance from the actual pattern (<code>(V D) N</code>) to the correct pattern (<code>V (D N)</code>). (which is just some random idea, I've never actually done this..)</p>
",97,1338964716
Run GATE pipeline from inside a Java program without the GUI. build a tomcat app with gate,"<p>i have built some plugin components to GATE and in combination with ANNIE tools, im running a pipeline in GATE platform. </p>

<p>Does anyone know how can i run a pipeline from the console?  I want to build a web application in Tomcat that will be taking a plain text from the web page, passing it to the GATE pipeline i have built and do something. So i need to run GATE in a simple Java file, how can it be done?</p>

<p>Thanks in advance and sorry for my poor grammar</p>
","java, tomcat, nlp, gate","<p>The <a href=""http://gate.ac.uk/wiki/code-repository/"" rel=""nofollow noreferrer"">GATE example code</a> shows you how to run GATE in a number of different ways, in particular the <a href=""http://gate.ac.uk/wiki/code-repository/javadoc/src-html/andrewgolightly/nlp/gate/TotalGoldfishCount.html"" rel=""nofollow noreferrer"">Goldfish example</a> illustrates how to run GATE from the command line.</p>
",9919,1264937193
Japanese language detection using java langdetect library,"<p>I have a problem with language detection for Japanese language using <a href=""http://code.google.com/p/language-detection/"" rel=""nofollow"">java library</a>: </p>

<p>Using Japanese text, I'm trying to detect it's text language, but instead of expected ""ja"" I got ""en"". Has anybody seen this problem before?</p>

<p>What is the expected output? </p>

<pre><code>[ja:0.9999952022259697]
</code></pre>

<p>What do you see instead? </p>

<pre><code>[en:0.9999952022259697]
</code></pre>

<p>Original issue description with Japanese text in attachments you can find <a href=""http://code.google.com/p/language-detection/issues/detail?id=36"" rel=""nofollow"">here</a></p>
","java, encoding, nlp","<p>This is almost certainly a problem related to the encoding of the input file (if that file contains Japanese at all -- I am not convinced it does).</p>

<p>The Java library you linked to assumes -- according to the documentation -- that the input is given as a <code>String</code> object. This means it <strong>assumes the encoding has already been correctly guessed</strong> and the input byte sequence been converted to a <strong>Java string.</strong></p>

<p>When you use the library, you must make sure that is the case, i.e. if you are dealing with texts in unknown encodings (such as Japanese EUC-JP or SJIS), you must detect the encoding first and convert the string properly.</p>

<p>(Because of these reasons, good language detectors are able to detect the encoding <em>and</em> the language at the same time, by using language-and-encoding specific internal dictionaries.)</p>
",872,1334910323
Maximum Entropy classifier for big data sets,"<p>I have been looking for a maximum entropy classification implementation which can deal with an output size of 500 classes and 1000 features. My training data has around 30,000,000 lines.
I have tried using MegaM, the 64-bit R maxent package, the maxent tool from the University of Edinburgh but as expected, none of them can handle the size of data. However, the size of the data set doesn't seem too out of the world for nlp tasks of this nature. 
Are there any techniques that I should be employing? Or any suggestion for a toolkit which I may use?
I am trying to run this on a 64-bit Windows machine with 8GB of RAM,using Cygwin where required.</p>
","nlp, machine-learning, classification","<p><a href=""http://hunch.net/~vw/"" rel=""noreferrer"">Vowpal Wabbit</a> is currently regarded as the fastest large-scale learner. <a href=""http://www.csie.ntu.edu.tw/~cjlin/liblinear/"" rel=""noreferrer"">LibLinear</a> is an alternative, but I'm not sure if it can handle matrices of 3e10 elements.</p>

<p>Note that the term ""MaxEnt"" is used almost exclusively by NLP people; machine learning folks call it logistic regression or logit, so if you search for that you might find many more tools than when you search for MaxEnt.</p>
",1649,1339077773
Chunking English words into graphemes corresponding to distinct sounds,"<p>How to convert english input word into combinations of graphemes? Is there a library or function that does the job? </p>

<p>What I'm looking for is an algorithm/implementation that splits orthographic words into segments which map to phonemes. That is, the sequence of letters in a word should be broken in between distinct sounds.</p>

<p>To my mind, this would look something like the following:</p>

<pre><code>physically --&gt; ph-y-s-i-c-a-ll-y
psychology --&gt; ps-y-ch-o-l-o-g-y
thrush --&gt;     th-r-u-sh
bought --&gt; b-ough-t
chew --&gt; ch-ew
palm --&gt; p-al-m
</code></pre>
","text, nlp, character","<p>Googling for <code>split english words into graphemes</code>, the first result appears to be a paper about mapping English orthography onto a phonemic representation using a Machine Learning approach. <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.7616&amp;rep=rep1&amp;type=%E2%80%8C%E2%80%8Bpdf"" rel=""nofollow"">This paper</a> appears to be doing the kind of thing you're looking for.</p>
",1660,1338420663
semantic search engine and navigator,"<p>Hi I did a short course in AI and we designed a chatbot based on AIML and python. I have a new task to design some form of Semantic search engine. I want people to be able to navigate data or search for questions and I give them results. Initially it will be for specific topic e.g. transporation and geography. Some sample input from a user:</p>

<ol>
<li><p>How much will it cost for me to get from x to y? </p>

<p>Ans: It will cost you 26$</p></li>
<li><p>How far is x from z?</p>

<p>Ans: It is 25 Miles</p></li>
<li><p>A user can add facourite routes so they can simply type in, Add favourite roAnd the user will then be asked to enter the f routes. </p>

<p>Ans: Are you asking to add an entry to your favourite routes?</p>

<p>User:yes.</p>

<p>Ans: Please enter a favourite route.</p></li>
<li><p>Show my common routes.</p>

<p>Ans: Your common routes are x,y and z.</p></li>
</ol>

<p>So the data being searched may be specific to a user hence may have to use a database. Some data is external maybe envoke google maps to enquire on the distances. Some questions may simply require a response from a chatbot.</p>

<p>So what should i do upon user input? 
Tokenize it, stem it, parse it? </p>

<p>I was hoping to use AIML somewhere but an article i read <a href=""http://knytetrypper.proboards.com/index.cgi?board=gbot&amp;action=print&amp;thread=285"" rel=""nofollow"">http://knytetrypper.proboards.com/index.cgi?board=gbot&amp;action=print&amp;thread=285</a> . Says AIML is only good for pattern matching. Someone please point me in the correct direction. I downloaded NLTK, it seems usefull but i don't know if it on its own can do what I require. </p>

<p>Any similar projects articles?</p>
","text, nlp, semantics, nltk, aiml","<p>This is a really hard problem. If you restrict the inputs to a very small space, it can be doable though. At that point though you are just using a vocabulary and have basic commands for each possible query.</p>

<p>There are several ways to discriminate between types of queries:
1) parse and try to use all that info
2) partial parse/pos tag- find verbs
3) machine learning/classification approach, using pos as feature, distances, words/constructions like 'to'/'from'</p>

<p>... and then you can try to pull out the query params once you've classified the query correctly.</p>

<p>I would avoid doing a parse until you are very sure what kind of query it is- a classification approach is the best first step, and for messing with that NLTK is very useful.</p>
",367,1338456429
"How to calculate probabilities from confusion matrices? need denominator, chars matrices","<p><a href=""http://acl.ldc.upenn.edu/C/C90/C90-2036.pdf"" rel=""noreferrer"">This paper</a> contains confusion matrices for spelling errors in a noisy channel. It describes how to correct the errors based on conditional properties.</p>

<p>The conditional probability computation is on page 2, left column. In footnote 4, page 2, left column, the authors say: ""The chars matrices  can  be   easily  replicated, and are therefore omitted from the appendix."" I cannot figure out how can they be replicated!</p>

<p><strong>How to replicate them? Do I need the original corpus? or, did the authors mean they could be recomputed from the material in the paper itself?</strong></p>
","nlp, machine-learning, stanford-nlp, opennlp, confusion-matrix","<p>Looking at the paper, you just need to calculate them using a corpus, either the same one or one relevant to your application.</p>

<p>In replicating the matrices, note that they implicitly define two different <code>chars</code> matrices: a vector and an n-by-n matrix. For each character <code>x</code>, the vector <code>chars</code> contains a count of the number of times the character <code>x</code> occurred in the corpus. For each character sequence <code>xy</code>, the matrix <code>chars</code> contains a count of the number of times that sequence occurred in the corpus.</p>

<p><code>chars[x]</code> represents a look-up of <code>x</code> in the vector; <code>chars[x,y]</code> represents a look-up of the sequence <code>xy</code> in the matrix. Note that <code>chars[x]</code> = the sum over <code>chars[x,y]</code> for each value of <code>y</code>.</p>

<p>Note that their counts are all based on the 1988 AP Newswire corpus (<a href=""http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC93T3A"" rel=""nofollow"">available from the LDC</a>). If you can't use their exact corpus, I don't think it would be unreasonable to use another text from the same genre (i.e. another newswire corpus) and scale your counts such that they fit the original data. That is, the frequency of a given character shouldn't vary too much from one text to another if they're similar enough, so if you've got a corpus of 22 million words of newswire, you could count characters in that text and then double them to approximate their original counts.</p>
",2014,1337715196
Wordnet edit tree structure,"<p>I'm developing an application that uses the Wordnet conceptual hierarchy for its operation. I found that some words I need are missing in the database. Is there an API or tool, or any other way I can insert new words, edit the structure etc.? (I'm using Wordnet 3.0.)</p>

<p>Thanks.</p>
","nlp, wordnet",,955,1334826062
How to programmatically retrieve web pages with a query in java,"<p>what I need is to submit a query to the search engine and to retrieve the first 100 web pages to deal with their contents later on. Submitting the query to the search engine and dealing with the web pages resulted from the seach engine should be done within the programme. should I use search-engine api for example google-api or bing-api or there is another way? </p>
","java, nlp, search-engine, bing-api","<p>To summarize what has been said in comments: yes, search engine API is the best way.</p>

<p>Retrieving results as a web page and then parsing it is the worst approach imaginable(if you don't have any certain needs that make using API unbearable).</p>
",346,1337770677
How to understand this formula in Lingpipe language model?,"<p><a href=""http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/NGramProcessLM.html"" rel=""nofollow noreferrer"">This is from manual of Lingpipe doc</a> in building a language model. But I only partly understand the theory behind it. </p>

<p>I especially do not know the base probability.</p>

<p><img src=""https://i.sstatic.net/e9Ud0.gif"" alt=""enter image description here""></p>

<p><img src=""https://i.sstatic.net/WMNyN.gif"" alt=""enter image description here""></p>

<p>Here, how to get base p(d). If below is portion of token and their freq in unigram file.</p>

<pre><code>ab  20
aba 3
abd 2
abef 2
abkk 3
</code></pre>

<p>Under such condition, what is lamda(),1-lamda(), extcount, numExtentions and Base P(ab)?
This is one question but they are chained. </p>

<p>Thanks a lot.</p>
","java, nlp",,357,1338289462
Does Lucene / Solr support hypernyms and hyponyms?,"<p>For example, houses are buildings, therefore when searching for 'buildings' Lucene would return matches for 'house' as well.  This is not the same as synonyms, searching for 'house' shouldn't match 'building'.</p>
","solr, lucene, nlp","<p>You can simply construct a dictionary/hash-table of hypernyms and write a Query Expansion Module having support for hypernyms. To put it simply (1) when the user types in say ""Building"" in the search Box (2) send your query to your hash table (3) retrieve hypernyms for Building (4) Expand your query something like q=Building+House+Apartment+Villa. </p>
",677,1338202085
Wiktionary API to retrieve word forms (or other free service),"<p>This is a question particularly for Russian/Ukrainian languages but may be useful for other languages too. </p>

<p>Is there a possibility to retrieve word forms as raw data? To use in mobile application for example. These forms are present on the general wiki page. For example <a href=""http://ru.wiktionary.org/wiki/be#.D0.9C.D0.BE.D1.80.D1.84.D0.BE.D0.BB.D0.BE.D0.B3.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D0.B8_.D1.81.D0.B8.D0.BD.D1.82.D0.B0.D0.BA.D1.81.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D1.81.D0.B2.D0.BE.D0.B9.D1.81.D1.82.D0.B2.D0.B0_2"" rel=""nofollow"">Forms of verb 'to be'</a>. The same you can find for nouns <a href=""http://ru.wiktionary.org/wiki/%D1%8F%D0%B1%D0%BB%D0%BE%D0%BA%D0%BE#.D0.9C.D0.BE.D1.80.D1.84.D0.BE.D0.BB.D0.BE.D0.B3.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D0.B8_.D1.81.D0.B8.D0.BD.D1.82.D0.B0.D0.BA.D1.81.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D1.81.D0.B2.D0.BE.D0.B9.D1.81.D1.82.D0.B2.D0.B0"" rel=""nofollow"">Noun forms for 'apple' in Russian</a>. </p>

<p>I need these forms with description of the form. What I mean is for example:
to be - infinitive; am - first person singular, present time; are - first person plural, present time; etc.</p>

<p>So far I have found that only wiktionary.org provides such information for Russian language. It would be nice if someone could point me to some other services/dictionaries for Russian, Ukrainian and English. </p>
","nlp, dictionary, wiktionary",,2388,1337967335
Does anybody know an Implementation of yarowsky&#39;s algorithm?,"<p>I want to find collocation in huge text using yarowsky's algorithm.
I have read about this algorithm in these links:</p>

<p><a href=""http://en.wikipedia.org/wiki/Yarowsky_algorithm"" rel=""noreferrer"">wikipedia and Yarowsky</a></p>

<p><a href=""http://books.google.com/books?id=cwNxbeSlIXoC&amp;pg=PA361&amp;dq=yarowsky%20%2b%20collocation&amp;hl=en&amp;ei=kK8VTt7lEort0gGYlNVf&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=10&amp;ved=0CFkQ6AEwCQ#v=onepage&amp;q=yarowsky%20%2b%20collocation&amp;f=false"" rel=""noreferrer"">google book and yarowsky</a>
I wanted to know if there is an implementation of the yarowsky's algorithm`? 
please help me find some code for this algorithm.
thanks</p>
","python, nlp, nltk, word-sense-disambiguation",,1583,1310483682
How to evaluate and explain the trained model in this machine learning?,"<p>I am new in machine learning. I did a test but do not know how to explain and evaluate.</p>

<p>Case 1:</p>

<p>I first divide randomly the data (data A, about 8000 words) into 10 groups (a1..a10). Within each group, I use 90% of data to build ngram model. This ngram model is then tested on the other 10% data of the same group. The result is below 10% accuracy. Other 9 groups are done same way (respectively build model and respectively tested on the remained 10% data of that group). All results are about 10% accuracy. (Is this 10 fold cross-validation?)</p>

<p>Case 2:</p>

<p>I first build a ngram model based on <strong>entire</strong> data set (data A) of about 8000 words. Then I divide this A into 10 groups(a1,a2,a3..a10), randomly of course. I then use this ngram to test respectively a1,a2..a10. I found the model is almost 96% accuracy on all groups.</p>

<p>How to explain such situations.
Thanks in advance.</p>
","statistics, nlp, machine-learning, artificial-intelligence, data-mining","<ol>
<li><p>Yes, 10-fold cross validation.</p></li>
<li><p>This testing method has the common flaw of testing on the training set. That is why the accuracy is inflated. It is unrealistic because, in real life, your test instances are novel and previously unseen by the system.</p></li>
</ol>

<p>N-fold cross validation is a valid evaluation method used in many works.</p>
",165,1337795422
Is there an existing library or api I can use to separate words in character based languages?,"<p>I'm working on a little hobby Python project that involves creating dictionaries for various languages using large bodies of text written in that language. For most languages this is relatively straightforward because I can use the space delimiter between words to tokenize a paragraph into words for the dictionary, but for example, Chinese does not use a space character between words. How can I tokenize a paragraph of Chinese text into words?</p>

<p>My searching has found that this is a somewhat complex problem, so I'm wondering if there are off the shelf solutions to solve this in Python or elsewhere via an api or any other language. This must be a common problem because any search engine made for asian languages would need to overcome this issue in order to provide relevant results.</p>

<p>I tried to search around using Google, but I'm not even sure what this type of tokenizing is called, so my results aren't finding anything. Maybe just a nudge in the right direction would help.</p>
","python, unicode, utf-8, nlp",,407,1337463939
Generate parse tree from parse description,"<p>I want to generate a parse tree(Java Object) from a parse description(Condensed Form of a syntactic parse) of an English sentence. I am using Java for the same and need to define an efficient tree too. Eg. of description :</p>

<pre><code>    (ROOT (S (NP (PRP I)) (VP (MD would) (VP (VB love) (S (VP (TO to) (VP (VB go) (PRT (RP out)) (PP (IN with) (NP (PRP you)))))))) (. .))
</code></pre>
","parsing, nlp, text-processing","<p>I finally worked it out myself :)</p>

<pre class=""lang-java prettyprint-override""><code>public static Node getParseTree(String[] parseTokens, ArrayList&lt;Node&gt; leafNodeList)
{
    Node top = new Node(""TOP"");
    Node rest = getParseTree(parseTokens, 2, top, false, leafNodeList);
    return top;
}

public static Node getParseTree(String[] parseTokens, int currIndex, Node lastNode, Boolean closeBrace, ArrayList&lt;Node&gt; leafNodeList)
{
    if(currIndex&gt;=parseTokens.length) return lastNode;
    else if(""("".equals(parseTokens[currIndex]))
    {
        Node newNode = lastNode.addChild(parseTokens[currIndex+1]);//The next token is the data for the new node constructed
        return getParseTree(parseTokens, currIndex+2, newNode, false, leafNodeList);
    }
    else if("")"".equals(parseTokens[currIndex]))
    {
        if(closeBrace) return getParseTree(parseTokens, currIndex+1, lastNode.getParent(), true, leafNodeList);
        else return getParseTree(parseTokens, currIndex+1, lastNode, true, leafNodeList);
    }
    else //leaf node 
    {
        Node newNode = lastNode.addChild(parseTokens[currIndex]);
        leafNodeList.add(newNode);
        return getParseTree(parseTokens, currIndex+2, lastNode.getParent(), true, leafNodeList);
    }       
}

Node test(String parseDesc)
{
        parseDesc = parseDesc.replace(""("", "" ( "");
        parseDesc = parseDesc.replace("")"", "" ) "");
        String[] parseDescTokens = parseDesc.trim().split(""\\s+"");
        Node treeReqd = getParseTree(parseDescTokens, leafNodes);// Required Tree
}
</code></pre>
",271,1337323270
Efficient to search for a list of terms in a document,"<p>I have a list of thousands of synonyms.  I also have tens of thousands of documents that I want to search for these terms.  Using python (or pseudocode) what is an efficient way of doing this?  </p>

<pre><code># this would work for single word synonyms, but there are multiple word synonyms too
synonymSet = set([...])
wordsInDocument = set([...])
synonymsInDocument = synonymSet.intersection(wordsInDocument)

# this would work, but sounds slow
matches = []
for document in documents:
    for synonym in synonymSet:
        if synonym in document:
            matches.append(synonym)
</code></pre>

<p>Is there a good solution to this problem, or will it just take a while?
Thank you in advance</p>
","python, algorithm, search, nlp, string-matching",,123,1337809162
Dataset for common words to construct basic sentences,"<p>So I am making a ""fridge magnet"" interactive and I was trying to figure out a valid dataset for words to have for the user to drag around.</p>

<p>I am using this data set .. but it is not that great</p>

<p><a href=""http://en.wikipedia.org/wiki/Most_common_words_in_English"" rel=""nofollow"">http://en.wikipedia.org/wiki/Most_common_words_in_English</a></p>

<p>and ideas where to find a more valid set of words</p>
","dataset, nlp, linguistics","<p>One way you could do this yourself is to download a corpus of text, and then run a script that counts up the number of each word that appears.  Then pick some value <em>N</em> and divide every count by <em>N</em> (rounding down).  For each word, make a magnet for each divided count.  You should pick <em>N</em> based on how many magnets you want out at the end.</p>

<p>This has the advantage of having the distribution of magnets match the distribution of words.  For example, if ""the"" appears 1000 times, ""man"" 320 times, ""walks"" 150 times, and ""skips"" 2 times, and you pick <em>N</em> to be 100, then you will end up making 10 ""the"" magnets, 3 ""man"", 1 ""walks"", and 0 ""skips"".</p>

<p>You might also want to take the logarithm of the counts to try and reduce the skew.  Since word distributions are <a href=""http://en.wikipedia.org/wiki/Zipfian_distribution"" rel=""nofollow"">Zipfian</a>, you might end up with thousands of ""the"" magnets for each ""walks"").</p>

<p>Finally, the nice thing about this approach is that you could run it on a particular domain to make a word magnet set for that domain.  For example, if you want to make word magnets that sound like news stories, then run it on a corpus of news stories.  If you want to make word magnets that sound like fairy tales, then run it on a corpus of fairy tales.</p>

<p>If you really want to get fancy you could use something like <a href=""http://en.wikipedia.org/wiki/Tf%2aidf"" rel=""nofollow"">TF-IDF</a> to pick out the words that are most representative of that domain and then mix them with common function words.</p>
",804,1337699561
Regex replace for large number of search replace pairs,"<p>I want to be able to do large scale search and replace across documents for purposes of text normalisation.</p>

<p>For example: </p>

<ol>
<li>Find all uses of <strong>U.S.A</strong>, <strong>USA</strong> and replace with <strong>United States Of America</strong></li>
<li>Find all ampersands (&amp;) and replace with the word <strong>and</strong>    </li>
</ol>

<p>I also want to be able to add new rules to the system without having to change any code. So the search replace pairs are stored in a data store that means anyone could add,update,remove the rules. </p>

<p>I've been working with the Python re module which is quite nice and ideally I would want to pass a list of tuples to the sub command for it to then go through each one and do the replacements. Is there a better way to do this other than iterating over a list of tuples and then creating a regular expression for each - it's very slow and inefficient especially with large documents:</p>

<pre><code>replacements = [
  r('USA','United States Of America'),
  (r'U\.S\.A','United States Of America'),
  (r'US of A', 'United States of America')]

for replacement in replacements:
  document = re.sub(replacement[0],replacement[1],document
</code></pre>
","python, regex, nlp, text-processing",,756,1337696470
machine representation of natural text,"<p>I'm currently working on high-level machine representation of natural text.
For example,
""I had one dog but I gave it to Danny who didn't have any""
would be </p>

<pre><code>I.have.dog =1
I.have.dog -=1
Danny.have.dog = 0
Danny.have.dog +=1
</code></pre>

<p>something like this....</p>

<p>I'm trying to find resources, but can't really find matching topics..
Is there a valid subject name for this type of research? Any library of resources?
Natural logic sounds like something related but it's not really the same thing I'm working on. Please help me out!</p>
",nlp,"<p>Representing natural language's meaning is the domain of <strong><a href=""https://en.wikipedia.org/wiki/Computational_semantics"" rel=""nofollow"">computational semantics</a></strong>. Within that area, lots of frameworks have been developed, though the basic one is still first-order logic.</p>

<p>Specifically, your problem seems to be that of recognizing <strong>discourse semantics</strong>, which deals with information change brought about by language use. This is pretty much an open area of research, so expect to find a lot of research papers and PhD positions, but little readily-usable software.</p>
",120,1337511788
NLP library for determining if a sentence is equivalent to another sentence?,"<p>I would like to find an NLP library in Python, PHP or even JavaScript for determining whether a sentence in a string is equivalent to a differently structured sentence?</p>

<p>For example, the library would need to be able to determine whether these two sentences are equivalent:</p>

<p>""Would you like the order for here or to go?""
""Do you want the order for here or to go?""</p>

<p>Is there such a thing?  Or would it actually be easier for me to build something like this myself for the specific application I need it for?</p>
","php, javascript, python, nlp","<p>What you are describing is the task of ""paraphrase"" (or bidirectional ""textual entailment"").  This is an extremely hard problem and an open research area.  I doubt that there is a system available that would do well enough on this task for real-world, general use.</p>

<p>If you have a very narrow set of transformations in mind (such as the ""would you like"" &lt;-> ""do you want"" alternation), you could try and construct a set of transformation rules that convert one sentence to the other.  These rules could act directly on the sentence or on a parse tree produced from a statistical parser.</p>
",813,1337463021
Ontonotes Treebanked Sentences Notation,"<p>Here is an example of a sentence from <a href=""http://www.bbn.com/ontonotes/"" rel=""nofollow"">Ontonotes</a>-<a href=""http://www.ldc.upenn.edu/Catalog/docs/LDC2009T24/OntoNotes-Release-3.0.pdf"" rel=""nofollow"">V3.0</a>. I wish to know the meaning of *T*-1 and *-2 in Treebanked sentence[ which I think is a tokenized version of the plain sentence]</p>

<h2>Plain sentence:</h2>

<pre><code>In the summer of 2005, a picture that people have long been looking forward to
started emerging with frequency in various major Hong Kong media.
</code></pre>

<h2>Treebanked sentence:</h2>

<pre><code>[Speaker#1] In the summer of 2005 , a picture that people have long been looking
forward to *T*-1 started *-2 emerging with frequency in various major Hong Kong
media .
</code></pre>

<p>There are also a few other tags like *PRO* and *PRO*-1</p>
","nlp, text-processing","<p>The <code>*T*-1</code> thing represents a ""trace"" in the parse tree.  The reason you're seeing them is that OntoNotes builds on the Penn Treebank, which uses that annotation.</p>

<p>If you search the web for ""penn treebank trace"" you should find plenty to read about.  I found <a href=""http://www.ling.ohio-state.edu/~hinrichs/course07/ptb.pdf"" rel=""nofollow"">this</a> document which has short explanation and examples for various <code>*X*</code> tags in the Penn Treebank, but there are probably more complete explanations out there.</p>
",213,1337315520
What methods are used for recognizing language a text is written in?,"<p>If I have a given text (both long or short), with which methods do you usually detect which language it is written in?</p>

<p>It is clear that:</p>

<ul>
<li>You need a training corpus to train the models you use (e.g. neural networks, if used)</li>
</ul>

<p>Easiest thing coming to my mind is:</p>

<ul>
<li>Check characters used in the text (e.g. hiragana are only used in Japanese, Umlauts probably only in European languages, ç in French, Turkish, …)</li>
<li>Increase the check to two or three letter pairs to find specific combinations of a language</li>
<li>Lookup a dictionary to check which words occur in which language (probably only without stemming, as stemming depends on the language)</li>
</ul>

<p>But I guess there are better ways to go. I am not searching for existing projects (those questions have already been answered), but for methods like Hidden-Markov-Models, Neural Networks, … whatever may be used for this task.</p>
",nlp,"<p>In product I'm working on we use dictionary-based approach.
First relative probabilities for all words in training corpus are calculated and this is stored as a model.</p>

<p>Then input text is processed word by word to see if particular model gives best match (much better then the other models).</p>

<p>In some cases all models provide quite bad match.</p>

<p>Few interesting points:</p>

<ol>
<li>As we are working with social media both normalized and non-normalized matches are attempted (in this context normalization is removal of diacritics from symbols). Non-normalized matches have a higher weight</li>
<li>This method works rather bad on very short phrases (1-2 words) in particular when these words are there in few languages, which is the case of few European languages</li>
</ol>

<p>Also for a better detection we are considering added per-character model as you have described (certain languages have certain unique characters)</p>

<p>Btw, we use ICU library to split words. Works rather good for European and Eastern languages (currently we support Chinese)</p>
",271,1337254645
Fake reviews datasets,"<p>There are datasets with usual mail spam in the Internet, but I need datasets with fake reviews to conduct some research and I can't find any of them. 
Can anybody give me advices on where fake reviews datasets can be obtained?</p>
","nlp, spam, review, corpus","<p>Our dataset is available on my Cornell homepage: <a href=""http://www.cs.cornell.edu/~myleott/"" rel=""noreferrer"">http://www.cs.cornell.edu/~myleott/</a></p>
",4962,1337019883
Adding a new line character at the end of the sentence,"<p>I have a string which is a fragment of a book (its around 1 chapter) 
this string is all one line. 
I would like to make a new line at the end of each sentence</p>

<p>I solved it by a not-so-sophisticated code of </p>

<pre><code>text = text.replaceAll(""\\."",""\\.\n""); //same for ? same for !
</code></pre>

<p>and of course this does not yield very nice results.
I dont need this to be perfect but the nicer i can get it the better.</p>

<p>I would like at least to check for following before making a new line character:</p>

<pre><code>the word before the . is longer then 2 characters
there are no dots before the . in the same ""word""
the character before the . is not a number
the character after the dot (and possibly a whitespace after that dot) is not a (
</code></pre>

<p>Any other suggestions would be really appreciated, along with actual code which will make it happen.</p>

<p>Similar question: 
<a href=""https://stackoverflow.com/questions/4373612/how-to-parse-text-into-sentences-in-java"">Here</a></p>

<p><strong>Update:</strong></p>

<p>Although not high on my list of priorities because my book doesnt contain a lot of direct quotations nor direct speeches but a rule that handles sentences that are inside those would also be in order so that sentences from the same qoute dont end up on new lines  </p>
","java, nlp","<p><a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford's CoreNLP</a> toolkit has a class that does sentence segmentation.  See more <a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">here</a>.</p>

<p>If you say <code>new DocumentPreprocessor(new StringReader(s)).iterator()</code> where <code>s</code> is a string containing the text, it will give you back an iterator of sentences.</p>

<p>Note that this will tokenize the sentence as well.  If you want the sentence to look the way it started, you can either just use this output as a guide for splitting, or run the <code>PTBTokenizer -untok</code> command (see same link as above) to make each tokenized sentence look normal again.</p>

<p>This will almost certainly work better than your list of rules since your rules don't account for many of the important cases.</p>
",2658,1337269867
Problems installing srilm 1.6.0 with llvm-gcc 4.x,"<h1>The problem</h1>

<p>I am trying to build <a href=""http://www.speech.sri.com/projects/srilm/"" rel=""nofollow"">SRI's Language Modeling tool</a>, <code>srilm</code> version 1.6.0 on my mac, and coming across some fairly strange compilation problems (""strange"" in that a few hours of Google-fu did not help), so I am turning to you to see if anyone sees how I can fix this.</p>

<p>I have already <a href=""http://www.speech.sri.com/projects/srilm/docs/INSTALL"" rel=""nofollow"">checked that I have the required dependencies and followed the install instructions</a> as well as gone through <a href=""http://www-speech.sri.com/projects/srilm/manpages/srilm-faq.7.html"" rel=""nofollow"">the build troubleshooting section of the FAQ</a>.</p>

<h1>System Specifications</h1>

<p>I have a pretty vanilla install of OS X, with some packages installed through homebrew. XCode 4.3.2 (latest version) is installed. Here are the other relevant system details.</p>

<h2>OS version</h2>

<p>Mac OS X 10.7.4</p>

<h2><code>gcc -v</code> printout</h2>

<pre><code>$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-apple-darwin11.0.0/4.6.1/lto-wrapper
Target: x86_64-apple-darwin11.0.0
Configured with: ../gcc-4.6.1/configure --enable-languages=fortran,c++
Thread model: posix
gcc version 4.6.1 (GCC)
</code></pre>

<h2><code>g++ -v</code> printout</h2>

<pre><code>$ gcc -v
Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-apple-darwin11.0.0/4.6.1/lto-wrapper
Target: x86_64-apple-darwin11.0.0
Configured with: ../gcc-4.6.1/configure --enable-languages=fortran,c++
Thread model: posix
gcc version 4.6.1 (GCC) 
</code></pre>

<h2><code>uname -a</code> printout</h2>

<pre><code>$ uname -a
Darwin MacBook-Air.local 11.4.0 Darwin Kernel Version 11.4.0: Mon Apr  9 19:32:15 PDT 2012; root:xnu-1699.26.8~1/RELEASE_X86_64 x86_64
</code></pre>

<h1>The error itself</h1>

<p>The following is the end of the output produced by running <code>make World</code> from the srlim top-level directory. Everything up until this point compiles fine in any of the following circumstances:</p>

<ul>
<li>I run <code>make World</code> on its own.</li>
<li>I run <code>make World MACHINE_TYPE=macosx</code></li>
<li>I run <code>make World MACHINE_TYPE=macosx-m64</code> (specific makefile for 64bit processors)</li>
<li>I run <code>make World MACHINE_TYPE=macosx-m32</code> (specific makefile for 32bit processors)</li>
</ul>

<p>And the error that pops up is always the same (shown below).</p>

<h2><code>stderr</code> printout</h2>

<pre><code>$ make World

(...) # a bunch of stuff compiles with no errors or warnings

c++ -Wreturn-type -Wimplicit -DINSTANTIATE_TEMPLATES -DHAVE_ZOPEN  -I/usr/include -I. -I../../include -DHAVE_ZOPEN   -c -g -O2 -fno-common -o ../obj/macosx/LatticeIndex.o LatticeIndex.cc
LatticeIndex.cc:78:6: error: variable length array of non-POD element type
      'NBestWordInfo'
            makeArray(NBestWordInfo, roundedNgram, len + 1);
            ^
../../include/Array.h:93:33: note: expanded from macro 'makeArray'
# define makeArray(T, A, n)             T A[n]
                                           ^
LatticeIndex.cc:126:4: warning: data argument not used by format string
      [-Wformat-extra-args]
                        (float)ngram[0].start);
                        ^
LatticeIndex.cc:128:4: warning: data argument not used by format string
      [-Wformat-extra-args]
                        (float)(ngram[len-1].start + ngram[len-1].duration));
                        ^
2 warnings and 1 error generated.
make[2]: *** [../obj/macosx/LatticeIndex.o] Error 1
make[1]: *** [release-libraries] Error 1
make: *** [World] Error 2
</code></pre>

<p>Any idea what could be going wrong? It seems to compile fine on some other people's macs in my department, and I've checked their makefiles for differences, but nothing popped up. No one here has any idea why the build fails, but we'd really appreciate it if you can help us out. Thanks in advance for any help you can provide me with! :-)</p>
","c++, build, installation, nlp","<p>The problem is due to Apple using llvm-gcc/clang, which does not support variable length arrays. This problem can actually be addressed by modifying $SRILM/dstruct/src/Array.h, and has been noted and addressed in the upcoming release of <code>srilm</code>.</p>

<p>For the time being, on a mac, build <code>srilm</code> using g++ 4.2 instead, using the following command:</p>

<pre><code>$ make MACHINE_TYPE=macosx-m64 CXX=g++-4.2 World
</code></pre>

<p>This builds <code>srilm</code> without problem on all my macs.</p>
",917,1337107748
best way to test a character for being from some category in .net,"<p>I need an efficient way to test a given character (System.Char) for being a consonant. Then I need the same thing for vowels. More generally I need an efficient implementation of the following interface, given the target set of characters is of some 20 chars. Please advise. Thanks!</p>

<pre><code>using System;

public interface ITester
{
    Boolean IsInCategory(Char something);
}
</code></pre>

<p><strong>UPDATE</strong></p>

<p>Ok guys, I've run some tests. And here's what I got. The best way is to use precalculated map where each char is mapped to a boolean. (See IndexBased in the code below). HashSet as turned out is not the best one.
If anyone has more ideas, let me know.</p>

<pre><code>[TestClass]
public class Runner
{
    public const Int32 NumberOfRuns = 10000;
    public const String TextSample = @""The Letter It was November. Although it was not yet late, the sky was dark when I turned into Laundress Passage. Father had finished for the day, switched off the shop lights and closed the shutters; but so I would not come home to darkness he had left on the light over the stairs to the flat. Through the glass in the door it cast a foolscap rectangle of paleness onto the wet pavement, and it was while I was standing in that rectangle, about to turn my key in the door, that I first saw the letter. Another white rectangle, it was on the fifth step from the bottom, where I couldn't miss it. I closed the door and put the shop key in its usual place behind Bailey's Advanced Principles of Geometry. Poor Bailey. No one has wanted his fat gray book for thirty years. Sometimes I wonder what he makes of his role as guardian of the bookshop keys. I don't suppose it's the destiny he had in mind for the masterwork that he spent two decades writing. A letter. For me. That was something of an event. The crisp-cornered envelope, puffed up with its thickly folded contents, was addressed in a hand that must have given the postman a certain amount of trouble. Although the style of the writing was old-fashioned, with its heavily embellished capitals and curly flourishes, my first impression was that it had been written by a child. The letters seemed untrained. Their uneven strokes either faded into nothing or were heavily etched into the paper. There was no sense of flow in the letters that spelled out my name."";
    private interface ITester
    {
        Boolean IsConsonant(Char something);
    }

    // results in millisecs: 14807, 16411, 15050, 
    private class HashSetBasedTester : ITester
    {
        private HashSet&lt;Char&gt; hash;
        public HashSetBasedTester()
        {
            this.hash = new HashSet&lt;Char&gt;(""bcdfghjklmnpqrstvwxz"");
        }
        public Boolean IsConsonant(Char something)
        {
            return this.hash.Contains(Char.ToLower(something));
        }
    }

    // results in millisecs: 12270, 12495, 12853,
    private class HardcodedTester : ITester
    {
        public Boolean IsConsonant(Char something)
        {
            var lower = Char.ToLower(something);
            return lower == 'b' || lower == 'c' || lower == 'd' || lower == 'f' || lower == 'g' || lower == 'h' || lower == 'j' || lower == 'k' || lower == 'l' || lower == 'm' || lower == 'n' || lower == 'p' || lower == 'q' || lower == 'r' || lower == 's' || lower == 't' || lower == 'v' || lower == 'w' || lower == 'x' || lower == 'z';
        }
    }

    // WORST RESULTS
    // results in millisecs: 32140, 31549, 31856
    private class ListBasedTester : ITester
    {
        private List&lt;Char&gt; list;
        public ListBasedTester()
        {
            this.list = new List&lt;Char&gt; { 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z' };
        }
        public Boolean IsConsonant(Char something)
        {
            return this.list.Contains(Char.ToLower(something));
        }
    }

    // WINNER! (fastest and most consistent)
    // results in millisecs: 11335, 11349, 11386, 
    private class IndexBased : ITester
    {
        private Boolean[] map;
        private char min;
        private char max;
        public IndexBased()
        {
            var chars = ""bcdfghjklmnpqrstvwxz"".ToArray();
            this.min = chars.Min();
            this.max = chars.Max();
            var length = this.max - this.min + 1;
            this.map = new Boolean[length];
            foreach (var at in chars)
            {
                map[at - min] = true;
            }
        }
        public Boolean IsConsonant(Char something)
        {
            something = Char.ToLower(something);
            return something &lt;= this.max &amp;&amp; something &gt;= this.min &amp;&amp; this.map[something - this.min];
        }
    }


    [TestMethod]
    public void RunTest()
    {
        var tester = new IndexBased(); // new HashSetBasedTester(); // new HardcodedTester(); // new ListBasedTester(); //
        var stopwatch = Stopwatch.StartNew();
        for (var i = 0; i &lt; NumberOfRuns; i++)
        {
            foreach (var at in TextSample)
            {
                var tested = tester.IsConsonant(at);
            }
        }
        Trace.WriteLine(stopwatch.ElapsedMilliseconds.ToString());
    }
}
</code></pre>

<p><strong>UPDATE 2:</strong></p>

<p>This set of tests doesn't do casting to lower/upper and we have much better results! Anyways the favorites/losers are the same. Check it out:</p>

<pre><code>[TestClass]
public class Runner
{
    public const Int32 NumberOfRuns = 10000;
    public const String TextSample = @""The Letter It was November. Although it was not yet late, the sky was dark when I turned into Laundress Passage. Father had finished for the day, switched off the shop lights and closed the shutters; but so I would not come home to darkness he had left on the light over the stairs to the flat. Through the glass in the door it cast a foolscap rectangle of paleness onto the wet pavement, and it was while I was standing in that rectangle, about to turn my key in the door, that I first saw the letter. Another white rectangle, it was on the fifth step from the bottom, where I couldn't miss it. I closed the door and put the shop key in its usual place behind Bailey's Advanced Principles of Geometry. Poor Bailey. No one has wanted his fat gray book for thirty years. Sometimes I wonder what he makes of his role as guardian of the bookshop keys. I don't suppose it's the destiny he had in mind for the masterwork that he spent two decades writing. A letter. For me. That was something of an event. The crisp-cornered envelope, puffed up with its thickly folded contents, was addressed in a hand that must have given the postman a certain amount of trouble. Although the style of the writing was old-fashioned, with its heavily embellished capitals and curly flourishes, my first impression was that it had been written by a child. The letters seemed untrained. Their uneven strokes either faded into nothing or were heavily etched into the paper. There was no sense of flow in the letters that spelled out my name."";
    private interface ITester
    {
        Boolean IsConsonant(Char something);
    }

    // results in millisecs: 8378, 7980, 7533, 7752
    private class HashSetBasedTester : ITester
    {
        private HashSet&lt;Char&gt; hash;
        public HashSetBasedTester()
        {
            this.hash = new HashSet&lt;Char&gt;(""bcdfghjklmnpqrstvwxzBCDFGHJKLMNPQRSTVWXZ"");
        }
        public Boolean IsConsonant(Char something)
        {
            return this.hash.Contains(something);
        }
    }

    // results in millisecs: 6406, 6667, 6500, 6708
    private class HardcodedTester : ITester
    {
        public Boolean IsConsonant(Char something)
        {
            return something == 'b' || something == 'c' || something == 'd' || something == 'f' || something == 'g' || something == 'h' || something == 'j' || something == 'k' || something == 'l' || something == 'm' || something == 'n' || something == 'p' || something == 'q' || something == 'r' || something == 's' || something == 't' || something == 'v' || something == 'w' || something == 'x' || something == 'z' ||
                something == 'B' || something == 'C' || something == 'D' || something == 'F' || something == 'G' || something == 'H' || something == 'J' || something == 'K' || something == 'L' || something == 'M' || something == 'N' || something == 'P' || something == 'Q' || something == 'R' || something == 'S' || something == 'T' || something == 'V' || something == 'W' || something == 'X' || something == 'Z';
        }
    }

    // WORST RESULTS
    // results in millisecs: 36585, 37702, ...
    private class ListBasedTester : ITester
    {
        private List&lt;Char&gt; list;
        public ListBasedTester()
        {
            this.list = new List&lt;Char&gt; { 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z',
                'B', 'C', 'D', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Z' };
        }
        public Boolean IsConsonant(Char something)
        {
            return this.list.Contains(something);
        }
    }

    // WINNER!
    // results in millisecs: 4716, 4846, 4756, 4550
    private class IndexBased : ITester
    {
        private Boolean[] map;
        private char min;
        private char max;
        public IndexBased()
        {
            var chars = ""bcdfghjklmnpqrstvwxzBCDFGHJKLMNPQRSTVWXZ"".ToArray();
            this.min = chars.Min();
            this.max = chars.Max();
            var length = this.max - this.min + 1;
            this.map = new Boolean[length];
            foreach (var at in chars)
            {
                map[at - min] = true;
            }
        }
        public Boolean IsConsonant(Char something)
        {
            return something &lt;= this.max &amp;&amp; something &gt;= this.min &amp;&amp; this.map[something - this.min];
        }
    }


    [TestMethod]
    public void RunTest()
    {
        var tester = new IndexBased();//new IndexBased(); // new HashSetBasedTester(); // new HardcodedTester(); // new ListBasedTester(); //
        var stopwatch = Stopwatch.StartNew();
        for (var i = 0; i &lt; NumberOfRuns; i++)
        {
            foreach (var at in TextSample)
            {
                var tested = tester.IsConsonant(at);
            }
        }
        Trace.WriteLine(stopwatch.ElapsedMilliseconds.ToString());
    }
}
</code></pre>
","c#, .net, algorithm, text, nlp",,287,1337109522
How to get relationship between noun phrases in a sentence?,"<p>I have this pattern to get cause-effect relationship between noun phrases in a sentence:</p>

<pre><code>&lt;NP I&gt; * have * effect/impact on/in &lt;NP II&gt;
</code></pre>

<p>NP is Noun Phrase.</p>

<p>If I have a sentence:</p>

<pre><code>Technology can have negative impact on social interactions
</code></pre>

<p>then based on above pattern, <strong>NP I</strong> match with <strong>Technology</strong> and <strong>NP II</strong> match with <strong>social interactions</strong></p>

<p>The question: what is appropriate algorithm to get NP I and NP II?</p>

<p>Thanks</p>
","java, nlp","<p>Regular expression (RegEx) is extremely useful in cases like this.  The following regex matches your string format and allows for you to analyze the differing variables of the input.</p>

<pre><code>([\\w\\s]*?) (\\w*?) have (\\w*?) (effect|impact) (on|in) ([\\w\\s]*?)(\\.)
</code></pre>

<p>By running the following program, you can see how regex matcher groups work, and that group 1 is NP 1, and group 6 is NP 2.</p>

<pre><code>public class Regex {

    public static void main(String[] args) {
        Pattern p = Pattern.compile(""([\\w\\s]*?) (\\w*?) have (\\w*?) (effect|impact) (on|in) ([\\w\\s]*?)(\\.)"");
        String s = ""Greenhouse gases can have negative impact on global warming."";
        Matcher m = p.matcher(s);
        if (m.find()) {
            for (int i = 0; i &lt; m.groupCount(); i++) {
                System.out.println(""Group "" + i + "": "" + m.group(i));
            }
        }
    }
}
</code></pre>

<p>In the above example, the string <code>""Greenhouse gases can have negative impact on global warming.""</code> is analyzed.  The following is the output of the program.</p>

<pre><code>Group 0: Greenhouse gases can have negative impact on global warming.
Group 1: Greenhouse gases
Group 2: can
Group 3: negative
Group 4: impact
Group 5: on
Group 6: global warming
</code></pre>
",630,1337069186
Reaching an appropriate balance between performance and scalability in a large database,"<p>I'm trying to determine which of the many database models would best support probabilistic record comparison. Specifically, I have approximately 20 million documents defined by a variety of attributes (name, type, author, owner, etc.). Text attributes dominate the data set, yet there are still plenty of images. Read operations are the most crucial vis-a-vis performance, but I expect roughly 20,000 new documents to insert each week. Luckily, insert speed does not matter at all, and I am comfortable queuing the incoming documents for controlled processing.</p>

<p>Database queries will most typically take the following forms:</p>

<ul>
<li><code>Find documents containing at least five sentences that reference someone who'a a member of the military</code></li>
<li><code>Predict whether User A will comment on a specific document written by User B, given User A's entire comment history</code></li>
<li><code>Predict an author for Document X by comparing vocabulary, word ordering, sentence structure, and concept flow</code></li>
</ul>

<p>My first thought was to use a simple <a href=""http://en.wikipedia.org/wiki/Document-oriented_database"" rel=""nofollow"">document store</a> like, like <a href=""http://www.mongodb.org/"" rel=""nofollow"">MongoDB</a>, since each document does not necessarily contain the same data. However, complex queries effectively degrade this to a file system wrapper, since I cannot construct a query yielding the results I desire. As such, this approach corners me into walking the entire database and processing each file separately. Although document stores scale well horizontally, the benefits are not realized here.</p>

<p>This led me to realize that my granularity <em>isn't</em> at the document level, but rather the <em>entity-relationship</em> level. As such, <a href=""http://en.wikipedia.org/wiki/Graph_database"" rel=""nofollow"">graph databases</a> seemed like logical choice, since they facilitate relating each word in a sentence to the next word, next paragraph, current paragraph, part of speech, etc. Graph databases limit data replication, increase the speed of statistical clustering, and scale horizontally, among other things. Unfortunately, ensuring a definitive answer to your query still necessitates traversing the entire graph. Even still, indexing will help with performance.</p>

<p>I've also evaluated the use of relational databases, which are very efficient when designed properly (i.e., by avoiding unnecessary normalization). A relational database excels at finding all documents authored by User A, but fails at structural comparisons (which involves expensive joins). Relational databases also enforce constraints (primary keys, foreign keys, uniqueness, etc.) efficiently--a task with which some NoSQL solutions struggle.</p>

<p><strong>After considering the above-listed requirements, are there any database models that combine the ""exactness"" of relational models (<em>viz.</em>, efficient exhaustion of the domain) with the flexibility of graph databases?</strong></p>
","database, database-design, nlp, data-mining",,119,1337010944
NER naive algorithm,"<p>I never really dealt with NLP but had an idea about NER which should NOT have worked and somehow DOES exceptionally well in one case. I do not understand why it works, why doesn't it work or weather it can be extended. </p>

<p>The idea was to extract names of the main characters in a story through:</p>

<ol>
<li>Building a dictionary for each word</li>
<li>Filling for each word a list with the words that appear right next to it in the text</li>
<li>Finding for each word a word with the max correlation of lists (meaning that the words are used similarly in the text)</li>
<li>Given that one name of a character in the story, the words that are used like it, should be as well (Bogus, that is what should not work but since I never dealt with NLP until this morning I started the day naive)  </li>
</ol>

<p>I ran the overly simple code (attached below) on <a href=""http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt"" rel=""nofollow"">Alice in Wonderland</a>, which for ""Alice"" returns:</p>

<blockquote>
  <p>21 ['Mouse', 'Latitude', 'William', 'Rabbit', 'Dodo', 'Gryphon', 'Crab', 'Queen', 'Duchess', 'Footman', 'Panther', 'Caterpillar', 'Hearts', 'King', 'Bill', 'Pigeon', 'Cat', 'Hatter', 'Hare', 'Turtle', 'Dormouse']</p>
</blockquote>

<p>Though it filters for upper case words (and receives ""Alice"" as the word to cluster around), originally there are ~500 upper case words, and it's still pretty spot on as far as <a href=""http://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland#Characters"" rel=""nofollow"">main characters</a> goes.</p>

<p>It does not work that well with other characters and in other stories, though gives interesting results. </p>

<p>Any idea if this idea is usable, extendable or why does it work at all in this story for ""Alice"" ?</p>

<p>Thanks!</p>

<pre><code>#English Name recognition
import re
import sys
import random
from string import upper

def mimic_dict(filename):
  dict = {}
  f = open(filename)
  text = f.read()
  f.close()
  prev = """"
  words = text.split()
  for word in words:
    m = re.search(""\w+"",word)
    if m == None:
      continue
    word = m.group()
    if not prev in dict:
      dict[prev] = [word]
    else :
      dict[prev] = dict[prev] + [word] 
    prev = word
  return dict

def main():
  if len(sys.argv) != 2:
    print 'usage: ./main.py file-to-read'
    sys.exit(1)

  dict = mimic_dict(sys.argv[1])
  upper = []
  for e in dict.keys():
    if len(e) &gt; 1 and  e[0].isupper():
      upper.append(e)
  print len(upper),upper

  exclude = [""ME"",""Yes"",""English"",""Which"",""When"",""WOULD"",""ONE"",""THAT"",""That"",""Here"",""and"",""And"",""it"",""It"",""me""]
  exclude = [ x  for x in exclude if dict.has_key(x)] 
  for s in exclude :
    del dict[s]

  scores = {}
  for key1 in dict.keys():
    max = 0
    for key2 in dict.keys():
      if key1 == key2 : continue
      a =  dict[key1]
      k =  dict[key2]
      diff = []
      for ia in a:
        if ia in k and ia not in diff:
          diff.append( ia)
      if len(diff) &gt; max:
        max = len(diff)
        scores[key1]=(key2,max)
  dictscores = {}
  names = []
  for e in scores.keys():
    if scores[e][0]==""Alice"" and e[0].isupper():
      names.append(e)
  print len(names), names     


if __name__ == '__main__':
  main()
</code></pre>
","python, nlp","<p>From the looks of your program and previous experience with NER, I'd say this ""works"" because you're not doing a proper evaluation. You've found ""Hare"" where you should have found ""March Hare"".</p>

<p>The difficulty in NER (at least for English) is not finding the names; it's detecting their full extent (the ""March Hare"" example); detecting them even at the start of a sentence, where all words are capitalized; classifying them as person/organisation/location/etc.</p>

<p>Also, <em>Alice in Wonderland</em>, being a children's novel, is a rather easy text to process. Newswire phrases like ""Microsoft CEO Steve Ballmer"" pose a much harder problem; here, you'd want to detect</p>

<pre><code>[ORG Microsoft] CEO [PER Steve Ballmer]
</code></pre>
",423,1337007576
Clean text coming from PDFs,"<p>this is more of an algorithmic question rather than a specific language question, so I am happy to receive an answer in any language - even pseudocode, even just an idea.</p>

<p>Here is my problem: I need to work on large dataset of papers that come from articles in PDF and that were brutally copied/pasted into .txt. I only have the result of this abomination, which is around 16k papers, for 3.5 GB or text (the corpus I am using is the ACL Antology Network, <a href=""http://clair.si.umich.edu/clair/aan/DatasetContents.html"">http://clair.si.umich.edu/clair/aan/DatasetContents.html</a> ).</p>

<p>The ""junk"" comes from things like formulae, images, tables, and so on. It just pops in the middle of the running text, so I can't use regular expressions to clean it, and I can't think of any way to use machine learning for it either. I already spent a week on it, and then I decided to move on with a quick&amp;dirty fix. I don't care about cleaning it completely anymore, I don't care about false negatives and positives as long as the majority of this areas of text is removed.</p>

<p>Some examples of the text: note that formulae contain junk characters, but tables and caption don't (but they still make my sentence very long, and thus unparsable). Junk in bold.</p>

<p>Easy one:</p>

<blockquote>
  <p>The experiments were repeated while inhibiting specialization of first the scheme with the most expansions, and then the two most expanded schemata.
  Measures of coverage and speedup are important 1 As long as we are interested in preserving the f-structure assigned to sentences, this notion of coverage is stricter than necessary.
  The same f-structure can in fact be assigned by more than one parse, so that in some cases a sentence is considered out of coverage even if the specialized grammar assigns to it the correct f-structure.
  <strong>2'VPv' and 'VPverb[main]' cover VPs headed by a main verb.
  'NPadj' covers NPs with adjectives attached.
  205 The original rule: l/Pperfp --+ ADVP* SE (t ADJUNCT) ($ ADV_TYPE) = t,padv ~/r { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl $) { AD VP+ SE ('~ ADJUNCT) ($ ADV_TYPE) = vpadv is replaced by the following: ADVP,[.E (~ ADJUNCT) (.l.
  ADV_TYPE) = vpadv l/'Pperfp --+ @PPadjunct @PPcase_obl {@M.Head_Pevfp [@M..Head_Passp} @( Anaph_Ctrl ~ ) V { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl ~) Figure 1: The pruning of a rule from the actual French grammar.</strong>
  The ""*"" and the ""+"" signs have the usual interpretation as in regular expressions.
  A sub-expression enclosed in parenthesis is optional.
  Alternative sub-expressions are enclosed in curly brackets and separated by the ""["" sign.
  An ""@"" followed by an identifier is a macro expansion operator, and is eventually replaced by further functional descriptions.
  <strong>Corpus --..
  ,, 0.1[ Disambiguated Treebank treebank Human expert Grammar specialization Specialized grammar Figure 2: The setting for our experiments on grammar specialization.
  indicators of what can be achieved with this form of grammar pruning.</strong>
  However, they could potentially be misleading, since failure times for uncovered sentences might be considerably lower than their sentences times, had they not been out of coverage.</p>
</blockquote>

<p>Hard one:</p>

<blockquote>
  <p>Table 4 summarizes the precision results for both English and Romanian coreference.
  The results indicate that the English coreference is more indicate than the Romanian coreference, but SNIZZLE improves coreference resolution in both languages.
  There were 64% cases when the English coreference was resolved by a heuristic with higher priority than the corresponding heuristic for the Romanian counterpart.
  This result explains why there is better precision enhancement for 
  <strong>English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal Pronominal 73% 89% 66% 78% 76% 93% 71°/o 82% Table 4: Coreference precision Total 84% 72% 87% 76% English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal 69% 63% 66% 61% Pronominal Total 89% 78% 83% 72% 87% 77% 80% 70% Table 5: Coreference recall</strong> the English coreference. Table 5 also illustrates the recall results.
  The advantage of the data-driven coreference resolution over other methods is based on its better recall performance.
  This is explained by the fact that this method captures a larger variety of coreference patterns.
  Even though other coreference resolution systems perform better for some specific forms of systems, their recall results are surpassed by the systems approach.
  Multilingual coreference in turn improves more the precision than the recall of the monolingual data-driven coreference systems.
  In addition, Table 5 shows that the English coref- erence results in better recall than Romanian coref- erence.
  However, the recall shows a decrease for both languages for SNIZZLE because imprecise coreference links are deleted.
  As is usually the case, deleting data lowers the recall.
  All results were obtained by using the automatic scorer program developed for the MUC evaluations.</p>
</blockquote>

<p>Note how the table does not contain strange characters and goes right in the middle of the sentence: ""This result explains why there is better precision enhancement for -TABLE HERE- the English coreference."" I can't know where the table will be in regard to the running text. It may occur before a sentence, after it or within it like in this case. Also note that the table shit does not end with a full stop (most captions in papers don't...) so I can't rely on punctuation to spot it. I am happy with non-accurate boundaries of course, but I still need to do something with these tables. Some of them contain words rather than numbers, and I don't have enough information in those cases: no junky characters, nothing. It is obvious to only humans :S</p>
","language-agnostic, nlp, stanford-nlp","<p>(I hate crappy copy&amp;pastes. )</p>

<p>Few ideas that you might find helpful (I used each and every one of them myself in that point or another)</p>

<ol>
<li><p>(Very brute force) : Using a tokenizer and a dictionary (real dictionary, not the data structure) - parse the words out and any word which is not a dictionary word - remove it. It might prove problematic if your text contains a lot of company/products names - but this too can be solved using the correct indexes (there are a few on the web - I'm using some propriety ones so I can't share them, sorry) </p></li>
<li><p>Given a set of clean documents (lets say a 2K), build an tf/idf index of them, and use this as a dictionary - every term from the other documents that doesn't appear in the index (or appears with a very low tf/idf) - remove it. This should give you a rather clean document.</p></li>
<li><p>Use Amazon's mechanical turk mechanism : set up a task where the person reading the document needs to mark the paragraph that doesn't make sense. Should be rather easy for the mechanical turk platform (16.5K is not that much) - this will probably cost you a couple of hundred $ , but you'll probably get a rather nice cleanup of the text (So if it's on corporate money, that can be your way out - they need to pay for their mistakes :) ).</p></li>
<li><p>Considering your documents are from the same domain (same topics, all in all), and the problems are quite the same (same table headers, roughly same formulas): Break all the documents to sentences, and try clustering the sentences using ML. If the table headers / formulas are relatively similar, they should cluster nicely away from the rest of the sentences, and then you can clean the documents sentence-by-sentence (Get a document, break it to sentences,  for each sentence, if it's part of the ""weird"" cluster, remove it)</p></li>
</ol>
",767,1335969751
what&#39;s the maximum number of nominal values in a nominal attribute for SVM training in WEKA?,"<p>I have an NLP problem and I plan to use classifying in WEKA with SVMs.
I'm trying to classify words - the POS tagset has 24 tags, and the base phrase chunk (BPC) tagset has 15 tags.</p>

<p>But I have ""feature sets"", and I want each word to be classified for each of its features.</p>

<p>The first feature set is {POS}, so that'll be 24 nominal values for the nominal attribute POS.
The second is {POS+BPC}, so that'll be 24*15=375 nominal values for the POS+BPC nominal attribute.</p>

<p>So for example a word might be output like this, with each nominal attribute classified:</p>

<pre><code>word, POS=tag1, POS+BPC=tag234
</code></pre>

<p>I'm just wondering if this is possible? What's the maximum number of (classes) nominal values I can have for a nominal attribute? Because I might be using more tagsets and more combinations. Would I need to use the LibSVM package? Does it even make sense to do this multi-class problem using SVMs?</p>

<p>My training dataset is approx. 288K words, and my testing dataset is approx. 35k words.</p>
","nlp, weka, svm, libsvm",,711,1336738215
What is Oracle experiment?,"<p>I have read a paper about machine learning and it contains an Oracle experiment to compare between his study and another study?
But it does not seem to be so clear what is Oracle experiment?</p>
","nlp, machine-learning","<p>An ""oracle"" is an imaginary entity that always gives the right answer.  An oracle experiment is used to compare your actual system to how your system would behave if some component of it always did the right thing.</p>

<p>For example, in the NLP domain, let's assume you built a parser that takes part-of-speech (POS) tagged sentences as input.  In the real world, you would have to run real sentences through an actual POS tagger.  This tagger would probably produce results with accuracy above 90%, but less than 100%.  Since the accuracy of your parser depends on the accuracy of the incoming tags, your parser's performance will be negatively affected by this loss.  </p>

<p>In order to see how well your parser would perform <em>if the POS tagger was perfect</em>, you could run an experiment with an oracle tagger.  In this experiment, you would replace the real POS tagger with a program that knows the actual POS tags for the sentences, thus always returning tag results with 100% accuracy.</p>

<p>So, if your parser gets 85% accuracy in an experiment with a real tagger, and 90% in an experiment with an oracle tagger, then you know that 5% of your performance loss is directly due to the mistakes of the tagger.</p>
",6029,1336688126
Get only word before special char,"<p>I have the file with ""chunked"" sentences from medical sector.</p>

<p>The sentences looks like:</p>

<blockquote>
  <p>"" [ADVP again/RB ] [VP seen/VBN ] [NP is/VBZ ] [NP a/DT focal/JJ
  asymmetry/NN ].</p>
  
  <p>"" [NP diagnostic/JJ mammogram/NN ] [PP of/IN ] [NP the/DT left/JJ
  breast/NN ].</p>
</blockquote>

<p>I need the way to extract only <strong>words</strong> from those ""chunks"". The common rule for all sentences is that the words comes before '/' char.</p>

<p>Is there is option to combine REGEX or something else in order to get string without POS?</p>

<p>Thanks</p>
","c#, regex, string, nlp","<p>This seem to work:</p>

<pre><code>var patt = @""\s(\b(.+?))/"";
var matches = Regex.Matches(""[ADVP again/RB ] [VP seen/VBN ] [NP is/VBZ ] [NP a/DT focal/JJ asymmetry/NN ]."", patt);

var matchedValues = matches
    .Cast&lt;Match&gt;()
    .Select(match =&gt; match.Groups[1].Value);

var output = string.Join("" "", matchedValues);
</code></pre>

<p>=> ""again seen is a focal asymmetry""</p>
",186,1336736281
Counting the number of occurrences of words in a textfile,"<p>How could I go about keeping track of the number of times a word appears in a textfile? I would like to do this for every word.</p>

<p>For example, if the input is something like:</p>

<p>""the man said hi to the boy.""</p>

<p>Each of ""man said hi to boy"" would have an occurrence of 1.</p>

<p>""the"" would have an occurence of 2.</p>

<p>I was thinking of keeping a dictionary with word/occurrence pairs but I'm not sure how to implement this in C. A link to any similar or related problems with a solution would be great.</p>

<hr>

<p>EDIT: To avoid rolling out my own hash table I decided to learn how to use glib. Along the way I found an excellent tutorial which walks through a similar problem. <a href=""http://bo.majewski.name/bluear/gnu/GLib/ch03s03.html"" rel=""nofollow noreferrer"">http://bo.majewski.name/bluear/gnu/GLib/ch03s03.html</a></p>

<p>I am awestruck by the number of different approaches, and in particular the simplicity and elegance of the Ruby implementation.</p>
","c, algorithm, nlp, counting","<p>Yes, a dictionary with word-occurence pairs would work fine, and the usual way to implement such a dictionary would be to use a hash table (or, sometimes, a binary search tree).</p>

<p>You could also use a <a href=""http://en.wikipedia.org/wiki/Trie"" rel=""noreferrer"">trie</a> (or its compressed version, <a href=""http://en.wikipedia.org/wiki/Radix_tree"" rel=""noreferrer"">""Patricia trie""</a>/Radix trie) whose complexity is asymptotically optimal for this problem, although I suspect that in practice it might be slower than a (good) hash table implementation.</p>

<p>[I really think whether hash tables or tries are better depends on the distribution of words in your input -- e.g. a hash table will need to store each word in its hash bucket (to guard against collisions), while if you have a lot of words with common prefixes, in a trie those common prefixes are shared and need to be stored only once each, but there is still the overhead of all the pointers... if you do happen to try both, I'm curious to know how they compare.]</p>
",17093,1230244491
&#39;Following&#39; an object,"<p>I am trying to develop a real-time feed, and have run into a dilemma. I have the standard event, with a user as the subject -</p>

<ul>
<li><strong>Person</strong> is now <strong>action</strong>. [ For example, ""<strong>John</strong> is now <strong>connected with Steve</strong>"". ]</li>
</ul>

<p>This event occurs when a person that I am following does something.</p>

<p>In addition, I also have an event that is dependent on a non-user object, a ""production"" -</p>

<ul>
<li><strong>Production</strong> is now <strong>action</strong> [ For example, <strong>Titanic</strong> now <strong>includes Kate Winslet in its credits</strong>.</li>
</ul>

<p>In other words, users can follow other 1) users or 2) productions. However, the two events may butt into each other. Suppose the following example.</p>

<p>1) </p>

<ul>
<li>David is not following the user Steve.</li>
<li>David is following the production Titanic</li>
<li>Titanic has a new event, which involves the user Steve.</li>
<li>Since David is following Titanic and not Steve, David;s feed will say something like:<br>
   ""<strong>Titanic</strong> now includes <strong>Steve</strong> in its credits"".</li>
<li>Now David starts following Steve, the above grammar works but suppose this case:</li>
</ul>

<p>2) </p>

<ul>
<li>David is following Steve now, but not Titanic.</li>
<li>Steve joins the production Titanic. On David's feed would say something like:<br>
""<strong>Steve</strong> joined the production <strong>Titanic</strong>"".</li>
<li>Now David starts following the Titanic, the above grammar works, but is in conflict with 1) based upon the ordering of events.</li>
</ul>

<p>It seems this is why sites like Facebook/LinkedIn <strong>always</strong> have a single consistent object as the subject of the event (a user, in their cases). </p>

<p>Would it be possible to have both a user and a production? I know it would be more difficult, but how could it be done?</p>
","java, python, sql, nosql, nlp","<blockquote>
  <p>Would it be possible to have both a user and a production? </p>
</blockquote>

<p>Several ways:</p>

<ul>
<li>Database:  set or clear status flags (in action or not)</li>
<li>Python:  either multiple inheritance with status flags or use composition with delegation</li>
<li>Java: composition with delegation</li>
</ul>

<p>Another way to think about the problem is to use <a href=""http://en.wikipedia.org/wiki/Role-oriented_programming"" rel=""nofollow"">role-oriented programming</a> techniques.</p>
",99,1336257637
Using query expressions for Unicode strings with LINQ,"<p>I am working with LINQ and and I have a database with columns for storing local content(non-english characters). Now I want to make a query using linq as follows</p>

<pre><code>var desc = from p in db.GetDesc                            
           where  p.Category.Contains(""xxxx"".ToString())
           orderby p.Date descending
           select p;
</code></pre>

<p>Here the Category column contains unicode strings and the above query string doesn't work. How can I use natural language queries with LINQ?</p>
","c#, sql-server, linq, nlp",,2501,1336207243
Convert Chinese Pinyin with accents to numerical form,"<p>I'm looking to convert Pinyin where the tone marks are written with accents (e.g.: Nín hǎo) to Pinyin written in numerical/ASCII form (e.g.: Nin2 hao1).</p>

<p>Does anyone know of any libraries for this, preferably PHP? Or know Chinese/Pinyin well enough to comment?</p>

<p>I started writing one myself that was rather simple, but I don't speak Chinese and don't fully understand the rules of when words should be split up with a space.</p>

<p>I was able to write a translator that converts:</p>

<p><code>Nín hǎo. Wǒ shì zhōng guó rén</code> ==> <code>Nin2 hao3. Wo3 shi4 zhong1 guo2 ren2</code></p>

<p>But how do you handle words like the following - do they get split up with a space into multiple words, or do you interject the tone numbers within the word (if so, where?) :
<code>huā shíjiān</code>, <code>wèishénme</code>, <code>yuèláiyuè</code>, <code>shēngbìng</code>, etc.</p>
","php, nlp, cjk","<p>The problem with parsing pinyin without the space separating each word is that there will be ambiguity. Take, for instance, the name of an ancient Chinese capital <a href=""http://en.wikipedia.org/wiki/Changan"">长安</a>: Cháng'ān (notice the disambiguating apostrophe). If we strip out the apostrophe however this can be interpreted in two ways: <code>Chán gān</code> or <code>Cháng ān</code>. A Chinese would tell you that the second is far more likely, depending on the context of course, but there's no way your computer can do that. </p>

<p>Assuming no ambiguity, and that all input are valid, the way I would do it would look something like this: </p>

<ol>
<li>Create accent folding function</li>
<li>Create an array of valid pinyin (You should take it from the Wikipedia page for pinyin)</li>
<li>Match each word to the list of valid pinyin</li>
<li>Check ahead to the next word when there is ambiguity about the possibility of the last character belonging to the next word, such as:</li>
</ol>

 <pre>
 shēngbìng
     ^ Does this 'g' belong to the next word?
 </pre>

<p>Anyway, the correct positioning of the numerical representation of the tones, and the correct numerals to represent each accent are covered fairly well in this section of the Wikipeda article on pinyin: <a href=""http://en.wikipedia.org/wiki/Pinyin#Numerals_in_place_of_tone_marks"">http://en.wikipedia.org/wiki/Pinyin#Numerals_in_place_of_tone_marks</a>. You might also want to have a look at how <a href=""http://en.wikipedia.org/wiki/Input_method_editor"">IMEs</a> do their job. </p>
",1970,1289426844
Removing numbers from strings,"<p>So, I am working with a text file on which I am doing the following operations on the string</p>

<pre><code>     def string_operations(string):

        1) lowercase
        2) remove integers from string
        3) remove symbols
        4) stemming
</code></pre>

<p>After this, I am still left with strings like:</p>

<pre><code>  durham 28x23
</code></pre>

<p>I see the flaw in my approach but would like to know if there is a good, fast way to identify if there is a numeric value attached with the string.</p>

<p>So in the above example, I want the output to be</p>

<pre><code>  durham
</code></pre>

<p>Another example:</p>

<pre><code> 21st ammendment
</code></pre>

<p>Should give:</p>

<pre><code>ammendment
</code></pre>

<p>So how do I deal with this stuff?</p>
","python, algorithm, nlp","<p>If you requirement is, ""remove any terms that start with a digit"", you could do something like this:</p>

<pre><code>def removeNumerics(s):
  return ' '.join([term for term in s.split() if not term[0].isdigit()])
</code></pre>

<p>This splits the string on whitespace and then joins with a space all the terms that do not start with a number.</p>

<p>And it works like this:</p>

<pre><code>&gt;&gt;&gt; removeNumerics('21st amendment')
'amendment'
&gt;&gt;&gt; removeNumerics('durham 28x23')
'durham'
</code></pre>

<p>If this isn't what you're looking for, maybe show some explicit examples in your questions (showing both the initial string and your desired result).</p>
",131,1336158687
Negating sentences using POS-tagging,"<p>I'm trying to find a way to negate sentences based on POS-tagging. Please consider:</p>

<pre><code>include_once 'class.postagger.php';

function negate($sentence) {  
  $tagger = new PosTagger('includes/lexicon.txt');
  $tags = $tagger-&gt;tag($sentence);
  foreach ($tags as $t) {
    $input[] = trim($t['token']) . ""/"" . trim($t['tag']) .  "" "";
  }
  $sentence = implode("" "", $input);
  $postagged = $sentence;

  // Concatenate ""not"" to every JJ, RB or VB
  // Todo: ignore negative words (not, never, neither)
  $sentence = preg_replace(""/(\w+)\/(JJ|MD|RB|VB|VBD|VBN)\b/"", ""not$1/$2"", $sentence);

  // Remove all POS tags
  $sentence = preg_replace(""/\/[A-Z$]+/"", """", $sentence);

  return ""$postagged&lt;br&gt;$sentence"";
}
</code></pre>

<p>BTW: In this example, I'm using the <a href=""http://phpir.com/part-of-speech-tagging"" rel=""noreferrer"">POS-tagging implementation</a> and <a href=""http://phpir.com/user/files/text/lexicon.txt"" rel=""noreferrer"">lexicon</a> of Ian Barber. An example of this code running would be:</p>

<pre><code>echo negate(""I will never go to their place again"");
I/NN will/MD never/RB go/VB to/TO their/PRP$ place/NN again/RB 
I notwill notnever notgo to their place notagain
</code></pre>

<p>As you can see, (and this issue is also commented in the code), negating words themselves are being negated as wel: <code>never</code> becomes <code>notnever</code>, which obviously shouldn't happen. Since my regex skills aren't all that, is there a way to exclude these words from the regex used?</p>

<p>[edit] Also, I would very much welcome other comments / critiques you might have in this negating implementation, since I'm sure it's (still) quite flawed :-)</p>
","php, regex, nlp","<p>Give this a try:</p>

<pre><code>$sentence = preg_replace(""/(\s)(?:(?!never|neither|not)(\w*))\/(JJ|MD|RB|VB|VBD|VBN)\b/"", ""$1not$2"", $sentence);
</code></pre>
",853,1335878647
English to babel-lang,"<p>Recently I was checking up on some AI possibilities and found out about <a href=""http://babelproject.com/"" rel=""nofollow"">babel-lang</a>, which seems to be an interesting concept.</p>

<p>The project appears to be abandoned but the concept is quite interesting.
It offers a babel > english translator and a good spec to do so, but no way to do the oposite.</p>

<p>I was wondering how one could go about making an english > babel translator.</p>

<p>From what I can think of, the only way is to have a large set of verbs, nouns, adjectives, etc which can then be used translate the sentence into babel.</p>

<p>Is this really the only (or best) way?</p>
","nlp, artificial-intelligence, translation","<p>Babel is my little project. It's correct that there hasn't been any work done to the project in a long time.  I still hope to pick it up again though.</p>

<p>An english->babel translator would be awesome and I would love to see such a project.  But the difficulty of the problem is the entirety of Natural Language Processing.  In theory an NLP parsing engine could be modified to output babel.</p>

<p>The whole point of the babel project is to ""divide the problem"" cleanly.  By having the user write what they really mean (aka babel code), then we don't have to worry about the NLP problems and can instead focus on the real problem of creating and manipulating thoughts.</p>

<p>Bryan</p>
",163,1315984649
Algorithm to determine if a word could be English?,"<p>I have a list of strings that I need to check against an English dictionary.
However I don't want to start checking every piece of gibberish in the list. First, I want to check if the string could be an English word.</p>

<p>Does anyone know of an algorithm that does this or at least the rules that I need to apply to verify a word?</p>

<p>For example:</p>

<p>No spoken word can start with more than 3 consonants, and if there are are 3 initial consonants in a word, the first one must be ""s"".</p>
","algorithm, nlp","<p>Finding a word in a data structure is going to be fast (e.g. use a <a href=""http://en.wikipedia.org/wiki/Bloom_filter"" rel=""nofollow"">Bloom filter</a> (mind the false positives!), or a set)  so chances are it is not worth doing this for efficiency reasons.</p>

<p>If you want to provide suggestions, then look at Peter Norvig's <a href=""http://norvig.com/spell-correct.html"" rel=""nofollow"">spell checking</a>  implementation.</p>

<p>If you really want to go that way, then I'd construct frequencies of A follows B from existing text to see whether any given sequence is contained within English words.</p>
",2040,1311251005
Regexp for Tokenizing English Text,"<p>What would be the best regular expression for tokenizing an English text?</p>

<p>By an English token, I mean an atom consisting of maximum number of characters that can be meaningfully used for NLP purposes. An analogy is a ""token"" in any programming language (e.g. in C, '{', '[', 'hello', '&amp;', etc. can be tokens). There is one restriction: Though English punctuation characters can be ""meaningful"", let's ignore them for the sake of simplicity when they do not appear in the middle of \w+. So, ""Hello, world."" yields 'hello' and 'world'; similarly, ""You are good-looking."" may yield either [you, are, good-looking] or [you, are, good, looking].</p>
","regex, text, nlp",,4477,1284407816
Mapping words to numbers with respect to definition,"<p>As part of a larger project, I need to read in text and represent each word as a number. For example, if the program reads in ""<em>Every good boy deserves fruit</em>"", then I would get a table that converts '<strong><em>every</em></strong>' to '<strong>1742</strong>', '<strong><em>good</em></strong>' to '<strong>977513</strong>', etc. </p>

<p>Now, obviously I can just use a hashing algorithm to get these numbers. However, it would be more useful if words with similar meanings had numerical values close to each other, so that '<strong><em>good</em></strong>' becomes '<strong>6827</strong>' and '<strong><em>great</em></strong>' becomes '<strong>6835</strong>', etc. </p>

<p>As another option, instead of a simple integer representing each number, it would be even better to have a vector made up of multiple numbers, eg (<i>lexical_category</i>, <em>tense</em>, <em>classification</em>, <i>specific_word</i>) where <i>lexical_category</i> is noun/verb/adjective/etc, <em>tense</em> is future/past/present, <em>classification</em> defines a wide set of general topics and <i>specific_word</i> is much the same as described in the previous paragraph.</p>



<p>Does any such an algorithm exist? If not, can you give me any tips on how to get started on developing one myself? I code in C++.</p>
","c++, nlp, hash, semantic-markup",,1793,1269222481
Generate a list of English words containing consecutive consonant sounds,"<p>Start with this:</p>

<pre><code>[G|C] * [T] *
</code></pre>

<p>Write a program that generates this:</p>

<pre><code>Cat
Cut
Cute
City &lt;-- NOTE: this one is wrong, because City has an ""ESS"" sound at the start.
Caught
...
Gate
Gotti
Gut
...
Kit
Kite
Kate
Kata
Katie
</code></pre>

<p>Another Example, This:</p>

<p>[C] * [T] * [N]</p>

<p>Should produce this:</p>

<p>Cotton
   Kitten </p>

<p>Where should I start my research as I figure out how to write a program/script that does this?</p>
","algorithm, nlp","<p>You can do this by using regular expressions against a dictionary containing phonetic versions of words.</p>

<p>Here's an example in Javascript:</p>

<pre><code>     &lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;Test&lt;/title&gt;
        &lt;script type=""text/javascript"" src=""https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js""&gt;&lt;/script&gt;
        &lt;script&gt;

            $.get('cmudict0.3',function (data) {
                matches = data.match(/^(\S*)\s+K.*\sT.*\sN$/mg);
                $('body').html('&lt;p&gt;'+matches.join('&lt;br/&gt; ')+'&lt;/p&gt;');
            })

        &lt;/script&gt;
    &lt;/head&gt;
    &lt;body&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>You'll need to download the list of all words from <a href=""http://icon.shef.ac.uk/Moby/mpron.tar.Z"" rel=""nofollow noreferrer"">http://icon.shef.ac.uk/Moby/mpron.tar.Z</a> and put it (uncompressed) in the same folder as the HTML file. I've only translated the [C] * [T] * [N] version into a regular expression and the output isn't very nice but it'll give you the idea. Here's a sample of the output:</p>

<pre><code>CALTON K AE1 L T AH0 N
CAMPTON K AE1 M P T AH0 N
CANTEEN K AE0 N T IY1 N
CANTIN K AA0 N T IY1 N
CANTLIN K AE1 N T L IH0 N
CANTLON K AE1 N T L AH0 N
...
COTTERMAN K AA1 T ER0 M AH0 N
COTTMAN K AA1 T M AH0 N
COTTON K AA1 T AH0 N
COTTON(2) K AO1 T AH0 N
COULSTON K AW1 L S T AH0 N
COUNTDOWN K AW1 N T D AW2 N
..
KITSON K IH1 T S AH0 N
KITTELSON K IH1 T IH0 L S AH0 N
KITTEN K IH1 T AH0 N
KITTERMAN K IH1 T ER0 M AH0 N
KITTLESON K IH1 T L IH0 S AH0 N
...
</code></pre>
",1869,1266531700
Canadian to US English,"<p>Does there exist something like Canadian to US English e-dictionary which I can use in my application?</p>
","list, dictionary, nlp",,210,1258695650
I have text files in multiple languages. How to selectively delete one language in NLTK?,"<p>Maybe this is just impossible and I should give up all hope. Or maybe there's a really clever way to do it that I haven't thought of.</p>

<p>Here's two examples of what I've got:</p>

<blockquote>
  <p>يَبِسَ - يَيْبَسُ (yabisa,
  yaybasu)[y-b-s][ي-ب-س] (To become dry,
  stiff, rigid) 20:77 yabasan = dry.
  يَسَّرَ - يُيَسِّرُ (yassara,
  yuyassiru)[y-s-r][ي-س-ر] (To
  facilitate, make it easy) 92:7
  nuyassiruhuu = We will ease him.</p>
</blockquote>

<p>and </p>

<blockquote>
  <p>Zu Hülfe!  zu Hülfe!      Help!  Help!<br>
  Sonst bin ich verloren!   Otherwise I am
  lost! Zu Hülfe!  Zu Hülfe!    Help! 
  Help! Sonst bin ich
  verloren! Otherwise I am lost! Der
  listigen Schlange zum Opfer erkoren, 
    Selected as offering to the cunning
  snake, Barmherzigige Götter!  Merciful
  Gods! Schon nahet sie sich,   Already it
  gets closer, Schon nahet sie
  sich, Already it gets closer,</p>
</blockquote>

<p>... it would be really annoying to go through and delete one language in order to further process these lines of text.</p>

<p>One way I was thinking this could be done in NLTK was to split the text into tokens, have some way of knowing the provenance of each token based on a small corpus, and then ask NLTK to 'reconstitute' only the tokens of my choosing. Is this just a wild fantasy?</p>
","python, localization, nlp, nltk","<p>You can use nltk.NaiveBayesClassifier to do the job exactly as you said above.</p>

<p>The following link should help:
<a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html"" rel=""nofollow noreferrer"">http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html</a></p>

<p>It has an example of using nltk.NaiveBayesClassifier for gender identification. you use the same for language identification.</p>

<p>The first example you quoted will work well with nltk.NaiveBayesClassifier since the unicode set is completely different.</p>

<p>In the second example, there is a possibility of words like proper nouns spelled the same in both the languages which might cause some error in identification of the language.</p>
",767,1282777931
Algorithm for Natural-Looking Sentence in English Language,"<p>I'm building an application that does sentence checking. Do you know are there any DLLs out there that recognize sentences and their logic and organize sentences correctly? Like put words in a sentence into a correct sentence.</p>

<p>If it's not available, maybe you can suggest search terms that I can research.</p>
","c#, .net, nlp",,2963,1333106480
sentence identification/detection: decide whether some text is a sentence or not,"<p>Most sentence splitters are able to split a stream of text at the correct position.</p>

<p>I am looking for a model that will <strong>decide whether some text is a sentence or not</strong>. </p>
","java, nlp",,612,1336063194
Grouping Similar Strings,"<p>I'm trying to analyze a bunch of search terms, so many that individually they don't tell much.  That said, I'd like to group the terms because I think similar terms should have similar effectiveness.  For example,</p>

<pre><code>Term               Group
NBA Basketball     1
Basketball NBA     1
Basketball         1
Baseball           2
</code></pre>

<p>It's a contrived example, but hopefully it explains what I'm trying to do.  So then, what is the best way to do what I've described?  I thought the <code>nltk</code> may have something along those lines, but I'm only barely familiar with it.</p>

<p>Thanks </p>
","python, nlp, nltk","<p>You'll want to cluster these terms, and for the similarity metric I recommend <a href=""http://en.wikipedia.org/wiki/Dice%27s_coefficient"">Dice's Coefficient</a> at the character-gram level. For example, partition the strings into two-letter sequences to compare (term1=""NB"", ""BA"", ""A "", "" B"", ""Ba""...).</p>

<p><strong>nltk</strong> appears to provide dice as <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.metrics.association.BigramAssocMeasures-class.html"">nltk.metrics.association.BigramAssocMeasures.dice()</a>, but it's simple enough to implement in a way that'll allow tuning. Here's how to compare these strings at the character rather than word level.</p>

<pre><code>import sys, operator

def tokenize(s, glen):
  g2 = set()
  for i in xrange(len(s)-(glen-1)):
    g2.add(s[i:i+glen])
  return g2

def dice_grams(g1, g2): return (2.0*len(g1 &amp; g2)) / (len(g1)+len(g2))

def dice(n, s1, s2): return dice_grams(tokenize(s1, n), tokenize(s2, n))

def main():
  GRAM_LEN = 4
  scores = {}
  for i in xrange(1,len(sys.argv)):
    for j in xrange(i+1, len(sys.argv)):
      s1 = sys.argv[i]
      s2 = sys.argv[j]
      score = dice(GRAM_LEN, s1, s2)
      scores[s1+"":""+s2] = score
  for item in sorted(scores.iteritems(), key=operator.itemgetter(1)):
    print item
</code></pre>

<p>When this program is run with your strings, the following similarity scores are produced:</p>

<pre><code>./dice.py ""NBA Basketball"" ""Basketball NBA"" ""Basketball"" ""Baseball""

('NBA Basketball:Baseball', 0.125)
('Basketball NBA:Baseball', 0.125)
('Basketball:Baseball', 0.16666666666666666)
('NBA Basketball:Basketball NBA', 0.63636363636363635)
('NBA Basketball:Basketball', 0.77777777777777779)
('Basketball NBA:Basketball', 0.77777777777777779)
</code></pre>

<p>At least for this example, the margin between the <em>basketball</em> and <em>baseball</em> terms should be sufficient for clustering them into separate groups. Alternatively you may be able to use the similarity scores more directly in your code with a threshold.</p>
",3298,1324841813
NLP - Improving Running Time and Recall of Fuzzy string matching,"<p>I have made a working algorithm but the running time is very horrible. Yes, I know from the start that it will be horrible but not that much. For just 200000 records, the program runs for more than an hour.</p>

<p>Basically what I am doing is:</p>

<pre><code>for each searchfield in search fields
    for each sample in samples
        do a q-gram matching
    if there are matches then return it
    else
        split the searchfield into uniwords
        for each sample in samples
            split sample into uniwords
            for each uniword in samples
                if the uniword is a known abbreviation
                    then search the dictionary for its full word or other known abbr
                else do a jaro-winkler matching
            average the distances of all the uniwords
            if the average is above threshold then make it as a match and break
        end for
        if there is a match make a comment that it matched one of the samples partially
    end else
end for
</code></pre>

<p>Yes, this code is very loop-happy. I am using brute-force because the recall is very important. So, I'm wondering how can I make it faster since I am not only running it for 200000 data for millions of data and the computers of the client are not high-end (1GB-2GB of Ram Pentium 4 or Dual-Core, the computer where I test this program is a Dual Core with 4GB of Ram). I came across TF/IDF but I do not know if it will be sufficient. And I wonder how can google make searches real time.</p>

<p>Thanks in advance!</p>

<p>Edit:
This program is a data filterer. From 200,000 dummy data (actual data is about 12M), I must filter data that is irrelevant to the samples (500 dummy samples, I still do not know how much the actual amount of samples).</p>

<p>With the given dummy data and samples, the running time is about 1 hour but after tinkering here and there, I have successfully lessen it to 10-15 minutes. I have lessen it by grouping the fields and samples that begin with the same character (discounting special and non-meaningful words e.g. the, a, an) and matching the fields to the sample with the same first character. I know there is a problem there. What if the field was misspelled at the first character? But I think the number of those are negligible. The samples are spelled correctly since it is always maintained.</p>
","nlp, fuzzy-search, tf-idf, jaro-winkler",,415,1335231208
mapping English words to their singular forms,"<p>Examples:
mapping a plural noun to its singular form:  children --> child, boxes ---> box
mapping comparative and superlative of adjectives and adverbs to their basic form
mapping verb .... </p>

<p>I once found an open source resource to achieve it. As I remember, it is a datasheet?/spreadsheet?/database? of the mapping. Does anyone know the open source resource, or any alternative/better solution?
I'm coding in java.</p>
",nlp,"<p>That is called <em>stemming</em> and <a href=""http://lucene.apache.org/java/docs/index.html"" rel=""nofollow noreferrer"">Lucene</a> (specifically <a href=""http://snowball.tartarus.org/"" rel=""nofollow noreferrer"">snowball</a> contrib) can do that.</p>

<p>There is a number of <a href=""https://stackoverflow.com/questions/5068790/difference-between-lucene-stemmers-englishstemmer-porterstemmer-lovinsstemmer"">different stemming algorithms</a>.</p>
",220,1333762361
Natural language generator for dates (Java),"<p>I'm building a system that needs to provide a commentary on things in natural English. One thing that is of use is to be able to express dates in a casual format. What I'm looking for is essentially the inverse of <a href=""https://github.com/samtingleff/jchronic"" rel=""nofollow noreferrer"">Chronic</a>, <a href=""http://natty.joestelmach.com/"" rel=""nofollow noreferrer"">Natty</a>, or the task described in this question: <a href=""https://stackoverflow.com/questions/1410408/natural-language-date-and-time-parser-for-java"">Natural Language date and time parser for java</a>.</p>

<p>Is this too out-there to have been done? Should I try and roll my own simple hardwired piece for the date ranges that make sense to me? Or is there some clever way to reverse existing parsers to spit out (even garbled) sentences describing dates?</p>

<p>EDIT - To clarify, although <em>any</em> kind of output is interesting and useful, I'm particularly interested in varied/creative output generation. i.e. ""Next week"", ""seven days from now"", ""next Thursday"" and ""late next week"" all for the same date.</p>
","java, date, nlp","<p>There isn't much out there. Personally I'd love to see a variant of SimpleDateFormatter called RelativeDateFormatter that would let one create their own variations of relative date formats. Here is what I found:</p>

<ul>
<li>A simple <a href=""http://kurtischiappone.com/programming/java/relative-date"" rel=""noreferrer"">Relative Date</a> class.</li>
<li>A <a href=""http://www.java2s.com/Tutorial/Java/0040__Data-Type/Aformatterthatformatsdatestoshowtheelapsedtimerelativetosomebasedate.htm"" rel=""noreferrer"">relative date formatter</a> (so someone <em>did</em> make one already!). </li>
<li><a href=""http://ocpsoft.org/prettytime/"" rel=""noreferrer"">Pretty time</a> also looks like it fits the bill - <em>and</em> its in a maven repo somewhere, if thats something you find desirable...</li>
</ul>

<p>Just an aside, I noticed this post about <a href=""http://code.skyrocket.be/post/20337852596/why-relative-date-notations-suck-most-of-the-time"" rel=""noreferrer"">why relative date formats suck</a>. Providing <em>only</em> relative dates can be a mistake: if the information is going to be saved at a later date, the relative date could be useless. In a website I made I provided a mouseover with the absolute date, and CSS rules to print the absolute date. I guess the thing to keep in mind: consider how/when the dates will be viewed. If they're anything other than ""NOW"" then you might want to consider including the absolute date as well.</p>
",1251,1335983669
Finding a &quot;subject&quot; from an array of part of speech tags,"<p>I know this question is more of a grammar question however how do you determine a ""subject"" of a sentence if you have a array of <code>Penn Treebank</code> tokens like:</p>

<pre><code>[WP][VBZ][DT][NN]
</code></pre>

<p>Is there any java library that can take in such tokens and determine which one is the subject? Or which ones? </p>
","java, nlp, opennlp","<p>I have been successfully classifying subjects for Portuguese using OpenNLP. I created a shallow parser tweaking a little the OpenNLP Chunker component.</p>

<p>You can use the existing OpenNLP models for pos tagging and chunking, but you will train a new chunk model that takes the PoS tags + chunk tags to classify subjects.</p>

<p>The data format to train the Chunker is based on <a href=""http://www.cnts.ua.ac.be/conll2000/chunking/"" rel=""nofollow"">Conll 2000</a>:</p>

<pre><code>He        PRP  B-NP
reckons   VBZ  B-VP
the       DT   B-NP
current   JJ   I-NP
account   NN   I-NP
deficit   NN   I-NP
will      MD   B-VP
narrow    VB   I-VP
...
</code></pre>

<p>I then created a new corpus that looks like the following</p>

<pre><code>He        PRP+B-NP  B-SUBJ
reckons   VBZ+B-VP  B-V  
the       DT+B-NP   O
current   JJ+I-NP   O
account   NN+I-NP   O
deficit   NN+I-NP   O
will      MD+B-VP   O
narrow    VB+I-VP   O
</code></pre>

<p>If you have access to Penn Treebank you can create such data by looking for subject nodes in the corpus. Maybe you can start with <a href=""http://ilk.uvt.nl/team/sabine/homepage/software.html"" rel=""nofollow"">this Perl script</a> used to generate the data for the CoNLL-2000 Shared Task.</p>

<p>The evaluation results for Portuguese are 87.07 % for precision, 75.48 % for recall, and 80.86 % for F1.</p>
",1598,1335182930
Python/YACC Lexer: Token priority?,"<p>I'm trying to use reserved words in my grammar:</p>

<pre><code>reserved = {
   'if' : 'IF',
   'then' : 'THEN',
   'else' : 'ELSE',
   'while' : 'WHILE',
}

tokens = [
 'DEPT_CODE',
 'COURSE_NUMBER',
 'OR_CONJ',
 'ID',
] + list(reserved.values())

t_DEPT_CODE = r'[A-Z]{2,}'
t_COURSE_NUMBER  = r'[0-9]{4}'
t_OR_CONJ = r'or'

t_ignore = ' \t'

def t_ID(t):
 r'[a-zA-Z_][a-zA-Z_0-9]*'
 if t.value in reserved.values():
  t.type = reserved[t.value]
  return t
 return None
</code></pre>

<p>However, the t_ID rule somehow swallows up DEPT_CODE and OR_CONJ. How can I get around this? I'd like those two to take higher precedence than the reserved words.</p>
","python, parsing, nlp, yacc",,4384,1274852759
NLP Library (Subject Extraction+Sentiment Analysis) for a Java-based Web Application,"<p>I'm a college student looking for a NLP library to perform subject extraction and sentiment analysis in a Java-based web application for a summer-hobby project.</p>

<p>To give you a little context on what I'm trying to do... I want to build a Java-based web application that will extract subjects out of a Reddit submission's headlines, as well as identify the OP's sentiment for the headline (when possible). </p>

<p>Example Inputs:</p>

<ul>
<li>Reddit, we took the anti-SOPA petition from 943,702 signatures to
3,460,313. The anti-CISPA petition is at 691,768, a bill expansively
worse than SOPA. Please bump it, then let us discuss further measures
or our past efforts are in vain. We did it before, I'm afraid we are
called on to do it again.</li>
<li>My friend calls him ""Mr Ridiculously Photogenic Guy""</li>
<li>Insanity: CISPA Just Got Way Worse, And Then Passed On Rushed Vote</li>
</ul>

<p>I'm currently trying out AlchemyAPI, but it sounds like better NLP libraries exist out there. Preferablly, I wouldn't be restricted to a limited number of API requests in a given time period (AlchemyAPI has a quota). I've heard the names of GATE, LingPipe, and OpenNLP - however, I'm unsure whether they fit my needs.</p>

<p>I'm looking for framework/library/api recommendations, or even better, comparisons from experienced users. My experience with NLP is extremely limited, which is why I'm asking for help here (ps: if anyone has any resources for learning more, outside of www.nlp-class.org, please let me know!) :)</p>
","java, nlp, sentiment-analysis","<p>First, I'd highly recommend using python, as the NLP libraries are a bit more user friendly than java, and it'd be a lot less code to maintain for a one-man project.</p>

<p>I can't think of anything off the top of my head to do either classification, so my recommendation would be to train two classifiers, one for subject, and one for sentiment. You'll have to label data and define features, but I think that wouldn't be too hard, especially with sentiment where you build up a dictionary of 'emotion' words. Labeling data is a pain in the ass, but that and good features are how you get good classification.</p>

<p><strong>Subject Classifier:</strong></p>

<p>Use NLTK with a Naive Bayes classifier, and define features as the word (lowercased), and word bigrams and trigrams.</p>

<p><strong>Sentiment Classifier:</strong></p>

<p>Same features as subject classifier, but also have a feature that says word w is in emotion dictionary with connection c. So, word 'bad' means 'bad sentiment'.</p>

<p>Once you've amassed sufficient training/testing data, you train your classifiers and optimize features, if necessary, and then you can run the classifiers against whatever other data you want.</p>

<p>General Purpose Libraries (Java):</p>

<ul>
<li>OpenNLP </li>
<li>LingPipe</li>
<li>Weka</li>
<li>Stanford stuff</li>
</ul>

<p>Libraries (Python):</p>

<ul>
<li>NLTK</li>
<li>Scipy</li>
</ul>
",2253,1335982761
Latent Semantic Analysis in Python discrepancy,"<p>I'm trying to follow the <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow"">Wikipedia Article on latent semantic indexing</a> in Python using the following code:</p>

<pre><code>documentTermMatrix = array([[ 0.,  1.,  0.,  1.,  1.,  0.,  1.],
                            [ 0.,  1.,  1.,  0.,  0.,  0.,  0.],
                            [ 0.,  0.,  0.,  0.,  0.,  1.,  1.],
                            [ 0.,  0.,  0.,  1.,  0.,  0.,  0.],
                            [ 0.,  1.,  1.,  0.,  0.,  0.,  0.],
                            [ 1.,  0.,  0.,  1.,  0.,  0.,  0.],
                            [ 0.,  0.,  0.,  0.,  1.,  1.,  0.],
                            [ 0.,  0.,  1.,  1.,  0.,  0.,  0.],
                            [ 1.,  0.,  0.,  1.,  0.,  0.,  0.]])
u,s,vt = linalg.svd(documentTermMatrix, full_matrices=False)

sigma = diag(s)
## remove extra dimensions...
numberOfDimensions = 4
for i in range(4, len(sigma) -1):
    sigma[i][i] = 0
queryVector = array([[ 0.], # same as first column in documentTermMatrix
                     [ 0.],
                     [ 0.],
                     [ 0.],
                     [ 0.],
                     [ 1.],
                     [ 0.],
                     [ 0.],
                     [ 1.]])
</code></pre>

<p>How the math says it should work:</p>

<pre><code>dtMatrixToQueryAgainst = dot(u, dot(s,vt))
queryVector = dot(inv(s), dot(transpose(u), queryVector))
similarityToFirst = cosineDistance(queryVector, dtMatrixToQueryAgainst[:,0]
# gives 'matrices are not aligned' error.  should be 1 because they're the same
</code></pre>

<p>What does work, with math that looks incorrect: ( from <a href=""http://www.gototheboard.com/articles/An_Example_of_Latent_Semantic_Indexing"" rel=""nofollow"">here</a>)</p>

<pre><code>dtMatrixToQueryAgainst = dot(s, vt)
queryVector  = dot(transpose(u), queryVector)
similarityToFirst = cosineDistance(queryVector, dtMatrixToQueryAgainsst[:,0]) 
# gives 1, which is correct
</code></pre>

<p>Why does route work, and the first not, when everything I can find about the math of LSA shows the first as correct?  I feel like I'm missing something obvious...</p>
","python, numpy, nlp, scipy, latent-semantic-indexing","<p>There are several inconsistencies in your code that cause errors before your point of confusion. This makes it difficult to understand exactly what you tried and why you are confused (clearly you did not run the code as it is pasted, or it would have thrown an exception earlier).</p>

<p>That said, if I follow your intent correctly, your first approach is nearly correct. Consider the following code:</p>

<pre><code>documentTermMatrix = array([[ 0.,  1.,  0.,  1.,  1.,  0.,  1.],
                            [ 0.,  1.,  1.,  0.,  0.,  0.,  0.],
                            [ 0.,  0.,  0.,  0.,  0.,  1.,  1.],
                            [ 0.,  0.,  0.,  1.,  0.,  0.,  0.],
                            [ 0.,  1.,  1.,  0.,  0.,  0.,  0.],
                            [ 1.,  0.,  0.,  1.,  0.,  0.,  0.],
                            [ 0.,  0.,  0.,  0.,  1.,  1.,  0.],
                            [ 0.,  0.,  1.,  1.,  0.,  0.,  0.],
                            [ 1.,  0.,  0.,  1.,  0.,  0.,  0.]])
numDimensions = 4
u, s, vt = linalg.svd(documentTermMatrix, full_matrices=False)
u = u[:, :numDimensions]
sigma = diag(s)[:numDimensions, :numDimensions]
vt = vt[:numDimensions, :]
lowRankDocumentTermMatrix = dot(u, dot(sigma, vt))
queryVector = documentTermMatrix[:, 0]
lowDimensionalQuery = dot(inv(sigma), dot(u.T, queryVector))
lowDimensionalQuery
vt[:,0]
</code></pre>

<p>You should see that <code>lowDimensionalQuery</code> and <code>vt[:,0]</code> are equal. Think of <code>vt</code> as a representation of the documents in a low-dimensional subspace. First we map our query into that subspace to get <code>lowDimensionalQuery</code>, and then we compare it with the corresponding column of <code>vt</code>. Your mistake was trying to compare the transformed query to the document vector from <code>lowRankDocumentTermMatrix</code>, which lives in the original space. Since the transformed query has fewer elements than the ""reconstructed"" document, Python complained.</p>
",1588,1335396286
Natural Language Processing Toolkit for .NET,"<p>Can you give me some toolkits and libraries for natural language processing in .NET.</p>

<p>Are there tools like UIMA for .NET?</p>
",".net, nlp","<p>There is  <a href=""http://sharpnlp.codeplex.com/"" rel=""noreferrer"">SharpNLP</a>  ....</p>
",3796,1306402267
Where to find wordlists with gender and plural for German?,"<p>I'm trying to write a simple text mining application to try to tell a German word's gender and plural form.</p>

<p>So, first of all, I need a big wordlist for training. I've searched around but could not find any list having either gender nor plural.</p>
","nlp, linguistics, corpus","<p>You could use data from the <a href=""http://www.semanticsoftware.info/durm-german-lemmatizer"" rel=""noreferrer"">Durm German Lemmatizer</a>, the <a href=""http://www.ids-mannheim.de/lexik/TextGrid/morphisto.html"" rel=""noreferrer"">Morphisto Lexikon</a>, or the <a href=""http://www.j3e.de/ispell/igerman98/"" rel=""noreferrer"">ispell dictionary for German</a>. You might find some other resources by looking at me <a href=""http://www.delicious.com/hmuelner/german"" rel=""noreferrer"">del.ico.us page with tag ""german""</a></p>
",1537,1293581872
Word Map for Emotions,"<p>I am looking for a resource similar to WordNet. However, I want to be able to look up the positive/negative connotation of a word. For example:</p>

<pre><code>bribe - negative
offer - positive
</code></pre>

<p>I'm curious as to whether anyone has run across any tool like this in AI/NLP research, or even in linguistics.</p>

<p><strong>UPDATE:</strong>
For the curious, the accepted answer below put me on the right track towards what I needed. Wikipedia listed several different resources. The two I would recommend (because of ease of use/free use for a small number of API calls) are <a href=""http://www.alchemyapi.com/"" rel=""nofollow"">AlchemyAPI</a> and <a href=""http://www.lymbix.com/"" rel=""nofollow"">Lymbix</a>. I decided to go with AlchemyAPI, since people affiliated with academic institutions (like myself) and non-profits can get even more API calls per day if they just email the company.</p>
",nlp,"<p>Start looking up topics on 'sentiment analysis':  <a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""nofollow"">http://en.wikipedia.org/wiki/Sentiment_analysis</a></p>
",1516,1303755887
CJK Languages Pronunciation APIs,"<p>Are there any good (preferably open) APIs or databases of pronunciation audio files for Chinese/Japanese/Korean languages? I’ve been looking around, but somehow couldn’t find anything other than <a href=""http://api.forvo.com/"" rel=""nofollow"">Forvo</a> or Google Translate. Both are an overkill for me, since I only need data for those languages, and only pronunciations, no translations.</p>
",nlp,,710,1331762554
Speech Recognition: detecting Japanese Kana (consonant + vowel),"<p>I would like to find some open source code (although I would settle for a closed source product)  to convert an incoming audio stream of Japanese Kana (ie consonant+vowel pairs) and print them out pretty much in real-time.</p>

<p>However, I want to use these basic sound units for my own custom purpose, so I don't want any high-level processing that tries to extract genuine Japanese words. I just want to get the raw Kana.</p>

<p>Is anyone aware of such a technology?</p>

<p>I just learned today that the Japanese ' alphabet ' is basically a 10x5 grid of <a href=""http://en.wikipedia.org/wiki/Kana"" rel=""nofollow"">Kana</a>.  10 columns ( empty + 9 consonants ) and 5 rows ( vowels )</p>

<p>and each element is called a 'Kana', and the language consists of sequences of these Kana; these are the basic building blocks.</p>

<p>This must surely have a large impact on speech recognition algorithms.</p>

<p>For Western languages, all commercial speech recognition engines I am aware of derive from <a href=""http://cmusphinx.sourceforge.net/"" rel=""nofollow"">CMUSphinx</a> which operates on a tri-gram model:  it represents each movement between three phonemes with a unique MFCC vector and figures out the most likely tri-gram sequence(s) for an utterance (from which it can deduce trivially the phonemes, and then run through its dictionary of WORD-triplets, to figure out the most likely sentence).</p>

<p>But for a language such as Japanese, I would guess that this may no longer be the most efficient algorithm.</p>

<p>Instead, it may make sense to try and catch each individual Kana,  or Kana-pair.</p>

<p>...which is going to be 2-gram or 4-gram. but not 3!</p>

<p>Is there anything out there? Or do they just use the same engines the Western world does?</p>
","nlp, speech-recognition",,465,1320958952
Language converter in C++ (from japanese to english),"<p>Once again i need ur help, i have a file in japanese language and i want to convert that file into english using C++, since i dont think that i can use any API's of google in c++, so any general idea can prove helpful for me, Please suggest something.</p>

<p>Thanks a lot</p>

<p>Owais Masood</p>
","c++, nlp",,1072,1286809983
Compose synthetic English phrase that would contain 160 bits of recoverable information,"<p>I have 160 bits of random data.</p>

<p>Just for fun, I want to generate pseudo-English phrase to ""store"" this information in. I want to be able to recover this information from the phrase. </p>

<p><strong><em>Note:</strong> This is not a security question, I don't care if someone else will be able to recover the information or even detect that it is there or not.</em></p>

<p>Criteria for better phrases, from most important to the least: </p>

<ul>
<li>Short</li>
<li>Unique</li>
<li>Natural-looking</li>
</ul>

<p>The current approach, suggested <a href=""https://stackoverflow.com/questions/4683551/generating-a-pseudo-natural-phrase-from-a-big-integer-in-a-reversible-way/4684842#4684842"">here</a>:</p>

<p>Take three lists of 1024 nouns, verbs and adjectives each (picking most popular ones). Generate a phrase by the following pattern, reading 20 bits for each word:</p>

<pre>
Noun verb adjective verb,
Noun verb adjective verb,
Noun verb adjective verb,
Noun verb adjective verb.
</pre>

<p>Now, this seems to be a good approach, but the phrase is a bit too long and a bit too dull.</p>

<p>I have found a corpus of words <a href=""http://wordlist.sourceforge.net/"" rel=""nofollow noreferrer"">here</a> (Part of Speech Database).</p>

<p>After some ad-hoc filtering, I calculated that this corpus contains, approximately</p>

<ul>
<li>50690 usable adjectives</li>
<li>123585 nouns</li>
<li>15301 verbs</li>
<li>13010 adverbs (not included in pattern, but mentioned in answers)</li>
</ul>

<p>This allows me to use up to</p>

<ul>
<li>16 bits per adjective (actually 16.9, but I can't figure how to use fractional bits)</li>
<li>15 bits per noun</li>
<li>13 bits per verb</li>
<li>13 bits per adverb</li>
</ul>

<p>For noun-verb-adjective-verb pattern this gives 57 bits per ""sentence"" in phrase. This means that, if I'll use all words I can get from this corpus, I can generate three sentences instead of four (160 / 57 ≈ 2.8).</p>

<pre>
Noun verb adjective verb,
Noun verb adjective verb,
Noun verb adjective verb.
</pre>

<p>Still a bit too long and dull.</p>

<p>Any hints how can I improve it?</p>

<p>What I see that I can try:</p>

<ul>
<li><p>Try to compress my data somehow before encoding. But since the data is completely random, only some phrases would be shorter (and, I guess, not by much).</p></li>
<li><p>Improve phrase pattern, so it would look better.</p></li>
<li><p>Use several patterns, using the first word in phrase to somehow indicate for future decoding which pattern was used. (For example, use the last letter or even the length of the word.) Pick pattern according to the first bytes of the data.</p></li>
</ul>

<p>...I'm not that good with English to come up with better phrase patterns. Any suggestions?</p>

<ul>
<li>Use more linguistics in the pattern. Different tenses etc. </li>
</ul>

<p>...I guess, I would need much better word corpus than I have now for that. Any hints where can I get a suitable one?</p>
","nlp, steganography","<p>I would consider adding adverbs to your list. Here is a pattern I came up with:</p>

<pre><code>&lt;Adverb&gt;, the
    &lt;adverb&gt; &lt;adjective&gt;, &lt;adverb&gt; &lt;adjective&gt; &lt;noun&gt; and the
    &lt;adverb&gt; &lt;adjective&gt;, &lt;adverb&gt; &lt;adjective&gt; &lt;noun&gt;
&lt;verb&gt; &lt;adverb&gt; over the &lt;adverb&gt; &lt;adjective&gt; &lt;noun&gt;.
</code></pre>

<p>This can encode 181 bits of data. I derived this figure using lists I made a while back from WordNet data (probably a bit off because I included compound words):</p>

<ul>
<li>12650 usable nouns (13.6 bits/noun, rounded down)</li>
<li>5247 usable adjectives (12.3 bits/adjective)</li>
<li>5009 usable verbs (12.2 bits/verb)</li>
<li>1512 usable adverbs (10.5 bits/adverb)</li>
</ul>

<p><strong>Example sentence:</strong> ""Soaking, the habitually goofy, socially speculative swatch and the fearlessly cataclysmic, somewhere reciprocal macrocosm foreclose angelically over the unavoidably intermittent comforter.""</p>
",512,1295070099
Generate a pseudo-poem that would contain 160 bits of recoverable information,"<p>I have 160 bits of random data.</p>
<p>Just for fun, I want to generate an English pseudo-poem to &quot;store&quot; this information in. I want to be able to recover this information from the poem. (&quot;Poem&quot; here is a vague term for any kind of poetry.)</p>
<p><em><strong>Note:</strong> This is not a security question, I don't care if someone else will be able to recover the information or even detect that it is there or not.</em></p>
<p>Criteria for a better poem:</p>
<ul>
<li>Better aestetics</li>
<li>Better rhyme and foot</li>
<li>Uniqueness</li>
<li>Shorter length</li>
</ul>
<p>I'd say that the acceptable poem is no longer than three stanzas of four lines each. (But the other, established forms of poetry, like sonnets are good as well.)</p>
<p>I like this idea, but, I'm afraid, that I'm completely clueless in how to do English computer-generated poetry. (I programmed that for Russian when I was young, but looks like that experience will not help me here.)</p>
<p>So, any clues?</p>
<p><em><strong>Note:</strong> I already <a href=""https://stackoverflow.com/q/4698229/6236"">asked a similar question</a>. I want to try both approaches. Note how good poem criteria are different from the good phrase in parallel question. Remember, this is &quot;just for fun&quot;.</em></p>
<p><em>Also, I have to note this: There is an <a href=""https://www.rfc-editor.org/rfc/rfc1605"" rel=""nofollow noreferrer"">RFC 1605</a> on somewhat related matters. But it do not suggest any implementation details, so it is not quite useful for me, sorry. &lt;g&gt;</em></p>
","nlp, steganography","<h2>My naive solution/algorithm:</h2>

<ol>
<li>Write a beautiful 160-word poem</li>
<li>Take out a thesaurus, and find an equivalent word for each word in your poem.</li>
<li>The value each word in your original poem is <code>0</code> and the value of the word that you found in the thesaurus is <code>1</code></li>
<li>Now, encode your 160 bits into the poem</li>
</ol>

<p>Done.</p>
",389,1295071594
Natural language grammar and user-entered names,"<p>Some languages, particularly Slavic languages, change the endings of people's names according to the grammatical context. (For those of you who know grammar or studied languages that do this to words, such as German or Russian, and to help with search keywords, I'm talking about noun declension.)</p>

<p>This is probably easiest with a set of examples (in Polish, to save the whole different-alphabet problem):</p>

<ol>
<li>Dorothy saw the cat — <em>Dorota zobaczyła kota</em></li>
<li>The cat saw Dorothy — <em>Kot zobaczył Dorotę</em></li>
<li>It is Dorothy’s cat — <em>To jest kot Doroty</em></li>
<li>I gave the cat to Dorothy — <em>Dałam kota Dorotie</em></li>
<li>I went for a walk with Dorothy — <em>Poszłam na spacer z Dorotą</em></li>
<li>“Hello, Dorothy!” — <em>“Witam, Doroto!”</em></li>
</ol>

<p>Now, if, in these examples, the name here were to be user-entered, that introduces a world of grammar nightmares. Importantly, if I went for Katie (<em>Kasia</em>), the <a href=""http://en.wikibooks.org/wiki/Polish/Feminine_noun_declension"" rel=""nofollow noreferrer"">examples are not directly comparable</a> — 3 and 4 are both <em>Kasi</em>, rather than <em>*Kasy</em> and <em>*Kasie</em> — and male names will be <a href=""http://en.wikibooks.org/wiki/Polish/Masculine_noun_declension"" rel=""nofollow noreferrer"">wholly different again</a>.</p>

<p>I'm guessing someone has dealt with this situation before, but my Google-fu appears to be weak today. I can find a lot of links about natural-language processing, but I don'think that's quite what I want. To be clear: I'm only ever gonna have one user-entered name per user and I'm gonna need to decline them into known configurations — I'll have a localised text that will have placeholders something like <code>{name nominative}</code> and <code>{name dative}</code>, for the sake of argument. I really don't want to have to do lexical analysis of text to work stuff out, I'll only ever need to decline that one user-entered name.</p>

<p>Anyone have any recommendations on how to do this, or do I need to start calling round localisation agencies  ;o)</p>

<hr/>

<p>Further reading (all on Wikipedia) for the interested:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Declension"" rel=""nofollow noreferrer"">Declension</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Grammatical_case"" rel=""nofollow noreferrer"">Grammatical case</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Polish_language#Nouns_and_adjectives"" rel=""nofollow noreferrer"">Declension in Polish</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Russian_grammar#Nouns"" rel=""nofollow noreferrer"">Declension in Russian</a></li>
<li>Declension in Czech <a href=""http://en.wikipedia.org/wiki/Czech_declension#Nouns"" rel=""nofollow noreferrer"">nouns</a> and <a href=""http://en.wikipedia.org/wiki/Czech_declension#Pronouns"" rel=""nofollow noreferrer"">pronouns</a></li>
</ul>

<p>Disclaimer: I know this happens in many other languages; highlighting Slavic languages is merely because I have a project that is going to be localised into some Slavic languages.</p>
","internationalization, nlp, grammar, linguistics",,625,1274290054
How to detect if a event/action occurred from a text?,"<p>I was wondering if there's a NLP/ML technique for this.</p>

<p>Suppose given a set of sentences, </p>

<ol>
<li>I watched the movie.</li>
<li>Heard the movie is great, have to watch it.</li>
<li>Got the tickets for the movie. </li>
<li>I am at the movie.</li>
</ol>

<p>If i have to assign a probability to each of these sentences, that they have ""actually"" watched the movie, i would assign it in decreasing order of 1,4,3,2.</p>

<p>Is there a way to do this automatically, using some classifier or rules? Any paper/link would help.</p>
","nlp, machine-learning, data-mining, information-retrieval","<p>These are common issues in textual entailment. I'll refer you to some papers. While their motivation is for textual entailment, I believe your problem should be easier than that.</p>

<p><a href=""http://pages.cs.brandeis.edu/~roser/pubs/sauri_icsc07.pdf"" rel=""noreferrer"">Determining Modality and Factuality for Textual Entailment</a></p>

<p><a href=""http://nlp.stanford.edu/pubs/rte-naacl06.pdf"" rel=""noreferrer"">Learning to recognize features of valid textual entailments</a></p>

<p>Some of these suggestions should help you decide on some features/keywords to consider when ranking.</p>
",3073,1335024796
Does any functional programming language have significant libraries for natural language processing?,"<p>I am doing natural language processing using NLTK for Python.</p>

<p>I find myself doing my programming more in the functional style, and am thinking about switching to a functional language. However, I need a good NLP library, at least comparable to NLTK.</p>

<p>Do you know a functional programming language which has good NLP libraries? I discovered that there is some NLP activity in the Haskell community, but it seems still in it's infancy compared to NLTK.</p>
","functional-programming, nlp","<p>Scala and Clojure</p>

<p>Both run on the JVM, so you can take advantage of all the java nlp and machine learning libraries (Stanford Parser, Lingpipe, Mallet, etc). Java blows everything except Python away in terms of NLP libraries except python, but is a bit less user friendly. To use the Stanford Parser programmatically can be a bit of the pain in the ass; I had to write my own wrapper for it.</p>
",1249,1335527556
Defining Trending Topics in a specific collection of tweets,"<p>Im doing a Java application where I'll have to determine what are the Trending Topics from a specific collection of tweets, obtained trough the Twitter Search. While searching in the web, I found out that the algorithm defines that a topic is trending, when it has a big number of mentions in a specific time, that is, in the exact moment. So there must be a decay calculation so that the topics change often. However, I have another doubt:</p>

<p>How does twitter determines what specific terms in a tweet should be the TT? For example, I've observed that most TT's are hashtag or proper nouns. Does this make any sense? Or do they analyse all words and determine the frequency? </p>

<p>I hope someone can help me! Thanks!  </p>
","java, twitter, nlp, trending","<p>I don't think anyone knows except Twitter, however it seems hashtags do play a big part, but there are other factors in play.  I think mining the whole text would take more time than needed, and would result in too many false positives.</p>

<p>Here is an interested article from Mashable:</p>

<p><a href=""http://www.sparkmediasolutions.com/pdfs/SMS_Twitter_Trending.pdf"" rel=""nofollow"">http://www.sparkmediasolutions.com/pdfs/SMS_Twitter_Trending.pdf</a></p>

<p>-Ralph Winters</p>
",937,1302285760
Term Document Frequencies in Java with Lucene/Lingpipe in a large corpus,"<p>I am trying to analyze a large corpus of documents, which are in a huge file (3.5GB, 300K lines, 300K documents), one document per line. In this process I am using Lucene for indexing and Lingpipe for preprocessing.</p>

<p>The problem is that I want to get rid of very rare words in the documents. For example, if a word occurs less than MinDF times in the corpus (the huge file), I want to remove it. </p>

<p>I can try to do it with Lucene: Compute the Document Frequencies for all distinct terms, sort them in ascending order, get the terms that have DF lower than MinDF, go over the huge file again, and remove these terms line per line. </p>

<p>This process will be insanely slow. Does anybody know of any quicker way to do this using Java?</p>

<p>Regards</p>
","java, lucene, text-analysis","<p>First create a temp index, then use the information in it to produce the final index. Use <code>IndexReader.terms()</code>, iterate over that, and you have <code>TermEnum.docFreq</code> for each term. Accumulate all low-freq terms and then feed that info into an analyzer that extends <code>StopWordAnalyzerBase</code> when you are creating the final index.</p>
",1180,1335521269
Any better pre processing library or implementation in python?,"<p>I need to pre-process some text documents so that I can apply classification techniques like fcm e.t.c and other topic modeling techniques like latent dirichlet allocation e.t.c</p>

<p>To elaborate a bit in preprocessing I need to remove the stop words, extract the nouns and keywords and perform stemming. The code which I used for this purpose is:</p>

<pre><code>#--------------------------------------------------------------------------
#Extracting nouns
#--------------------------------------------------------------------------
for i in range (0,len(a)) :
    x=a[i]          
    text=nltk.pos_tag(nltk.Text(nltk.word_tokenize(x)))
    for noun in text:
        if(noun[1]==""NN"" or noun[1]==""NNS""):
            temp+=noun[0]
            temp+=' '
documents.append(temp)
print documents

#--------------------------------------------------------------------------
#remove unnecessary words and tags
#--------------------------------------------------------------------------

texts = [[word for word in document.lower().split() if word not in stoplist]for    document in documents]
allTokens = sum(texts, [])
tokensOnce = set(word for word in set(allTokens) if allTokens.count(word)== 0)
texts = [[word for word in text if word not in tokensOnce]for text in texts]
print texts

#--------------------------------------------------------------------------
#Stemming
#--------------------------------------------------------------------------

for i in texts:
    for j in range (0,len(i)):        
        k=porter.stem(i[j])
        i[j]=k
print texts
</code></pre>

<p>The problem with the code I mentioned above is </p>

<ol>
<li>The nltk module used for extracting nouns and keywords is missing many words.
For example the pre-processing was performed on some documents and names like 'Sachin' were not recognized as keywords and missed after pre-processing.</li>
<li>The words are not properly stemmed. There is either too much stemming (network and networking to net) and some times nouns are also stemmed.</li>
</ol>

<p>Is there any better module for the functions needed or is there any better implementation of the same module?
Kindly help  </p>
","python, preprocessor, nlp, data-mining, web-mining",,1878,1335186758
Better approach to filtering Wikipedia edits,"<p>When you are watching for news of particular <a href=""http://en.wikipedia.org/wiki/Kristen_Stewart"" rel=""nofollow"">Wikipedia article</a> via its <a href=""http://en.wikipedia.org/w/index.php?title=Kristen_Stewart&amp;feed=atom&amp;action=history"" rel=""nofollow"">RSS channel</a>,
its annoying without filtering the information, because most of the edits is spam,
vandalism, minor edits etc.</p>

<p>My approach is to create filters. I decided to remove all edits that don't contain a nickname of the contributor but are identified only by the IP address of the contributor, because most of such edits is spam (though there are some good contributions). This was easy to do with regular expressions.
I also removed edits that contained vulgarisms and other typical spam keywords.</p>

<p>Do you know some better approach utilizing algorithms or heuristics with regular expressions, AI, text-processing techniques etc.? The approach should be able to detect bad posts (minor edits or vandalisms) and should be able to incrementally learn what is good/bad contribution and update its database.</p>

<p>thank you</p>
","artificial-intelligence, wikipedia, text-processing, spam-prevention, text-analysis","<p>There are many different approaches you can take here, but traditionally spam filters with incremental learning have been implemented using <a href=""http://bionicspirit.com/blog/2012/02/09/howto-build-naive-bayes-classifier.html"" rel=""nofollow"">Naive bayesian classifiers</a>. Personally, I prefer the even easier to implement Winnow2 algorithm (details can be found in this <a href=""http://www.siefkes.net/ie/winnow-spam.pdf"" rel=""nofollow"">paper</a>).</p>

<p>First you need to extract features from the text you want to classify. Unfortunately the Wikipedia RSS feeds don't seem to be particularly machine readable, so you probably need to do some preprocessing. Alternatively you could directly use the <a href=""http://www.mediawiki.org/wiki/API%3aMain_page"" rel=""nofollow"">Mediawiki API</a> or see if one of the bot frameworks linked at the bottom of <a href=""http://meta.wikimedia.org/wiki/Bot"" rel=""nofollow"">this page</a> is of help to you.</p>

<p>Ideally you would end up with a list of words that were added, words that were removed, various statistics you can compute from that, and the metadata of the edit. I imagine the list of features would look something like this:</p>

<ul>
<li><code>editComment: wordA</code> (wordA appears in edit comment)</li>
<li><code>-wordB</code> (wordB removed from article)</li>
<li><code>+wordC</code> (wordC added to article)</li>
<li><code>numWordsAdded: 17</code></li>
<li><code>numWordsRemoved: 22</code></li>
<li><code>editIsMinor: Yes</code></li>
<li><code>editByAnIP: No</code></li>
<li><code>editorUsername: Foo</code></li>
<li>etc.</li>
</ul>

<p>Anything you think might be helpful in distinguishing good from bad edits. </p>

<p>Once you have extracted your features, it is fairly simple to use them to train the Winnow/Bayesian classifier.</p>
",112,1329221967
Difference between word stemming and depluralization,"<p>In understanding string matching: What is the exact difference between <em>word stemming</em> and <em>depluralization?</em></p>

<p>Or do they mean the same thing?</p>
","nlp, stemming, pluralize","<p>First, <strong>stemming</strong> refers to the process of reducing a word to its stem. However, that may mean a number of different things. Most linguists differentiate between at least two ways of doing it:</p>

<ol>
<li><p>Removing <em>grammatical</em>, but not <em>derivational</em> morphemes. Grammatical morphemes are components of the word that are related to its grammatical role in a particular sentence, e.g. <em>number</em>, <em>case</em>, <em>gender</em>, <em>tense</em>, <em>aspect</em> etc.</p></li>
<li><p>Removing <em>both</em> grammatical and derivational morphemes. Derivational morphemes are components of the word that are related to its derivation from another word, e.g. the ""-er"" in ""worker"" is related to how it is derived (or can be considered as derived) from ""work"".</p></li>
</ol>

<p>Therefore, <strong>depluralization</strong>, which is a rather unusual term, but obviously refers to removing a plural morpheme (such as the ""-s"" at the end of ""computers""), is <strong>part of a kind of stemming</strong>, specifically a part of the removal of grammatical (but not derivational) morphemes.</p>

<p>In <strong>English</strong>, the morphology of <strong>nouns</strong> is largely limited to plural (""computers"") and genitive (second case, ""computer's""), hence as far as English is concerned, depluralization may be seen as (almost) synonymous with (grammatical) stemming, at least to the extent that stemming is applied to <em>nouns</em>, and, to some degree, <em>adjectives</em>, (which it is e.g. in the context of information retrieval). However, wherever <em>verbs</em> are considered, <em>past tense</em>, <em>passive voice</em> and other inflectional forms are subject to stemming (but not to depluralization).</p>

<p>Furthermore, in languages other than English, even nouns may have a very rich morphology, including morphemes for such things as case, politeness level, or special kinds of plural (such as dual). And then, depluralization (if you want to use that term at all) would refer to only a very small part of the overall stemming process.</p>

<p>Another related term is <strong>lemmatization</strong>, which is often used synonymously with <strong>stemming</strong>. One distinction between the two that I found many people (including myself) to make is this:</p>

<ul>
<li><p><strong>Stemming</strong> is used to refer to a <strong>rule-based or machine-learning based</strong> technique that removes parts of a word (mostly endings) that look like grammatical morphemes</p></li>
<li><p><strong>Lemmatization</strong> is used to refer to a process that does the same, but using an actual <strong>dictionary</strong> of the language to deal with highly irregular forms (such as the plural ""women"")</p></li>
</ul>

<p>(But, again, not everyone will agree with this distinction.)</p>
",2687,1335271986
Sentence Classification (Categorization),"<p>I have been reading about text classification and found several Java tools which are available for classification, but I am still wondering: Is text classification the same as sentence classification!</p>

<p>Is there any tool which focuses on sentence classification?</p>
","java, nlp, weka, gate","<p>Theres no formal difference between 'Text classification' and 'Sentence classification'. After all, a sentence is a type of text. But generally, when people talk about text classification, IMHO they mean larger units of text such as an essay, review or speech. Classifying a politician's speech into democrat or republican is a lot easier than classifying a tweet. When you have a lot of text per instance, you don't need to squeeze each training instance for all the information it can give you and get pretty good performance out a bag-of-words naive-bayes model.  </p>

<p>Basically you might not get the required performance numbers if you throw off-the-shelf weka classifiers at a corpora of sentences. You might have to augment the data in the sentence with POS tags, parse trees, word ordering, ngrams, etc. Also get any related metadata such as creation time, creation location, attributes of sentence author, etc. Obviously all of this depends on what exactly are you trying to classify.. the features that will work out for you need to be intuitively meaningful to the problem at hand.</p>
",2225,1334737344
apache openNLP chuker/POS noun detection,"<p>I am experimenting apache Open NLP for one of my project, my requirement is to detect nouns out of email contents and check with our customer data base (this DB consist of individual names, organization names etc and my search engine is Solr base).</p>

<p>For normal english nouns, default trained model works properly (for most of the cases), but 
One of the tricky requirement is, we have business organization with abbreviations like OK, LET etc and thus in few scenarios I need to consider OK, LET etc as noun.</p>

<p>As an example
1) ""sending some items to LET, please expect delay in payment""
2) ""let us go for a party""</p>

<p>In #1 I want to consider LET as noun and in #2 case LET is not noun.</p>

<p>If I can achieve this requirement, I can reduce significant amount of false positive matches in my search engine.</p>

<p>Any help is highly appreciated.</p>
","solr, nlp, opennlp, part-of-speech","<p>Make a <strong>dictionary</strong> of the special nouns and perform dictionary-based extraction as a post-processing step. The dictionary-based extraction should take the distinction between lowercase and uppercase into account, in particular for those entries that are acronyms.</p>

<p>In terms of <strong>implementation of the dictionary lookup:</strong></p>

<ul>
<li><p>As long as the entities in question are single tokens (or consist only of a predefined, small maximum number M of tokens each), implementing the dictionary as <code>HashSet&lt;String&gt;</code>, tokenising the text and making look-ups in the hash for each token (and groups of up to M tokens) should work very well</p></li>
<li><p>If you are dealing with very long entities, or if tokenization is a problem, the use of a search trie or finite state machine implementation of the dictionary is sensible.</p></li>
</ul>

<p>Finally, as always with NLP, you will need to <strong>look at a significant sample of the results</strong> to identify any further problems. Depending on the level of ambiguity in your entity list, you may need to further refine the detection method by adding either a heuristics or a statistical / ML-based decision mechanism on top of the case-sensitive dictionary look-up.</p>
",1268,1335141359
"Proper approach to get words like &quot;dentistry&quot;, &quot;dentist&quot; from query like &quot;dental&quot; (and vice versa)","<p>I'm somewhat familiar with stemming, but the stemming library I've been given to use for a project doesn't work very well for a case where I want to find related words like if I do a query for any of these:</p>

<p>""dental"", ""dentist"", ""dentistry"" </p>

<p>I should get a match for the others. I've been looking into this and I'm learning about parts of speech I didn't even know existed, like pertainyms and troponyms so I'm wondering if there isn't a library out there that has a mapping between all of these different parts of speech that could give back the sort of match I'm looking for?</p>

<p>I've been searching on this and haven't found a whole lot that I can make sense of. I probably don't know the right terminology, etc and I would greatly appreciate if anyone can point me in the right direction.</p>
","search, text, nlp, stemming","<p>One approach common in IR is to stem all words in the index and the query itself. Meaning, documents containing the word 'dentistry' will be stemmed and stored in the index as 'dentist'. The keyword 'dental' is also stemmed as 'dentist' thereby matching  it in the index.</p>
",151,1334952591
Why isn&#39;t Stanford Topic Modeling Toolbox producing lda-output directory?,"<p>I tried to run this <a href=""https://github.com/echen/sarah-palin-lda"" rel=""nofollow"">code from github</a> (following the 1-2-3 steps) which identifies 30 topics in Sarah Palin's 14,500 emails. The topics discovered by the author are <a href=""https://github.com/echen/sarah-palin-lda/blob/master/lda-output/02500/summary.txt"" rel=""nofollow"">here</a>. However, Stanford Topic Modeling Toolbox is not producing lda-output directory for me. It produced the lda-86a58136-30-2b1a90a6, but the summary.txt in this folder only shows the initial assignment of topics, not the final one. Any idea how to produce lda-output directory with the final summary of topics discovered? Thanks in advance!</p>
","nlp, machine-learning, stanford-nlp, text-analysis, lda",,1384,1334779172
OpenNLP Extract Grammar,"<p>I'm currently looking through opennlp source code trying to find/understand the grammar that they use for chunking. This is not one of the easiest tasks. I started looking through the chunkermodel and associated classes but haven't gotten too far..</p>

<p>Has anyone ever searched for this? If so any suggestions or ideas that will put me on path?</p>
","java, nlp, grammar, opennlp",,782,1334809564
Why the 30 topics identified by Stanford Topic Modeling Toolkit so similar to each other?,"<p>What can be the possible reasons why the 30 topics identified by Stanford
Topic Modeling Toolkit (it took ~4 hours) on the corpus of 19,500
articles (shared by Twitter users) so similar to each other? They
have pretty much the same terms, and frequencies => essentially, I
just have a single topic :)</p>

<p>The topics are identified can be found <a href=""http://www.cs.princeton.edu/~asuleime/data/summary.txt"" rel=""nofollow"">here</a></p>

<p>I do standard prep of text docs before learning and inferring stages:
removing stop words, collapsing whitespaces, lowercasing everything,
etc.</p>

<p>Some of my params:</p>

<ul>
<li>numTopics = 30</li>
<li>TermMinimumDocumentCountFilter = (10) ~>  // filter terms which occur in &lt; 10 docs</li>
<li>TermDynamicStopListFilter(30) ~> // filter out 30 most common terms</li>
<li>DocumentMinimumLengthFilter(10) // take only docs with >= 10 terms</li>
<li>topicSmoothing = SymmetricDirichletParams(0.01)</li>
<li>termSmoothing = SymmetricDirichletParams(0.01)</li>
<li>maxIterations = 10</li>
</ul>
","machine-learning, stanford-nlp, text-analysis, lda",,802,1334735854
How to get synonyms ordered by their occurrence probability from Wordnet,"<p>I am searching in Wordnet for synonyms for a big list of words. The way I have it done it, when some word has more than one synonym, the results are returned in alphabetical order. What I need is to have them ordered by their <strong>probability of occurrence</strong>,  and I would take just the top 1 synonym. </p>

<p>I have used the prolog wordnet database and Syns2Index to convert it into Lucene type index for querying synonyms. Is there a way to get them ordered by their probabilities in this way, or I should use another approach?</p>

<p>Speed not important, this synonym lookup will not be done online.</p>
","java, nlp, wordnet",,2080,1278998205
Natural Language parsing of an appointment?,"<p>I'm looking for a Java library to help parse user entered text that represents an 'appointment' for a calendar application.  For instance:</p>

<p>Lunch with Mike at 11:30 on Tuesday</p>

<p>or</p>

<p>5pm Happy hour on Friday </p>

<p>I've found some promising leads like <a href=""https://github.com/samtingleff/jchronic"" rel=""nofollow noreferrer"">https://github.com/samtingleff/jchronic</a> and <a href=""http://www.datejs.com/"" rel=""nofollow noreferrer"">http://www.datejs.com/</a> which can parse dates - but I also need to be able to extract the title of the event like ""Lunch with Mike"".</p>

<p>If such an API doesn't exist, I'm also interested in any thoughts on how best to approach the problem from a coding perspective.</p>
","java, datetime, parsing, nlp",,2728,1270581995
Processing commands with inaccurate natural language strings,"<p>We're designing a system which can accepts commands in this format</p>

<pre><code>command context
</code></pre>

<p>The context is defined from a list of about 200 tuples of words such as:</p>

<pre><code>physical therapy
cardiac
physician visit
hospital inpatient
hospital outpatient
etc.
</code></pre>

<p>We want the system to be able to correct user errors such as spelling mistakes but also to understand that ""physical therapy"" is the same as ""physical therapist"" AND also to accept synonyms</p>

<p>Finally, if it's not an exact match, it should ask the user to disambiguate between the best matches</p>

<p>This is how I'm thinking of doing it:</p>

<ol>
<li>Stem both the context words and incoming queries </li>
<li>Delete/isolate command strings from the query </li>
<li>Check for and correct any anagrams (however: this only covers one category of spelling mistakes) </li>
<li>Look for an exact word match </li>
<li>Look for ""close matches""</li>
</ol>

<p>This doesn't feel like a neat solution, especially steps 3 and 5.</p>

<p>What's a better/easier way to do this? Any libraries to do it in C#, bonus.</p>

<p>Can Lucene do this perhaps? Any guidance appreciated.</p>

<p>Thanks!</p>
","c#, .net, nlp",,111,1334620223
English word database with plurals/conjugations,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/8424806/verb-conjugations-database"">Verb Conjugations Database</a>  </p>
</blockquote>



<p>I'm looking for an English word database in MySQL, or easily convertible to MySQL, that contains verb conjugations and plural/singular forms. I've looked at a couple of options: WordNet, GCIDE, etc.</p>

<p>However GCIDE does not seem to be comprehensive and WordNet does not seem to label conjugations by tense (correct me if I'm wrong).</p>

<p>My problem is similar to this one:
<a href=""https://stackoverflow.com/questions/8424806/verb-conjugations-database/10192400"">Verb Conjugations Database</a></p>

<p>But it seems like no satisfactory, free solution was shared.</p>
","mysql, database, nlp",,1818,1334680026
run a java program with lingpipe classes on a website?,"<p>I am working on a text classification project for a class and I am having some difficulties setting it up correctly. My classification code is in Java and uses methods from the Lingpipe toolkit, but I have to run the program from a website. I've been attempting to put together a servlet for this purpose, and so far have downloaded and set up a container (Tomcat), but I'm finding the process of setting up all the necessary files in the right directories to be complicated. Does anyone out there have any advice as to how to run such a Java program from a website, either using a servlet or not?
Thanks!!!</p>
","java, web-applications, nlp",,393,1303966444
OpenNLP Name Finder,"<p>I am using the <a href=""http://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.namefind"">NameFinder</a> API example doc of OpenNLP. After initializing the Name Finder the documentation uses the following code for the input text:</p>

<pre><code>for (String document[][] : documents) {

  for (String[] sentence : document) {
    Span nameSpans[] = nameFinder.find(sentence);
    // do something with the names
  }

  nameFinder.clearAdaptiveData()
}
</code></pre>

<p>However when I bring this into eclipse the 'documents' (not 'document') variable is giving me an error saying the <em>variable documents cannot be resolved</em>. What is the documentation referring to with the 'documents' array variable? Do I need to initialize an array called 'documents' which hold txt files for this error to go away?</p>

<p>Thank you for your help.</p>
","apache, nlp, data-mining, opennlp","<p>The <a href=""http://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html#tools.namefind.recognition.api"">OpenNLP documentation</a> states that the input text should be segmented into documents, sentences and tokens. The piece of code you provided illustrates how to deal with several documents.</p>

<p>If you have only one document you don't need the first for, just the inner one with the array of sentences, which is composed by as an array of tokens.</p>

<p>To create an array of sentences from a document you can use the OpenNLP SentenceDetector, and for each sentence you can use OpenNLP Tokenizer to get the array of tokens.</p>

<p>Your code will look like this:</p>

<pre><code>// somehow get the contents from the txt file 
//      and populate a string called documentStr

String sentences[] = sentenceDetector.sentDetect(documentStr);
for (String sentence : sentences) {
    String tokens[] = tokenizer.tokenize(sentence);
    Span nameSpans[] = nameFinder.find(tokens);
    // do something with the names
    System.out.println(""Found entity: "" + Arrays.toString(Span.spansToStrings(nameSpans, tokens)));
}
</code></pre>

<p>You can learn how to use the SentenceDetector and the Tokenizer from <a href=""http://opennlp.apache.org/documentation/1.5.2-incubating/manual/opennlp.html"">OpenNLP documentation</a> documentation.</p>
",7426,1334604818
How to neglect the output of OCR Engine that has no meaning?,"<p>Tesseract OCR engine sometimes outputs text that has no meaning, i want to design an algorithm  that neglects any text or word that has no meaning, below is some sort of output text that i want to neglect,my simple solution is to count the words in the recognized text that's separated by "" "" and the text which has too many words will be garbage(Hint: i'm scanning images which at most will contains 40 words) any idea will be helpful,thanks. </p>

<pre><code> wo:&gt;""|axnoA1wvw\
 ldﬂﬁg
 °J!9O‘ !P99W M9N 6 13!-|15!Cl ‘I-/Vl
 978 89l9 Z0 3+ 3 'l9.l.
 97 999 VLL lLOZ+ 3 9l!q°lN
 wo0'|axno/(@|au1e&gt;1e: new;
 1=96r2a1ey\1 1uauud0|e/\e(]
 |8UJB){ p8UJL|\7'
</code></pre>
","algorithm, nlp, ocr, tesseract","<p>Divide the output text into words.  Divide the words into triples.  Count the triple frequencies, and compare to triple frequencies from text of a known-good text corpus (EG all the articles from some mailing list discussing what you intend to OCR, minus the header lines).</p>

<p>When I say ""triples"", I mean:</p>

<p>whe, hen, i, say, tri, rip, ipl, ple, les, i, mea, ean</p>

<p>...so ""i"" has a frequency of 2 in this short example, while the others are all frequency 1.</p>

<p>If you do a frequency count of each of these triples for a large document in your intended language, it should become possible to be reasonably accurate in guessing whether a string is in the same language.</p>

<p>Granted, it's heuristic.</p>

<p>I've used a similar approach for detecting English passwords in a password changing program.  It worked pretty well, though there's no such thing as a perfect ""obvious password rejecter"".</p>
",322,1334602937
Splitting string containing letters and numbers not separated by any particular delimiter in PHP,"<p>Currently I am developing a web application to fetch Twitter stream and trying to create a natural language processing by my own. </p>

<p>Since my data is from Twitter (limited by 140 characters) there are many words shortened, or on this case, <strong>omitted space</strong>.</p>

<p>For example:</p>

<pre><code>""Hi, my name is Bob. I m 19yo and 170cm tall""
</code></pre>

<p>Should be tokenized to:</p>

<pre><code>- hi
- my
- name
- bob
- i
- 19
- yo
- 170
- cm
- tall
</code></pre>

<p>Notice that <code>19</code> and <code>yo</code> in <code>19yo</code> have <strong>no space</strong> between them. I use it mostly for extracting numbers with their units. </p>

<p>Simply, what I need is a way to 'explode' each tokens that has number in it by chunk of numbers or letters <strong>without</strong> delimiter. </p>

<p><code>'123abc'</code> will be <code>['123', 'abc']</code></p>

<p><code>'abc123'</code> will be <code>['abc', '123']</code></p>

<p><code>'abc123xyz'</code> will be <code>['abc', '123', 'xyz']</code></p>

<p>and so on.</p>

<p>What is the best way to achieve it in PHP?</p>

<hr>

<p>I found something close to it, but it's C# and spesifically for day/month splitting. <a href=""https://stackoverflow.com/q/2362153/670623"">How do I split a string in C# based on letters and numbers</a></p>
","php, regex, string, algorithm, nlp","<p>You can use <a href=""http://de3.php.net/preg-split"" rel=""noreferrer"">preg_split</a></p>

<pre><code>$string = ""Hi, my name is Bob. I m 19yo and 170cm tall"";
$parts = preg_split(""/(,?\s+)|((?&lt;=[a-z])(?=\d))|((?&lt;=\d)(?=[a-z]))/i"", $string);
var_dump ($parts);
</code></pre>

<p>When matching against the digit-letter boundary, the regular expression match must be zero-width. The characters themselves must not be included in the match. For this the <a href=""http://www.regular-expressions.info/lookaround.html"" rel=""noreferrer"">zero-width lookarounds</a> are useful.</p>

<p><a href=""http://codepad.org/i4Y6r6VS"" rel=""noreferrer"">http://codepad.org/i4Y6r6VS</a></p>
",2520,1334606140
How to conjugate English words in Java?,"<p>Say I have a base form of a word and a tag from the <a href=""http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench/CQP-HTMLDemo/PennTreebankTS.html"" rel=""nofollow noreferrer"">Penn Treebank Tag Set</a>. How can I get the conjugated form? For example for ""do"" and ""VBN"" how can I get ""done""?</p>

<p>I thinks this task is already implemented in some nlp library, so I'd rather not invent the bicycle. Does something like that exist?</p>
","java, nlp, linguistics","<p>If you have a class:</p>

<pre><code>public Treebank {
    public String conjugate(String base, String formTag);

    ...
}
</code></pre>

<p>Then:</p>

<pre><code>String conjugated = treebank.conjugate(base, formTag);
</code></pre>

<p>If you don't have the Treebank class it might look a bit like this:</p>

<pre><code>public Treebank {
    private Map&lt;String, Map&lt;String, String&gt;&gt; m_map = new HashMap&lt;String, Map&lt;String, String&gt;&gt;();

    public Treebank() {
        populate();
    }

    public String conjugate(String base, String formTag) {
        return m_map.get(base, formTag);
    }

    private void populate() {
        InputStream istream = openDataFile();

        try {
            for (Record record = readRecord(istream); record !== null; record = readRecord(istream)) {

                // Add the entry
                Map&lt;String, String&gt; entry = m_map.get(record.base);

                if (entry == null)
                    entry = new HashMap&lt;String, String&gt;();

                entry.put(record.formTag, record.conjugatedForm);
                m_map.put(record.base, entry);
           }
        }
        finally {
            closeDataFile(istream);
        }
    }

    // Data management - to be implemented.
    private InputStream openDataFile()                     { ... }
    private Record      readRecord(InputStream istream)    { ... }
    private void        closeDataFile(InputStream istream) { ... }

    private static class Record {
        String base;
        String formTag;
        String conjugatedForm;
    }
}
</code></pre>

<p>A better solution might involve a database instead of a data file. I would also refactor the data access code into a Data Access Object.</p>
",1057,1270886861
Extract Articles&#39; text from Wikipeda,"<p>I'm writing some java code in order to get the raw text of some Wikipedia articles (Giving a jList of words, search them in wikipedia and extract the first sentence of the corresponding article). My GUI contains a button for which I defined the following action listener:</p>

<pre><code>private void loadButtonActionPerformed(java.awt.event.ActionEvent evt) {                                           

final DefaultListModel conceptsListFilesModel = new DefaultListModel();

conceptsList.setModel(conceptsListFilesModel);

final List definitionWiki = new ArrayList();        

//Remplir la list avec la première collone de la liste
final Thread updater = new Thread(){
@Override public void run() {        
for(int i=0; i&lt; 20 /*dataTable.getRowCount()*/ ; i++) {
conceptsListFilesModel.addElement(dataTable.getValueAt(i, 0));

try {
Object concept = conceptsListFilesModel.elementAt(i);
WikipediaParser parser = new WikipediaParser(""en"");
System.out.println(concept+"""");
String firstParagraph = parser.fetchFirstParagraph(concept+"""");
int point = firstParagraph.indexOf(""."");
String firstsentence = firstParagraph.substring(0, point+1);
definitionWiki.add(i, firstsentence) ;
} catch (IOException ex) {
Logger.getLogger(Tex2TaxView.class.getName()).log(Level.SEVERE, null, ex);
}

try { Thread.sleep(1000);
} catch (InterruptedException e) {throw new RuntimeException(e) ;}
}
JOptionPane.showMessageDialog(null, ""Successful loading !"")  ;
}
};
updater.start(); 
} 
</code></pre>

<p>The WikipediaParser class:</p>

<pre><code>public class WikipediaParser {

private final String baseUrl; 

public WikipediaParser(String lang) {
this.baseUrl = String.format(""http://%s.wikipedia.org/wiki/"", lang);
}

public String fetchFirstParagraph(String article) throws IOException {
String url = baseUrl + article;
Document doc = Jsoup.connect(url).get();
Elements paragraphs = doc.select("".mw-content-ltr p"");
Element firstParagraph = paragraphs.first();
return firstParagraph.text();
}

}
</code></pre>

<p>The execution generates the following list of exceptions:</p>

<pre><code>nov. 30, 2011 12:42:55 AM tex2tax.Tex2TaxView$11 run
Grave: null java.net.SocketTimeoutException: Read timed out
at java.net.SocketInputStream.socketRead0(Native Method)
at java.net.SocketInputStream.read(SocketInputStream.java:150)
at java.net.SocketInputStream.read(SocketInputStream.java:121)

at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:641)
at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:589)
at  
sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1319)
at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:381)
at org.jsoup.helper.HttpConnection$Response.execute(HttpConnection.java:364)
at org.jsoup.helper.HttpConnection.execute(HttpConnection.java:143)
at org.jsoup.helper.HttpConnection.get(HttpConnection.java:132)
at tex2tax.WikipediaParser.fetchFirstParagraph(WikipediaParser.java:25)
at tex2tax.Tex2TaxView$11.run(Tex2TaxView.java:595)
</code></pre>

<p>Need help to solve this problem                     </p>
","java, nlp, jsoup, wikipedia",,1201,1322610786
"Parsing words into (prefix, root, suffix) in Python","<p>I'm trying to create a simple parser for some text data. (The text is in a language that NLTK doesn't have any parsers for.) </p>

<p>Basically, I have a limited number of prefixes, which can be either one or two letters; a word can have more than one prefix. I also have a limited number of suffixes of one or two letters. Whatever's in between them should be the ""root"" of the word. Many words will have more the one possible parsing, so I want to input a word and get back a list of possible parses in the form of a tuple (prefix,root,suffix).</p>

<p>I can't figure out how to structure the code though. I pasted an example of one way I tried (using some dummy English data to make it more understandable), but it's clearly not right. For one thing it's really ugly and redundant, so I'm sure there's a better way to do it. For another, it doesn't work with words that have more than one prefix or suffix, or both prefix(es) and suffix(es).</p>

<p>Any thoughts?</p>

<pre><code>prefixes = ['de','con']
suffixes = ['er','s']

def parser(word):
    poss_parses = []
    if word[0:2] in prefixes:
        poss_parses.append((word[0:2],word[2:],''))
    if word[0:3] in prefixes:
        poss_parses.append((word[0:3],word[3:],''))
    if word[-2:-1] in prefixes:
        poss_parses.append(('',word[:-2],word[-2:-1]))
    if word[-3:-1] in prefixes:
        poss_parses.append(('',word[:-3],word[-3:-1]))
    if word[0:2] in prefixes and word[-2:-1] in suffixes and len(word[2:-2])&gt;2:
        poss_parses.append((word[0:2],word[2:-2],word[-2:-1]))
    if word[0:2] in prefixes and word[-3:-1] in suffixes and len(word[2:-3])&gt;2:
        poss_parses.append((word[0:2],word[2:-2],word[-3:-1]))
    if word[0:3] in prefixes and word[-2:-1] in suffixes and len(word[3:-2])&gt;2:
        poss_parses.append((word[0:2],word[2:-2],word[-2:-1]))
    if word[0:3] in prefixes and word[-3:-1] in suffixes and len(word[3:-3])&gt;2:
        poss_parses.append((word[0:3],word[3:-2],word[-3:-1]))
    return poss_parses



&gt;&gt;&gt; wordlist = ['construct','destructer','constructs','deconstructs']
&gt;&gt;&gt; for w in wordlist:
...   parses = parser(w)
...   print w
...   for p in parses:
...     print p
... 
construct
('con', 'struct', '')
destructer
('de', 'structer', '')
constructs
('con', 'structs', '')
deconstructs
('de', 'constructs', '')
</code></pre>
","python, parsing, nlp","<p>Pyparsing wraps the string indexing and token extracting into its own parsing framework, and allows you to use simple arithmetic syntax to build up your parsing definitions:</p>

<pre><code>wordlist = ['construct','destructer','constructs','deconstructs']

from pyparsing import StringEnd, oneOf, FollowedBy, Optional, ZeroOrMore, SkipTo

endOfString = StringEnd()
prefix = oneOf(""de con"")
suffix = oneOf(""er s"") + FollowedBy(endOfString)

word = (ZeroOrMore(prefix)(""prefixes"") + 
        SkipTo(suffix | endOfString)(""root"") + 
        Optional(suffix)(""suffix""))

for wd in wordlist:
    print wd
    res = word.parseString(wd)
    print res.dump()
    print res.prefixes
    print res.root
    print res.suffix
    print
</code></pre>

<p>The results are returned in a rich object called ParseResults, which can be accessed as a simple list, as an object with named attributes, or as a dict.  The output from this program is:</p>

<pre><code>construct
['con', 'struct']
- prefixes: ['con']
- root: struct
['con']
struct


destructer
['de', 'struct', 'er']
- prefixes: ['de']
- root: struct
- suffix: ['er']
['de']
struct
['er']

constructs
['con', 'struct', 's']
- prefixes: ['con']
- root: struct
- suffix: ['s']
['con']
struct
['s']

deconstructs
['de', 'con', 'struct', 's']
- prefixes: ['de', 'con']
- root: struct
- suffix: ['s']
['de', 'con']
struct
['s']
</code></pre>
",8647,1334430194
Python natural language processing for named entities,"<p>I am writing a python web application for which I need to process search queries having named entities in it. For example,
If search query is:
""mac os lion""
And lets say I have to process this query with the candidates available on my database:</p>

<ul>
<li>Google Android.</li>
<li>Microsoft Windows.</li>
<li>Apple Mac OS X Lion</li>
<li>...</li>
</ul>

<p>We all know that 3rd result is the right result. But is there any way we could map user's query i.e. ""mac os lion"" to ""Apple Mac OS X Lion"" (which is the available entry on my database)
Can someone please tell me what to look for or what to do.</p>
","python, nlp","<p>You need some sort of normalisation of user queries and have to ""learn"" a mapping from these to the correct ""classes"".</p>

<p>A simple way would be computing the overlap of ""tokens"" matching any of your ""classes"". The following sample code may help:</p>

<pre><code>CLASSES = ['Google Android', 'Microsoft Windows', 'Apple Mac OS X Lion']

def classify_query(query_string):
    """"""
    Computes the most ""likely"" class for the given query string.

    First normalises the query to lower case, then computes the number of
    overlapping tokens for each of the possible classes.

    The class(es) with the highest overlap are returned as a list.

    """"""
    query_tokens = query_string.lower().split()
    class_tokens = [[x.lower() for x in c.split()] for c in CLASSES]

    overlap = [0] * len(CLASSES)
    for token in query_tokens:
        for index in range(len(CLASSES)):
            if token in class_tokens[index]:
                overlap[index] += 1

    sorted_overlap = [(count, index) for index, count in enumerate(overlap)]
    sorted_overlap.sort()
    sorted_overlap.reverse()

    best_count = sorted_overlap[0][0]

    best_classes = []
    for count, index in sorted_overlap:
        if count == best_count:
            best_classes.append(CLASSES[index])
        else:
            break

    return best_classes
</code></pre>

<p>Example output</p>

<pre><code>classify_query('mac OS x') -&gt; ['Apple Mac OS X Lion']
classify_query('Google') -&gt; ['Google Android']
</code></pre>

<p>Of course, this is only a very basic solution. You might want to add some spell checking to be more robust in case of typos in query strings...</p>

<p>Hope that helps :)</p>
",323,1334297531
"n-gram name analysis in non-english languages (CJK, etc)","<p>I'm working on deduping a database of people. For a first pass, I'm following a basic 2-step process to avoid an O(n^2) operation over the whole database, as described <a href=""http://nike.psu.edu/publications/jcdl05.pdf"" rel=""nofollow noreferrer"">in the literature</a>. First, I ""block""- iterate over the whole dataset, and bin each record based on n-grams AND initials present in the name. Second, all the records per bin are compared using Jaro-Winkler to get a measure of the likelihood of their representing the same person.</p>

<p>My problem- the names are Unicode. Some (though not many) of these names are in CJK (Chinese-Japanese-Korean) languages. I have no idea how to <a href=""https://stackoverflow.com/questions/1738788/python-split-unicode-string-on-word-boundaries"">find word boundaries</a> for something like initials in these languages. I have no idea whether n-gram analysis is valid on names in languages where names can be 2 characters. I also don't know if string edit-distance or other similarity metrics are valid in this context.</p>

<p>Any ideas from linguist programmers or native speakers?</p>
","python, nlp, similarity, n-gram, cjk",,2196,1333654498
"Extracting text from webpage, processing with Perl/Python, then rebuilding the page with links added","<p>I'm building a web app that processes the text in a web page, adds links to certain entities, then re-displays the page exactly as it was, but with some links added. My server-side code is in Perl and Python, and I'm currently using HTML::Parser to extract the text from a page. I can clean the markup, extract, and process the text without issue, but I want to display the original page exactly as it was, only with some links added to previously unlinked text. </p>

<p>I'm hoping to find out the best way to redisplay the exact same page with links added to certain words or phrases in the text. All of the original markup should be preserved exactly as it was before the text was extracted. </p>

<p>I've searched thoroughly, but I cannot find a precise solution to this issue. Any help would be greatly appreciated.  </p>
","python, perl, nlp, html-parsing","<p>I do know that Python has a module for opening webpages, called urllib:</p>

<pre><code>import urllib
url = 'https://www.google.com/'
page = urllib.urlopen(url)
print page.read()    
#page.read is the url's source code, so you would print the source  code here. 
</code></pre>

<p>you could also save a new html file with python like this:</p>

<pre><code>page = page.read()
file = open('url.html', 'w')
file.writelines(page)
file.close()
</code></pre>

<p>In between you could modify the html source. Keep in mind that the webpages will look silly if you don't figure out how to save the files the pages are using. Hope this helps.</p>
",634,1334248906
Are there any recipes to get rid of text ouput of binary files in between human content?,"<p>In grabbing content from web pages (within <code>p</code> tags) occasionally between human text I see long blobs of stuff like:</p>

<pre><code>ªjßûÞà&amp;fnof;^_^\,_&gt;xî¹Ág?;´Â¼ú8&amp;#x17E;^S^R +^R ^A^R ^A^R ^A^R ^A&amp;rsaquo;^DÆ^Z^_&amp;#x17D;m&amp;rdquo;A&amp;þÅ^R^N&amp;OElig;^RÝmîÜaÎ^W¿¸;^Oéµk^G&amp;Yuml;ÿü`ß}^G^G^\0¸ì²  &amp;Yuml;b1oÞà#^_^Y&amp;#x17D;EO9eâ?&amp;#x17D;&lt;ÅFw#^R     9æè^Q^AZß&amp;ª¿lÙ`×®Ý¡wî^\&amp;dagger;Â* &amp;circ;&amp;Scaron;«MB^BtE^B$@^B$@^B$@^B$`&amp;ldquo;&amp;euro;&amp;#x17D;ýüÄ¬Y»Çª&amp;tilde;òÅ^XrÉ&amp;rsquo;Á;Þ1Ì¹þúÝ&amp;ndash;[¶^LÞ÷¾Áyç^M×o`Åõ&amp;lsquo;G^N°ôzÊ&amp;rdquo;¡?N^W^Ow^F&amp;fnof;iÓ^F0&gt;ä?azæL?Ð0wàg^V&amp;#x17D;Z^S&amp;rsquo;^\^Y·µê?öØ°úc¾O&amp;scaron;?^@]&amp;lsquo;^@  ?^@     ?^@     ?^@     Ø$0r¤&amp;Scaron;¡òA^G^MÇ?7Þ¸Ûæ&amp;tilde;c&amp;dagger;»:¤\¿~ðþ÷^O^^?}pÂ    Ã|^YÏc¬&amp;#x17D;íS&amp;Yuml;Ú]d¸3Ø={&amp;OElig;!:¶C^Oõ#^Nsí^MÔ[«&gt;Ö«^?à^C&amp;fnof;íÛ},m=ªØì&amp;ndash;TE^B$@^B$@^B$@^B$P4&gt;¼æ&amp;scaron;á^PúÓ&amp;Yuml;Þmó?ï^Lw±&amp;circ;EÆ?'?48ç&amp;oelig;a^ZÃuo{÷»wÛH¾ØoØ0ÜÛ^?ÿÝ&amp;Dagger;t êÚhfa""a«ùbÜ¸-T^_Óéï|çàþû&amp;lsaquo;d¸&amp;rsquo;ö¤^S^R +^R ^A^R ^A^R ^A^R  &amp;dagger;        `R:ü¯ywY&amp;trade;Q_½:ÇÕÐz8J^\þ&amp;Dagge
</code></pre>

<p>and </p>

<pre><code>  `i=æºeÙpÁ&amp;mdash;UêsÀÂo^LÑñþ^EÞyÁK    ø+¨X²^ß-=?^@    ?^@     ?^@     ?^@     ?À¤$&amp;euro;Ñ2^[Æ\:^õ?&amp;rdquo;^Ud¥H&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;^DH&amp;euro;
</code></pre>

<p>My only familiarity with them is when I accidently open some binary files in a text editor. These are long sequences and are polluting my sample. Are there any techniques to get rid of them?</p>

<p><strong>PS:</strong> What are they? For eg, some of them are accompanied by </p>

<pre><code>endstream
endobj
37 0 obj
817
endobj
38 0 obj
&lt;&lt; /Length 39 0 R /N 3 /Alternate /DeviceRGB /Filter /FlateDecode &gt;&gt;
stream
</code></pre>
","text, full-text-search, nlp, web-crawler",,44,1334217607
Word partitions with maximum weight,"<p>I'm working on a game where I need to find the biggest weight for a specific sentence.</p>

<p>Suppose I have the sentence ""the quick brown fox"" and assume both single and double words with their defined weight: ""the"" -> 10, ""quick"" -> 5, ""brown"" -> 3, ""fox"" -> 8, ""the quick"" -> 5, ""quick brown"" -> 10, ""brown fox"" -> 1</p>

<p>I'd like to know which combination of single and double words provides the biggest weight, in this case it would be ""the"", ""quick brown"", ""fox"" (weight = 28)</p>

<p>I've been told this problem can be solved through linear programming, but I fail to see how to implement such method. Specifically, I don't know how to express the constraints of the problem, in this case the fact that some double words can't be combined with a single word which contains (ie. ""the quick"" can't be combined with either ""the"" or ""quick"")</p>

<p>May someone provide some guidance as to how approach this problem? I'm not an expert in the area and have some basic understanding of how Simplex works (back from school), but I lack the knowledge about how to model this kind of problem.</p>

<p>Also, any other approach (not involving linear programming nor brute force) would be welcomed too.</p>

<p>Thanks.</p>
","algorithm, nlp, dynamic-programming, linear-programming",,200,1334160962
Searching through an entire series of synsets (from NLTK) in Python NLP,"<p><em>Original question:</em></p>

<p><em>The polysemy of a word is the number of senses it has. Using WordNet, we can
determine that the noun dog has seven senses with len(wn.synsets('dog', 'n')).
Compute the average polysemy of nouns, verbs, adjectives, and adverbs according
to WordNet.</em></p>

<p>From what I gathered from the question, I was to use a built in polysemy method in NLTK with WordNet to find out the number of senses something has.</p>

<p>More importantly, I am trying to use all of the available synsets available and loop them through,
and placing all of the values returned into a set.
After this, I would have intended to add the total number of all the synsets in the set(filled with integers) that was newly created to receive a sum.
After I obtain this sum, I would have divided the total number of entries to receive the average.</p>

<p>My biggest question is... how would I go about looping through <strong>all</strong> of the synsets available as well as the nouns, verbs, adjectives, and adverbs?</p>

<ul>
<li>After researching through various websites, I've found that for ""wn.all_synsets('n')"" will return all of the possible noun synsets.</li>
</ul>

<p>However the type it returns is a ""generator"", how would I go about using loops to iterate through a ""generator"" type?</p>

<p>.>>>allsynsets = wn.all_synsets('n')</p>

<p>.>>> allsynsets</p>

<p>&lt;.generator object all_synsets at 0x04359F30></p>

<p>.>>> type(allsynsets)</p>

<p>&lt;.type 'generator'></p>

<p>I hope I've provided enough information to allow anyone to find an answer to this problem.
I do not have source code, aside from imports and tests to understand the generator type for this problem.</p>

<p>Thank you for your time.</p>
","python, nlp, nltk, wordnet","<p>either of these methods should work</p>

<pre><code>list = [x for x in wx.all_synsets('n')]
</code></pre>

<p>or</p>

<pre><code>for x in wx.all_synsets('n'):
    print x
</code></pre>

<p>[edit] this talks more about generators (among many other sources on the web)
<a href=""http://www.dalkescientific.com/writings/NBN/generators.html"" rel=""nofollow"">http://www.dalkescientific.com/writings/NBN/generators.html</a></p>
",1620,1334123281
Can OpenNLP use HTML tags as part of the training?,"<p>I'm creating a training set for the TokenNameFinder using html documents converted into plain text, but my precision is low and I want to use the HTML tags as part of the training. Like words in bold, and sentences in differents margin sizes. 
Will OpenNLP accept and use those tags to create rules?
Is there another way to make use of those tags to improve precision?</p>
","html, nlp, pattern-matching, named-entity-recognition, opennlp","<p>It is not clear what you mean with using HTML tags to train OpenNLP.
The train input is an annotated tokenized sentence:</p>

<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of &lt;START:company&gt; Elsevier N.V. &lt;END&gt; , the Dutch publishing group .
</code></pre>

<p>To train an OpenNLP model using the standard tooling you need annotations follows this convention. Note that the annotations does not follow the XML standard.</p>

<p>You can embed annotations directly to the HTML documents you will use for training. It might even help the classifier with the extra context, but I've never read any experimental results about it.</p>

<p>You should keep in mind that the training data should be tokenized. It means that you should include white spaces between words and punctuation, as well as between text elements and html:</p>

<pre><code>&lt;p&gt; &lt;i&gt; Mr . &lt;START:person&gt; Vinken &lt;END&gt; &lt;/i&gt; is chairman of &lt;b&gt; &lt;START:company&gt; Elsevier N.V. &lt;END&gt; &lt;/b&gt;, the Dutch publishing group .
</code></pre>
",514,1334079541
NLTK Chunking Error,"<p>I'm trying to implement a sentence chunker based on a Maxent Classifier, as described in the NLTK book (Example 7.9):</p>

<p><a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html#code-classifier-chunker"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html#code-classifier-chunker</a></p>

<p>When I try to evaluate the chunker with</p>

<pre><code>chunker = ConsecutiveNPChunker(train_sents)
print chunker.evaluate(test_sents)
</code></pre>

<p>or to chunk a sentence with</p>

<pre><code>print chunker.parse(test_sents[1])
</code></pre>

<p>I receive the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#13&gt;"", line 1, in &lt;module&gt;
    print chunker.parse(test_sents[1])
  File ""/usr/local/lib/python2.6/dist-packages/nltk/chunk/api.py"", line 34, in parse
    assert 0, ""ChunkParserI is an abstract interface""
AssertionError: ChunkParserI is an abstract interface
</code></pre>

<p>I didn't find anything on google and I'm stuck on this point. Any help would be really useful!</p>
","python, nlp, nltk, chunking","<p>You evidently haven't implemented your chunker yet. ""ChunkParserI is an abstract interface"" means that you need to derive a class from it and define your own <code>parse()</code> method. The NLTK chapter you link to shows how to define an example class, <code>ConsecutiveNPChunker</code>.</p>

<p>The final step will be to create an instance of your new class and call its <code>eval()</code> method (which it inherits from <code>ChunkParserI</code>, so you don't to provide a replacement).</p>
",705,1333913074
Render linguistic syntax tree in browser,"<p>The input is <strong>either:</strong></p>

<p>(1) a bracketed representation of a tree with labeled internal nodes such as:</p>

<pre><code>(S (N John) (VP (V hit) (NP (D the) (N ball))))
</code></pre>

<p>with output:</p>

<p><img src=""https://i.sstatic.net/ZD8S2.jpg"" alt=""enter image description here""></p>

<p>(Whether the lines are dashed and whether the caption is present are not significant.)</p>

<p><strong>Or the input could be:</strong></p>

<p>(2) a bracketing on words without labels e.g.:</p>

<pre><code>((John) ((hit) ((the) (ball))))
</code></pre>

<p>with output same as above (no internal labels this time, just the tree structure).</p>

<p>Another component of the input is whether the tree is labeled as in (1) or unlabeled as in (2).</p>

<hr>

<p><strong>My question:</strong> What is the best way (fastest development time) to render these trees in the browser in javascript? Everything should happen on the client side.</p>

<p>I'm imagining a simple interface with just a textbox (and a radio button specifying whether it is a labeled tree or not), that, when changed, triggers a tree to render (if the input does not have any syntax errors). </p>
","javascript, rendering, nlp, linguistics","<p>Actually there is server side library to convert bracketed representation into tree,  <a href=""http://ironcreek.net/phpsyntaxtree"" rel=""nofollow"">phpsyntaxtree</a>.<br>
You can reimplement them using javascript graphic library like <a href=""http://jsdraw2d.jsfiction.com"" rel=""nofollow"">jsdraw2d</a> or roll you on javascript using HTML5 canvas.</p>

<p><a href=""http://javascript.open-libraries.com/utilities/drawing/10-best-javascript-drawing-and-canvas-libraries/"" rel=""nofollow"">10 Best Javascript Drawing and Canvas Libraries</a></p>
",1228,1330914808
chunks using python,"<p>I'm trying to extract all the proper nouns from a tagged paragraph. What I did in my code is that first I've extracted the paragraph separately and then I have checked whether there is any proper noun in it. But the problem is, I haven't been able to extract the proper noun. My code doesn't even go inside the loop where it checks for a specific tag.</p>

<p>My code:</p>

<pre><code>def noun(sen):
m=[]
if (sen.split('/')[1].lower().startswith('np')&amp;sen.split('/')[1].lower().endswith('np')):
         w=sen.strip().split('/')[0]
         m.append(w)
return m


import nltk
rp = open(""tesu.txt"", 'r')
text = rp.read()
list = []
sentences = splitParagraph(text)
for s in sentences:
 list.append(s)
</code></pre>

<p>Sample input from 'tesu.txt'</p>

<pre><code>Several/ap defendants/nns in/in the/at Summerdale/np police/nn burglary/nn trial/nn      made/vbd statements/nns indicating/vbg their/pp$ guilt/nn at/in the/at.... 

Bellows/np made/vbd the/at disclosure/nn when/wrb he/pps asked/vbd Judge/nn-tl Parsons/np to/to grant/vb his/pp$ client/nn ,/, Alan/np Clements/np ,/, 30/cd ,/, a/at separate/jj trial/nn ./.
</code></pre>

<p>How can I extract all the tagged proper nouns from a paragraph?</p>
","python, nlp","<p>Thanks for the data sample.</p>

<p>You need to:</p>

<ul>
<li>read each paragraph/line</li>
<li>split the line by whitespace to extract each tagged word, e.g. <code>Summerdale/np</code></li>
<li>split the word by <code>/</code> to see if it is tagged <code>np</code></li>
<li>if so, add the other half of the split (the actual word) to your noun list</li>
</ul>

<p>So something like the following (based on <em>Bogdan</em>'s answer, thanks!)</p>

<pre><code>def noun(word):
    nouns = []
    for word in sentence.split():
      word, tag = word.split('/')
      if (tag.lower() == 'np'):
        nouns.append(word);
    return nouns

if __name__ == '__main__':
    nouns = []
    with open('tesu.txt', 'r') as file_p:
         for sentence in file_p.read().split('\n\n'): 
              result = noun(sentence)
              if result:
                   nouns.extend(result)
    print nouns
</code></pre>

<p>which for your example data, produces:</p>

<pre><code>['Summerdale', 'Bellows', 'Parsons', 'Alan', 'Clements']
</code></pre>

<p><strong>Update</strong>: In fact, you can shorten the whole thing down to this:</p>

<pre><code>nouns = []
with open('tesu.txt', 'r') as file_p:
  for word in file_p.read().split(): 
    word, tag = word.split('/')
    if (tag.lower() == 'np'):
      nouns.append(word)
print nouns
</code></pre>

<p>if you don't care which paragraph the nouns come from.</p>

<p>You could also get rid of the <code>.lower()</code> if the tags are always lowercase as they are in your example.</p>
",451,1329984855
Crawling all wikipedia pages for phrases in python,"<p>I need to design a program that finds certain four or five word phrases across the entire wikipedia collection of articles (yes, I know it's lot of pages, and I don't need answers calling me an idiot for doing this).</p>

<p>I haven't programmed much stuff like this before, so there are two issues that I would greatly appreciate some help with:</p>

<ul>
<li><p>First, how I would be able to get the program to crawl through all of the pages (i.e NOT hardcoding each one of the millions of pages. I have downloaded all the articles onto my hard drive, but I'm not sure how I can tell the program to iterate through each one in the folder) 
<strong>EDIT</strong> - I have all the wikipedia articles on my hard drive</p></li>
<li><p>The snapshots of the pages have pictures and tables in them. How would I extract solely the main text of the article?</p></li>
</ul>

<p>Your help on either of the issues is greatly appreciated!</p>
","python, nlp, wikipedia",,2762,1333686404
Creating a corpus from data in a custom format,"<p>I have hundreds of files containing text I want to use with NLTK. Here is one such file:</p>

<pre>
বে,বচা ইয়াণ্ঠা,র্চা ঢার্বিত তোখাটহ নতুন, অ প্রবঃাশিত।
তবে ' এ বং মুশায়েরা ' পত্রিব্যায় প্রকাশিত তিনটি লেখাই বইযে
সংব্যজান ব্যরার জনা বিশেষভাবে পরিবর্ধিত। পাচ দাপনিকেব
ড:বন নিয়ে এই বই তৈরি বাবার পরিব্যল্পনাও ম্ভ্রাসুনতন
সামন্তেরই। তার আর তার সহকারীদেব নিষ্ঠা ছাডা অল্প সময়ে
এই বই প্রব্যাশিত হতে পারত না।,তাঁদের সকলকে আমাধ
নমস্কার জানাই।
বতাব্যাতা শ্রাবন্তা জ্জাণ্ণিক
জানুয়ারি ২ ণ্ট ণ্ট ৮ 
Total characters: 378
</pre>

<p>Note that each line does <b>not</b> contain a new sentence. Rather, the sentence terminator - the equivalent of the period in English - is the '।' symbol.</p>

<p>Could someone please help me create my corpus? If imported into a variable MyData, I would need to access MyData.words() and MyData.sents(). Also, the last line should not appear in the corpus (it merely contains a character count).</p>

<p>Please note that I will need to run operations on data from <b>all</b> the files at once.</p>

<p>Thanks in advance!</p>
","python, nlp, nltk","<p>You don't need to input the files yourself or to provide <code>words</code> and <code>sents</code> methods. 
Read in your corpus with <code>PlaintextCorpusReader</code>, and it will provide those for you.
The corpus reader constructor accepts arguments for the path and filename pattern of the files, and for the input encoding (be sure to specify it). </p>

<p>The constructor also has optional arguments for the sentence and word tokenization functions, so you can pass it your own method to break up the text into sentences. If word and sentence detection is really simple, i.e., if the | character has other uses, you can configure a tokenization function from the nltk's RegexpTokenizer family, or you can write your own from scratch. (Before you write your own, study the docs and code or write a stub to find out what kind of input it's called with.)</p>

<p>If recognizing sentence boundaries is non-trivial, you can later figure out how to train the nltk's PunktSentenceTokenizer, which uses an unsupervized statistical algorithm to learn which uses of the sentence terminator actually end a sentence.</p>

<p>If the configuration of your corpus reader is fairly complex, you may find it useful to create a class that specializes <code>PlaintextCorpusReader</code>. But much of the time that's not necessary. Take a look at the NLTK code to see how the gutenberg corpus is implemented: It's just a <code>PlainTextCorpusReader</code> instance with appropriate arguments for the constructor.</p>
",302,1333523464
Shorten string to just a product name,"<p>I need some help. I am trying to shorten a full product name from Amazon. For example:</p>

<p>Samsung Galaxy S II Epic Touch 4G Android Phone (Sprint)</p>

<p>Galaxy S II</p>

<p>I want to do this programatically. Any ideas?</p>
","php, nlp, linguistics","<p>@bdares points out what seems to be the easiest way to handle this -- make a big list of all of the product names, then use some sort of text search to find the product name your string contains.</p>

<p>I asked <a href=""https://stackoverflow.com/questions/10022062/given-a-list-of-search-terms-how-can-i-tell-which-ones-my-string-contains"">Given a list of search terms, how can I tell which ones my string contains?</a> since I can't think of a good way to do this at the moment.</p>

<p>EDIT: And I got a <a href=""https://stackoverflow.com/a/10056567/212555"">pretty good answer</a>.</p>
",204,1333593307
Organisation Details Recognition,"<p>I am trying to write a company's details parser that can split text like the following into it's constituent parts:</p>
<blockquote>
<p>THALES LAND AND JOINT SYSTEMS</p>
<p>Total Signature Management</p>
<p>Wookey Hole Road</p>
<p>Wells</p>
<p>Somerset</p>
<p>BA5 1AA</p>
<p>Tel: +44(0)1749 682384</p>
<p>Fax: +44 (0)1749 682235</p>
</blockquote>
<p>The problem I am having is, how I can tell that &quot;Total Signature Management&quot; is not actually part of the address?  Normally, a company will display its name &quot;THALES LAND AND JOINT SYSTEM&quot; and line 2 would normally be the first part of the address.</p>
<p>In the case above, the company name is followed by a non address part, is there anyway to tell the difference?</p>
<p>Thanks</p>
",nlp,,45,1289112112
Algorithm to match natural text in mail,"<p>I need to separate natural, coherent text/sentences in emails from lists, signatures, greetings and so on  before further processing.</p>

<p>example:</p>

<blockquote>
  <p>Hi tom,</p>
  
  <p><em><strong>last monday we did bla bla, lore Lorem ipsum dolor sit amet, consectetur adipisici elit, sed eiusmod tempor incidunt ut labore et
  dolore magna aliqua.</em></strong></p>
  
  <ul>
  <li>list item 2</li>
  <li>list item 3</li>
  <li>list item 3</li>
  </ul>
  
  <p><em><strong>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquid x ea commodi consequat. Quis aute iure reprehenderit
  in voluptate velit</em></strong></p>
  
  <p>regards, K.</p>
  
  <p>---line-of-funny-characters-#######</p>
  
  <p>example inc. </p>
  
  <p>33 evil street, london</p>
  
  <p>mobile: 00 234534/234345</p>
</blockquote>

<p>Ideally the algorithm would match only the bold parts. </p>

<p>Is there any recommended approach - or are there even existing algorithms for that problem? Should I try approximate regular expressions or more statistical stuff based on number of punctation marks, length and so on?</p>
","python, regex, algorithm, nlp","<p>You need to go through serious NLP stuff to get the desired processing done (depends on what level of precision you are expecting and the randomness and vagueness of the input email data for your code). </p>

<p>Read <a href=""http://research.microsoft.com/en-us/um/people/joshuago/conference/papers-2004/135.pdf"" rel=""nofollow"">this</a> one. See the references section for other relevant stuff. </p>

<p><a href=""http://www.infonomics-society.org/IJICR/Email%20Classification%20Using%20Back%20Propagation%20Technique.pdf"" rel=""nofollow"">This</a> deals with different issue of classification but involves operating on the email text.</p>
",278,1333730640
How to tell if two web contents are similar?,"<p>Given 2 html sources, I want to first extract the main content out of it using something like <a href=""http://code.google.com/p/boilerpipe/"" rel=""nofollow"">this</a>. Are there any <a href=""http://tomazkovacic.com/blog/122/evaluating-text-extraction-algorithms/"" rel=""nofollow"">other better libraries</a> - I am specifically looking for Python/Javascript ones?</p>

<p>Once I have the two extracted contents, I want to return a score between 0 and 1 denoting how similar they are e.g. news articles on the same topic from CNN and BBC would have higher similarity scores since they are on the same topic or webpages pertaining to the same product on Amazon.com and Walmart.com would have a high score too. How can I do this? Are there existing libraries that do this already? What are some good libraries I can use? Basically I am looking for a combination of <a href=""http://en.wikipedia.org/wiki/Automatic_summarization"" rel=""nofollow"">automatic summarization</a>, <a href=""http://en.wikipedia.org/wiki/Terminology_extraction"" rel=""nofollow"">keyword extraction</a>, <a href=""http://en.wikipedia.org/wiki/Named_entity_recognition"" rel=""nofollow"">named-entity recognition</a> and <a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""nofollow"">sentiment-analysis</a>.</p>
","python, nlp, machine-learning, text-mining, semantic-analysis","<p>There are many things embedded in your question. I will try to provide you with a library or else will suggest you Algorithms that can solve your tasks (which you can Google and you will get many python implementations)</p>

<p><strong>Point 1</strong>. To extract main content out of html (http://nltk.googlecode.com/svn/trunk/doc/book/ch03.html) &amp; for other NLP related stuff you can check out NLTK. Its written in Python. You can also check out for a library called BeautifulSoup, its awesome (http://www.crummy.com/software/BeautifulSoup/)</p>

<p><strong>Point 2</strong>. When you say:</p>

<p>Once I have the two extracted contents, I want to return a score between 0 and 1 denoting how similar they are....</p>

<p>For this I suggest you can cluster your document set using any unsupervised learning clustering technique. Since your problem falls under the distance-metric based clustering so it should be really easy for you to cluster similar documents and then assign a score to them based on their similarity with the cluster centroid. Try either K-Means or Adaptive Resonance Theory. In the latter you dont need to define the number of clusters in advance. OR as larsman points out in his comments you can simply use TF-IDF (http://www.miislita.com/term-vector/term-vector-3.html)</p>

<p><strong>Point 3</strong>.When you say:</p>

<p>Basically I am looking for a combination of automatic summarization, keyword extraction, named-entity recognition and sentiment-analysis</p>

<p>For Automatic Summarization use Non Negative Matrix Factorization</p>

<p>For Keyword extraction use NLTK</p>

<p>For Named-Entity Recognition use NLTK</p>

<p>For Sentiment Analysis use NLTK</p>
",509,1333656576
Filter words belonging to a broad category,"<p>I have a list of words (assume they are stored in String[] if you must). I want to filter out words that belong to a broad general category such as <em>Music</em> or <em>Sports</em>.</p>

<p>Is there a ready-made solution for this (even if it's only for a limited set of general categories)?</p>

<p>Or how would you go about doing this?</p>

<p>It is to be done in <em>Java 1.6</em> and it is an NLP (Natural Language Processing) problem. The input list of words has random words, and I want to extract from this large list, only the words that belong to a given general category (which will be a subset).</p>

<p><strong>Another way of thinking:</strong> Given a single word, I want to determine if this word belongs to a category. <em>Something like this:</em></p>

<pre><code>String word1 = ""football""; //the strings will always be single word units
String word2 = ""telephone"";
boolean b1 = belongsToCategory(Categories.SPORTS, word1); //true
boolean b2 = belongsToCategory(Categories.SPORTS, word2); //false
</code></pre>

<p>If you need more info, please ask.</p>
","java, nlp","<p>Well, my idea would be to hold a set of words for each category and look the word up in each set.</p>

<p>Of course, this set would get huge and impossible to maintain if you held all the inflected forms for a single word. I'd consider using lemmatization to limit the size of this set.
You might be interested in checking the following links:
<a href=""http://en.wikipedia.org/wiki/Lemmatization"" rel=""nofollow noreferrer"" title=""Lemmatisation - Wikipedia, the free encyclopedia"">Lemmatization on Wikipedia</a>
and
<a href=""https://stackoverflow.com/questions/1578062/lemmatization-java"" title=""nlp - Lemmatization java - Stack Overflow"">Lemmatization java</a></p>
",225,1333358932
Find Synonyms for multi-word phrases,"<p><strong>Is it possible for the python library NLTK to suggest/create synonyms for groups of words?</strong></p>

<p>For example; for the word/group ""main course"" can I use NLTK to get the synonyms ""main dish"", ""main meal"", ""dinner"" etc.?</p>

<p>Heres my code that works for single word synonyms but not multiwords:</p>

<pre><code>from nltk.corpus import wordnet as wn
print wn.synset(""eat.v.01"").lemma_names # prints synonyms of eat
print wn.synset(""main course.n.01"").lemma_names # throws WordNetError
</code></pre>
","python, nlp, nltk","<p>Use an underscore:</p>

<pre><code>print wn.synset(""main_course.n.01"").lemma_names
</code></pre>
",4357,1333336309
Finding synonyms for a certain word creates a WordNetError,"<p>I am attempting to get synonyms for a word using the python library NLTK.</p>

<p><strong>My Problem:</strong> Some words create an error when I use them. For example 'eat' throws a WordNetError of ""WordNetError: no lemma 'eat' with part of speech 'n'"". What does that mean? How can I retrieve synonyms for the word eat?</p>

<p>Here's my code, note how words like 'dog' do work:</p>

<pre><code>from nltk.corpus import wordnet as wn
print wn.synset(""dog.n.01"").lemma_names
print wn.synset(""eat.n.01"").lemma_names
</code></pre>

<p><em>Also is it possible to get synonyms for a group of words? For example; for 'main course', can I get the synonyms 'main dish', 'main meal', 'dinner'?</em></p>
","python, nlp, nltk","<p>The error says <code>no lemma 'eat' with part of speech 'n'</code>. That means that ""eat"" isn't in WordNet as a <b>n</b>oun. Try it as a verb:</p>

<pre><code>&gt;&gt;&gt; wn.synset('eat.v.01').lemma_names
['eat']
</code></pre>
",848,1333320445
Performing Stemming outputs jibberish/concatenated words,"<p>I am experimenting with the python library NLTK for Natural Language Processing.</p>

<p><strong>My Problem:</strong> I'm trying to perform stemming; reduce words to their normalised form. But its not producing correct words. Am I using the stemming class correctly? And how can I get the results I am attempting to get?</p>

<p>I want to normalise the following words:</p>

<pre><code>words = [""forgot"",""forgotten"",""there's"",""myself"",""remuneration""]
</code></pre>

<p>...into this:</p>

<pre><code>words = [""forgot"",""forgot"",""there"",""myself"",""remunerate""]
</code></pre>

<p>My code:</p>

<pre><code>from nltk import stem
words = [""forgot"",""forgotten"",""there's"",""myself"",""remuneration""]
for word in words:
    print stemmer.stem(word)

#output is:
#forgot forgotten there' myself remuner
</code></pre>
","python, nlp, nltk",,184,1333250057
How to determine the proper weights for metric scores,"<p>I'm doing some personal research into text analysis, and have come up with close to 70 metrics (pronoun usage frequency, reading levels, vowel frequency, use of bullet points, etc) to ""score"" a piece of text. </p>

<p>Ideally, separate pieces of text from the same author would have similar scores. The ultimate goal is to index a great deal of authors, and use scores to guess at who wrote a separate, anonymous piece of text. </p>

<p>I'd like the scores to normalize from 0 to 100 and represent a percentage of how ""similar"" two pieces of text are in writing style. Questions like <a href=""https://stackoverflow.com/questions/7041867/how-to-decide-on-weights"">How to decide on weights?</a> and <a href=""https://stackoverflow.com/questions/994819/how-to-calculate-scores"">How to calculate scores?</a> describe the math behind scoring metrics and how to normalize, but assume every metric is weighted the same.</p>

<p><strong>My question is this: how do I determine the proper weight to use when scoring each metric, to ensure that the cumulative score per-user most accurately describes the writing from that specific user?</strong></p>

<p>Also, weights can be assigned per-user. If syllables per word most aptly describes who wrote a piece for Alice, while the frequency of two-letter words is the best for Bob, I'd like Alice's heaviest weight to be on syllables per word, and Bob's to be on frequency of two-letter words.</p>
","algorithm, statistics, nlp, metrics, text-analysis","<p>If you want to do it with weighted scores, have a look at <a href=""http://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">http://en.wikipedia.org/wiki/Principal_component_analysis</a> - you could plot the values of the first (largest) couple of principal components for different authors and see if you find a clustering. You can also take a plot of the smallest few principal components and see if anything stands out - if it does, it is probably from a glitch or a mistake - it tends to pick out exceptions from general rules.</p>

<p>Another option is <a href=""http://en.wikipedia.org/wiki/Linear_discriminant_analysis"" rel=""nofollow"">http://en.wikipedia.org/wiki/Linear_discriminant_analysis</a></p>

<p>I suppose you could build per-author weights if you built weights for the classification Alice vs not-Alice, and weights for the classification Bob vs not-Bob.</p>

<p>Another way of trying to identify authors is to build a <a href=""http://en.wikipedia.org/wiki/Language_model"" rel=""nofollow"">http://en.wikipedia.org/wiki/Language_model</a> for each author.</p>

<p>It occurs to me that if you are prepared to claim that your different measures are independent, you can then combine them with <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">http://en.wikipedia.org/wiki/Naive_Bayes_classifier</a>. The log of the final Bayes factor will then be the sum of the logs of the individual Bayes factors, which gives you your sum of weighted scores.</p>
",4821,1332562517
Building a lemmatizer: speed optimization,"<p>I am building a lemmatizer in python. As I need it to run in realtime/process fairly large amount of data the processing speed
is of the essence. 
Data: I have all possible suffixes that are linked to all wordtypes that they can be combined with. Additionally I have lemmaforms that are linked to both their wordtype(s) and lemma(s). The program takes a word as input and outputs its lemma.
word = lemmafrom + suffix</p>

<p>For example (Note: although the example is given in English I am not building a lemmatizer for English):</p>

<p>word:  forbidding</p>

<p>lemmaform: forbidd</p>

<p>suffix: ing</p>

<p>lemma: forbid</p>

<p>My solution:</p>

<p>I have converted the data to (nested) dicts:</p>

<pre><code>suffixdict : {suffix1:[type1,type2, ... , type(n)], suffix2:[type1,type2, ... ,
type(n)]}    
lemmaformdict : {lemmaform:{type1:lemma}}
</code></pre>

<p>1) Find all possible suffixes and word types that they are linked to.
If the longest possible suffix is 3 characters long, the program tries to match 'ing', 'ng', 'n' to the keys in 
suffixdict. If the key exists it returns a value (a set of wordtypes).</p>

<p>2) For each matching suffix search the lemmaform from the dict.
If lemmaform exists it returns the wordtypes.</p>

<p>3) Finally, the program tries to intersect the wordtypes produced in steps 1) ans 2) and if the intersection is
sucessful it returns the lemma of the word.</p>

<p>My question: could there be a better solution to my problem from the prespective of speed? (Disregarding the option to keep frequent words and lemmas in the dictionary) 
Help much appriciated.</p>
","python, optimization, nlp, lemmatization","<p>This would be a wonderful application for finite state transducers. Why? Because they allow you to do string rewriting efficiently (in time linear to the size of the input). Consider the following s[ia]mple transducer:</p>

<p><img src=""https://i.sstatic.net/2fz5R.jpg"" alt=""enter image description here""></p>

<p>It takes a string as input and checks whether there exists a path from the initial state (here, 0) to a final state (10, 12 and 17, respectively) given the sequence of input characters. If it reaches a final state, it produces the appropriate output, e.g. (forbidd, ing) if the input was ""forbidding"".</p>

<p>I don't know whether you have any background on finite state automata, though. If not, give them a try - it will be worth the effort. :) <a href=""http://en.wikipedia.org/wiki/Trie"" rel=""nofollow noreferrer"">Tries</a> are a special kind of finite state automaton (the sample transducer above is a trie), so they might be a good start.</p>
",1352,1332522279
Loading a file into main memory,"<p>I have this text file that contains a list of words and a bunch of scores associated with each of them. I want to read the words off the file and the words and use them in my project. The code is in Java and the file itself is around 13MB. When I try to read file it takes around 15 minutes to complete, sometime even more. Any ideas on how I should optimize or approach this problem from scratch.</p>

<p>The operations being performed on file are - 
1. Read each line (Around 50 chars on the average case)
2. Split into tokens using a single whitespace (Around 10 tokens in the average case)
3. Store the generated tokens in an array (Nothing more at this stage)</p>

<p>EDIT: Sorry for not posting my code before, it just slipped my mind. <a href=""http://pastie.org/3646388"" rel=""nofollow"">http://pastie.org/3646388</a>
There is an line there where I loop to 10000 values which was to see if I got any output as opposed to looping till EOF.</p>
","java, file, optimization, nlp, tokenize",,198,1332388580
Extracting information from context-free phrase structure output from Stanford Parser,"<p>The Stanford Parser (http://nlp.stanford.edu/software/lex-parser.shtml) gives context-free phrase structure trees as following. What is the best way to extract things like all the Noun Phrases(NP) and Verb Phrases(NP) in the tree? Is there any Python (or Java) library that can allow me to read structures like these? Thank you.</p>

<pre><code>(ROOT
  (S
    (S
      (NP
        (NP (DT The) (JJS strongest) (NN rain))
        (VP
          (ADVP (RB ever))
          (VBN recorded)
          (PP (IN in)
            (NP (NNP India)))))
      (VP
        (VP (VBD shut)
          (PRT (RP down))
          (NP
            (NP (DT the) (JJ financial) (NN hub))
            (PP (IN of)
              (NP (NNP Mumbai)))))
        (, ,)
        (VP (VBD snapped)
          (NP (NN communication) (NNS lines)))
        (, ,)
        (VP (VBD closed)
          (NP (NNS airports)))
        (CC and)
        (VP (VBD forced)
          (NP
            (NP (NNS thousands))
            (PP (IN of)
              (NP (NNS people))))
          (S
            (VP (TO to)
              (VP
                (VP (VB sleep)
                  (PP (IN in)
                    (NP (PRP$ their) (NNS offices))))
                (CC or)
                (VP (VB walk)
                  (NP (NN home))
                  (PP (IN during)
                    (NP (DT the) (NN night))))))))))
    (, ,)
    (NP (NNS officials))
    (VP (VBD said)
      (NP-TMP (NN today)))
    (. .)))
</code></pre>
","python, nlp, stanford-nlp","<p>Check out the Natural Language Toolkit (NLTK) at <a href=""http://www.nltk.org"" rel=""nofollow"">nltk.org</a>.</p>

<p>The toolkit is written in Python and provides code for reading precisely these kinds of trees (as well as lots of other stuff).</p>

<p>Alternatively, you could write your own recursive function for doing this.  It would be pretty straightforward.</p>

<hr>

<p>Just for fun: here's a super simple implementation of what you want:</p>

<pre><code>def parse():
  itr = iter(filter(lambda x: x, re.split(""\\s+"", s.replace('(', ' ( ').replace(')', ' ) '))))

  def _parse():
    stuff = []
    for x in itr:
      if x == ')':
        return stuff
      elif x == '(':
        stuff.append(_parse())
      else:
        stuff.append(x)
    return stuff

  return _parse()[0]

def find(parsed, tag):
  if parsed[0] == tag:
    yield parsed
  for x in parsed[1:]:
    for y in find(x, tag):
      yield y

p = parse()
np = find(p, 'NP')
for x in np:
  print x
</code></pre>

<p>yields:</p>

<pre><code>['NP', ['NP', ['DT', 'The'], ['JJS', 'strongest'], ['NN', 'rain']], ['VP', ['ADVP', ['RB', 'ever']], ['VBN', 'recorded'], ['PP', ['IN', 'in'], ['NP', ['NNP', 'India']]]]]
['NP', ['DT', 'The'], ['JJS', 'strongest'], ['NN', 'rain']]
['NP', ['NNP', 'India']]
['NP', ['NP', ['DT', 'the'], ['JJ', 'financial'], ['NN', 'hub']], ['PP', ['IN', 'of' ['NP', ['NNP', 'Mumbai']]]]
['NP', ['DT', 'the'], ['JJ', 'financial'], ['NN', 'hub']]
['NP', ['NNP', 'Mumbai']]
['NP', ['NN', 'communication'], ['NNS', 'lines']]
['NP', ['NNS', 'airports']]
['NP', ['NP', ['NNS', 'thousands']], ['PP', ['IN', 'of'], ['NP', ['NNS', 'people']]]]
['NP', ['NNS', 'thousands']]
['NP', ['NNS', 'people']]
['NP', ['PRP$', 'their'], ['NNS', 'offices']]
['NP', ['NN', 'home']]
['NP', ['DT', 'the'], ['NN', 'night']]
['NP', ['NNS', 'officials']]
</code></pre>
",1510,1332301848
"Given a noun, what&#39;s the best way to determine the &quot;gender&quot;?","<p>I'm given the task of determining whether an advertisement is suited for male or female.
What's the best way of determining this?</p>

<p>The words looks like this:</p>

<pre><code>Cheetos
Coca Cola
Nike
Ferrari
24
Arrested Development
Transformers
Nestle
American Eagle
</code></pre>

<p>For each word, I'd like to know if it's more associated to male or female. It doesn't have to be correct.  I know it's hard to tell if ""nike"" is suited towards male or female. Just any methodology would help me brainstorm.</p>
","python, nlp, nltk","<p>Testing for these kinds of associations is very difficult, and I doubt you could do it without doing actual research. However, if you're up for it, you should look into the ""Implicit Association Test."" You can find a demonstration of it here:</p>

<p><a href=""http://www.understandingprejudice.org/iat/index2.htm"" rel=""nofollow"">http://www.understandingprejudice.org/iat/index2.htm</a></p>

<p>Here's a Wikipedia page with more information. </p>

<p><a href=""http://en.wikipedia.org/wiki/Implicit_Association_Test"" rel=""nofollow"">http://en.wikipedia.org/wiki/Implicit_Association_Test</a></p>

<p>It's controversial, but there's at least some evidence that it is a reliable tool for measuring unconscious bias. </p>

<p>I would have some ethical qualms about using this kind of research for writing advertisements. But I'm inclined to take your question seriously; I'll leave you to consider the ethical implications on your own. </p>
",1808,1297022891
Word Translation by letter/vowels,"<p>I'm looking to translate words by letters/vowels.
I'll try to explain.
I have a an Arabic text with ~300,000 words, my goal is to enable users to search the text using one of 10 languages I'll define. So if some search for Stack overflow in English I'll need to break down to words as S-TA-CK O-VE-R-F-LOW (I need to break it that way to get the Arabic equivalent letters).
Is there something like that already exsiting, or I just need to start from scratch and do a linguistic research???
Thank you for your time.</p>
","c#, java, string, language-agnostic, nlp","<p>You need to analyze your words by finding the relative <strong>syllables</strong>. Take a look at <a href=""http://cmusphinx.sourceforge.net/sphinx4"" rel=""nofollow"">Sphinx-4</a> Java library, I guess there are some example codes for extracting a word to its syllables based on defined grammar rules.</p>
",118,1332248394
&quot;pre-built&quot; matrices for latent semantic analysis,"<p>I want to use Latent Semantic Analysis for a small app I'm building, but I don't want to build up the matrices myself. (Partly because the documents I have wouldn't make a very good training collection, because they're kinda short and heterogeneous, and partly because I just got a new computer and I'm finding it a bitch to install the linear algebra and such libraries I would need.)</p>

<p>Are there any ""default""/pre-built LSA implementations available? For example, things I'm looking for include:</p>

<ul>
<li>Default U,S,V matrices (i.e., if D is a term-document matrix from some training set, then D = U S V^T is the singular value decomposition), so that given any query vector q, I can use these matrices to compute the LSA projection of q myself.</li>
<li>Some black-box LSA algorithm that, given a query vector q, returns the LSA projection of q.</li>
</ul>
","nlp, machine-learning, latent-semantic-indexing, gensim","<p>You'd probably be interested in the <a href=""http://nlp.fi.muni.cz/projekty/gensim/"" rel=""nofollow"">Gensim</a> framework for Python; notably, it has <a href=""http://nlp.fi.muni.cz/projekty/gensim/wiki.html"" rel=""nofollow"">an example on building the appropriate matrices from English Wikipedia</a>.</p>
",490,1289019562
Method/Tool for Extracting Keywords from List of Sentences,"<p>I have a large list of sentences and would like to tag each of them with their own unique keywords, to help me identify which sentences are similar for grouping purposes.  </p>

<p>As an example:</p>

<pre>
The dog ran fast. - tagged as: dog
The cat is sleeping - tagged as: cat
The German Sheppard is awake. - tagged as dog
</pre>

<p>I've been looking into tools like alchemy api and openCalais for the keyword extraction, however, it seems you moreso use these to extract meaning out of a block of data, like an entire document or paragraph rather than tagging 1000s of unique but similar individual sentences.  </p>

<p>In short, ideally I'd like to:</p>

<ol>
<li>Take a sentence from a document or webpage (perhaps from a large spreadsheet or a list of tweets)</li>
<li>Place a unique identifier on it (some type of keyword)</li>
<li>Group the sentences together by keywrd</li>
</ol>
","nlp, search-engine, data-mining, text-mining, semantic-analysis",,1466,1332092540
Spell checker solution in java,"<p>I need to implement a spell checker in java , let me give you an example for a string lets say ""sch aproblm iseasili solved"" my output is ""such a problem is easily solved"".The maximum length of the string to correct is 64.As you can see my string can have spaces inserted in the wrong places or not at all and even misspelled words.I need a little help in finding a efficient algorithm of coming up with the corrected string. I am currently trying to delete all spaces in my string and inserting spaces in every possible position , so lets say for the word (it apply to a sentence as well) ""hot"" i generate the next possible strings to afterwords be corrected word by word using levenshtein distance : h o t ; h ot; ho t; hot. As you can see i have generated 2^(string.length() -1) possible strings. So for a string with a length of 64 it will generate 2^63 possible strings, which is damn high, and afterwords i need to process them one by one and select the best one by a different set of parameters such as : - total editing distance (must take the smallest one)
-if i have more strings with same editing distance i have to choose the one with the fewer number of words
-if i have more strings with the same number of words i need to choose the one with the total maximum frequency the words have( i have a dictionary of the most frequent 8000 words along with their frequency )
-and finally if there are more strings with the same total frequency i have to take the smallest lexicographic one.</p>

<p>So basically i generate all possible strings (inserting spaces in all possible positions into the original string) and then one by one i calculate their total editing distance, nr of words ,etc. and then choose the best one, and output the corrected string. I want to know if there is a easier(in terms of efficiency) way of doing this , like not having to generate all possible combinations of strings etc.</p>

<p>EDIT:So i thought that i should take another approach on this one.Here is what i have in mind: I take the first letter from my string , and extract from the dictionary all the words that begin with that letter.After that i process all of them and extract from my string all possible first words. I will remain at my previous example , for the word ""hot"" by generating all possible combinations i got 4 results , but with my new algorithm i obtain only 2 ""hot"" , and ""ho"" , so it's already an improvement.Though i need a little bit of help in creating a recursive or PD algorithm for doing this . I need a way to store all possible strings for the first word , then for all of those all possible strings for the second word and so on and finally to concatenate all possibilities and add them into an array or something. There will still be a lot of combinations for large strings but not as many as having to do ALL of them. Can someone help me with a pseudocode or something , as this is not my strong suit.</p>

<p>EDIT2: here is the code where i generate all the possible first word from my string <a href=""http://pastebin.com/d5AtZcth"" rel=""nofollow"">http://pastebin.com/d5AtZcth</a> .I need to somehow implement this to do the same for the rest and combine for each first word with each second word and so on , and store all these concatenated into an array or something.</p>
","java, nlp",,2537,1331848350
Interesting NLP/machine-learning style project -- analyzing privacy policies,"<p>I wanted some input on an interesting problem I've been assigned.  The task is to analyze hundreds, and eventually thousands, of privacy policies and identify core characteristics of them.  For example, do they take the user's location?, do they share/sell with third parties?, etc. </p>

<p>I've talked to a few people, read a lot about privacy policies, and thought about this myself.  Here is my current plan of attack:</p>

<p>First, read a lot of privacy and find the major ""cues"" or indicators that a certain characteristic is met.  For example, if hundreds of privacy policies have the same line: ""We will take your location."", that line could be a cue with 100% confidence that that privacy policy includes taking of the user's location.  Other cues would give much smaller degrees of confidence about a certain characteristic.. For example, the presence of the word ""location"" might increase the likelihood that the user's location is store by 25%.</p>

<p>The idea would be to keep developing these cues, and their appropriate confidence intervals to the point where I could categorize all privacy policies with a high degree of confidence.  An analogy here could be made to email-spam catching systems that use Bayesian filters to identify which mail is likely commercial and unsolicited.</p>

<p>I wanted to ask whether you guys think this is a good approach to this problem.  How exactly would you approach a problem like this?  Furthermore, are there any specific tools or frameworks you'd recommend using.  Any input is welcome.  This is my first time doing a project which touches on artificial intelligence, specifically machine learning and NLP.</p>
","language-agnostic, artificial-intelligence, nlp, machine-learning","<blockquote>
  <p>The idea would be to keep developing these cues, and their appropriate confidence intervals to the point where I could categorize all privacy policies with a high degree of confidence. An analogy here could be made to email-spam catching systems that use Bayesian filters to identify which mail is likely commercial and unsolicited.</p>
</blockquote>

<p>This is <a href=""http://nlp.stanford.edu/IR-book/html/htmledition/the-text-classification-problem-1.html"" rel=""nofollow"">text classification</a>. Given that you have multiple output categories per document, it's actually <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow"">multilabel classification</a>. The standard approach is to <a href=""https://en.wikipedia.org/wiki/Supervised_learning"" rel=""nofollow"">manually label</a> a set of documents with the classes/labels that you want to predict, then train a classifier on features of the documents; typically word or n-gram occurrences or counts, possibly weighted by tf-idf.</p>

<p>The popular learning algorithms for document classification include naive Bayes and linear SVMs, though other classifier learners may work too. Any classifier can be extended to a multilabel one by the <a href=""http://scikit-learn.org/0.10/modules/multiclass.html#one-vs-the-rest"" rel=""nofollow"">one-vs.-rest</a> (OvR) construction.</p>
",751,1331755038
Finding Similar Paragraphs in Different Documents,"<p>I am trying to figure out what type of analysis and currently available libraries there are out there to, when feed in two text paragraphs, determine if there is some degree of similarity between the two (whether with regards to subjects and verbs, or context).  I imagine there might be some NLP type analysis that could be done but seeing what experiences people have had in these solutions.</p>

<p>For example, consider the following two paragraphs:</p>

<ol>
<li><p>Governing Law.  This Agreement is governed by the laws of the State of Missouri without reference to its conflict of law principles.</p></li>
<li><p>Law to Apply.  The laws of the State of Missouri shall apply to this Agreement.</p></li>
</ol>

<p>Fundamentally, I'd want the have those two clauses picked as identical.</p>

<p>I am looking for a library available under .Net.</p>
","c#, nlp","<p>Well that is an interesting question. You could use NLTK to extract the core concepts (Noun groups) and compare those directly. In this case you'd get:</p>

<ol>
<li>Governing Law, Agreement, Laws, State, Missouri, conflict, law principles</li>
<li>Law, laws, State, Missouri, Agreement</li>
</ol>

<p>Now, similarity is not bi-directional. Group 2 is fully represented in Group 1, but not the other way around. You could apply a harmonic mean where you count the percentage of a group in another group so G21 would be 1.0 and G12 would be 0.57. So the harmonic mean would be H = 2AB/(A+B) == 2(1.0)*(0.57)/(1.0 + 0.57) = 0.72.</p>

<p>Now, this isn't identical but in your example you wanted there to be a match between the two paragraphs. In this case their harmonic mean, H, is 0.72. The higher the number, the harder it is to achieve. H>0.8 is considered good. H>0.9 for most systems is exceptional. So what you must decide is where do you want your arbitrary line in the sand drawn? It has to be arbitrary because you haven't given a definition of the degree of similarity. So do you set it at 0.6, 0.7? How about 0.12948737? A good way of discovering this threshold is to take test examples and without doing the math just judge for yourself their similarity and then run the numbers and see what you come up with.</p>
",1882,1318358886
How do I manipulate parse trees?,"<p>I've been playing around with natural language parse trees and manipulating them in various ways. I've been using Stanford's Tregex and Tsurgeon tools but the code is a mess and doesn't fit in well with my mostly Python environment (those tools are Java and aren't ideal for tweaking). I'd like to have a toolset that would allow for easy hacking when I need more functionality. Are there any other tools that are well suited for doing pattern matching on trees and then manipulation of those matched branches?</p>

<p>For example, I'd like to take the following tree as input:</p>

<pre><code>(ROOT
  (S
    (NP
      (NP (NNP Bank))
      (PP (IN of)
        (NP (NNP America))))
    (VP (VBD used)
      (S
        (VP (TO to)
          (VP (VB be)
            (VP (VBN called)
              (NP
                (NP (NNP Bank))
                (PP (IN of)
                  (NP (NNP Italy)))))))))))
</code></pre>

<p>and (this is a simplified example):</p>

<ol>
<li>Find any node with the label NP that has a first child with the label NP and some descendent named ""Bank"", and a second child with the label PP.</li>
<li>If that matches, then take all of the children of the PP node and move them to end of the matched NP's children.</li>
</ol>

<p>For example, take this part of the tree:</p>

<pre><code>(NP
  (NP (NNP Bank))
  (PP (IN of)
    (NP (NNP America))))
</code></pre>

<p>and turn it into this:</p>

<pre><code>(NP
  (NP (NNP Bank) (IN of) (NP (NNP America))))
</code></pre>

<p>Since my input trees are S-expressions I've considered using Lisp (embedded into my Python program) but it's been so long that I've written anything significant in Lisp that I have no idea where to even start.</p>

<p>What would be a good way to describe the patterns? What would be a good way to describe the manipulations? What's a good way to think about this problem?</p>
","lisp, nlp, pattern-matching, stanford-nlp, s-expression","<p>This is a typical case of using Lisp. You would need a function that maps another function over the tree.</p>

<p>Here is a procedural matching example using Common Lisp. There are matchers in Lisp that work over list structures, which could be used instead. Using a list matcher would simplify the example (see my other answer for an example using a pattern matcher).</p>

<p>The code:</p>

<pre><code>(defun node-children (node)
  (rest node))

(defun node-name (node)
  (second node))

(defun node-type (node)
  (first node))


(defun treemap (tree matcher transformer)
  (cond ((null tree) nil)
        ((consp tree)
         (if (funcall matcher tree)
             (funcall transformer tree)
           (cons (node-type tree)
                 (mapcar (lambda (child)
                           (treemap child matcher transformer))
                         (node-children tree)))))
        (t tree))))
</code></pre>

<p>The example:</p>

<pre><code>(defvar *tree*
  '(ROOT
    (S
     (NP
      (NP (NNP Bank))
      (PP (IN of)
          (NP (NNP America))))
     (VP (VBD used)
         (S
          (VP (TO to)
              (VP (VB be)
                  (VP (VBN called)
                      (NP
                       (NP (NNP Bank))
                       (PP (IN of)
                           (NP (NNP Italy))))))))))))



(defun example ()
  (pprint
   (treemap *tree*
            (lambda (node)
              (and (= (length (node-children node)) 2)
                   (eq (node-type (first (node-children node))) 'np)
                   (some (lambda (node)
                           (eq (node-name node) 'bank))
                         (children (first (node-children node))))
                   (eq (first (second (node-children node))) 'pp)))
            (lambda (node)
              (list (node-type node)
                    (append (first (node-children node))
                            (node-children (second (node-children node)))))))))
</code></pre>

<p>Running the example:</p>

<pre><code>CL-USER 75 &gt; (example)

(ROOT
 (S
  (NP
   (NP (NNP BANK) (IN OF) (NP (NNP AMERICA))))
  (VP
   (VBD USED)
   (S
    (VP
     (TO TO)
     (VP
      (VB BE)
      (VP
       (VBN CALLED)
       (NP
        (NP
         (NNP BANK)
         (IN OF)
         (NP (NNP ITALY)))))))))))
</code></pre>
",3980,1284253390
java lang Class Cast Exception,"<p>I have written a code which can reduce the grammatical boundaries for a text, but when I run the program this exception comes up     </p>

<pre><code>java.lang.ClassCastException
</code></pre>

<p>here is the class that i run,</p>

<pre><code>public class paerser {
public static void main (String [] arg){
    LexicalizedParser lp = new LexicalizedParser(""grammar/englishPCFG.ser.gz"");
        lp.setOptionFlags(""-maxLength"", ""500"", ""-retainTmpSubcategories"");
        TreebankLanguagePack tlp = new PennTreebankLanguagePack();
       GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
       String text = ""John, who was the CEO of a company, played golf."";
       edu.stanford.nlp.trees.Tree parse = lp.apply(Arrays.asList(text));
       GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
       List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
       System.out.println(tdl);

}
}
</code></pre>

<p>Updated,</p>

<p>here is the full stack trace ...</p>

<pre><code>Loading parser from serialized file grammar/englishPCFG.ser.gz ... done [1.5 sec].
Following exception caught during parsing:
java.lang.ClassCastException: java.lang.String cannot be cast to edu.stanford.nlp.ling.HasWord
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:346)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:386)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.apply(LexicalizedParser.java:304)
    at paerser.main(paerser.java:19)
Recovering using fall through strategy: will construct an (X ...) tree.
Exception in thread ""main"" java.lang.ClassCastException: java.lang.String cannot be  cast to edu.stanford.nlp.ling.HasWord
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.apply(LexicalizedParser.java:317)
    at paerser.main(paerser.java:19)
</code></pre>
","java, eclipse, exception, nlp, stanford-nlp","<p>Stacktrace shows that <a href=""http://cogcomp.cs.illinois.edu/trac/browser/Public/public/curator-src/curator-annotators/stanford-parser/src/edu/stanford/nlp/parser/lexparser/ExhaustivePCFGParser.java"" rel=""nofollow"">ExhaustivePCFGParser's parse method</a> is being used. It expects a List of HasWord objects. You are passing a list of String. Hence, the exception.</p>

<pre><code>public boolean parse(List&lt;? extends HasWord&gt; sentence) { // ExhaustivePCFGParser
</code></pre>
",1131,1331611509
How to find out the entropy of the English language,"<p>How to find out the entropy of the English language by using isolated symbol probabilities of the language?</p>
","nlp, entropy",,10910,1331134806
Tool for creating own rules for word lemmatization and similar tasks,"<p>I'm doing a lot of natural language processing with a bit unsusual requirements. Often I get tasks <strong>similar to lemmatization</strong> - given a word (or just piece of text) I need to find some patterns and transform the word somehow. For example, I may need to correct misspellings, e.g. given word ""eatin"" I need to transform it to ""eating"". Or I may need to transform words ""ahahaha"", ""ahahahaha"", etc. to just ""ahaha"" and so on. </p>

<p>So I'm looking for some <strong>generic tool</strong> that allows to define <strong>transormation rules</strong> for such cases. Rules may look something like this:</p>

<pre><code> {w}in   -&gt;  {w}ing
 aha(ha)+  -&gt;  ahaha
</code></pre>

<p>That is I need to be able to use captured patterns from the left side on the right side.</p>

<p>I work with linguists who don't know programming at all, so <em>ideally</em> this tool should use <strong>external files</strong> and <strong>simple language for rules</strong>.</p>

<p>I'm doing this project in Clojure, so <em>ideally</em> this tool should be a library for one of JVM languages (Java, Scala, Clojure), but other languages or command line tools are ok too. </p>

<p>There are several very cool NLP projects, including <a href=""http://gate.ac.uk/"" rel=""nofollow"">GATE</a>, <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford CoreNLP</a>, <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a> and others, and I'm not expert in all of them, so I could miss the tool I need there. If so, please let me know. </p>

<p>Note, that I'm working with several languages and perform very different tasks, so concrete lemmatizers, stemmers, misspelling correctors and so on for concrete languages do not fit my needs - I really need more generic tool. </p>

<p><strong>UPD.</strong> It seems like I need to give some more details/examples of what I need. </p>

<p>Basically, I need a function for replacing text by some kind of regex (similar to Java's <code>String.replaceAll()</code>) but with possibility to <strong>use caught text in replacement string</strong>. For example, in real world text people often repeat characters to make emphasis on particular word, e.g. someoone may write ""This film is soooo boooring..."". I need to be able to replace these repetitive ""oooo"" with only single character. So there may be a rule like this (in syntax similar to what I used earlier in this post):</p>

<pre><code>{chars1}&lt;char&gt;+{chars2}?  -&gt;  {chars1}&lt;char&gt;{chars2}
</code></pre>

<p>that is, replace word starting with some chars (<code>chars1</code>), at least 3 chars  and possibly ending with some other chars (<code>chars2</code>) with similar string, but with only a single . Key point here is that we catch  on a left side of a rule and use it on a right side. </p>
","java, regex, nlp, stemming, lemmatization","<p>I've found <a href=""http://userguide.icu-project.org/transforms/general"" rel=""nofollow"">http://userguide.icu-project.org/transforms/general</a> to be useful as well for some general pattern/transform tasks like this, ignore the stuff about transliteration, its nice for doing a lot of things.</p>

<p>You can just load up rules from a file into a String and register them, etc.</p>

<p><a href=""http://userguide.icu-project.org/transforms/general/rules"" rel=""nofollow"">http://userguide.icu-project.org/transforms/general/rules</a></p>
",984,1331345964
changing every non letter character to \n in a file using unix utilities,"<p>i was watching  a tutorial about using unix utilities the guy was using it on a MAC i had a windows laptop so i downloaded <a href=""http://getgnuwin32.sourceforge.net/"" rel=""nofollow"">Gnuwin32 Package</a> 
then came a part where i want to replace any non letter character in a file with a   newline ""\n"" </p>

<p>the command line in the tutorial was : </p>

<pre><code>tr -sc 'A-Za-z' '\n'  &lt; filename.txt |less 
</code></pre>

<p>it worked with him but when i tried it it put a singleqoute  ""'"" character after character </p>

<pre><code>'S'h'a'k'e's'p'e'a'r'e'T'H'E'T'E'M'P'E'S'T'f'r'o'm'O'n'l'i'n'e'L'i'b'r'a'r'y'o'f'L'i'b'e'r't'y'h't't'p'o'l'l'l'i'b'e'r't'y'f'u'n'd'o'r'g'
</code></pre>

<p>i tried  </p>

<pre><code>tr -sc ""A-Za-z"" ""\n""  &lt; filename.txt |less 
</code></pre>

<p>it added a new line after each character </p>

<pre><code>n
e
L
i
b
r
a
</code></pre>

<p>i tried to remove the compliment option and add ^ in the regex  </p>

<pre><code>tr ""[^A-Za-z]"" ""\n""  &lt; filename.txt |less 
</code></pre>

<p>the result was replacing every <code>letter</code> with a <code>newline</code> </p>

<p>the Question is does Command line options in UNIX utilities of GNUwin32 differ than others ?  and does putting the regex between single quotes like 'A-Z' differ than ""A-Z""
and if so what would be the best answer to replace every non-letter character with a newline , other than the failed trials above </p>

<p><a href=""http://pastie.org/3550558"" rel=""nofollow"">the source of the text i was trying on</a> </p>
","unix, nlp, gnuwin32, tokenize",,367,1331235477
How can I differentiate between a person&#39;s name and other names that are derived from verbs,"<p>How can I extract person names from the text?</p>

<p>I have applied some NLP toolkit for this, specifically I used the Stanford NER toolkit to extract names from text. With that, I can extract person names from the text, but when I want the program to extract words like 'programmer', 'lecturer' or 'engineer', the libraries couldn't extract those. Is there any way to extract these from the text?</p>
","java, nlp, stanford-nlp","<p>Since ""Programmer, lecturer, and engineer"" are not named-entities,  you may have to maintain a list of those words. I think you can obtain them from word derivation relationships in Wordnet, like ""sing"" (verb) and ""singer"" or ""lecture"" (verb) and ""lecturer"" (noun).</p>

<p>A <a href=""http://medialab.di.unipi.it/wiki/SuperSense_Tagger"" rel=""nofollow"">SuperSense tagger</a> may also be used as NER, I think it can tag those words you mentioned as ""noun.person"" which is what you need. <a href=""http://www.ark.cs.cmu.edu/ARKref/"" rel=""nofollow"">ArkRef</a> (Java) is a coreference tool that uses it (through a Java port of supersense tagger, bundled), and there's an online demo there, so you can check if your target words are tagged in square brackets.</p>
",752,1330918977
What is the meaning of &quot;isolated symbol probabilities of English&quot;,"<p>In a note I found this phrase: </p>

<blockquote>
  <p>Using isolated symbol probabilities of English language, you can find out the entropy of the language.</p>
</blockquote>

<p>What is actually meant by ""isolated symbol probabilities""? This is related to the entropy of an information source.</p>
","nlp, entropy","<p>It would be helpful to know where the note came from and what the context is, but even without that I am quite sure this simply means that they use the <em>frequency of individual symbols</em> (e.g. characters) as the basis for entropy, rather than for example the <em>joint probability</em> (of character <em>sequences</em>), or the <em>conditional probability</em> (of one particular character to follow another).</p>

<p>So if you have an alphabet X={a,b,c,...,z} and a probability P(a), P(b),... for each character to appear in text (e.g. based on the frequency found in a data example), you'd compute the entropy by computing -P(x) * log(P(x)) for each character x <em>individually</em> and then taking the sum of all. Then, obviously, you'd have used the probability of each character <em>in isolation</em>, rather than the probability of each character <em>in context</em>.</p>

<p>Note, however, that the term <em>symbol</em> in the note you found does not necessarily refer to characters. It might refer to <em>words</em> or other units of text. Nevertheless, the point they are making is that they apply the classical formula for entropy to <strong>probabilities of individual events</strong> (characters, words, whatever), <strong>not probabilities of complex or conditional events.</strong></p>
",1429,1330942846
How to know if two words have the same base?,"<p>I want to know, in several languages, if two words are:</p>

<ul>
<li>either the same word,</li>
<li>or the grammatical variants of the same word.</li>
</ul>

<p>For example:</p>

<ul>
<li><code>had</code> and <code>has</code> has the same base: in both cases, it's the verb <code>have</code>,</li>
<li><code>city</code> and <code>cities</code> has the same base.</li>
<li><code>went</code> and <code>gone</code> has the same base.</li>
</ul>

<p>Is there a way to use the Microsoft Word API to not just spell check text, but also normalize a word to a base or, at least, determine if two words have the same base?</p>

<p>If not, what are the (free or paid) libraries (not web services) which allow me to do it (again, in several languages)?</p>
","c#, grammar, nlp","<p>Inflector.NET is an open source library that you can use to normalize the inflection of <strong>English nouns</strong>.  Available at: <a href=""https://github.com/davidarkemp/Inflector/tree/master/Inflector"" rel=""nofollow"">https://github.com/davidarkemp/Inflector/tree/master/Inflector</a></p>
",1073,1326484009
Data structures for parsed sentence,"<p>Please find an example sentence:</p>

<pre><code>(S1 (S (S (NP (NP (NP (NN Activation)) (PP (IN of) (NP (NN protein) (NN kinase) (NN C)))) (CC and) (NP (NP (NN elevation)) (PP (IN of) (NP (NN cAMP))))) (VP (VBP interact) (ADVP (RB synergistically)) (S (VP (TO to) (VP (VB raise) (NP (NP (NP (NP (NN c-Fos)) (CC and) (NP (NN AP-1))) (NN activity)) (PP (IN in) (NP (NN Jurkat) (NNS cells))))))))) (. .)))
</code></pre>

<p>The objective is create a tree from this sentence; the leaves are the words, the intermediate nodes are the Part of speech tags, and the root is S1. The parenthesis indicate the span of the phrases contained within the sentence; they need not be included in the tree.</p>

<p>What would be a good combination of data-structures to achieve the above objective, and would you also be able to share the pseudo-code which supports your suggestion? </p>

<p>I have in mind a HashMap and ArrayList, but am confused how to actually begin with the implementation. Just that the logic isn't coming intuitively to me at this point. Your suggestions would be appreciated.</p>

<p>Thanks.</p>
","java, parsing, tree, nlp","<p>I've done something similar in Python recently.  I'll address just the part about data structures:</p>

<p>Each part of speech is either a list of parts of speech (a non-terminal), or just a single word (a terminal).  So you could create classes something like:</p>

<pre><code>enum Type {
    Sentence,
    NounPhrase,
    VerbPhrase,
    Conjunction,
    ...
};

interface PartOfSpeech { };

class NonTerminal implements PartOfSpeech {
    Type type;
    List&lt;PartOfSpeech&gt; subparts;
};

class Terminal implements PartOfSpeech {
    String word;
};
</code></pre>

<p>This is relatively untyped: your program needs to construct only valid strutures of these (e.g. no making a VerbPhrase consisting of a list of Sentences -- you can do it but it's meaningless!).</p>

<p>The alternative route is to define a more explicit type system.  So you could define classes for each type of part of speech, e.g.</p>

<pre><code>class VerbPhrase {
    Verb verb;
    AdverbPhrase adverb; /* optional */
    ...
};
</code></pre>

<p>Since there are several ways of making a verb phrase, you could instead have classes for each type:</p>

<pre><code>interface VerbPhrase { };

class IntransitiveVerbPhrase implements VerbPhrase {
    Verb verb;
    AdverbPhrase adverb; /* optional */
};

class TransitiveVerbPhrase implements VerbPhrase {
    Verb verb;
    AdverbPhrase adverb; /* optional */
    NounPhrase obj;
};
</code></pre>

<p>And so on.  The optimal degree of type explicitness at the Java level might not be obvious at the outset.  Start by writing something that handles simple sentences, and see how it feels.</p>

<p>In my case I created classes for each part of speech type, though each inherits from either a Terminal or Nonterminal.  Then I have rules for how you can construct each type.  This got kind of messy for some of them, e.g.</p>

<pre><code>add_rule(NounPhrase, [Noun])
add_rule(NounPhrase, [RelativeNounWord])
add_rule(NounPhrase, [Noun, AdjectiveClause])
add_rule(NounPhrase, [Article, Noun])
add_rule(NounPhrase, [Article, Adjectives, Noun])
add_rule(NounPhrase, [Article, Noun, AdjectiveClause])
add_rule(NounPhrase, [Article, Adjectives, Noun, AdjectiveClause])
add_rule(NounPhrase, [Adjectives, Noun])
add_rule(NounPhrase, [Adjectives, Noun, AdjectiveClause])
add_rule(NounPhrase, [Article, Adjectives, Noun])
...
</code></pre>

<p>That's code for saying, ""a NounPhrase is a Noun.  Or, it's a RelativeNoun.  Or, it's a Noun followed by an AdjectiveClause.  Or, etc."".  There's a general parser which attempts to apply rules to a list of words until it gets a tree.  (You can see the messy and undocumented code at <a href=""http://code.google.com/p/ejrh/source/browse/trunk/ircbot/nl.py"" rel=""nofollow"">http://code.google.com/p/ejrh/source/browse/trunk/ircbot/nl.py</a> .)</p>

<p>There's a certain amount of combinatorial explosion here.  It could possibly be improved by introducing new types of parts of speech, or just making some parts of it as optional: intead of having a rule for every possible combination of Article/Adjectives/AdjectiveClause/etc. present/absent, you could just make them optional.</p>
",829,1330911205
approach to solve two char column scramble of a text,"<p>I have a paragraph of text scrambled by columns of two chars. The purpose of my assignment is to unscramble it:</p>

<pre><code>|de|  | f|Cl|nf|ed|au| i|ti|  |ma|ha|or|nn|ou| S|on|nd|on|
|ry|  |is|th|is| b|eo|as|  |  |f |wh| o|ic| t|, |  |he|h |
|ab|  |la|pr|od|ge|ob| m|an|  |s |is|el|ti|ng|il|d |ua|c |
|he|  |ea|of|ho| m| t|et|ha|  | t|od|ds|e |ki| c|t |ng|br|
|wo|m,|to|yo|hi|ve|u | t|ob|  |pr|d |s |us| s|ul|le|ol|e |
| t|ca| t|wi| M|d |th|""A|ma|l |he| p|at|ap|it|he|ti|le|er|
|ry|d |un|Th|"" |io|eo|n,|is|  |bl|f |pu|Co|ic| o|he|at|mm|
|hi|  |  |in|  |  | t|  |  |  |  |ye|  |ar|  |s |  |  |. |
</code></pre>

<p>My current approach to find the right order of columns is trying to recursively find each column's best position according to a word occurrence count criteria.</p>

<p>The pseudo-code of the algorithm's core I have in mind would be: </p>

<pre><code>function unscramble(scrambledMatrix,indexOfColumnIveJustMoved)
    for each column on scrambledMatrix as currentIndex=&gt;currentColumn
       if (currentIndex!=indexOfColumnIveJustMoved)
           maxRepeatedWords=0;maxIndex=0;
           for (i=0;i&lt;numberOfColumnsOfScrambledMatrix;i++)
              repWordsCount=countRepWords(moveFromToOn(currentIndex,i,scrambledMatrix))
              if (maxRepeatedWords&lt;repWordsCount)
                  maxRepeatedWords=repWordsCount;
                  maxIndex=i;
              endif
           endfor
           if (maxIndex!=currentIndex)
               return unscramble(moveFromToOn(currentIndex,maxIndex,scrambledMatrix),maxIndex); //recursive call
           endif
       endif
    endfor
    return(scrambledMatrix); //returns the unscrambled matrix;
endfunction
</code></pre>

<p>The algorithm stops when no column is moved after iterating on each one. I'm guessing it should work for any language (though I'm only interested on a solution for english) as long as the writing is based on words formed by letters and the sample is big enough.</p>

<p>Any suggestions on any other approaches or improvements? I would like to know the best solution for this problem (probably a dictionary based one looking for occurrences of common words instead? How about rebuilding the algorithm to avoid recursion, would it be much faster?).</p>
","sorting, nlp, text-recognition","<p>A couple more ideas:</p>

<ul>
<li>Quotes, for each open quote there must be an end quote after it.</li>
<li>Capital letters, usually start of sentence or noun etc (any additional grammar rules that apply</li>
<li>Use a small enough dictionary to fit all in memory, and count the number of valid words in a particular arrangement.</li>
</ul>

<p>One way, although generally this approach is one of the most time consuming- is to use a genetic algorithm.</p>

<p>Lets say the current default arrangement of columns is </p>

<pre><code>|de|  | f|Cl|nf|ed|au| i|ti|  |ma|ha|or|nn|ou| S|on|nd|on|
[0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18] &lt;--- define this to be a chromosome
</code></pre>

<p>You could create a population of 100, 1000 w/e number of chromosomes that start off randomly assigned (keep in mind the 'random' assignment cannot have duplicate numbers and must be valid)</p>

<p>Then run a fitness function on each assignment, or multiple fitness functions if you would like to break it down that way. Start off with one super fitness function that assigns a fitness value to each assignment.</p>

<p>Only take the top 50% of chromosomes and move them onto the next generation, where you create 'children' chromosomes based on your choice of a crossover function and a probability of mutation- for this type of problem I recommend a very light crossover function (or none...) and a decent mutation rate. If you can find the columns that do not contribute to words/ the fitness function much then maybe flip those around.</p>

<p>Keep doing this for many generations and see how the top rated assignment looks like each generation, you would expect the valid to plateau at some point and that would be your correct assignment.</p>

<p>This approach could only be mildly better than brute force with fitness function, but it could also turn out to be pretty good.</p>

<p>One last idea: try to abstract away from 'first column, second column' and assign the columns into chunks that form words, because just because [1,4,6....] turns out to form ""the"" ""him"" ""her"" etc, doesnt mean it belongs right in the beginning.</p>

<p>I have a different approach that I kind of like better, I think a Dynamic Algorithm would be better suited for this.</p>

<p><strong>EDIT: Another Approach</strong></p>

<p>Again based on the dictionary approach, but you would focus on choosing the first few columns before the rest, and if it falls apart and you are not getting words in any particular row it means your earlier selections were wrong and you will need to backtrack.</p>

<p>Select row 1.. well chances are there are not too many words here but you will narrow yourself down to a subset of your dictionary- the subset that has words that start with the chars in your first column.</p>

<p>Now you have a row that works, select an adjacent row to the right. If it either forms full words or still has valid words possible (given no space is present signifying end of word). 
Repeat.</p>

<p>If no adjacent rows are possible given your previous choices, backtrack one row to the left, and dont select the same thing again.</p>

<p>The weakness here is that your dictionary would need to contain all the words in your sentence, as well as all variants of the words. You might need to come up with a heuristic similar to a fitness function that says, ""90% of words match, so this is still a valid attempt..."" or something of that sort.</p>
",175,1329351523
Increase speed of JAVA program for Machine Learning,"<p>I am doing machine learning in java using GATE Learning. I have a huge data set of documents to learn from. While using netbeans, I was getting java heap space error. So I provided 1600MB in the -Xmx parameter. Now, I do not get the heap space error but it takes ample of time to run!! (runs for 90 mins and I had to stop the process since I lost my patience!).</p>

<p>I do not understand whether I should increase my RAM(currently 4GB) or upgrade my OS(currently XP SP3, I have heard vista and win 7 better utilize RAM and Processor) or upgrade my processor(currently Dual Core E5500 2.80 GHz)?</p>

<p>Please throw some insight into what I can do to make this process run faster!</p>

<p>Thanks Rishabh</p>
","java, operating-system, nlp, ram, heap-memory","<p>Before you can answer what will make it run faster, you have to find the bottleneck. </p>

<p>I'm not very familiar with Windows, but there is some sort of system load monitoring widget, IIRC. </p>

<p>What I would do is as follows:</p>

<ul>
<li>Create some datasets of increasing sizes (more documents) </li>
<li>Run your program against those datasets</li>
<li>On each run, work out if the CPU maxes out, or if the memory maxes out and starts swapping, or if the whole thing is IO bound</li>
</ul>

<p>Then fix the one that is causing the problem.</p>

<p>Just for context, it's not that unusual for ML algorithms to take a long time to run on large data sets. You can use the above approach to plot out the run time as the size of the input datasets increase, at least then you'll know if your program would have stopped in 100 minutes or 100 centuries.</p>
",655,1318319807
How should I weight an N-gram sentence generator so that it doesn&#39;t favor short sentences?,"<p>I'm playing around with writing an n-gram sentence comparison/generation script. The model heavily favors shorter sentences, any quick suggestions on how I might weight it more towards longer sentences?</p>
","artificial-intelligence, nlp","<p>Assuming that you compute a score for each n-gram and rank the ngrams by these scores, you can adjust the scores of these n-grams by applying a different scalar weight for each value of <code>n</code>, e.g., <code>v = &lt;0.1, 0.2, 0.5, 0.9, 1.0&gt;</code>, where <code>v[0]</code> would be applied to an n-gram where <code>n == 1</code>.  Such a vector could be determined from a larger text corpus by measuring the relative frequencies of a set of representative solution n-grams (e.g., if you are looking for sentences, then calculate <code>n</code> for each sentence, count the frequencies of each value of <code>n</code>, and create a <a href=""http://en.wikipedia.org/wiki/Probability_distribution"" rel=""nofollow"">probability distribution</a> from that data.</p>
",384,1324422339
Part-of-speech tagger in PHP?,"<p>I am looking for a simple part-of-speech library or code that I can download. My criteria is that it must be simple to use and free is possible.</p>

<p>Do you know such a library ?</p>
","php, nlp, part-of-speech",,3184,1330034556
Identifying the components in a English sentence that do not make sense,"<p>I'm wondering is there an algorithm or a library which helps me identify the components in an English which has no meaning? e.g., <strong>very serious</strong> grammar error? If so, could you explain how it works, because I would really like to implement that or use that for my own projects.</p>

<p>Here's a random example:</p>

<p>In the sentence: ""I closed so etc page hello the door."" </p>

<p>As a human, we can quickly identify that [so etc page hello] does not make any sense. Is it possible for a machine to point out that the string does not make any sense and also contains grammar errors?</p>

<p>If there's such a solution, how precise can that be? Is it possible, for example, given a clip of an English sentence, the algorithm returns a measure, indicating how meaningful, or correct that clip is? Thank you very much!</p>

<p>PS: I've looked at CMU's link grammar as well as the NLTK library. But still I'm not sure how to use for example link grammar parser to do what I would like to do as the if the parser doesn't accept the sentence, I don't know how to tweak it to tell me which part it is not right.. and I'm not sure whether NLTK supported that.</p>

<p>Another thought I had towards solving the problem is to look at the frequencies of the word combination. Since I'm currently interested in correcting very serious errors only. If I define the ""serious error"" to be the cases where words in a clip of a sentence are rarely used together, i.e., the frequency of the combo should be much lower than those of the other combos in the sentence. </p>

<p>For instance, in the above example: [so etc page hello] these four words really seldom occur together. One intuition of my idea comes from when I type such combo in Google, no related results jump out. So is there any library that provides me such frequency information like Google does? Such frequencies may give a good hint on the correctness of the word combo.</p>
","artificial-intelligence, nlp, machine-learning, nltk, information-retrieval","<p>I think that what you are looking for is a <a href=""http://en.wikipedia.org/wiki/Language_model"" rel=""nofollow"">language model</a>. A language model assigns a probability to each sentence of <code>k</code> words appearing in your language. The simplest kind of language models are n-grams models: given the first <code>i</code> words of your sentence, the probability of observing the <code>i+1</code>th word only depends on the <code>n-1</code> previous words.</p>

<p>For example, for a bigram model (<code>n=2</code>), the probability of the sentence <code>w1 w2 ... wk</code> is equal to</p>

<pre><code>P(w1 ... wk) = P(w1) P(w2 | w1) ... P(wk | w(k-1)).
</code></pre>

<p>To compute the probabilities <code>P(wi | w(i-1))</code>, you just have to count the number of occurrence of the bigram <code>w(i-1) wi</code> and  of the word <code>w(i-1)</code> on a large corpus.</p>

<p>Here is a good tutorial paper on the subject: <a href=""http://research.microsoft.com/en-us/um/people/joshuago/longcombine.pdf"" rel=""nofollow"">A Bit of Progress in Language Modeling</a>, by Joshua Goodman.</p>
",873,1329881866
"It&#39;s probably simpler in awk, but how can I say this in Python?","<p>I have:</p>

<p>Rutsch is for rutterman ramping his roe</p>

<p>which is a phrase from Finnegans Wake. The epic riddle book is full of leitmotives like this, such as 'take off that white hat,' and 'tip,' all which get mutated into similar sounding words depending on where you are in the book itself. All I want is a way to find obvious occurrences of this particular leitmotif, IE</p>

<p>[word1] is for [word2] [word-part1]ing his [word3]</p>
","python, nlp, nltk","<pre><code>import re
# read the book into a variable 'text'
matches = re.findall(r'\w+ is for \w+ \w+ing his \w+', text)
</code></pre>
",303,1284039778
Using Sentiment Analysis to Detect Contradictory Arguments?,"<p>I don't have much background in sentiment analysis or natural language processing at all, but I have been reading a bit about it in my spare time.  I would like to conduct and experiment to analyze forum threads/comments such as reddit, digg, blogs, etc.  I'm particularity interested in doing something like counting the number of for, against, and neutral comments for threads of heated religious and political debates.  Here's what I am thinking.</p>

<p>1) Find a thread that the original poster has defined a touchy political or religious topic.</p>

<p>2) For each comment categorize it as supporting the original poster or otherwise taking a contradicting or neutral stance.  </p>

<p>3) Compare various mediums with the numbers of for or against arguments to determine what platforms are good ""debate platforms"" (i.e. balanced argument counts).</p>

<p>One big problem that I'm anticipating is that heated topics will invoke strong reactions from both supporting and contradicting parties so a simple happy/sad sentiment analysis won't cut it.  I'm just sort of interested in this project for my own curiosities, so if anyone knows of similar research or utilities to conduct this experiment I'd be interested to hear more.  </p>

<p>Can someone recommend a good sentiment analysis, word dictionary, training set, etc. for this task?</p>
","algorithm, artificial-intelligence, nlp, text-processing, sentiment-analysis",,745,1329664964
Retrival of character string from email addresses,"<p>I am trying to come up with a way in flex through which I can extract only the character series from a list of email addresses.
The email addresses are valid. Example</p>

<pre><code>Input: a12l3i.ce@example.com
output: alice
</code></pre>

<p>So far, I came with the regular expression <code>([^@]+)</code> through which I can extract the username part of the email. However, without the use of input/unput (which is not allowed in flex) can we retrive the text from the email username ? If yes, can you suggest me a regular expression or any other method for that ? (Perhaps a grammar in Bison)</p>
","c, regex, bison, lex, nlp","<p>You have to specify the transformation from <code>a12l3i.ce</code> to <code>alice</code> as it isn't very self-explanatory.  If you consider <code>[a-zA-Z]</code> as ""names"":</p>

<pre><code>%option noyywrap
%%
[a-zA-Z]+|\n   ECHO;
@.*\n          putchar ('\n');
.              /* ignore */
</code></pre>

<p>will output those.</p>
",80,1329649879
Finding interrogative sentences,"<p>I need to detect whether a given sentence have interrogative characteristics? Is there any available gem(ROR) that implements this with NLP libraries or any current state of art implementation. </p>
","ruby-on-rails, nlp",,321,1328099130
Max edit distance and suggestion based on word frequency,"<p>I need a spell checker with the following specification:  </p>

<ul>
<li>Very scalable.</li>
<li>To be able to set a maximum edit distance for the suggested words.</li>
<li>To get suggestion based on provided words frequencies (most common word first).</li>
</ul>

<hr>

<p>I took a look at Hunspell:<br>
I found the parameter MAXDIFF in the man but doesn't seem to work as expected. Maybe I'm using it the wrong way</p>

<p>file <strong>t.aff</strong>:  </p>

<pre><code>MAXDIFF 1 
</code></pre>

<p>file <strong>dico.dic</strong>:  </p>

<pre><code>5  
rouge  
vert  
bleu  
bleue  
orange  
</code></pre>

<p>-  </p>

<pre><code>NHunspell.Hunspell h = new NHunspell.Hunspell(""t.aff"", ""dico.dic"");
List&lt;string&gt; s = h.Suggest(""bleuue"");
</code></pre>

<p>returns the same thing <code>t.aff</code> being empty or not:</p>

<pre><code>bleue
bleu
</code></pre>
","c#, c++, nlp, spell-checking, hunspell","<p>We decided to use <code>Apache Solr</code>, which exactly fulfills our needs.<br>
<a href=""http://wiki.apache.org/solr/SpellCheckComponent#spellcheck"" rel=""nofollow"">http://wiki.apache.org/solr/SpellCheckComponent#spellcheck</a></p>
",990,1304344293
How to implement LSA (Latent semantic analysis) in Python?,"<p>How to implement Latent semantic analysis in Python and compare corps of text against query using Cosine similarity ?</p>
","python, math, nlp",,7911,1329119046
How to implement LSA (Latent semantic analysis) in Python?,"<p>How to implement Latent semantic analysis in Python and compare corps of text against query using Cosine similarity ?</p>
","python, math, nlp",,7911,1329119046
Python vs Java for natural language processing,"<p>I have been working on java to find the similarity between two documents. I prefer finding semantic similarity , but havent made efforts to find it yet . I am using the following approach . </p>

<ol>
<li>Extract terms / tokens (I am using JAWS with wordnet to remove synonyms thus improves the similarities )</li>
<li>make a term document matrix </li>
<li>LSA </li>
<li>Cosine similarity </li>
</ol>

<p>When i was looking at few stackoverflow pages , i got quite a few links to python implementations. </p>

<p>I would like to know if python is a better language to find the text similarity and would also like to know if i can find semantic similairty between two documents in python  </p>
","java, python, text, nlp, similarity","<p>Assuming you don't have a platform restriction that would constrain your choice of language, you should choose your language based on whatever you're most comfortable with (I prefer Python myself), and which has the best libraries for your application (as @GregHewgill pointed out the Python tools (<a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">Natural Language Toolkit</a>) are mature and comprehensive).</p>

<p>So while I personally would choose Python, it's really something you have to choose for yourself.</p>

<p>== EDIT == </p>

<p>This <a href=""https://stackoverflow.com/questions/870460/java-is-there-a-good-natural-language-processing-library"">question</a> about Java NLP libraries might help you decide if you can use Java for your analysis; the top answer has a list you can investigate.  Without more information about your problem set, I can't provide more specific advice.</p>
",2776,1329108837
Automatic negation of words,"<p>Consider the following statements</p>

<pre><code>We are not talking about a well established company in the NASDAQ
I will not initiate any trades until those clowns hammer out a deal
</code></pre>

<p>I am writing a simple Naive Bayes classifier, basically marking a training set of statements by hand (as either positive or negative sentiment) and storing the words that make up the statement accordingly.</p>

<p>Problem: if I mark both of these statements as having a negative sentiment, the words ""well"", ""established"" (statement 1) and ""any"", ""until"" (statement 2) would be indivudually marked as negatives. Whereas in another case (i.e., ""This company is performing well""), the same words (""well"" in this case) would be marked as a positive, making the sum of sentiment for ""well"" -1 + 1 = 0. I would overcome this by tagging these words as negated words, for example:</p>

<pre><code>We are talking about a not-well not-established company in the NASDAY.
I will initiate not-anymore trades not-until those clowns hammer out a deal
</code></pre>

<p>Is there a standard or best way of tagging these kinds of words (I don't even know if they are of a same group of words)? Obviously, tagging ""company"" wouldn't make sense ""not-company"" doesn't hold any sentimental value. I have (in PHP) made a function that would tag all words after the negation word (not, no, couldn't, etc) but many of them didn't make real sense afterwards (such as ""not-company"", ""not-NASDAQ"", ""not-clowns"").</p>

<p>Since English is not my mother language, I'm asking you if there's a common name for the words I have marked here and if what I want is (rudimentary) possible. I am aware that there are a lot of exceptions possible (double negations etc.) but I do not want to go into that; I believe if this would be possible, it would cover a lot of ground.</p>
","php, nlp, sentiment-analysis","<p>Taking from your example,</p>

<pre><code>We are talking about a not-well not-established company in the NASDAY.
I will initiate not-anymore trades not-until those clowns hammer out a deal
</code></pre>

<p>I think you want to tag <strong>adjectives</strong> (and their variants) so they would be negated, right? It is called ""part of speech tagging"". There is a good tutorial with PHP <a href=""http://phpir.com/part-of-speech-tagging"" rel=""nofollow"">here</a>.</p>

<p>You need a dictionary (or list of word) of common English adjectives, however.</p>
",256,1329000230
Named Entity Recognition in political domain,"<p>For my research project in text classification, I need to identify named entities in the political domain (using NER to improve the text classification). </p>

<p>Where can I find the named entities in the political domain, so that I can train the classifier with?</p>

<p>If you know of any other dataset than the political domain let me know.</p>

<p>Thanks!</p>
","text, nlp, classification, named-entity-recognition",,498,1328632036
Ruby-wordnet - can not create lexicon,"<p>I am trying out Ruby-Wordnet. I can require it in console, but then when I try to create a lexicon, I get an error:</p>

<pre><code>   $ lex = WordNet::Lexicon.new
    TypeError: can't convert Symbol into Integer
    from /Users/user1/.rvm/gems/ruby-1.9.3-p0/gems/wordnet-0.0.5/lib/wordnet/lexicon.rb:93:in `%'
</code></pre>
","ruby, nlp, wordnet",,216,1325029188
Clean and natural scripting functionality without parsing,"<p>I'm experimenting with creating a semi-natural scripting language, mostly for my own learning purposes, and for fun. The catch is that it needs to be in native C#, no parsing or lexical analysis on my part, so whatever I do needs to be able to be done through normal syntactical sugar.</p>

<p>I want it to read somewhat like a sentence would, so that it is easy to read and learn, especially for those that aren't especially fluent with programming, but I also want the full functionality of native code available to the user.</p>

<p>For example, in the perfect world it would look like a natural language (English in this case):</p>

<pre><code>When an enemy is within 10 units of player, the enemy attacks the player
</code></pre>

<p>In C#, allowing a sentence like this to actually do what the scripter intends would almost certainly require that this be a string that is run through a parser and lexical analyzer. My goal isn't that I have something this natural, and I don't want the scripter to be using strings to script. I want the scripter to have full access to C#, and have things like syntax highlighting, intellisense, debugging in IDE, etc. So what I'm trying to get it something that reads easily, but is in native C#. A couple of the major hurdles that I don't see a way to overcome is getting rid of periods <code>.</code>, commas <code>,</code>, and parentheses for empty methods <code>()</code>. For example, something like this is feasible but doesn't read very cleanly:</p>

<pre><code>// C#
When(Enemy.Condition(Conditions.isWithinDistance(Enemy, Player, 10))), Event(Attack(Enemy, Player))
</code></pre>

<p>Using a language like Scala you can actually get much closer, because periods and parentheses can be replaced by a single whitespace in many cases. For example, you could take the above statement and make it look something like this in Scala:</p>

<pre><code>// Scala
When(Enemy is WithinDistance(Player, 10)) =&gt; Then(Attack From(Enemy, Player))
</code></pre>

<p>This above code would actually compile assuming you setup your engine to handle it, in fact you might be able to coax further parentheses and commas out of this. Without the syntactical sugar in the above example it would be more like this, in Scala:</p>

<pre><code>// Scala (without syntactical sugar)
When(Enemy.is(WithinDistance(Player, 10)) =&gt; Then(Attack().From(Enemy, Player))
</code></pre>

<p>The bottom line is I want to get as close as possible to something like the first scala example using native C#. It may be that there is really nothing I can do, but I'm willing to try any tricks that may be possible to make it read more natural, and get the periods, parentheses, and commas out of there (except when they make sense even in natural language).</p>

<p>I'm not as experienced with C# as other languages, so I might not know about some syntax tricks that are available, like macros in C++. Not that macros would actually be a good solution, they would probably cause more problems then they would solve, and would be a debugging nightmare, but you get where I'm going with this, at least in C++ it would be feasible. Is what I'm wanting even possible in C#?</p>

<p>Here's an example, using LINQ and Lambda expressions you can sometimes get the same amount of work done with fewer lines, less symbols, and code the reads closer to English. For example, here's an example of three collisions that happen between pairs of objects with IDs, we want to gather all collisions with the object that has ID 5, then sort those collisions by the ""first"" ID in the pair, and then output the pairs. Here is how you would do this without LINQ and/or Lambra expressions:</p>

<pre><code>struct CollisionPair : IComparable, IComparer
{
    public int first;
    public int second;

    // Since we're sorting we'll need to write our own Comparer
    int IComparer.Compare( object one, object two )
    {
        CollisionPair pairOne = (CollisionPair)one;
        CollisionPair pairTwo = (CollisionPair)two;

        if (pairOne.first &lt; pairTwo.first)
            return -1;
        else if (pairTwo.first &lt; pairOne.first)
            return 1;
        else
            return 0;
    }

    // ...and our own compable
    int IComparable.CompareTo( object two )
    {
        CollisionPair pairTwo = (CollisionPair)two;

        if (this.first &lt; pairTwo.first)
            return -1;
        else if (pairTwo.first &lt; this.first)
            return 1;
        else
            return 0;
    }
}

static void Main( string[] args )
{           
    List&lt;CollisionPair&gt; collisions = new List&lt;CollisionPair&gt;
    {
        new CollisionPair { first = 1, second = 5 },
        new CollisionPair { first = 2, second = 3 },
        new CollisionPair { first = 5, second = 4 }
    };

    // In a script this would be all the code you needed, everything above
    // would be part of the game engine   
    List&lt;CollisionPair&gt; sortedCollisionsWithFive = new List&lt;CollisionPair&gt;();
    foreach (CollisionPair c in collisions)
    {
        if (c.first == 5 || c.second == 5)
        {
            sortedCollisionsWithFive.Add(c);
        }
    }
    sortedCollisionsWithFive.Sort();

    foreach (CollisionPair c in sortedCollisionsWithFive)
    {
        Console.WriteLine(""Collision between "" + c.first +
                          "" and "" + c.second);
    }
}
</code></pre>

<p>And now the same example with LINQ and Lambda. Notice in this example we don't have to both with making <code>CollisionPair</code> both <code>IComparable</code> and <code>IComparer</code>, and don't have to implement to the <code>Compare</code> and <code>CompareTo</code> methods:</p>

<pre><code>struct CollisionPair
{
    public int first;
    public int second;
}

static void Main( string[] args )
{           
    List&lt;CollisionPair&gt; collisions = new List&lt;CollisionPair&gt;
    {
        new CollisionPair { first = 1, second = 5 },
        new CollisionPair { first = 2, second = 3 },
        new CollisionPair { first = 5, second = 4 }
    };

    // In a script this would be all the code you needed, everything above
    // would be part of the game engine
    (from c in collisions 
    where ( c.first == 5 || c.second == 5 )
    orderby c.first select c).ForEach(c =&gt;
        Console.WriteLine(""Collision between "" + c.first +
                          "" and "" + c.second));
}
</code></pre>

<p>In the end we're left with a LINQ and Lambda expression that read closer to natural language, and are much less code for both a game engine and for the script. These kinds of changes are really what I'm looking for, but obviously LINQ and Lambda are both limited to specific syntax, not something as generic as I would like in the end.</p>
","c#, nlp","<p>Another approach would be to use FluentInterface ""pattern"", implement something like:</p>

<pre><code>When(enemy).IsWithin(10.units()).Of(player).Then(enemy).Attacks(player);
</code></pre>

<p>If you make the functions like When, IsWithin, Of, Then return some interfaces, then you will be able easily add new extension methods to expand your rules language.</p>

<p>For example let's take a look at function Then:</p>

<pre><code>public IActiveActor Then(this ICondition condition, Actor actor) {
   /* keep the actor, etc */
}

public void Attacks(this IActiveActor who, Actor whom) {
   /* your business logic */
}
</code></pre>

<p>In the future it would be easy to implement another function, say RunAway() without changing anything in your code:</p>

<pre><code>public void RunAway(this IActiveActor who) {
    /* perform runaway logic */
}
</code></pre>

<p>so it with this little addition you will be able to write:</p>

<pre><code>When(player).IsWithin(10.units()).Of(enemy).Then(player).RunAway();
</code></pre>

<p>Same for conditions, assuming When returns something like ICheckActor, you can introduce new conditions by simply defining new functions:</p>

<pre><code>public ICondition IsStrongerThan(this ICheckActor me, Actor anotherGuy) {
    if (CompareStrength(me, anotherGuy) &gt; 0)
       return TrueActorCondition(me);
    else
       return FalseActorCondition(me);
}
</code></pre>

<p>so now you can do:</p>

<pre><code>When(player)
  .IsWithin(10.units()).Of(enemy)
  .And(player).IsStrongerThan(enemy)
  .Then(player)
  .Attacks(enemy);
</code></pre>

<p>or</p>

<pre><code>When(player)
  .IsWithin(10.units()).Of(enemy)
  .And(enemy).IsStrongerThan(player)
  .Then(player)
  .RunAway();
</code></pre>

<p>The point is that you can improve your language without experiencing heavy impact on the code you already have.</p>
",246,1328422985
language detector,"<p>I want a java code which reads a text inside a document and say that it is in which language (English, Spanish, ...). The format of the document is not important. I want the output to be for example : ""This document is in Spanish"". Please guide me in this way and give me a sample code for it. </p>
","java, nlp",,860,1328545022
Override handling for basic/primitive types within a scope,"<p>I'd like to be able to make code that looks like this:</p>

<pre><code>30, 60, 90
</code></pre>

<p>And have it make a <code>List&lt;int&gt;</code> with those numbers in it. The problem is that in order to do this I believe I would need to be able to override how an <code>int</code> works within this scope. Is there any way that I could have my own ""Int"" class that could be used within a specific scope whenever a primitive constant like <code>30</code> was entered? If I'm unable to override <code>int</code> then the class must be used explicitly in this scope I'll end up with something like this:</p>

<pre><code>Int(30),Int(60),Int(90)
</code></pre>

<p>Which is still less code than the standard way:</p>

<pre><code>var nums = new List&lt;int&gt;(); nums.Add(30); nums.Add(60); nums.Add(90);
</code></pre>

<p>or even:</p>

<pre><code>var nums = new List&lt;int&gt;{ 30, 60, 90 };
</code></pre>

<p>I'm playing around with my own scripting language, for education purposes only, not intended for anyone else to use. I'm trying to see how far I can simplify syntax to read closer to a natural language. Is something like in first code block at the top of this question possible? If this is possible, can I extend it to other primitive types?</p>

<p>EDIT: I want something that is pure C#, I'm not doing my own parsing, or writing my own compiler. I'm curious if somehow I can have a primitive type like a constant int be handled through my own type instead? I don't think it's possible, but if not at least StackOverflow is good at giving alternatives I might not have thought about.</p>

<p>Ok, here's what I've come up with so far to get me closer:</p>

<pre><code>public static class Values
{
    static public List&lt;int&gt; Make(params int[] vals)
    {
        return vals.ToList();
    }
}
</code></pre>

<p>This lets me do something like:</p>

<pre><code>var nums = Values.Make(30, 60, 90);
</code></pre>

<p>Of course, this isn't much shorter than <code>var nums = new List&lt;int&gt;{ 30, 60, 90 };</code> but it lets me abstract the int away so they could also do:</p>

<pre><code>var nums = Values.Make(30.0f, 60.0f, 90.0f);
</code></pre>

<p>So long as I make a handler for a list of floats. Unfortunately if I make a generic method, like this:</p>

<pre><code>public static List&lt;T&gt; Make&lt;T&gt;(params T[] vals) where T : IComparable&lt;T&gt;
{
    return vals.ToList();
}
</code></pre>

<p>Then it make the syntax of using it longer:</p>

<pre><code>var nums = Values.Make&lt;float&gt;(30.0f, 60.0f, 90.0f);
</code></pre>
","c#, nlp, overriding","<p>If you're on C# 4.0 you can simplify it to:</p>

<pre><code>var nums = new[] { 30, 60, 90 };
</code></pre>

<p>which will create an <code>int[]</code>. This can also be used with other types:</p>

<pre><code>var floats = new[] { 30.0f, 60.0f, 90.0f };
</code></pre>

<p>If you really need a <code>List&lt;T&gt;</code> then you can do:</p>

<pre><code>var nums = new[] { 30, 60, 90 }.ToList();
</code></pre>
",387,1328592856
Detecting syllables in a word containing non-alphabetical characters,"<p>I'm implementing readability test and have implemented simple algorithm of detecting sylables.
Detecting sequences of vowels I'm counting them in words, for example word ""shoud"" contains one sequence of vowels which is 'ou'. Before I'm counting them i'm removing suffixes like -les, -e, -ed (for example word ""like"" contains one syllable but two sequences of vowels, so this method works).</p>

<p>But...
Consider these words / sequences: </p>

<ul>
<li>x-ray  (it contains two syllables)</li>
<li>I'm  (One syllable, maybe I may use removal of all apostrophes in the text?)</li>
<li>goin' </li>
<li>I'd've </li>
<li>n' (for example Pork n' Beans)</li>
<li>3rd (how to treat this ?)</li>
<li>12345 </li>
</ul>

<p>What to do with special characters? Remove them all? It will be ok for most of words, but not with ""n'"" and ""x-ray"". And how treat cyphers.</p>

<p>These are special cases of words but I'll be very glad to see some experience or ideas in this subject.</p>
","nlp, spell-checking, readability, hyphenation","<p>I'd advise you to first determine how much of your data consists of these kinds of words and how much it matters to your program's overall performance. Also compile some statistics of which kinds occur most.</p>

<p>There's no simple correct solution for this problem, but I can suggest a few heuristics:</p>

<ul>
<li>A <code>'</code> between two consonants (<code>shouldn't</code>) seems to mark the elision of a syllable</li>
<li>A <code>'</code> with a vowel or word boundary on one side (<code>I'd</code>, <code>goin'</code>) seems not to do so (but note that <code>goin'</code> is still two syllables)</li>
<li>Any word, including <code>n'</code> is at least one syllable long</li>
<li>Dashes (<code>-</code>) may be handled by treating the text on both sides as separate words</li>
</ul>

<p><code>3rd</code> can be handled by code that writes ordinals out as words, or by simpler heuristics.</p>
",218,1287250158
exracting words using nltk,"<p>from the website <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch05.html"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/book/ch05.html</a> i've come to know about splitting tagged words from a tagged corpus.</p>

<p>The code in the website:</p>

<pre><code>&gt;&gt;&gt; sent = '''
... The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN
... other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC
... Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS
... said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB
... accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT
... interest/NN of/IN both/ABX governments/NNS ''/'' ./.
... '''
&gt;&gt;&gt; [nltk.tag.str2tuple(t) for t in sent.split()]
  [('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'),
  ('on', 'IN'), ('a', 'AT'), ('number', 'NN'), ... ('.', '.')]
</code></pre>

<p>here i get a list of tagged words. What i want is a list containing only the words. For example:</p>

<pre><code>  [('The'), ('grand'), ('jury')...
</code></pre>

<p>instead of </p>

<pre><code>  ('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN')...
</code></pre>

<p>Any suggestion how can i obtain this?</p>

<p>Thanks in advance.</p>
","python, nlp, nltk","<p>I'm not an <code>nltk</code> expert but you can directly pick the first tuple element with:</p>

<pre><code>[nltk.tag.str2tuple(t)[0] for t in sent.split()]
</code></pre>

<p>That will give you a list of all the words:</p>

<pre><code>['The', 'grand', 'jury'...
</code></pre>

<p>What you're asking is a little confusing, becuase in your output example every element is wrapped inside a 1-tuple, I don't really see the point for that.</p>

<p><strong>Edit:</strong> Even though as larsman pointed out: <code>('The',)</code> would be a 1-tuple, while <code>('The') == 'The'</code>.</p>
",438,1328439632
object in python,"<p>I'm trying to exract the proper noun from executive sentences of a tagged corpus. When i run the code it only shows the list of the proper noun of 1st couple of sentences and then it stops and gives the following error:</p>

<pre><code>          Traceback (most recent call last):
          print(list(words4))
          TypeError: 'NoneType' object is not iterable
</code></pre>

<p>The code:</p>

<pre><code>      #extracting 
      def noun(sen):
          for t in range(len(sen)):
              if (sen[t].split('/')[1].lower() == 'np'):
                  w=sen[t].split('/')
                  return w

      if __name__ == '__main__':
          import nltk
          from nltk.corpus import brown
          f = brown.raw('ca01')
          print f
          mylist = []
          #splitting 
          sentences = splitParagraphIntoSentences(f)
          for s in sentences:
              mylist.append(s.strip())

          for i in mylist:
              print i

          for s in range(len(mylist)):
              words1 = mylist[s].split()
              words2 = mylist[s+1].split()
              words3= noun(words1)
              words4= noun(words2)
              print(list(words3))
              print(list(words4))
</code></pre>

<p>how to solve this error. Thanks.</p>
","python, nlp, nltk","<p>You are making two errors here:</p>

<ol>
<li><p>""NP"" isn't the only noun-phrase that NLTK uses. It also uses ""NNP"" and some others. You'll need to look into the tagset to figure out what there is. But I don't know what you're aiming for right now, so looking for just ""np"" may not be entirely wrong.</p></li>
<li><p>Your <code>noun(sen)</code> function doesn't <strong>always</strong> return something. It only returns if it find a word tagged ""np"" in your sentence. Now while it is a reasonable assumption that every sentence has a noun, consider that NLTK is not state-of-the-art and may therefore miss out on some nouns from time to time. Also, like I mentioned in (1), NLTK may also tag words with ""NNP"", so watch out.</p></li>
</ol>

<p>To rectify this problem, try the following <code>noun(sen)</code> function (slightly modified version of yours:</p>

<pre><code>def noun(sen):
    for t in range(len(sen)):
        if (sen[t].split('/')[1].lower() == 'np'):
            w=sen[t].split('/')
            return w
    return []
</code></pre>

<p>PS: please try to not use single-space indenting. It's not the best style and makes the eyes hurt</p>
",177,1328295901
Is there a natural language parser for dates/times in ColdFusion?,"<p>Is there a <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""noreferrer"">natural language parser</a> for date/times in ColdFusion?</p>
","datetime, coldfusion, nlp","<p>There's a (reportedly -- I've not used it) good one for Java called <a href=""https://github.com/samtingleff/jchronic"" rel=""nofollow noreferrer"">JChronic</a> -- a port of the Ruby Chronic date parser.  You could try using it.</p>

<p>It hasn't been updated since 2006, but should still be useful.</p>
",1328,1245178554
Natural language interface to semantic web,"<p>we are working on an educational project one of it's components is a  smart search engine that is placed on top sparql end points like DBpedia , imdb  ...etc </p>

<p>we explored some related work in how to make NLI to semantic web </p>

<ul>
<li><p>Ginseng: <a href=""http://www.ifi.uzh.ch/pax/uploads/pdf/publication/106/Bernstein_JenaConf_2006.pdf"" rel=""nofollow"">A Guided Input Natural
Language Search Engine for Querying
Ontologies</a></p></li>
<li><p><a href=""http://www.ifi.uzh.ch/pax/uploads/pdf/publication/843/Kaufmann_ISWC2007.pdf"" rel=""nofollow"">How Useful Are Natural Language
Interfaces to the Semantic Web for
Casual End-Users?</a>-</p></li>
<li><p>The author of the second paper wrote
her entire PhD thesis about that
topic: Talking to the Semantic Web –
<a href=""http://www.ifi.uzh.ch/pax/uploads/pdf/publication/1384/Kaufmann_2007.pdf--"" rel=""nofollow"">Natural Language Query Interfaces
for Casual End-users</a>-</p></li>
</ul>

<p>still we face some problems in implementations because the papers are too broad and no source code is available . so i wonder if there are some open sources  or Documented projects to learn how to build similar projects . 
or if  i can use similar techniques of  NLI to SQL databases , if so  from where to start.     </p>
","nlp, sparql, semantic-web",,1297,1327963721
extracting from a tagged corpus in python,"<p>hi i'm trying exract proper noun from a tagged corpus, lets say for example- from the nltk tagged corpus brown i'm trying to extract the words only tagged with ""NP"".</p>

<p>my code:</p>

<pre><code>  import nltk
  from nltk.corpus import brown
  f = brown.raw('ca01')
  print nltk.corpus.brown.tagged_words()
  w=[nltk.tag.str2tuple(t) for t in f.split()]
  print w
</code></pre>

<p>but it is not showing the words istead it is showing only </p>

<p>[]</p>

<p>sample output:</p>

<pre><code>    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]
    []
</code></pre>

<p>why is it??</p>

<p>thanks.</p>

<p>I i only prit f.split()..then i get</p>

<pre><code>             [('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (""Atlanta's"", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (""''"", ""''""), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.'), ('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL').....
</code></pre>
","python, nlp, nltk","<p>Can't really tell from what you've given us, but have you tried going into the problem step by step? It seems that under no circumstances does <code>t.split('/')[1] == 'NP'</code> evaluate to True. So you should start by:</p>

<ol>
<li>print/debug to see what exactly does <code>f.split()</code> contain</li>
<li>make sure your condition is actually the correct one, from the little sample of output you gave there it looks to me you are looking more for: <code>if t.split('/')[1].startswith('NP')</code> but can't really tell.</li>
</ol>

<p>EDIT:</p>

<p>Ok, first if that is really what <code>f.split()</code> prints to you then you should get an exception sicne <code>t</code> is a tuple and a tuple doesnt have a <code>split()</code> method. So you made me curious and I installed <code>nltk</code> and downloaded the 'brown' corpus and tried your code. Now first, to me if I do:</p>

<pre><code>  import nltk
  from nltk.corpus import brown
  f = brown.raw('ca01')
  print f.split()

  ['The/at', 'Fulton/np-tl', 'County/nn-tl', 'Grand/jj-tl', 'Jury/nn-tl', 'said/vbd', 'Friday/nr', 'an/at', 'investigation/nn', 'of/in', ""Atlanta's/np$"", 'recent/jj', 'primary/nn', 'election/nn', 'produced/vbd', '``/``', 'no/at', 'evidence/nn', ""''/''"", 'that/cs', 'any/dti', 'irregularities/nns', 'took/vbd', 'place/nn', './.', 'The/at', 'jury/nn', 'further/rbr', 'said/vbd', 'in/in', 'term-end/nn', 'presentments/nns', 'that/cs', 'the/at', 'City/nn-tl', 'Executive/jj-tl', 'Committee/nn-tl', ',/,', 'which/wdt', 'had/hvd', 'over-all/jj', 'charge/nn', 'of/in', 'the/at', 'election/nn', ',/,', '``/``', 'deserves/vbz', 'the/at', 'praise/nn', 'and/cc', 'thanks/nns', 'of/in', 'the/at', 'City/nn-tl' .....]
</code></pre>

<p>So I have no ideea what you did there to get the result but it was incorrect. Now as you can see from the groups, the second part of the word is in lowercase, that is why your code failed. So if you do:</p>

<pre><code>w=[nltk.tag.str2tuple(t) for t in f.split() if t.split('/')[1].lower() == 'np']
</code></pre>

<p>This will get you the result:</p>

<pre><code>[('September-October', 'NP'), ('Durwood', 'NP'), ('Pye', 'NP'), ('Ivan', 'NP'), ('Allen', 'NP'), ('Jr.', 'NP'), ('Fulton', 'NP'), ('Atlanta', 'NP'), ('Fulton', 'NP'), ('Fulton', 'NP'), ('Jan.', 'NP'), ('Fulton', 'NP'), ('Bellwood', 'NP'), ('Alpharetta', 'NP'), ('William', 'NP'), ('B.', 'NP'), ('Hartsfield', 'NP'), ('Pearl', 'NP'), ('Williams', 'NP'), ('Hartsfield', 'NP'), ('Aug.', 'NP'), ('William', 'NP'), ('Berry', 'NP'), ('Jr.', 'NP'), ('Mrs.', 'NP'), ('J.', 'NP'), ('M.', 'NP'), ('Cheshire', 'NP'), ('Griffin', 'NP'), ('Opelika', 'NP'), ('Ala.', 'NP'), ('Hartsfield', 'NP'), ('E.', 'NP'), ('Pelham', 'NP'), ('Henry', 'NP'), ('L.', 'NP'), ('Bowden', 'NP'), ('Hartsfield', 'NP'), ('Atlanta', 'NP'), ('Jan.', 'NP'), ('Ivan', 'NP'), ....]
</code></pre>

<p>Now for future reference double check before you post information like the one I asked for,  just because if it's not correct then it's missleading and it won't help neither the ones who try to help you, nor yourself. Not as a critic but as constructive advice :)</p>
",1172,1328022994
how to find similar sentences / phrases in R?,"<p>Example, I have billions of short phrases, and I want to clusters of them that are similar. </p>

<pre><code>&gt; strings.to.cluster &lt;- c(""Best Toyota dealer in bay area. Drive out with a new car today"",
                        ""Largest Selection of Furniture. Stock updated everyday"" , 
                        "" Unique selection of Handcrafted Jewelry"",
                        ""Free Shipping for orders above $60. Offer Expires soon"",
                        ""XXXX is where smart men buy anniversary gifts"",
                        ""2012 Camrys on Sale. 0% APR for select customers"",
                        ""Closing Sale on office desks. All Items must go"" 
                         )
</code></pre>

<p>assume that this vector is hundreds of thousands of rows. Is there a package in R to cluster these phrases by meaning? 
or could someone suggest a way to rank ""similar"" phrases by meaning to a given phrase. </p>
","r, statistics, nlp","<p>You can view your phrases as ""bags of words"", i.e., build a matrix (a ""term-document"" matrix), with one row per phrase, one column per word, with 1 if the word occurs in the phrase and 0 otherwise. (You can replace 1 with some weight that would account for phrase length and word frequency). You can then apply any clustering algorithm. The <code>tm</code> package can help you build this matrix.</p>

<pre><code>library(tm)
library(Matrix)
x &lt;- TermDocumentMatrix( Corpus( VectorSource( strings.to.cluster ) ) )
y &lt;- sparseMatrix( i=x$i, j=x$j, x=x$v, dimnames = dimnames(x) )  
plot( hclust(dist(t(y))) )
</code></pre>
",6598,1327556143
Get important words in title java library,"<p>Is there any java library that with given text (title) gets collection of important words in it. <br>
EDITED: By important I mean the one that has define the main idea of the sentence.
Thank You.</p>
","java, text-analysis","<p>You might want to take a look at <a href=""http://mahout.apache.org/"" rel=""nofollow"">Apache Mahout</a>.</p>

<p>You also might want to read more on <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">tf-idf model</a> which is often used for cases similar to the one you describe.</p>

<p><strong>EDIT:</strong> more info on Tf-Idf model:</p>

<p>The tf-idf model basically says 2 things:</p>

<ol>
<li>If a term appears many times in your data, it is probably important. [tf]</li>
<li>If a term appears many times in the world, an appearance of it in your data is expected - however, if it is rare - and it appears in your data - it indicates it is a very ""important"" [idf]</li>
</ol>

<p>The tf-idf model utilize this assumptions and gives a rating for each term according to the tf,idf values. 
<br>To find the idf value you might want to index your collection or use some search engine API and estimate how common each term is, based on the number of results [note that the number returned by the engine is not exact, but it might be used as a rough estimation]</p>
",1047,1327699272
Is there any way of using SenseRelate in Java?,"<p>I need to use WSD for my java project..i got to SenseRelate..but its all in Perl..is there any library available for using SenseRelate in Java?</p>
","java, nlp","<p>You may want to try <a href=""http://nlp.comp.nus.edu.sg/software"" rel=""nofollow"">IMS (It Makes Sense)</a>, a Java WSD from NUS. Based on their <a href=""http://www.aclweb.org/anthology-new/P/P10/P10-4014.pdf"" rel=""nofollow"">paper</a>, their system was performing well as compared to state-of-the-art systems in different evaluations.</p>
",280,1303246794
exracting chunks in python using nltk,"<p>let's say i have a tagged corpus (like brown corpus) and i want to extract the words which are only tagged with '/nn'. For example :</p>

<pre><code>            Daniel/np termed/vbd ``/`` extremely/rb conservative/jj ''/'' his/pp$    estimate/nn.....
</code></pre>

<p>this is a part of tagged corpus 'brown'. what i want to do is to extract the words, like- estimate (as it is tagged with /nn) and add them to a list. But most of the example i have found it usually about tagging a corpus. I'm really getting confused seeing these example.
Can any one please help me by providing an example or tutorial about extracting words from a tagged corpus.</p>

<p>Thanks in advance.</p>
","python, nlp, nltk","<p>See: <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch05.html"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/book/ch05.html</a></p>

<pre><code>&gt;&gt;&gt; sent = '''
... The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN
... other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC
... Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS
... said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB
... accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT
... interest/NN of/IN both/ABX governments/NNS ''/'' ./.
... '''
&gt;&gt;&gt; [nltk.tag.str2tuple(t) for t in sent.split()]
[('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'),
('on', 'IN'), ('a', 'AT'), ('number', 'NN'), ... ('.', '.')]
</code></pre>

<p>If you just want those tagged with <code>NN</code>, you could do:</p>

<pre><code>&gt;&gt;&gt; [nltk.tag.str2tuple(t) for t in sent.split() if t.split('/')[1] == 'NN']
[('jury', 'NN'), ('number', 'NN'), ('interest', 'NN')]
</code></pre>

<p><strong>Edit:</strong></p>

<p>Here is <code>sent</code> as a string minus the ellipses.</p>

<pre><code>sent = """"""The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT interest/NN of/IN both/ABX governments/NNS ''/'' ./.""""""
</code></pre>
",483,1327507258
How can I vary the sentence prefix &quot;I am working on [X]&quot; such that it has correct sentence structure for all X?,"<p>I want the user to be able to enter a task and I will prefix it appropriately such that it has correct sentence structure.</p>

<p>E.g.</p>

<pre><code>I am working on [making the world a better place]
</code></pre>

<p>...sounds good.</p>

<pre><code>I am working on [discuss draft proposal]
</code></pre>

<p>...doesn't sound good. In this case it would want the program to respond with something like:</p>

<pre><code>I am discussing a draft proposal
</code></pre>

<p>Basically the way people write tasks or todos appears to be imperative (e.g. pick up milk, write essay, etc.) or simply a noun (e.g. assignment 1, client meeting, etc.). I want to convert these to <a href=""http://www.englishpage.com/verbpage/presentcontinuous.html"">Present Progressive</a> tense.</p>

<p>I am looking into the field of Natural Language Processing at the moment, but I was wondering if there was some sort of API available that would do what I need, or if someone has had experience with a similar problem.</p>
",nlp,"<p>In addition to natural language processing, you're also asking about natural language generation: <a href=""http://en.wikipedia.org/wiki/Natural_language_generation"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Natural_language_generation</a></p>

<p>You can try to use a parser (like the <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow noreferrer"">Stanford parser</a>) to figure out which kind of phrase you have on hand and to identify the main verb if there is one.  You might just fall back on a part-of-speech tagger for this.  In English you'll also want to identify ""helping"" verbs (called ""auxiliaries"" in technical articles) like ""will"", ""may"", ""can"", etc. that often come right before the verb because these can change the tense as well.</p>

<p>If it's just a noun phrase, ""I'm working on X"" will likely sound okay.  If it's a nominal, (if the Stanford parser gives you only NNs without any NPs or NNPs or DETs inside the top NP), then it might sound better with an article attached.  E.g. ""pepper project"" -> ""I'm working on <em>the</em> pepper project"".  You wouldn't do that for ""Pepper's project"" or if it's already ""the pepper project"", or for most proper nouns.  There are always tricky cases though.</p>

<p>If it's a verb phrase:
If it's already progressive, great. Else:</p>

<p>Use a lemmatizer (or fall back on a stemmer) to get the root form of the main verb.<br>
Expand that root form into the present progressive.  For this, probably a few heuristics will suffice, based on whether or not the lemma ends in a vowel or a consonant that gets doubled.  E.g. ""walk"" -> ""walking"", ""run"" -> ""running"" (double n), ""fly"" -> ""flying"" (y doesn't behave like a vowel in this case), ""glide"" -> ""gliding"" (drop the last e after a consonant), but ""flee"" -> ""fleeing"" (not after a vowel).  The most comprehensive place to look for regularities and exceptions is the <a href=""https://rads.stackoverflow.com/amzn/click/com/0582517346"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Comprehensive Grammar of the English Language</a> or a similar online resource.  Tools for this include <a href=""http://www.informatics.sussex.ac.uk/research/groups/nlp/carroll/morph.html"" rel=""nofollow noreferrer"">morphg</a> and <a href=""http://morphadorner.northwestern.edu/"" rel=""nofollow noreferrer"">MorphAdorner</a>.</p>

<p>Finally, remove any helper verbs and substitute the present progressive form for the main verb.  While this won't be perfect, it'll probably look smarter than most.</p>

<p>If it's an entire clause (sentence-like thing with a subject too) or a question, or some other larger thing, you might cop out and just use a generic prefix, like ""Right now: Has Jenn gotten back to me?""  ""Right now: I must head out!""</p>

<p>I'm not an expert, so I may have missed some tools already out there for this kind of thing, and if so, I hope to learn that from others.  It's not an easy thing to do, but it sounds pretty useful.  There will always be mistakes, and they might be jarring to your users, or perhaps they'll find the oddities endearing.  If you put something together, will you post the API here?</p>
",411,1327146774
remove duplicates matched on 3 columns,"<p>I have a large data frame with the following fields (example data).</p>

<p><code>#dput(data) gives...</code></p>

<p><code>data &lt;- structure(list(idNum = 1:11, personID = c(111L, 112L, 113L, 113L, 
111L, 112L, 114L, 112L, 111L, 113L, 115L), Name = c(""PETER PAN"", 
""RUPERT BEAR"", ""LONG JOHN SILVER"", ""SILVER LONG JOHN"", ""PAN PETER"", 
""BEAR RUPERT"", ""R BEAR"", ""RUPERT BEAR"", ""PETER PAN"", ""LONG J SILVER"", 
""LJ SILVER ""), DOB = c(""1/01/2001"", ""2/01/2001"", ""3/01/2001"", 
""3/01/2001"", ""1/01/2001"", ""2/01/2001"", ""10/01/2001"", ""2/01/2001"", 
""1/01/2001"", ""1/01/2001"", ""5/01/2001""), date = c(""12/01/2012"", 
""12/01/2012"", ""14/01/2012"", ""12/01/2012"", ""14/01/2012"", ""11/01/2012"", 
""10/01/2012"", ""16/01/2012"", ""10/01/2012"", ""16/01/2012"", ""10/01/2012""
), colour = c(""RED"", ""BLUE"", ""RED"", ""GREEN"", ""YELLOW"", ""BLUE"", 
""RED"", ""BLUE"", ""ORGANGE"", ""BLUE"", ""ORANGE""), firstName = c(""PETER"", 
""RUPERT"", ""LONG"", ""SILVER"", ""PAN"", ""BEAR"", ""R"", ""RUPERT"", ""PETER"", 
""LONG"", ""LJ""), lastName = c(""PAN"", ""BEAR"", ""SILVER"", ""JOHN"", 
""PETER"", ""RUPERT"", ""BEAR"", ""BEAR"", ""PAN"", ""SILVER"", ""SILVER"")), .Names = c(""idNum"", 
""personID"", ""Name"", ""DOB"", ""date"", ""colour"", ""firstName"", ""lastName""
), row.names = c(NA, -11L), class = ""data.frame"")</code></p>

<p>The firstName and lastName are not in the original data. The name format in the original data set are generated with a free format entry system. It contains a large number of foreign names so data entry clerks do not accurately collect first name and last name. I derived them using:</p>

<pre><code>data$firstName &lt;-sapply(strsplit(data$Name, split="" ""), head, 1)
data$lastName &lt;- sapply(strsplit(data$Name, split="" ""), tail, 1)
</code></pre>

<p>What I need to achieve is a subset data frame that removes duplicates  matched on personID, Name and DOB such that the value returned contains the most entries with the most recent date for each unique case.</p>

<p>That is, I would like to return rows 5, 7, 8, 10 and 11.</p>

<p>I separated first name and last name because I envisaged that it would work by initially extracting cases where <code>lastName == firstName</code> then ording by date. I then tough that I could use  case where lastName was in firstName and other consitions were met.</p>

<p>None if this worked and now I am lost.</p>

<p>Is there a relatively simple way to remove duplicates matched on columns personID, Name and DOB retaining the most recent unique cases?</p>

<p>Many thanks in advance.</p>
","r, nlp, duplicates, dataframe","<p>I used @Vincent's</p>

<p><code>data[ !duplicated( data$personID, fromLast=TRUE ), ]</code></p>

<p>after had sorted by:</p>

<p><code>data &lt;- ddply(.data=data, .variables= 'date')</code></p>
",474,1326794058
Try to figure out a good way to split English document into sentences in C#,"<p>Is there a good way to split English document into sentences? I mean English document frequently includes Mr. Mrs. U.S.A, etc. It is difficult to separate them out. Do we need a special natural language library to accomplish this? I suspect that we need it.</p>

<p>Thank you.</p>
","regex, nlp",,287,1326771167
simple sentiment analysis with java,"<p>I am very new to Sentiment analysis. How can I judge if a given word or sentence is positive or negative. I have to implement it with java. I tried to read something like lingpipe, rapidminer tutorial, but I do not understand. In their examples they use a lot of data. In my case I do not have much data. All I have is a word or a sentence, lets say. I tried to read the questions from stackoverflow too. But they do not help me much.
Thanks in advance.</p>
","java, android, nlp, analysis, rapidminer",,3317,1326419586
Tree traversing or what?,"<p>In Python, I'm writing a Natural Language Processing module and can't work out how to code a function to do the following.
Input: a list of parts of speech (POS) derived from an inputted sentence as short strings. Some items in the list are themselves lists because that part of the program doesn't know which part of speech to choose out of two or more possibles.
e.g. a particular six word sentence results in <code>[""DET"", ""NOUN"", [""VERB"", ""NOUN""], ""CONJ"", [""ADJ"", ""ADV"", ""NOUN""], ""ADV""]</code>
i.e the first word is definitely a DET
the 2nd word is definitely a NOUN
the 3rd word could be a VERB or a NOUN
the 4th word is definitely a CONJ
the 5th word could be a ADJ, ADV or NOUN
the 6th word is definitely a ADV.</p>

<p>So INPUT = <code>[""DET"", ""NOUN"", [""VERB"", ""NOUN""], ""CONJ"", [""ADJ"", ""ADV"", ""NOUN""], ""ADV""]</code></p>

<p>I need the function to return each possible combination as a list of lists. So the return value for the above should be:</p>

<pre><code>[[""DET"", ""NOUN"", ""NOUN"", ""CONJ"", ""NOUN"", ""ADV""],
 [""DET"", ""NOUN"", ""NOUN"", ""CONJ"", ""ADV"", ""ADV""],
 [""DET"", ""NOUN"", ""NOUN"", ""CONJ"", ""ADJ"", ""ADV""],
 [""DET"", ""NOUN"", ""VERB"", ""CONJ"", ""NOUN"", ""ADV""],
 [""DET"", ""NOUN"", ""VERB"", ""CONJ"", ""ADV"", ""ADV""],
 [""DET"", ""NOUN"", ""VERB"", ""CONJ"", ""ADJ"", ""ADV""]]
</code></pre>

<p>The sentences could be from one to n words long. Each word might come back with from one to n parts of speech.</p>
","python, tree, nlp",,474,1326571366
Simple Phrases recognition,"<p>I am looking to recognize simple phrases like the ones what happens in <a href=""https://stackoverflow.com/questions/8483719/parsing-calendar-event-entry-like-google-calendar"">google calendar</a>
but rather than parsing Calendar Entries I have to parse Sentence related to finance, accounting and to do lists. So For example I have to parse sentences like </p>

<blockquote>
  <p>I spent 50 dollars on food yesterday</p>
</blockquote>

<p>I need to mark an separate the info as <code>Reason : 'food'</code> , <code>Cost : 50</code> and <code>Time: &lt;Yesterday's Date&gt;</code></p>

<p>My question is do I go in for a full fledged Natural Language Processing like 
given in these Questions and use Something like <a href=""http://gate.ac.uk/"" rel=""nofollow noreferrer"">GATE</a> </p>

<p><a href=""https://stackoverflow.com/q/2233435/492561"">Machine Learning and Natural Language Processing</a></p>

<p><a href=""https://stackoverflow.com/q/999410/492561"">Natural Language Processing in Ruby</a></p>

<p><a href=""https://stackoverflow.com/q/1793516/492561"">Ideas for Natural Language Processing project?</a></p>

<p><a href=""https://stackoverflow.com/a/3058063/492561"">https://stackoverflow.com/a/3058063/492561</a></p>

<p>Or is it better to Write simple grammars using Something like AntLR and try to recognize it .</p>

<p>Or should I go really low and just Define a syntax and use Regular expressions . </p>

<p>Time is a Constraint , I have about 45 - 50 Days , And I don't know how to use AntLR or NLP libraries like GATE.</p>

<p>Preferred languages : Python , Java , Ruby (Not in any particular order) </p>

<p><em>PS : This is not home-work , So please Don't tag it as so.</em> </p>

<p><em>PPS : Please try to give an answer with Facts on why using a particular method is better. 
even if a particular method may not fit inside the time constraint please feel free to share it because It might benefit someone else .</em></p>
","regex, parsing, nlp","<p>You could look at named entity recognition indeed. From your question I understand your domain is pretty well defined, so you can identify the (few?) entities (dates, currencies, money amount, time expressions, etc.) that are relevant for you. If the phrases are very simple, you could go with a rule-based approach, otherwise it's likely to get too complex too soon.</p>

<p>Just to get yourself up and running in a few sec, <a href=""http://timmcnamara.co.nz/post/2650550090/extracting-names-with-6-lines-of-python-code"" rel=""nofollow"">http://timmcnamara.co.nz/post/2650550090/extracting-names-with-6-lines-of-python-code</a> is an extremely nice example of what you could do. Of course I would not expect an high accuracy from just 6 lines of python, but it should give you an idea of how it works:</p>

<pre><code>1&gt;&gt;&gt; import nltk
2&gt;&gt;&gt; def extract_entities(text):
3...     for sent in nltk.sent_tokenize(text):
4...         for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
5...             if hasattr(chunk, 'node'):
6...                 print chunk.node, ' '.join(c[0] for c in chunk.leaves())
</code></pre>

<p>The core idea is on line 3 and 4: on line 3 it split text in sentences and iterates them. 
On line 4, it splits the sentence in tokens, it runs <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow"">""part of speech"" tagging</a> on the sentence, and then it feeds the pos-tagged sentence to the named entity recognition algorithm. That's the very basic pipeline. </p>

<p>In general, nltk is an extremely beautiful piece of software, and very well documented: I would look at it. Other answers contain very useful links.</p>
",754,1326457750
MySQL routine for porter or porter 2 stemming algorithm,"<p>I'm looking for a port of the Porter or Porter 2 stemming algorithm written as a MySQL routine (user defined function).  Has anybody seen one?</p>

<p>Thanks very much!</p>
","mysql, nlp, porter-stemmer",,347,1301283317
Finding synonyms and the basic form of tilted words,"<p>I am looking for a tool in Java that would allow me to do the following things:</p>

<p>1) Find for a tilted word its basic form. Example:</p>

<ul>
<li>For the words ""connection"", ""connecting"", ""connects"" and etc, it will return the word ""connect"".</li>
<li>For the words ""running"", ""runs"" it will return the word ""run"".</li>
</ul>

<p>2) Return for each words a collection of it's synonyms.</p>

<p>Does anybody know of such a tool?</p>
","java, nlp, synonym",,769,1326275995
Using my own corpus for category classification in Python NLTK,"<p>I'm a NTLK/Python beginner and managed to load my own corpus using CategorizedPlaintextCorpusReader but how do I actually train and use the data for classification of text?</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus.reader import CategorizedPlaintextCorpusReader
&gt;&gt;&gt; reader = CategorizedPlaintextCorpusReader('/ebs/category', r'.*\.txt', cat_pattern=r'(.*)\.txt')
&gt;&gt;&gt; len(reader.categories())
234
</code></pre>
","python, nlp, machine-learning, nltk, corpus","<p>Assuming you want a naive Bayes classifier with bag of words features:</p>

<pre><code>from nltk import FreqDist
from nltk.classify.naivebayes import NaiveBayesClassifier

def make_training_data(rdr):
    for c in rdr.categories():
        for f in rdr.fileids(c):
            yield FreqDist(rdr.words(fileids=[f])), c

clf = NaiveBayesClassifier.train(list(make_training_data(reader)))
</code></pre>

<p>The resulting <code>clf</code>'s <code>classify</code> method can be used on any <code>FreqDist</code> of words.</p>

<p>(But note: from your <code>cap_pattern</code>, it seems you have sample <em>and</em> a single category per file in your corpus. Please check whether that's really what you want.)</p>
",3056,1326280403
Stanford CoreNLP: Building Error (NoSuchMethodError),"<p>Sorry if this is a newbie's question.
I was trying to use maven in Netbeans to build CoreNLP parser.</p>

<p>I first added dependency of stanford-corenlp 1.2.0. However, I always got an error while compiling my code. I tried to simplify my code to just create the StanfordCoreNLP object, but it still did not function with the same error message. I guess here might come with the main trouble spot then.</p>

<p>My simplified code shows as:</p>

<pre><code>import java.util.Properties;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;

Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>The error message:</p>

<pre><code>Exception in thread ""main"" java.lang.NoSuchMethodError: 
edu.stanford.nlp.process.PTBTokenizer.factory
(Ledu/stanford/nlp/process/LexedTokenFactory;Ljava/lang/String;)Ledu/stanford/nlp/objectbank/TokenizerFactory;
        at edu.stanford.nlp.pipeline.PTBTokenizerAnnotator.&lt;init&gt;(PTBTokenizerAnnotator.java:42)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$1.create(StanfordCoreNLP.java:365)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$1.create(StanfordCoreNLP.java:355)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:328)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:194)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:184)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:176)
        at com.mycompany.hellocore.App.main(App.java:26)
</code></pre>

<p>I also tried the same thing via maven on Eclipse, the error message is still the same. Can anyone give me some suggestions? Thanks!</p>

<p>OS: Mac Lion /
Java version: 1.6.0_29 </p>

<hr>

<p>[Update]
01-6-2012 Based on Sri Sankaran's suggestion, i tried <code>mvn dependency: tree</code>:</p>

<pre><code>[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hellocore 1.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-dependency-plugin:2.1:tree (default-cli) @ hellocore ---
[INFO] com.mycompany:hellocore:jar:1.0-SNAPSHOT
[INFO] +- junit:junit:jar:3.8.1:test
[INFO] \- edu.stanford.nlp:stanford-corenlp:jar:1.2.0:compile
[INFO]    +- xom:xom:jar:1.2.5:compile
[INFO]    |  +- xml-apis:xml-apis:jar:1.3.03:compile
[INFO]    |  +- xerces:xercesImpl:jar:2.8.0:compile
[INFO]    |  \- xalan:xalan:jar:2.7.0:compile
[INFO]    \- joda-time:joda-time:jar:2.0:compile
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 4.483s
[INFO] Finished at: Fri Jan 06 08:55:06 EST 2012
[INFO] Final Memory: 5M/81M
[INFO] ------------------------------------------------------------------------
</code></pre>

<p>The setting in my Netbeans:</p>

<p><img src=""https://i.sstatic.net/zSuor.jpg"" alt=""enter image description here""></p>

<p>But it seems like the library in need looks the same as what is already downloaded in Netbeans. The project still stops while <code>Adding annotator tokenize</code>.</p>

<hr>

<p>[Update]
01-09-2012</p>

<p>After i reinstalled my system, the problem was gone. So i think the code and the module are both correct. The classpath directories might be just messed up by me. Thank you for all people's helps.</p>

<p>Just a gentle reminder for people using corenlp via Netbeans. In addition to the standard dependency of stanford-corenlp.jar. If you want to inlcude the stanford-corenlp-models.jar into your project. Seems like you also need to specify the <code>&lt;classifier&gt;</code> to add the models to the dependency repository.</p>

<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
  &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
  &lt;version&gt;1.2.0&lt;/version&gt;
  &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
</code></pre>
","netbeans, maven, nlp, stanford-nlp",,2723,1325817482
How to remove duplicate phrases from a document?,"<p>Is there a simple way to remove duplicate contents from a large textfile? It would be great to be able to detect duplicate sentences (as separated by ""."" or even better to find duplicates of sentence fragments (such as 4-word pieces of text).</p>
","bash, text, nlp, duplicates",,3131,1326103726
Anaphora resolution using Stanford Coref,"<p>I have sentences <strong>(Text I)</strong>:</p>

<blockquote>
  <p><em>Tom is a smart boy. <strong>He</strong> know a lot of thing.</em></p>
</blockquote>

<p>I want to change <strong>He</strong> in the second sentence to <strong>Tom</strong>, so final sentences will become <strong>(Text II)</strong>:</p>

<blockquote>
  <p><em>Tom is a smart boy. <strong>Tom</strong> know a lot of thing.</em></p>
</blockquote>

<p>I've wrote some code, but my <em><strong>coref</strong></em> object always <strong><em>null</em></strong>.<br>
Besides I have no idea what to do next to get correct result.</p>

<pre><code>    String text = ""Tom is a smart boy. He know a lot of thing."";
    Annotation document = new Annotation(text);
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, parse, lemma, ner, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);

    List&lt;Pair&lt;IntTuple, IntTuple&gt;&gt; coref = document.get(CorefGraphAnnotation.class);
</code></pre>

<p>I want to know if I'm doing it wrong and what I should do next to get <strong>Text II</strong> from <strong>Text I</strong>.<br>
PS: I'm using Stanford CoreNLP 1.3.0.</p>

<p>Thanks.</p>
","nlp, stanford-nlp","<pre><code>List&lt;Pair&lt;IntTuple, IntTuple&gt;&gt; coref = document.get(CorefGraphAnnotation.class);
</code></pre>

<p>This is an old coref output format.</p>

<p>You can change this line to     </p>

<pre><code>Map&lt;Integer, CorefChain&gt; graph = document.get(CorefChainAnnotation.class);
</code></pre>

<p>or you can use the <code>oldCorefFormat</code> option:</p>

<pre><code>props.put(""oldCorefFormat"", ""true"");
</code></pre>
",3520,1325929636
Words Prediction - Get most frequent predecessor and successor,"<p>Given a word I want to get the list of most frequent predecessors and successors of the word in English language.
I have developed a code that does bigram analysis on any corpus ( I have used Enron email corpus) and can predict the most frequent next possible word but I want some other solution because
a) I want to check the working / accuracy of my prediction
b) Corpus or dataset based solutions fail for an unseen word</p>

<p>For example, given the word ""excellent"" I want to get the words that are most likely to come before excellent and after excellent</p>

<p>My question is whether any particular service or api exists for the purpose?</p>
","cpu-word, nlp, prediction",,2317,1325938306
"How can I tweak Levenshtein distance in classifying linguistically similar words (e.g. verb tenses, adjective comparisons, singular and plural)","<p>I am out of ideas on how to complete this task. I am counting the frequency of a word, actually the base form of the word (e.g. running will be counted as run). I looked up on some implementations of Levenshtein distance (one implementation I run into is <a href=""http://dotnetperls.com/levenshtein"" rel=""noreferrer"">from dotnerperls</a>).</p>

<p>I also tried the double Metaphone, but it isn't what I'm looking for.</p>

<p>So, please give me some ideas on how to tweak Levenshtein distance algorithm in classifying linguistically similar words since the algorithm is only for determining the number of edits needed not considering if they are linguistically similar or not</p>

<p>Example:
1. ""running"" will be counted as one occurrence of the word ""run""
2. ""word"" will likewise be an occurrence of ""word""
3. ""fear"" will NOT be counted as an occurrence of ""gear""</p>

<p>Also, I am implementing it in C#.</p>

<p>Thanks in advance.</p>

<p>Edit: I edited it as Rene suggested.
Another note:
I am trying to consider to consider if a word is a substring of another word but that implementation will not be as much dynamic.
Another idea I think is: ""if adding -s or -ing to string1, string1 == string2, then string2 is an occurrence of string1."" However, this is not the case as some words have irregular plurals.</p>
","nlp, levenshtein-distance, similarity","<p>The task you are trying to solve is called <a href=""http://en.wikipedia.org/wiki/Stemming"" rel=""noreferrer"">Stemming</a> or <a href=""http://en.wikipedia.org/wiki/Lemmatisation"" rel=""noreferrer"">Lemmatisation</a>.</p>

<p>As you figured out already, Levenshtein-Distance is not the way to go here.
Common stemming-algorithms for english include the Porter- and Snowball-Stemmer.
If you google for that I'm sure you will find a C#-implementation of one of them.</p>
",853,1325931315
TextRank Run time,"<p>I implemented textrank in java but it seems pretty slow. Does anyone know about its expected performance?</p>

<p><strong>If it's not expected to be slow</strong>, could any of the following be the problem:</p>

<p>1) It didn't seem like there was a way to create an edge and add a weight to it at the same in JGraphT time so I calculate the weight and if it's > 0, I add an edge. I later recalculate the weights to add them while looping through the edges. Is that a terrible idea?</p>

<p>2) I'm using JGraphT. Is that a slow library?</p>

<p>3) Anything else I could do to make it faster?</p>
","java, graph, nlp, jgrapht, opennlp",,1433,1325891664
Good separators for key-value pairs in English,"<p>I want to store a number of key-value pairs in a flat file. A key can have multiple values. Keys and values are in English and may contain Unicode characters occasionally. 
What are the good separators that can be used while storing these keys and values in a file, so that the separators are not encountered as a part of any key or value. For example, '&amp;' is not a good separator as a key or value can contain an '&amp;'. </p>

<p>I need two separators, one to separate the key from values, and another to separate the values.</p>
",nlp,"<p>Agh, no! Don't invent your own format here! Use an existing one, like <a href=""http://en.wikipedia.org/wiki/Json"">JSON</a>, <a href=""http://en.wikipedia.org/wiki/XML"">XML</a>, or <a href=""http://en.wikipedia.org/wiki/YAML"">YAML</a>. <strong><em>Please.</em></strong></p>

<p>I like JSON, personally, so here's a JSON example:</p>

<pre><code>{
    ""key1"": ""singleVal1"",
    ""key2"": [""singleVal2""],
    ""key3"": [""multiVal1"", ""multiVal2"", ""multiVal3""]
}
</code></pre>
",1188,1325813184
Common Service Locator and implementations of IDependencyResolver,"<p>I'm building a library for <a href=""http://nlp.abodit.com"" rel=""nofollow"">conversational natural language processing</a>.  In many ways it acts much like MVC3 in that it has Controllers and Action Methods.  It also uses dependency injection in much the same way as MVC3 does when instantiating constructors for the Controller classes.  The main differences being that an English sentence replaces both the URL and the form values of HTTP; routing is based on matching sentence structure; and the parameters passed in are the meanings of words and phrases used in the English sentence.  </p>

<p>Currently it uses Autofac for Dependency Injection but I'd like to remove that dependency and allow callers to use any DI container.  </p>

<p>If I use the P&amp;P / Codeplex <a href=""http://commonservicelocator.codeplex.com/SourceControl/changeset/view/27688#332676"" rel=""nofollow"">Common Service Locator</a> project in my solution then callers would still need to provide their own implementations of <code>IServiceLocator</code> against the instance of that interface exposed by my engine.  If I use <code>IDependencyResolver</code> from MVC3 instead there are at least existing implementations of the mapping from the various DI container to that interface.</p>

<p>Should I:-</p>

<ol>
<li>use the Common Service Locator and force callers to implement the mapping classes.</li>
<li>use the MVC 3 <code>IDependencyResolver</code> interface which already has mappings to other containers.</li>
<li>accept a <code>object</code> as the dependency resolver and duck type it to get the one method I need from it so I can use the MVC3 interface without even taking a dependency on ASP.NET MVC3.</li>
<li>other?</li>
</ol>
","c#, .net, design-patterns, nlp, common-service-locator","<p>The Common Service Locator is, by definition, an interface assembly that never changes and doesn't need a specific version.</p>

<p>In addition all of the common IOC libraries now have implementations to connect to the Common Service Locator.</p>

<p>Thus option 1 is the best option and the risk of it breaking with a new release of the Common Service Locator is virtually zero.</p>

<p>Thanks to <a href=""https://twitter.com/#!/philiplaureano"" rel=""nofollow"">Philip Laureano</a> for helping answer this.</p>
",774,1315534354
Stop-word elimination and stemmer in python,"<p>I have a somewhat large document and want to do stop-word elimination and stemming on the words of this document with Python. Does anyone know an of the shelf package for these?
If not a code which is fast enough for large documents is also welcome.
Thanks</p>
","python, nlp, stemming, stop-words","<p><a href=""http://www.nltk.org/"" rel=""noreferrer"">NLTK</a> supports this.</p>
",2311,1286463190
quicker way to detect n-grams in a string?,"<p>I found this solution on SO to detect n-grams in a string:
(here: <a href=""https://stackoverflow.com/questions/3656762/n-gram-generation-from-a-sentence"">N-gram generation from a sentence</a>)</p>

<pre><code>import java.util.*;

public class Test {

    public static List&lt;String&gt; ngrams(int n, String str) {
        List&lt;String&gt; ngrams = new ArrayList&lt;String&gt;();
        String[] words = str.split("" "");
        for (int i = 0; i &lt; words.length - n + 1; i++)
            ngrams.add(concat(words, i, i+n));
        return ngrams;
    }

    public static String concat(String[] words, int start, int end) {
        StringBuilder sb = new StringBuilder();
        for (int i = start; i &lt; end; i++)
            sb.append((i &gt; start ? "" "" : """") + words[i]);
        return sb.toString();
    }

    public static void main(String[] args) {
        for (int n = 1; n &lt;= 3; n++) {
            for (String ngram : ngrams(n, ""This is my car.""))
                System.out.println(ngram);
            System.out.println();
        }
    }
}
</code></pre>

<p>=> this bit of code takes by far the longest processing time (28 seconds for the detection of 1-grams, 2-grams, 3-grams and 4grams for my corpus: 4Mb of raw text), as compared to milliseconds for other operations (removal of stopwords etc.)</p>

<p>Does anybody know of solutions in Java which would go faster than the looping solution presented above? (I was thinking multithreading, use of collections, or maybe creative ways to split a string...?) Thanks!</p>
","java, nlp, n-gram","<p>You could try something like this:</p>

<pre><code>public class NGram {

    private final int n;
    private final String text;

    private final int[] indexes;
    private int index = -1;
    private int found = 0;

    public NGram(String text, int n) {
        this.text = text;
        this.n = n;
        indexes = new int[n];
    }

    private boolean seek() {
        if (index &gt;= text.length()) {
            return false;
        }
        push();
        while(++index &lt; text.length()) {
            if (text.charAt(index) == ' ') {
                found++;
                if (found&lt;n) {
                    push();
                } else {
                    return true;
                }
            }
        }
        return true;
    }

    private void push() {
        for (int i = 0; i &lt; n-1; i++) {
            indexes[i] = indexes[i+1];
        }
        indexes[n-1] = index+1;
    }

    private List&lt;String&gt; list() {
        List&lt;String&gt; ngrams = new ArrayList&lt;String&gt;();
        while (seek()) {
            ngrams.add(get());
        }
        return ngrams;
    }

    private String get() {
        return text.substring(indexes[0], index);
    }
}
</code></pre>

<p>Testing on about 5mb of text it seems to perform about 10 times faster than the original code. The main difference here is that regex is not used to split and the ngram strings are not created by concatenation.</p>

<p>Update:
This is the output I get when running on the text mentioned above, ngram 1-4. I run with 2GB of memory to decide the impact on GC during the runs. I ran multiple times to see the impact of the hotspot compiler.</p>

<pre><code>Loop 01 Code mine ngram 1 time 071ms ngrams 294121
Loop 01 Code orig ngram 1 time 534ms ngrams 294121
Loop 01 Code mine ngram 2 time 016ms ngrams 294120
Loop 01 Code orig ngram 2 time 360ms ngrams 294120
Loop 01 Code mine ngram 3 time 082ms ngrams 294119
Loop 01 Code orig ngram 3 time 319ms ngrams 294119
Loop 01 Code mine ngram 4 time 014ms ngrams 294118
Loop 01 Code orig ngram 4 time 439ms ngrams 294118

Loop 10 Code mine ngram 1 time 013ms ngrams 294121
Loop 10 Code orig ngram 1 time 268ms ngrams 294121
Loop 10 Code mine ngram 2 time 014ms ngrams 294120
Loop 10 Code orig ngram 2 time 323ms ngrams 294120
Loop 10 Code mine ngram 3 time 013ms ngrams 294119
Loop 10 Code orig ngram 3 time 412ms ngrams 294119
Loop 10 Code mine ngram 4 time 014ms ngrams 294118
Loop 10 Code orig ngram 4 time 423ms ngrams 294118
</code></pre>
",9353,1325512076
Algorithm (or C# library) for identifying &#39;keywords&#39; in a set of messages?,"<p>I want to build a list of ~6 keywords (or even better: couple word keyphrases) for each message in a message forum.</p>

<ul>
<li>The primary use of keywords is to replace subject lines in some instances. For example: <em>Message from Terry sent Dec 5, keywords: <strong>norweigan blue, plumage, not dead</em></strong></li>
<li>In a super ideal world keywords would identify both unique phases, and phrases that cluster the discussion into ""topics"", i.e. words that are highly relevant to the message in question, and a few other messages in the forum, but not found frequently in the forum as a whole.</li>
<li>I expect junk phrases to show up, no big deal.</li>
<li>Can't be too computationally expensive: I need something that can handle several hundred messages in several seconds, as I'll need to re-run this every time a new message comes in.</li>
</ul>

<p>Anyone know a good C# library for accomplishing this? Maybe there's a way to bend Lucene.NET into providing this sort of info?</p>

<p>Or, failing that, can anyone suggest an algorithm (or set of algos) to read up on? If I'm implementing myself I need something not terribly complex, I can only tackle this if its tractable in about a week. Right now, the best I've found in terms of simple-but-effective is <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF</a>.</p>

<p><strong>UPDATE:</strong> I've uploaded the results of using TF-IDF to select the top 5 keywords from a real dataset here: <a href=""http://jsbin.com/oxanoc/2/edit#preview"" rel=""nofollow"">http://jsbin.com/oxanoc/2/edit#preview</a> </p>

<p>The results are mediocre, but not totally useless... maybe with the addition of detecting multi-word phrases, this would be good enough.</p>
","c#, algorithm, search, nlp, text-mining",,3875,1325452138
How to read the Alignment type from the BerkeleyAligner? - Java,"<p>After downloading the trunk code from <code>http://code.google.com/p/berkeleyaligner/</code>, I added the project into my build path on Eclipse. Then with the code below i can extract the alignments for each sentence pair that i've read from the sourceFile and targetFile.
After the alignment, how to read the <code>Alignment</code> type from the BerkeleyAligner?</p>

<pre><code>import edu.berkeley.nlp.wa.mt.Alignment;
import edu.berkeley.nlp.wa.mt.SentencePair;
import edu.berkeley.nlp.wordAlignment.combine.WordAlignerCombined;
public static void main(String[] args) {
BufferedReader brSrc = new BufferedReader(new FileReader (""sourceFile""));
BufferedReader brTrg = new BufferedReader(new FileReader (""targetFile""));
while ((currentSrcLine = brSrc.readLine()) !=null) {
    String currentTrgLine = brTrg.readline();
    // Reads into BerkeleyAligner SentencePair format.
    SentencePair src2trg = new SentencePair(sentCounter, params.get(""source""),
        Arrays.asList(srcLine.split("" "")), Arrays.asList(trgLine.split("" "")));
    // Generate Alignment type from SentencePair
    WordAlignerCombined aligner;
    Alignment alignedPair = aligner.alignSentencePair(src2trg);
    // How do i print out the Alignment???
    }
}
</code></pre>

<p>e.g. sourceFile:</p>

<pre><code>this is the first line in the textfile.
that is the second line.
foo bar likes to eat bar foo.
</code></pre>

<p>e.g. targetFile:</p>

<pre><code>Dies ist die erste Textzeile in der Datei.
das ist die zweite Zeile.
foo bar gerne bar foo essen.
</code></pre>
","java, text, nlp, alignment, text-alignment","<p>Print the GIZA. <a href=""http://code.google.com/p/berkeleyaligner/source/browse/trunk/src/edu/berkeley/nlp/wa/mt/Alignment.java"" rel=""nofollow"">Alignment</a> has a method for that:</p>

<pre><code>public void writeGIZA(PrintWriter out, int idx)
</code></pre>

<p>GIZA is:</p>

<pre><code>""# sentence pair (%d) source length %d target length %d alignment score : 0\n""
""NULL ({ %s })""
"" %s ({ %s })"" (englishSentence.get(i), StrUtils.join(alignments))
</code></pre>

<p></p>

<p><code>idx</code> is just the sentence pair id.</p>

<p><code>out</code> is just where you want to print it.</p>
",354,1325529483
Exact phrase search using lucene without increasing number of fields,"<p>For a phrase search, we want to bring up results only if there's an exact match (without ignoring stopwords). If it's a non-phrase search, we are fine displaying results even if the root form of the word matches etc.</p>

<p>We currently pass our data through standardTokenizer, StopFilter, PorterStemFilter and LowerCaseFilter. Due to this when user wants to search for ""password management"", search brings up results containing ""password manager"".</p>

<p>If I remove StemFilter, then I will not be able to match for the root form of the word for non-phrase queries. I was thinking if I should index the same data as part of two fields in document.</p>

<p>I have asked same question at <a href=""https://stackoverflow.com/questions/8292446/exact-phrase-search-using-lucene"">Different indexing and search strategies on same field without doubling index size?</a>. However folks at office are not happy about indexing the same data as part of two fields. (we currently have around 20 text fields in lucene document). Is there any way to support both the cases I listed above using TokenFilters?</p>

<p>Say, for a StopFilter, make changes so that it emits both the input token and ? (for ignored word) with same position increments. Similarly for StemFilter, it emits both the input token and stemmed token with same position increments. Basically input and output tokens (even ignored ones) have same positions.</p>

<p>Is it safe to go ahead with this approach? Has anyone else faced the requirements listed here? Are there any Filters readily available which do something similar to what I mentioned in my approach?</p>

<p>Thanks</p>
","full-text-search, lucene, text-analysis, exact-match","<p>I don't understand what you mean by ""input and output tokens."" Are you storing the data twice - once as stemmed and once non-stemmed?</p>

<p>If you aren't storing it twice, I don't think your method will work. Suppose the stored word is <code>jumping</code> and they search for <code>jumped</code>. Your query parser can emit <code>jump</code> and <code>jumped</code> but it still won't match <code>jumping</code> unless you have a value stored as <code>jump</code>. </p>

<p>And if you're going to store the value once as stemmed and once as non-stemmed, then why not just store it in two fields? Then you won't have to deal with weird tokenizer changes.</p>
",926,1325500732
Multitask learning,"<p>Can anybody please explain multitask learning in simple and intuitive way? May be some real
world problem would be useful.Mostly, these days i am seeing many people are using it for natural language processing tasks.</p>
","nlp, machine-learning, stanford-nlp","<p>Let's say you've built a sentiment classifier for a few different domains.  Say, movies, music DVDs, and electronics.  These are easy to build high quality classifiers for, because there is tons of training data that you've scraped from Amazon.  Along with each classifier, you also build a similarity detector that will tell you for a given piece of text, how similar it was to the dataset each of the classifiers was trained on.</p>

<p>Now you want to find the sentiment of some text from an unknown domain or one in which there isn't such a great dataset to train on.  Well, how about we take a similarity weighted combination of the classifications from the three high quality classifiers we already have.  If we are trying to classify a dish washer review (there is no giant corpus of dish washer reviews, unfortunately), it's probably most similar to electronics, and so the electronics classifier will be given the most weight.  On the other hand, if we are trying to classify a review of a TV show, probably the movies classifier will do the best job.</p>
",371,1325337036
Identifying (spoken) language in javascript,"<p>Does anyone know if there is any language detection script/library available for javascript? I like to incorporate it into nodejs but didn't found any.</p>

<p>I don't want a browser language detection but a string language detection.
'Hello World' would be detected english and 'Hallo Wereld' would be detected as dutch.</p>

<p>Thanks in advance.</p>
","javascript, node.js, nlp","<p>I'm not that big into natural language processing but I suppose you could use <a href=""http://en.wikipedia.org/wiki/Trigram"" rel=""nofollow"">Trigrams</a>.</p>

<p>ActiveState has a <a href=""http://code.activestate.com/recipes/326576-language-detection-using-character-trigrams/"" rel=""nofollow"">Recipe</a> of a simple Python version.</p>

<p>Of course you can always use a C++ library and write a wrapper for Node.js and compile that.</p>
",2024,1292583480
Point me in the right direction on NLP datastructures and search algorithm,"<p>I've got a school assignment to make a language analyzer that's able to guess the language of an input. The assignment states this has to be done by pre-parsing language defined texts and making statistics about letters used, combinations of letter etc and then making a guess based on this data.</p>

<p>The data structure we're supposed to use is simple multi-dimensional hashtables but I'd like to take this opportunity to learn a bit more about implementing structures etc. What'd I'd like to know is what to read up about. My knowledge of algorithms is very limited but I'm keen on learning if someone could point me in the right direction.</p>

<p>Without any real knowledge and just reading up on different posts I'm currently planing on studying undirected graphs as a datastructure for letter combinations (and somehow storing the statistics within the graph as well) and boyer-moore for the per-word search algorithm.</p>

<p>Am I totally on the wrong track and these would be impossible to implement in this situation or is there something else superior for this problem?</p>
","java, algorithm, data-structures, graph, nlp",,213,1298545686
Sort an array exactly like example string,"<p>What would be the optimal solution for the following problem?</p>

<p>I have  </p>

<pre><code>original_string = ""This is a string that I am trying to sort""
</code></pre>

<p>I also have </p>

<pre><code>array_to_sort = ['sort', 'string', 'This is', 'I', 'trying to', 'am', 'a'] 
</code></pre>

<p>I need to sort the array, so that elements are in the same order as in string. The elements are sometimes grouped together, but always in the same way as they are in string (i.e. there can be no 'is This' element in the array, only 'This is').. </p>

<p>All this is happening within the Rails application, so I was thinking of maybe taking the database approach and saving elements in database and then using some keys to reconstruct the original_string.. but maybe just doing some .sort trick is better.. The result does not necessarily have to be an array, can be anything.. </p>

<p>Thanks for any input. </p>

<p>P.S. including an nlp tag, because this is a result of some nlp exercise. </p>
","ruby, arrays, string, sorting, nlp","<pre><code>array_to_sort.sort_by { |substr| original_string.index(substr) }
</code></pre>

<p>The result is a new array, sorted by the position of the substring in the original string.</p>

<p>If you want to sort in-place (by changing the original array), you can use the <code>sort_by!</code> method instead.</p>

<p>Obviously, it's too stupid to detect doubles (i.e. <code>""I am what I am"", [""I am"", ""I am"", ""what""]</code> will not be sorted as one hopes).</p>

<p><strong>EDIT</strong> Making it not quite so stupid is not quite so trivial:</p>

<pre><code>def get_all_positions(str, substr)                                                                                                                                                                                           
  pattern = Regexp.new('\b' + Regexp::escape(substr) + '\b')
  result = []
  pos = -1
  while match = pattern.match(str, pos + 1)
    pos = match.offset(0)[0] + 1
    result &lt;&lt; pos
  end
  result
end

def sort_array_according_to_string(arr, str, i=0, positions=nil)
  positions ||= Hash.new
  if i &lt; arr.count
    current = arr[i]
    current_positions = get_all_positions(str, current)
    result = []
    current_positions.each do |pos|
      if !positions[pos]
        positions[pos] = [pos, i, current]
        result += sort_array_according_to_string(arr, str, i + 1, positions)
        positions.delete(pos)
      end
    end
  else
    sorted = positions
      .values
      .sort_by { |position, i| position }
      .map { |position, i| arr[i] }
    result = [sorted]
  end
  if i == 0
    result.uniq!
  end
  result
end

original_string = 'this is what this is not'
example_array = ['this', 'is', 'is not', 'what', 'this']
solution = sort_array_according_to_string(example_array, original_string)
puts solution.inspect
</code></pre>
",419,1325107048
"Looking for word parsing web site that tells if a word is a Noun, Verb, etc","<p>Is there a web site I can call that will allow me to send it a word and it will tell me if that word is a Noun, Verb, etc. ?</p>

<p>I'm hoping to find that kind of site that maybe would return xml that I can use to parse out the returned results.</p>

<p>Thanks.</p>
","xml, dictionary, web, nlp","<p>I found exactly what I was looking for at dictionary.com because they have an API that returns XML code.</p>

<p>Here's the link:
<a href=""http://developer.dictionary.com"" rel=""nofollow"">http://developer.dictionary.com</a></p>

<p>Hopefully this will help other people out there looking to do the same as I.</p>
",1340,1324902156
OpenNLP HeadRules,"<p>I'm trying to train a parser for a new model using the openNLP tutorial <a href=""http://sourceforge.net/apps/mediawiki/opennlp/index.php?title=Parser#Training"" rel=""nofollow"">http://sourceforge.net/apps/mediawiki/opennlp/index.php?title=Parser#Training</a> . The only problem is that is requires a head_rules file. I can't seem to find any information anywhere on generating this file and the the only link to a head_rules file can be found here:
<a href=""http://opennlp.sourceforge.net/models/english/parser/head_rules"" rel=""nofollow"">http://opennlp.sourceforge.net/models/english/parser/head_rules</a> but I can't make sense of it. Does anyone know how to generate this from  training data?</p>
","java, parsing, nlp, opennlp",,1004,1308147020
Difference between named entity recognition and resolution?,"<p>What is the difference between named entity recognition and named entity resolution? Would appreciate a practical example.</p>
","nlp, named-entity-recognition, named-entity-extraction","<p>Named entity recognition is picking up the names and classifying them in running text. E.g., given (<a href=""http://www.guardian.co.uk/football/2011/dec/21/john-terry-racism-case-cps"">1</a>)</p>

<pre><code>John Terry to face criminal charges over alleged racist abuse
</code></pre>

<p>an NE recognizer will output</p>

<pre><code>[PER John Terry] to face criminal charges over alleged racist abuse
</code></pre>

<p>NE resolution or normalization means finding out which entity in the outside world a name refers to. E.g., in the above example, the output would be annotated with a unique identifier for the footballer John Terry, like his Wikipedia URL:</p>

<pre><code>[https://en.wikipedia.org/wiki/John_Terry John Terry] to face criminal charges
over alleged racist abuse
</code></pre>

<p>as opposed to, e.g.</p>

<pre><code>https://en.wikipedia.org/wiki/John_Terry_%28actor%29
https://en.wikipedia.org/wiki/John_Terry_%28baseball%29
</code></pre>

<p>or any of the other John Terry's the Wikipedia knows.</p>
",2615,1324466534
NLTK context-free grammars,"<p>I am just wondering how would you add an optional grammer in the rule</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.app.rdparser()
</code></pre>

<p>For example, the normal way to add a optional grammer is by putting it in parentheses: NP -> NP (PP) </p>

<p>But in the program how would you do it? parentheses doesnt  work.</p>

<pre><code>S   Þ NP VP
NP  Þ NP PP | Det N
VP  Þ V NP PP
PP  Þ P NP

Det Þ 'the' | 'a'
N   Þ 'man' | 'park' | 'dog' | 'boy' | 'girl'
V   Þ 'was' | 'saw' 
P   Þ 'in' | 'under' | 'with'
</code></pre>

<p>Thanks,</p>

<p>Ray</p>
","python, nlp, nltk, context-free-grammar","<pre><code>NP -&gt; NP | NP PP
</code></pre>

<p>But note that, with this rule, you can stack <code>NP</code> nodes indefinitely in the parse tree.</p>
",1333,1324478412
calling Stanford POS Tagger maxentTagger from java program,"<p>I am new to Stanford POS tagger. </p>

<p>I need to call the Tagger from my java program and direct the output to a text file.
I have extracted the source files from Stanford-postagger and tried calling the maxentTagger, but all I find is errors and warnings.</p>

<p>Can somebody tell me from the scratch about how to call maxentTagger in my program, setting the classpath if required and other such steps. Please help me out.</p>
","java, nlp, stanford-nlp",,4262,1270612465
Keyword detection from a query,"<p>I have a database full of keywords Where each keyword could be a phrase (collection of words), and when I send a query, I want to match all possible keywords in the query. I am wondering how this could be effectively done. The problems I am facing are</p>

<ul>
<li>The query could have one or more keywords.</li>
<li>The match needn't be exact (could be a close match).</li>
</ul>

<p>I went through this <a href=""https://stackoverflow.com/questions/70560/how-do-i-compare-phrases-for-similarity"">question and solution</a>, but again, I am not sure how I could be grouping words in the query to form phrases which could be compared to each keyword in the database. There could be n(n+1)/2 phrases which could be formed for a query with n words. So the simplest solution is to compare each of this phrase with every keyword in the database and find matches.
Is there a better solution? Are there standard algorithms/libraries to do this?</p>
","search, nlp, pattern-matching, information-retrieval",,142,1324388593
text to facts for Inference Engine,"<p>I am looking for a program or algorithm that will analyze text and produce facts/rules from it that can be fed to an inference engine for question answering. Are there any good commercial or open source solution for this available?</p>

<p>If not what algorithms should I be looking at, to try and code a solution myself.</p>
","artificial-intelligence, nlp, data-mining, inference",,401,1323706847
Noun Type Determination in Java,"<p>I am currently working on a project where I need to create possible answers to WH question based on a given sentence.</p>

<p>Example sentence is:</p>

<pre><code>Anna went to the store.
</code></pre>

<p>I need to create possible answers to the following questions.</p>

<pre><code>Who went to the store?
Where did ana go?
</code></pre>

<p>I had already implemented the POST-Tagging of the words and I now know the Part of Speech of the word.</p>

<p>I had difficulty on determining what type of noun is the word if it is a noun so I can create a template for my answer.</p>

<p>Example is:</p>

<pre><code>Anna - noun - person: possible answer to question who.

Store - noun - place: possible answer to question where.
</code></pre>

<p>I want to implement this using java</p>
","java, nlp","<p>You should not try to infer possible questions and their answers from the nouns present in a sentence. Instead, you should infer the type of questions you can ask from the activity described by the verb. In your example, the system should infer that the activity of going requires a subject who goes (possible answer to the ""who"" question), a place where the subject goes (possible answer to the ""where to"" question) as well as the time when the subject went there (possible answer to the ""when"" question) and potentially more (from where? with whom? by what means? which way? etc). Then it should check which answers are provided in the question. In your example, answers to ""who"" and ""where"" are provided, but ""when"" is not. In other words, you should have a mapping from verbs to questions that make sense for each verb.</p>

<p>Then for each question applicable to the verb you should store prepositions that are used to indicate that answer in a sentence. For example, the answer to the ""where to"" question is often preceded by ""to"" and an answer to ""when"" question is often preceded by ""at"" or ""on"". Note that subject (here answer to the ""who"" question) needs special treatment. Also, some verbs can be immediately followed by an object without prepositions and your verb dataset should indicate the question to which they constitute an answer. For example the verb ""to enter"" can be followed by an object which answer the ""where/what"" question as in ""Anna entered the room"". Also, some nouns are exceptions and are never preceded by a preposition. For example ""home"" as in ""Anna went home"". You need to treat these specially as well. Another thing to watch out for are idiomatic expressions like ""Anna went to great lengths"". Again, special treatment is required. </p>

<p>Generally, nouns in English do not have enough structure to allow you to determine what type of thing (e.g. a place, an object, a person, a concept etc) they denote. You would need to have a large dataset that breaks up all the words known to the system into different categories. If you do use a noun set like this, it should have a supporting role to increase system's accuracy.</p>

<p>Relying on verbs and prepositions is far more flexible as it allows the system to handle unknown expressions. For example, someone might say ""Anna went to Bob"", but ""Bob"" is not a place. System that infers the role of each element from verb and preposition still handles this situation and properly treats ""Bob"" as the right answer for the ""where to"" question. </p>
",443,1324210247
Determining the Similarity Between Items in a Database,"<p>We have a database with hundreds of millions of records of log data. We're attempting to 'group' this log data as being likely to be of the same nature as other entries in the log database. For instance:</p>

<p>Record X may contain a log entry like:</p>

<blockquote>
  <p>Change Transaction ABC123 Assigned To Server US91</p>
</blockquote>

<p>And Record Y may contain a log entry like:</p>

<blockquote>
  <p>Change Transaction XYZ789 Assigned To Server GB47</p>
</blockquote>

<p>To us humans those two log entries are easily recognizable as being likely related in some way. Now, there may be 10 million rows between Record X and Record Y. And there may be thousands of other entries that are similar to X and Y, and some that are totally different but that have other records they are similar to.</p>

<p>What I'm trying to determine is the best way to group the similar items together and say that with XX% certainty Record X and Record Y are probably of the same nature. Or perhaps a better way of saying it would be that the system would look at Record Y and say based on your content you're most like Record X as apposed to all other records.</p>

<p>I've seen some mentions of Natural Language Processing and other ways to find similarity between strings (like just brute-forcing some Levenshtein calculations) - however for us we have these two additional challenges:</p>

<ol>
<li>The content is machine generated - not human generated</li>
<li>As opposed to a search engine approach where we determine results for a given query - we're trying to classify a giant repository and group them by how alike they are to one another.</li>
</ol>

<p>Thanks for your input!</p>
","artificial-intelligence, nlp, search-engine, data-mining, full-text-indexing","<p>Interesting problem.  Obviously, there's a scale issue here because you don't really want to start comparing each record to every other record in the DB.  I believe I'd look at growing a list of ""known types"" and scoring records against the types in that list to see if each record has a match in that list.</p>

<p>The ""scoring"" part will hopefully draw some good answers here -- your ability to score against known types is key to getting this to work well, and I have a feeling you're in a better position than we are to get that right.  Some sort of soundex match, maybe?  Or if you can figure out how to ""discover"" which parts of new records change, you could define your known types as regex expressions.</p>

<p>At that point, for each record, you can hopefully determine that you've got a match (with high confidence) or a match (with lower confidence) or very likely no match at all.  In this last case, it's likely that you've found a new ""type"" that should be added to your ""known types"" list.  If you keep track of the score for each record you matched, you could also go back for low-scoring matches and see if a better match showed up later in your processing.</p>
",1175,1323984559
Associating free text statements with pre-defined attributes,"<p>I have a list of several dozen product attributes that people are concerned with, like</p>

<ul>
<li>Financing</li>
<li>Manufacturing quality</li>
<li>Durability</li>
<li>Sales experience</li>
</ul>

<p>and several million free-text statements from customers about the product, e.g.</p>

<blockquote>
  <p>""The financing was easy but the housing is flimsy.""</p>
</blockquote>

<p>I would like to score each free text statement in terms of how strongly it relates to each of the attributes, and whether that is a positive or negative association.</p>

<p>In the given example, there would be a strong positive association to <code>Financing</code> and a strong negative association to <code>Manufacturing quality</code>.</p>

<p>It feels like this type of problem is probably the realm of Natural Language Programming (NLP).  However, I spent several hours reading up on things like OpenNLP and NLTK and find there's so much domain specific terminology that I cannot figure out where to focus to solve this specific problem.</p>

<p>So my three-part question:</p>

<ul>
<li>Is NLP the correct route to solve this class of problem?</li>
<li>What aspect of NLP should I focus on learning for this specific problem?</li>
<li>Are there alternatives I have not considered?</li>
</ul>
",nlp,"<p>Yes, this is a NLP problem by the name of <a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""nofollow"">Sentiment analysis</a>. Sentiment analysis is an active research area with different approaches and a task where a lot of other NLP-methods have to work together, so it is certainly not the easiest field to get started with in NLP.</p>

<p>A more or less recent survey of the academic research in the field can be found in <a href=""http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf"" rel=""nofollow"">Pang &amp; Lee (2008)</a>.</p>
",383,1324076470
"NLP for java, which toolkit should I use?","<p>I'm working on a project that needs to count the occurrence of every word of a txt file.
For example, I have a text file like this:</p>

<blockquote>
  <p>What Silver Lake Looks For in IPO Candidates
  3 Companies Crushed by Earnings: Apple, Cirrus Logic, IBM
  IBM's Palmisano: How You Get To Be A 100-Year Old Company</p>
</blockquote>

<p>If there are 3 sentences shown above in the file and I want to calculate every word's occurrence. Here, Companies and company should be considered as the same word ""company""(lowercase), so the total occurrence for the word ""company"" is 2. </p>

<p>Is there any NLP toolkit for java that can tell two words like ""families"" and ""family"" are actually from the same word ""family""?</p>

<p>I'll count the occurrence of every word to further do the Naive Bayes training, so it's very important to get the accurate numbers of occurrences of each word.</p>
","java, text, nlp, text-mining","<p><a href=""http://lucene.apache.org/java/docs/index.html"" rel=""nofollow"">Apache Lucene</a> and <a href=""http://opennlp.sourceforge.net/"" rel=""nofollow"">OpenNLP</a> provide good stemming algorithm implementations. You can review and use the best one that suites you. I've been using Lucene for my projects.</p>
",1155,1323924865
How to detect nonsensical text in PHP?,"<p>I have comments enabled on my site and I require users to enter at least 30 characters to publish their comments (Just to get some value because they usualy just submitted ""I like it"")
But some users now use simple technique to overcome this and enter e.g.:</p>

<p>""<strong>I like it. asdsdf dfdsfsdf tt erretrt re""</strong> </p>

<p>As you can see the rest of the text is <strong><em>nonsense</em></strong>. Is there a way (<strong>algorithm</strong>) how to filter these comments out in PHP ?</p>
","php, nlp","<p>Get a dictionary of English words from the net. Check the post has a certain % (maybe 50%? maybe 70%?) of words that are in the dictionary. You can't look for 100%, or names and technical jargon will not be found.</p>

<p>users will get around this by entering.<br>
<strong>I like it ....................................................</strong><br>
So then add logic to parse out punctuation. <br>
Then users will get around it with<br>
<strong>I like it. the the the the the the the the</strong><br>
Then you will need to parse it for proper English grammer<br>
Then no one will be able to post on your site becuase it has too many rules. <br></p>

<p>Better suggestion: Add comment moderation. Dumb posts get downvoted and go away. Good posts stay.</p>
",680,1282065625
How to detect the language type of a given text via Python?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/4545977/python-can-i-detect-unicode-string-language-code"">Python - can I detect unicode string language code?</a>  </p>
</blockquote>



<p>similar question is <a href=""https://stackoverflow.com/q/4545977/4279"">Python - can I detect unicode string language code?</a></p>

<p>I had used Google Translate API to detect language type before. The result is perfect, but it's not free now. So I have to find a substition. Any suggestion?</p>
","python, nlp",,105,1323704699
Efficient Lemmatizer that avoids dictionary lookup,"<p>I want to convert string like 'eat' to 'eating', 'eats'. I searched and found the lemmatization as the solution, but all the lemmatizer tools that I have come across uses wordlist or dictionary-lookup. Is there any lemmatizer which avoids dictionary lookup and gives high efficiency, may be a lemmatizer that is based on rules. Yes and I am not looking for ""stemmer"".</p>
","java, relevance, text-analysis, lemmatization",,916,1323670472
Heuristic Approaches to Finding Main Content,"<p>Wondering if anybody could point me in the direction of academic papers or related implementations of heuristic approaches to finding the real meat content of a particular webpage.</p>

<p>Obviously this is not a trivial task, since the problem description is so vague, but I think that we all have a general understanding about what is meant by the primary content of a page.  </p>

<p>For example, it may include the story text for a news article, but might not include any navigational elements, legal disclaimers, related story teasers, comments, etc.  Article titles, dates, author names, and other metadata fall in the grey category.</p>

<p>I imagine that the application value of such an approach is large, and would expect Google to be using it in some way in their search algorithm, so it would appear to me that this subject has been treated by academics in the past.</p>

<p>Any references?</p>
","parsing, nlp, web-crawler","<p>One way to look at this would be as an information extraction problem.</p>

<p>As such, one high-level algorithm would be to collect multiple examples of the same page type and deduce parsing (or extraction) rules for the parts of the page which are different (this is likely to be the main topic).  The intuition is that common boilerplate (header, footer, etc) and ads will eventually appear on multiple examples of those web pages, so by training on a few of them, you can quickly start to reliably identify this boilerplate/additional code and subsequently ignore it.  It's not foolproof, but this is also the basis of web scraping technologies, both commercial and academic, like RoadRunner:</p>

<p><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.8672&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.8672&amp;rep=rep1&amp;type=pdf</a></p>

<p>The citation is:</p>

<blockquote>
  <p>Valter Crescenzi, Giansalvatore Mecca,
  Paolo Merialdo: RoadRunner: Towards
  Automatic Data Extraction from Large
  Web Sites. VLDB 2001: 109-118</p>
</blockquote>

<p>There's also a well-cited survey of extraction technologies:</p>

<blockquote>
  <p>Alberto H. F. Laender , Berthier A.
  Ribeiro-Neto , Altigran S. da Silva ,
  Juliana S. Teixeira, A brief survey of
  web data extraction tools, ACM SIGMOD
  Record, v.31 n.2, June 2002 
  [doi>10.1145/565117.565137]</p>
</blockquote>
",1003,1297920680
"Need to extract information from free text, information like location, course etc","<p>I need to write a text parser for the education domain which can extract out the information like institute, location, course etc from the free text.</p>

<p>Currently i am doing it through lucene, steps are as follows:</p>

<ol>
<li>Index all the data related to institute, courses and location.</li>
<li>Making shingles of the free text and searching each shingle in location, course and institute index dir and then trying to find out which part of text represents location, course etc. </li>
</ol>

<p>In this approach I am missing lot of cases like B.tech can be written as btech, b-tech or b.tech.</p>

<p>I want to know is there any thing available which can do all these kind of things, I have heard about Ling-pipe and Gate but don't know how efficient they are.</p>
","lucene, nlp, text-parsing",,766,1320756609
Regular expression to match object dimensions,"<p>I'll put it right out there: I'm terrible with regular expressions. I've tried to come up with one to solve my problem but I really don't know much about them. . .</p>

<p>Imagine some sentences along the following lines:</p>

<blockquote>
  <ul>
  <li>Hello blah blah. It's around 11 1/2"" x 32"".</li>
  <li>The dimensions are 8 x 10-3/5!</li>
  <li>Probably somewhere in the region of 22"" x 17"".</li>
  <li>The roll is quite large: 42 1/2"" x 60 yd.</li>
  <li>They are all 5.76 by 8 frames.</li>
  <li>Yeah, maybe it's around 84cm long.</li>
  <li>I think about 13/19"".</li>
  <li>No, it's probably 86 cm actually.</li>
  </ul>
</blockquote>

<p>I want to, as cleanly as possible, extract item dimensions from within these sentences. In a perfect world the regular expression would output the following:</p>

<blockquote>
  <ul>
  <li>11 1/2"" x 32""</li>
  <li>8 x 10-3/5</li>
  <li>22"" x 17""</li>
  <li>42 1/2"" x 60 yd</li>
  <li>5.76 by 8</li>
  <li>84cm</li>
  <li>13/19""</li>
  <li>86 cm</li>
  </ul>
</blockquote>

<p>I imagine a world where the following rules apply:</p>

<ul>
<li>The following are valid units: <code>{cm, mm, yd, yards, "", ', feet}</code>, though I'd prefer a solution that considers an arbitrary set of units rather than an explicit solution for the above units.</li>
<li>A dimension is always described numerically, may or may not have units following it and may or may not have a fractional or decimal part. Being made up of a fractional part on it's own is allowed, e.g., <code>4/5""</code>.</li>
<li>Fractional parts always have a <code>/</code> separating the numerator / denominator, and one can assume there is no space between the parts (though if someone takes that in to account that's great!).</li>
<li>Dimensions may be one-dimensional or two-dimensional, in which case one can assume the following are acceptable for separating two dimensions: <code>{x, by}</code>. If a dimension is only one-dimensional it <strong>must</strong> have units from the set above, i.e., <code>22 cm</code> is OK, <code>.333</code> is not, nor is <code>4.33 oz</code>.</li>
</ul>

<p>To show you how useless I am with regular expressions (and to show I at least tried!), I got this far. . .</p>

<pre><code>[1-9]+[/ ][x1-9]
</code></pre>

<p><strong>Update (2)</strong></p>

<p>You guys are very fast and efficient! I'm going to add an extra few of test cases that haven't been covered by the regular expressions below:</p>

<blockquote>
  <ul>
  <li>The last but one test case is 12 yd x.</li>
  <li>The last test case is 99 cm by.</li>
  <li>This sentence doesn't have dimensions in it: 342 / 5553 / 222.</li>
  <li>Three dimensions? 22"" x 17"" x 12 cm</li>
  <li>This is a product code: c720 with another number 83 x better.  </li>
  <li>A number on its own 21.</li>
  <li>A volume shouldn't match 0.332 oz.</li>
  </ul>
</blockquote>

<p>These should result in the following (# indicates nothing should match):</p>

<blockquote>
  <ul>
  <li>12 yd</li>
  <li>99 cm</li>
  <li>#</li>
  <li>22"" x 17"" x 12 cm</li>
  <li>#</li>
  <li>#</li>
  <li>#</li>
  </ul>
</blockquote>

<p>I've adapted <a href=""https://stackoverflow.com/a/8434591/646300"">M42's</a> answer below, to:</p>

<pre><code>\d+(?:\.\d+)?[\s-]*(?:\d+)?(?:\/\d+)?(?:cm|mm|yd|""|'|feet)(?:\s*x\s*|\s*by\s*)?(?:\d+(?:\.\d+)?[\s*-]*(?:\d+(?:\/\d+)?)?(?:cm|mm|yd|""|'|feet)?)?
</code></pre>

<p>But while that resolves some new test cases it now fails to match the following others. It reports:</p>

<blockquote>
  <ul>
  <li>11 1/2"" x 32"" PASS</li>
  <li>(nothing) FAIL</li>
  <li>22"" x 17"" PASS</li>
  <li>42 1/2"" x 60 yd PASS</li>
  <li>(nothing) FAIL</li>
  <li>84cm PASS</li>
  <li>13/19"" PASS</li>
  <li>86 cm PASS</li>
  <li>22"" PASS</li>
  <li>(nothing) FAIL</li>
  <li><p>(nothing) FAIL</p></li>
  <li><p>12 yd x FAIL</p></li>
  <li>99 cm by FAIL</li>
  <li>22"" x 17"" [and also, but separately '12 cm'] FAIL</li>
  <li><h1>PASS</h1></li>
  <li><h1>PASS</h1></li>
  </ul>
</blockquote>
","regex, parsing, text, nlp, text-extraction",,2803,1323361575
translate by replacing words inside existing text,"<p>What are common approaches for translating certain words (or expressions) inside a given text, when the text must be reconstructed (with punctuations and everythin.) ?</p>

<p>The translation comes from a lookup table, and covers words, collocations, and emoticons like L33t, CUL8R, :-), etc.</p>

<p>Simple string search-and-replace is not enough since it can replace part of longer words (cat > dog  ≠>  caterpillar > dogerpillar).</p>

<p>Assume the following input:</p>

<pre><code>s = ""dogbert, started a dilbert dilbertion proces cat-bert :-)""
</code></pre>

<p>after translation, i should receive something like:</p>

<blockquote>
  <p>result = ""<strong>anna</strong>, started a <strong>george</strong> dilbertion <strong>process</strong> cat-bert <strong>smiley</strong>""</p>
</blockquote>

<p>I can't simply tokenize, since i loose <strong>punctuations</strong> and <strong>word positions</strong>. </p>

<p>Regular expressions, works for normal words, but don't catch special expressions like the smiley :-) but it does .</p>

<pre><code>re.sub(r'\bword\b','translation',s) ==&gt; translation
re.sub(r'\b:-\)\b','smiley',s) ==&gt; :-)
</code></pre>

<p>for now i'm using the above mentioned regex, and simple replace for the non-alphanumeric words, but it's far from being bulletproof.</p>

<p>(p.s. i'm using python)</p>
","python, regex, language-agnostic, string, nlp","<p>I had a similar problem with standard emoticons to be replaced with values. <a href=""http://en.wikipedia.org/wiki/List_of_emoticons"" rel=""nofollow"">Here</a> is a list of emoticons. I had them in a plain text file (so that I can append/delete to it as and when required) separated by tab like.  </p>

<pre><code>:[        -1
:/        -1
:(          -1
:)         1
</code></pre>

<p>Then read it into a dictionary</p>

<pre><code>emoticons = {}          
for line in open('data/emoticons.txt').xreadlines():   
        symbol, value = line.split('\t')                                           
        emoticons[str(symbol)] = int(value)
</code></pre>

<p>Then a lookup function</p>

<pre><code>def mark_emoticons(t):
    for w, v in emoticons.items():
        match = re.search(re.escape(w),t)
            if match:
                print w, ""found ""
</code></pre>

<p>Call the function with </p>

<pre><code>mark_emoticons('Hello ladies! How are you? Fantastic :) Look at your man ...')
</code></pre>

<p>As for L33t-speak I have a separate file slangs.txt, which looks like</p>

<pre><code>u   you
ur  you are
uw  you are welcome 
wb  welcome back 
wfm works for me 
wtf what the fuck
</code></pre>

<p>A similar function to read it to dictionary slangs{} and a similar function to replace the slangs.</p>

<pre><code>def mark_slangs(t):        
    for w, v in slangs.items():
            s = r'\b' + w + r'\b'
            match = re.search(s,t)
            if match:
                    #print w, ""found in:"",t, ""replacing with"",readtable.slangs[w]
                    t = re.sub(w,slangs[w].rstrip(),t)
                    ...
</code></pre>

<p>From <a href=""http://docs.python.org/library/re.html"" rel=""nofollow"">Python library</a> the re.escape()</p>

<blockquote>
  <p>re.escape(string) 
  Return string with
  all non-alphanumerics backslashed;
  this is useful if you want to match an
  arbitrary literal string that may have
  regular expression metacharacters in
  it.</p>
</blockquote>

<p>Based on your needs you might want to use re.findall()</p>
",1000,1265663590
Topia Term Extract - Italian Lexicon,"<p>I'm looking for a tool to extract topic keywords from text. Topia seems to be a good solutions, anyway it does not came with an italian-lexicon file.</p>

<p>Searching on the web i couldn't find a precompiled file, so I guess I need to build my own. Does anyone have suggestion on how to build it without spending a lifetime?</p>

<p>The english file is formatted like this:</p>

<pre><code>images NNS
psychiatric JJ
Hope NNP NN VB VBP
elimination NN
</code></pre>

<p>Thanks in advance for any advice.</p>
","python, nlp, pos-tagger","<p>NLP resources for Italian:</p>

<ul>
<li><a href=""http://aclweb.org/aclwiki/index.php?title=Resources_for_Italian"" rel=""nofollow"">http://aclweb.org/aclwiki/index.php?title=Resources_for_Italian</a></li>
</ul>

<p>Of special interest to you are the lexicons and taggers.</p>

<p>Another lexicon:</p>

<ul>
<li><a href=""http://aune.lpl.univ-aix.fr/projects/multext/LEX/LEX.SmpIt.html"" rel=""nofollow"">http://aune.lpl.univ-aix.fr/projects/multext/LEX/LEX.SmpIt.html</a></li>
</ul>
",771,1323266123
Is there a way to convert nltk featuresets into a scipy.sparse array?,"<p>I'm trying to use scikit.learn which needs numpy/scipy arrays for input.
The featureset generated in nltk consists of unigram and bigram frequencies. I could do it manually, but that'll be a lot of effort. So wondering if there's a solution i've overlooked.</p>
","python, nlp, nltk, scikits","<p>Not that I know of, but note that scikit-learn can do <em>n</em>-gram frequency counting itself. Assuming word-level <em>n</em>-grams:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer, WordNGramAnalyzer
v = CountVectorizer(analyzer=WordNGramAnalyzer(min_n=1, max_n=2))
X = v.fit_transform(files)
</code></pre>

<p>where <code>files</code> is a list of strings or file-like objects. After this, <code>X</code> is a scipy.sparse matrix of raw frequency counts.</p>
",693,1323135172
Could you recommend a NLP toolkit in Prolog?,"<p>I need to parse or tokenize English sentences.
Is there any NLP toolkit in Prolog?
Thanks.</p>
","prolog, nlp",,1374,1322801435
Natural Language parser for parsing sports play-by-play data,"<p>I'm trying to come up with a parser for football plays. I use the term ""natural language"" here very loosely so please bear with me as I know little to nothing about this field.</p>

<p>Here are some examples of what I'm working with 
(Format: TIME|DOWN&amp;DIST|OFF_TEAM|DESCRIPTION):</p>

<pre><code>04:39|4th and 20@NYJ46|Dal|Mat McBriar punts for 32 yards to NYJ14. Jeremy Kerley - no return. FUMBLE, recovered by NYJ.|
04:31|1st and 10@NYJ16|NYJ|Shonn Greene rush up the middle for 5 yards to the NYJ21. Tackled by Keith Brooking.|
03:53|2nd and 5@NYJ21|NYJ|Mark Sanchez rush to the right for 3 yards to the NYJ24. Tackled by Anthony Spencer. FUMBLE, recovered by NYJ (Matthew Mulligan).|
03:20|1st and 10@NYJ33|NYJ|Shonn Greene rush to the left for 4 yards to the NYJ37. Tackled by Jason Hatcher.|
02:43|2nd and 6@NYJ37|NYJ|Mark Sanchez pass to the left to Shonn Greene for 7 yards to the NYJ44. Tackled by Mike Jenkins.|
02:02|1st and 10@NYJ44|NYJ|Shonn Greene rush to the right for 1 yard to the NYJ45. Tackled by Anthony Spencer.|
01:23|2nd and 9@NYJ45|NYJ|Mark Sanchez pass to the left to LaDainian Tomlinson for 5 yards to the 50. Tackled by Sean Lee.|
</code></pre>

<p>As of now, I've written a dumb parser that handles all the easy stuff (playID, quarter, time, down&amp;distance, offensive team) along with some scripts that goes and gets this data and sanitizes it into the format seen above. A single line gets turned into a ""Play"" object to be stored into a database.</p>

<p>The tough part here (for me at least) is parsing the description of the play.  Here is some information that I would like to extract from that string:</p>

<p>Example string:</p>

<pre><code>""Mark Sanchez pass to the left to Shonn Greene for 7 yards to the NYJ44. Tackled by Mike Jenkins.""
</code></pre>

<p>Result:</p>

<pre><code>turnover = False
interception = False
fumble = False
to_on_downs = False
passing = True
rushing = False
direction = 'left'
loss = False
penalty = False
scored = False
TD = False
PA = False
FG = False
TPC = False
SFTY = False
punt = False
kickoff = False
ret_yardage = 0
yardage_diff = 7
playmakers = ['Mark Sanchez', 'Shonn Greene', 'Mike Jenkins']
</code></pre>

<p>The logic that I had for my initial parser went something like this:</p>

<pre><code># pass, rush or kick
# gain or loss of yards
# scoring play
    # Who scored? off or def?
    # TD, PA, FG, TPC, SFTY?
# first down gained
# punt?
# kick?
    # return yards?
# penalty?
    # def or off?
# turnover?
    # INT, fumble, to on downs?
# off play makers
# def play makers
</code></pre>

<p>The descriptions can get pretty hairy (multiple fumbles &amp; recoveries with penalties, etc) and I was wondering if I could take advantage of some NLP modules out there. Chances are I'm going to spend a few days on a dumb/static state-machine like parser instead but if anyone has suggestions on how to approach it using NLP techniques I'd like to hear about them.</p>
","python, parsing, nlp",,907,1321754757
rule based questioning answering for reading comprehension (QUARC),"<p>Does anyone know  the algorithm and asp classic script about rule based questioning answering for reading comprehension (QUARC) for WHAT, WHY, and HOW ? I need to integrate it with the database. thank you :)</p>
","asp-classic, nlp",,141,1323183881
"Match trigrams, bigrams, and unigrams to a text; if unigram or bigram a substring of already matched trigram, pass; python","<p>main_text is a list of lists containing sentences that've been part-of-speech tagged:</p>

<pre><code> main_text = [[('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN'), ('likes','VB'),    
              ('tea','NN'), ('and','CC'), ('hats', 'NN')], [('the', 'DT'), ('red','JJ')                   
               ('queen', 'NN'), ('hates','VB'),('alice','NN')]]  
</code></pre>

<p>ngrams_to_match is a list of lists containing part-of-speech tagged trigrams:</p>

<pre><code> ngrams_to_match = [[('likes','VB'),('tea','NN'), ('and','CC')],
                    [('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN')],
                    [('hates', 'DT'), ('alice', 'JJ'), ('but', 'CC') ],
                    [('and', 'CC'), ('the', 'DT'), ('rabbit', 'NN')]]
</code></pre>

<p>(a) For each sentence in main_text, first check to see if a complete trigram in ngrams_to _match matches.  If the trigram matches, return the matched trigram and the sentence. </p>

<p>(b) Then, check to see if the the first tuple (a unigram) or the first two tuples (a bigram) of each of the trigrams match in main_text.   </p>

<p>(c) If the unigram or bigram forms a substring of an already matched trigram, don't return anything. Otherwise, return the bigram or unigram match and the sentence.</p>

<p>Here is what the output should be:</p>

<pre><code> trigram_match = [('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN')], sentence[0]
 trigram_match = [('likes','VB'),('tea','NN'), ('and','CC')], sentence[0]
 bigram_match = [('hates', 'DT'), ('alice', JJ')], sentence[1]
</code></pre>

<p>Condition (b) gives us the bigram_match.</p>

<p>The WRONG output would be:</p>

<pre><code> trigram_match = [('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN')], sentence[0]
 bigram_match =  [('the', 'DT'), ('mad', 'JJ')] #*bad by condition c
 unigram_match = [ [('the', 'DT')] #*bad by condition c
 trigram_match = [('likes','VB'),('tea','NN'), ('and','CC')], sentence[0]
 bigram_match = [('likes','VB'),('tea','NN')] #*bad by condition c
 unigram_match [('likes', 'VB')]# *bad by condition c
</code></pre>

<p>and so on.</p>

<p>The following, very ugly code works okay for this toy example. But I was wondering if anyone had a more streamlined approach.</p>

<pre><code> for ngram in ngrams_to_match:
  for sentence in main_text:
        for tup in sentence:

            #we can't be sure that our part-of-speech tagger will
            #tag an ngram word and a main_text word the same way, so 
            #we match the word in the tuple, not the whole tuple

        if ngram[0][0] == tup[0]: #if word in the first ngram matches...
            unigram_index = sentence.index(tup) #...then this is our index
            unigram = (sentence[unigram_index][0]) #save it as a unigram

            try:   
                        if sentence[unigram_index+2][0]==ngram[2][0]:
                 if sentence[unigram_index+2][0]==ngram[2][0]:  #match a trigram
                      trigram = (sentence[unigram_index][0],span[1][0], ngram[2][0])#save the match
                      print 'heres the trigram--&gt;', sentence,'\n', 'trigram---&gt;',trigram
            except IndexError:
            pass
            if ngram[0][0] == tup[0]:# == tup[0]:  #same as above
                unigram_index = sentence.index(tup)               
                if sentence[unigram_index+1][0]==span[1][0]:  #get bigram match     

                bigram = (sentence[unigram_index][0],span[1][0])#save the match
                if bigram[0] and bigram[1] in trigram:  #no substring matches
                                     pass                             
                else:
                    print 'heres a sentence--&gt;', sentence,'\n', 'bigram---&gt;', bigram
                if unigram in bigram or trigram:  #no substring matches
                    pass
                else:
                    print unigram 
</code></pre>
","python, nlp, text-processing","<p>I've had a stab at implementing this using a generator. I found some gaps in your spec, so I've made assumptions.</p>

<p><em>If the unigram or bigram forms a substring of an already matched trigram, don't return anything.</em> - Is a bit ambiguous about which gram is referring to the search elements or the matched elements. Makes me start to hate the use of the <code>N-gram</code> words (which I'd never heard of before last week).</p>

<p>Play with what gets added to the <code>found</code> set in order to modify excluded search elements.</p>

<pre><code># assumptions:
# - [('hates','DT'),('alice','JJ'),('but','CC')] is typoed and should be:
#   [('hates','VB'),('alice','NN'),('but','CC')]
# - matches can't overlap, matched elements are excluded from further checking
# - bigrams precede unigrams

main_text = [
  [('the','DT'),('mad','JJ'),('hatter','NN'),('likes','VB'),('tea','NN'),('and','CC'),('hats','NN')],
  [('the','DT'),('red','JJ'),('queen','NN'),('hates','VB'),('alice','NN')]
]
ngrams_to_match = [
  [('likes','VB'),('tea','NN'),('and','CC')],
  [('the','DT'),('mad','JJ'),('hatter','NN')],
  [('hates','VB'),('alice','NN'),('but','CC')],
  [('and','CC'),('the','DT'),('rabbit','NN')]
]

def slice_generator(sentence,size=3):
  """"""
  Generate slices through the sentence in decreasing sized windows. If True is sent to the
  generator, the elements from the previous window will be excluded from future slices.
  """"""
  sent = list(sentence)
  for c in range(size,0,-1):
    for i in range(len(sent)):
      slice = tuple(sent[i:i+c])
      if all(x is not None for x in slice) and len(slice) == c:
        used = yield slice
        if used:
          sent[i:i+size] = [None] * c

def gram_search(text,matches):
  tri_bi_uni = set(tuple(x) for x in matches) | set(tuple(x[:2]) for x in matches) | set(tuple(x[:1]) for x in matches)
  found = set()
  for i, sentence in enumerate(text):
    gen = slice_generator(sentence)
    send = None
    try:
      while True:
        row = gen.send(send)
        if row in tri_bi_uni - found:
          send = True
          found |= set(tuple(row[:x]) for x in range(1,len(row)))
          print ""%s_gram_match, sentence[%s] = %r"" % (len(row),i,row)
        else:
          send = False
    except StopIteration:
      pass

gram_search(main_text,ngrams_to_match)
</code></pre>

<p>Yields:</p>

<pre>3_gram_match, sentence[0] = (('the', 'DT'), ('mad', 'JJ'), ('hatter', 'NN'))
3_gram_match, sentence[0] = (('likes', 'VB'), ('tea', 'NN'), ('and', 'CC'))
2_gram_match, sentence[1] = (('hates', 'VB'), ('alice', 'NN'))
</pre>
",6753,1322526636
NLP using Wikipedia (java programming),"<p>Seeking for help in order to make a definitive decision.
For some months, I'm looking for a Java API which helps me to access Wikipedia and get the content of articles. My project is to build a taxonomy of concepts of a given domain.
Details:</p>

<ol>
<li>I have a corpus of domain texts, I extract the first set of terms (that represents the domain).</li>
<li>I search in Wikipedia the articles of these words in order to extract their definitions. The definition of the word helps me to find the hyperonym of this word. The call for Wikipedia will surely be done in a java loop.</li>
<li>I search the definitions of the hyperonyms found in the previous step to find their hyperonyms, and so on.</li>
<li>I draw a graph linking the words to their hyperonyms.</li>
</ol>

<p>My problem is that for the step 2, I cannot make a definitive decision.</p>

<ol>
<li>I wrote Java code to access Wikipedia online. It succeeds but the speed of my connexion determines if the execusion succeeds or fails giving a set of exceptions. Sometimes, the execution gives me only 2 or 3 articles.</li>
<li>I tryed to use JWPL to treat Wikipedia dumps. I failed because I have not enough RAM.</li>
<li>I'm now hesitating between a set of Java APIs.</li>
</ol>

<p>Please give me your points of views if you have already done something in this sense. I made a serious investigation and I found the following links:</p>

<ol>
<li><a href=""http://wdm.cs.waikato.ac.nz:8080/wiki/Wiki.jsp?page=Installing%20the%20Java%20API"" rel=""nofollow"">http://wdm.cs.waikato.ac.nz:8080/wiki/Wiki.jsp?page=Installing%20the%20Java%20API</a></li>
<li><a href=""http://jwikiapi.sourceforge.net/index.html"" rel=""nofollow"">http://jwikiapi.sourceforge.net/index.html</a></li>
<li><a href=""http://code.google.com/p/gwtwiki/"" rel=""nofollow"">http://code.google.com/p/gwtwiki/</a></li>
<li><a href=""http://www.mediawiki.org/wiki/API%3aMain_page"" rel=""nofollow"">http://www.mediawiki.org/wiki/API%3aMain_page</a></li>
<li><a href=""http://jwbf.sourceforge.net/"" rel=""nofollow"">http://jwbf.sourceforge.net/</a></li>
</ol>

<p>I'd appreciate any suggestions.</p>
","java, nlp, wikipedia",,705,1322784454
How can I select a FAQ entry from a user&#39;s natural-language inquiry?,"<p>I am working on an app where the user submits a series of questions. These questions are freeform text, but are based on a specific product, so I have a general understanding of the context.  I have a FAQ listing, and I need to try to match the user's question to a question in the FAQ.</p>

<p>My language is Delphi.  My general thought approach is to throw out small ""garbage words"", a, an, the, is, of, by, etc...  Run a stemming program over these words to get the root words, and then try to match as many of the remaining words as possible.  </p>

<p>Is there a better approach?  I have thought about some type of natural language processing, but I am afraid that I would be looking at years of development, rather than a week or two.</p>
","delphi, text, nlp, matching","<p>Not sure if this solution is precisely what you're looking for, but if you're looking to parse natural language, you could use the Link-Grammar Parser.</p>

<p>Thankfully, I've translated this for use with Delphi (complete with a demo), which you can download (free and 100% open source) <a href=""http://www.simonjstuart.com/delphi-stuff/link-grammar-parser/"" rel=""nofollow"">from this page on my blog</a>.</p>
",356,1322684572
How to understand if an input sentence has a negative or a positive meaning?,"<p>I am doing a project in Java, and I am looking for some open-source/tool that will given a sentence return if the sentence has a negative or a positive meaning.</p>

<p>For example:</p>

<ol>
<li>I have a problem-Negative.</li>
<li>I don't have a problem-Positive.</li>
<li>I have a working connection-Positive</li>
</ol>

<p>Does such tool exist?</p>
","java, nlp",,3127,1322648538
What are good starting points for someone interested in natural language processing?,"<h1>Question</h1>

<p>So I've recently came up with some new possible projects that would have to deal with deriving 'meaning' from text submitted and generated by users.</p>

<p><a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow noreferrer"">Natural language processing</a> is the field that deals with these kinds of issues, and after some initial research I found the <a href=""http://opennlp.sourceforge.net/"" rel=""nofollow noreferrer"">OpenNLP Hub</a> and university collaborations like the <a href=""http://attempto.ifi.uzh.ch/site/"" rel=""nofollow noreferrer"">attempto project</a>. And stackoverflow has <a href=""https://stackoverflow.com/questions/88984/your-favorite-natural-language-parser"">this</a>.</p>

<p>If anyone could link me to some good resources, from reseach papers and introductionary texts to apis, I'd be happier than a 6 year-old kid opening his christmas presents!</p>

<h1>Update</h1>

<p>Through one of your recommendations I've found <a href=""http://www.opencyc.org/"" rel=""nofollow noreferrer"">opencyc</a> (<em>'the world's largest and most complete general knowledge base and commonsense reasoning engine'</em>). Even more amazing still, there's a project that is a distilled version of opencyc called <a href=""http://umbel.org/"" rel=""nofollow noreferrer"">UMBEL</a>. It features semantic data in rdf/owl/skos n3 syntax.</p>

<p>I've also stumbled upon <a href=""http://antlr.org/"" rel=""nofollow noreferrer"">antlr</a>, a parser generator for <em>'constructing recognizers, interpreters, compilers, and translators from grammatical descriptions'</em>.</p>

<p>And there's a question on here by me, that lists tons of <a href=""https://stackoverflow.com/questions/202092/where-can-i-find-free-and-open-data"">free and open data</a>.</p>

<p>Thanks stackoverflow community!</p>
","nlp, dcg","<p>Tough call, NLP is a much wider field than most people think it is. Basically, language can be split up into several categories, which will require you to learn totally different things.</p>

<p>Before I start, let me tell you that I doubt you'll have any notable success (as a professional, at least) without having a degree in some (closely related) field. There is a lot of theory involved, most of it is dry stuff and hard to learn. You'll need a lot of endurance and most of all: time.</p>

<p>If you're interested in the meaning of text, well, that's the Next Big Thing. Semantic search engines are predicted as initiating Web 3.0, but we're far from 'there' yet. Extracting logic from a text is dependant on several steps:</p>

<ul>
<li>Tokenization, Chunking</li>
<li>Disambiguation on a lexical level (Time flies like an arrow, but fruit flies like a banana.)</li>
<li>Syntactic Parsing</li>
<li>Morphological analysis (tense, aspect, case, number, whatnot)</li>
</ul>

<p>A small list, off the top of my head. There's more :-), and many more details to each point. For example, when I say ""parsing"", what is this? There are <em>many</em> different parsing algorithms, and there are just as many parsing formalisms. Among the most powerful are <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Tree_adjoining_grammar"" rel=""noreferrer"">Tree-adjoining grammar</a> and <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/HPSG"" rel=""noreferrer"">Head-driven phrase structure grammar</a>. But both of them are hardly used in the field (for now). Usually, you'll be dealing with some half-baked generative approach, and will have to conduct morphological analysis yourself.</p>

<p>Going from there to semantics is a big step. A Syntax/Semantics interface is dependant both, on the syntactic <em>and</em> semantic framework employed, and there is no single working solution yet. On the semantic side, there's classic generative semantics, then there is <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Discourse_Representation_Theory"" rel=""noreferrer"">Discourse Representation Theory</a>, <a href=""http://staff.science.uva.nl/~stokhof/papers.html"" rel=""noreferrer"">dynamic semantics</a>, and many more. Even the logical formalism everything is based on is still not well-defined. Some say one should use first-order logic, but that hardly seems sufficient; then there is intensional logic, as used by Montague, but that seems overly complex, and computationally unfeasible. There also is dynamic logic (Groenendijk and Stokhof have pioneered this stuff. Great stuff!) and very recently, this summer actually, <a href=""http://home.medewerker.uva.nl/j.a.g.groenendijk/"" rel=""noreferrer"">Jeroen Groenendijk</a> presented a new formalism, <em>Inquisitive</em> <em>Semantics</em>, also very interesting.</p>

<p>If you want to get started on a very simple level, read <a href=""http://homepages.inf.ed.ac.uk/jbos/comsem/"" rel=""noreferrer"">Blackburn and Bos (2005)</a>, it's great stuff, and the de-facto introduction to Computational Semantics! I recently extended their system to cover the partition-theory of questions (question answering is a beast!), as proposed by Groenendijk and Stokhof (1982), but unfortunately, the theory has a complexity of O(n²) over the domain of individuals. While doing so, I found B&amp;B's implementation to be a bit, erhm… hackish, at places. Still, it is going to really, really help you dive into computational semantics, and it is still a very impressive showcase of what can be done. Also, they deserve extra cool-points for implementing a grammar that is settled in Pulp Fiction (the movie).</p>

<p>And while I'm at it, pick up Prolog. A lot of research in computational semantics is based on Prolog. <a href=""http://www.coli.uni-saarland.de/~kris/learn-prolog-now/"" rel=""noreferrer"">Learn Prolog Now!</a> is a good intro. I can also recommend ""The Art of Prolog"" and Covington's ""Prolog Programming in Depth"" and ""Natural Language Processing for Prolog Programmers"", the former of which is available for free online.</p>
",8738,1224251571
Natural Language processing for getting qualitative info,"<p>I have a requirement for which I do not know if NL can do it. Plz advise.</p>

<p>My requirement is to scan a sentence in english language and figure out some qualitative info about it. Such as , what are the subjects , nouns in the sentence and what is said about them is a descriptive , suggestive  or does it affects positively or negatively. </p>

<p>As an example, lets say I have a Fan page in facebook , and someone posts a post on my wall. I need to know if the post says something good or bad about me and accordingly I can map it to a perception scale from , say -10 to +10. </p>

<p>Can something like this be done with with Natural language processing toolkits?
If no, then what is the way.</p>

<p>Thanks in advance
Shyam </p>
",nlp,"<p>Yes, Natural Language Processing methods <em>can</em> do what you ask for:</p>

<p>To find out which are the ""nouns"" in a text is called <a href=""http://en.wikipedia.org/wiki/Pos-tagging"" rel=""noreferrer"">POS-Tagging</a>.
Identifying the syntactical strucure of sentences is called <a href=""http://en.wikipedia.org/wiki/Parsing#Human_languages"" rel=""noreferrer"">parsing</a>. Depending on your parsing method and how strict the syntax of the language is you might want to look into <a href=""http://en.wikipedia.org/wiki/Semantic_role_labeling"" rel=""noreferrer"">semantic role labeling</a> aswell to find subjects/objects in a sentence. Classifying the polarity (positive/negative) of a statement is called <a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""noreferrer"">sentiment analysis</a>.</p>

<p>The Python <a href=""http://www.nltk.org/"" rel=""noreferrer"">NLTK</a> provides some tools you can get started with, but sentiment analysis is an active research area and a task where a lot of other NLP-methods have to work together, so it is certainly not the easiest field to get started with in NLP. Anyways, a survey of the academic research in the field can be found in <a href=""http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf"" rel=""noreferrer"">Pang &amp; Lee (2008)</a>.</p>
",182,1322341171
Problems with Prolog&#39;s DCG,"<p>The project is about translating semi-natural language to SQL tables. The code:</p>

<pre><code>label(S) --&gt; label_h(C), {atom_codes(A, C), string_to_atom(S, A)}, !.

label_h([C|D]) --&gt; letter(C), letters_or_digits(D), !.

letters_or_digits([C|D]) --&gt; letter_or_digit(C), letters_or_digits(D), !.
letters_or_digits([C]) --&gt; letter_or_digit(C), !.
letters_or_digits([]) --&gt; """", !.

letter(C) --&gt; [C], {""a""=&lt;C, C=&lt;""z""}, !.
letter(C) --&gt; [C], {""A""=&lt;C, C=&lt;""Z""}, !.
letter_or_digit(C) --&gt; [C], {""a""=&lt;C, C=&lt;""z""}, !.
letter_or_digit(C) --&gt; [C], {""A""=&lt;C, C=&lt;""Z""}, !.
letter_or_digit(C) --&gt; [C], {""0""=&lt;C, C=&lt;""9""}, !.

table(""student"").

sbvr2sql --&gt; label(Name), "" is an integer."", {assert(fields(Name, ""INT""))}.
sbvr2sql --&gt; label(Name), "" is a string."", {assert(fields(Name, ""VARCHAR(64)""))}.

sbvr2sql(Table, Property)  --&gt; label(Table), "" has "", label(Property), ""."".
</code></pre>

<p>Here is how it works fine:</p>

<pre><code>?- sbvr2sql(""age is an integer."", []).
true 

?- sbvr2sql(""firstName is a string."", []).
true.

?- sbvr2sql(T, P, ""student has firstName."", []).
T = ""student"",
P = ""firstName"".

?- fields(F, T).
F = ""age"",
T = [73, 78, 84] n
F = ""firstName"",
T = [86, 65, 82, 67, 72, 65, 82, 40, 54|...].

?- sbvr2sql(T, P, ""student has firstName."", []), fields(P, _).
T = ""student"",
P = ""firstName"".
</code></pre>

<p>But it doesn't work here:</p>

<pre><code>?- table(T).
T = [115, 116, 117, 100, 101, 110, 116]. % ""student""

?- sbvr2sql(T, P, ""student has firstName."", []), table(T).
false.
</code></pre>

<p>Apparently it doesn't recognise <code>table(""student"")</code> as true. It recognises ""student"" as a label as seen above. What gives?</p>
","prolog, nlp, grammar, dcg","<p>I can't reproduce the error, but I suspect it may be in your <code>label/3</code> rule. When I used the following definition of this rule:</p>

<pre><code>label([C|S]) --&gt;
    [C], {[Sp|_] = "" "", C \= Sp, [Dot|_] = ""."", C \= Dot}, !,
    label(S).
label([],X,X).
</code></pre>

<p>I get correct results:</p>

<pre><code>?- sbvr2sql(TS, PS, ""student has firstName."", []), table(TS),
   atom_codes(P,PS), atom_codes(T,TS).
TS = [115, 116, 117, 100, 101, 110, 116],
PS = [102, 105, 114, 115, 116, 78, 97, 109, 101],
P = firstName,
T = student.
</code></pre>

<p>In general, I'd recommend tokenizing strings into lists of atoms before doing DCG manipulation. That way, it's much easier to debug because of Prolog's awkward string output.</p>
",428,1296211791
using Dependency Parser in Stanford coreNLP,"<p>I am using the Stanford coreNLP ( <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a> ) in order to parse sentences and extract dependencies between the words.</p>

<p>I have managed to create the dependencies graph like in the example in the supplied link, but I don't know how to work with it. I can print the entire graph using the <code>toString()</code> method, but the problem I have is that the methods that search for certain words in the graph, such as <code>getChildList</code>, require an IndexedWord object as a parameter. Now, it is clear why they do because the nodes of the graph are of IndexedWord type, but it's not clear to me how I create such an object in order to search for a specific node.</p>

<p>For example: I want to find the children of the node that represents the word ""problem"" in my sentence. How I create an IndexWord object that represents the word ""problem"" so I can search for it in the graph?</p>
","nlp, stanford-nlp","<p>In general, you shouldn't be creating your own IndexedWord objects. (These are used to represent ""word tokens"", i.e., particular words in a text, not ""word types"", and so asking for the word ""problem"" -- a word type -- isn't really valid; in particular, a sentence could have multiple tokens of this word type.)</p>

<p>There are a couple of convenience methods that let you do what you want:</p>

<ul>
<li>sg.getNodeByWordPattern(String pattern)</li>
<li>sg.getAllNodesByWordPattern(String pattern)</li>
</ul>

<p>The first is a little dangerous, since it just returns the first IndexedWord matching the pattern, or null if there are none.  But it's most directly what you asked for.</p>

<p>Some other methods to start from are:</p>

<ul>
<li>sg.getFirstRoot() to find the (first, usually only) root of the graph and then to navigate down from there, such as by using the sg.getChildren(root) method.</li>
<li>sg.vertexSet() to get all of the IndexWord objects in the graph.</li>
<li>sg.getNodeByIndex(int) if you already know the input sentence, and therefore can ask for words by their integer index.</li>
</ul>

<p>Commonly these methods leave you iterating through nodes. Really, the first two get...Node... methods just do the iteration for you.  </p>
",4318,1321544027
"Finding city, country, company name from a tweet text using Java","<p>I am trying to build a sample app where in I want to parse a tweet and find the city name, 
country name and company name in that tweet. </p>

<p>The dumb way to do this can be maintaining list 
of names of country, city and company names and finding those in a tweet text but that 
approach will require change every time I want to add something new. </p>

<p>Is there a <em>library</em> which can parser a string and give me this information? Or can you suggest me a way that I should take? </p>
","java, text, twitter, nlp",,921,1322053477
IR and QA - Beginner Project Scope,"<p>I have been brainstorming for an Undergraduate Project in Question Answering domain. A project that has components of IR and NLP.</p>

<p>The first thing that popped up, was of course factoid question answering, but that seemed to be an already conquered problem. #IBM Watson!</p>

<p>Non-factoid QA seems interesting, so I took it up. Now, we are in scope-it-out phase of the project description. So, from the ambitious goal - of answering any question put up by the user - I need to scope out our project.    </p>

<p>So I took the following decisions: </p>

<ol>
<li>It will be closed-domain - C++ Programming</li>
<li>The corpus will consist of just one website. (cplusplus or wikipedia) or just one document (the complete reference)</li>
<li>We will develop only one module of the entire QA architecture - Passage Retrieval or Answer Extraction.</li>
</ol>

<p>Our mentor insists on implementing an already existing solution, to start with. 
I am stuck at this point, to search for existing implementations. <a href=""http://alchemy.cs.washington.edu/papers/poon09/"" rel=""nofollow"">Here is one</a>. But when I read through the environment requirements, it was staggering. There are a lot of libraries and tool kits, but I didn't find any non-factoid QA system, that was good to know at least on a very small scale.</p>

<p>Suggest a good scope for the project. I wish to continue working on this through my masters, so it what would be a good start? We have about 4 months for the project, and it is important not to end up doing a research project. It should have a tangible output.  </p>
","nlp, information-retrieval",,455,1321727714
Simple toolkits for emotion (sentiment) analysis (not using machine learning),"<p>I am looking for a tool that can analyze the emotion of short texts. I searched for a week and I couldn't find a good one that is publicly available. The ideal tool is one that takes a short text as input and guesses the emotion. It is preferably a standalone application or library.</p>

<p>I don't need tools that is trained by texts. And although similar questions are asked before no satisfactory answers are got.</p>

<p>I searched the Internet and read some papers but I can't find a good tool I want. Currently I found SentiStrength, but the accuracy is not good. I am using emotional dictionaries right now. I felt that some syntax parsing may be necessary but it's too complex for me to build one. Furthermore, it's researched by some people and I don't want to reinvent the wheels. Does anyone know such publicly/research available software? I need a tool that doesn't need training before using.
Thanks in advance.</p>
","nlp, sentiment-analysis","<p>Maybe <a href=""http://dtminredis.housing.salle.url.edu:8080/EmoLib/"" rel=""nofollow"">EmoLib</a> could be of help.</p>
",1914,1309198686
How to prepare text data for orange SVM train?,"<p>I used NLTK classifiers 2 years ago. Now I want to learn to use orange SVM for text classification. The example for SVM in orange tutorial is iris.tab: </p>

<pre><code>sepal length    sepal width petal length    petal width iris
c   c   c   c   d
                class
5.1 3.5 1.4 0.2 Iris-setosa
4.9 3.0 1.4 0.2 Iris-setosa
</code></pre>

<p>If I want to classify text, how to prepare data. Is it like the below?</p>

<pre><code>token     frequency     tokenlength

the        23             3
for        21             3
at         10             2
</code></pre>

<p>Please give me examples of different ways of preparing data. Can token be seen as label in SVM, if not, how to do it?</p>

<p>Thanks very much in advance.  </p>
","python, nlp, svm, orange","<p>Short answer: No.</p>

<p>Long answer: The label refers to the category of documents you want to process. For example if you are trying to categorize documents into two categories, such as SPAM and HAM, then the labels should be SPAM and HAM. For data representation you may use tecnhiques such as Bag of Words (http://en.wikipedia.org/wiki/Bag_of_words_model). </p>

<p>For further information I suggest the following:</p>

<ul>
<li>SVM Text Classification, <a href=""http://www.igvita.com/2006/06/02/svm-text-classification/"" rel=""nofollow"">http://www.igvita.com/2006/06/02/svm-text-classification/</a></li>
<li>Learning to Classify Text using Support Vector Machines, <a href=""http://www.cs.cornell.edu/People/tj/svmtcatbook/"" rel=""nofollow"">http://www.cs.cornell.edu/People/tj/svmtcatbook/</a></li>
</ul>
",2175,1321540486
API to find Nouns in Sentence and Nearest Adjective in Meaning in Ruby,"<p>I'm looking for an API or Ruby Gem that can do two things. The first is look up each word and see if it is a noun or not. The second thing I want to be able to do is look up adjectives (and maybe nouns) and find the word that is most identical to it. What is the best way to do this? </p>
","ruby, linguistics, nlp","<p>I don't know about Ruby, but to determine the part of speech of a word (like whether it's a noun) you need what's called a ""part of speech tagger"". For the second part, it sounds like WordNet will help you. WordNet is a database of English words (you didn't say what language you're interested in) with relationships like ""similar in meaning"", ""more specific"" (like ""cat"" is more specific than ""animal""), ""opposite in meaning"", etc.</p>
",1480,1321807857
Finding word collocations in Java,"<p>I am trying to find <a href=""http://nlp.stanford.edu/fsnlp/promo/colloc.pdf"" rel=""nofollow"">collocations (PDF)</a> in Java.</p>

<p>I know <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.collocations-module.html"" rel=""nofollow"">NLTK</a> has a collocations module, but do not want to use Jython.</p>

<p>I looked at OpenNLP and GATE, but they did not seem to have a collocation finder.</p>

<p>Does anybody know a free open source collocation finder implemented 
in Java? </p>
","java, nlp","<p>For me the best ready to use algorithm for collcation is <a href=""http://dragon.ischool.drexel.edu/xtract.asp"" rel=""nofollow"">xTract</a> in DragonToolkit it uses basic statistics collocation features like lingpipe and more sophisticated such as POS tagging</p>
",2011,1318763931
"Ntlk &amp; Python, plotting ROC curve","<p>I am using nltk with Python and I would like to plot the ROC curve of my classifier (Naive Bayes). Is there any function for plotting it or should I have to track the True Positive rate and False Positive rate ?</p>

<p>It would be great if someone would point me to some code already doing it...</p>

<p>Thanks.</p>
","python, nlp, machine-learning, nltk","<p>PyROC looks simple enough: <a href=""http://aimotion.blogspot.com/2010/09/tools-for-machine-learning-performance.html"" rel=""nofollow"">tutorial</a>, <a href=""http://aimotion.blogspot.com/2010/09/tools-for-machine-learning-performance.html"" rel=""nofollow"">source code</a></p>

<p>This is how it would work with the NLTK naive bayes classifier:</p>

<pre><code># class labels are 0 and 1
labeled_data = [
    (1, featureset_1),
    (0, featureset_2),
    (1, featureset_3),
    # ...
]

# naive_bayes is your already trained classifier,
# preferrably not on the data you're testing on :)

from pyroc import ROCData

roc_data = ROCData(
    (label, naive_bayes.prob_classify(featureset).prob(1))
    for label, featureset
    in labeled_data
)
roc_data.plot()
</code></pre>

<p><strong>Edits:</strong></p>

<ul>
<li>ROC is for binary classifiers only. If you have three classes, you can measure the performance of your positive and negative class separately (by counting the other two classes as 0, like you proposed).</li>
<li>The library expects the output of a decision function as the second value of each tuple. It then tries all possible thresholds, e.g. f(x) >= 0.8 => classify as 1, and plots a point for each threshold (that's why you get a curve in the end). So if your classifier guesses class 0, you actually want a value closer to zero. That's why I proposed <code>.prob(1)</code></li>
</ul>
",3008,1321685544
"Is vim able to detect the natural language of a file, then load the correct dictionary?","<p>I am using several languages, and currently I am obliged to indicate to vim with which of these the spell check must be done. Is there a way to set up vim so that it automatically detects the correct one? I vaguely remember that in a previous version of vim, when the spell check was not integrated, the vimspell script made this possible.</p>

<p>It would be even better if this could apply not only to a file but also to a portion of a file, since I frequently mix several languages in a single file. Of course, I would like to avoid to load several dictionaries simultaneously.</p>
","vim, nlp, spell-checking",,879,1269763969
Regular expression for PennTreeBank tags in Java,"<p>Please consider some examples of PennTreeBank Tags:</p>

<pre><code>ADJP -ADV ADVP -BNF CC CD -CLF -CLR -HLN PRP$ PR-P$ NP
</code></pre>

<p>Please consider an instance of my program execution. </p>

<pre><code>Enter your regex: ^-{0,1}[A-Z]{1,6}-{0,1}[A-Z]{0,1}\${0,1}
Enter input string to search: -HLN
I found the text ""-HLN"" starting at index 0 and ending at index 4.
</code></pre>

<p>It works fine.</p>

<p>My task actually is to successfully identify any tag (please refer to tag examples above) except the ""NP"" tag. I wrote the regex as below.</p>

<pre><code>Enter your regex: (^-{0,1}[A-Z]{1,6}-{0,1}[A-Z]{0,1}\${0,1})&amp;&amp;^(NP)
Enter input string to search: -HLN
No match found.
</code></pre>

<p>It is not the desired outcome.</p>

<p>Could someone help me modify the regex to suit the task?</p>

<p>Thank you.</p>
","regex, nlp, regex-negation",,115,1321399570
Python Data Structure for Treebank?,"<p>I'm looking for a Python data structure that handles the Penn Treebank structure. This is a sample of what the Treebank looks like:</p>

<pre><code>( (S
    (NP-SBJ (PRP He) )
    (VP (VBD shouted) )
    (. .) ))
</code></pre>

<p>Essentially, I would like a data structure that I can ask things like ""What are the children of the subject NP?"" or ""What types of phrases dominate the pronoun?"", preferably in Python. Does anyone have a clue? </p>
","python, nltk, corpus, nlp",,2045,1321289867
How to extract keywords from a block of text in Haskell,"<p>So I know this is a kind of a large topic, but I need to accept a chunk of text, and extract the most interesting keywords from it. The text comes from TV captions, so the subject can range from news to sports to pop culture references. It is possible to provide the type of show the text came from. </p>

<p>I have an idea to match the text against a dictionary of terms I know to be interesting somehow. </p>

<p>Which libraries for Haskell can help me with this? </p>

<p>Assuming I do have a dictionary of interesting terms, and a database to store them in, is there a particular approach you'd recommend to matching keywords within the text? </p>

<p>Is there an obvious approach I'm not thinking of?</p>
","haskell, nlp","<p>I'd stem the words in the chunks and then search for all terms in the dict 
just two random libs:</p>

<p>stem <a href=""http://hackage.haskell.org/packages/archive/stemmer/0.2/doc/html/NLP-Stemmer-C.html"" rel=""nofollow"">http://hackage.haskell.org/packages/archive/stemmer/0.2/doc/html/NLP-Stemmer-C.html</a></p>

<p>search <a href=""http://hackage.haskell.org/packages/archive/sphinx/0.2.1/doc/html/Text-Search-Sphinx.html"" rel=""nofollow"">http://hackage.haskell.org/packages/archive/sphinx/0.2.1/doc/html/Text-Search-Sphinx.html</a></p>
",527,1321135363
How to use Freebase to label a very large unlabeled NLP dataset?,"<p>Vocabulary that I am using:</p>

<p>nounphrase -- A short phrase that refers to a specific person, place, or idea. Examples of different nounphrases include ""Barack Obama"", ""Obama"", ""Water Bottle"", ""Yellowstone National Park"", ""Google Chrome web browser"", etc.</p>

<p>category -- The semantic concept defining which nounphrases belong to it and which ones do not. Examples of categories include, ""Politician"", ""Household items"", ""Food"", ""People"", ""Sports teams"", etc. So, we would have that ""Barack Obama"" belongs to ""Politician"" and ""People"" but does not belong to ""Food"" or ""Sports teams"".</p>

<p>I have a very lage unlabeled NLP dataset consisting of millions of nounphrases. I would like to use Freebase to label these nounphrases. I have a mapping of Freebase types to my own categories. What I need to do is download every single examples for every single Freebase type that I have. </p>

<p>The problem that I face is that need to figure out how to structure this type of query. At a high level, the query should ask Freebase ""what are all of the examples of topic XX?"" and Freebase should respond with ""here's a list of all examples of topic XX."" I would be very grateful if someone could give me the syntax of this query. If it can be done in Python, that would be awesome :)</p>
","python, nlp, freebase","<p>The basic form of the query (for a person, for example) is </p>

<pre><code>[{
  ""type"":""/people/person"",
  ""name"":None,
  ""/common/topic/alias"":[],
  ""limit"":100
}]​
</code></pre>

<p>There's documentation available at <a href=""http://wiki.freebase.com/wiki/MQL_Manual"" rel=""nofollow"">http://wiki.freebase.com/wiki/MQL_Manual</a></p>

<p>Using freebase.mqlreaditer() from the Python library <a href=""http://code.google.com/p/freebase-python/"" rel=""nofollow"">http://code.google.com/p/freebase-python/</a> is the easiest way to cycle through all of these.  In this case, the ""limit"" clause determines the chunk size used for querying, but you'll get each result individually at the API level.</p>

<p>BTW, how do you plan to disambiguate Jack Kennedy the president, from the hurler, from the football player, from the book, etc, etc <a href=""http://www.freebase.com/search?limit=30&amp;start=0&amp;query=jack+kennedy"" rel=""nofollow"">http://www.freebase.com/search?limit=30&amp;start=0&amp;query=jack+kennedy</a>  You may want to consider capturing additional information from Freebase (birth &amp; death dates, book authors, other types assigned, etc) if you'll have enough context to be able to use it to disambiguate.</p>

<p>Past a certain point, it may be easier and/or more efficient to work from the bulk data dumps rather than the API <a href=""http://wiki.freebase.com/wiki/Data_dumps"" rel=""nofollow"">http://wiki.freebase.com/wiki/Data_dumps</a></p>

<p>Edit - here's a working Python program which assumes you've got a list of type IDs in a file called 'types.txt':</p>

<pre><code>import freebase

f = file('types.txt')
for t in f:
    t=t.strip()
    q = [{'type':t,
          'mid':None,
          'name':None,
          '/common/topic/alias':[],
          'limit':500,
          }]
    for r in freebase.mqlreaditer(q):
        print '\t'.join([t,r['mid'],r['name']]+r['/common/topic/alias'])
f.close()
</code></pre>

<p>If you make the query much more complex, you'll probably want to lower the limit to keep from running into timeouts, but for a simple query like this, boosting the limit above the default of 100 will make it more efficient by querying in bigger chunks.</p>
",1773,1321045977
Collocations in text classification,"<p>Suppose i have trained my classifier and i want to find the right sense of a word in a sentence. One feature people use is called collocation where you consider words to the left/right of the confusing word and position is important . I am curious why this approach works? What information does considering collocations give us that helps us in text classification? Moreover, why is the position important</p>
","algorithm, nlp, word-sense-disambiguation",,1111,1321076062
where to download multi language word list from Wiktionary?,"<p>I was wondering if there was a place to download multi-language word lists from Wiktionary? </p>
","nlp, n-gram, wiktionary",,2492,1303667346
PHP library for word clustering/NLP?,"<p>What I am trying to implement is a rather trivial ""take search results (as in title &amp; short description), cluster them into meaningful named groups"" program in PHP.</p>

<p>After hours of googling and countless searches on SO (yielding interesting results as always, albeit nothing really useful) I'm still unable to find any PHP library that would help me handle clustering.</p>

<ul>
<li>Is there such a PHP library out there that I might have missed?</li>
<li>If not, is there any FOSS that handles clustering and has a decent API?</li>
</ul>
","php, nlp, cluster-analysis, information-retrieval","<p>Like this:</p>

<p>Use a list of stopwords, get all words or phrases not in the stopwords, count occurances of each, sort in descending order.</p>

<p>The stopwords needs to be a list of all common English terms. It should also include punctuation, and you will need to preg_replace all the punctuation to be a separate word first, e.g. ""Something, like this."" -> ""Something , like this ."" OR, you can just remove all punctuation.</p>

<pre><code>$content=preg_replace('/[^a-z\s]/', '', $content); // remove punctuation

$stopwords='the|and|is|your|me|for|where|etc...';
$stopwords=explode('|',$stopwords);
$stopwords=array_flip($stopwords);

$result=array(); $temp=array();
foreach ($content as $s)
if (isset($stopwords[$s]) OR strlen($s)&lt;3)
 {
 if (sizeof($temp)&gt;0)
  {
  $result[]=implode(' ',$temp);
  $temp=array();
  }            
 } else $temp[]=$s;
if (sizeof($temp)&gt;0) $result[]=implode(' ',$temp);

$phrases=array_count_values($result);
arsort($phrases);
</code></pre>

<p>Now you have an associative array in order of the frequency of terms that occur in your input data.</p>

<p>How you want to do the matches depends upon you, and it depends largely on the length of the strings in the input data.</p>

<p>I would see if any of the top 3 array keys match any of the top 3 from any other in the data. These are then your groups.</p>

<p>Let me know if you have any trouble with this.</p>
",3085,1320231903
Decoding Permutated English Strings,"<p>A coworker was recently asked this when trying to land a (different) research job:</p>

<p>Given 10 128-character strings which have been permutated in exactly the same way, decode the strings. The original strings are English text with spaces, numbers, punctuation and other non-alpha characters removed.</p>

<p>He was given a few days to think about it before an answer was expected. How would you do this? You can use any computer resource, including character/word level language models.</p>
","algorithm, puzzle, nlp","<p>This is a basic <a href=""http://en.wikipedia.org/wiki/Transposition_cipher"" rel=""nofollow noreferrer"">transposition cipher</a>.  My question above was simply to determine if it was a transposition cipher or a substitution cipher.  Cryptanalysis of such systems is fairly straightforward.  Others have already alluded to basic methods.  Optimal approaches will attempt to place the hardest and rarest letters first, as these will tend to uniquely identify the letters around them, which greatly reduces the subsequent search space.  Simply finding <em>a</em> place to place an ""a"" (no pun intended) is not hard, but finding a location for a ""q"", ""z"", or ""x"" is a bit more work.</p>

<p>The overarching goal for an algorithm's quality isn't to decipher the text, as it can be done by better than brute force methods, nor is it simply to be fast, but it should eliminate possibilities <em>absolutely as fast as possible</em>.</p>

<p>Since you can use multiple strings simultaneously, attempting to create words from the rarest characters is going to allow you to test dictionary attacks in parallel.  Finding the correct placement of the rarest terms in each string as quickly as possible will decipher that ciphertext PLUS all of the others at the same time.</p>

<p>If you search for cryptanalysis of transposition ciphers, you'll find a bunch with genetic algorithms. These are meant to advance the research cred of people working in GA, as these are not really optimal in practice.  Instead, you should look at some basic optimizatin methods, such as branch and bound, A*, and a variety of statistical methods. (How deep you should go depends on your level of expertise in algorithms and statistics. :)  I would switch between deterministic methods and statistical optimization methods several times.) </p>

<p>In any case, the calculations should be <em>dirt cheap</em> and fast, because the scale of initial guesses could be quite large.  It's best to have a cheap way to filter out a LOT of possible placements first, then spend more CPU time on sifting through the better candidates.  To that end, it's good to have a way of describing the stages of processing and the computational effort for each stage.  (At least that's what I would expect if I gave this as an interview question.)</p>

<p>You can even <a href=""https://rads.stackoverflow.com/amzn/click/com/0894122541"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">buy a fairly credible reference book</a> on deciphering double transposition ciphers.</p>

<hr>

<p>Update 1: Take a look at <a href=""http://www.autonlab.org/tutorials/hillclimb02.pdf"" rel=""nofollow noreferrer"">these slides</a> for more ideas on iterative improvements.  It's not a great reference set of slides, but it's readily accessible.  What's more, although the slides are about GA and simulated annealing (methods that come up a lot in search results for transposition cipher cryptanalysis), the author advocates against such methods when you can use A* or other methods.  :)</p>
",458,1315174018
"NLP, algorithms for determining if block of text is &quot;similar&quot; to other (after already having matched for keyword)","<p>I've been reading up on NLP as much as I can and searching on here but haven't found anything that seems to address exactly what I am trying to do. I am pretty new to NLP, only having had some minor exposure before, so far I have gotten the NLP processor I'm using working to where I am able to extract the POS from the text.</p>

<p>I am just working with a small sample document and then with one ""input phrase"" that I am basically trying to find a match for. The code I've written so far basically does this:</p>

<ul>
<li>takes the input phrase and the ""searchee (document being searched on)"" and breaks them down into Lists of individual words, then also gets the POS for each word. User also puts in one kewyord that is in the input phrase (and should be in doc being searched)</li>
<li>both Lists are searched for the keyword that the user input, then, for the first place this keyword is found in each document, a set number of words before and after are taken (such as 5). These are put into a dataset for processing, so if one article had:</li>
</ul>

<p>keyword: football</p>

<p>""A lot of sports are fun, football is a great, yet very physical sport.""
- Then my process would truncate this down to ""are fun, football is a""</p>

<p>My goal is to compare the pieces, such as the ""are fun, football is a"" for similarity as far as if they are likely to be used in a similar context, etc.</p>

<p>I'm wondering if anyone can point me in the right direction as far as patterns that could be used for this, algorithms, etc. The example above is simplistic, just to give an idea, but I would be planning to make this more complex if I can find the right place to learn more about this. Thanks for any info</p>
","algorithm, artificial-intelligence, nlp, information-retrieval","<p>It seems you're solving the good old <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Key_Word_in_Context"" rel=""nofollow"">KWIC</a> problem. That can be done with indexing, or just a simple <code>for</code> loop through the words in a text:</p>

<pre><code>for i = 0 to length(text):
    if text[i] == word:
        emit(text[i-2], text[i-1], text[i], text[i+1], text[i+2])
</code></pre>

<p>Where <code>emit</code> might mean print them, store them in a hashtable, whatever.</p>
",3014,1321091659
Parse::RecDescent Module in Perl,"<p>I found that <a href=""http://search.cpan.org/perldoc/Parse%3a%3aRecDescent"" rel=""nofollow"">Parse::RecDescent</a> is a Perl module to implement natural language parser. Is there any way to implement Parse::RecDescent in C# language also?</p>
","c#, perl, nlp, recursive-descent",,372,1320953025
Question about Latent Dirichlet Allocation (MALLET),"<p>Honestly, I'm not familiar with LDA, but am required to use MALLET's topic modeling for one of my projects.</p>

<p>My question is: given a set of documents within a specific timestamp as the training data for the topic model, how appropriate is it to use the model (using the inferencer) to track the topic trends, for documents + or -  the training data's timestamp. I mean, is the topic distributions being provided by MALLET a suitable metric to track the popularity of the topics over time if during the model building stage, we only provide a subset of the dataset I am required to analyze.</p>

<p>thanks.   </p>
","nlp, mallet","<p>Are you famailiar with <a href=""http://en.wikipedia.org/wiki/Latent_semantic_indexing"" rel=""nofollow"">Latent Semantic Indexing</a>? Latent Dirichlet Analysis is just a different way of doing the same kind of thing, so LSI or <a href=""http://www.cs.brown.edu/~th/papers/Hofmann-SIGIR99.pdf"" rel=""nofollow"">pLSI</a> you may be an easier starting point to gain knowledge about the goals of LDA.</p>

<p>All three techniques lock on to topics in an unsupervised fashion (you tell it how many topics to look for), and then assume that each document covers each topic in varying proportions. Depending on how many topics you allocate, they may behave more like <em>subfields</em> of whatever your corpus is about, and may not be as specific as the ""topics"" that people think about when they think about trending topics in the news.</p>

<p>Somehow I suspect that you want to assume that each document represents a particular topic. LSI/pLSI/LDA don't do this -- they model each document as a mixture of topics. That doesn't mean you won't get good results, or that this isn't worth trying, but I suspect (though I don't have a comprehensive knowledge of LSI literature) that you'd be tackling a brand new research problem.</p>

<p>(FWIW, I suspect that using clustering methods like <a href=""http://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow"">k-Means</a> more readily model the assumption that each document has exactly one topic.)</p>
",1905,1289386314
Incrementally Trainable Entity Recognition Classifier,"<p>I'm doing some semantic-web/nlp research, and I have a set of sparse records, containing a mix of numeric and non-numeric data, representing entities labeled with various features extracted from simple English sentences.</p>

<p>e.g.</p>

<pre><code>uid|features
87w39423|speaker=432, session=43242, sentence=34, obj_called=bob,favorite_color_is=blue
4535k3l535|speaker=512, session=2384, sentence=7, obj_called=tree,isa=plant,located_on=wilson_street
23432424|speaker=997, session=8945305, sentence=32, obj_called=salty,isa=cat,eats=mice
09834502|speaker=876, session=43242, sentence=56, obj_called=the monkey,ate=the banana
928374923|speaker=876, session=43242, sentence=57, obj_called=it,was=delicious
294234234|speaker=876, session=43243, sentence=58, obj_called=the monkey,ate=the banana
sd09f8098|speaker=876, session=43243, sentence=59, obj_called=it,was=hungry
...
</code></pre>

<p>A single entity may appear more than once (but with a different UID each time), and may have overlapping features with its other occurrences. A second data set represents which of the above UIDs are definitely the same.</p>

<p>e.g.</p>

<pre><code>uid|sameas
87w39423|234k2j,234l24jlsd,dsdf9887s
4535k3l535|09d8fgdg0d9,l2jk34kl,sd9f08sf
23432424|io43po5,2l3jk42,sdf90s8df
09834502|294234234,sd09f8098
...
</code></pre>

<p>What algorithm(s) would I use to <strong>incrementally</strong> train a classifier that could take a set of features, and instantly recommend the N most similar UIDs and probability of whether or not those UIDs actually represent the <strong>same</strong> entity? Optionally, I'd also like to get a recommendation of missing features to populate and then re-classify to get a more certain matches.</p>

<p>I researched traditional approximate nearest neighbor algorithms. such as <a href=""http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN"" rel=""nofollow"">FLANN</a> and <a href=""http://pypi.python.org/pypi/scikits.ann"" rel=""nofollow"">ANN</a>, and I don't think these would be appropriate since they're not trainable (in a supervised learning sense) nor are they typically designed for sparse non-numeric input.</p>

<p>As a very naive first-attempt, I was thinking about using a naive bayesian classifier, by converting each SameAs relation into a set of training samples. So, for each entity A with B sameas relations, I would iterate over each and train the classifier like:</p>

<pre><code>classifier = Classifier()
for entity,sameas_entities in sameas_dataset:
    entity_features = get_features(entity)
    for other_entity in sameas_entities:
        other_entity_features = get_features(other_entity)
        classifier.train(cls=entity, ['left_'+f for f in entity_features] + ['right_'+f for f in other_entity_features])
        classifier.train(cls=other_entity, ['left_'+f for f in other_entity_features] + ['right_'+f for f in entity_features])
</code></pre>

<p>And then use it like:</p>

<pre><code>&gt;&gt;&gt; print classifier.findSameAs(dict(speaker=997, session=8945305, sentence=32, obj_called='salty',isa='cat',eats='mice'), n=7)
[(1.0, '23432424'),(0.999, 'io43po5', (1.0, '2l3jk42'), (1.0, 'sdf90s8df'), (0.76, 'jerwljk'), (0.34, 'rlekwj32424'), (0.08, '09843jlk')]
&gt;&gt;&gt; print classifier.findSameAs(dict(isa='cat',eats='mice'), n=7)
[(0.09, '23432424'), (0.06, 'jerwljk'), (0.03, 'rlekwj32424'), (0.001, '09843jlk')]
&gt;&gt;&gt; print classifier.findMissingFeatures(dict(isa='cat',eats='mice'), n=4)
['obj_called','has_fur','has_claws','lives_at_zoo']
</code></pre>

<p>How viable is this approach? The initial batch training would be horribly slow, at least O(N^2), but incremental training support would allow updates to happen more quickly.</p>

<p>What are better approaches?</p>
","nlp, machine-learning, classification, semantic-web",,409,1320421852
How to make text file (or other documents&#39;) parser?,"<p>I have following task to do: to fill spell check dictionary (simple txt file) I need parser
which should: - parse within text file (or another type of document), extract
each word and then create text file with simple list of words like this:
adfadf
adfasdfa
adfasfdasdf
adsfadf
...
etc
What scripting language and library you would suggest? If possible, please, give example of code (especially for extracting each word). Thanks!</p>
","parsing, text, nlp, tokenize","<p>What you want is not a parser, but just a tokenizer. This can be done in any language with a bunch of regular expressions, but I do recommend Python with <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a>:</p>

<pre><code>&gt;&gt;&gt; from nltk.tokenize import word_tokenize
&gt;&gt;&gt; word_tokenize('Hello, world!')
['Hello', ',', 'world', '!']
</code></pre>

<p>Generally, just about any <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow"">NLP</a> toolkit will include a tokenizer, so there's no need to reinvent the wheel; tokenizing isn't hard, but it involves writing a lot of heuristics to handle all the exceptions such as abbreviations, acronyms, etc.</p>
",231,1320922605
Specify repeating tokens in Recursive Descent Parser with NLTK,"<p>I am trying to parse a grammar similar to this.</p>

<pre><code>PER -&gt; 'noun' | 'noun1' | 'noun2'

PERS -&gt; PER+
</code></pre>

<p>This is not a valid grammar for NLTK's recursive descent parser.
How do specifiy one of more repeating tokens in this grammar</p>
","python, nlp, nltk, recursive-descent",,337,1320736728
Using Lingpipe for word-level language model,"<p>I have been trying to get a word-level language model to work on lingpipe. All the examples and tutorials I have come across show the character-n-gram model. How to I go about using lingpipe to train a word-level model and then use that model to test it on other documents?</p>

<p>Additionally, I noticed that TokenizedLM is not serializable. Is there no way I can save it and load it later without having to go through re-training every time?</p>

<p>Lastly, are there any other frameworks/tools that will allow me to do this without any coding on my part?</p>
","java, nlp, classification, document-classification",,698,1320626952
What are the basic algorithms for text mining?,"<p>I'm trying to do an application for mining some texts from the web, but I'm not sure of what is the best way to perform text mining.</p>

<p>What I want with this question is know about what are the most used techniques/algorithms to perform text mining and do some information retrieval in documents (not for indexing). </p>
","nlp, information-retrieval, text-mining","<p><em>Text mining</em> is a rather broad term, it roughly means machine learning applied to text. Common techniques include <em>k</em>-means clustering, Naive Bayes and linear SVM classification, tf-idf vectorization, <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Singular_value_decomposition"" rel=""noreferrer"">SVD</a> (called <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Latent_semantic_analysis"" rel=""noreferrer"">LSA</a> when applied to text), latent Dirichlet allocation. So, performing ""some text mining"" might mean just about anything, just like doing ""some information retrieval"".</p>

<p>See Bing Liu's book <em>Web Data Mining</em> for a good intro to the field.</p>
",9531,1320457425
How do I determine if a random string sounds like English?,"<p>I have an algorithm that generates strings based on a list of input words. How do I separate only the strings that sounds like English words? ie. discard <strong>RDLO</strong> while keeping <strong>LORD</strong>.</p>

<p><strong>EDIT:</strong> To clarify, they do not need to be actual words in the dictionary. They just need to sound like English. For example <strong>KEAL</strong> would be accepted.</p>
","string, linguistics, nlp","<p>You can build a markov-chain of a huge english text.</p>

<p>Afterwards you can feed words into the markov chain and check how high the probability is that the word is english.</p>

<p>See here: <a href=""http://en.wikipedia.org/wiki/Markov_chain"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Markov_chain</a> </p>

<p>At the bottom of the page you can see the markov text generator. What you want is exactly the reverse of it. </p>

<p>In a nutshell: The markov-chain stores for each character the probabilities of which next character will follow. You can extend this idea to two or three characters if you have enough memory.</p>
",5856,1221740420
Preserving only domain-specific keywords?,"<p>I am trying to determine the most popular keywords for certain class of documents in my collection. Assuming that the domain is ""computer science"" (which of course, includes networking, computer architecture, etc.) what is the best way to preserve these domain-specific keywords from text? I tried using Wordnet but I am not quite how to best use it to extract this information.</p>

<p>Are there any well-known list of words that I can use as a whitelist considering the fact that I am not aware of all domain-specific keywords beforehand? Or are there any good nlp/machine learning techniques to identity domain specific keywords?</p>
","python, nlp, machine-learning, nltk","<p>You need a huge training set of documents. Small subset of this collection (but still large set of documents) should represent given domain. Using nltk calculate words statistics taking into account morphology, filter out stopwords. The good statistics is TF*IDF which is roughly a number of occurenses of a word in the domain subset divided by number of documents containing the word in a whole collection. Keywords are words with greatest TF*IDF.</p>
",1198,1320265633
Most efficient way to index words in a document?,"<p>This came up in another question but I figured it is best to ask this as a separate question. Give a large list of sentences (order of 100 thousands):</p>

<pre><code>[
""This is sentence 1 as an example"",
""This is sentence 1 as another example"",
""This is sentence 2"",
""This is sentence 3 as another example "",
""This is sentence 4""
]
</code></pre>

<p>what is the best way to code the following function?</p>

<pre><code>def GetSentences(word1, word2, position):
    return """"
</code></pre>

<p>where given two words, <code>word1</code>, <code>word2</code> and a position <code>position</code>, the function should return the list of all sentences satisfying that constraint. For example:</p>

<pre><code>GetSentences(""sentence"", ""another"", 3)
</code></pre>

<p>should return sentences <code>1</code> and <code>3</code> as the index of the sentences. My current approach was using a dictionary like this:</p>

<pre><code>Index = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))

for sentenceIndex, sentence in enumerate(sentences):
    words = sentence.split()
    for index, word in enumerate(words):
        for i, word2 in enumerate(words[index:):
            Index[word][word2][i+1].append(sentenceIndex)
</code></pre>

<p>But this quickly blows everything out of proportion on a dataset that is about 130 MB in size as my 48GB RAM is exhausted in less than 5 minutes. I somehow get a feeling this is a common problem but can't find any references on how to solve this efficiently. Any suggestions on how to approach this?</p>
","python, text, nlp","<p>Use database for storing values.</p>

<ol>
<li>First <strong>add all the sentences to one table</strong> (they should have IDs). You may call it eg. <code>sentences</code>.</li>
<li>Second, <strong>create table with words</strong> contained within all the sentences (call it eg. <code>words</code>, give each word an ID), saving connection between sentences' table records and words' table records within separate table (call it eg. <code>sentences_words</code>, it should have two columns, preferably <code>word_id</code> and <code>sentence_id</code>).</li>
<li><p>When searching for sentences containing all the mentioned words, your job will be simplified:</p>

<ol>
<li><p>You should first <strong>find records from <code>words</code> table</strong>, where words are exactly the ones you search for. The query could look like this:</p>

<pre><code>SELECT `id` FROM `words` WHERE `word` IN ('word1', 'word2', 'word3');
</code></pre></li>
<li><p>Second, you should <strong>find <code>sentence_id</code> values from table <code>sentences</code></strong> that have required <code>word_id</code> values (corresponding to the words from <code>words</code> table). The initial query could look like this:</p>

<pre><code>SELECT `sentence_id`, `word_id` FROM `sentences_words`
WHERE `word_id` IN ([here goes list of words' ids]);
</code></pre>

<p>which could be simplified to this:</p>

<pre><code>SELECT `sentence_id`, `word_id` FROM `sentences_words`
WHERE `word_id` IN (
    SELECT `id` FROM `words` WHERE `word` IN ('word1', 'word2', 'word3')
);
</code></pre></li>
<li><p><strong>Filter the result within Python</strong> to return only <code>sentence_id</code> values that have all the required <code>word_id</code> IDs you need.</p></li>
</ol></li>
</ol>

<p>This is basically a solution based on storing big amount of data in the form that is best suited for this - the database.</p>

<p><strong>EDIT:</strong></p>

<ol>
<li>If you will only search for two words, you can do even more (almost everything) on DBMS' side.</li>
<li>Considering you need also position difference, you should store the position of the word within third column of <code>sentences_words</code> table (lets call it just <code>position</code>) and when searching for appropriate words, you should calculate difference of this value associated with both words.</li>
</ol>
",3788,1320455395
Prioritizing text based on content,"<p>If you have a list of texts and a person interested in certain topics what are the algorithms dealing with choosing the most relevant text for a given person?</p>

<p>I believe that this is quite a complex topic and as an answer I expect a few directions to study various methodologies of text analysis, text statistics, artificial intelligence etc.</p>

<p>thank you</p>
","algorithm, statistics, artificial-intelligence, text-processing, text-analysis","<p>There are quite a few algorithms out there for this task. At least way too many to mention them all here. First some starting points:</p>

<ul>
<li><p>Topic discovery and recommendation are two quite distinctive tasks, although they often overlap. If you have a stable userbase, you might be able to give very good recommendations without any topic discovery.</p></li>
<li><p>Discovering topics and assigning names to them are also two different tasks. This means it is often easier to be able to tell that text A and text B share a similar topic, than to explicetly be able to state what this common topic might be. Giving names to the topics is best done by humans, for example by having them tag the items.</p></li>
</ul>

<p>Now to some actual examples.</p>

<ul>
<li><p>TF-IDF is often a good starting point, however it also has severe drawbacks. For example it will not be able to tell that ""car"" and ""truck"" in two texts mean that these two probably share a topic.</p></li>
<li><p><a href=""http://websom.hut.fi/websom/"" rel=""nofollow"">http://websom.hut.fi/websom/</a> A Kohonen map for automatically clustering data. It learns the topics and then organizes the texts by topics.</p></li>
<li><p><a href=""http://de.wikipedia.org/wiki/Latent_Semantic_Analysis"" rel=""nofollow"">http://de.wikipedia.org/wiki/Latent_Semantic_Analysis</a> Will be able to boost TF-IDF by detecting semantic similarity among different words. Also note, that this has been patented, so you might not be able to use it.</p></li>
<li><p>Once you have a set of topics assigned by users or experts, you can also try almost any kind of machine learning method (for example SVM) to map the TF-IDF data to topics.</p></li>
</ul>
",189,1320407770
Where can I find the expansions for the short codes used by POS taggers?,"<p>All of the Parts of Speech (POS) Taggers give the tags in the form of shortcodes like (NNS,VBZ etc). </p>

<blockquote>
  <p>This/DT page/NN is/VBZ about/IN the/DT Brill-tagger/NNP</p>
</blockquote>

<p>Like the above example which is from one of the <a href=""http://cst.dk/online/pos_tagger/uk/"" rel=""nofollow"">online POS Taggers</a>.</p>

<p>Where can I find the meaning of each shortcodes?</p>
","nlp, nltk, opennlp","<p>English taggers often use the Penn Treebank Tagset.  Here you can find a <a href=""http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench/CQP-HTMLDemo/PennTreebankTS.html"" rel=""nofollow"">quick overview</a> and the <a href=""ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz"" rel=""nofollow"">official detailed annotation guidelines</a>.</p>
",161,1320405385
Is (a^p )(b^q) a regular language,"<p>I read somewhere that {(a^p)(b^q):p,Q belong to N} is a regular language. However, i dont think this is correct. This can be proved using pumping lemma. Just want to verify if my solution is correct</p>

<p>Let y be ab . Thus, x(y^n)z does not belong to L as there will be some b's before a's for n>=1. However, expression does not allow this. Thus, (a^p)(b^q) is not a RL</p>
","nlp, regular-language",,375,1320155213
Library to classify text on affect,"<p>Are there libraries out there for the automatic classification of text in for example English on affect? If it's quite rough it might still be good enough.</p>
","nlp, classification",,318,1319557664
Most efficient way to generate a list of Unigrams from a text field in MongoDB,"<p>I need to generate a vector of unigrams, i.e. a vector of all the unique words which appear in a specific text field that I have stored as part of a broader JSON object in MongoDB.</p>

<p>I'm not really sure what's the easiest and most efficient way to generate this vector. I was thinking of writing a simple Java app which could handle the tokenization (using something like OpenNLP), however I think that a better approach may be to try to tackle this using Mongo's Map-Reduce feature... However I'm not really sure how I could go about this. </p>

<p>Another option would be to use Apache Lucene indexing, but it would mean I'd still need to export this data in one by one. Which is really the same issue I would have with the custom Java or Ruby approach... </p>

<p>Map reduce sounds good however the Mongo data is growing by the day as more document are inserted. This isn't really a one off task as there are new documents being added all the time. Updates are very rare. I really don't want to run a Map-Reduce over the millions of documents every time I want to update my Unigram vector as I fear this will be very inefficient use of resources... </p>

<p>What would be the most efficient way to generate the unigram vector and then keep it updated?</p>

<p>Thanks!</p>
","mongodb, lucene, nlp, mapreduce, opennlp","<p>Since you have not provided a sample document (object) format take this as a sample collection called <strong>'stories'</strong>.</p>

<pre><code>{ ""_id"" : ObjectId(""4eafd693627b738f69f8f1e3""), ""body"" : ""There was a king"", ""author"" : ""tom"" }
{ ""_id"" : ObjectId(""4eafd69c627b738f69f8f1e4""), ""body"" : ""There was a queen"", ""author"" : ""tom"" }
{ ""_id"" : ObjectId(""4eafd72c627b738f69f8f1e5""), ""body"" : ""There was a queen"", ""author"" : ""tom"" }
{ ""_id"" : ObjectId(""4eafd74e627b738f69f8f1e6""), ""body"" : ""There was a jack"", ""author"" : ""tom"" }
{ ""_id"" : ObjectId(""4eafd785627b738f69f8f1e7""), ""body"" : ""There was a humpty and dumpty . Humtpy was tall . Dumpty was short ."", ""author"" : ""jane"" }
{ ""_id"" : ObjectId(""4eafd7cc627b738f69f8f1e8""), ""body"" : ""There was a cat called Mini . Mini was clever cat . "", ""author"" : ""jane"" }
</code></pre>

<p>For the given dataset, you can use the following javascript code to get to your solution. The collection ""<strong>authors_unigrams</strong>"" contains the result. All the code is supposed to be run using mongo console (http://www.mongodb.org/display/DOCS/mongo+-+The+Interactive+Shell).</p>

<p><strong>First</strong>, we need to mark of all the new documents that have come afresh into the <strong>'stories'</strong> collection. We do it using following command. It will add a new attribute called ""mr_status"" into each document and assign value ""inprocess"". Later, we will see that map-reduce operation will only take those documents in account which are having the value ""inprocess"" for the field ""mr_status"". This way, we can avoid reconsidering all the documents for map-reduce operation that have been already considered in any of the previous attempt, making the operation efficient as  asked.</p>

<pre><code>db.stories.update({mr_status:{$exists:false}},{$set:{mr_status:""inprocess""}},false,true);
</code></pre>

<p><strong>Second</strong>, we define both <strong>map()</strong> and <strong>reduce()</strong> function. </p>

<pre><code>var map = function() {
        uniqueWords = function (words){
            var arrWords = words.split("" "");
            var arrNewWords = [];
            var seenWords = {};
            for(var i=0;i&lt;arrWords.length;i++) {
                if (!seenWords[arrWords[i]]) {
                    seenWords[arrWords[i]]=true;
                    arrNewWords.push(arrWords[i]);
                }
            }
            return arrNewWords;
        }
      var unigrams =  uniqueWords(this.body) ;
      emit(this.author, {unigrams:unigrams});
};

var reduce = function(key,values){

    Array.prototype.uniqueMerge = function( a ) {
        for ( var nonDuplicates = [], i = 0, l = a.length; i&lt;l; ++i ) {
            if ( this.indexOf( a[i] ) === -1 ) {
                nonDuplicates.push( a[i] );
            }
        }
        return this.concat( nonDuplicates )
    };

    unigrams = [];
    values.forEach(function(i){
        unigrams = unigrams.uniqueMerge(i.unigrams);
    });
    return { unigrams:unigrams};
};
</code></pre>

<p><strong>Third</strong>, we actually run the map-reduce function.</p>

<pre><code>var result  = db.stories.mapReduce( map,
                                  reduce,
                                  {query:{author:{$exists:true},mr_status:""inprocess""},
                                   out: {reduce:""authors_unigrams""}
                                  });
</code></pre>

<p><strong>Fourth</strong>, we mark all the records that have been considered for map-reduce in last run as processed by setting ""mr_status"" as ""processed"".</p>

<pre><code>db.stories.update({mr_status:""inprocess""},{$set:{mr_status:""processed""}},false,true);
</code></pre>

<p><strong>Optionally</strong>, you can see the result collection <strong>""authors_unigrams""</strong> by firing following command.</p>

<pre><code>db.authors_unigrams.find();
</code></pre>
",434,1310139665
How do you find the subject of a sentence?,"<p>I am new to NLP and was doing research about what language toolkit I should be using to do the following. I would like to do one of the two things which accomplishes the same thing:</p>

<ol>
<li><p>I basically would like to classify a text, usually one sentence that contains 15 words. Would like to classify if the sentence is talking about a specific subject.  </p></li>
<li><p>Is there a tool that given a sentence, it finds out the subject of a sentence.  </p></li>
</ol>

<p>I am using PHP and Java but the tool can be anything that runs on Linux command line</p>

<p>Thank you very much.</p>
","java, php, nlp",,5127,1308787745
java vs C++ for natural language processing,"<p>Could you please let me know which one is better to learn amongst Java and C++ for natural language processing with respect to library,support etc.</p>

<p>Best, Thetna</p>
","java, c++, nlp",,1312,1320019525
Using Natural Language Processing to parse websites,"<p>I'm interested generally in the data mining by crawling websites, but I've never been able to find a lot of documentation on the process I'd really like to implement. I'm very keen on the idea of writing a base set of rules that define how to parse a page, then training the tool when it makes mistakes.</p>

<p>Let's say I want to parse menus from restaurant websites. I'd like to create a tool that would allow me to write a set of rules that show generally where the menu items + prices are. Then, I could run the tool and tell it which menu items it parsed out correctly, and which ones were wrong. The tool would then ""learn"" from these corrections, and the next time I run it, I'd get better results.</p>

<p>I've looked a bit at the NLTK toolkit, and it's got me wondering if the best way to solve this problem is with a NLP tool, like NLTK. Can anyone point me in the correct direction for finding books and (ideally) libraries that can help me get started? Is NLP the way to go? Thanks!</p>
","nlp, web-crawler, data-mining, nltk",,983,1319834228
"Match &quot;byte spans&quot; to a text document, Python","<p>I'm working with an annotated corpus that contains two sets of .txt files. The first set contains the documents that were annotated (i.e, articles, blog-posts,etc.) and the second set contains the actual annotations.  The way to match the annotation to the text annotated is via  ""byte spans.""  From the readme file:</p>

<pre><code>""The span is the starting and ending byte of the annotation in 
the document.  For example, the annotation listed above is from 
the document, temp_fbis/20.20.10-3414.  The span of this annotation 
is 730,740.  This means that the start of this annotation is 
byte 730 in the file docs/temp_fbis/20.20.10-3414, and byte 740 
is the character after the last character of the annotation.""
</code></pre>

<p>So, question: How to do I index the start and end byte in the document so that I can match the annotation to the text in the original document? Any ideas?  I'm working in Python on this...</p>
","python, nlp, tagged-corpus",,174,1319833273
What language can be recommended for text mining/parsing?,"<p>I'm doing some text mining in web pages. Currently I'm working with Java, but maybe there is more appropriate languages to do what I want.</p>

<p>Example of some things I want to do:</p>

<p>Determine the char type of a word based on it parts (letter, digit, symbols, etc.) as Alphabetic, Number, Alphanumeric, Symbol, etc.(there is more types).</p>

<p>Discover stop words based on statistics.</p>

<p>Discover some gramatical class (verb, noun, preposition, conjuntion) based on statistics and some logics.</p>

<p>I was thinking about using Prolog and R (I don't know much about these languages), but I don't know if they are good for this or maybe, another language more appropriate.</p>

<p>Which can I use? Good libs for Java are welcome too.</p>
","java, r, prolog, nlp, text-mining",,1350,1319567529
Maximum Entropy for Natural Language Processing,"<p>Can anyone explain simply how how maximum entropy models work when used in Natural Language Processing. I need to statistically parse simple words and phrases to try to figure out the likelihood of specific words and what objects they refer to or what phrases they are contained within.</p>
","statistics, nlp, named-entity-recognition","<p>I recommend the NLTK python package. You can also use MALLET or WEKA.
For a theoretical background, you should ask at <a href=""https://stats.stackexchange.com/"">https://stats.stackexchange.com/</a> or <a href=""http://metaoptimize.com/qa/"" rel=""nofollow noreferrer"">http://metaoptimize.com/qa/</a> . </p>
",290,1319270478
Default Category in Lingpipe&#39;s Text Categorization,"<p>I'm using the Lingpipe's Text Categorization feature in an application. The classifier is working just fine, however, I've noticed the it does not support a ""Default Category"". That is, there is no possibility the text will be categorized as ""neutral"", if it does not fit a specified category. Is there a way I can do this? Perhaps if I determine a threshold for the score.</p>

<p>I appreciate any sugestions. Thank you!   </p>
","nlp, categorization",,173,1319034476
Creating corpora for tgrep2 from bracket-delineated trees,"<p>How do I create a tgrep2-able corpus file from bracket- (or roundbrace-) delineated trees in a text file? I've done a bit of looking now and only see how to convert from tgrep to tgrep2 filetypes. TFM has been no help, nor has much googling. I just want to convert my corpora to t2c files or whatever. </p>
","text, binary, nlp, corpus",,79,1318995497
How to parse a list of words according to a simplified grammar?,"<p>Just to clarify, this isn't homework. I've been asked for help on this and am unable to do it, so it turned into a personal quest to solve it.</p>

<p>Imagine you have a grammar for an English sentence like this:</p>

<pre><code>S =&gt; NP VP | VP
NP =&gt; N | Det N | Det Adj N
VB =&gt; V | V NP
N =&gt; i you bus cake bear
V =&gt; hug love destroy am
Det =&gt; a the
Adj =&gt; pink stylish
</code></pre>

<p>I've searched for several hours and really am out of ideas. 
I found articles talking about shallow parsing, depth-first backtracking and related things, and while I'm familiar with most of them, I still can't apply them to this problem. I tagged Lisp and Haskell because those are the languages I plan to implement this in, but I don't mind if you use other languages in your replies.</p>

<p>I'd appreciate hints, good articles and everything in general.</p>
","algorithm, haskell, lisp, nlp","<p>Here's a working Haskell example. It turns out there's a few tricks to learn before you can make it work! The zeroth thing to do is boilerplate: turn off the dreaded monomorphism restriction, import some libraries, and define some functions that aren't in the libraries (but should be):</p>

<pre><code>{-# LANGUAGE NoMonomorphismRestriction #-}
import Control.Applicative ((&lt;*))
import Control.Monad
import Text.ParserCombinators.Parsec

ensure p x = guard (p x) &gt;&gt; return x
singleToken t = tokenPrim id (\pos _ _ -&gt; incSourceColumn pos 1) (ensure (==t))
anyOf xs = choice (map singleToken xs)
</code></pre>

<p>Now that the zeroth thing is done... first, we define a data type for our abstract syntax trees. We can just follow the shape of the grammar here. However, to make it more convenient, I've factored a few of the grammar's rules; in particular, the two rules</p>

<pre><code>NP =&gt; N | Det N | Det Adj N
VB =&gt; V | V NP
</code></pre>

<p>are more conveniently written this way when it comes to actually writing a parser:</p>

<pre><code>NP =&gt; N | Det (Adj | empty) N
VB =&gt; V (NP | empty)
</code></pre>

<p>Any good book on parsing will have a chapter on why this kind of factoring is a good idea. So, the AST type:</p>

<pre><code>data Sentence
    = Complex NounPhrase VerbPhrase
    | Simple VerbPhrase
data NounPhrase
    = Short Noun
    | Long Article (Maybe Adjective) Noun
data VerbPhrase
    = VerbPhrase Verb (Maybe NounPhrase)
type Noun      = String
type Verb      = String
type Article   = String
type Adjective = String
</code></pre>

<p>Then we can make our parser. This one follows the (factored) grammar even more closely! The one wrinkle here is that we always want our parser to consume an entire sentence, so we have to explicitly ask for it to do that by demanding an ""eof"" -- or end of ""file"".</p>

<pre><code>s   = (liftM2 Complex np vp &lt;|&gt; liftM Simple vp) &lt;* eof
np  = liftM Short n &lt;|&gt; liftM3 Long det (optionMaybe adj) n
vp  = liftM2 VerbPhrase v (optionMaybe np)
n   = anyOf [""i"", ""you"", ""bus"", ""cake"", ""bear""]
v   = anyOf [""hug"", ""love"", ""destroy"", ""am""]
det = anyOf [""a"", ""the""]
adj = anyOf [""pink"", ""stylish""]
</code></pre>

<p>The last piece is the tokenizer. For this simple application, we'll just tokenize based on whitespace, so the built-in <code>words</code> function works just fine. Let's try it out! Load the whole file in ghci:</p>

<pre><code>*Main&gt; parse s ""stdin"" (words ""i love the pink cake"")
Right (Complex (Short ""i"") (VerbPhrase ""love"" (Just (Long ""the"" (Just ""pink"") ""cake""))))
*Main&gt; parse s ""stdin"" (words ""i love pink cake"")
Left ""stdin"" (line 1, column 3):
unexpected ""pink""
expecting end of input
</code></pre>

<p>Here, <code>Right</code> indicates a successful parse, and <code>Left</code> indicates an error. The ""column"" number reported in the error is actually the word number where the error occurred, due to the way we're computing source positions in <code>singleToken</code>.</p>
",980,1318921328
Brute-Force language detection,"<p>I need an algorithm (any programming language) to test the vitality with an hill climbing algorithm for breaking a cipher for a crypto challenge. The algorithm should test how likely it is that an random-decryption (has no spaces) is an English text (also giving points for yet incomplete words!)  or just a random sequence of characters.</p>

<p>I tried it with several algorithms I developed but they were not so good.</p>

<p>My research:</p>

<p>An enigma M4 crypto project ( <a href=""http://www.bytereef.org/m4_project.html"" rel=""nofollow"">http://www.bytereef.org/m4_project.html</a> ) uses the Sinkov statistics, which I want to use, too.</p>

<p>The only thing I found was a document of «quebra -pedra», a Java framework that includes the Sinkov log-weight analysis I am searching for.</p>

<p><a href=""http://www.google.com/m?client=ms-android-samsung&amp;source=android-home#q=Quebra-pedra+framework+java"" rel=""nofollow"">http://www.google.com/m?client=ms-android-samsung&amp;source=android-home#q=Quebra-pedra+framework+java</a> </p>

<p>But I have not found where to download the framework. Also I have not found any implementation or description of the Sinkov test.</p>

<p>I would be glad for any hints. Thanks.</p>
","java, algorithm, cryptography, nlp",,494,1318894659
Unable to resolve error on loading Stanford NLP Parser,"<p>I keep getting the same error when using the Stanford NLP parser JAR files.</p>

<p>Code:</p>

<pre><code>import java.io.Reader;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.trees.*;

class TypedDependenciesDemo {
  public static void main(String[] args) {

  }

}
</code></pre>

<p>error:</p>

<blockquote>
  <p>usage: Relation treebank numberRanges</p>
</blockquote>

<p>Is it a bug or is there some way to fix this? Thanks!</p>
","java, nlp, stanford-nlp",,662,1318824070
how did WordNet come in being,"<p>I wonder how the hierarchical relationship in WordNet between the words are retrieved.</p>

<p>Is that manually done or via computer techniques. </p>

<p>If based on computer techniques, what are they?</p>
","wordnet, nlp","<p>From the FAQ:</p>

<blockquote>
  <p>q.1.2 Where do you get the definitions for WordNet? (short answer) Our
  lexicographers write them.</p>
  
  <p>Where do you get the definitions for WordNet? (long answer) From the
  foreword to WordNet: An Electronic Lexical Database, pp. xviii-xix:</p>
  
  <p>People sometimes ask, ""Where did you get your words?"" We began in 1985
  with the words in Kučera and Francis's Standard Corpus of Present-Day
  Edited English (familiarly known as the Brown Corpus), principally
  because they provided frequencies for the different parts of speech.
  We were well launched into that list when Henry Kučera warned us that,
  although he and Francis owned the Brown Corpus, the syntactic tagging
  data had been sold to Houghton Mifflin. We therefore dropped our plan
  to use their frequency counts (in 1988 Richard Beckwith developed a
  polysemy index that we use instead). We also incorporated all the
  adjectives pairs that Charles Osgood had used to develop the semantic
  differential. And since synonyms were critically important to us, we
  looked words up in various thesauruses: for example, Laurence Urdang's
  little ""Basic Book of Synonyms and Antonyms"" (1978), Urdang's revision
  of Rodale's ""The Synonym Finder"" (1978), and Robert Chapman's 4th
  edition of ""Roget's International Thesaurus"" (1977) -- in such works,
  one word quickly leads on to others. Late in 1986 we received a list
  of words compiled by Fred Chang at the Naval Personnel Research and
  Development Center, which we compared with our own list; we were
  dismayed to find only 15% overlap.</p>
  
  <p>So Chang's list became input. And in 1993 we obtained the list of
  39,143 words that Ralph Grishman and his colleagues at New York
  University included in their common lexicon, COMLEX; this time we were
  dismayed that WordNet contained only 74% of the COMLEX words. But that
  list, too, became input. In short, a variety of sources have
  contributed; we were not well disciplined in building our vocabulary.
  The fact is that the English lexicon is very large, and we were lucky
  that our sponsors were patient with us as we slowly crawled up the
  mountain.</p>
</blockquote>
",141,1318500099
Which is the best document clustering open-source package?,"<p>Which open-source package is the best for clustering a large corpus of documents? It should either decide the number of clusters by itself or it can also accept that as a parameter.</p>

<p>We have a large corpus of documents that don't really revolve around a particular topic - they are documents produced by sales and management people on various projects and clients in the organisation. I know that having such a spread corpus will degrade the performance, but we are trying to live with the best that we can get. Now, what is the best we can get :-)</p>
","nlp, machine-learning, cluster-analysis","<p>A list of topic modeling software from the homepage of an expert in the field:
<a href=""http://www.cs.princeton.edu/~blei/topicmodeling.html"" rel=""nofollow"">http://www.cs.princeton.edu/~blei/topicmodeling.html</a></p>

<p>A competing leading group (with open source code): <a href=""http://nlp.stanford.edu/software/tmt/tmt-0.3/"" rel=""nofollow"">http://nlp.stanford.edu/software/tmt/tmt-0.3/</a></p>

<p>Another open source java project:
<a href=""http://mallet.cs.umass.edu/topics.php"" rel=""nofollow"">http://mallet.cs.umass.edu/topics.php</a></p>
",5172,1318501800
"NLP project, python or C++","<p>We are working on Arabic Natural Language Processing project, we have limited our choices to either write the code in Python or C++ (and Boost library). We are thinking of these points:</p>

<ul>
<li><p>Python</p>

<ul>
<li>Slower than C++ (There is ongoing work to make Python faster)</li>
<li>Better UTF8 support</li>
<li>Faster in writing tests and trying different algorithms</li>
</ul></li>
<li><p>C++</p>

<ul>
<li>Faster than Python</li>
<li>Familiar code, every programmer knows C or C-like code</li>
</ul></li>
</ul>

<p>After the project is done, it should be not very hard to port the project to another programming languages.</p>

<p>What do you think is better and suitable for the project?</p>
","c++, python, boost, nlp","<p>Write it in Python, profile it, and if you need to speed parts of it up, write them in C++.  Python and C++ are similar enough that the ""familiar"" advantage with C++ will be irrelevant pretty quick.</p>

<p>I say this as someone who has developed primarily in C++ and has recently gotten serious with Python.  I like them both, but I can get Python code working a lot faster than C++.  Seriously, <code>dict</code> beats <code>std::map</code> in usability.</p>

<p>P.S. Here's some <a href=""http://docs.python.org/c-api/index.html"" rel=""noreferrer"">information on how to call C code from Python.</a></p>
",4565,1267210601
Extract business titles and time periods from string,"<p>I am extracting information about certain companies from Reuters using Python. I have been able to get the officer/executive names, biographies, and compensation from  <a href=""http://www.reuters.com/finance/stocks/companyOfficers?symbol=WWW.N"" rel=""nofollow"">this page</a></p>

<p>Now, I want to extract previous position titles and companies from the biography section, which looks something like this:</p>

<blockquote>
  <p>Mr. Donald T. Grimes is Senior Vice President, Chief Financial Officer and Treasurer of Wolverine World Wide, Inc., since May 2008. From 2007 to 2008, he was the Executive Vice President and Chief Financial Officer for Keystone Automotive Operations, Inc., a distributor of automotive accessories and equipment. Prior to Keystone, Mr. Grimes held a series of senior corporate and divisional finance roles at Brown-Forman Corporation, a manufacturer and marketer of premium wines and spirits. During his employment at Brown-Forman, Mr. Grimes was Vice President, Director of Beverage Finance from 2006 to 2007; Vice President, Director of Corporate Planning and Analysis from 2003 to 2006; and Senior Vice President, Chief Financial Officer of Brown-Forman Spirits America from 1999 to 2003.</p>
</blockquote>

<p>I can use simple regex to get the from and to years, but I am at a loss on how to write regex to get the titles and the company name as well. I know the string format is inconsistent, so I would take an answer that works for at least 70% of cases. Here's the output I would like:</p>

<pre><code>2007-2008, executive vice president and chief financial officer, Keystone Automotive operations
</code></pre>
","python, regex, nlp","<p>The problem you are trying to solve is well known and researched, and you will find a large amount of research paper describing approaches and algorithms if you google for the terms ""Named Entity Extraction"" and ""Relationship Extraction"" Some good starting points are:</p>

<ul>
<li><p>Chapter 7 of the book ""Natural Language Processing with Python"", in fact that entire book would probably be helpful. <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""nofollow"">Chapter online here</a></p></li>
<li><p>This paper on <a href=""http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CCMQFjAA&amp;url=http://www.lrec-conf.org/proceedings/lrec2008/pdf/192_paper.pdf&amp;ei=hZOXTvrJGcjh0QH32Ni5BA&amp;usg=AFQjCNGXBC8gRC_Gy4uL6eK4AaDSdPHwNQ"" rel=""nofollow"">""Named Entity Relation Mining using Wikipedia""</a></p></li>
<li><p>This paper ""dd<a href=""http://www.google.com/url?sa=t&amp;source=web&amp;cd=2&amp;sqi=2&amp;ved=0CCQQFjAB&amp;url=http://www.indect-project.eu/files/deliverables/public/deliverable-9.14/at_download/file&amp;rct=j&amp;q=%22Relation%20Mining%22%20%22dates%22&amp;ei=lJaXTsqSH8nF0AG1xpCNBg&amp;usg=AFQjCNGYuN7jIH0jLM3QY2lbh5CTzmDWIA&amp;cad=rja"" rel=""nofollow"">Novel Algorithms for Relationship Mining</a> which describes mining job titles and organizations as one of the examples.</p></li>
</ul>

<p>These are just a few links I've found interesting, there are a ton more and probably better ones than these, but this should get you started.</p>
",1274,1318524619
Java query about finding a word in a sentence,"<p>I am using Stanford's NLP parser (http://nlp.stanford.edu/software/lex-parser.shtml) to split a block of text into sentences and then see which sentences contain a given word. </p>

<p>Here is my code so far:</p>

<pre><code>import java.io.FileReader;
import java.io.IOException;
import java.util.List;

import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.process.*;

public class TokenizerDemo {

    public static void main(String[] args) throws IOException {
        DocumentPreprocessor dp = new DocumentPreprocessor(args[0]);
        for (List sentence : dp) {
            for (Object word : sentence) {
                System.out.println(word);
                System.out.println(word.getClass().getName());
                if (word.equals(args[1])) {
                    System.out.println(""yes!\n"");
                }
            }
        }
    }
}
</code></pre>

<p>I run the code from the command line using ""java TokenizerDemo testfile.txt wall""</p>

<p>The contents of testfile.txt is:</p>

<pre><code>Humpty Dumpty sat on a wall. Humpty Dumpty had a great fall.
</code></pre>

<p>So I want the program to detect ""wall"" in the first sentence (""wall"" entered as the second argument on the command line). But the program doesn't detect ""wall"", because it never prints ""yes!"". The output of the program is:</p>

<pre><code>Humpty
edu.stanford.nlp.ling.Word
Dumpty
edu.stanford.nlp.ling.Word
sat
edu.stanford.nlp.ling.Word
on
edu.stanford.nlp.ling.Word
a
edu.stanford.nlp.ling.Word
wall
edu.stanford.nlp.ling.Word
.
edu.stanford.nlp.ling.Word
Humpty
edu.stanford.nlp.ling.Word
Dumpty
edu.stanford.nlp.ling.Word
had
edu.stanford.nlp.ling.Word
a
edu.stanford.nlp.ling.Word
great
edu.stanford.nlp.ling.Word
fall
edu.stanford.nlp.ling.Word
.
edu.stanford.nlp.ling.Word
</code></pre>

<p>DocumentPreprocessor from the Stanford parser correctly splits the text into two sentences. The problem appears to be with the use of the equals method. Each word has type ""edu.stanford.nlp.ling.Word"". I've tried accessing the underlying string of the word, so I can then check if the string equals ""wall"", but I can't figure out how to access it.  </p>

<p>If I write the second for loop as ""for (Word word : sentence) {"" then I get an incompatible types error message on complilation. </p>
","java, string, nlp, stanford-nlp, sentence","<p>The <code>String</code> content can be accessed by calling the method: <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/Word.html#word%28%29"" rel=""nofollow""><code>word()</code></a> on <code>edu.stanford.nlp.ling.Word</code>; e.g.</p>

<pre><code>import edu.stanford.nlp.ling.Word;

List&lt;Word&gt; words = ...
for (Word word : words) {
  if (word.word().equals(args(1))) {
    System.err.println(""Yes!"");
  }
}
</code></pre>

<p>Also note that it is better to use generics when defining the <code>List</code> as it means the compiler or IDE will typically warn you if you attempt to compare classes of incompatible types (e.g. <code>Word</code> versus <code>String</code>).</p>

<p><strong>EDIT</strong></p>

<p>Turns out I was looking at an older version of the NLP API.  Looking at the most recent <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/DocumentPreprocessor.html"" rel=""nofollow""><code>DocumentPreprocessor</code></a> documentation I see that it implements <code>Iterable&lt;List&lt;HasWord&gt;&gt;</code> whereby <code>HasWord</code> defines the <code>word()</code> method.  Hence your code should look something like this:</p>

<pre><code>DocumentPreprocessor dp = ...
for (HasWord hw : dp) {
  if (hw.word().equals(args[1])) {
    System.err.println(""Yes!"");
  }
}
</code></pre>
",1514,1318512981
Clustering phrases around a theme,"<p>I have encountered a very unusual problem. I have a set of phrases (noun phrases) extracted from a large corpus of documents. These phrases are >=2 and &lt;=3 words of length. There is a need to cluster these phrases because the number of phrases extracted are very large in number and showing them as a simple list might not be useful for the user. </p>

<p>We are thinking of nice very simple ways of clustering these. Is there a quick tool/software/method that I could use to cluster these so that all phrases inside a cluster belong to a particular theme/topic, if I keep the number of topics as a fixed initially? I don't have any training set or any other clusters that I can use as a training set.</p>
","text, nlp, machine-learning",,1565,1318403314
Can i use nltk to build an answer engine using wikipedia&#39;s content,"<p>I have built an answer engine using wikipedia's content with php. It just returns the most relevant wikipedia article for a particular query which in most of the cases turn out to be useles.The users tend to ask in a question format like ""`what is the height of wayne rooney"" and i want to give the exact height of rooney which is available in his wikipedia article. Can i solve this if i use nltk. Please advice. </p>
","python, search, nlp, nltk","<p>in general: yes it is possible, BUT it is a very difficult task to build such a program.</p>

<p>what you are searching trying to build is called a 'semantic search engine' (see <a href=""http://en.wikipedia.org/wiki/Semantic_search"" rel=""nofollow"">wikipedia</a>) and there is a lot of research going on, how we can build a 'semantic web' and how to extract information of webpages, so that questions like the one you mentioned can be answered by computers, instead of just supplying links to relevant documents.
but the results are still far from perfect.</p>

<p>one of the better semantic search engines seems to be <a href=""http://www.trueknowledge.com/q/what_is_the_height_of_wayne_rooney"" rel=""nofollow"">trueknowledge</a> and of course the previously mentioned <a href=""http://www.wolframalpha.com/input/?i=What+is+the+weight+of+1+liter+rice%3F"" rel=""nofollow"">wolframalpha</a> (which has its strengths in science).</p>

<p>if you really want to build such a semantic search engine the nltk might provide some helpful basic tools, but don't expect it to be an easy task at all.</p>
",1926,1318364262
searching for language recognition library in Python or web service,"<p>Is there any tool that provides language recognition functionality? </p>

<p>Like if I input a sentence of English, it will suggest that the string <em>may</em> be English.</p>

<p>I think I need one of the following stuff.</p>

<ol>
<li>Python language recognition library.</li>
<li>Web service that provides such functionality.</li>
</ol>

<p>Anyone can help?</p>
","python, web-services, nlp","<p>I have used the <a href=""http://pypi.python.org/pypi/guess-language"" rel=""nofollow"">GuessLanguage</a> module pretty extensively. It detects 100+ languages and works really well for LONG pieces of text. English is very good. Other languages I am not sure. If you want to process short sentences at a time or tweets it is pretty bad as it was designed for doing detection on full articles.</p>

<p>My company (social media analytics) is currently looking for a replacement package as well and are exploring <a href=""http://www.nltk.org/download"" rel=""nofollow"">NLTK</a> as an option.</p>
",514,1318340284
How to create a bag of words using Weka?,"<p>I have a corpus of documents and I want to represent each document as a vector. Basically, the vector would have 1 for words that are present inside a document and for other words (which are present in other documents in the corpus and not in this particular document) it would have a 0. How do I create this vector for all the documents in Weka?</p>

<p>Is there a quick way to do this using Weka? I also want Weka to remove stopwords and so some pre-processing if possible before it creates this vector.</p>

<p>Thanks
Abhishek S</p>
","nlp, weka","<p>You want the <a href=""http://weka.sourceforge.net/doc.dev/weka/filters/unsupervised/attribute/StringToWordVector.html"" rel=""noreferrer"">StringToWordVector</a> filter.</p>

<p>It has options for binary occurrence and stopping, amongst many others, such as stemming, truncating the word list, discarding infrequent terms, case folding.</p>
",5751,1318231612
Validating Syntax of a Sentence,"<p>I'm looking for a library to simply validate the the syntax of english natural language sentences. It doesn't have to be correct all the time (and obviously some sentences will be ambiguous/ humans will disagree on validity).</p>

<p>So for example:
jim likes the blue ball 
 would be valid, whereas
jim likes likes blue ball jim
 would not be.</p>

<p>I've tried ""Syntactic parser of English sentences"" by Andrej Pancik which appears to do what I want, but unfortunately most sentences I'd consider to be ""valid"" it doesn't consider to be.</p>

<p>Is there any code out there I can use? Otherwise I'm thinking to do this myself by creating parse tree with something like ANTLR and identifying nouns with WordNet.</p>
","c#, nlp, abstract-syntax-tree","<p>You won't find this a) easy to do, or b) likely available as a package that just works. </p>

<p>People don't agree on what English <em>is</em></p>

<pre><code>  Colorless green ideas slept furiously.
</code></pre>

<p>thus you can't really write such a program that relaibly does what you want.   There are NLP parsers that claim to process much of English, but they aren't simple or small; I belive the so-called Stanford parser is one.</p>

<p>You can try to build you own, but you'll smack into the definition-of-English problem, unless you strongly constrain what you consider to be valid english. And this will likely get you the same effect as you had with Pancik's parser. (The act of writing a parser is an insistence that the language looks like what the parser accepts, regardless of the truth).</p>
",489,1318277320
Simple keyword / key phrase analysis in Ruby,"<p>I would like to create a simple list of popular keywords or phrases within tweets containing a specific hashtag.</p>

<p>For example, for all tweets with the '#justinbieber' hashtag, I would like to obtain an ordered list of the top ten most popular words and/or phrases used in those tweets, disregarding the usual irrelevancies such as 'and', 'the', etc. It doesn't have to be perfect, just meaningful.</p>

<p>What Ruby tools are there available for performing text analysis? Of course, the analysis part does not have to be specific to Twitter.</p>

<p>I will most likely be periodically requesting and storing tweets with the given hashtag, and then applying analysis to tweets within a given time frame.</p>

<p>The work will be done within a Rails or Sinatra app on Heroku, but the analysis would be done in a rake task or scheduled job of some kind. I haven't decided how I'll be storing the tweets yet.</p>
","ruby, twitter, text-analysis",,480,1318175925
What&#39;s the name of this field?,"<p>Image an application that accepts human text as input data, application process the text and then user ask a question from application, finaly application answers the question according to input data.</p>

<p>What's the name of the above field in AI?</p>

<p>Is there any open source project that implements first paragraph?</p>

<p>For example:</p>

<pre><code>User&gt; My name is Amir.
Machine&gt; OK.
User&gt; What's my name?
Machine&gt; Your name is Amir.
</code></pre>
","artificial-intelligence, nlp","<p>That's <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Natural_language_processing"" rel=""nofollow"">natural language processing</a> (NLP), more specifically a combination of <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Question_answering"" rel=""nofollow"">question answering</a> and dialogue systems/<a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Chatterbot"" rel=""nofollow"">chatterbots</a>.</p>

<p>If you google those terms + ""open source"", you'll find lots of stuff, but don't expect anything to work magic out of the box. Better pick up a book on the topic first; the classic is <a href=""http://www.cs.colorado.edu/~martin/slp.html"" rel=""nofollow"">Jurafsky &amp; Martin</a>.</p>
",96,1317985474
"PHP: split a sentence along commas, except for parallel structures","<p>I would like to split a setnence into parts along commas, except if it contains a paralllel structure.</p>

<p>For example, given these sentences (http://owl.english.purdue.edu/owl/resource/623/01/):</p>

<blockquote>
  <p>Mary likes to hike, to swim, and to ride a bicycle. </p>
  
  <p>Mary likes to hike, swim, and ride a bicycle.</p>
</blockquote>

<p>I would split these along the first comma only so I would get:</p>

<pre><code>sentence_array ( ""Mary likes to hike"", ""swim, and ride a bicycle"")
</code></pre>

<p>Perhaps with a forward looking regex, checking for at least 2-3 white spaces not surrounded by a comma?</p>
","php, text, nlp, sentence","<p>Maybe something like this could work:</p>

<pre><code>&lt;?php

$str = ""Mary likes to hike, to swim, and to ride a bicycle, also, something more at the end."";
var_dump($str);

$str = preg_replace('/((\s\w*){3,},)/', '\1*', $str);
$str = explode('*', $str);

var_dump($str);
?&gt;
</code></pre>

<p>It has to be worked on tho like using something more unique than a mere <code>*</code></p>
",374,1318006450
GUI for sentence alignment tasks,"<p>Can someone introduce me on how to write a simple web (HTML/XML) interface for a simple sentence alignment task?</p>

<p>The task is as follows:</p>

<p>The 1st line of the webpage would be the English sentence that will need to be matched to the chinese sentences below:</p>

<pre><code>000325EN    Whatever goes upon two legs is an enemy.

(checkbox)001054ZH  凡靠两条腿行走者皆为仇敌；
(checkbox)001055ZH  凡靠四肢行走者，或者长翅膀者，皆为亲友；
(checkbox)001056ZH  任何动物不得着衣；
(checkbox)001057ZH  任何动物不得卧床；
(checkbox)001058ZH  任何动物不得饮酒；
(checkbox)001059ZH  任何动物不得伤害其他动物；
(checkbox)001060ZH  所有动物一律平等。
(checkbox)Nil       No matching sentence

(submit button) (clear selection button)
</code></pre>

<p>The user should be able to click 1 or more of the check boxes.
When the submit button is clicked the webpage will save a line in a appendable textfile in the format</p>

<p>SentID&lt;\TAB>@English_sentence&lt;&lt;\TAB>SentID2&lt;\TAB>=Chinese_sentence (e.g.:</p>

<pre><code>000325EN    @Whatever goes upon two legs is an enemy.   001054ZH    =凡靠两条腿行走者皆为仇敌；
</code></pre>

<p>if there are more than 1 match to the English sentence, it may look like this</p>

<pre><code>000325EN    @Whatever goes upon two legs is an enemy.   001054ZH    =凡靠两条腿行走者皆为仇敌；  001055ZH    =凡靠四肢行走者，或者长翅膀者，皆为亲友；
</code></pre>
","html, xml, nlp, text-alignment","<p>Depending on what should happen with the stored data, it indeed is possible to store them on the client, without any server-side scripting, see the HTML5 local storage facility: <a href=""https://developer.mozilla.org/en/dom/storage"" rel=""nofollow"">https://developer.mozilla.org/en/dom/storage</a> (including example for degradation to cookies).</p>

<p>A good starting point for getting into locally stored data with HTML5 is <a href=""http://diveintohtml5.ep.io/storage.html"" rel=""nofollow"">http://diveintohtml5.ep.io/storage.html</a>.</p>

<p>A simple example taken from <a href=""http://msdn.microsoft.com/en-us/library/cc197062(v=vs.85).aspx#_global"" rel=""nofollow"">http://msdn.microsoft.com/en-us/library/cc197062(v=vs.85).aspx#_global</a> and enhanced with localStorage detection from the link above:</p>

<pre><code>&lt;p&gt;
  You have viewed this page
  &lt;span id=""count""&gt;an untold number of&lt;/span&gt;
  time(s).
&lt;/p&gt;

&lt;script&gt;
  function supports_html5_storage() {
    try {
      return 'localStorage' in window &amp;&amp; window['localStorage'] !== null;
    } catch (e) {
      return false;
    }
  }

  if (supports_html5_storage()) {
    var storage = window.localStorage;
    if (!storage.pageLoadCount) storage.pageLoadCount = 0;
    storage.pageLoadCount = parseInt(storage.pageLoadCount, 10) + 1;
    document.getElementById('count').innerHTML = storage.pageLoadCount;
  }
  else {
    alert('No local storage available!');
  }
&lt;/script&gt;
</code></pre>
",296,1310967617
Dynamic text-pattern detection algorithm?,"<p>I was wondering if such algorithm exists. I have a bunch of text documents and would like to find a pattern among all these documents, if a pattern exists. Please note im NOT trying to classify the documents all i want to do is find a pattern if it exists among some documents. Thanks!</p>
","algorithm, text, nlp, machine-learning, data-modeling","<p>The question as it stands now is kinda vague.. you kinda need to know what you are looking for in order to be able to find it.<br>
Some ideas that may be of use -</p>

<ol>
<li>Get n-gram counts for each document separately for n = 1,2,3,4 and then compare the frequencies of each ngram across the documents. This should help you find commonly occuring phrases across all documents.</li>
<li>Get a part of speech tagger to get convert all the docs into a stream of POS tags and then do the same as 1</li>
<li>Use a PCFG software such as the Stanford Parser to get parse trees for all the sentences across all the documents, and then try to figure out how similar the distribution of sentence structures are for different documents.</li>
</ol>
",1535,1317785799
NLP algorithm to &#39;fill out&#39; search terms,"<p>I'm trying to write an algorithm (which I'm assuming will rely on natural language processing techniques) to 'fill out' a list of search terms. There is probably a name for this kind of thing which I'm unaware of. What is this kind of problem called, and what kind of algorithm will give me the following behavior?</p>

<p>Input:</p>

<pre><code>    docs = [
    ""I bought a ticket to the Dolphin Watching cruise"",
    ""I enjoyed the Dolphin Watching tour"",
    ""The Miami Dolphins lost again!"",
    ""It was good going to that Miami Dolphins game""
    ], 
    search_term = ""Dolphin""
</code></pre>

<p>Output:</p>

<pre><code>[""Dolphin Watching"", ""Miami Dolphins""]
</code></pre>

<p>It should basically figure out that if ""Dolphin"" appears at all, it's virtually always either in the bigrams ""Dolphin Watching"" or ""Miami Dolphins"". Solutions in Python preferred.</p>
","python, nlp, n-gram","<blockquote>
  <p>It should basically figure out that if ""Dolphin"" appears at all, it's virtually always either in the bigrams ""Dolphin Watching"" or ""Miami Dolphins"".</p>
</blockquote>

<p>Sounds like you want to determine the <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Collocation"" rel=""nofollow noreferrer"">collocations</a> that <em>Dolphin</em> occurs in. There are various methods for collocation finding, the most popular being to compute <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Pointwise_mutual_information"" rel=""nofollow noreferrer"">point-wise mutual information</a> (PMI) between terms in your corpus, then select the terms with the highest PMI for <em>Dolphin</em>. You might remember PMI from the <a href=""https://stackoverflow.com/questions/3920759/unsupervised-sentiment-analysis/3933874#3933874"">sentiment analysis algorithm</a> that I suggested earlier.</p>

<p>A Python implementation of various collocation finding methods is included in NLTK as <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.collocations-module.html"" rel=""nofollow noreferrer""><code>nltk.collocations</code></a>. The area is covered in some depth in <a href=""http://nlp.stanford.edu/fsnlp/"" rel=""nofollow noreferrer"">Manning and Schütze's <em>FSNLP</em></a> (1999, but still current for this topic).</p>
",378,1317339009
CWB encoding Corpus,"<p>According to the Corpus Work Bench, to encode a corpus i need to use the cwb-encode perl script</p>

<p>""encode the corpus, i.e. convert the verticalized text to CWB binary format with the cwb-encode tool. Note that the command below has to be entered on a single line."" <a href=""http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html"" rel=""nofollow"">http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html</a></p>

<pre><code>$ cwb-encode -d /corpora/data/example -f example.vrt -R /usr/local/share/cwb/registry/example -P pos -S s
</code></pre>

<p>when i tried it, it says the file is missing but i'm sure the file is in $HOME/corpora/data/example, the error was</p>

<pre><code>$ cwb-encode -d /corpora/data/example -f example.vrt -R /usr/local/share/cwb/registry/example -P pos -S s
example.vrt: No such file or directory
Can't open input file example.vrt!
</code></pre>

<p>can anyone figure out why?</p>
","nlp, stanford-nlp, corpus","<p>How about giving full path to <code>example.vrt</code>:</p>

<pre><code>cwb-encode -d /corpora/data/example -f /corpora/data/example/example.vrt -R /usr/local/share/cwb/registry/example -P pos -S s
</code></pre>
",508,1302228666
How to create Google Calendar Quick Add feature in Java?,"<p>I am working on a school project that requires a little bit of Natural Language Processing. We have to implement a feature that is similar to Google Calendar Quick Add feature in Java.</p>

<p><a href=""http://www.google.com/support/calendar/bin/answer.py?answer=36604"" rel=""nofollow"">http://www.google.com/support/calendar/bin/answer.py?answer=36604</a></p>

<p>I have done some research on NLP and so far have not managed to successfully implement the feature using LingPipe.</p>

<p>Is there any other Java library than could help me implement this feature?</p>

<p>Thanks.</p>
","java, calendar, nlp","<p>Yes&lt; I ve actually done this for <a href=""http://g-calendar.appspot.com/application/testmessage"" rel=""nofollow"">russian langauge</a> via this <a href=""http://code.google.com/p/graph-expression/"" rel=""nofollow"">toolkit</a></p>
",319,1317282848
ANTLR for Writing JAPE Grammar,"<p>I am using GATE to process texts written in natural language. I have to extract height, weight, bp etc from the text and store it in structured form. Now, these things(i.e height, weight etc) can be written in many forms which is unknown to me. Writing JAPE grammar for all the different ways I can come up with, is merely hard-coding it. Do I have any other option to process the docs in a more flexible way?</p>

<p>Also, I am confused as to whether I can use ANTLR to generate jape grammar by presenting different ways of writing height, weight etc? </p>

<p>The text will contain the following:</p>

<pre><code>Vitals: Height: 72 inches, Weight is 170 pounds, T is 89.9 degree Fahrenheit. OR
Vitals: He is 184 cm tall, his weight was 67.8 Kg, RR 16 (its respiration rate). OR
Vitals: height is 6ft 3 in, he weighs 70 kg, pulse is 67, Temperature 99.8 degrees.
</code></pre>

<p>Now I use JAPE grammar and ANNIE to tokenize the text to extract these vitals and store in structured form. Will using ANTLR be flexible to tokenize this such text? Flexible in the sense that I should not hard-code it for each way of representation, as you see height, weight etc can be represented in many forms.</p>

<p>Will that be a good idea to search for or develop any grammar generator that would generate jape grammars?</p>

<p>If you require any detail to better understand the problem pls let me know.</p>

<p>Thanks a lot!!</p>
","nlp, antlr, text-processing, gate","<p>You can try JAPE lightway alternative <a href=""http://code.google.com/p/graph-expression/"" rel=""nofollow"">Gexp</a></p>
",876,1317208216
General frameworks for preparing training data?,"<p>As a student of computational linguistics, I frequently do machine learning experiments where I have to prepare training data from all kinds of different resources like raw or annotated text corpora or syntactic tree banks. For every new task and every new experiment I write programs (normally in Python and sometimes Java) to extract the features and values I need and transform the data from one format to the other. This usually results in a very large number of very large files and a very large number of small programs which process them in order to get the input for some machine learning framework (like the arff files for Weka). </p>

<p>One needs to be extremely well organised to deal with that and program with great care not to miss any important peculiarities, exceptions or errors in the tons of data. Many principles of good software design like design patterns or refactoring paradigms are no big use for these tasks because things like security, maintainability or sustainability are of no real importance - once the program successfully processed the data one doesn't need it any longer. This has gone so far that I even stopped bothering about using classes or functions at all in my Python code and program in a simple procedural way. The next experiment will require different data sets with unique characteristics and in a different format so that their preparation will likely have to be programmed from scratch anyway. My experience so far is that it's not unusual to spend 80-90% of a project's time on the task of preparing training data. Hours and days go by only on thinking about how to get from one data format to another. At times, this can become quite frustrating.</p>

<p>Well, you probably guessed that I'm exaggerating a bit, on purpose even, but I'm positive you understand what I'm trying to say. My question, actually, is this:</p>

<p>Are there any general frameworks, architectures, best practices for approaching these tasks?    How much of the code I write can I expect to be reusable given optimal design?</p>
","machine-learning, nlp, code-reuse, training-data","<p>I find myself mostly using the textutils from  GNU coreutils and flex for corpus preparation, chaining things together in simple scripts, at least when the preparations i need to make are simple enough for regular expressions and trivial filtering etc.</p>

<p>It is still possible to make things reusable, the general rules also apply here. If you are programming with no regard to best practices and the like and just program procedurally there is IMHO really no wonder that you have to do everything from scratch when starting a new project. </p>

<p>Even though the format requirements will vary a lot there is still many common tasks, ie. tag-stripping, tag-translation, selection, tabulation, some trivial data harvesting such as number of tokens, sentences and the like. Programming these tasks aming for high reusability will pay off, even though it takes longer at first. </p>
",426,1263489063
Twitter Subjectivity Training Sets,"<p>I need a reliable and accurate method to filter tweets as subjective or objective. In other words I need to build a filter in something like Weka using a training set. </p>

<p>Are there any training sets available which could be used as a subjective/objective classifier for Twitter messages or other domains which may be transferable?</p>
","text, twitter, nlp, classification, training-data","<p>For research and non-profit purposes, SentiWordNet gives you exactly what you want. A commercial license is available too.</p>

<p>SentiWordNet : <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow noreferrer"">http://sentiwordnet.isti.cnr.it/</a></p>

<p>Sample Jave Code: <a href=""http://sentiwordnet.isti.cnr.it/code/SWN3.java"" rel=""nofollow noreferrer"">http://sentiwordnet.isti.cnr.it/code/SWN3.java</a></p>

<p>Related Paper: <a href=""http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf"" rel=""nofollow noreferrer"">http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf</a></p>

<hr>

<p>The other approach I would try:</p>

<p>Example</p>

<p>Tweet 1: @xyz u should see the dark knight. Its awesme.</p>

<p>1) First a dictionary lookup for the for meanings.</p>

<p>""u"" and ""awesme"" will not return anything.</p>

<p>2) Then go against the known abbreviations/shorthands and substitute matches with the expansions
(Some resources: netlingo <a href=""http://www.netlingo.com/acronyms.php"" rel=""nofollow noreferrer"">http://www.netlingo.com/acronyms.php</a> or  smsdictionary <a href=""http://www.smsdictionary.co.uk/abbreviations"" rel=""nofollow noreferrer"">http://www.smsdictionary.co.uk/abbreviations</a>)</p>

<p>Now the original tweet will look like:</p>

<p>Tweet 1: @xyz <strong>you</strong> should see the dark knight. Its awesme.</p>

<p>3) Then feed the remaining words in the spell checker and substitute with the best match (not always ideal and error prone for small words)</p>

<p>Related Link: 
<a href=""https://stackoverflow.com/questions/559510/looking-for-java-spell-checker-library"">Looking for Java spell checker library</a></p>

<p>Now the original tweet will look like:</p>

<p>Tweet 1: @xyz you should see the dark knight. Its <strong>awesome</strong>.</p>

<p>4) Split and feed the tweet into SWN3, aggregate the result </p>

<p>The problem with this approach is that </p>

<p>a) Negations should be handled outside SWN3. </p>

<p>b) Information in emoticons and exaggerated punctuations will be lost or they need to be handled separately.</p>
",2362,1312212842
"Can extract generic entities using Lingpipe other than People, Org and Loc?","<p>I have read through Lingpipe for NLP and found that we have a capability there to identify mentions of names of people, locations and organizations. My questions is that if I have a training set of documents that have mentions of let's say software projects inside the text, can I use this training set to train a named entity recognizer? Once the training is complete, I should be able to feed a test set of textual documents to the trained model and I should be able to identify mentions of software projects there.</p>

<p>Is this generic NER possible using NER? If so, what features should I be using that I should feed?</p>

<p>Thanks
Abhishek S</p>
","nlp, machine-learning, text-analysis, named-entity-extraction","<p>Provided that you have enough training data with tagged software projects that would be possible. </p>

<p>If using Lingpipe, I would use character n-grams model as the first option for your task. They are simple and usually do the work. If results are not good enough some of the standard NER features are: </p>

<ul>
<li>tokens</li>
<li>part of speech (POS)</li>
<li>capitalization</li>
<li>punctuaction</li>
<li>character signatures: these are some ideas: ( LUCENE -> AAAAAA -> A) , (Lucene -> Aaaaaa -> Aa ), (Lucene-core --> Aaaaa-aaaa --> Aa-a) </li>
<li>it may also be useful to compose a gazzeteer (list of software projects) if you can obtain that from Wikipedia, sourceforge or any other internal resource. </li>
</ul>

<p>Finally, for each token you could add contextual features, tokens before the current one (t-1, t-2...), tokens after the current one (t+1,t+2...) as well as their bigram combinations (t-2^t-1), (t+1^t+2). </p>
",448,1316858040
Term Extraction and Sentiment Analysis Open Source Project,"<p>I want to extract important terms from a text and create a domain specific term set. Then I want to learn how these words are used in text, positively or negatively. </p>

<p>Do you know any open source project which will help me to accomplish this tasks?</p>

<p>Edit: </p>

<p>Example Text:</p>

<pre><code>""Although car is not comfortable, I like the design of it.""
</code></pre>

<p>From this text, I want to extract something like these:</p>

<pre><code>design:        positive
comfort(able): negative
</code></pre>
","open-source, nlp, machine-learning",,2796,1302020870
Doing a hierarchical sentiment analysis with LingPipe,"<p>This is in the context of doing sentiment analysis using LingPipe machine learning tool. I have to classify if a sentence in a big paragraph has a positive/negative sentiment. I know of the following approach in LingPipe</p>

<ol>
<li><p>Classify if the complete paragraph based on its polarity - negative or positive.</p>

<p>Here, I yet don't know the polarity at the sentence level. We are still at the paragraph level. How do I determine the polarity at the sentence level of a paragraph, of whether a sentence in a paragraph is a positive/negative sentence? I know that LingPipe is capable of classifying if a sentence is subjective/objective. So using this approach,,,,</p>

<p>,,,, should I </p></li>
<li><p>First train LingPipe on a large set of sentences that are subjective/objective. </p></li>
<li>Use the trained model to extract all subjective sentences out of a test paragraph.</li>
<li>Train a LingPipe classifier based on the extracted subjective sentences for polarity by manually labeling them as positive/negative.</li>
<li><p>Now used the trained polarity model and feed a test subjective sentence (that is done by passing a sentence through the trained subjective/objective) model, and then determine if the statement is positive/negative?</p>

<p>Does the above approach work? In the above proposed approach, we know that LingPipe is capable of accepting a large textual content (paragraph) for polarity classification. Will it do a good job if we just pass a single subjective sentence for polarity classification? I am confused!</p></li>
</ol>
","machine-learning, nlp, sentiment-analysis",,1225,1316597049
Detect Programming Language from code snippet,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/475033/detecting-programming-language-from-a-snippet"">Detecting programming language from a snippet</a>  </p>
</blockquote>



<p>Is there a way to identify the following for a text snippet?</p>

<ol>
<li>Whether it is a piece of code</li>
<li>The programming language in which it is written</li>
</ol>
","java, javascript, python, programming-languages, nlp","<p>Depends on how long is the code snippet, for extremely short code sample it may not be possible at all, and still not easy for long code snippets.</p>

<p>But anything that is developed for this purpose cannot ensure 100% accuracy of detecting the programming language for any length of code snippet (except if everything in the language is in the code snippet).</p>

<p>A lot of the programming languages is similar. One very accurate (not 100% though) is to look for the header files or libraries etc that are included in the code, and how they are added (using, #include etc), it can be a huge and very accurate hint about the programming language.</p>
",2748,1316874300
Java: Multidimensional Scaling?,"<p>I'm doing a Natural Language Processing project where I compute a bunch of attributes of a text, giving me a vector of values for each text. I want to compare these vectors with multidimensional scaling. What Java libraries/toolkits do you recommend for doing this?</p>
","java, nlp, multi-dimensional-scaling","<p>I used the following MDSJ to reduce a matrix from k dimensions to n  dimensions. Where k was 4000 and n was 3 in my case. The app runs both as a standalone application and as a jar lib that can be included in your application. </p>

<p><a href=""http://www.inf.uni-konstanz.de/algo/software/mdsj/"" rel=""nofollow noreferrer"">http://www.inf.uni-konstanz.de/algo/software/mdsj/</a></p>
",1806,1260132790
which similarity function of nltk.corpus.wordnet is Appropriate for find similarity of two words?,"<p>which similarity function in <code>nltk.corpus.wordnet</code> is Appropriate for find similarity of two words?</p>

<pre><code> path_similarity()?
    lch_similarity()?
    wup_similarity()?
    res_similarity()?
    jcn_similarity()?
    lin_similarity()?
</code></pre>

<p>I want use a function for <code>word clustering</code> and <code>yarowsky</code> algorightm for find similar <code>collocation</code> in a large text.</p>
","python, nlp, nltk, wordnet, corpus",,2165,1315910576
The 2010 Loebner Prize winner bot: Suzette,"<p>What are some of the techniques that a machine could use to be able to carry a good conversation/pass the Turing Test? I know this has to do with Natural Language Understanding and Processing, but I need more details.</p>

<p>I'm particularly interested in the chatbot Suzette who won the 2010 Loebner Prize.</p>

<p>Thank you</p>
","artificial-intelligence, nlp, chatbot",,240,1316704341
Simple Sentiment Analysis,"<p>It appears that the simplest, naivest way to do basic sentiment analysis is with a Bayesian classifier (confirmed by what I'm finding here on SO). Any counter-arguments or other suggestions?</p>
","nlp, bayesian","<p>A Bayesian classifier with a bag of words representation is the simplest statistical method. You can get significantly better results by moving to more advanced classifiers and feature representation, at the cost of more complexity.</p>

<p>Statistical methods aren't the only game in town. Rule based methods that have more understanding of the structure of the text are the other main option. From what I have seen, these don't actually perform as well as statistical methods.</p>

<p>I recommend Manning and Schütze's Foundations of Statistical Natural Language Processing chapter 16, Text Categorization.</p>
",3673,1246733895
Finding words from Wordnet separated by a fixed Edit Distance from a given word,"<p>I am writing a spell checker using nltk and wordnet, I have a few wrongly spelt words say ""belive"". What I want to do is find all words from wordnet that are separated by a leveshtein's edit distance of 1 or 2 from this given word. 
Does nltk provide any methods to accomplish this? How to do this?</p>

<hr>

<p>May be, I put it wrongly. the <code>edit_distance</code> method takes 2 arguments like <code>edit_distance(word1,word2)</code> returns the levenshtein's distance between word1 and word2.
What I want is to find edit distance between the word I give with every other word in wordnet.</p>
","python, nlp, nltk, wordnet",,1503,1316543794
Looking for a way to optimize this algorithm for parsing a very large string,"<p>The following class parses through a very large string (an entire novel of text) and breaks it into consecutive 4-character strings that are stored as a Tuple. Then each tuple can be assigned a probability based on a calculation. I am using this as part of a monte carlo/ genetic algorithm to train the program to recognize a language based on syntax only (just the character transitions). </p>

<p>I am wondering if there is a faster way of doing this. It takes about 400ms to look up the probability of any given 4-character tuple. The relevant method _Probablity() is at the end of the class. </p>

<p>This is a computationally intensive problem related to another post of mine: <a href=""https://stackoverflow.com/questions/7423279/algorithm-for-computing-the-plausibility-of-a-function-monte-carlo-method"">Algorithm for computing the plausibility of a function / Monte Carlo Method</a></p>

<p>Ultimately I'd like to store these values in a 4d-matrix. But given that there are 26 letters in the alphabet that would be a HUGE task. (26x26x26x26). If I take only the first 15000 characters of the novel then performance improves a ton, but my data isn't as useful. </p>

<p>Here is the method that parses the text 'source':</p>

<pre><code>    private List&lt;Tuple&lt;char, char, char, char&gt;&gt; _Parse(string src)
    {
        var _map = new List&lt;Tuple&lt;char, char, char, char&gt;&gt;(); 

        for (int i = 0; i &lt; src.Length - 3; i++)
        {
          int j = i + 1;
          int k = i + 2;
          int l = i + 3;

          _map.Add
            (new Tuple&lt;char, char, char, char&gt;(src[i], src[j], src[k], src[l])); 
        }

        return _map; 
    }
</code></pre>

<p>And here is the _Probability method:</p>

<pre><code>    private double _Probability(char x0, char x1, char x2, char x3)
    {
        var subset_x0 = map.Where(x =&gt; x.Item1 == x0);
        var subset_x0_x1_following = subset_x0.Where(x =&gt; x.Item2 == x1);
        var subset_x0_x2_following = subset_x0_x1_following.Where(x =&gt; x.Item3 == x2);
        var subset_x0_x3_following = subset_x0_x2_following.Where(x =&gt; x.Item4 == x3);

        int count_of_x0 = subset_x0.Count();
        int count_of_x1_following = subset_x0_x1_following.Count();
        int count_of_x2_following = subset_x0_x2_following.Count();
        int count_of_x3_following = subset_x0_x3_following.Count(); 

        decimal p1;
        decimal p2;
        decimal p3;

        if (count_of_x0 &lt;= 0 || count_of_x1_following &lt;= 0 || count_of_x2_following &lt;= 0 || count_of_x3_following &lt;= 0)
        {
            p1 = e;
            p2 = e;
            p3 = e;
        }
        else
        {
            p1 = (decimal)count_of_x1_following / (decimal)count_of_x0;
            p2 = (decimal)count_of_x2_following / (decimal)count_of_x1_following;
            p3 = (decimal)count_of_x3_following / (decimal)count_of_x2_following;

            p1 = (p1 * 100) + e; 
            p2 = (p2 * 100) + e;
            p3 = (p3 * 100) + e; 
        }

        //more calculations omitted

        return _final; 
    }
}
</code></pre>

<p><b>EDIT</b> - I'm providing more details to clear things up,</p>

<p>1) Strictly speaking I've only worked with English so far, but its true that different alphabets will have to be considered. Currently I only want the program to recognize English, similar to whats described in this paper: <a href=""http://www-stat.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf"" rel=""nofollow noreferrer"">http://www-stat.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf</a></p>

<p>2) I am calculating the probabilities of n-tuples of characters where n &lt;= 4. For instance if I am calculating the total probability of the string ""that"", I would break it down into these independent tuples and calculate the probability of each individually first:</p>

<p>[t][h] </p>

<p>[t][h][a]</p>

<p>[t][h][a][t]</p>

<p>[t][h] is given the most weight, then [t][h][a], then [t][h][a][t]. Since I am not just looking at the 4-character tuple as a single unit, I wouldn't be able to just divide the instances of [t][h][a][t] in the text by the total no. of 4-tuples in the next. </p>

<p>The value assigned to each 4-tuple can't overfit to the text, because by chance many real English words may never appear in the text and they shouldn't get disproportionally low scores. Emphasing first-order character transitions (2-tuples) ameliorates this issue. Moving to the 3-tuple then the 4-tuple just refines the calculation. </p>

<p>I came up with a Dictionary that simply tallies the count of how often the tuple occurs in the text (similar to what Vilx suggested), rather than repeating identical tuples which is a waste of memory. That got me from about ~400ms per lookup to about ~40ms per, which is a pretty great improvement. I still have to look into some of the other suggestions, however. </p>
","c#, artificial-intelligence, nlp, tuples, montecarlo","<p>In yoiu probability method you are iterating the map 8 times. Each of your wheres iterates the entire list and so does the count. Adding a .ToList() ad the end would (potentially) speed things. That said I think your main problem is that the structure you've chossen to store the data in is not suited for the purpose of the probability method. You could create a one pass version where the structure you store you're data in calculates the tentative distribution on insert. That way when you're done with the insert (which shouldn't be slowed down too much) you're done or you could do as the code below have a cheap calculation of the probability when you need it.</p>

<p>As an aside you might want to take puntuation and whitespace into account. The first letter/word of a sentence and the first letter of a word gives clear indication on what language a given text is written in by taking punctuaion charaters and whitespace as part of you distribution you include those characteristics of the sample data. We did that some years back. Doing that we shown that using just three characters was almost as exact (we had no failures with three on our test data and almost as exact is an assumption given that there most be some weird text where the lack of information would yield an incorrect result). as using more (we test up till 7) but the speed of three letters made that the best case.</p>

<p><strong>EDIT</strong></p>

<p>Here's an example of how I think I would do it in C#</p>

<pre><code>class TextParser{
        private Node Parse(string src){
            var top = new Node(null);

            for (int i = 0; i &lt; src.Length - 3; i++){
                var first = src[i];
                var second = src[i+1];
                var third = src[i+2];
                var fourth = src[i+3];

                var firstLevelNode = top.AddChild(first);
                var secondLevelNode = firstLevelNode.AddChild(second);
                var thirdLevelNode = secondLevelNode.AddChild(third);
                thirdLevelNode.AddChild(fourth);
            }

            return top;
        }
    }

    public class Node{
        private readonly Node _parent;
        private readonly Dictionary&lt;char,Node&gt; _children 
                         = new Dictionary&lt;char, Node&gt;();
        private int _count;

        public Node(Node parent){
            _parent = parent;
        }

        public Node AddChild(char value){
            if (!_children.ContainsKey(value))
            {
                _children.Add(value, new Node(this));
            }
            var levelNode = _children[value];
            levelNode._count++;
            return levelNode;
        }
        public decimal Probability(string substring){
            var node = this;
            foreach (var c in substring){
                if(!node.Contains(c))
                    return 0m;
                node = node[c];
            }
            return ((decimal) node._count)/node._parent._children.Count;
        }

        public Node this[char value]{
            get { return _children[value]; }
        }
        private bool Contains(char c){
            return _children.ContainsKey(c);
        }
    }
</code></pre>

<p>the usage would then be:</p>

<pre><code>var top = Parse(src);
top.Probability(""test"");
</code></pre>
",884,1316415781
Conditional Random Fields and the Label Bias Problem,"<p>I was reading <a href=""http://www.cis.upenn.edu/~pereira/papers/crf.pdf"" rel=""nofollow"">this paper</a> about conditional random fields and was confused by the ""label bias problem"" mentioned in the paper.  I've looked online for some explanations of it but still don't feel like I have a good grasp of the issue. What exactly is the label bias problem? Can someone provide a clear example of it and why it is bad? </p>
","nlp, tagging",,681,1316528188
The best IR software for my use?,"<p>I want to take what people chat about in a chat room and do the following information retrieval:</p>

<ol>
<li>Get the keywords</li>
<li>Ignore all noise words, keep verb an nouns mainly</li>
<li>Perform stemming on the keywords so that I don't store the same keyword in many forms</li>
<li>If a synonym keyword is already stored in my storage then the existing synonym should be used instead of the new keyword</li>
<li>Store the processed keyword in a persistant storage with a reference to the chat message it was located in and the user who uttered it</li>
</ol>

<p>With this prosessed information I want to slowly get an idea of what people are talking about in chatrooms, and then use this to automatically find related chatrooms etc. based on these keywords.</p>

<p>My question to you is a follows: What is the best C/C++ or .NET tools for doing the above?</p>
","nlp, semantics, keyword, information-retrieval, stemming","<p>I partially agree with @larsmans comment. Your question, in practice, may indeed be more complex than the question you posted.</p>

<p>However, simplifying the question/problem, I guess the answer to your question could be one of Lucene's implementation: <a href=""http://lucene.apache.org/java/docs/index.html"" rel=""nofollow noreferrer"">Lucene</a> (Java), <a href=""http://incubator.apache.org/lucene.net/"" rel=""nofollow noreferrer"">Lucene.Net</a> (C#) or <a href=""http://clucene.sourceforge.net/"" rel=""nofollow noreferrer"">CLucene</a> (C++).</p>

<p>Following the points in your question:</p>

<p>Lucene would take care of point 1 by using String tokenizers (you can customize or use your own).
For point 2 you could use a <a href=""http://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/analysis/TokenFilter.html"" rel=""nofollow noreferrer"">TokenFilter</a> like <a href=""http://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/analysis/StopFilter.html"" rel=""nofollow noreferrer"">StopFilter</a> so Lucene can read a list of stopwords (""the"", ""a"", ""an""...) that it should not use.
For point 3 you could use <a href=""http://lucene.apache.org/java/2_3_1/api/core/org/apache/lucene/analysis/PorterStemFilter.html"" rel=""nofollow noreferrer"">PorterStemFilter</a>.
Point 4 is a little bit trickier, but could be done using a customized <a href=""http://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/analysis/TokenFilter.html"" rel=""nofollow noreferrer"">TokenFilter</a>.
Point 1 to 4 are perfomed in the Analysis/tokenization phase, which an <a href=""http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/analysis/Analyzer.html%5C"" rel=""nofollow noreferrer"">Analyzer</a> is responsible.</p>

<p>Regarding point 5, in Lucene you can store Documents with fields. A document can have an arbitrary number and mix of fields. So you could create a single Document for each chat room with all its text concatenated, and have another field of the document reference the chatroom it was extracted from. You will end up with a bunch of Lucene documents that <a href=""https://stackoverflow.com/questions/1844194/get-cosine-similarity-between-two-documents-in-lucene"">you can compare</a>. So you can compare your current chat room with others to see which one is more similar to the one you are on.</p>

<p>If all you want is a set of the best keywords to describe a chatrom your needs are closer to information extraction/automatic summarization/topic spotting task as @larsmans said. But you can <a href=""https://stackoverflow.com/questions/6334692/how-to-use-a-lucene-analyzer-to-tokenize-a-string/6335057#6335057"">still use Lucene for the parsing/tokenization</a> phase.</p>

<p>*I referenced the Java docs, but CLucene and Lucene.Net have very similar APIs so it won't be much trouble to figure out the differences.</p>
",262,1316453839
Perl or Java Sentiment Analysis,"<p>I was wondering if anybody knew of any good Perl modules and/or Java classes for sentiment analysis.  I have read about LingPipe, but the program would eventually need to be used for commercial use so something open-source would be better.  I also looked into GATE, but their documentation on sentiment analysis is sparse at best.</p>
","java, perl, nlp, sentiment-analysis","<p>Have a look at <a href=""http://search.cpan.org/perldoc?Rate_Sentiment"" rel=""noreferrer"">Rate_Sentiment</a> in the <a href=""http://search.cpan.org/dist/WebService-GoogleHack/"" rel=""noreferrer"">WebService::GoogleHack</a> module at <a href=""http://search.cpan.org/"" rel=""noreferrer"">CPAN</a>. There's more information about the project at <a href=""http://google-hack.sourceforge.net/"" rel=""noreferrer"">SourceForge</a>.</p>
",3952,1289830824
Lucene Porter Stemmer Thread Safe?,"<p>Quick question, is the porter stemmer from Lucene packages (Java) thread safe?</p>

<p>I'm guessing the answer is no as you need to set the current string, invoke stem method then get the current block to get the stemmed word. But perhaps I'm missing something - Is there are thread safe method to do stemming of a single word or string from Lucene?</p>

<p>Does anyone from experience know if it is faster to instantiate one Porter Stemmer instance and then use a synchronized block over that stemmer instance and do the <code>setCurrent(""...""); stem(); get();</code> routine or is it just faster to create a new porter stemmer instance for each string/document you want to process. </p>

<p>In this instance I have many 1000s of documents which are each taken up by a pool of threads (i.e. 1 thread has one document). </p>

<p>Edit FYI - Example usage pattern:</p>

<pre><code>import org.tartarus.snowball.ext.PorterStemmer;
...
private String stem(String word){
       PorterStemmer stem = new PorterStemmer();
       stem.setCurrent(word);
       stem.stem();
       return stem.getCurrent();
    }
</code></pre>

<p>Cheers!</p>
","java, multithreading, lucene, solr, nlp","<p>Looking at the docs, it seems the <a href=""http://lucene.apache.org/java/3_4_0/api/all/org/tartarus/snowball/ext/PorterStemmer.html"" rel=""nofollow""><code>PorterStemmer</code></a> class is not re-entrant, so I'd build an instance per thread if I were you. If stemming is one of the main things your program does, and it has no other way of keeping your CPU cores busy, then a synchronized block seems like a bad idea: the program would be blocking all the time, waiting for the stemmer to finish one document. I wouldn't create a single thread per document, either; a thread pool with one thread per core might be a wiser choice.</p>

<p>(No example code since I couldn't even figure out the usage from the API docs. RTFS to find out how this thing works...)</p>
",1504,1316512889
Java Open Source Text Mining Frameworks,"<p>I want to know what is the best open source Java based framework for Text Mining, to use botg Machine Learning and dictionary Methods.</p>

<p>I'm using Mallet but there are not that much documentation and I do not know if it will fit all my requirements.</p>
","java, frameworks, machine-learning, nlp, information-retrieval",,26829,1266689139
Suggestions for obtaining Google search results and cleaning HTML tags,"<p>I am working on a project to get Google search web pages and then clean HTML tags to obtain pure text content.</p>

<p>Any suggestion for available tools (esp. Python tools)</p>

<p>many thanks.</p>
","python, html, nlp","<p>Finally found a nice suite <a href=""http://bootcat.sslmit.unibo.it/"" rel=""nofollow"">BootCat</a>.</p>
",434,1299274610
How to classify text when pre defined categories are not available,"<p>I have a problem and not getting idea which algorithm have to apply. 
I am thinking to apply clustering  in case two but no idea on case one: </p>

<p>I have .5 million credit card activity documents. Each document is well defined and contains 1 transaction per line. The date, the amount, the retailer name, and a short 5-20 word description of the retailer. 
Sample:
2004-11-47,$500,Amazon,An online retailer providing goods and services including books, hardware, music, etc.
Questions:
1. How would classify each entry given no pre defined categories.
2. How would do this if you were given pre defined categories such as ""restaurant"", ""entertainment"", etc.</p>
","nlp, data-mining, text-processing","<p>1) How would classify each entry given no pre defined categories.</p>

<p>You wouldn't. Instead, you'd use some dimensionality reduction algorithm on the data's features to them in 2-d, make a guess at the number of ""natural"" clusters, then run a clustering algorithm.</p>

<p>2) How would do this if you were given pre defined categories such as ""restaurant"", ""entertainment"", etc.</p>

<p>You'd manually label a bunch of them, then train a classifier on that and see how well it works with the usual machinery of accuracy/F1, cross validation, etc. Or you'd check whether a clustering algorithm picks up these categories well, but then you still need some labeled data.</p>
",204,1316005634
Hundreds of RegEx on one string,"<p>In followup to my previous question
<a href=""https://stackoverflow.com/questions/7022379/hundreds-of-regex-on-one-string"">Hundreds of RegEx on one string</a>
I ended up with a regex like following</p>

<pre><code>(section1:|section2:|section3:|section[s]?4:|(special section:|it has:|synonyms:)).*?(?=section1:|section2:|section3:|section[s]?4:|(special section:|it has:|synonyms:)|$)
</code></pre>

<p><a href=""https://i.sstatic.net/n8yPm.png"" rel=""nofollow noreferrer"">section section in regex search</a></p>

<p>The regex that I have in my prod system has more then 1000 characters and is multiple lines long. All it does is chunking sections from big piece of text and then again these sections are individually processed to extract information. Also I want these section titles to be natural language tolerant that's why some sections can be typed in multiple ways resulting in increased size of the regex. Is there a better way of doing this in terms of performance and manageability? </p>
","java, regex, nlp, machine-learning","<ol>
<li><p>For dealing with performance in such regexp you can use prefix optimisation <a href=""https://code.google.com/p/graph-expression/wiki/RegexpOptimization"" rel=""nofollow"">https://code.google.com/p/graph-expression/wiki/RegexpOptimization</a></p></li>
<li><p>This framework allow you to write typechecked regexp with Java DSL. So it became refactorable and maintainable. <a href=""https://code.google.com/p/graph-expression/"" rel=""nofollow"">https://code.google.com/p/graph-expression/</a></p></li>
</ol>
",233,1315980917
getting into sentiment analysis,"<p>I've got a requirement of determining whether the entered sentence is positive or negative.... First I thought it is something to do with Social Network analysis and later I realised that it is Sentiment analysis. My first question is what is the difference between these two? I think SNA itself uses SA... plz correct me if i am wrong...</p>

<p>Regarding this I got a very good discussion by Alexander @ <a href=""https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence"">NLP: Qualitatively &quot;positive&quot; vs &quot;negative&quot; sentence</a>...</p>

<p>I want to get into this field with the power of open source and preferably with Java (but I am open to others too). Can anyone pls guide me on how to get started and move ahead</p>

<p>Thanks in advance 
Siva</p>
","java, nlp, sa","<p>This is sentiment analysis.  </p>

<p>Social Network Analysis doesn't really have anything to do with Sentiment Analysis - social network analysis deals with relationships between people or things - common problems deal with figuring out ""clustering"" or cliques within a social network, discovering group cohesion, prestige within groups, discovering ""ringleaders"", etc.  </p>

<p>Sentiment analysis is much closer to natural language processing - taking some textual, audio or video content and attempting to classify it in some way - subjective/objective, agreement/disagreement, etc.</p>

<p>As for tools/APIs that support this - I googled and found this blog post <a href=""http://lordpimpington.com/codespeaks/drupal-5.1/?q=node/5"" rel=""noreferrer"">listing several sentiment analysis and natural language processing tools</a>.</p>
",4009,1248704838
When would we extract verb phrases from a text?,"<p>I have come across plenty of material on extracting noun phrases from text. Noun phrases were defined as adjacent NN/NNS/NNP/NNPS modified by an optional JJ. It is easy to note that noun phrases are extracted to get a sense of what the text is all about and to may be generate a tag/cloud of words, or to display the distribution of noun phrases for a text corpus.</p>

<p>On the otherhand, what are the scenarios when a verb phrase would need to be extracted? What business problems exists that necessitate for the extraction of verb phrases?</p>

<p>Thanks
Abhishek S</p>
","nlp, machine-learning","<p>On case is to extract Predicates, in most SOV langauges it is sequence of NOUN VERB NOUN. Predicate is more descriptive then pure nouns and can be used for sentiments, </p>
",924,1315976453
How to Join Arabic letters to form words,"<p>I have to read arabic letters from xml file and display them as a word </p>

<p>input :س ع ا د ة
output :سعادة look like that ..</p>

<p>I dont know how do that in any language , what algorithm to read, I need some start point to acomplish this task </p>

<p>I am also not sure if i have added the right tags, please free to make changes.</p>
","algorithm, nlp, arabic","<p>For ref: <a href=""http://en.wikipedia.org/wiki/Arabic_alphabet"" rel=""nofollow"">http://en.wikipedia.org/wiki/Arabic_alphabet</a> and <a href=""http://en.wikipedia.org/wiki/Arabic_characters_in_Unicode"" rel=""nofollow"">http://en.wikipedia.org/wiki/Arabic_characters_in_Unicode</a></p>

<p>Firstly, I don't know much about Arabic word forms and I just read the Wikipedia doc on it (linked above).  Thanks for giving me a reason to read up on it but forgive me if I totally mess this up :-)... </p>

<p>The problem seems to be mapping the char to it's correct ""case"" based on it's position in the word, right?  I'm basing this on the changes you showed in your examples.  Anyway, in English this would be like upper-casing the the first letter.  In Arabic it appears there are 4 char cases( Beginning, Middle, End and Isolated ).  If this is correct here is an example in C# which does this mapping:</p>

<pre><code>class ArabicMapper
{ 
    enum CaseMap{End=0, Middle=1, Beginning=2, Isolated=3};
    Dictionary&lt;char, char[]&gt; charMap; // This maps base letters to one of their four cases.
    public ArabicMapper()
    {
        //Create the char map for each letter in the alphabet. {BaseLetter, {End, Middle, Beginning, Isolated}}
        charMap = new Dictionary&lt;char, char[]&gt;();
        charMap.Add(0627, new char[] { FE8D, 0627, 0627, FE8E }); // ʾalif : Not sure of the rules for middle/beginning, so just using the isolated...
        charMap.Add(0628, new char[] { FE90, FE92, FE91, FE8F }); // bāʾ :
        //... and so on for each char ...

    }
    public string charsToWord(char[] word)
    {

        if (word.Length &gt;= 2)
        {
            StringBuilder finalWord = new StringBuilder();

            for(int i=0; i&lt;word.Length; i++)
            {
                if (i == 0)
                    finalWord.Append((charMap[word[i]])[CaseMap.Beginning]);
                else if(i == word.Length-1)
                    finalWord.Append((charMap[word[i]])[CaseMap.End]);
                else
                    finalWord.Append((charMap[word[i]])[CaseMap.Middle]);
            }
            return finalWord.ToString();
        }
        else
        {
            (charMap[word[0]])[CaseMap.Isolated].ToString();
        }
    }
}
</code></pre>

<p>P.S. I didn't test this code so it may not work.  Treat it as pseudocode, please.</p>
",2388,1315730432
Bucketing sentences by mood,"<p>Let's start with a simple problem. Let's say that I have a 350 char sentence and would like to bucket the sentence into either a ""Good mood"" bucket or a ""Bad mood"" bucket. </p>

<p>What would be the best way to design an algorithm to bucket the sentence?</p>
","algorithm, nlp, sentiment-analysis, document-classification",,837,1311926458
Grammar production class implementation in C#,"<p>Grammar by definition contains productions, example of very simple grammar:</p>

<pre><code>E -&gt; E + E
E -&gt; n
</code></pre>

<p>I want to implement Grammar class in c#, but I'm not sure how to store productions, for example how to make difference between terminal and non-terminal symbol.
i was thinking about:</p>

<pre><code>struct Production
{
   String Left;       // for example E
   String Right;      // for example +
}
</code></pre>

<p>Left will always be non-terminal symbol (it's about context-free grammars)
But right side of production can contain terminal &amp; non-terminal symbols</p>

<p>So now I'm thinkig about 2 ways of implementation:</p>

<ol>
<li><p>Non-terminal symbols will be written using brackets, for example:</p>

<p>E+E will be represented as string ""[E]+[E]""</p></li>
<li><p>Create additional data structure NonTerminal</p>

<p>struct NonTerminal
{
  String Symbol;
}</p></li>
</ol>

<p>and E+E will be represented as array/list:</p>

<pre><code>[new NonTerminal(""E""), ""+"", new NonTerminal(""E"")]
</code></pre>

<p>but think that there are better ideas, it would be helpfull to hear some response</p>
","c#, grammar, nlp, context-free-grammar","<p>I'd use </p>

<pre><code> Dictionary&lt;NonTerminalSymbol,Set&lt;List&lt;Symbol&gt;&gt;&gt; 
</code></pre>

<p>enabling lookup by Nonterminal of the <em>set</em> of production rule right-hand-sides (themselves represented as lists of Terminal/Nonterminal Symbols) associated with the Nonterminal.  (OP's question shows that the Nonterminal E might be associated with <em>two</em> rules, but we only need the right-hand sides if we have the left hand side).</p>

<p>This representation works only for a vanilla BNF grammar definitions, in which there is no syntactic sugar for common grammar-defining idioms.  Such idioms typically include <em>choice</em>, <em>Kleene star/plus</em>, ... and when they are avialable in defining the grammar you get an so-called Extended BNF or EBNF.  If we write EBNF only allowing <em>choice</em> denoted by <strong>|</strong>, the Expression grammar in flat form hinted at by OP as an example is:</p>

<pre><code>         E = S ;
         S = P | S + P | S - P ; 
         P = T | P * T | P / T ;
         T = T ** M | ( E ) | Number | ID ;
</code></pre>

<p>and my first suggestion <em>can</em> represent this, because the alternation is only used to show different rule right-hand-sides.  However, it won't represent this:</p>

<pre><code>         E = S ;
         S = P A* ;
         A = + P | - P ;
         P = T M+ ; -- to be different
         M = * T | / T ;
         T = T ** M | ( E ) | Number | ID | ID ( E  ( # | C) * ) ; -- function call with skipped parameters
         C = , E ;
</code></pre>

<p>The key problem that this additional notation introduces is the ability to compose the WBNF operators repeatedly on sub-syntax definitions, and that's the whole point of EBNF. </p>

<p>To represent EBNF, you have to store productions essentially as <em>trees</em> that represent the, well, expression structure of the EBNF (in fact, this is essentially the same problem as representing any expression grammar).</p>

<p>To represent the EBNF (expression) tree, you need to define the tree structure of the EBNF. 
You need tree nodes for:</p>

<ul>
<li>symbols (terminal or not)</li>
<li>Alternation (having a list of alternatives)</li>
<li>Kleene *</li>
<li>Kleene +</li>
<li>""Optional"" ?</li>
<li>others that you decide your EBNF has as operators (e.g., comma'd lists, a way to say that one has a list of grammar elements seperated by a chosen ""comma"" character, or ended by a chosen ""semicolon"" character, ...)</li>
</ul>

<p>The easiest way to do that is to first write an EBNF grammar for the EBNF itself:</p>

<pre><code>EBNF = RULE+ ;
RULE = LHS ""="" TERM* "";"" ;
TERM = STRING | SYMBOL | TERM ""*"" 
       | TERM ""+"" | ';' STRING TERM | "","" TERM STRING 
      ""("" TERM* "")"" ;
</code></pre>

<p>Note that I've added comma'd and semicolon'ed list to the EBNF (extended, remember?)</p>

<p>Now we can simply inspect the EBNF to decide what is needed.
What you now need is a set of records (OK, classes for C#'er) to represent each of these rules.
So:</p>

<ul>
<li>a class for EBNF that contains a set of rules</li>
<li>a class for a RULE having an LHS symbol and a LIST</li>
<li>an abstract base class for TERM with several concrete variants, one for each alternative of TERM (a so-called ""discriminated union"" typically implemented by inheritance and instance_of checks in an OO language).</li>
</ul>

<p>Note that some of the concrete variants can refer to other class types in the representation, which is how you get a tree.  For instance:</p>

<pre><code>   KleeneStar inherits_from TERM {
        T: TERM:
   }
</code></pre>

<p>Details left to the reader for encoding the rest.</p>

<p>This raises an <em>unstated</em> problem for the OP:  how do you <em>use</em> this grammmar representation to drive parsing of strings?</p>

<p>The simple answer is <em>get a parser generator</em>, which means you need to figure out what EBNF <em>it</em> uses. (In this case, it might simply be easier to store your EBNF as text and hand it to that parser generator, which kind of makes this whole discussion moot).</p>

<p>If you can't get one (?), or want to build one of your own, well, now you have the representation you need to climb over to build it.   One other alternative is to build a recursive descent parser driven by this representation to do your parsing.  The approach to do that is too large to contain in the margin of this answer, but is straightforward for those with experience with recursion.</p>

<p>EDIT 10/22:  OP clarifies that he insists on parsing <em>all context free grammars</em> and ""especially NL"".   For all context free grammars, he will need very a stong parsing engine (Earley, GLR, full backtracking, ...).   For Natural Language, he will need parsers much stronger than those; people have been trying to build such parsers for decades with only some, but definitely not easy, success.   Either of these two requirements seems to make the discussion of representing the grammar rather pointless; if he does represent a straight context free grammar, it won't parse natural language (proven by those guys trying for decades), and if he wants a more powerful NL parser, he'll need to simply use what the bleeding edge types have produced.   Count me a pessimist on his probable success, unless he decides to become a real expert in the area of NL parsing.</p>
",3642,1287660980
Autocorrect spelling mistakes in text input,"<p>I am writing a natural language processor in C# that extracts the sentiment (positive/negative) of a sentence.  There is something of an issue, though, in being able to discern the sentiment of a misspelled word - if it's not in the dictionary, I can neither tag it nor rate it!</p>

<p>I know there has to be a way to handle this.  Google gives accurate suggestions all the time, I simply need to take the top suggestion from a similar algorithm and hit the database with it.  The problem is, I'm not sure where to start with algorithm names and so forth.  I need help figuring that out.</p>

<p>I checked around on the site for similar questions, and found some concepts that seemed useful, but the basic way of handling the distance between a misspelling and a real word basically relied on hitting every word in your data set, which seems horribly inefficient.  Some help with ideas to make the algorithm run quickly would also be much appreciated; this analysis engine is supposed to be able to handle multiple thousands of items a day.</p>

<p>Thanks in advance.</p>
","c#, algorithm, nlp, autocorrect","<p>This problem is not that stupid. Norvig wrote an <a href=""http://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">article</a> about it. Generally speaking the difficulty depends on the accuracy. The &quot;easiest&quot; way to do it is using a <em>prefix tree</em> or <em>trie</em> to avoid exploring all possibilities.
Basically you have something like this:</p>
<p><img src=""https://i.sstatic.net/9TzrF.png"" alt=""enter image description here"" /></p>
<p>and following the path you basically stay on track. Once you reach a point where you are stuck you should check how to move on based on the type of error you have.</p>
<p>You can read Norvig's article for a deeper analysis.</p>
",2836,1315236395
Problem training unigram tagger on nps_chat corpus,"<p>At first I tried training the tagger on tagged sentences, but unlike with 
Brown, the nps chat corpus doesn't seem to take the tagged_sents() method. 
So then I tried training on tagged words and Python returned the error 
message: </p>

<pre><code>&gt; Traceback (most recent call last):    File ""&lt;pyshell#55&gt;"", line 1, in
&gt; &lt;module&gt; 
&gt;     unigram_tagger = nltk.UnigramTagger(training_set)    File
&gt; ""C:\Python26\lib\site-packages\nltk\tag\sequential.py"", line 287, in 
&gt; __init__ 
&gt;     backoff, cutoff, verbose)    File
&gt; ""C:\Python26\lib\site-packages\nltk\tag\sequential.py"", line 270, in 
&gt; __init__ 
&gt;     self._train(train, cutoff, verbose)    File
&gt; ""C:\Python26\lib\site-packages\nltk\tag\sequential.py"", line 181, in 
&gt; _train 
&gt;     tokens, tags = zip(*sentence)  ValueError: need more than 1 value
&gt; to unpack
</code></pre>

<p>I suspect the issue has something to do with the fact that I'm trying to 
train the tagger on tagged words rather than sentences, but what's the 
solution to this if nps chat doesn't take the tagged_sents method? And why 
doesn't it accept that method?  Please advise. </p>
","nlp, nltk",,593,1315264355
"Pure statistical, or Natural Language Processing engine?","<p>What are the statistical engines that yield better results than the OpenNLP suite of tools, if any? What I'm looking for is an engine that picks keywords from texts and provides stemming on those verbs &amp; nouns, perhaps Natural Language Processing is not the way to go here. The engine should also work with different languages.</p>
","nlp, text-mining, opennlp, statistics","<p>LingPipe is probably worth a look as complete NLP tool.</p>

<p>However, if all you need to do is find verbs and nouns and stem them, then you could just
1) tokenize text
2) run a POS tagger
3) run a stemmer</p>

<p>The Stanford tools can do this for multiple languages I believe, and NLTK would be a quick way to try it out.</p>

<p>However, you want to be careful of just going after verbs and nouns- what do you do about noun phrases and multiword nouns? Ideally an nlp package can handle this, but a lot of it depends on the domain you are working in. Unfortunately a lot of NLP is how good your data is.</p>
",722,1310155170
Where can I find a large corpus of chess commentary?,"<p>I'm hoping to find a corpus of play-by-play style commentary* for an NLP project that involves predicting game outcomes from such commentary.</p>

<p>I can't shake the feeling that with the huge interest in chess within the A.I. community, there must've been some previous project involving using chess commentary for some similar purpose, but for I can't find one for the life of me.</p>

<p>I have found a few sites like <a href=""http://www.chessgames.com/index.html"" rel=""nofollow"">Chess Games</a> that claim to have written commentary for some of their games, but most don't and there appears to be no way to sort them by this property.</p>

<p>*By 'play-by-play style commentary' I mean anything involving the game at hand and nothing more. E.g. everything from ""Kasparov moved his queen to b3, taking Deep Blue's pawn"" to ""Kasparov's poor opening has left his Knight vulnerable"" but not things like ""Kasparov played a similar move back in his 1996 game"" or ""Kasparov's hair looks particularly pretty today"".</p>
","nlp, chess, corpus",,276,1314911764
Where can I find texts that describe topic-specific events? ,"<p>So, some background: I'm trying to train a ML system to answer questions about events, where both the event descriptions and questions are posed in natural language; the event descriptions are constrained to being single sentences.</p>

<p>So far the main problem with this has been locating a corpus that describes events with a limited enough vocabulary to pose similar questions across all of the events (e.g. if all of the events involved chess, I could reasonably ask 'what piece moved?' and an answer could be drawn from a decent percentage of the event description sentences).  </p>

<p>With that in mind, I'm hoping to find a text source that is tightly focused around describing events within some fairly limited topic (more along the lines of chess commentary than a chess forum, for example). </p>

<p>While I've had some luck with a corpus of <a href=""http://www.ldc.upenn.edu/Catalog/byType.jsp"" rel=""nofollow"">air-traffic controller dialogs</a>, most of sentences aren't typical English (they involve a lot of Charlie, Tango, etc.). However, if the format is as I've described then the actual topic of focus is irrelevant, so long as it has one.</p>

<p>Since I plan on building my own corpus out of this text, no tagging is necessary. </p>
","text, nlp, datasource, semantics, corpus","<p>The Reuters corpus has a fairly monotonous content (commercial news; CEO appointments, mergers and acquisitions, major deals, etc); I am more familiar with the multilingual v2 but IIRC the v1 corpus was monolingual English. These will be multiple-sentence news stories, but in keeping with journalistic conventions, you can expect the first sentence to form a reasonable gist of the full story. <a href=""http://about.reuters.com/researchandstandards/corpus/"" rel=""nofollow"">http://about.reuters.com/researchandstandards/corpus/</a></p>

<p>You might also look at other TREC and especially MUC competition materials; <a href=""http://en.wikipedia.org/wiki/Message_Understanding_Conference"" rel=""nofollow"">http://en.wikipedia.org/wiki/Message_Understanding_Conference</a></p>
",257,1314644567
c# tools for named entities recognizer,"<p>I am looking for a simple but ""good enough"" Named Entity Recognition tool (nlp tool) or library for C#, I am looking to process name entities in  medical domains.</p>

<p>Any recommendations?</p>

<p>Thanks.</p>
","c#, nlp",,1769,1314474637
Javascript Verbs Detection,"<p>I've the following problem. I need to find <strong>verbs</strong> in a string using <code>JavaScript</code>.
I would like to know, if there is something like (JAWS), the Java API for <code>Wordnet</code>, but for JavaScript.</p>

<p>More specifically, i'm searching for some kind of RESTful webservice able to return the list of verbs in a text or webpage.</p>

<p>If you know something useful, please don't hesitate to post your answers.</p>
","javascript, nlp","<p>Have a look at the <a href=""http://text-processing.com/demo/"" rel=""noreferrer"">Natural Language Processing APIs</a>. They seem to have an <a href=""http://text-processing.com/docs/"" rel=""noreferrer"">API</a>. There is even a <a href=""http://www.mashape.com/apis/Text-Processing"" rel=""noreferrer"">service</a> for larger amounts of requests (has a free plan as well).</p>

<p>It is based on simple POST requests and returns JSON, so it would be easy to use in JavaScript.</p>
",3481,1314564595
Building/Running a Streaming Weka Text Classifer in Java,"<p>We have been using the Weka Explorer GUI to build a few classifier models. Now Testing is complete we would like to implement this model within a Java application so it can take new messages.</p>

<p>So for new messages we need to tokenize the message, match up tokens in the message with tokens used to build the word vector for the model and then parse this word vector to the model. </p>

<p>How should we go about this process? Are there any examples available? </p>

<p>How do we deal with new tokens (i.e. words that appear in new text messages which are not a part of the word vector used to build the model)?</p>

<p>For the classifier preprocessing/tokenising we are using the NGram Tokenizer, Stemmer and IDF Transform. So we need to figure out how to do these steps before we can create a new instancebased on the text we would like to classify. </p>

<p>As a side When building a classifier in the explorer, under more options there is a button to choose 'output classifier code' which sounds like it outputs Java source code to build and use the model however this option is disabled. Tested with a number of different classifiers (RF, NB) and it doesnt change. I'm guessing its not implemented for these?</p>

<p>Cheers!</p>
","java, nlp, machine-learning, classification, weka","<p>To my best knowledge you need to retrain weka classifier when a new training sample arrives. I am not aware of an online classification algorithm in Wekka.</p>

<p>ps. Weka is Java based, so you can use its libs in your application. Here is a good example: <a href=""http://weka.wikispaces.com/Use+WEKA+in+your+Java+code"" rel=""nofollow"">http://weka.wikispaces.com/Use+WEKA+in+your+Java+code</a>. </p>
",1426,1314429962
Need resources for Statistical Natural Language Processing,"<p>I'm writing a program in Java that needs to parse natural language. I need this to be done using probability and statistics. Are there any resources that can easily explain Statistical Natural Language Processing techniques?</p>
","java, statistics, nlp",,277,1314138786
Applying SVD throws a Memory Error instantaneously?,"<p>I am trying to apply SVD on my matrix (3241 x 12596) that was obtained after some text processing (with the ultimate goal of performing Latent Semantic Analysis) and I am unable to understand why this is happening as my 64-bit machine has 16GB RAM. The moment <code>svd(self.A)</code> is called, it throws an error. The precise error is given below:</p>

<pre><code>Traceback (most recent call last):
  File "".\SVD.py"", line 985, in &lt;module&gt;
    _svd.calc()
  File "".\SVD.py"", line 534, in calc
    self.U, self.S, self.Vt = svd(self.A)
  File ""C:\Python26\lib\site-packages\scipy\linalg\decomp_svd.py"", line 81, in svd
    overwrite_a = overwrite_a)
MemoryError
</code></pre>

<p>So I tried using</p>

<pre><code>self.U, self.S, self.Vt = svd(self.A, full_matrices= False)
</code></pre>

<p>and this time, it throws the following error:</p>

<pre><code>Traceback (most recent call last):
  File "".\SVD.py"", line 985, in &lt;module&gt;
    _svd.calc()
  File "".\SVD.py"", line 534, in calc
    self.U, self.S, self.Vt = svd(self.A, full_matrices= False)
  File ""C:\Python26\lib\site-packages\scipy\linalg\decomp_svd.py"", line 71, in svd
    return numpy.linalg.svd(a, full_matrices=0, compute_uv=compute_uv)
  File ""C:\Python26\lib\site-packages\numpy\linalg\linalg.py"", line 1317, in svd
    work = zeros((lwork,), t)
MemoryError
</code></pre>

<p>Is this supposed to be such a large matrix that Numpy cannot handle and is there something that I can do at this stage without changing the methodology itself?</p>
","python, memory, numpy, nlp, scipy","<p>Apparently, as it turns out, thanks to @Ferdinand Beyer, I did not notice that I was using a 32-bit version of Python on my 64-bit machine. </p>

<p>Using a 64-bit version of Python and reinstalling all the libraries solved the problem.</p>
",7668,1313993326
How to start to build a corpus for NLP research,"<p>I am trying to build an NLP corpus for an under resource language, as there is no data available for the purpose of NLP research. Can any one suggest, how to build or proceed to make it a standard NLP corpus, any standard method or paper or link.
Thanks in advance</p>
",nlp,,1396,1313768589
Intelligent transliteration in PHP,"<p>I'm interested in writing a PHP script (I do welcome language-agnostic suggestions) that would transliterate a sentence or word written in English (phoenetically) into the script of another language. Since I'm looking at English written phoenetically (i.e. by ear): I'd have to deal with variant spellings of the same word.</p>

<p>It is assumed that no standard exists for romanization (for instance, in Chinese, you have the Simplified Wade, etc.) </p>

<p>Does anyone have any advice on where I could start? </p>

<p>EDIT: I'm doing this purely for educational purposes, and I was initially under the impression that in order to figure out the connection between variant spellings (which could be found in a corpus of IM messages, Facebook posts written in the romanized form of the language), you'd need some sort of machine learning tool. However, I'd like to know if I was on the right track, and I'd like some help in figuring out what next I should look into to get this working (for instance: which machine learning tool should I look into?). </p>
","php, nlp","<p>I know with Japanese at least, you have a set number of letter combinations. </p>

<p>So, you could do something like create a matching array like this</p>

<pre><code>array(
  'oo' =&gt; 'おう',
  'oh' =&gt; 'おう',
  'ou' =&gt; 'おう'
)
</code></pre>

<p>Of course, continuing on, and making sure you don't match 'su', when it should be 'tsu'.</p>

<p>This would only be a starting point, of course.</p>

<p>Machine learning is probably most practical with Chinese...but here's a rough start to hiragana: <a href=""https://gist.github.com/1154969"" rel=""nofollow"">https://gist.github.com/1154969</a></p>
",1021,1313506684
Comparing two English strings for similarities,"<p>So here is my problem. I have two paragraphs of text and I need to see if they are similar. Not in the sense of string metrics but in meaning. The following two paragraphs are related but I need to find out if they cover the 'same' topic. Any help or direction to solving this problem would be greatly appreciated. </p>

<blockquote>
  <p>Fossil fuels are fuels formed by natural processes such as anaerobic
  decomposition of buried dead organisms. The age of the organisms and
  their resulting fossil fuels is typically millions of years, and
  sometimes exceeds 650 million years. The fossil fuels, which contain
  high percentages of carbon, include coal, petroleum, and natural gas.
  Fossil fuels range from volatile materials with low carbon:hydrogen
  ratios like methane, to liquid petroleum to nonvolatile materials
  composed of almost pure carbon, like anthracite coal. Methane can be
  found in hydrocarbon fields, alone, associated with oil, or in the
  form of methane clathrates. It is generally accepted that they formed
  from the fossilized remains of dead plants by exposure to heat and
  pressure in the Earth's crust over millions of years. This biogenic
  theory was first introduced by Georg Agricola in 1556 and later by
  Mikhail Lomonosov in the 18th century.</p>
</blockquote>

<p>Second:</p>

<blockquote>
  <p>Fossil fuel reforming is a method of producing hydrogen or other
  useful products from fossil fuels such as natural gas. This is
  achieved in a processing device called a reformer which reacts steam
  at high temperature with the fossil fuel. The steam methane reformer
  is widely used in industry to make hydrogen. There is also interest in
  the development of much smaller units based on similar technology to
  produce hydrogen as a feedstock for fuel cells. Small-scale steam
  reforming units to supply fuel cells are currently the subject of
  research and development, typically involving the reforming of
  methanol or natural gas but other fuels are also being considered such
  as propane, gasoline, autogas, diesel fuel, and ethanol.</p>
</blockquote>
","algorithm, text, comparison, nlp, compare",,349,1313540850
Parsing Natural Language Music Citations Using Regex,"<p>I am struggling with nailing down a fairly complex regular expression to parse song titles with optional artist attribution from loosely-typed English. The user input comes from a single text field and the regex matches will be used to query a song database to get unique track IDs. I need to be able to get these matches:</p>
<ul>
<li><code>\1</code> = song title</li>
<li><code>\2</code> = artist</li>
</ul>
<p>while being fairly liberal in allowed formats.</p>
<h2>Examples</h2>
<p>The wold &quot;by&quot; should split the string into song title and artist (but only on word boundaries); as should a comma with/without trailing whitespace:</p>
<blockquote>
<p>baby one more time by britney spears</p>
<p>baby one more time, britney spears</p>
<p>baby one more time,britney spears</p>
</blockquote>
<ul>
<li><code>\1</code> = baby one more time</li>
<li><code>\2</code> = britney spears</li>
</ul>
<p>False positives like these are acceptable:</p>
<blockquote>
<p>down by the bay</p>
</blockquote>
<ul>
<li><code>\1</code> = down</li>
<li><code>\2</code> = the bay</li>
</ul>
<blockquote>
<p>whatever people say i am, that's what i'm not</p>
</blockquote>
<ul>
<li><code>\1</code> = whatever people say i am</li>
<li><code>\2</code> = that's what i'm not</li>
</ul>
<p>…assuming quotes can be used to mark a run of text as a song title explicitly:</p>
<blockquote>
<p>&quot;down by the bay&quot;</p>
</blockquote>
<ul>
<li><code>\1</code> = down by the bay</li>
<li><code>\2</code> not matched</li>
</ul>
<blockquote>
<p>&quot;whatever people say i am, that's what i'm not&quot; by arctic monkeys</p>
</blockquote>
<ul>
<li><code>\1</code> = whatever people say i am, that's what i'm not</li>
<li><code>\2</code> = arctic monkeys</li>
</ul>
<p>Single quotes should work too, but obviously not if they appear within the title:</p>
<blockquote>
<p>'whatever people say i am, that's what i'm not'</p>
</blockquote>
<ul>
<li><code>\1</code> = whatever people say i am, that</li>
<li><code>\2</code> = s what i'm not'</li>
</ul>
<p>Additionally, if quotes are in use, the word &quot;by&quot; or a comma are optional:</p>
<blockquote>
<p>&quot;down by the bay&quot; raffi</p>
</blockquote>
<ul>
<li><code>\1</code> = down by the bay</li>
<li><code>\2</code> = raffi</li>
</ul>
<p>However, if there are no quotes, and more than one &quot;by&quot;, then only the last &quot;by&quot; should be used as a delimiter:</p>
<blockquote>
<p>down by the bay by raffi</p>
</blockquote>
<ul>
<li><code>\1</code> = down by the bay</li>
<li><code>\2</code> = raffi</li>
</ul>
<p>Is this even possible with a single regex? Or would the more sane way be to split it up into multiple expressions? Either way, what might this look like?</p>
","regex, nlp","<p>Here is an example, using C#:</p>

<pre><code>var regex = @""^((""""(?&lt;title&gt;[^""""]+)""""|'(?&lt;title&gt;[^']+)')(\s*,\s*|\s+by\s+)?|(?&lt;title&gt;.*)(\s*,\s*|\s+by\s+))\s*(?&lt;artist&gt;.*)$"";

var items = new []{
    ""baby one more time by britney spears"",
    ""baby one more time, britney spears"",
    ""baby one more time,britney spears"",
    ""down by the bay"",
    ""whatever people say i am, that's what i'm not"",
    ""\""down by the bay\"""",
    ""\""whatever people say i am, that's what i'm not\"" by arctic monkeys"",
    ""'whatever people say i am, that's what i'm not'"",
    ""\""down by the bay\"" raffi"",
    ""down by the bay by raffi"",
};

foreach (var item in items)
{
    var match = Regex.Match(item, regex, RegexOptions.ExplicitCapture);
    Console.WriteLine(match.Groups[""title""] + "" - "" + match.Groups[""artist""]);
}
</code></pre>

<p>Output matches your specification, as far as I can tell:</p>

<pre><code>baby one more time - britney spears
baby one more time - britney spears
baby one more time - britney spears
down - the bay
whatever people say i am - that's what i'm not
down by the bay - 
whatever people say i am, that's what i'm not - arctic monkeys
whatever people say i am, that - s what i'm not'
down by the bay - raffi
down by the bay - raffi
</code></pre>

<p>You can actually make it better for the single-quote case by allowing apostrophes inside words:</p>

<pre><code>var regex = @""^((""""(?&lt;title&gt;[^""""]+)""""|'(?&lt;title&gt;([^']|(?&lt;=\w)'(?=\w))+)')(\s*,\s*|\s+by\s+)?|(?&lt;title&gt;.*)(\s*,\s*|\s+by\s+))\s*(?&lt;artist&gt;.*)$"";
</code></pre>

<p>Which fixes this case:</p>

<pre><code>whatever people say i am, that's what i'm not - 
</code></pre>

<p>Here's a commented version of the regex, which explains what each part does (should be matched with <code>RegexOptions.ExplicitCapture|RegexOptions.IgnorePatternWhitespace</code>):</p>

<pre><code>var regex = @""
^
  (
    (
      """"(?&lt;title&gt;[^""""]+)""""               (?# matches a double-quote string )
    | '(?&lt;title&gt;([^']|(?&lt;=\w)'(?=\w))+)' (?# matches a single-quote string, allowing quotes in words )
    ) (\s*,\s*|\s+by\s+)?   (?# optionally follow these by ',' or 'by' )
  | 
  (?&lt;title&gt;.*)(\s*,\s*|\s+by\s+) (?# otherwise, everything up to ',' or 'by' )
)
\s*(?&lt;artist&gt;.*) (?# everything after this is the artist name )
$"";
</code></pre>

<p><em>Edit:</em></p>

<p>I've played around a bit with the PHP code, but I can't get it to use named capturing groups properly. Here is a version using unnamed capturing groups:</p>

<pre><code>$regex = ""/^(?:(?:\""([^\""]+)\""|'((?:[^']|(?&lt;=\\w)'(?=\\w))+)')(?:\\s*,\\s*|\\s+by\\s+)?|(.*)(?:\\s*,\\s*|\\s+by\\s+))\s*(.*)\$/"";

preg_match($regex, '""down by the river""', $matches);

print_r($matches);
</code></pre>

<p>The title will be in group 1, 2, or 3, and the artist in group 4.</p>
",427,1313457984
Final Semester project about semantic analysis/information retrieval,"<p>I'm moving to my final year at college Engineering Computer Science  department and i wanted to have my graduation project in a topic related to information retrieval &amp; semantic analysis.</p>

<p>I've had my internship in those topics and i'm very interested to continue in them 
so if you could please give me some examples of good Real projects to do and practice in order to be good in those fields </p>

<p>I want something beyond simple recommendation systems in websites </p>

<p>my related backGround : 
i worked before on making classifiers using wikipedia data 
i have a fair knowledge working with Huge datasources like dbpedia , probase , freebase ..etc 
i know fair knowledge about , NLP , classifiers , semantics , RDF , sentiment analysis 
i have a good web development knowledge
i have a good knowledge about scrapping data from facebook blogs and other websites </p>
","artificial-intelligence, nlp, machine-learning, semantics, nltk",,791,1312830145
There is a entity recognizer classifier algorithm that doesn&#39;t needs entire texts for training data?,"<p>I want to recognize some entities on texts that I have and I found a lot of algorithms (NaiveBayes, Hidden Markov Models, Conditional Random Field, etc.), but seems that almost all needs a huge training data to classify the entities.</p>

<p>I want to know if there is some algorithm that can recognize without having texts in training data, but maybe only words representing the data I want to recognize, or maybe some String Patterns, or another way. </p>

<p>The only thing I want to avoid is the necessity of having huge text as training data.</p>
","java, algorithm, nlp, named-entity-recognition, training-data",,332,1312951314
Full-text personalized search product,"<p>What full-text search technology is out there to support full-text <em>personalized</em> search?</p>

<p>For instance, contact search in your webmail provider of choice: it's full text but only searches your personal contacts and not the entire universe of contacts.</p>

<p>There are countless full-text search packages out there but I don't know how you could use most full-text search packages such that every user only sees a small subset of the universe of documents.</p>
","search, full-text-search, nlp, information-retrieval",,119,1312826128
wordnet database editor,"<p>I downloaded the latest Wordnet version for Windows (2.1) and I want to establish a link between two verb synsets. Is there a wordnet synset editor out there I can use for this? They published documentation that describes their file structure, but writing an editor of my own is not something I want to be dedicating time towards.</p>

<p>Thanks</p>

<p>mj</p>
","nlp, wordnet",,538,1312854608
Can I use NLTK to determine if a comment is a positive one or a negative one?,"<p>Can you show me a simple example using <a href=""http://www.nltk.org/code"" rel=""noreferrer"">http://www.nltk.org/code</a> to determine if a string about a happy or upset mood?</p>
","nlp, nltk",,5816,1281565515
Named Entity Recognition from personal Gazetter using Python,"<p>I try to do named entity recognition in python using NLTK.
I want to extract personal list of skills.
I have the list of skills and would like to search them in requisition and tag the skills.
I noticed that NLTK has NER tag for predefine tags like Person, Location etc.
Is there a external gazetter tagger in Python I can use?
any idea how to do it more sophisticated than search of terms ( sometimes multi words term )?</p>

<p>Thanks,
Assaf</p>
","python, nlp, nltk, named-entity-recognition",,2337,1298412474
Reconstructing now-famous 17-year-old&#39;s Markov-chain-based information-retrieval algorithm &quot;Apodora&quot;,"<p>While we were all twiddling our thumbs, a 17-year-old Canadian boy has apparently found an information retrieval algorithm that: </p>

<p>a) performs with twice the precision of the current, and widely-used vector space model</p>

<p>b) is 'fairly accurate' at identifying similar words. </p>

<p>c) makes microsearch more accurate</p>

<p>Here is a good <a href=""http://www.theglobeandmail.com/news/technology/science-fair-gold-medalist-17-invents-better-way-to-search-internet/article2118962/"" rel=""noreferrer"">interview</a>.</p>

<p>Unfortunately, there's no published paper I can find yet, but, from the snatches I remember from the graphical models and machine learning classes I took a few years ago, I think we should be able to reconstruct it from his submision abstract, and what he says about it in interviews.</p>

<p>From interview:</p>

<blockquote>
  <p>Some searches find words that appear in similar contexts. That’s
  pretty good, but that’s following the relationships to the first
  degree. My algorithm tries to follow connections further. Connections
  that are close are deemed more valuable. In theory, it follows
  connections to an infinite degree.</p>
</blockquote>

<p>And the abstract puts it in context:</p>

<blockquote>
  <p>A novel information retrieval algorithm called ""Apodora"" is introduced,
  using limiting powers of Markov chain-like matrices to determine
  models for the documents and making contextual statistical inferences
  about the semantics of words. The system is implemented and compared
  to the vector space model. Especially when the query is short, the
  novel algorithm gives results with approximately twice the precision
  and has interesting applications to microsearch.</p>
</blockquote>

<p>I feel like someone who knows about markov-chain-like matrices or information retrieval would immediately be able to realize what he's doing. </p>

<p>So: what is he doing?</p>
","nlp, machine-learning, information-retrieval, markov-chains","<p>From the use of words like 'context' and the fact that he's introduced a second order level of statistical dependency, I suspect he is doing something related to the LDA-HMM method outlined in the paper: Griffiths, T., Steyvers, M., Blei, D., &amp; Tenenbaum, J. (2005). Integrating topics and syntax. Advances in Neural Information Processing Systems.  There are some inherent limits to the resolution of the search due to model averaging. However, I'm envious of doing stuff like this at 17 and I hope to heck he's done something independent and at least incrementally better. Even a different direction on the same topic would be pretty cool.</p>
",712,1312644282
Word Net - Word Synonyms &amp; related word constructs - Java or Python,"<p>I am looking to use WordNet to look for a collection of like terms from a base set of terms. </p>

<p>For example, the word <strong><em>'discouraged'</em></strong> - potential synonyms could be: <code>daunted, glum, deterred, pessimistic</code>. </p>

<p>I also wanted to identify potential bi-grams such as; <code>beat down, put off, caved in</code> etc.</p>

<p>How do I go about extracting this information using Java or Python? Are there any hosted WordNet databases/web interfaces which would allow such querying?</p>

<p>Thanks!</p>
","java, python, nlp, text-mining, wordnet","<p>It is easiest to understand the WordNet data by looking 
at the Prolog files. They are documented here:</p>

<p><a href=""http://wordnet.princeton.edu/wordnet/man/prologdb.5WN.html"" rel=""nofollow"">http://wordnet.princeton.edu/wordnet/man/prologdb.5WN.html</a></p>

<p>WordNet terms are group into synsets. A synset is a maximal 
synonym set. Synsets have a primary key so that they can be used 
in semantic relationships. </p>

<p>So answering your first question, you can list the different 
senses and corresponding synonyms of a word as follows:</p>

<pre><code>Input X: Term
Output Y: Sense  
Output L: Synonyms in this Sense  

s_helper(X,Y) :- s(X,_,Y,_,_,_).  
?- setof(H,(s_helper(Y,X),s_helper(Y,H)),L).  
</code></pre>

<p>Example:</p>

<pre><code>?- setof(H,(s_helper(Y,'discouraged'),s_helper(Y,H),L).  
Y = 301664880,  
L = [demoralised, demoralized, discouraged, disheartened] ;  
Y = 301992418,  
L = [discouraged] ;  
No  
</code></pre>

<p>For the second part of your question, WordNet terms are 
sequences of words. So you can search this WordNet terms 
for words as follows:</p>

<pre><code>Input X: Word  
Output Y: Term

s_helper(X) :- s(_,_,X,_,_,_).  
word_in_term(X,Y) :- atom_concat(X,' ',H), sub_atom(Y,0,_,_,H).
word_in_term(X,Y) :- atom_concat(' ',X,H), atom_concat(H,' ',J), sub_atom(Y,_,_,_,J).
word_in_term(X,Y) :- atom_concat(' ',X,H), sub_atom(Y,_,_,0,H).
?- s_helper(Y), word_in_term(X,Y).
</code></pre>

<p>Example:</p>

<pre><code>?- s_helper(X), word_in_term('beat',X).  
X = 'beat generation' ;  
X = 'beat in' ;  
X = 'beat about' ;  
X = 'beat around the bush' ;  
X = 'beat out' ;  
X = 'beat up' ;  
X = 'beat up' ;  
X = 'beat back' ;  
X = 'beat out' ;  
X = 'beat down' ;  
X = 'beat a retreat' ;  
X = 'beat down' ;  
X = 'beat down' ;  
No
</code></pre>

<p>This would give you potential n-grams, but no so much
morphological variation. WordNet does also exhibit some
lexical relations, which could be useful. </p>

<p>But both Prolog queries I have given are not very efficient.
The problem is the lack of some word indexing. A Java
implementation could of course implement something better.
Just imagine something along:</p>

<pre><code>class Synset {  
    static Hashtable&lt;Integer,Synset&gt; synset_access;  
    static Hashtable&lt;String,Vector&lt;Synset&gt;&gt; term_access;  
}
</code></pre>

<p>Some Prolog can do the same, by a indexing directive, it is
possible to instruct the Prolog system to index on multiple
arguments for a predicate.</p>

<p>Putting up a web service shouldn't be that difficult, either
in Java or Prolog. Many Prologs systems easily allow embedding
Prolog programs in web servers, and Java champions servlets.</p>

<p>A list of Prologs that support web servers can be found here:</p>

<p><a href=""http://en.wikipedia.org/wiki/Comparison_of_Prolog_implementations#Operating_system_and_Web-related_features"" rel=""nofollow"">http://en.wikipedia.org/wiki/Comparison_of_Prolog_implementations#Operating_system_and_Web-related_features</a></p>

<p>Best Regards</p>
",2634,1312816232
Find an efficient way to integrate different language libraries into one project using Python as the &quot;glue&quot;,"<p>I am about to get involved in a NLP-related project and I need to use various libraries. Some are in java, others in C/C++ (for tasks that require more speed) and finally some are in Python. I was thinking of using Python as the ""glue"" and create wrapper-classes for every task that I want to do that relies on a different language. In order to do that, the wrapper class, for example, would execute the java program and communicate with it using pipes. 
My questions are:</p>

<ol>
<li><p>Do you think that would work for cpu-demanding and highly repetitive tasks? Or would the overhead added by the pipe-communication be too heavy?</p></li>
<li><p>Is there any other (preferably simple) architecture that you would suggest?</p></li>
</ol>
","java, c++, python, nlp, wrapper",,404,1312684510
Using Markov models to convert all-caps to mixed-case and related problems,"<p>I've been thinking about using Markov techniques to restore missing information to natural language text.</p>

<ul>
<li>Restore all-caps text to mixed-case.</li>
<li>Restore accents / diacritics to languages which should have them but have been converted to plain ASCII.</li>
<li>Convert rough phonetic transcriptions back into native alphabets.</li>
</ul>

<p>That seems to be in order of least difficult to most difficult. Basically the problem is resolving ambiguities based on context.</p>

<p>I can use Wiktionary as a dictionary and Wikipedia as a corpus using n-grams and Hidden Markov Models to resolve the ambiguities.</p>

<p>Am I on the right track? Are there already some services, libraries, or tools for this sort of thing?</p>

<p><strong>Examples</strong></p>

<ul>
<li>GEORGE LOST HIS SIM CARD IN THE BUSH   ⇨   George lost his SIM card in the bush</li>
<li>tantot il rit a gorge deployee   ⇨   tantôt il rit à gorge déployée</li>
</ul>
","unicode, nlp, ambiguity, n-gram, markov-models",,288,1292897716
How to correct the user input (Kind of google &quot;did you mean?&quot;),"<p>I have the following requirement: -</p>

<p>I have many (say 1 million) values (names).
The user will type a search string.</p>

<p>I don't expect the user to spell the names correctly.</p>

<p>So, I want to make kind of Google ""Did you mean"". This will list all the possible values from my datastore. There is a similar but not same question <a href=""https://stackoverflow.com/questions/135777/a-stringtoken-parser-which-gives-google-search-style-did-you-mean-suggestions"">here</a>. This did not answer my question.</p>

<p>My question: -
1) I think it is not advisable to store those data in RDBMS. Because then I won't have filter on the SQL queries. And I have to do full table scan. So, in <strong>this situation how the data should be stored?</strong></p>

<p>2) The second question is the same as <a href=""https://stackoverflow.com/questions/135777/a-stringtoken-parser-which-gives-google-search-style-did-you-mean-suggestions"">this</a>. But, just for the completeness of my question: how do I search through the large data set?
Suppose, there is a name Franky in the dataset. 
If a user types as Phranky, how do I match the Franky? Do I have to loop through all the names?</p>

<p>I came across <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein Distance</a>, which will be a good technique to find the possible strings. But again, my question is do I have to operate on all 1 million values from my data store?</p>

<p>3) I know, Google does it by watching users behavior. But I want to do it without watching user behavior, i.e. by using, I don't know yet, say distance algorithms. Because the former method will require large volume of searches to start with!</p>

<p>4) As <a href=""https://stackoverflow.com/users/146077/kirk-broadhurst"">Kirk Broadhurst</a> pointed out in an answer <a href=""https://stackoverflow.com/questions/1284782/how-to-correct-the-user-input-kind-of-google-did-you-mean/1360275#1360275"">below</a>, there are two possible scenarios: -</p>

<ul>
<li>Users mistyping a word (an edit
distance algorithm)</li>
<li>Users not knowing a word and guessing
(a phonetic match algorithm)</li>
</ul>

<p>I am interested in both of these. They are really two separate things; e.g. Sean and Shawn sound the same but have an edit distance of 3 - too high to be considered a typo.</p>
","language-agnostic, nlp, spell-checking, information-retrieval, autosuggest","<p>The Soundex algorithm may help you out with this.</p>

<p><a href=""http://en.wikipedia.org/wiki/Soundex"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Soundex</a></p>

<p>You could pre-generate the soundex values for each name and store it in the database, then index that to avoid having to scan the table.</p>
",2833,1250442208
Should a Chunker find the head of a phrase?,"<p>My application requires that I point the head of a phrase (noum or verb). I have this kind of info in my Portuguese corpus:</p>

<p>Me pron-pers *B-NP <br/>
pergunto v-fin B-VP <br/>
sempre adv *B-ADVP <br/>
quem pron-indp *B-NP <br/>
podia v-fin B-VP <br/>
ter v-inf I-VP <br/>
sido v-pcp I-VP <br/>
aquele pron-det B-NP <br/>
jovem adj I-NP <br/>
alemão n *I-NP <br/>
. . O <br/></p>

<p>The syntax is similar to CONLL 2000, but the * marks the head of the phrase.
My question is: should a Chunker support head? Do you know any other corpus to train a Chunker that also includes head, or it is a particularity of mine?</p>

<p>-- edit --</p>

<p>I tried training the classifier and got good results: F1 score was 0.94 without head mark and 0.93 with it. I think it is OK. The problem is that the OpenNLP chunker API does not support this mark and gets confused while creating the spans. I changed the OpenNLP code to handle it and I was wondering if it is a good patch, but since it is not common I should not send the patch.</p>
","nlp, opennlp","<p>I've never seen a chunker that supports head-finding, so I can't help you with a corpus. What you might do, if you already have a chunker, is formulate a bunch of rules that designate the head after the chunker has found it, or train a classifier to do so. You can train it on your corpus and apply it on chunker output.</p>
",369,1312391861
"For Java, there is a tokenizator that is matches exactly what I want?","<p>I'm want to tokenize a text, but not separating only with whitespaces. </p>

<p>There some things like proper names that I want to set only one token (eg.: ""Renato Dinhani Conceição""). Another case: percentual (""60 %"") and not split into two tokens.</p>

<p>What I want to know if there is a Tokenizator from some libray that can provide high customization? If not, I will try to write my own, if there is some interface or practices to follow.</p>

<p>Not everything need to be universal recognition. Example: I don't need to reconigze chinese alphabet.</p>

<p>My application is a college application and it is mainly directed to portuguese language. Only some things like names, places and similars will be from another languages.</p>
","java, text, nlp, tokenize","<p>I would try to go about it not from a tokenization perspective, but from a rules perspective.  This will be the biggest challenge - creating a comprehensive rule set that will satisfy most of your cases.</p>

<ul>
<li>Define in human terms what are units that should not be split up based on whitespace.  The name example is one.</li>
<li>For each one of those exceptions to the whitespace split, create a set of rules for how to identify it.  For the name example: 2 or more consecutive capitalized words with or without language specific non-capitalized name words in between (like ""de"").</li>
<li>Implement each rule as its own class which can be called as you loop.</li>
<li>Split the entire string based on whitespace, and then loop it, keeping track of what token came before, and what is current, applying your rule classes for each token.</li>
</ul>

<p>Example for rule isName:</p>

<ul>
<li>Loop 1: <code>(eg.:</code> isName = false</li>
<li>Loop 2: <code>""Renato</code> isName = true</li>
<li>Loop 3: <code>Dinhani</code> isName = true</li>
<li>Loop 4: <code>Conceição"").</code> isName = true</li>
<li>Loop 5: <code>Another</code> isName = false</li>
</ul>

<p>Leaving you with: <code>(eg.:</code>, <code>""Renato Dinhani Conceição"").</code>, <code>Another</code></p>
",327,1311877466
A corpus with semantic role tags for an NLP application,"<p>So, I've constructed a NLP program that learns to extract a semantic event description from a sentence, but right now my training set is limited to sentences I've parsed into semantic event components my hand.</p>

<p>While this method does get the job done, its hardly a proper substitute for a large pre-parsed corpus of text. Unfortunately, all of my attempts at finding such a corpus have proven futile.</p>

<p>What I need specifically is a corpus that has tagged the semantic roles of each word (or group of words) in a sentence. Examples of roles I had in mind are things like:</p>

<ul>
<li>agent</li>
<li>action</li>
<li>patient</li>
<li>instrument</li>
<li>co-agent</li>
<li>co-patient</li>
<li>location</li>
<li>adverb</li>
</ul>

<p>If any more specifics are needed, feel free to ask, or refer to <a href=""http://psych.stanford.edu/~jlm/papers/StJohnMcC90.pdf"" rel=""nofollow"">this paper</a> that uses a toy corpa with the same constraints as mine.</p>
","nlp, semantic-markup, corpus, tagged-corpus","<p>The CoNLL Shared Task in 2005 was 'Semantic Role Labelling'. <a href=""http://www.lsi.upc.edu/~srlconll/spec.html"" rel=""nofollow"">This page</a> describes their corpus and what roles they labelled.</p>
",538,1312323259
Word Co-occurrence - find the co-occurence of a term in a set of n-grams,"<p>How would I go about writing a co-occurence class in something like Java that takes a file full of n-grams and calculates word co-occurence for a given input term. </p>

<p>Are there any librarys or packages which work with Lucene (indexes) or something like a map-reduce over the n-gram list in Hadoop..?</p>

<p>Thanks.</p>
","lucene, hadoop, nlp, weka","<p>Ok, so assuming you want to find the co-occurrence of two different words in a file of ngrams....</p>

<p>Here's pseudo code-ish Java:</p>

<pre><code>// Co-occurrence matrix
Hashmap&lt;String,HashMap&lt;String,Integer&gt;&gt; map = new HashMap();

// List of ngrams
ArrayList&lt;ArrayList&lt;String&gt;&gt; ngrams = ..... // assume we've loaded them into here already

// build the matrix
for(ArrayList&lt;String&gt; ngram:ngrams){
  // Calculate word co-occurrence in ngram for all words
  // result is an map strings-&gt; count
  // words in alphabetical order
  Hashmap&lt;String,&lt;ArrayList&lt;String&gt;,Integer&gt; wordCoocurrence = cooccurrence(ngram) // assume we have this

  // then just join this with original
}

// and just query with words in alphabetic order
</code></pre>

<p>Doing a count like this would probably be pretty with Pig but you're probably more familiar with that than me</p>
",2590,1309280637
social media search engine question,"<p>I came across this site called <a href=""http://socialmention.com/"" rel=""nofollow noreferrer"">social mention</a> and am curious about how applications like this work, hopefully somebody can offer some glimpses/suggestions on this.</p>
<ol>
<li><p>Upon looking at the search results, I realize that they grab results from facebook, twitter, google.... I suppose this is done on the fly, probably through some REST api exposed by the mentioned?</p>
</li>
<li><p>If what I mention in point 1 is probably true, does that means sentiment analysis on the documents/links return is done on the fly too? Wouldn't that be too computationally intensive? I am curious because other than sentiments, they also return the top keywords in the document set.</p>
</li>
<li><p>They have something called the &quot;trends&quot;. They looked like the trendingtopics in twitter, but seems like they also include phrases &gt;3 words long. Is this relevant to nlp's entity extraction or more to keyphrase extraction? Is there apis other than that of Twitter that provides this? Is &quot;trends&quot; generally done on search queries submitted by users or do the system actually processes the pages?</p>
</li>
</ol>
<p>A curious man.</p>
","search, nlp, sentiment-analysis",,162,1297059988
British National Corpus,"<p>Has anyone used bnc (british national corpus) <a href=""http://www.natcorp.ox.ac.uk/"" rel=""nofollow"">http://www.natcorp.ox.ac.uk/</a> . Is there a way to query it from a java application?</p>
","java, web-services, nlp","<p>The XML documentation is here. <a href=""http://natcorp.ox.ac.uk/docs/URG"" rel=""nofollow"">http://natcorp.ox.ac.uk/docs/URG</a> </p>

<p>Should be quite easy to parse in Java using your favourite XML parser. As long as you buy a copy of the BNC and adhere to the license you buy, I see no reason you cannot use it from Java.</p>
",349,1311849485
translator in php and mysql,"<p>ok i make this one but i have 83000 words in mysql database when i execute this script it will take too much time and some time it not runs. i think this script match every title in mysql database wather it is in the $row['full_story'] or not. so this make the opreation unusable if there is any method i can make this process faster ? or it just match those titles which are used $row['full_story'] code is below</p>

<pre><code>$user_name = ""root"";
    $password = """";
    $database = ""salar"";
    $server = ""127.0.0.1"";
$db_handle = mysql_connect($server, $user_name, $password);
$db_found = mysql_select_db($database, $db_handle);
if ($db_found) {
$SQL = ""SET NAMES 'utf8'"";
    mysql_query($SQL);
$SQL = ""SELECT * FROM dle_mylinks ORDER BY LENGTH( title ) DESC"";
$result = mysql_query($SQL);
while ($db_field = mysql_fetch_assoc($result)) {
$row['full_story'] = str_replace ($db_field['title'],""&lt;a href=\""?newsid="" . $db_field['id'] . ""\""&gt;"" . $db_field['title'] . ""&lt;/a&gt;"" ,$row['full_story']);
$row['short_story'] = str_replace ($db_field['title'],""&lt;a href=\""?newsid="" . $db_field['id'] . ""\""&gt;"" . $db_field['title'] . ""&lt;/a&gt;"" ,$row['short_story']);
}
$mydata =$row['short_story'] .  $row['full_story'];


mysql_close($db_handle);

}
else {
print ""Database NOT Found "";
mysql_close($db_handle);
}
</code></pre>
","php, mysql, nlp",,409,1311537278
Using Aiksaurus for NLP,"<p>What is the original source for the thesaurus data in <a href=""http://aiksaurus.sourceforge.net/"" rel=""nofollow noreferrer"">Aiksaurus</a>?</p>

<p>Is it possible to get data about antonyms for a word from Aiksaurus?</p>
",nlp,"<p>I've never used Aiksaurus, but it seems to based on the <a href=""http://www.gutenberg.org/etext/3202"" rel=""nofollow noreferrer"">'Moby Thesaurus List' by Grady Ward</a>. It's even mentioned on the main project page.</p>
",168,1260977371
Quiz Generator using NLTK/Python,"<p>The goal of this application is produce a system that can generate quizzes automatically. The user should be able to supply any word or phrase they like (e.g. ""Sachin Tendulkar""); the system will then look for suitable topics online, identify a range of interesting facts, and rephrase them as quiz questions.</p>

<p>If I have the sentence ""Sachin was born in year 1973"", how can I rephrase it to ""Which Year was sachin born?""</p>
","python, nlp, nltk",,1432,1311322819
parsing sizes from textual data,"<p>I want to extract sizes , from textual data (""the missile was fifty five inches long."" , or ""he weighted nine and a half pounds."") and convert them to data in a format usable by a program. </p>

<p>What's a good way to go about this ? 
Are there any helpful libraries ?</p>

<p>And how complex is this task ? </p>
","parsing, nlp, text-parsing",,50,1311257339
NLTK - when to normalize the text?,"<p>I've finished gathering my data I plan to use for my corpus, but I'm a bit confused about whether I should normalize the text. I plan to tag &amp; chunk the corpus in the future. Some of NLTK's corpora are all lower case and others aren't.</p>

<p>Can anyone shed some light on this subject, please?</p>
","python, nlp, nltk",,3168,1311192111
Finding words from a dictionary in a string of text,"<p>How would you go about parsing a string of free form text to detect things like locations and names based on a dictionary of location and names? In my particular application there will be tens of thousands if not more entries in my dictionaries so I'm pretty sure just running through them all is out of the question. Also, is there any way to add ""fuzzy"" matching so that you can also detect substrings that are within <code>x</code> edits of a dictionary word? If I'm not mistaken this falls within the field of natural language processing and more specifically named entity recognition (NER); however, my attempt to find information about the algorithms and processes behind NER have come up empty. I'd prefer to use Python for this as I'm most familiar with that although I'm open to looking at other solutions.</p>
","search, full-text-search, nlp, named-entity-recognition","<p>You might try downloading the Stanford Named Entity Recognizer:
<a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/CRF-NER.shtml</a></p>

<p>If you don't want to use someone else's code and you want to do it yourself, I'd suggest taking a look at the algorithm in their associated paper, because the Conditional Random Field model that they use for this has become a fairly common approach to NER. </p>

<p>I'm not sure exactly how to answer the second part of your question on looking for substrings without more details.  You could modify the Stanford program, or you could use a part-of-speech tagger to mark proper nouns in the text.  That wouldn't distinguish locations from names, but it would make it very simple to find words that are x words away from each proper noun.</p>
",575,1310654973
Is this the correct definition of a &quot;corpus&quot;?,"<p>I have a huge string of raw text that is about 200,000 words long. It's a book.</p>

<p>I want to use these words to analyze the word relationships, so that I can apply those relationships to other applications.</p>

<p>Is this called a ""corpus""?</p>
","nlp, computer-science, nltk","<p>A <em>corpus</em>, in linguistics, is any coherent body of real-life(*) text or speech being studied. So yes, a book is a corpus. The fact that it's in one string doesn't matter, as long as you don't randomly shuffle the characters.</p>

<p>(*) As opposed to a bunch of made up phrases being shown to test subjects to measure their responses, as is commonly done in psycholinguistics.</p>
",2135,1310682165
Is this the correct definition of a &quot;corpus&quot;?,"<p>I have a huge string of raw text that is about 200,000 words long. It's a book.</p>

<p>I want to use these words to analyze the word relationships, so that I can apply those relationships to other applications.</p>

<p>Is this called a ""corpus""?</p>
","nlp, computer-science, nltk","<p>A <em>corpus</em>, in linguistics, is any coherent body of real-life(*) text or speech being studied. So yes, a book is a corpus. The fact that it's in one string doesn't matter, as long as you don't randomly shuffle the characters.</p>

<p>(*) As opposed to a bunch of made up phrases being shown to test subjects to measure their responses, as is commonly done in psycholinguistics.</p>
",2135,1310682165
Large scale na&#239;ve Bayes classifier with top-k output,"<p>I need a library for naïve Bayes large scale, with millions of training examples and +100k binary features. It must be an online version (updatable after training). I also need top-k output, that is multiple classifications for a single instance. Accuracy is not very important.</p>

<p>The purpose is an automatic text categorization application.</p>

<p>Any suggestions for a good library is very appreciated.</p>

<p>EDIT: The library should preferably be in Java.</p>
","nlp, machine-learning, bayesian, classification","<p>If a learning algorithm other than naïve Bayes is also acceptable, then check out <a href=""https://github.com/JohnLangford/vowpal_wabbit"" rel=""nofollow"">Vowpal Wabbit</a> (C++), which has the reputation of being one of the best scalable text classification algorithms (online stochastic gradient descent + LDA). I'm not sure if it does top-K output.</p>
",1048,1310470819
Part-Of-Speech tagging and Named Entity Recognition for C/C++/Obj-C,"<p>need some help!</p>

<p>I'm trying to write some code in objective-c that requires part-of-speech tagging, and ideally also named entity recognition.  I don't have much interest in ""rolling my own"", so I'm looking for a decent library to use for this purpose.  Obviously the more accurate the better, but we're not talking anything critical here -- so as long as it's generally pretty accurate that's good enough.</p>

<p>It's going to be English-only, at least for the time being, but I don't want to have to do any training of models myself.  So whatever the solution, it has to have an English language model already built.</p>

<p>And finally, it has to be available via a commercial-friendly license (e.g. BSD/Berkeley, LGPL).  Can't do GPL or anything restrictive like that, though I'm open to paying a small amount for a commercial license if that's the only option.</p>

<p>C, C++ or Obj-C code is all fine.</p>

<p>So: Anyone familiar with something that'd do the trick here?  Thanks!!</p>
","objective-c, ios, nlp, named-entity-recognition, part-of-speech","<p>I suggest you check out the iOS 5 beta release notes.</p>
",2004,1309269227
How to create templates from html page automatically?,"<p>I have a use case in which I need to render an unformatted text in the format of a given web page programmatically in Java. i.e. The text should automatically be formatted like the web page with styles, paragraphs, bullet points etc.<br/>
As I see first I will have to analyze the piece of unformatted text to find out the candidates for paragraphs, bullet points, headings etc. I intend to use Lucene analyzers/tokenizers for this task. Are there any alternatives?<br/>
The second problem is to convert the formatted web page into some kind of template (e.g. velocity template) with place holders for various entities like titles, bullet points etc.<br/>
Is there any text analysis/templating library in Java that can help me do this? Preferably open source.<br/>
Are there any other suggestions for doing this sort of task in a better way in Java?</p>

<p>Thanks for your help.</p>
","java, lucene, velocity, template-engine, text-analysis",,468,1310114083
Clustering conceptually similar documents together?,"<p>This is more of a conceptual question than an actual implementation and am hoping someone could clarify. My goal is the following: Given a set of documents, I want to cluster them such that documents belonging to the same cluster have the same ""concept"". </p>

<p>From what I understand, <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">Latent Semantic Analysis</a> lets me find a low rank approximation of a term-document matrix i.e. given a matrix <strong>X</strong>, it will decompose <strong>X</strong> as a product of three matrices, out of which one would be a diagonal matrix <strong>Σ</strong>:</p>

<p><img src=""https://i.sstatic.net/Ejhy5.png"" alt=""SVD""></p>

<p>Now, I would proceed by choosing a low rank approximation i.e. choose only the top-k values from <strong>Σ</strong>, and then calculate <strong>X'</strong>. Once I have this matrix, I have to apply some clustering algorithm and the end result would be set of clusters grouping documents with similar concepts. Is this the right way of applying clustering? I mean, calculating <strong>X'</strong> and then applying clustering on top of it or is there some other method that is followed?</p>

<p>Also, in a somewhat <a href=""https://stackoverflow.com/questions/5751114/nearest-neighbors-in-high-dimensional-data"">related question</a> of mine, I was told that the meaning of a <em>neighbor</em> is lost as the number of dimensions increases. In that case, what is the justification for clustering these high dimensional data points from <strong>X'</strong>? I am guessing that the requirement to cluster similar documents is a real-world requirement in which case, how does one go about addressing this?</p>
","python, numpy, nlp, machine-learning, data-mining","<p>For your first part of your question: No, you do not need to perform any 'clustering' anymore. Such clustering is already available from your <code>singular value decomposition</code>. If this is still unclear, please study more on detailed manner your link <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow"">Latent Semantic Analysis</a>.</p>

<p>For your second part: please just figure out the first part of your question and then restate this part of your question based on that.</p>
",1330,1310066107
algorithm to find out if two sentence parts can be joined together,"<p>I'm looking to build an algorithm that can join together sentence parts. So, for example, it would know that</p>

<p>""drank it down and ordered a second glass,""</p>

<p>would be able to follow:</p>

<p>""He listened to what she had to say and ordered a glass of wine,""</p>

<p>but </p>

<p>""by trying to know what he could not understand,""</p>

<p>would not. </p>

<p>I'm familiar with the NLTK - any suggestions?</p>
","nlp, nltk",,115,1310064007
Format text with regard to punctuation,"<p>How can I format text in a natural language taking punctuation into account?  The built-in <code>gq</code> command of Vim, or command line tools, such as <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?fmt"" rel=""nofollow"">fmt</a> or <a href=""http://www.nicemice.net/par/"" rel=""nofollow"">par</a> break lines without regard to punctuation.  Let me give you an example,</p>

<p><code>fmt -w 40</code> gives not what I want:</p>

<pre><code>we had everything before us, we had
nothing before us, we were all going
direct to Heaven, we were all going
direct the other way
</code></pre>

<p><code>smart_formatter -w 40</code> would give:</p>

<pre><code>we had everything before us,
we had nothing before us,
we were all going direct to Heaven,
we were all going direct the other way
</code></pre>

<p>Of course, there are cases when no punctuation mark is found within the given text width, then it can fallback to the standard text formatting behavior.</p>

<p>The reason why I want this is to get a meaningful <code>diff</code> of text where I can spot which sentence or subsentence changed.</p>
","text, formatting, diff, nlp","<p>Here is a not very elegant, but working method I finally came up with.  Suppose, a line break at a punctuation mark is worth 6 characters.  It means, I'll accept a result which is more ragged but contains more lines ending in a punctuation mark if the ""raggedness"" is less than 6 characters long.  For example, this is OK (""raggedness"" is 3 characters).</p>

<pre><code>Wait!
He said.
</code></pre>

<p>This is not OK (""raggedness"" is more than 6 characters)</p>

<pre><code>Wait!
He said to them.
</code></pre>

<p>The method is to add 6 dummy characters after each punctuation mark, format the text, then remove the dummy characters.</p>

<p>Here is the code for this</p>

<pre><code>sed -e 's/\([.?!,]\)/\1 _ _ _/g' | fmt -w 34 | sed -e 's/ _//g' -e 's/_ //g'
</code></pre>

<p>I used <code>_</code> (space + underscore) as a pair of dummy characters, supposing they're not contained in the text.  The result looks quite good,</p>

<pre><code>we had everything before us,
we had nothing before us,
we were all going direct to
Heaven, we were all going
direct the other way
</code></pre>
",282,1305575285
Which classifier to choose in NLTK,"<p>I want to classify text messages into several categories like, ""relation building"", ""coordination"", ""information sharing"", ""knowledge sharing"" &amp; ""conflict resolution"". I am using NLTK library to process these data. I would like to know which classifier, in nltk, is better for this particular multi-class classification problem. </p>

<p>I am planning to use Naive Bayes Classification, is it advisable? </p>
","nlp, classification, nltk","<p>Naive Bayes is the simplest and easy to understand classifier and for that reason it's nice to use. Decision Trees with a beam search to find the best classification are not significantly harder to understand and are usually a bit better. MaxEnt and SVM tend be more complex, and SVM requires some tuning to get right.</p>

<p>Most important is the choice of features + the amount/quality of data you provide!</p>

<p>With your problem, I would focus first on ensuring you have a good training/testing dataset and also choose good features. Since you are asking this question you haven't had much experience with machine learning for NLP, so I'd say start of easy with Naive Bayes as it doesn't use complex features- you can just tokenize and count word occurrences.</p>

<p>EDIT:
The question <a href=""https://stackoverflow.com/questions/6448002/how-do-you-find-the-subject-of-a-sentence/6448053#6448053"">How do you find the subject of a sentence?</a> and my answer are also worth looking at.</p>
",5248,1309882479
Opennlp 1.5 for SentenceDetector?,"<p>Now I have the following code:</p>

<pre><code>SentenceModel sd_model = null;
  try {
   sd_model = new SentenceModel(new FileInputStream(
     ""opennlp/models/english/sentdetect/en-sent.bin""));
  } catch (InvalidFormatException e) {
   // TODO Auto-generated catch block
   e.printStackTrace();
  } catch (FileNotFoundException e) {
   // TODO Auto-generated catch block
   e.printStackTrace();
  } catch (IOException e) {
   // TODO Auto-generated catch block
   e.printStackTrace();
  }
  SentenceDetectorME mSD = new SentenceDetectorME(sd_model);
  String param = ""This is a good senttence.I'm very happy. Who can tell me the truth.And go to school."";
  String[] sents = mSD.sentDetect(param);
  for(String sent : sents){
   System.out.println(sent);
  }
</code></pre>

<p>But I got the follwing results:</p>

<pre><code>This is a good senttence.I'm very happy.
Who can tell me the truth.And go to school.
</code></pre>

<p>Absolutely, this isn't what we want. How can I fix the problem? thanx.</p>
","java, nlp, opennlp",,2706,1285750924
Does OpenNLP use WordNet under the hood for the Named Entity Recognition,"<p>I have tried using OpenNLP Tools 1.5 from SourceForge for getting the Named Entites from a text. I did find a JWNL file in the OpenNLP download. Does that imply that OpenNLP in turn is using WordNet for the Named Entity Recognition. (Specifically, does it mean that the Name Model files were generated with WordNet) ?</p>
","java, nlp, wordnet, named-entity-recognition, opennlp","<p>No, OpenNLP only uses WordNet for co-reference resolution, such as correlating ""He"" to ""Steve Jones"" in separate sentences.</p>

<p>WordNet is not used in Named Entity Recognition. The named entity training files are manually annotated fields from various conferences/tests, such as CoNLL. </p>

<p>Source: <a href=""http://opennlp.sourceforge.net/README.html"" rel=""nofollow"">http://opennlp.sourceforge.net/README.html</a></p>
",2059,1303936776
Speeding up OpenNLP&#39;s POSTagging when using it for several texts,"<p>I'm currently working on a keyphrase extraction tool, which should provide tag suggestions for texts or documents on a website. As I am following the method proposed in this paper: <a href=""http://www.ijcsi.org/papers/7-2-3-16-25.pdf"" rel=""nofollow"" title=""Sarkar et al."">A New Approach to Keyphrase Extraction Using Neural Networks</a> I am using the OpenNLP toolkit's POSTagger for the first step, i.e. candidate selection. </p>

<p>In general the keyphrase extraction works pretty well. My problem is that I have to perform this expensive loading of the models from their corresponding files every time I want to use the POSTagger: </p>

<pre><code>posTagger = new POSTaggerME(new POSModel(new FileInputStream(new File(modelDir + ""/en-pos-maxent.bin""))));
tokenizer = new TokenizerME(new TokenizerModel(new FileInputStream(new File(modelDir + ""/en-token.bin""))));
// ...
String[] tokens = tokenizer.tokenize(text);
String[] tags = posTagger.tag(tokens);
</code></pre>

<p>This is due to the fact that this code is not on the scope of the webserver itself but inside a ""handler"" with a lifecycle including only handling one specific request. My question is: <strong>How can I achieve loading the files only once?</strong> (I don't want to spend 10 seconds on waiting for the models to load and using it just for 200ms afterwards.)</p>

<p>My first idea was to serialize the <strong>POSTaggerME</strong> (<strong>TokenizerME</strong> resp.) and deserialize it every time I need it using Java's built-in mechanism. Unfortunately this doesn't work – it raises an exception. (I do serialize the classifier from the WEKA-toolkit which classifies my candidates at the end in order to not having to build (or train) the classifier every time. Therefore I thougth this may be applicable to the POSTaggeME as well. Unfortunately this is not the case.) </p>

<p>In the case of the Tokenizer I could refer to a simple <strong>WhitespaceTokenizer</strong> which is an inferior solution but not that bad at all:</p>

<pre><code>tokenizer = WhitespaceTokenizer.INSTANCE;
</code></pre>

<p>But I don't see this option for a reliable POSTagger. </p>
","java, performance, serialization, nlp, opennlp","<p>Just wrap your tokenization/POS-tagging pipeline in a <strong><a href=""http://en.wikipedia.org/wiki/Singleton_pattern"" rel=""nofollow"">singleton</a></strong>. </p>

<p>If the underlying OpenNLP code isn't thread safe, put the calls in synchronization blocks, e.g.:</p>

<pre><code>// the singletons tokenization/POS-tagging pipeline 
String[] tokens;
synchronized(tokenizer) { 
   tokens = tokenizer.tokenize(text);
}
String[] tags;
synchronized(posTagger) { 
   tags = posTagger.tag(tokens);
}
</code></pre>
",1848,1291652081
opennlp vs stanford nlptools vs berkeley,"<p>Hi the aim is to parse a sizeable corpus like wikipedia to generate the most probable parse tree,and named entity recognition. Which is the best library to achieve this in terms of performance and accuracy?  Has anyone used more than one of the above libraries?</p>
","parsing, nlp, stanford-nlp, opennlp","<p>I'm currently using Enju HPSG parser  which seems to be better than the others.</p>
",4907,1285903799
Stanford NLP tools and lots of text,"<p>I'm currently using the Stanford CoreNLP tools to extract triplets from text. I've been dealing with relatively small pieces of text, but I need to make it work with larger ones. Also, I need this work on the average machine with an average amount of memory. Any suggestions on how I can reduce the memory load? Perhaps split the text into smaller pieces (this would screw up coreference however...)?</p>

<p>Thank you</p>
","java, nlp, stanford-nlp",,435,1309518324
Is POS tagging deterministic?,"<p>I have been trying to wrap my head around why this is happening but am hoping someone can shed some light on this. I am trying to tag the following text:</p>

<pre><code>ae0.475      X  mod 
ae0.842      X  mod
ae0.842      X  mod 
ae0.775      X  mod 
</code></pre>

<p>using the following code:</p>

<pre><code>import nltk

file = open(""test"", ""r"")

for line in file:
        words = line.strip().split(' ')
        words = [word.strip() for word in words if word != '']
        tags = nltk.pos_tag(words)
        pos = [tags[x][1] for x in range(len(tags))]
        key = ' '.join(pos)
        print words, "" : "", key
</code></pre>

<p>and am getting the following result:</p>

<pre><code>['ae0.475', 'X', 'mod']  :  NN NNP NN
['ae0.842', 'X', 'mod']  :  -NONE- NNP NN
['ae0.842', 'X', 'mod']  :  -NONE- NNP NN
['ae0.775', 'X', 'mod']  :  NN NNP NN
</code></pre>

<p>And I don't get it. Does anyone know what is the reason for this inconsistency? I am not very particular about the accuracy about the pos tagging because I am attempting to extract some templates but it seems to be using different tags at different instances for a word that looks ""almost"" the same.</p>

<p>As a solution, I replaced all numbers with 1 and solved the problem:</p>

<pre><code>['ae1.111', 'X', 'mod']  :  NN NNP NN
['ae1.111', 'X', 'mod']  :  NN NNP NN
['ae1.111', 'X', 'mod']  :  NN NNP NN
['ae1.111', 'X', 'mod']  :  NN NNP NN
</code></pre>

<p>but am curious why it tagged the instance with different tags in my first case. Any suggestions?</p>
","python, nlp, machine-learning, nltk","<p>My best effort to understand uncovered <a href=""http://nltk.googlecode.com/svn/trunk/doc/howto/tag.html"" rel=""noreferrer"">this</a> from someone not using the whole Brown corpus:</p>

<blockquote>
  <p>Note that words that the tagger has
  not seen before, such as decried,
  receive a tag of None.</p>
</blockquote>

<p>So, I guess something that looks like <code>ae1.111</code> must appear in the corpus file, but nothing like <code>ae0.842</code>. That's kind of weird, but that's the reasoning for giving the <code>-NONE-</code> tag. </p>

<p>Edit: I got super-curious, <a href=""http://www.archive.org/details/BrownCorpus"" rel=""noreferrer"">downloaded the Brown corpus</a> myself, and plain-text-searched inside it. The number <code>111</code> appears in it 34 times, and the number <code>842</code> only appears 4 times. <code>842</code> only appears either in the middle of dollar amounts or as the last 3 digits of a year, and <code>111</code> appears many times on its own as a page number. <code>775</code> also appears once as a page number. </p>

<p>So, I'm going to make a conjecture, that because of <a href=""http://en.wikipedia.org/wiki/Benford%27s_law"" rel=""noreferrer"">Benford's Law</a>, you will end up matching numbers that start with 1s, 2s, and 3s much more often than numbers that start with 8s or 9s, since these are more often the page numbers of a random page that would be cited in a book. I'd be really interested in finding out if that's true (but not interested enough to do it myself, of course!).</p>
",467,1309468263
Discovering &quot;templates&quot; in a given text?,"<p>If I have significant amounts of text and am trying to discover templates that occur most frequently, I was thinking of solving it using the N-Gram approach and in fact it was suggested as a solution in <a href=""https://stackoverflow.com/questions/1426383/what-techniques-tools-are-there-for-discovering-common-phrases-in-chunks-of-text"">this</a> question as well but my requirement is slightly different. Just to clarify, I have some text like this:</p>

<pre><code>I wake up every day morning and read the newspaper and then go to work
I wake up every day morning and eat my breakfast and then go to work
I am not sure that this is the solution but I will try
I am not sure that this is the answer but I will try
I am not feeling well today but I will get the work done and deliver it tomorrow
I was not feeling well yesterday but I will get the work done and let you know by tomorrow
</code></pre>

<p>and am trying to extract ""templates"" like this:</p>

<pre><code>I wake up every day morning and ... and then go to work
I am not sure that this is the ... but I will try
I ... not feeling well ... but I will get the work done and ... tomorrow
</code></pre>

<p>I am looking for an approach that can scale to million of lines of text so I was just wondering if I can adapt the same N-gram approach to solve this problem or are there any alternatives? </p>
","language-agnostic, nlp, machine-learning, data-mining, nltk","<p>Millions of lines of text isn't a really big number :)</p>

<p>What you're looking for is at least similar to collocation finding. You could try to compute pointwise mutual information on n-grams. See <a href=""http://www-nlp.stanford.edu/fsnlp/colloc/"" rel=""noreferrer"">Manning &amp; Schütze (1999)</a> for this and other approaches to the problem.</p>
",169,1309381651
Can someone explain how this mgiza script works?,"<pre><code>$:~/mgizapp/scripts$ ./plain2snt-hasvcb.py
Error, the input should be 
./plain2snt-hasvcb.py evcb fvcb etxt ftxt esnt(out) fsnt(out) evcbx(out) fvcbx(out)
You should concatenate the evcbx and fvcbx to existing vcb files
</code></pre>

<p>can someone explain what the all the acrane inputs for the plain2snt script? the script is from the mgiza++ program for word alignment from <a href=""http://geek.kyloo.net/software/doku.php/mgiza:forcealignment"" rel=""nofollow"">http://geek.kyloo.net/software/doku.php/mgiza:forcealignment</a></p>

<p>evcb = ? #is it the source.vcb file?
fvcb = ? #is it the target.vcb file?</p>

<p>esnt(out) = ? 
fsnt(out) = ? </p>

<p>evcbx(out) = ?
fvcbx (out) = ?</p>

<p><strong>ANSWER</strong></p>

<p>I managed to get it to work </p>

<pre><code>$mkcls -n10 -psourcelangfile.vcb -Vsourcelangfile.vcb.classes
$mkcls -n10 -psourcelangfile.vcb -Vtargetlangfile.vcb.classes
$plain2snt sourcelangfile targetlangfile
$snt2cooc sourcelang_targetlang.cooc sourcelangfile.vcb targetlangfile.vcb sourcelangfile_targetlangfile.snt
</code></pre>
","python, nlp, corpus, machine-translation","<p>Based on my (not equivalent) experience with GIZA++ and the page you link to, I'd say evcb and fvcb are the ""English"" and ""Foreign"" vocab files you've generated already and that etxt and ftxt are the ""English"" and ""Foreign"" text inputs. It seems then that esnt and fsnt are the ""English"" and ""Foreign"" sentence output files (probably the sentences with the words replaced by their unique identifiers from the vcb files). Finally, evcbx and fvcbx seem to be output locations for eXtending the original vocab files by concatenation.</p>

<p>I hope this helps, and I hope someone else who's used MGIZA can jump in and correct me if I am wrong.</p>
",367,1300701029
Least used unicode delimiter,"<p>I'm trying to tag my text with a delimiter at specific places that will be used later for parsing. I want to use a delimiter character that is least frequently used. I'm currently looking at the ""\2"" or the U+0002 character. Is that safe enough to use? What other suggestions are there? The text is unicode and will have both english and non-english characters.</p>

<p>A want to use a character that can still be ""exploded()"" by PHP.</p>

<p><strong>Edit:</strong></p>

<p>Also I want to be able to display this piece of text on screen (to the browser) and the delimiter will be ""invisible"" to the user. I can definitely use a str_replace() to get rid of visible delimiters, but if there are good invisible delimiters, then no such processing is needed.</p>
","php, parsing, unicode, nlp, unicode-string","<p>If this is only for an internal representation (i.e. <em>not</em> for interchange and storage), then you can use a non-character code point such as U+FFFF. <a href=""https://stackoverflow.com/questions/3482683/can-a-valid-unicode-string-contain-ffff-is-java-characteriterator-broken/3482698#3482698"">Java uses that as the signal that a CharacterIterator is done, for example</a>.</p>
",3124,1309182870
Extracting &#39;useful&#39; information out of sentences?,"<p>I am currently trying to understand sentences of this form: </p>

<p><code>The problem was more with the set-top box than the television. Restarting the set-top box solved the problem.</code></p>

<p>I am totally new to Natural Language Processing and started using Python's NLTK package to get my hands dirty. However, I am wondering if someone could give me an overview of the high-level steps involved in achieving this.</p>

<p>What I am trying to do is to identify what the problem was so in this case, <code>set-top box</code> and whether the action that was taken resolved the problem so in this case, <code>yes</code> because restarting fixed the problem. So if all the sentences were of this form, my life would have been easier but because it is natural language, the sentences could also be of the following form:</p>

<p><code>I took a look at the car and found nothing wrong with it. However, I suspect there is something wrong with the engine</code></p>

<p>So in this case, the problem was with the <code>car</code>. The action taken did not resolve the problem because of the presence of the word <code>suspect</code>. And the potential problem could be with the <code>engine</code>.</p>

<p>I am not looking for an absolute answer as I suspect this is very complex. What I am looking for is more rather a high-level overview that will point me in the right direction. If there is an easier/alternate way to do this, that is welcome as well.</p>
","language-agnostic, nlp, machine-learning, nltk","<p>Probably, if the sentences are well-formed, I would experiment with <a href=""http://en.wikipedia.org/wiki/Dependency_grammar"" rel=""nofollow"">dependency parsing</a> (http://nltk.googlecode.com/svn/trunk/doc/api/nltk.parse.malt.MaltParser-class.html#raw_parse). That gives you a graph of the constituents of a sentence and you can tell the relations between the lexical items. Later, you can extract phrases from the output of a dependency parser (http://nltk.googlecode.com/svn/trunk/doc/book/ch08.html#code-cfg2) That could help you to extract the direct object of a sentence, or the verb phrase in a sentence.</p>

<p>If you just want to get phrases or ""chunks"" from a sentence, you can try chunk parser (http://nltk.googlecode.com/svn/trunk/doc/api/nltk.chunk-module.html). You can also carry out named entity recognition (http://streamhacker.com/2009/02/23/chunk-extraction-with-nltk/). It's usually used to extract instances of places, organizations or people names but it could work in your case as well.</p>

<p>Assuming that you solve the problem of extracting noun/verb phrases from a sentence, you may need to filter them out to ease the job of your domain expert (too many phrases could overwhelm a judge). You may carry out a frequency analysis on your phrases, remove very frequent ones that are not usually related to the problem domain, or compile a white-list and keep the phrases that contain a pre-defined set of words, etc. </p>
",2458,1309062830
Algorithm to understand meaning,"<p>I want to know if is there any specific algorithm that can be followed to understand the meaning of a word/sentence/paragraph. Basically, I want to write a program that will take text/paragraph as input and try to find out what its meaning is. And thereby highlight the emotions within the text.</p>

<p>Also, if there is an algorithm to understand things, can the same algorithm be applied to itself? It reduces the quest further to a point where we become interested in knowing meaning of meaning OR rather definition of definition.</p>
","algorithm, artificial-intelligence, nlp, semantics",,4287,1309111502
How to identify tags (key words) automatically from a given text?,"<p>It should behave like <a href=""https://addons.mozilla.org/en-us/firefox/addon/delicious-bookmarks/"" rel=""nofollow noreferrer"">Delicious toolbar</a> for Firefox does; it lists possible tags to click. The effect is shown as below: </p>

<p><img src=""https://i.sstatic.net/8PU8T.png"" alt=""enter image description here""></p>

<p>The code should be able to find key words for the text. Any good algorithm or open source project to recommend?</p>

<p>I found <a href=""https://stackoverflow.com/questions/2853384/how-to-identify-ideas-and-concepts-in-a-given-text"">this post</a>, but it is a bit too general for my specific need.</p>
","algorithm, full-text-search, text-analysis","<p>I think you're looking for one of these answers,</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content"">tag generation from a text content</a></li>
<li><a href=""https://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries"">How to extract common / significant phrases from a series of text entries</a></li>
<li><a href=""https://stackoverflow.com/questions/2764116/tag-generation-from-a-small-text-content-such-as-tweets"">tag generation from a small text content (such as tweets)</a></li>
</ul>

<p>In a nutshell - you're looking to extract unigrams from the text that somehow represent the concepts within it - a technique to do this is called Pointwise Mutual Information, which is illustrated with an example in the first two links. Using the Python NLTK framework (which already has a bunch of these algorithms built in) might be your best starting point to work off from.</p>

<p>Good luck!</p>
",4692,1296704707
"python re match, findall or search and then NLP (what to do with it?)","<p>I am starting to write code that would capture part of sentence ""types"" and if they match a criteria, start a specific python script that deals with the ""type."" I am ""finding"":) that findall kind of works better for what i am doing hence:</p>

<pre><code>m = re.compile(r'([0-9] days from now)')
m.match(""i think maybe 7 days from now i hope"")
print m.match(""i think maybe 7 days from now i hope"")
None
f= m.findall(""i think maybe 7 days from now i hope"")
print f[0]
7 days from now
</code></pre>

<p>This seems to give me the part of sentence that i was looking for. I  can then give this to for example - the pyparsing module using its example datetime conversion script that returns a datetime from a similar NL statement (I know there are other modules but they are rigid in input statements they can handle) .<br>
Then I could do a db insert into my online diary for example or on a hosted web  app if other parts of the sentence matched another ""type"" ie. appointments, deadlines etc.<br>
I am just tinkering here but slowly i am building something useful. Is this structure /process logical or are there better methods/ ways: that is what i am asking myself now. Any feedback is appreciated</p>
","python, regex, nlp",,3999,1303814469
Scraping English Words using Python,"<p>I would like to scrape all English words from, say, New York Times front page. I wrote something like this in Python:</p>

<pre><code>import re
from urllib import FancyURLopener

class MyOpener(FancyURLopener):
    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'            

opener = MyOpener()
url = ""http://www.nytimes.com""
h = opener.open(url)
content = h.read()
tokens = re.findall(""\s*(\w*)\s*"", content, re.UNICODE) 
print tokens
</code></pre>

<p>This works okay, but I get HTML keywords such as ""img"", ""src"" as well as English words. Is there a simple way to get only English words from Web scaping / HTML ? </p>

<p>I saw <a href=""https://stackoverflow.com/questions/5635400/scraping-with-python"">this</a> post, it only seems to talk about the mechanics of scraping, none of the tools mentioned talk about how to filter out non-language elements. I am not interested in links, formatting, etc. Just plain words. Any help would be appreciated. </p>
","python, nlp, urllib2, web-scraping, urllib","<p>Are you sure you want ""English"" words -- in the sense that they appear in some dictionary?  For example, if you scraped an NYT article, would you want to include ""Obama"" (or ""Palin"" for you Blue-Staters out there), even though they probably don't appear in any dictionaries yet?</p>

<p>Better, in many cases, to parse the HTML (using BeautifulSoup as Bryan suggests) and include only the text-nodes (and maybe some aimed-at-humans attributes like ""title"" and ""alt"").</p>
",1860,1308613194
Parser to parse search terms and extract valuable information,"<p>I would like to understand the serarh term of a user. Think of someone is searching for ""staples in NY"" - I would like to understand that its a location search where keyword is staples and location is new york. Similarly if someone types ""cat in hat"", the parser should not flag that also as a location search, here the entire keyword is ""cat in hat"".
Is there any algorithm or open source library available to parse a search term and understand its a comparison (like A vs B) or its a location based search (like A in X)?</p>
","algorithm, parsing, nlp, information-extraction",,544,1308599208
Parse WordNet database into SQL?,"<p>I would like to have the <a href=""http://wordnet.princeton.edu/wordnet/"" rel=""nofollow"">WordNet database</a> in SQL format so I'm thinking about trying to write a <a href=""http://wordnet.princeton.edu/wordnet/man/wndb.5WN.html"" rel=""nofollow"">parser for the WordNet files</a>. However, before I get started I was wondering if there are any existing parsers or if someone has already created a SQL version of the database?</p>
","parsing, nlp, wordnet",,2029,1304610109
Get POS probabilities from Wordnet command line tool,"<p>I'm writing some experiments with ruby accessing wordnet through the wn command line tool because I gave up on getting the wordnet gem to work.</p>

<p>I want to be able to lookup the frequencies of senses, ultimately to be able to calculate the probability that a given word is a noun/adjective/verb/adverb.</p>

<p>I've tried the documentation but it's not always so explicit.</p>

<p>Is this possible without using just the wn tool? and am I write in thinking wordnet includes this info?</p>
","nlp, wordnet","<p>As far as I can tell, it does not include frequencies per se, though synsets are ordered from most to least frequent in the return results.  </p>

<p>You can get actual frequencies a number of ways.  Perhaps the most reliable is to use a POS tagged corpus like the Penn TreeBank, then just compute the values yourself.  Unfortunately, getting a free copy of that is difficult if you're not in a university.  Another option is to build your own corpus (maybe from blogs, Project Gutenberg books, Wikipedia, whatever), run a POS tagger over it and then compute the frequencies from that.  Obviously, this method is going to be skewed, but it's a lot easier than tagging a corpus manually.</p>
",439,1308325801
Format relative dates,"<p>Is there a ruby gem that will format dates relative to the current time? I want output like ""Tomorrow at 5pm"", ""Thursday next week at 5:15pm"", I'm not too concerned about the exact output, just as long as it's relative dates in natural language</p>
","ruby, datetime, rubygems, nlp","<p>if you have rails, I think <strong><a href=""http://api.rubyonrails.org/classes/ActionView/Helpers/DateHelper.html#M001695"" rel=""nofollow noreferrer"">ActionView::Helpers::DateHelper#distance_of_time_in_words</a></strong> helps that.</p>

<pre><code>require 'rubygems'
require 'action_view'
include ActionView::Helpers::DateHelper

from_time = Time.now
distance_of_time_in_words(from_time, from_time + 50.minutes)        # =&gt; about 1 hour
distance_of_time_in_words(from_time, 50.minutes.from_now)           # =&gt; about 1 hour
distance_of_time_in_words(from_time, from_time + 15.seconds)        # =&gt; less than a minute
distance_of_time_in_words(from_time, from_time + 15.seconds, true)  # =&gt; less than 20 seconds
distance_of_time_in_words(from_time, 3.years.from_now)              # =&gt; over 3 years
distance_of_time_in_words(from_time, from_time + 60.hours)          # =&gt; about 3 days
distance_of_time_in_words(from_time, from_time + 45.seconds, true)  # =&gt; less than a minute
distance_of_time_in_words(from_time, from_time - 45.seconds, true)  # =&gt; less than a minute
distance_of_time_in_words(from_time, 76.seconds.from_now)           # =&gt; 1 minute
distance_of_time_in_words(from_time, from_time + 1.year + 3.days)   # =&gt; about 1 year
distance_of_time_in_words(from_time, from_time + 4.years + 9.days + 30.minutes + 5.seconds) # =&gt; over 4 years

to_time = Time.now + 6.years + 19.days
distance_of_time_in_words(from_time, to_time, true)     # =&gt; over 6 years
distance_of_time_in_words(to_time, from_time, true)     # =&gt; over 6 years
distance_of_time_in_words(Time.now, Time.now)           # =&gt; less than a minute
</code></pre>

<p>In case of relative to the current time, use <code>distance_of_time_in_words_to_now</code> instead of <code>distance_of_time_in_words</code>.</p>

<p>If your app is rails-based, just use <code>distance_of_time_in_words</code>, <code>distance_of_time_in_words_to_now</code>, <code>time_ago_in_words</code> in view.</p>
",3650,1271732602
Using Markov chains (or something similar) to produce an IRC-bot,"<p>I tried google and found little that I could understand.</p>

<p>I understand <a href=""http://en.wikipedia.org/wiki/Markov_chain"" rel=""noreferrer"">Markov chains</a> to a very basic level: It's a mathematical model that only depends on previous input to change states..so sort of a FSM with weighted random chances instead of different criteria?</p>

<p>I've heard that you can use them to generate semi-intelligent nonsense, given sentences of existing words to use as a dictionary of kinds. </p>

<p>I can't think of search terms to find this, so can anyone link me or explain how I could produce something that gives a semi-intelligent answer? (if you asked it about pie, it would not start going on about the vietnam war it had heard about)</p>

<p>I plan on:</p>

<ul>
<li>Having this bot idle in IRC channels for a bit</li>
<li>Strip any usernames out of the string and store as sentences or whatever</li>
<li>Over time, use this as the basis for the above.</li>
</ul>
","artificial-intelligence, nlp, markov-chains","<p>Yes, a Markov chain is a finite-state machine with probabilistic state transitions. To generate random text with a simple, first-order Markov chain:</p>

<ol>
<li>Collect bigram (adjacent word pair) statistics from a corpus (collection of text).</li>
<li>Make a markov chain with one state per word. Reserve a special state for end-of-text.</li>
<li>The probability of jumping from state/word <em>x</em> to <em>y</em> is the probability of the words <em>y</em> immediately following <em>x</em>, estimated from relative bigram frequencies in the training corpus.</li>
<li>Start with a random word <em>x</em> (perhaps determined by how often that word occurs as the first word of a sentence in the corpus). Then pick a state/word <em>y</em> to jump to randomly, taking into account the probability of <em>y</em> following <em>x</em> (the state transition probability). Repeat until you hit end-of-text.</li>
</ol>

<p>If you want to get something semi-intelligent out of this, then your best shot is to train it on lots of carefully collected texts. The ""lots"" part makes it produce proper sentences (or plausible IRC speak) with high probability; the ""carefully collected"" part means you control what it talks about. Introducing higher-order Markov chains also helps in both areas, but takes more storage to store the necessary statistics. You may also look into things like statistical smoothing.</p>

<p>However, having your IRC bot actually respond to what is said to it takes a <em>lot</em> more than Markov chains. It may be done by doing <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Document_classification"" rel=""noreferrer"">text categorization</a> (aka topic spotting) on what is said, then picking a domain-specific Markov chain for text generation. Naïve Bayes is a popular model for topic spotting.</p>

<p>Kernighan and Pike in <a href=""http://cm.bell-labs.com/cm/cs/tpop/"" rel=""noreferrer""><em>The Practice of Programming</em></a> explore various implementation strategies for Markov chain algorithms. These, and natural language generation in general, is covered in great depth by Jurafsky and Martin, <a href=""http://www.cs.colorado.edu/~martin/slp.html"" rel=""noreferrer""><em>Speech and Language Processing</em></a>.</p>
",8351,1301586886
How to determine if a sentence is talking about a specific subject?,"<p>I have predefined words and would like to know if the sentence primary subject is about the predefined words.</p>

<p>Example:</p>

<p>Predefined words:
iPhone, Nexus, HTC</p>

<p>Sentence:</p>

<p>I like the new design of iPhone - primary subject is iPhone</p>

<p>I am listing to Nirvana on my Nexus. - primary subject is not in predefined words</p>

<p>The HTC phone is better than iPhone - primary subject is HTC</p>

<p>Would like to do this in PHP or something I that can have PHP interface.</p>
","php, nlp","<p>Alias-i has a <a href=""http://alias-i.com/lingpipe/web/demos.html"" rel=""nofollow"">natural language parser for PHP</a>. </p>

<p>Edit: <a href=""http://www.akshatsinghal.com/content/natural-language-processing-php"" rel=""nofollow"">this page</a> says Alias-i's parser is written in PHP, but Alias-i's website says it is written in Java.</p>
",855,1308161335
ARFF for natural language processing,"<p>I'm trying to take a set of reviews, and convert them into the ARFF format for use with WEKA. Unfortunately either I completely misunderstand how the format works, or I'll have to have an attribute for ALL possible words, then a presence indicator. Does anyone know a better way, or ideally have a sample ARFF file?</p>
","nlp, machine-learning, weka, arff","<p>Took a while to work out, but with this input.arff:</p>

<pre><code>@relation text_files

@attribute review string
@attribute sentiment {0, 1}

@data
""this is some text"", 1
""this is some more text"", 1
""different stuff"", 0
</code></pre>

<p>And this command:</p>

<pre><code>java -classpath ""C:\\Program Files\\Weka-3-6\\weka.jar"" weka.filters.unsupervised.attribute.StringToWordVector -i input.arff -o output.arff
</code></pre>

<p>The following is produced:</p>

<pre><code>@relation 'text_files-weka.filters.unsupervised.attribute.StringToWordVector-R1-W1000-prune-rate-1.0-N0-stemmerweka.core.stemmers.NullStemmer-M1-tokenizerweka.core.tokenizers.WordTokenizer -delimiters \"" \\r\\n\\t.,;:\\\'\\\""()?!\""'

@attribute sentiment {0,1}
@attribute different numeric
@attribute is numeric
@attribute more numeric
@attribute some numeric
@attribute stuff numeric
@attribute text numeric
@attribute this numeric

@data

{0 1,2 1,4 1,6 1,7 1}
{0 1,2 1,3 1,4 1,6 1,7 1}
{1 1,5 1}
</code></pre>
",1858,1306592353
textalyser functionality using Python,"<p>Which Python based library is out that gives the text-analysis functionality similar to the <a href=""http://textalyser.net/"" rel=""nofollow"">http://textalyser.net/</a></p>
","python, text-analysis",,438,1307848643
Word lists for a lot of articles - document-term matrix,"<p>I have nearly 150k articles in Turkish. I will use articles for natural language processing research.
I want to store words and frequency of them per article after processing articles.</p>

<p>I'm storing them in RDBS now.</p>

<p>I have 3 tables:</p>

<p>Articles -> article_id,text<br>
Words -> word_id, type, word<br>
Words-Article -> id, word_id, article_id, frequency (index for word_id, index for article_id )  </p>

<p>I will query for  </p>

<ul>
<li>ALL Words in an article   </li>
<li>one Word's frequency per article  </li>
<li>Word occurrences in all articles and in which articles</li>
</ul>

<p>I have millions of rows in words-article table. I always worked with RDBS in this project. started with mysql and using oracle now. But I don't want to use oracle and want better performance than mysql.</p>

<p>Also I have to handle this job in a machine with 4gb ram.<br>
Simply, how to store document-term matrix and make some query on it? performance is necessary. can ""key-value databases"" beat mysql at performance?  or what can beat mysql?</p>

<p>if your answer programming language depended, I'm writing code in python. But C/C++ , Java is ok. </p>
","words, full-text-indexing, text-analysis, frequency-analysis",,612,1261707813
Controlled Natural Language scheme for describing system architectures?,"<p>Are there any good Controlled Natural Language models, written in something like W3C Metalog PNL, for describing system architectures?</p>

<p>I'm looking for a text-based alternative to UML diagrams... to represent System Components, Relationships, Dependencies, Use Cases, Data Flows, etc. The goal is to have a model that can be programatically queried (unlike a diagram) to answer simple questions like, ""what other components connect to this component?"".</p>

<p>Thanks</p>
","architecture, uml, nlp, semantic-web",,221,1305573290
English lemmatizer databases?,"<p>Do you know any big enough lemmatizer database that returns correct result for following sample words:</p>

<pre><code>geese: goose
plantes: //not found
</code></pre>

<p>Wordnet's morphological analyzer is not sufficient, since it gives the following incorrect results:</p>

<pre><code>geese: //not found
plantes: plant
</code></pre>
","nlp, stemming, lemmatization, morphological-analysis","<p><a href=""http://morphadorner.northwestern.edu/morphadorner/lemmatizer/example/"" rel=""nofollow"">MorphAdorner</a> seems to be better at this, but it still finds the incorrect result for ""plantes""</p>

<pre><code>plantes: plante
geese: goose
</code></pre>

<p>Maybe you'd like to use MorphAdorner to do the lemmatization, and then check its results against WordNet. You can use the WordNet API to perform lookups <em>without</em> first performing lemmatization by calling <code>findtheinfo_ds</code>. This allows you to use a lemmatizer like MorphAdorner first. (If you wanted to use the lemmaitzer, you'd need to call <code>morph</code> separately and call <code>findtheinfo_ds</code> on the lemmas that it returned.)</p>

<p>On the other hand, I only spent about 5 seconds looking at MorphAdorner for this purpose, and there may be a way to eliminate the incorrect ""plantes"" answer without having to use any other outside resource.</p>
",2518,1307718050
Wordnet in Spanish,"<p>Is there something similar to Princeton's WordNet in Spanish?
I need to find synonyms in Spanish. </p>
","nlp, wordnet","<p>Have you checked here : <a href=""http://www.globalwordnet.org/gwa/wordnet_table.htm"" rel=""nofollow noreferrer"">http://www.globalwordnet.org/gwa/wordnet_table.htm</a> ? There's a link to  a Spanish project.</p>
",2230,1266337828
using python nltk to find similarity between two web pages?,"<p>I want to find whether two web pages are similar or not. Can someone suggest if python nltk with wordnet similarity functions helpful and how? What is the best similarity function to be used in this case?</p>
","python, nlp, nltk, wordnet","<p>The <a href=""http://ilpubs.stanford.edu:8090/821/"" rel=""noreferrer"">spotsigs</a> paper mentioned by <em>joyceschan</em> addresses content duplication detection and it contains plenty of food for thought. </p>

<p>If you are looking for a quick comparison of key terms, <code>nltk</code> standard functions might suffice. </p>

<p>With <code>nltk</code> you can pull synonyms of your terms by looking up the <strong>synsets</strong> contained by <strong>WordNet</strong></p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet

&gt;&gt;&gt; wordnet.synsets('donation')
[Synset('contribution.n.02'), Synset('contribution.n.03')]

&gt;&gt;&gt; wordnet.synsets('donations')
[Synset('contribution.n.02'), Synset('contribution.n.03')]
</code></pre>

<p>It understands plurals and it also tells you which part of speech the synonym corresponds to</p>

<p><strong>Synsets</strong> are stored in a tree with more specific terms at the leaves and more general ones at the root. The root terms are called <strong>hypernyms</strong></p>

<p>You can measure similarity by how close the terms are to the common <strong>hypernym</strong></p>

<p><em>Watch out for different parts of speech, according to the NLTK cookbook they don't have overlapping paths, so you shouldn't try to measure similarity between them.</em></p>

<p>Say, you have two terms <em>donation</em> and <em>gift</em>, you can get them from <code>synsets</code> but in this example I initialized them directly:</p>

<pre><code>&gt;&gt;&gt; d = wordnet.synset('donation.n.01')
&gt;&gt;&gt; g = wordnet.synset('gift.n.01')
</code></pre>

<p>The cookbook recommends Wu-Palmer Similarity method</p>

<pre><code>&gt;&gt;&gt; d.wup_similarity(g)
0.93333333333333335
</code></pre>

<p>This approach gives you a quick way to determine if the terms used correspond to related concepts. Take a look at <a href=""http://www.nltk.org/book"" rel=""noreferrer"">Natural Language Processing with Python</a> to see what else you can do to help your analysis of text.</p>
",6949,1307364446
Extracting keywords from an article,"<p>I have articles and keywords stored inside MySQL. The site will preprocess the new articles to find how many matching keywords there are and then update a table which stores the relevant keywords related to the article. This will then be used on the front-end by highlighting keywords within the article and will link users to articles with the same matching keywords.</p>

<p>My concern here is how to do this processing efficiently. My idea is: when processing new articles, it finds the ngrams of the text (up to 3- or 4-gram) and then search each against the keywords table in the MySQL database. This may end up being a slow mess, I haven't tried. But maybe I'm approaching this the wrong way?</p>

<p>Any resources on how to do this efficiently would be awesome. Language used here is primarily PHP.</p>
","php, mysql, full-text-search, nlp, text-processing","<p>I've never used PHP to do it, but in .NET, I'll usually do what was alluded to by samxli. I load all keywords into a hashtable.  I've done it with up to 120,000 keywords and it works pretty fast.</p>

<p>The .NET hashtable object has a contains([key]) method.  So for each word in the article I'll just call:</p>

<pre><code>theHashTable.contains(theWord)
</code></pre>

<p>If it does contain the word, I'll index it.  Has worked pretty well for me without having to use other frameworks.  I don't know how hashtables work in PHP.  You'd have to google that.  I think their normal arrays work like hashtables?</p>

<p>The key to using a hashtable is that the keys are indexed for fast searching -- I think they use bTrees, but someone may correct me on that.  If you're not familiar with the btree concept, you might want to look that up.</p>
",855,1307331200
Performance trade-offs for loading DB table into array vs searching within the DB table,"<p>I have a text document that will be loaded into a string and will need to search it's content for matching keywords in a <strong>Keyword</strong> table in MySQL.</p>

<p>Would it be better to load the keywords from MySQL into a PHP array (using the keywords as the key) and then doing searches against that array by iterating through the ngrams of the text? OR would it be better to iterate through the ngrams of the string and then searching each against the MySQL DB (this would lead to many DB queries)?</p>
","php, mysql, regex, arrays, nlp",,183,1307338864
"Programmatically detect the name, location and start time of a social event on any html page","<p>I would love to see an app/browser extension that automatically detects meatspace events on a page by looking for words like location, date, time, etc... and then offers to add these to a calendar</p>

<p>If there is a large enough sample of these occurrences is there anyway to train an app to detect it? Are there any machine learning technique that could help with this? </p>
","architecture, nlp, machine-learning",,116,1307044545
Named entity recognition with Java,"<p>I would like to use named entity recognition (NER) to find adequate tags for texts in a database. Instead of using tools like NLTK or Lingpipe I want to build my own tool.</p>

<p>So my questions are:</p>

<ul>
<li><p>Which algorithm should I use?</p></li>
<li><p>How hard is to build this tool?</p></li>
</ul>
","java, tags, nlp, semantics",,2728,1302116264
nlg building a sentence,"<p>I would like to generate a sentence having as input words.
E.g.</p>

<p>Input:</p>

<pre><code>Mary
chase
the monkey
</code></pre>

<p>Output:</p>

<pre><code>Mary chases the monkey.
</code></pre>

<p>This could be done using a simpleNLG library: <a href=""http://code.google.com/p/simplenlg/"" rel=""noreferrer"">http://code.google.com/p/simplenlg/</a> in the following way:</p>

<pre><code>String subject = ""Mary"";
String verb = ""chase"";
String object = ""the monkey"";

p.setSubject(subject);
p.setVerb(verb);
p.setObject(object);

String output = realiser.realiseSentence(p);
System.out.println(output);
</code></pre>

<p>This will generate the sentence Mary chases the monkey. But I would like to make it automated where I input words and the sentence gets generated. This would require some preprocessing that would specify which word is a subject which word is a verb and which is an object. I know there are POS (parts of speech) tagging libraries but they don't specify whether it is a subject or object. Any suggestions how this could be done? Also for make it work for bigger sentences with multiple objects, adverbs etc.</p>
",nlp,"<p>In order to obtain the subject, verb or object for the input sentence you need to perform syntactic analysis or parsing. </p>

<p>There are two main groups of parsing tools, constituent parsers and dependency parsers, but usually the former is the more direct path to obtain what you need. </p>

<p>These are some research constituent parsers that you may try: </p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">Stanford parser</a> </li>
<li><a href=""http://code.google.com/p/berkeleyparser/"" rel=""nofollow noreferrer"">Berkeley parser</a></li>
<li><a href=""http://code.google.com/p/bubs-parser/"" rel=""nofollow noreferrer"">BUBS parser</a></li>
</ul>

<p>This related question may also help: <a href=""https://stackoverflow.com/questions/5833030/simple-natural-language-processing-startup-for-java"">Simple Natural Language Processing Startup for Java</a></p>
",4311,1307014420
Tagging and Analysing a Search Query,"<p>I'm developing a search engine which functions taking the semantics of data into account, unlike the usual keyword based index. I managed to develop a reasonable index for the search using <strong>metadata extraction methods and RDF</strong>, but I have <strong>difficulty in using such methods on the search query itself since the search query is very much shorter</strong> that the actual data. any idea how to perform a successful tagging of a search query, using similar methods, natural language processing, etc. ?</p>

<p>Thank You!</p>
","search, metadata, nlp, rdf, tagging","<p>Yes, the sample size of a typical query is too small for semantic analysis to be of any value.</p>

<p>One approach might be to constrain or expand your query using drop-down menus for things like ""Named Entities"" or ""Subject Verb Object"" tuples.</p>

<p>Another approach would be to expand simple keywords using rules created from your metadata so that, for example, a query for 'car' might be expanded to the tuple pattern</p>

<pre><code>(*,[drive,operate,sell],[car,automobile,vehicle])
</code></pre>

<p>before submission.</p>

<p>Finally, you might try expanding the query with a non-semantically valuable prefix and/or suffix to get the query size large enough to trigger OpenCalais' recognizer.  </p>

<p>Something like 'The user has specified the following terms in her query: one, two, three.'.</p>

<p>And once the results are returned, filter out all results that match only the added prefix/suffix.</p>

<p>Just a few quick thoughts.</p>
",234,1304996723
1 million sentences to save in DB - removing non-relevant English words,"<p>I am trying to train a Naive Bayes classifier with positive/negative words extracting from a sentiment. example:  </p>

<p>I love this movie :))  </p>

<p>I hate when it rains :(  </p>

<p>The idea is I extract positive or negative sentences based on the emoctions used, but in order to train a classifier and persist it into database.  </p>

<p>The problem is that I have more than 1 million such sentences, so if I train it word by word, the database will go for a toss. I want to remove all non-relevant word example 'I','this', 'when', 'it' so that number of times I have to make a database query is less.  </p>

<p>Please help me in resolving this issue to suggest me better ways of doing it  </p>

<p>Thank you</p>
","database, hadoop, nlp, classification, sentiment-analysis","<p>You might want to check this out
<a href=""http://books.google.com/books?id=CE1QzecoVf4C&amp;lpg=PA390&amp;ots=OHuYwLRhag&amp;dq=sentiment%20%20mining%20for%20fortune%20500&amp;pg=PA379#v=onepage&amp;q=sentiment%20%20mining%20for%20fortune%20500&amp;f=false"" rel=""nofollow"">http://books.google.com/books?id=CE1QzecoVf4C&amp;lpg=PA390&amp;ots=OHuYwLRhag&amp;dq=sentiment%20%20mining%20for%20fortune%20500&amp;pg=PA379#v=onepage&amp;q=sentiment%20%20mining%20for%20fortune%20500&amp;f=false</a></p>
",944,1290533972
Are There Any Good C++ Suffix Trie Libraries?,"<p>Does anyone know of a really rock solid C++ library for suffix tries?  Other than the one in Mummer?<br>
Ideally, I'd like:<br>
Some concept of concurrency.<br>
Good caching behavior.<br>
Permissive license.<br>
Support for arbitrary alphabets.</p>
","c++, algorithm, tree, nlp, trie","<p>Being a bioinformatician, my pick would be <a href=""http://www.seqan.de/"">SeqAn</a> (check out the <a href=""http://trac.mi.fu-berlin.de/seqan/wiki/Tutorial/Indices"">sequence index</a> section). It implements a lazy suffix tree and an enhanced suffix array (an equivalent data structure), both of which have good cache behaviour.</p>
",8068,1306320336
Get central content from a Web Page,"<p>What are the possible ways to get the central content of a web page?</p>

<p>By central content I mean the content which is most important in the page.</p>

<p>Eg: in the web page <a href=""http://techcrunch.com/2011/05/27/iphone-app-notifies-you-when-your-laundrys-done/"" rel=""nofollow"">http://techcrunch.com/2011/05/27/iphone-app-notifies-you-when-your-laundrys-done/</a></p>

<p>the central content would be:</p>

<pre><code>&lt;p&gt;&lt;img src=""http://tctechcrunch.files.wordpress.com/2011/05/screen-shot-2011-05-27-at-10-11-36-pm.png"" alt=""""&gt;&lt;br&gt;
The folks that brought you &lt;a href=""http://itsthisforthat.com/""&gt;It’sthisforthat&lt;/a&gt; have created another way to make your life just a little bit easier and funnier. Meet&amp;nbsp;&lt;a href=""http://www.dryerbro.com""&gt;DryerBro&lt;/a&gt;, an app that uses an accelerometer to let you know when your laundry’s done.&lt;/p&gt;
&lt;p&gt;With DryerBro you put your iPhone or iTouch on your laundry machine and it texts you and the remaining members of your laundry party when your laundry’s done. I’m thinking this is going to be HUGE. I mean Facebook took off at colleges right?&lt;/p&gt;
&lt;p&gt;Once set up, DryerBro uses an accelerometer and Twilio to send a SMS, email or call to multiple phones when your unmentionables are ready to be picked up.&lt;/p&gt;
&lt;p&gt;Says creator Eric Kerr, “We live in a house with 11 dudes, and we’re seriously unorganized about laundry. We all want to use the machine on the weekends, but no one ever knows when the last load was done. It bothered me as hackers that we had the tools (accelerometer, Twilio) to solve the problem, but didn’t do anything about it.”&lt;/p&gt;
&lt;p&gt;So they built DryerBro. “We originally looked to see if an app already used the accelerometer to detect when your laundry is done but we couldn’t find anything – it’s a blue ocean strategy,” he says.&lt;/p&gt;
&lt;p&gt;Kerr and company are completely ridiculous, but their thing apparently works. When asked about future plans for DryerBro he told TechCrunch:&lt;/p&gt;
&lt;p&gt;“Ultimately we want to build out a hyper-local group buying ad platform for laundry detergents. Rough back of the napkin calculations indicate that we’d need roughly $41 million in financing, so we’re asking friends and family to help pony up the dough. We also want to build out the map of every active dryer in the world to hang on the wall of our office.”&lt;/p&gt;
&lt;p&gt;Both the DryerBro&lt;a href=""http://dryerbro.com/""&gt; FAQ&lt;/a&gt; and Promo video are awesome. You can download the iPhone &lt;a href=""http://itunes.apple.com/us/app/dryer-bro/id425920156?mt=8""&gt;app here.&lt;/a&gt;&amp;nbsp;Promo video below.&lt;/p&gt;
&lt;div style=""text-align:center;""&gt;
&lt;object type=""application/x-shockwave-flash"" width=""620"" height=""300"" data=""http://www.vimeo.com/moogaloop.swf?clip_id=20732587&amp;amp;server=www.vimeo.com&amp;amp;fullscreen=1&amp;amp;show_title=1&amp;amp;show_byline=0&amp;amp;show_portrait=0&amp;amp;color=01AAEA""&gt;
&lt;param name=""quality"" value=""best""&gt;
&lt;param name=""allowfullscreen"" value=""true""&gt;
&lt;param name=""scale"" value=""showAll""&gt;
&lt;param name=""movie"" value=""http://www.vimeo.com/moogaloop.swf?clip_id=20732587&amp;amp;server=www.vimeo.com&amp;amp;fullscreen=1&amp;amp;show_title=1&amp;amp;show_byline=0&amp;amp;show_portrait=0&amp;amp;color=01AAEA""&gt;
&lt;param name=""wmode"" value=""opaque""&gt;
&lt;/object&gt;
&lt;/div&gt;
</code></pre>

<p>Any pointers in this regard would be helpful.</p>

<p>Thanks</p>
","parsing, nlp, semantic-markup",,107,1306571206
How to use wordnet to find semantic relation between two web pages?,"<p>I want some way to determine whether two web pages are semantically related.I googled and found something called as the WordNet (a large lexical database). I want to know that how can I accomplish this using python and WordNet? </p>
","python, nlp, wordnet","<p>The easiest and crudest way to do this would be:</p>

<ol>
<li><p>extract top N terms (keywords) from each page (could be as simple as top N terms by frequency, excluding stop words such as 'a, the, an' in English). This will give you a feature set for each page.</p></li>
<li><p>Compare top terms between pages for overlaps. You could use Wordnet to compare synonyms of your terms e.g. sneakers &lt;==> trainers. If you have some degree of keyword overlap then pages are in some way related.</p></li>
</ol>

<p>EDIT:</p>

<p>A better way to derive a feature set of keywords for each page would be to extract statistically significant words for each page. You can do this by acquiring or compiling a list of (1 2 and 3 word)  <a href=""http://en.wikipedia.org/wiki/N-gram"" rel=""nofollow"">n-grams</a> from a reference text e.g. wikipedia) and then computing the n-grams for the words/phrases on your page and comparing the frequency of occurrence of your n-grams with that of those in  the global n-gram set. If you find you have n-grams on your page that occur more frequently than what you would expect given the reference corpus then they are likley to be statistically significant for that page. </p>

<p>The hard part in this is acquiring or compiling  the reference n-gram (it needs to be big enough to be statistically viable) which you ned to compare with the n-grams on your webpages. You can acquire google's n-gram corpus, or possibly build your own by looking at freely available to download websites like wikipedia. Others may have published a freely available n-gram set if you look around on google.</p>
",1871,1306479184
Looking up positivity or negativity in a sentence using WordNet,"<p>I have to ask some questions in an interactive system and user is free to type just anything as response. I have to conclude from response whether it is positive (yes) or negative (no). In some presentation that I attended few months ago I understood that a specific library of wordnet (not wordnet itself) exists that does the exact same job but I have forgot what it was. Does anyone know about it? If not can you suggest some alternative to attain the same result?</p>

<p>Thanks in advance</p>
","c#, nlp, wordnet, sentiment-analysis","<p>Nevermind I recalled it now. It was <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordnet</a>.</p>
",1196,1306331660
How to use reuters-21578 dataset with svm.net for text classification?,"<p>I've just started an application for text classification and I've read lots of papers about this topic, but till now I don't know how to start, I feel like I've not got the whole image. I've got the training dataset and read its description and got a great implementation for SVM algorithm (SVM.Net) but I don't know how to use that dataset with this implementation. I know that I should extract features from the dataset's texts and use these features as input to the SVM so could any body please tell me about a detailed tutorial about how to extract text's features and use them as input to the SVM algorithm, and then use this algorithm to classify a new text?
And if there is a full example about using SVM for text classification, that's would be great.</p>

<p>Any help would be appreciated.
Thanks in advance.</p>
","nlp, machine-learning, svm, document-classification","<p>Creating features for text classification can be as complex as you want it to be.</p>

<p>A simple approach is to just map each distinct term to a feature index.  You then represent each document as a vector of the frequencies of each term.  (You can remove stop words, weight terms etc etc).  For text classification you would also assign each vector with the label.</p>

<p>For example, if the document was the sentence:</p>

<pre><code>John loves Mary
</code></pre>

<p>with a label ""spam"".</p>

<p>Then you might have the following mapping:</p>

<pre><code>John : 1
loves: 2
Mary: 3
</code></pre>

<p>Your vector then becomes:</p>

<pre><code>1 1 2 1 3 1
</code></pre>

<p>(I has assumed that each feature has a weight of one)</p>

<p>I don't know about SVM.NET, but most supervised machine learning methods will accept vector-based input.</p>
",1795,1306154387
Wordnet database has letters in weird/invalid places,"<p>I was noticing that some lines in the <a href=""http://wordnet.princeton.edu/wordnet/man/wndb.5WN.html"" rel=""nofollow"">database files</a> (like data.verb) are not following the correct format. (The database format is <a href=""http://wordnet.princeton.edu/wordnet/man/wndb.5WN.html"" rel=""nofollow"">outlined here</a>).</p>

<pre><code>02286687 40 v 0a fall_upon d strike 0 come_upon 9 light_upon 0 chance_upon 0 come_across 2 chance_on 0 happen_upon 0 attain d discover 0 003 @ 02285629 v 0000 + 07214432 n 0a01 + 00043195 n 0a01 01 + 08 00 | find unexpectedly; ""the archeologists chanced upon an old tomb""; ""she struck a goldmine""; ""The hikers finally struck the main path to the lake"" 
</code></pre>

<p>Where the w_cnt <strong>0a</strong> should be a the number 10. This also happens in other places like:</p>

<pre><code>02575723 41 v 08 flim-flam 0 play_a_joke_on 1 play_tricks 0 trick 0 fob 0 fox 0 pull_a_fast_one_on 0 play_a_trick_on 0 008 @ 02575082 v 0000 + 10022759 n 0602 + 00171618 n 0401 + 10463714 n 0404 + 06760722 n 0401 + 00752954 n 0401 + 00779248 n 010c ~ 02578384 v 0000 02 + 09 00 + 30 04 | deceive somebody; ""We tricked the teacher into thinking that class would be cancelled next week""  
</code></pre>

<p>Where <strong>010c</strong> isn't a valid number. Unless [digit][letter] is a valid format, <em>but is not described in the documentation I have read so far</em>.</p>

<p>Why are their random letters among the numbers?</p>
","nlp, text-processing, wordnet","<p>Looks like the numbers are in <a href=""http://en.wikipedia.org/wiki/Hexadecimal"" rel=""nofollow"">hexadecimal format</a> - A is 10, for example.</p>
",53,1306111121
"Redefining &quot;sentence&quot; in Emacs? (single space between sentences, but ignoring abbreviations)","<p>I would like to be able to navigate by sentence in Emacs (M-a, M-e). Here's the problem: by default, Emacs expects that each sentence is separated by two spaces, and I'm used to just putting a single space. Of course, that setting can be turned off, to allow for sentences separated by only a single space, like so:</p>



<pre class=""lang-lisp prettyprint-override""><code>(setq sentence-end-double-space nil)   
</code></pre>

<p>But then Emacs thinks that a sentence has ended after abbreviations with a full stop ("".""), e.g. after something like ""...a weird command, e.g. foo..."".</p>

<p>So rather than using the above code, is there a way to define the      sentence-end variable so that it counts [.!?] as marking the end of the sentence, iff what follows is one or more spaces followed by a capital letter [A-Z]?</p>

<p>And...to also allow [.!?] to mark the end of a sentence, if followed by zero or more spaces followed by a ""\""? [The reason for this latter condition is for writing LaTeX code: where a sentence is followed by a LaTeX command like \footnote{}, e.g. ""...and so we can see that the point is proved.\footnote{In some alternate world, at least.}""] </p>

<p>I tried playing around with the definition of sentence-end, and came up with:</p>



<pre class=""lang-lisp prettyprint-override""><code>(setq sentence-end ""[.!?][]'\"")}]*\\(\\$\\|[ ]+[A-Z]\\|[ ]+[A-Z]\\| \\)[
 ;]*"")
</code></pre>

<p>But this doesn't seem to work at all.</p>

<p>Any suggestions? </p>
","regex, emacs, nlp, typography, punctuation","<p>I don't think sentence-end will do what you need it to do. You really need look-ahead regexps for this, and Emacs doesn't support them. </p>

<p>You can roll your own function to do what you need though. I don't understand all of your requirements, but the following is a start:</p>

<pre><code>(defun my-next-sentence ()
""Move point forward to the next sentence.
Start by moving to the next period, question mark or exclamation.
If this punctuation is followed by one or more whitespace
characters followed by a capital letter, or a '\', stop there. If
not, assume we're at an abbreviation of some sort and move to the
next potential sentence end""
  (interactive)
  (re-search-forward ""[.?!]"")
  (if (looking-at ""[    \n]+[A-Z]\\|\\\\"")
      nil
    (my-next-sentence)))

(defun my-last-sentence ()
  (interactive)
  (re-search-backward ""[.?!][   \n]+[A-Z]\\|\\.\\\\"" nil t)
  (forward-char))
</code></pre>

<p>Most of your tweaking will need to focus on the looking-at regexp, to make sure it hits all the potential end-of-sentence conditions you need. It would be relatively easy to modify it to move the cursor to particular locations based on what it finds: Leave it be if it's a normal sentence, move past the next { if you're at a latex command, or whatever suits you.</p>

<p>Once you've got that working, can bind the functions to a M-a and M-e, probably using mode-hooks unless you want to use them for every mode.</p>
",2629,1295573261
Looking for a way to check if a word is pronounceable,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/1186213/measure-the-pronounceability-of-a-word"">Measure the pronounceability of a word?</a>  </p>
</blockquote>



<p>There are a lot of pronounceable random password generators.
I am looking for the reverse.
I like to know if a given word is pronounceable.</p>

<p>Purpose:
I am looking for a new domain name, you probably have gone though this as well.</p>
","php, string, nlp",,2028,1305840377
"Python or Java for text processing (text mining, information retrieval, natural language processing)","<p>I'm soon to start on a new project where I am going to do lots of text processing tasks like searching, categorization/classifying, clustering, and so on. </p>

<p>There's going to be a huge amount of documents that need to be processed; probably millions of documents. After the initial processing, it also has to be able to be updated daily with multiple new documents.</p>

<p>Can I use Python to do this, or is Python too slow? Is it best to use Java?</p>

<p>If possible, I would prefer Python since that's what I have been using lately. Plus, I would finish the coding part much faster. But it all depends on Python's speed. I have used Python for some small scale text processing tasks with only a couple of thousand documents, but I am not sure how well it scales up.</p>
","java, python, nlp, information-retrieval, text-mining",,10802,1305632779
corenlp package of stanford&#39;s StaggerDemo,"<p>I wanna make a program automatically tagger the text in a directory. Here's my first step.
I made a little change to TaggerDemo.java. But it is not working properly as expected.</p>

<pre><code>import java.io.BufferedReader;
import java.io.FileReader;
import java.util.ArrayList;
import java.util.List;

import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.Sentence;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;

class auto{

  public static void main (String[] args) throws Exception{

    MaxentTagger tagger = new MaxentTagger(""models/left3words-wsj-0-18.tagger"");
    @SuppressWarnings(""unchecked"")
    List&lt;List&lt;HasWord&gt;&gt; sentences = tagger.tokenizeText(new BufferedReader(new FileReader(args[0])));
    for (List&lt;HasWord&gt; sentence : sentences) {
      ArrayList&lt;TaggedWord&gt; tSentence = tagger.tagSentence(sentence);
      System.out.println(Sentence.listToString(tSentence, false));
    }
  }

}
</code></pre>

<p>This is the error i got.</p>

<pre><code>Loading default properties from trained tagger models/left3words-wsj-0-18.tagger
Reading POS tagger model from models/left3words-wsj-0-18.tagger ... done [2.9 sec].
Exception in thread ""main"" java.io.FileNotFoundException: sample-input.txt (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:106)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:66)
    at java.io.FileReader.&lt;init&gt;(FileReader.java:41)
    at auto.main(auto.java:17)
</code></pre>

<p>Why it says file not found?</p>

<p>When i tried to compile it under terminal, it says edu.stanford.nlp.ling.* can not be imported...</p>

<p>Thanks a lot.</p>
","java, nlp, stanford-nlp","<p>It seems that you don't have the file <code>sample-input.txt</code> in the current directory.</p>

<p>To see your current directory, use:</p>

<pre><code>File f = new File(""."");
System.out.println(f.getAbsolutePath());
</code></pre>
",470,1305546569
Text Game Commands Parsing?,"<p>I'm interested in writing a parsing engine such as the ones in Inform, which is very robust.</p>

<p>I've tried a couple of different approach: </p>

<ul>
<li>regex. For regex, I can only match <code>{verb} {noun1 optional-noun2} {optional-preposition} {indirect-noun1 optional-indirect-noun2}</code> This doesn't work that well.</li>
<li>list of predefined verbs and their matched preposition and nouns. This approach is too complex, since there has to be A LOT of predefined data.</li>
<li>splitting into different words and run them through a dictionary maps. Again this is too complex and not practical due to the amiguity of certain words such as ""look"" or ""put (on, out)""</li>
</ul>

<p>I know about language processing though I have actually no idea how to start and some of the stuff are too complicated. (No CS background, I'm just learning on my own right now).</p>

<p>Some ideas I have:</p>

<ul>
<li>As I've said, natural language processing.</li>
<li>Design a parser that parses something similar to SQL.</li>
</ul>

<p>Is there any other ideas? How would I go about design and implementing them?</p>

<p>Note I'm using Python as my language. and in case you missed it, I'm not design a programming language, I'm just parsing user command such as ""go north"" or complex stuff, like ""put the fire out with the blanket""</p>
","python, nlp","<p>Besides the mentioned <a href=""http://pyparsing.wikispaces.com/"" rel=""nofollow""><code>pyparsing</code></a>, which looks very appropriate (I've only used it a tiny bit), you should look into using <a href=""http://www.nltk.org/"" rel=""nofollow""><code>nltk</code></a>. It's a great collection of tools for natural language processing, and there's even a <a href=""http://www.nltk.org/book"" rel=""nofollow"">book</a>.</p>
",1242,1305500636
how do I change hungggrrrrryyyy = hungry and other such english words,"<p>The problem is simpler to understand, but I think is difficult to solve. Given a word how to form a proper english word. example:   </p>

<pre><code>hunggrrryyy to hungry  
awweeeeseom to awesome  
frusstrated to frustrated  
looooooove  to love  
</code></pre>

<p>Are there any known solutions to such problem?  </p>

<p>Thanks</p>
",nlp,"<p>You probably could use a similar tack to what Peter Norvig did with <a href=""http://norvig.com/spell-correct.html"" rel=""nofollow"">this spell checker</a>.</p>

<p>At the very least you'll need some context checking.  Does ""loooooooooser"" map to ""looser"" or ""loser"" within a particular phrase?</p>
",285,1292272894
How to prioritize internationalization parameters,"<p>given that these all have different values:</p>

<ul>
<li><p>HTTP browser accept language header
parameter </p></li>
<li><p>HTTP GET human language parameter eg. hl=en or hl=fr </p></li>
<li><p>Cookie value for language choice</p></li>
</ul>

<p>How should we decide which language to display pages in if deciding based on these values? It's also thinkable saving user's preferred language to the data layer for a fourth way to let agents and users decide language. 
Thanks in advance for answers and comments</p>
","python, django, google-app-engine, internationalization, nlp","<p>If you have a saved preference somewhere, then that would be the first choice.</p>

<p>The cookie value is, presumably, what they chose last time they were around so that would be the first thing to check.</p>

<p>The <code>hl</code> parameter is something that Google has figured out and they probably know what they're doing so that seems like a sensible third choice.</p>

<p>Then we have the HTTP headers or a final default so check the accept language header next. And finally, have a default language in place just in case all else fails.</p>

<p>So, in order:</p>

<ol>
<li>Saved preference.</li>
<li>Cookie.</li>
<li><code>hl</code> parameter.</li>
<li>HTTP accept language header.</li>
<li>Built in default.</li>
</ol>

<p>Ideally you'd backtrack up the list once you get a language from somewhere so that you'd have less work to do on the next request. For example, if you ended up getting the language from the accept language header, you'd want to: set <code>hl</code> (possibly redirecting), store it in the cookie, and save the preference in their user settings (if you have such a permanent store and a signed in person).</p>
",147,1305440356
Determine the positivenes or negativeness of a generic statement,"<p>I want to know if a twit is positive or negative.
 For example:</p>

<p>thesis: jonas brothers eat charcoal
""Jonas Brothers are going to eat charcoal"" >>> Positive</p>

<p>""Jonas Brothers have nothing to do with charcoal"" >>> Negative</p>

<p>thesis: melmac is destroyed 
""Melmac is dead"" >>> positive</p>

<p>""Alf is living with his friends in melmac"" >>> negative</p>

<p>I want to know if there's some kind of algorithm to do this, on a generic, non languaje specific basis.</p>

<p>What's the easiest approach?
Thanks in advance.</p>
","text, artificial-intelligence, logic, nlp, syntax-checking",,118,1305351726
"In NLTK pos_tag, why &quot;hello&quot; is classified as Noun?","<p>I've tried</p>

<pre><code>text = nltk.word_tokenize(""hello, my name is John"")
words = nltk.pos_tag(text)

for w in words:
    print ""%s = %s"" % (w[0], w[1])
</code></pre>

<p>And I got:</p>

<pre><code>hello = NN
, = ,
my = PRP$
name = NN
is = VBZ
John = NNP
</code></pre>
","python, nlp, nltk","<p>According to the Penn Treebank tagset, <em>hello</em> is definitely an interjection and is consistently tagged UH.  The problem you're running into is that the taggers that NLTK ships with were most likely trained on the part of the Wall Street Journal section of the Penn Treebank that is available for free, which unfortunately for you contains zero occurrences of the word <em>hello</em> and only three words tagged UH (interjection). If you want to tag spoken text, you'll need to train your tagger on the whole Penn Treebank, which includes something like 3 million words of spoken English.</p>

<p>By the way, the NLTK taggers won't always call <em>hello</em> a noun -- try tagging ""don't hello me!"" or ""he said hello"".</p>
",1941,1304912016
How to classify words to their correspoding categories?,"<p>I've to implement text classification for a long list of words. I've some categories defined e.g. If the word ""UK"" is in the list, it will come under ""Regions"". If the word is ""Pizza"", it will come under category ""food"".</p>

<p>How can I classify the words under different categories? Is there any open source tool available to do that? </p>
","java, nlp, classification, ontology, text-mining","<p>I'm not entirely sure what you're trying to do, but if what you want is to build up a list representative words for a number of categories then you could do this by selecting the top N most frequent words, excluding <a href=""http://en.wikipedia.org/wiki/Stop_words"" rel=""nofollow"">stop words</a>, from a set of documents representative of each category. This is an easy way of creating a very basic ontology.</p>

<p>For example, to create a set of words about <strong>food</strong> you could crawl the web for <strong><a href=""http://allrecipes.co.uk/"" rel=""nofollow"">recipies</a> and <a href=""http://www.just-eat.co.uk/"" rel=""nofollow"">menus</a></strong> and then select the most frequent words from these. I'd expect that once you have excluded stop words you'll have a good list of food related words.  For words related to <strong>programming</strong> you could crawl <strong>stackoverflow.com</strong>, etc etc...</p>

<p>Then again, this may not be what you're trying to do...</p>
",2720,1296414144
Indexing and Searching Over Word Level Annotation Layers in Lucene,"<p>I have a data set with multiple layers of annotation over the underlying text, such as <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""noreferrer"">part-of-tags</a>, <a href=""http://www.cnts.ua.ac.be/conll2000/chunking/"" rel=""noreferrer"">chunks from a shallow parser</a>, <a href=""http://en.wikipedia.org/wiki/Named_entity_recognition"" rel=""noreferrer"">name entities</a>, and others from various  <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""noreferrer"">natural language processing</a> (NLP) tools. For a sentence like <code>The man went to the store</code>, the annotations might look like:</p>

<pre>

Word  POS  Chunk       NER
====  ===  =====  ========
The    DT     NP    Person     
man    NN     NP    Person
went  VBD     VP         -
to     TO     PP         - 
the    DT     NP  Location
store  NN     NP  Location
</pre>

<p>I'd like to index a bunch of documents with annotations like these using Lucene and then perform searches across the different layers. An example of a simple query would be to retrieve all documents where <strong>Washington</strong> is tagged as a <strong>person</strong>. While I'm not absolutely committed to the notation, syntactically end-users might enter the query as follows:</p>

<p><strong>Query</strong>: <code>Word=Washington,NER=Person</code> </p>

<p>I'd also like to do more complex queries involving the <strong>sequential order of annotations</strong> across different layers, e.g. find all the documents where there's a word tagged <strong>person</strong> followed by the words <strong><code>arrived at</code></strong> followed by a word tagged <strong>location</strong>. Such a query might look like:</p>

<p><strong>Query</strong>: <code>""NER=Person Word=arrived Word=at NER=Location""</code></p>

<p>What's a good way to go about approaching this with Lucene? Is there anyway to index and search over document fields that contain structured tokens?</p>

<p><strong>Payloads</strong></p>

<p>One suggestion was to try to use Lucene <a href=""http://lucene.apache.org/java/2_9_2/api/all/org/apache/lucene/search/payloads/package-summary.html"" rel=""noreferrer"">payloads</a>. But, I thought payloads could only be used to adjust the rankings of documents, and that they aren't used to select what documents are returned. </p>

<p>The latter is important since, for some use-cases, the <strong>number of documents</strong> that contain a pattern is really what I want.</p>

<p>Also, only the payloads on terms that match the query are examined. This means that <strong>payloads could only even help with the rankings of the first example query</strong>, <code>Word=Washington,NER=Person</code>, whereby we just want to make sure the term <strong><code>Washingonton</code></strong> is tagged as a <strong><code>Person</code></strong>. However, for the second example query,  <code>""NER=Person Word=arrived Word=at NER=Location""</code>, I need to check the tags on unspecified, and thus non-matching, terms.   </p>
","java, lucene, nlp, data-mining, text-mining",,1385,1274452652
Book translation data format,"<p>I'm thinking of translating a book from English to my native language. I can translate just fine, and I'm happy with <code>vim</code> as a text editor. My problem is that I'd like to somehow preserve the semantics, i.e. which parts of my translation correspond to the original.</p>

<p>I could basically create a simple XML-based markup language, that'd look something like</p>

<pre><code>&lt;book&gt;
  &lt;chapter&gt;
    &lt;paragraph&gt;
      &lt;sentence&gt;
        &lt;original&gt;This is an example sentence.&lt;/original&gt;
        &lt;translation lang=""fi""&gt;Tämä on esimerkkilause.&lt;/translation&gt;
      &lt;/sentence&gt;
    &lt;/paragraph&gt;
  &lt;/chapter&gt;
&lt;/book&gt;
</code></pre>

<p>Now, that would probably have its benefits but I don't think editing that would be very fun.</p>

<p>Another possibility that I can think of would be to keep the original and translation in separate files. If I add a newline after each translation chunk and keep line numbering consistent, editing would be easy and I'd be able to programmatically match the original and translation.</p>

<pre><code>original.txt:
  This is an example sentence.
  In this format editing is easy.

translation-fi.txt:
  Tämä on esimerkkilause.
  Tässä muodossa muokkaaminen on helppoa.
</code></pre>

<p>However, this doesn't seem very robust. It would be easy to mess up. Probably someone has better ideas. Thus the question:</p>

<p><strong>What would be the best data format for making a book translation with a text editor?</strong></p>

<p>EDIT: added tag <code>vim</code>, since I'd prefer to do this with vim and believe that some vim guru might have ideas.</p>

<p>EDIT2: started a bounty on this. I'm currently leaning to the second idea I describe, but I hope to get something about as easy to edit (and quite easy to implement) but more robust.</p>
","vim, nlp, translation, file-format","<p>One thought: if you keep each translatable chunk (one or more sentences) in its own line, vim's option <code>scrollbind</code>, <code>cursorbind</code> and a simple vertical split would help you keeping the chunks ""synchronized"". It looks very much like to what vimdiff does by default. The files should then have the same amount of lines and you don't even need to switch windows!</p>

<p>But, this isn't quite perfect because wrapped lines tend to mess up a little bit. If your translation wraps over two or three more virtual lines than the original text, the visual correlation fades as the lines aren't one-on-one anymore. I couldn't find a solution or a script for fixing that behavior. </p>

<p>Other suggestion I would propose is to interlace the translation into the original. This approaches the diff method of Benoit's suggestion. After the original is split up into chunks (one chunk per line), I would prepend a <code>&gt;&gt;</code> or similar on every line. A translation of one chunk would begin by <code>o</code>. The file would look like this:</p>

<pre><code>  &gt;&gt; This is an example sentence.
  Tämä on esimerkkilause.
  &gt;&gt; In this format editing is easy.
  Tässä muodossa muokkaaminen on helppoa.
</code></pre>

<p>And I would enhance the readability by doing a <code>:match Comment /^&gt;&gt;.*$/</code> or similar, whatever looks nice with your colorscheme. Probably it would be worthwhile to write a <code>:syn</code> region that disables spell checking for the original text. Finally, as a detail, I'd bind <code>&lt;C-j&gt;</code> to do <code>2j</code> and <code>&lt;C-k&gt;</code> to <code>2k</code> to allow easy jumping between the parts that matter.</p>

<p>Pros for this latter approach also include that you could wrap things in 80 columns if you feel like I do :) It would still be trivial to write <code>&lt;C-j/k&gt;</code> to jump between translations.</p>

<p>Cons: buffer-completion suffers as now it completes both original and translated words. English words don't hopefully occur in the translations that often! :) But this is as robust as it gets. A simple <code>grep</code> will peel the original text off after you are done.</p>
",329,1301480713
Where can I download tagged word dictionary and rule?,"<p>I am learning to tag part of speech by applying transformational rules. The first step is to tag the possible POS to each word in a text by using a dictionary like:</p>

<pre><code>communicative JJ
communicator NN
communicators NNS
communion NN
communique NN
communiques NNS
communism NN
</code></pre>

<p>The second step is to apply transformational rules to change tags. I have only a very small dictionary containing the above word/tag pairs. Where can I find a large one and where can I find transformational rules? It is said tagging based on transformation may have a lot of rules. Where can I find the rules?</p>

<p>Thank you in advance.</p>
","cpu-word, nlp, tagging","<p>You'd obtain the possibilities from a corpus, such as those available in <a href=""http://nltk.org"" rel=""nofollow"">NLTK</a>. That would also give you frequencies from which to estimate probabilities, if you want to do machine-learned tagging (<a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Brill_tagger"" rel=""nofollow"">Brill</a>-style).</p>

<p>The rules must be handcrafted, after which the machine learner can find out when to apply which ones. See, e.g., <a href=""http://ucrel.lancs.ac.uk/acl/H/H92/H92-1022.pdf"" rel=""nofollow"">Brill's PhD thesis</a> for English rules.</p>
",229,1304690184
What is the effective method to handle word contractions using Java?,"<p>I have a list of words in a file. They might contain words like who's, didn't etc. So when reading from it I need to make them proper like ""who is"" and ""did not"". This has to be done in Java. I need to do this without losing much time.</p>

<p>This is actually for handling such queries during a search that uses solr.</p>

<p>Below is a sample code I tried using a hash map</p>

<pre><code>Map&lt;String, String&gt; con = new HashMap&lt;String, String&gt;();
        con.put(""'s"", "" is"");
        con.put(""'d"", "" would"");
        con.put(""'re"", "" are"");
        con.put(""'ll"", "" will"");
        con.put(""n't"", "" not"");
        con.put(""'nt"", "" not"");

        String temp = null;
        String str = ""where'd you're you'll would'nt hello"";

        String[] words = str.split("" "");
        int index = -1 ;
        for(int i = 0;i&lt;words.length &amp;&amp; (index =words[i].lastIndexOf('\''))&gt;-1;i++){
            temp = words[i].substring(index);
            if(con.containsKey(temp)){
                 temp = con.get(temp);
            }
            words[i] = words[i].substring(0, index)+temp;
            System.out.println(words[i]);           
        }
</code></pre>
","java, solr, nlp","<p>If you are worried about queries containing for eg ""who's"" finding documents containing for eg ""who is"" then you should look at using a Stemmer, which is designed exactly for this purpose.</p>

<p>You can easily add a stemmer buy configuring it as a filter in your solr config. See <a href=""http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters"" rel=""nofollow"">http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters</a></p>

<p>Edit:<br>
A SnowballPorterFilterFactory will probably do the job for you.</p>
",1910,1304681643
Simple language identification using LINQ,"<p>I'm experimenting with LINQ for the first time and decided to try basic human language identification. The input text gets tested against <code>HashSet</code>s of the most common 10,000 words in the language and receives a score.</p>

<p>My question is, is there a better approach to the LINQ query? Maybe the other form that I don't know? It works, but I'm sure that the experts here will be able to provide a much cleaner solution!</p>

<pre><code>public PolyAnalyzer() {
    Dictionaries = new Dictionary&lt;string, AbstractDictionary&gt;();
    Dictionaries.Add(""Bulgarian"", new BulgarianDictionary());
    Dictionaries.Add(""English"", new EnglishDictionary());
    Dictionaries.Add(""German"", new GermanDictionary());
    Dictionaries.Values.Select(n =&gt; new Thread(() =&gt; n.LoadDictionaryAsync())).ToList().ForEach(n =&gt; n.Start());            
}  

public string getResults(string text) {
    int total = 0;
    return string.Join("" "",
        Dictionaries.Select(n =&gt; new {
            Language = n.Key,
            Score = new Regex(@""\W+"").Split(text).AsQueryable().Select(m =&gt; n.Value.getScore(m)).Sum()
        }).
        Select(n =&gt; { total += n.Score; return n; }).
        ToList().AsQueryable(). // Force immediate evaluation
        Select(n =&gt;
        ""["" + n.Score * 100 / total + ""% "" + n.Language + ""]"").
        ToArray());
}
</code></pre>

<p>P.S. I'm aware that this is an extremely simplistic approach to language identification, I'm just interested in the LINQ side of things.</p>
","c#, linq, lambda, functional-programming, nlp","<p>I would refactor it like this:</p>

<pre><code>    public string GetResults(string text)
    {
        Regex wordRegex = new Regex(@""\W+"");
        var scores = Dictionaries.Select(n =&gt; new
            {
                Language = n.Key,
                Score = wordRegex.Split(text)
                                 .Select(m =&gt; n.Value.getScore(m))
                                 .Sum()
            });

        int total = scores.Sum(n =&gt; n.Score);
        return string.Join("" "",scores.Select(n =&gt; ""["" + n.Score * 100 / total + ""% "" + n.Language + ""]"");
    }
</code></pre>

<p>A few points:</p>

<ol>
<li><p>The <code>AsQueryAble()</code> are unnecessary -
this is all Linq to Objects, which
is <code>IEnumerable&lt;T&gt;</code> - good enough.</p></li>
<li><p>Removed a few <code>ToList()</code> - also
unnecessary and avoids eager loading
of results when not needed.</p></li>
<li><p>While its nice having just one LINQ
query <strong>it's not a competition</strong> - aim
for readability overall and think about how
you (and others) have to maintain the code. I split up your query into three more readable (imo) parts.</p></li>
<li><p><strong>Avoid side effects</strong> by all means
possible - I removed the one you had
to the variable <code>total</code> - it's
confusing - LINQ queries shouldn't
have side effects, because running the same query twice might yield different results. In your case you can just calculate the total in a separate Linq query.</p></li>
<li><p><strong>Don't re-new or re-calculate variables inside a Linq
projection if not necessary</strong> - I
removed the regex from the Linq
query and initialized the variable
once outside - otherwise you are
re-newing the Regex instance <code>N</code> times
instead of just once. This might have huge performance implications depending on the query.</p></li>
</ol>
",611,1304645510
Python NLTK code snippet to train a classifier (naive bayes) using feature frequency,"<p>I was wondering if anyone could help me through a code snippet that demonstrates how to train Naive Bayes classifier using a feature frequency method as opposed to feature presence.</p>

<p>I presume the below as shown in Chap 6 <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html#document-classify-all-words"" rel=""nofollow noreferrer"">link text</a> refers to creating a featureset using Feature Presence (FP) -</p>

<pre><code>def document_features(document): 
    document_words = set(document) 

    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in document_words)

    return features
</code></pre>

<p>Please advice</p>
","python, nlp, nltk, stanford-nlp",,8224,1264777100
Is there a way to convert a natural language date NSString to an NSDate,"<p>Say I have the NSString <code>@""tomorrow""</code></p>

<p>Is there any library that takes strings such as this and converts them into NSDates? I'm imagining/hoping for something like this:</p>

<pre><code>NSString* humanDate = @""tomorrow at 4:15"";
NSDateFormatter *dateFormatter = [[NSDateFormatter alloc] init];
[dateFormatter setDateFormat:@""x at HH:MM""];
NSDate* date = [dateFormatter dateFromString:humanDate];
</code></pre>

<p>I would also want to do things like ""Next monday"", etc. but it doesn't have to be super sophisticated. I can enforce rules on input, but I'd like a little bit of natural language available.</p>

<p>My alternative is to take the string, break it up into pieces, and format it manually. But I was hoping someone has already done this.</p>
","objective-c, ios, parsing, nlp, nsdate","<p>Do you mean something like <a href=""http://developer.apple.com/library/mac/documentation/Cocoa/Reference/Foundation/Classes/NSDate_Class/Reference/Reference.html#//apple_ref/occ/clm/NSDate/dateWithNaturalLanguageString%3a"" rel=""nofollow"">dateWithNaturalLanguageString:</a>?</p>

<p>From the link:</p>

<blockquote>
  <p>dateWithNaturalLanguageString:</p>
  
  <p>Creates and returns an NSDate object set to the date and time specified by a given string.</p>
  
  <p>+ (id)dateWithNaturalLanguageString:(NSString *)string</p>
  
  <p>A string that contains a colloquial specification of a date, such as “last Tuesday at     dinner,” “3pm December 31, 2001,” “12/31/01,” or “31/12/01.”</p>
</blockquote>
",2082,1304483143
Best path to get into natural language processing,"<p>Currently I'm a PHP programmer and I would like to know the best way to learn about NLP, from theory to practice. Doesn't matter the language.</p>

<p>For example:</p>

<p><strong>Theory</strong> </p>

<ol>
<li>Firt learn the Basic grammar</li>
<li>Then learn about first order logic, Description logic etc.</li>
</ol>

<p><strong>Technical</strong></p>

<ol>
<li>Learn PROLOG</li>
<li>Learn about openCyc</li>
<li>For web applications you can use prolog and python with the library pylog.</li>
</ol>

<p>Is it possible to create a topic based tutorial?</p>
",nlp,"<p>Get a good book. I recommend <em>Speech and Language Processing</em> by Jurafsky and Martin. Learn Java, or better, Python, and one or more of the many <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/List_of_natural_language_processing_toolkits"" rel=""noreferrer"">toolkits</a> for NLP.</p>

<p>If you want Prolog, try <a href=""http://www.mtome.com/Publications/PNLA/pnla.html"" rel=""noreferrer""><em>Prolog and Natural Language Analysis</em></a>, but do realize that the logic-oriented approach has almost entirely gone out of favor in the last two decades because it proved too brittle and too expensive. Statistical methods and machine learning are everything now, and approaches that combine logic and statistics are really cutting-edge research.</p>
",2524,1304412407
Is string.GetHashCode() sufficent for word dictionaries in nlp?,"<p>can I use the <code>string.GetHashCode()</code> function for storing stemming data reliably?</p>
","c#, nlp","<p>You can use string.GetHashCode to balance a hash table. That's all it is designed to be used for, and that's all it should be used for.</p>

<p>In particular, the documentation specifically states that you should under no circumstances persist hashcodes to permanent storage; the string hash code algorithm is permitted to change at any time, and in fact has done so in the past.</p>
",218,1304233068
What is the most accurate open-source tool for sentence splitting?,"<p>I need to split text into sentences. I'm currently playing around with OpenNLP's sentence detector tool. I've also heard of NLTK and Stanford CoreNLP tools. What is the most accurate English sentence detection tools out there? I don't need too many NLP features--only a good tool for sentence splitting/detection.</p>

<p>I've also heard about Lucene...but that may be too much. But if it has a kick-ass sentence detection module, then I'll use it.</p>
","parsing, nlp, tokenize",,2375,1300121296
find key words when keys are pressed,"<p>I am looking for a .net implementation of a text analyzer. The text analyzing should happen after each character is entered. the analyzer should be able to store several key phrases and fire an event whenever one of the phrases are entered. For example if the searched phrase is ""Hello world"" the analyzer should fire an event on ""123Hello world"" immediately after 'd' is typed, but it wont fire an event if ""Helloworld"" is typed.</p>
",".net, algorithm, text-analysis","<p><a href=""http://en.wikipedia.org/wiki/Trie"" rel=""nofollow"">binary trees</a> are your friends
and if it is not obvious enough - make a trie of all your options, follow the nodes on keyUp, when reached a leaf - fire an event.</p>
",46,1303893708
python re.compile strings with vars and numbers,"<p>Hi I want to get a match for the following:</p>

<p><code>test = re.compile(r' [0-12](am|pm) [1-1000] days from (yesterday|today|tomorrow)')</code></p>

<p>with this match:</p>

<p><code>print test.match("" 3pm 2 days from today"")</code></p>

<p>It returns none, what am i doing wrong? I am just getting into regex and reading the docs I thought this should work! ANY HELP APPRECIATED
chrism</p>

<h2>--------------------------------------------------------------------------------------</h2>

<p>I am asking a new question about the design of a sytem using similar process to above in NLP <a href=""https://stackoverflow.com/questions/5789126/pyton-re-match-findall-or-search-and-then-nlp-what-to-do-with-it"">HERE</a></p>
","python, regex, nlp",,6133,1303606907
C++ Get a function pointer or eval a function using the functions name stored in a string,"<p>I am working on a parser for natural language. Words from natural language are parsed to the concepts that they represent. The purpose is to allow a robot to interpret commands given in natural language. One way to execute these commands would be by creating an enormous amount of if statements, which would turn into at east one for each concept. I was wondering if there was a way to retrieve a function pointer to a function whose name is stored as string. Each concept could then contain a string that represented the function that would need to executed if the concept is present in the parse. This way the concept would already know what to do.</p>
","c++, nlp, function-pointers",,482,1303789470
Is there a library or web service that provides pronunciations for text?,"<p>Is there a library or web service that can tell you the pronunciation of a string? I'm thinking of character-based languages, where the pronunciation of the word is not apparent from how it's written.</p>
","nlp, multilingual, text-to-speech","<p>Check out the Wordnik API:</p>

<p><a href=""http://developer.wordnik.com/"" rel=""noreferrer"">http://developer.wordnik.com/</a></p>

<p>Free API key will get pronunciations, definitions, and a whole lot more. Easy to use, and the API documentation site is interactive and lets you get your feet wet quickly. Lots of wrappers in various languages.</p>

<p>Disclosure: I work there and love it.</p>
",3358,1289439349
Information extraction. Counting mentions to measure relevance,"<p>Is it possible to count how many times an entity has been mentioned in an article? For example</p>

<blockquote>
  <p><strong><code>ABC Company</code></strong> is one of the largest car manufacturers in the
  world. <strong><code>It</code></strong> is also the largest
  company in terms of annual production.
  <strong><code>It</code></strong> is also the second largest exporter of luxury cars, after XYZ
  company. Both <strong><code>ABC</code></strong> and XYZ
  together produces over n% of total car
  production in the country.</p>
</blockquote>

<p>mentions ABC company 4 times.</p>
","nlp, information-extraction, named-entity-recognition","<p>Yes, this is possible. It's a combination of</p>

<ul>
<li>named-entity recognition (NER), which for English is practically a <a href=""http://nlp.stanford.edu/ner/index.shtml"" rel=""nofollow"">solved problem</a>, and</li>
<li>coreference resolution, which is the subject of ongoing research (but give <a href=""http://cogcomp.cs.illinois.edu/page/software_view/18"" rel=""nofollow"">this package</a> a try)</li>
</ul>
",169,1303192574
converting from treebank tags to wordnet-compatible tags in Java?,"<p>I have POS tagged input from OpenNLP...i need to use these with WordNet...but wordnet uses only 4 tags - noun, verb, adjective, adverb...where OpenNLP generates tags based on <a href=""http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench/CQP-HTMLDemo/PennTreebankTS.html"" rel=""nofollow noreferrer""> Penn treebank tagset </a>. I need to convert them to wordnet-compatible tags... i did find <a href=""https://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk""> this </a> on the site - but its all python..i need java</p>

<p>agreed that i can write a simple function myself to rename these tags..but what to do with other tags like DT, PP, PDT, POS, MD, etc...</p>
","java, nlp",,1749,1303155011
Comparing overlapping ranges,"<p>I'm going to ask this question using Scala syntax, even though the question is really language independent.</p>

<p>Suppose I have two lists</p>

<pre><code>val groundtruth:List[Range]
val testresult:List[Range]
</code></pre>

<p>And I want to find all of the elements of <code>testresult</code> that overlap some element in <code>groundtruth</code>.</p>

<p>I can do this as follows:</p>

<pre><code>def overlaps(x:Range,y:Range) = (x contains y.start) || (y contains x.start)
val result = testresult.filter{ tr =&gt; groundtruth.exists{gt =&gt; overlaps(gt,tr)}}
</code></pre>

<p>But this takes <code>O(testresult.size * groundtruth.size)</code> time to run.</p>

<p>Is there a faster algorithm for computing this result, or a data structure that can make the <code>exists</code> test more efficient?</p>

<hr>

<p>P.S. The algorithm should work on <code>groundtruth</code> and <code>testresult</code> generated with an expression like the following. In other words, there are no guarantees about the relationships between the ranges within a list, the <code>Range</code>s have an average size of 100 or larger. </p>

<pre><code>(1 to 1000).map{x =&gt;
   val midPt = r.nextInt(100000);
   ((midPt - r.nextInt(100)) to (midPt + r.nextInt(100)));
}.toList
</code></pre>
","algorithm, data-structures, nlp, overlapping-matches","<p>Try an <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Interval_tree"">interval tree</a>. <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Introduction_to_Algorithms"">Cormen, Leiserson, Rivest and Stein</a> discuss these in (IIRC) chapter 14.</p>

<p>Alternatively, if your lists of intervals are both sorted and the intervals within a list are non-overlapping, then the following algorithm solves your problem in linear time and in a single pass over both lists:</p>

<pre><code>(define interval cons)
(define lower car)
(define upper cdr)

(define (overlap a b)
  (cond ((or (null? a) (null? b)) '())
        ((&lt; (upper a) (lower b))
         (overlap (cdr a) b))
        ((&gt; (lower a) (upper b))
         (overlap a (cdr b)))
        (#t  ;; (car a) and (car b) overlap
             ;; EDIT: there's a bug in the following part.
             ;; The code shouldn't skip over both cars at once,
             ;; since they may also overlap with further intervals.
             ;; However, I'm too tired to fix this now.
         (cons (interval (max (lower a) (lower b))
                         (min (upper a) (upper b)))
               (overlap a b)))))
</code></pre>

<p>(I hope you can read Scheme :)</p>
",2082,1302806856
How to convert this sentence into a first order logic well formed formula?,"<p>I am trying to convert the following sentence to a well formed formula using first-order logic(Predicate logic).</p>
<blockquote>
<p>All towers are of the same color.</p>
</blockquote>
<p>I have defined the following predicates:</p>
<blockquote>
<p>Tower(x) :: x is a tower.</p>
<p>Color(x, y) :: x is of color y</p>
</blockquote>
<p>I am not able to convert the aforementioned sentence into a well formed formula using the above predicates. Is it possible to convert it using the above predicates or some new predicate should be required. Please advise.</p>
<p>EDIT: Forgot to add a detail. There are only three available colours in the world (red, green, blue). Can this detail be used. Does that make any difference to the solution?</p>
","artificial-intelligence, logic, nlp, first-order-logic","<p>There exists a Y1 such that for all X Tower(X) implies Color(X, Y1)</p>
",3448,1302554858
POS Pattern Filter?,"<p>I'm writing some code that iterates a set of POS tags (generated by pos_tag in NLTK) to search for POS patterns. Matching sets of POS tags are stored in a list for later processing. Surely a regex-style pattern filter already exists for a task like this, but a couple of initial google searches didn't give me anything.</p>

<p>Are there any code snippets out there that can do my POS pattern filtering for me?</p>

<p>Thanks,
Dave</p>

<p>EDIT: Complete solution (using RegexParser, and where messages is any string)</p>

<pre><code>text = nltk.word_tokenize(message)
tags = nltk.pos_tag(text)
grammar = r""""""
    RULE_1: {&lt;JJ&gt;+&lt;NNP&gt;*&lt;NN&gt;*}
    """"""
chunker = nltk.RegexpParser(grammar)
chunked = chunker.parse(tags)
def filter(tree):
    return (tree.node == ""RULE_1"")
for s in chunked.subtrees(filter):
    print s
</code></pre>

<p>Check out <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html</a> and <a href=""http://www.regular-expressions.info/reference.html"" rel=""nofollow"">http://www.regular-expressions.info/reference.html</a> for more on creating the rules.</p>
","nlp, nltk","<p>I think you're looking for <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.chunk.regexp.RegexpChunkParser-class.html"" rel=""nofollow""><code>RegexpChunkParser</code></a>.</p>
",1853,1302489583
Simple library for Natural Language Processing in C#,"<p>I am working on a project which requires simple sort of NLP. The features that I need include: </p>

<ul>
<li>Sentence splitter</li>
<li>Phrase splitter (not word splitter)</li>
<li>Phrase nature identifier (common noun, proper noun, verb etc.)</li>
</ul>

<p>I am aware of libraries like SharpNLP, NLPTK and Antelope but all of them are too big and fancy for my project. Can someone suggest a simple one that provides only the above mentioned features. If it is a bit bigger and provides other features that's ok but I guess having a 139MB large library for this simple stuff might not be a good idea, especially when this project is already going to be complex in other areas.</p>

<p>Thanks in advance for any ideas.</p>
","c#, nlp",,2581,1302464921
"Algorithms or libraries for textual analysis, specifically: dominant words, phrases across text, and collection of text","<p>I'm working on a project where I need to analyze a page of text and collections of pages of text to determine dominant words.   I'd like to know if there is a library (prefer c# or java) that will handle the heavy lifting for me.  If not, is there an algorithm or multiple that would achieve my goals below.  </p>

<p>What I want to do is similar to word clouds built from a url or rss feed that you find on the web, except I don't want the visualization.  They are used all the time for analyzing the presidential candidate speeches to see what the theme or most used words are.  </p>

<p>The complication, is that I need to do this on thousands of short documents, and then collections or categories of these documents.  </p>

<p>My initial plan was to parse the document out, then filter common words - of, the, he, she, etc..  Then count the number of times the remaining words show up in the text (and overall collection/category).  </p>

<p>The problem is that in the future, I would like to handle stemming, plural forms, etc..   I would also like to see if there is a way to identify important phrases. (Instead of a count of a word, the count of a phrase being 2-3 words together)</p>

<p>Any guidance on a strategy, libraries or algorithms that would help are appreciated.  </p>
","algorithm, text, nlp, analysis, lexical-analysis",,11035,1224542302
Possible anticrawler,"<p>For an educational NLP project I need a list of all Italian words. I thought I would write a crawler that will get the words from www.wordreference.com. I use Python with the mechanize crawler framework. but when i use the code:</p>

<pre><code> br = mechanize.Browser()
 br.open(""http://www.wordreference.com/iten/abaco"")
 html = br.response().get_data()
 print html
</code></pre>

<p>I get some page from ""yahoo.com"". is it possible this website has an anticrawler mechanism? </p>
","nlp, web-crawler","<p>I would suggest to use existing datasets, here are few examples from this <a href=""http://www.aclweb.org/aclwiki/index.php?title=Resources_for_Italian"" rel=""nofollow"">acl wiki page</a>:</p>

<blockquote>
  <p>Corpuses:</p>
  
  <ul>
  <li>...</li>
  <li><a href=""http://www.uni-duisburg.de/Fak2/FremdPhil/Romanistik/Personal/Burr/humcomp/"" rel=""nofollow"">Oxford Text Archive Corpus of Italian Newspapers</a>  ...</li>
  <li>...</li>
  </ul>
  
  <p>WordNets</p>
  
  <ul>
  <li><a href=""http://www.elda.fr/"" rel=""nofollow"">EuroWordNet</a></li>
  <li><a href=""http://multiwordnet.itc.it/english/home.php"" rel=""nofollow"">MultiWordNet</a> - a multilingual lexical database in which the Italian
  WordNet is strictly aligned with
  Princeton WordNet 1.6 ...</li>
  </ul>
</blockquote>

<p>Please check the full list on the acl wiki page, I think you should find an italian corpus, which let you to define italian words.</p>
",138,1302177048
Ruby: is there a stemmer that &quot;knows&quot; English irregular verbs?,"<p>There is a ruby stemmer <a href=""https://github.com/aurelian/ruby-stemmer"" rel=""nofollow"">https://github.com/aurelian/ruby-stemmer</a>, but it 1) does not stem English irregular verbs 2) fails to build native extensions on Windows. Is there an alternative that fixes at least one of the problems?</p>
","ruby, nlp, stemming","<p>I think you should be searching for a lemmatizer (which has information about morphology and can handle irregular words) rather than a stemmer (which usually just lops off the ends of words).  See <a href=""http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"">this explanation</a> in Manning, Raghavan, and Schütze's online book on information retrieval.</p>

<p>I haven't tried it out, but a quick search came across this English lemmatizer for Ruby: <a href=""http://mastarpj.nict.go.jp/~mutiyama/software.html#elemma"">elemma</a>.</p>

<p>A commonly-used (non-Ruby) English morphological analyzer that can do lemmatization is <a href=""http://www.informatics.sussex.ac.uk/research/groups/nlp/carroll/morph.html"">morpha</a>.</p>
",851,1292948567
how to replace all non alphanumeric characters with space in php?,"<pre><code>$html=strip_tags($html);
$html=ereg_replace(""[^A-Za-zäÄÜüÖö]"","" "",$html);
$words = preg_split(""/[\s,]+/"", $html);
</code></pre>

<p>doesnt this replace all non (A-Z, a-z, a o u with umlauts) characters with space?
I am losing words like zugänglich etc with umlauts</p>

<p>is there any thing wrong with the regex?</p>

<p>edit:</p>

<p>I replaced ereg_replace with preg_replace but somehow the special characters like :, ® are not getting replace by space...</p>
","php, regex, nlp",,2852,1302000533
Stanford Parser questions,"<p>I am writing a project that works with NLP (natural language parser). I am using the stanford parser.</p>

<p>I create a thread pool that takes sentences and run the parser with them.
When I create one thread its all works fine, but when I create more, I get errors.
The ""test"" procedure is finding words that have some connections.
If I do an synchronized its supposed to work like one thread but still I get errors.
My problem is that I have errors on this code:</p>

<pre><code>public synchronized String test(String s,LexicalizedParser lp )
{

    if (s.isEmpty()) return """";
    if (s.length()&gt;80) return """";
    System.out.println(s);
    String[] sent = s.split("" "");
 Tree parse = (Tree) lp.apply(Arrays.asList(sent));
TreebankLanguagePack tlp = new PennTreebankLanguagePack();
GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
Collection tdl = gs.typedDependenciesCollapsed();
List list = new ArrayList(tdl);


//for (int i=0;i&lt;list.size();i++)
//System.out.println(list.get(1).toString());

//remove scops and numbers like sbj(screen-4,good-6)-&gt;screen good

 Pattern p = Pattern.compile("".*\\((.*?)\\-\\d+,(.*?)\\-\\d+\\).*"");

       if (list.size()&gt;2){
    // Split input with the pattern
        Matcher m = p.matcher(list.get(1).toString());
        //check if the result have more than  1 groups
       if (m.find()&amp;&amp; m.groupCount()&gt;1){
           if (m.groupCount()&gt;1)
           {
               System.out.println(list);
 return  m.group(1)+m.group(2);
    }}
}
        return """";

}
</code></pre>

<p>the errors that I have are:</p>

<blockquote>
  <p>at blogsOpinions.ParserText.(ParserText.java:47)
    at blogsOpinions.ThreadPoolTest$1.run(ThreadPoolTest.java:50)
    at blogsOpinions.ThreadPool$PooledThread.run(ThreadPoolTest.java:196)
  Recovering using fall through
  strategy: will construct an (X ...)
  tree. Exception in thread
  ""PooledThread-21""
  java.lang.ClassCastException:
  java.lang.String cannot be cast to
  edu.stanford.nlp.ling.HasWord</p>
  
  <p>at
  edu.stanford.nlp.parser.lexparser.LexicalizedParser.apply(LexicalizedParser.java:289)
      at blogsOpinions.ParserText.test(ParserText.java:174)
      at blogsOpinions.ParserText.insertDb(ParserText.java:76)
      at blogsOpinions.ParserText.(ParserText.java:47)
      at blogsOpinions.ThreadPoolTest$1.run(ThreadPoolTest.java:50)
      at blogsOpinions.ThreadPool$PooledThread.run(ThreadPoolTest.java:196)</p>
</blockquote>

<p>and how can i get the discription of the subject like the screen is very good, and I want to get screen good from the list the I get and not like <code>list.get(1)</code>.</p>
","java, nlp, stanford-nlp","<p>You can't call <code>LexicalizedParser.parse</code> on a <code>List</code> of <code>String</code>s; it expects a list of <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/HasWord.html"" rel=""nofollow""><code>HasWord</code></a> objects. It's much easier to call the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html#apply%28java.lang.Object%29"" rel=""nofollow""><code>apply</code></a> method on your input string. This will also run a proper tokenizer on your input (instead of your simple <code>split</code> on spaces).</p>

<p>To get relations such as subjectness out of the returned <code>Tree</code>, call its <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html#dependencies%28%29"" rel=""nofollow""><code>dependencies</code></a> member.</p>
",2618,1295713871
python data mining,"<p>I am not too much onto data mining but I require some ideas on clustering. Let me first describe my problem.</p>

<p>I have a around 100 data sheets which contain user reviews. I am trying to find for instances words that describe quality. One can say it is amazing quality another person can say great quality now I have to cluster those documents which describe those similar sentences and get the frequency of such sentences. What concept to apply here?</p>

<p>Guess I have to specify some stop words and synonyms. I am not too familiar with this concept.</p>

<p>Can some one give me some detailed links or explanation? and what tool to be used? I am basically a python programmer so any python module would be appreciated.</p>

<p>Thank You</p>
","python, nlp, data-mining","<p>There is <a href=""http://www.nltk.org/"" rel=""nofollow"">http://www.nltk.org/</a> for language processing. With this library you are able to split text into sentences, calculate term frequences, find synonyms and more.</p>

<p><a href=""http://sourceforge.net/projects/carrot2/"" rel=""nofollow"">Carrot^2</a> is a nice opensource project for clustering text snippets, unfortunately it's written in Java. The idea behind its clustering is terms and phrases (bigrams and trigrams) frequences. After preprocessing each document (snippet, review) is represented as a vector of term/phrase frequences. To calculate clusters they use some linear algebra and find principal components in that terms space. Then this components are used to form clusters and labels for them.</p>

<p>In yuor case it's worth considering reviews as documents, cluster them and get labels for clusters. May be labels would somehow evaluate reviews.</p>

<p>In your specific case it's worth eliminate words of interest so dramatically decreasing dimensionality which is very critical in such tasks</p>

<p>Another useful project - <a href=""http://web.media.mit.edu/~hugo/montylingua/"" rel=""nofollow"">montylingua</a></p>
",1336,1301901698
Prolog Parsing Output,"<p>I'm doing a piece of university coursework, and I'm stuck with some Prolog.</p>

<p>The coursework is to make a really rudimentary Watson (the machine that answers questions on Jeapoardy).</p>

<p>Anyway, I've managed to make it output the following:</p>

<pre><code>noun_phrase(det(the),np2(adj(traitorous),np2(noun(tostig_godwinson)))),
verb_phrase(verb(was),np(noun(slain)))).
</code></pre>

<p>But the coursework specifies that I now need to extract the first and second noun, and the verb, to make a more concise sentence; i.e. [Tostig_godwinson, was, slain].</p>

<p>I much prefer programming in languages like C etc., so I'm a bit stuck. If this were a procedural language, I'd use parsing tools, but Prolog doesn't have any... What do I need to do to extract those parts?</p>

<p>Thank you in advance</p>
","parsing, prolog, nlp",,248,1301939121
how to get specific  elements from Collection tdl with stanford nlp parser,"<p>I am using the <code>nlp parser stanord</code>.
I want to extract some elements like nsubj and more from <code>Collection</code> tdl.
My code is:</p>

<pre><code>TreebankLanguagePack tlp = new PennTreebankLanguagePack();
GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
Collection tdl = gs.typedDependenciesCollapsed();
</code></pre>

<p>but my problem is I don't know how to compare the elements.that I get from the Collection.</p>

<p>Thanks a lot for helping!</p>
","nlp, stanford-nlp",,473,1301577468
Which of these projects should I choose for summer workshop on NLP?,"<p>I am planning to attend a project oriented advanced summer workshop here in India on Natural Language Processing.
Before start of the workshop, I have to make a project preference out of the following four areas about which I have limited knowledge.</p>

<blockquote>
  <p><strong>Machine Translation</strong> Develop an English-Indian language translation
  system.</p>
  
  <p><strong>Parsing</strong> Build an Indian Language (IL) Parser.</p>
  
  <p><strong>Morphological Analysis</strong> Develop and test Morphological Analyzers for
  Indian Languages.</p>
  
  <p><strong>Speech</strong> Spoken Dialog Systems, Emotion/Prosody Detection, Synthesis
  and Conversion</p>
</blockquote>

<p>I have taken a course in Artificial Intelligence where NLP was introduced and fundamental sub-topics like <em>POS tagging(Transformation Based Learning), word prediction using N-grams, Hidden Markov Models, Viterbi Algorithm, Natural Language Parsing, Context Free Grammar, CKY Algorithm</em> were covered.</p>

<p>I understand this is a slightly vague question and the choice would depend primarily on my interests, but would appreciate guidance on which area would be better in terms of the research scope, practical application, industry opportunities etc. </p>

<p>EDIT: Application of skills/experience acquired while working on the project, outside NLP would also be a factor in the decision.</p>
","artificial-intelligence, nlp, machine-learning","<p>Let's first group the four options as first three in one category-NL-Text and another fourth in other-NL-Speech, as the skill set and inclination needed to pursue those are slightly different.
So first use the criteria of do you like working in the first group or second. Once you are done with that and you choose Speech. you are done. 
But if you are on other side, now there are two categories MT and remaining MA and IL Parsing. Building a MT will focus on using the ready made components and adapt them to you language pair. If you are supposed to do it statistically, it's little more on data side and you don't gain much, I mean learn/work much as in other two.If that is rule based, there are very few making it big- but you will learn a lot. Building an IL parser is a good option and might be useful for future task where will have lot of data in IL and then text processing industry will flourish. So considering future scope in industry consider my +1. Same case is with Morphological Analysis. </p>
",774,1300909701
Text Mining to extract animal types from text,"<p>I need to do an experiment and I am new in NLP. I have read books that explain the theoritical issues but when it comes to practical I found it hard to find a guide. so please who knows anything in NLP especially the practical issues tell me and point me to the right path because I feel I am lost  (useful books, useful tools and useful websites)</p>

<p>what I am trying to do is to take a text and find specific words for example animals such as dogs, cats,...etc in it then I need to extract this word and 2 words on each side. 
For example </p>

<pre><code>I was watching TV with my lovely cat last night.
</code></pre>

<p>the extracted text will be </p>

<pre><code>(my lovely cat last night)
</code></pre>

<p>This will be my training example to the machine tool</p>

<p>Q1: there will be around 100 training examples similar to what I explained above. I used tocknizer to extracts words but how can I extract specific words(for our example all types of animals) with 2 words on each side. do I need to use tags for example or what is your idea?</p>

<p>Q2: If I have these training examples how can I prepare appropriate datasets that I can give it to the machine tool to train it? what should I write in this dataset to specify the animal and should I need to give other features? and how can I arrange it in a dataset .</p>

<p>many words from you might help me a lot please do not hesitate to tell what you know</p>
","text, dataset, nlp, mining",,1875,1300154630
memory usage of a probabilistic parser,"<p>I am writing a CKY parser for a Range Concatenation Grammar. I want to use a treebank as grammar, so the grammar will be large. I've written a prototype <a href=""http://www.nltk.org/download"" rel=""nofollow"">1</a> in Python and it seems to work well when I simulate a treebank of a couple tens of sentences, but the memory usage is unacceptable. I tried writing it in C++ but so far that has been very frustrating as I have never used C++ before. Here's some data (n is number of sentences the grammar is based on):</p>

<pre><code>n    mem
9    173M
18   486M
36   836M
</code></pre>

<p>This growth pattern is what is to be expected given the best-first algorithm, but the amount of overhead is what concerns me. The memory usage according to heapy is a factor ten smaller than these numbers, valgrind reported something similar. What causes this discrepancy and is there anything I can do about it in Python (or Cython)? Perhaps it's due to fragmentation? Or maybe it is the overhead of python dictionaries?</p>

<p>Some background: the two important datastructures are the agenda mapping edges to probabilities, and the chart, which is a dictionary mapping nonterminals and positions to edges. The agenda is implemented with a heapdict (which internally uses a dict and a heapq list), the chart with a dictionary mapping nonterminals and positions to edges. The agenda is frequently inserted and removed from, the chart only gets insertions and lookups. I represent edges with tuples like this:</p>

<pre><code>((""S"", 111), (""NP"", 010), (""VP"", 100, 001))
</code></pre>

<p>The strings are the nonterminal labels from the grammar, the positions are encoded as a bitmask. There can be multiple positions when a constituent is discontinuous. So this edge could be represent an analysis of ""is Mary happy"", where ""is"" and happy"" both belong to the VP. The chart dictionary is indexed by the first element of this edge, (""S"", 111) in this case. In a new version I tried transposing this representation in the hope that it would save memory due to reuse:</p>

<pre><code>((""S"", ""NP"", ""VP), (111, 100, 011))
</code></pre>

<p>I figured that Python would store the first part only once if it would occur in combination  with different positions, although I'm not actually sure this is true. In either case, it didn't seem to make any difference.</p>

<p>So basically what I am wondering is if it is worth pursuing my Python implementation any further, including doing things with Cython and different datastructures, or that writing it from the ground up in C++ is the only viable option.</p>

<p><strong>UPDATE</strong>: After some improvements I no longer have issues with memory usage. I'm working on an optimized Cython version. I'll award the bounty to the most useful suggestion for increasing efficiency of the code. There is an annotated version at <a href=""http://student.science.uva.nl/~acranenb/plcfrs_cython.html"" rel=""nofollow"">http://student.science.uva.nl/~acranenb/plcfrs_cython.html</a></p>

<p><a href=""http://www.nltk.org/download"" rel=""nofollow"">1</a> <a href=""https://github.com/andreasvc/disco-dop/"" rel=""nofollow"">https://github.com/andreasvc/disco-dop/</a>
-- run test.py to parse some sentences. Requires python 2.6, <a href=""http://www.nltk.org/download"" rel=""nofollow"">nltk</a> and <a href=""http://pypi.python.org/pypi/HeapDict"" rel=""nofollow"">heapdict</a></p>
","python, memory, nlp, cython",,551,1300720005
Culture-independent stemmer/analyzer for Lucene.NET,"<p>We're currently developing a full-text-search-enabled app and we Lucene.NET is our weapon of choice. What's expected is that an app will be used by people from different countries, so Lucene.NET has to be able to search across Russian, English and other texts equally well.</p>

<p>Are there any universal and culture-independent stemmers and analyzers to suit our needs? I understand that eventually we'd have to use culture-specific ones, but we want to get up and running with this potentially quick and dirty approach.</p>
","internationalization, lucene, nlp, lucene.net","<p>Given that the spelling, grammar and character sets of English and Russian are significantly different, any stemmer which tried to do both would either be massively large or poorly performant (most likely both).</p>

<p>It would probably be much better to use a stemmer for each language, and pick which one to use based on either UI clues (what language is being used to query) or by explicit selection.</p>

<p>Having said that, it's unlikely that any Russian text will match an English search term correctly or vice-versa.</p>

<p>This sounds like a case where a little more business analysis would help more than code.</p>
",640,1301304269
Runtime grammar productions creation,"<p>I am interested in natural languge processing with python-django.</p>

<p>My project requires creation of grammar productions at runtime. That means, whenever i ask a question or write a sentence, basic nlp steps like pos-tagging should be done and get them added to grammar productions or any other structure ( even if the words are not present in grammar) , so that i can do the further chunking and extraction of different nouns, verbs ,etc separately from that grammar.</p>

<p>Please guide me the same.</p>
","python, django, nlp",,145,1301051006
Can I digitalize a dictionary?,"<p>I've found a public domain latin&lt;->portuguese dictionary in PDF which I'd like to convert to plain text, parse and use as the database of a program. After some testing, however, I got a little skeptical. Take a look at the <a href=""https://i.sstatic.net/ynNq4.png"" rel=""nofollow"">original file</a> and at the <a href=""http://dabc50b2975da0ae.paste.se/"" rel=""nofollow"">resulting text of gocr</a>. Is there any hope that I might reach 99%+ accuracy in some method? I thought of reCaptcha's database, but I guess it is Google's property, isn't it?</p>

<p>Thanks!</p>
","artificial-intelligence, nlp, computer-vision, ocr",,132,1300807525
"code throws std::bad_alloc, not enough memory or can it be a bug?","<p>I am parsing using a pretty large grammar (1.1 GB, it's data-oriented parsing). The parser I use (bitpar) is said to be optimized for highly ambiguous grammars. I'm getting this error:</p>

<pre><code>1terminate called after throwing an instance of 'std::bad_alloc'
  what():  St9bad_alloc
dotest.sh: line 11: 16686 Aborted                 bitpar -p -b 1 -s top -u unknownwordsm -w pos.dfsa /tmp/gsyntax.pcfg /tmp/gsyntax.lex arbobanko.test arbobanko.results
</code></pre>

<p>Is there hope? Does it mean that it has ran out of memory? It uses about 15 GB before it crashes. The machine I'm using has 32 GB of RAM, plus swap as well. It crashes before outputting a single parse tree; I think it crashes after reading the grammar, during an attempt to construct a chart parse for the first sentence.</p>

<p>The parser is an efficient CYK chart parser using bit vector representations; I presume it is already pretty memory efficient. If it really requires too much memory I could sample from the grammar rules, but this will decrease parse accuracy of course.</p>

<p>I think the problem is probably that I have a very large number of non-terminals, I should probably try to look for a different parser (any suggestions?)</p>

<p>UPDATE: for posterity's sake, I found the problem a long time ago. The grammar was way too big due to a bug, so the parser couldn't handle it with the available memory. With the correct grammar (which is an order of magnitude smaller) it works fine.</p>
","c++, memory-management, nlp",,2404,1273803963
How to ensure user submit only english text,"<p>I am building a project involving natural language processing, since the nlp module currently only deal with english text, so I have to make sure the user submitted content (not long, only several words) is in english. Are there established ways to achieve this? Python or Javascript way preferred.</p>
","javascript, python, nlp","<p>If the content is long enough I would suggest some <a href=""http://en.wikipedia.org/wiki/Frequency_analysis"" rel=""nofollow noreferrer"">frequency analysis</a> on the letters. </p>

<p>But for a few words I think your best bet is to compare them to an English dictionary and accept the input if half of them match.</p>
",1251,1223883132
Split string into sentences using regular expression,"<p>I need to match a string like ""one. two.    three. four. five.  six. seven. eight. nine. ten. eleven"" into groups of four sentences.  I need a regular expression to break the string into a group after every fourth period. Something like: </p>

<pre><code>  string regex = @""(.*.\s){4}"";

  System.Text.RegularExpressions.Regex exp = new System.Text.RegularExpressions.Regex(regex);

  string result = exp.Replace(toTest, "".\n"");
</code></pre>

<p>doesn't work because it will replace the text before the periods, not just the periods themselves.  How can I count just the periods and replace them with a period and new line character?</p>
","c#, regex, nlp",,10234,1225228953
natural language processing fix for combined words,"<p>I have some text that was generate by another system.  It combined some words together in what I assume was some sort of wordwrap by-product.  So something simple like 'the dog' is combine into 'thedog'. </p>

<p>I checked the ascii and unicode string to see is there wasn't some unseen character in there, but there wasn't.  A confounding problem is that this is medical text and a corpus to check against aren't that available.  So, real example is '...test to rule out SARS versus pneumonia' ends up as '... versuspneumonia.'</p>

<p>Anyone have a suggestion for finding and separating these?</p>
","regex, nlp","<p>Here is what I did.  I combined a couple of ideas and using a general bootstrapping methodology came up with a pretty good solution.  I used Python for all of this.</p>

<ol>
<li>took a sample of reports, tokenized all the words and created a frequency table.</li>
<li>For words with a frequency of 3 or under (frequency of 4 or more was deemed common enough to be correct), I spell checked them using PyEnchant package (enchant library)</li>
<li>built a medical dictionary from the 'misspelled' words, in step 2, that were clinical.</li>
<li>for all the reports, created a frequency table</li>
<li>for words with a frequency under 4, I spell checked each using PyEnchant and my medical dictionary</li>
<li>Took each misspelled word and split them in all possible ways.  The splits were tested for the creation of 2 correctly spelled words.  kept any successful split</li>
<li>For each potential solutions the highest weighted solution was used.</li>
</ol>
",1140,1300232487
How do I approximate &quot;Did you mean?&quot; without using Google?,"<p>I am aware of the duplicates of this question:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work"">How does the Google “Did you mean?” Algorithm work?</a></li>
<li><a href=""https://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean"">How do you implement a “Did you mean”?</a></li>
<li>... and many others. </li>
</ul>

<p>These questions are interested in how the algorithm actually works. My question is more like: Let's assume Google did not exist or maybe this feature did not exist and we don't have user input. How does one go about implementing an approximate version of this algorithm? </p>

<p><strong>Why is this interesting?</strong></p>

<p>Ok. Try typing ""<a href=""http://www.google.com/#sclient=psy&amp;hl=en&amp;site=&amp;source=hp&amp;q=qualfy&amp;aq=f&amp;aqi=&amp;aql=&amp;oq=&amp;pbx=1&amp;bav=on.2,or.&amp;fp=170344a196d61403"" rel=""nofollow noreferrer"">qualfy</a>"" into Google and it tells you:</p>

<blockquote>
  <p><strong>Did you mean:</strong> <em>qualify</em></p>
</blockquote>

<p>Fair enough. It uses Statistical Machine Learning on data collected from billions of users to do this. But now try typing this: ""<a href=""http://www.google.com/search?sourceid=chrome&amp;ie=UTF-8&amp;q=Trytoreconnectyou"" rel=""nofollow noreferrer"">Trytoreconnectyou</a>"" into Google and it tells you:</p>

<blockquote>
  <p><strong>Did you mean:</strong> <em>Try To Reconnect You</em></p>
</blockquote>

<p>Now this is the more interesting part. How does Google determine this? Have a dictionary handy and guess the most probably words again using user input? And how does it differentiate between a misspelled word and a sentence? </p>

<p>Now considering that most programmers do not have access to input from billions of users, I am looking for the best approximate way to implement this algorithm and what resources are available (datasets, libraries etc.). Any suggestions?</p>
","algorithm, language-agnostic, nlp, machine-learning","<p>Assuming you have a dictionary of words (all the words that appear in the dictionary in the worst case, all the phrases that appear in the data in your system in the best case) and that you know the relative frequency of the various words, you should be able to reasonably guess at what the user meant via some combination of the <a href=""http://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""noreferrer"">similarity of the word</a> and the number of hits for the similar word.  The weights obviously require a bit of trial and error, but generally the user will be more interested in a popular result that is a bit linguistically further away from the string they entered than in a valid word that is linguistically closer but only has one or two hits in your system.</p>

<p>The second case should be a bit more straightforward.  You find all the valid words that begin the string (""T"" is invalid, ""Tr"" is invalid, ""Try"" is a word, ""Tryt"" is not a word, etc.) and for each valid word, you repeat the algorithm for the remaining string.  This should be pretty quick assuming your dictionary is indexed.  If you find a result where you are able to decompose the long string into a set of valid words with no remaining characters, that's what you recommend.  Of course, if you're Google, you probably modify the algorithm to look for substrings that are reasonably close typos to actual words and you have some logic to handle cases where a string can be read multiple ways with a loose enough spellcheck (possibly using the number of results to break the tie).</p>
",4673,1299788219
emacs M-e doesn&#39;t work properly in tex-mode,"<p>I'm using emacs and auctex to write LaTeX documents. For some reason, M-e doesn't move to the end of the sentence in tex-mode as it did when I went through the tutorial. It moves to the end of the paragraph. (That is, it moves to just before the next double line break)</p>

<p>What is wrong? Do I need to turn on/off some mode to skip to the next full stop? How do I check which modes are active?</p>
","emacs, latex, nlp, punctuation, auctex","<p>I noticed that the same happens in my Emacs. The problem is that the variable <code>sentence-end-double-space</code> is set to <code>t</code>. This means that Emacs expects a sentence to end with a double space. By setting to <code>nil</code> things work properly, i.e., Emacs recognizes a period followed by a single space as the end of sentences.</p>
",556,1268762622
Running WINE on bash ubuntu,"<p>Does anyone know how to run a windows .exe on WINE on an ubuntu bash script? running on ubuntu 10.10</p>

<p>this is the program i'm trying to run ""POSTAG-Sejong"" from <a href=""http://isoft.postech.ac.kr/Course/CS730b/2005/index.html"" rel=""nofollow"">http://isoft.postech.ac.kr/Course/CS730b/2005/index.html</a> it runs properly when i right-click and open with WINE windows program loader.</p>

<p>but when i try to run it with the command in terminal </p>

<pre><code>$ wine ./postagsejongk/sjTaggerInteg.exe
</code></pre>

<p>it fails and gives the error:</p>

<pre><code>./dic/Dic.strie ╞─└╧└╗ ┐¡ ╝÷ ╛°╜└┤╧┤┘.wine: Unhandled exception 0x80000003 at address 0x441ce1 (thread 0009), starting debugger...
0x00441ce1: int $3
Modules:
Module  Address         Debug info  Name (48 modules)
PE    400000- 13e1000   Export          sjtaggerinteg
ELF 20000000-20077000   Deferred        libfreetype.so.6
ELF 20077000-20194000   Deferred        libx11.so.6
ELF 20194000-20199000   Deferred        libuuid.so.1
ELF 20199000-2019d000   Deferred        libxau.so.6
ELF 2019d000-201be000   Deferred        imm32&lt;elf&gt;
  \-PE  201a0000-201be000   \               imm32
ELF 201be000-201c4000   Deferred        libxxf86vm.so.1
ELF 201c4000-201c8000   Deferred        libxcomposite.so.1
ELF 201c8000-201d2000   Deferred        libxcursor.so.1
ELF 26d2d000-26dd6000   Deferred        winex11&lt;elf&gt;
  \-PE  26d40000-26dd6000   \               winex11
ELF 2786c000-27885000   Deferred        version&lt;elf&gt;
  \-PE  27870000-27885000   \               version
ELF 2f3dc000-2f3e4000   Deferred        libxrandr.so.2
ELF 48ced000-48cf7000   Deferred        libxrender.so.1
ELF 4c7d8000-4c90c000   Deferred        user32&lt;elf&gt;
  \-PE  4c7f0000-4c90c000   \               user32
ELF 4d766000-4d77f000   Deferred        libice.so.6
ELF 50721000-50727000   Deferred        libxfixes.so.3
ELF 532d7000-532fe000   Deferred        libexpat.so.1
ELF 593aa000-593bf000   Deferred        libz.so.1
ELF 5abfc000-5ac58000   Deferred        advapi32&lt;elf&gt;
  \-PE  5ac10000-5ac58000   \               advapi32
ELF 5d36b000-5d36f000   Deferred        libxinerama.so.1
ELF 68000000-6801e000   Deferred        ld-linux.so.2
ELF 6801e000-6815f000   Dwarf           libwine.so.1
ELF 6815f000-68179000   Deferred        libpthread.so.0
ELF 68179000-6817d000   Deferred        libdl.so.2
ELF 6817d000-681a3000   Deferred        libm.so.6
ELF 681a3000-681ab000   Deferred        libnss_compat.so.2
ELF 681ab000-681c2000   Deferred        libnsl.so.1
ELF 681c2000-681cd000   Deferred        libnss_nis.so.2
ELF 681cd000-681d9000   Deferred        libnss_files.so.2
ELF 681d9000-68212000   Deferred        libncurses.so.5
ELF 6a619000-6a629000   Deferred        libxext.so.6
ELF 72bac000-72bb2000   Deferred        libxdmcp.so.6
ELF 72df1000-72dfa000   Deferred        libsm.so.6
ELF 74a4f000-74a69000   Deferred        libxcb.so.1
ELF 75fe8000-76145000   Deferred        libc.so.6
ELF 76d42000-76d72000   Deferred        libfontconfig.so.1
ELF 7ab01000-7ab8f000   Deferred        gdi32&lt;elf&gt;
  \-PE  7ab10000-7ab8f000   \               gdi32
ELF 7b800000-7b990000   Dwarf           kernel32&lt;elf&gt;
  \-PE  7b810000-7b990000   \               kernel32
ELF 7bc00000-7bcbb000   Dwarf           ntdll&lt;elf&gt;
  \-PE  7bc10000-7bcbb000   \               ntdll
ELF 7bf00000-7bf04000   Deferred        &lt;wine-loader&gt;
Threads:
process  tid      prio (all id:s are in hex)
00000008 (D) Z:\home\ubi\postagsejongk\sjTaggerInteg.exe
    00000009    0 &lt;==
0000000e services.exe
    0000001b    0
    00000017    0
    00000015    0
    00000014    0
    00000010    0
    0000000f    0
00000011 winedevice.exe
    00000016    0
    00000013    0
    00000012    0
00000018 plugplay.exe
    0000001c    0
    0000001a    0
    00000019    0
0000001d explorer.exe
    0000001e    0
Backtrace:
=&gt;0 0x00441ce1 in sjtaggerinteg (+0x41ce1) (0x00326770)
  1 0x00404aa3 in sjtaggerinteg (+0x4aa2) (0x003269fc)
  2 0x00401187 in sjtaggerinteg (+0x1186) (0x0032fe90)
  3 0x7b85839c call_process_entry+0xb() in kernel32 (0x0032fea8)
  4 0x7b85903f ExitProcess+0xc9e() in kernel32 (0x0032fee8)
  5 0x7bc71c68 call_thread_func+0xb() in ntdll (0x0032fef8)
  6 0x7bc74750 call_thread_entry_point+0x6f() in ntdll (0x0032ffc8)
  7 0x7bc49e4a call_dll_entry_point+0x629() in ntdll (0x0032ffe8)
</code></pre>
","bash, nlp, ubuntu-10.10, wine, pos-tagger","<p>WINE is going to want the fake Windows path, not the real linux one. Example: </p>

<blockquote>
  <p>wine ""D:\Setup\URPROG.EXE""</p>
</blockquote>

<p>Here's the <a href=""http://wiki.winehq.org/FAQ#head-3b297df7a5411abe2b8d37fead01a2b8edc21619"" rel=""nofollow"">documentation. </a></p>

<p>You can run it with the local path if you do 'wine start' like this (from the docs): </p>

<p>wine start /Unix ""$HOME/installers/TronSetup.exe""</p>
",4083,1300245758
NLP programming tools using PHP?,"<p>Since big web applications came into existence, searching for data (and doing it lightning fast and accurate) has been one of the most important problems in web applications. For a while, I've worked using <a href=""http://incubator.apache.org/lucene.net/"">Lucene.NET</a>, which is a C# port of the <a href=""http://lucene.apache.org/java/"">Lucene project</a>. </p>

<p>I also work using PHP using <a href=""http://framework.zend.com/manual/en/zend.search.lucene.html"">Zend Framework's Lucene API</a>, which brings me to my question. Most times for providing good indexing we need to perform some NLP tools like <strong>tokenizing</strong>, <strong>lemmatizing</strong>, and many more, the question is:</p>

<p>Do you know of any good NLP programming framework/toolset using PHP?</p>

<p>PS: I'm very aware of the Zend API for Lucene, but indexing data properly is not just storing and relying in Lucene, you need to perform some extra tasks, like those above.</p>
","php, lucene, nlp","<p>I would suggest that you look at <a href=""http://lucene.apache.org/solr"" rel=""noreferrer"">Solr</a>, which is a best practice implementation of Lucene. Solr uses a REST based API that also has a very good <a href=""http://code.google.com/p/solr-php-client/"" rel=""noreferrer"">PHP client</a>. This will allow you to leverage the power of Lucene without needing to perform any of the low level programming to get the NLP power that you want. Also, you would probably want to grab the trunk version of Solr as the NLP development is very active right now and new capabilities are being added every day.</p>
",5915,1292478686
using minipar parser output,"<p>how to use minipar parser output to extract features like subject , object ,verb, tense etc to be use for english text to ASL conversion project</p>
",nlp,,2408,1291664123
reading # char in python,"<p>can someone help with me reading ""#"" char in python? i can't seem to get the file. because this is an output from the stanford postagger, is there any scripts available to convert the stanford postagger <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a> file to cwb. <a href=""http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html"" rel=""nofollow"">http://cogsci.uni-osnabrueck.de/~korpora/ws/CWBdoc/CWB_Encoding_Tutorial/node3.html</a></p>

<p>so this is the utf-8 txt file that i'm trying to read:</p>

<pre><code> 如果#CS 您#PN 在#P 新加坡#NR 只#AD 能#VV 前往#VV 一#CD 间#M 俱乐部#NN ，#PU 祖卡#NN 酒吧#NN 必然#AD 是#VC 您#PN 的#DEG 不二#JJ 选择#NN 。#PU
    作为#P 或许#AD 是#VC 新加坡#NR 唯一#JJ 一#CD 家#M 国际#NN 知名#VA 的#DEC 夜店#NN ，#PU 祖卡#NN 既#CC 是#VC 一#CD 个#M 公共#JJ 机构#NN ，#PU
</code></pre>

<p>So with this code i'm not readin the # char in the utf-8 txt files:</p>

<pre><code>#!/usr/bin/python # -*- coding: utf-8 -*-

'''
stanford POS tagger to CWB format
'''

import codecs
import nltk
import os, sys, re, glob

reload(sys)
sys.setdefaultencoding('utf-8')

cwd = './path/to/file.txt' #os.getcwd()

for infile in glob.glob(os.path.join(cwd, 'zouk.txt')):
        print infile
        (PATH, FILENAME) = os.path.split(infile)
        reader = codecs.open(infile, 'r', 'utf-8')
        for line in reader:
                for word in line:
                        if word == '\#':
                                print 'hex is here'
</code></pre>
","python, utf-8, nlp, stanford-nlp","<pre><code>if word == '\#':
</code></pre>

<p>This probably doesn't do what you think it does.  (Hint: <code>print ""\#""</code>)</p>
",341,1299901797
How to calculate tag-wise precision and recall for POS tagger?,"<p>I am using some rule-based and statistical POS taggers to tag a corpus(of around <strong>5000 sentences</strong>) with Parts of Speech(POS). Following is a snippet of my test corpus where each word is seperated by its respective POS tag by '/'.</p>

<pre><code>No/RB ,/, it/PRP was/VBD n't/RB Black/NNP Monday/NNP ./.
But/CC while/IN the/DT New/NNP York/NNP Stock/NNP Exchange/NNP did/VBD n't/RB fall/VB apart/RB Friday/NNP as/IN the/DT Dow/NNP Jones/NNP Industrial/NNP Average/NNP plunged/VBD 190.58/CD points/NNS --/: most/JJS of/IN it/PRP in/IN the/DT final/JJ hour/NN --/: it/PRP barely/RB managed/VBD *-2/-NONE- to/TO stay/VB this/DT side/NN of/IN chaos/NN ./.
Some/DT ``/`` circuit/NN breakers/NNS ''/'' installed/VBN */-NONE- after/IN the/DT October/NNP 1987/CD crash/NN failed/VBD their/PRP$ first/JJ test/NN ,/, traders/NNS say/VBP 0/-NONE- *T*-1/-NONE- ,/, *-2/-NONE- unable/JJ *-3/-NONE- to/TO cool/VB the/DT selling/NN panic/NN in/IN both/DT stocks/NNS and/CC futures/NNS ./.
</code></pre>

<p>After tagging the corpus, it looks like this:</p>

<pre><code>No/DT ,/, it/PRP was/VBD n't/RB Black/NNP Monday/NNP ./. 
But/CC while/IN the/DT New/NNP York/NNP Stock/NNP Exchange/NNP did/VBD n't/RB fall/VB apart/RB Friday/VB as/IN the/DT Dow/NNP Jones/NNP Industrial/NNP Average/JJ plunged/VBN 190.58/CD points/NNS --/: most/RBS of/IN it/PRP in/IN the/DT final/JJ hour/NN --/: it/PRP barely/RB managed/VBD *-2/-NONE- to/TO stay/VB this/DT side/NN of/IN chaos/NNS ./. 
Some/DT ``/`` circuit/NN breakers/NNS ''/'' installed/VBN */-NONE- after/IN the/DT October/NNP 1987/CD crash/NN failed/VBD their/PRP$ first/JJ test/NN ,/, traders/NNS say/VB 0/-NONE- *T*-1/-NONE- ,/, *-2/-NONE- unable/JJ *-3/-NONE- to/TO cool/VB the/DT selling/VBG panic/NN in/IN both/DT stocks/NNS and/CC futures/NNS ./. 
</code></pre>

<p>I need to calculate the tagging accuracy(<strong>Tag wise- Recall &amp; Precision</strong>), therefore need to find an error(if any) in tagging for each word-tag pair. </p>

<p>The approach I am thinking of is to loop through these 2 text files and store them in a list and later compare the 'two' lists element by element. </p>

<p>The approach seems really crude to me, so would like you guys to suggest some better solution to the above problem.</p>

<p>From the <a href=""http://en.wikipedia.org/wiki/Precision_and_recall"" rel=""noreferrer"">wikipedia</a> page:</p>

<blockquote>
  <p>In a classification task, the
  <strong>precision</strong> for a class is the number of
  true positives (i.e. the number of
  items correctly labeled as belonging
  to the positive class) divided by the
  total number of elements labeled as
  belonging to the positive class (i.e.
  the sum of true positives and false
  positives, which are items incorrectly
  labeled as belonging to the class).
  <strong>Recall</strong> in this context is defined as
  the number of true positives divided
  by the total number of elements that
  actually belong to the positive class
  (i.e. the sum of true positives and
  false negatives, which are items which
  were not labeled as belonging to the
  positive class but should have been).</p>
</blockquote>
","python, shell, nlp, machine-learning, text-processing","<p>Note that since every word has exactly one tag, overall recall and precision scores are meaningless for this task (they'll both just equal the accuracy measure). But it does make sense to ask for recall and precision measures per tag - for example, you could find the recall and precision for the DT tag.</p>

<p>The most efficient way to do this for all tags at once is similar to the way you suggested, though you can save one pass over the data by skipping the list-making stage. Read in a line of each file, compare the two lines word by word, and repeat until you reach the end of the files. For each word comparison, you probably want to check the words are equal for sanity, rather than assuming the two files are in sync. For each kind of tag, you keep three running totals: true positives, false positives and false negatives. If the two tags for the current word match, increment the true positive total for the tag. If they don't match, you need to increment the false negative total for the true tag and the false positive total for the tag your machine mistakenly chose. At the end, you can calculate recall and precision scores for each tag by following the formula in your Wikipedia excerpt.</p>

<p>I haven't tested this code and my Python's a but rusty, but this should give you the idea. I'm assuming the files are open and the <code>totals</code> data structure is a dictionary of dictionaries:</p>

<pre><code>finished = false
while not finished:
    trueLine = testFile.readline()
    if not trueLine: # end of file
        finished = true
    else:
        trueLine = trueLine.split() # tokenise by whitespace
        taggedLine = taggedFile.readline()
        if not taggedLine:
            print 'Error: files are out of sync.'
        taggedLine = taggedLine.split()
        if len(trueLine) != len(taggedLine):
            print 'Error: files are out of sync.'
        for i in range(len(trueLine)):
            truePair = trueLine[i].split('/')
            taggedPair = taggedLine[i].split('/')
            if truePair[0] != taggedPair[0]: # the words should match
                print 'Error: files are out of sync.'
            trueTag = truePair[1]
            guessedTag = taggedPair[1]
            if trueTag == guessedTag:
                totals[trueTag]['truePositives'] += 1
            else:
                totals[trueTag]['falseNegatives'] += 1
                totals[guessedTag]['falsePositives'] += 1
</code></pre>
",5114,1299783843
What is the easiest way to compare two web pages using python?,"<p>Hello I want to Compare two webpages using python script.
how can i achieve it? thanks in advance!</p>
","python, comparison, nlp","<p>First, you want to retrieve both webpages. You can use wget, urlretrieve, etc.:<br>
<a href=""https://stackoverflow.com/questions/974741/wget-vs-urlretrieve-of-python"">wget Vs urlretrieve of python</a></p>

<p>Second, you want to ""compare"" the pages.  You can use a ""diff"" tool as Chinmay noted.  You can also do a keyword analysis of the two pages:  </p>

<ol>
<li>Parse all keywords from page. e.g.  <a href=""https://stackoverflow.com/questions/1575246/how-do-i-extract-keywords-used-in-text"">How do I extract keywords used in text?</a></li>
<li>Optionally take the ""stem"" of the words with something like:<br>
<a href=""http://pypi.python.org/pypi/stemming/1.0"" rel=""nofollow noreferrer"">http://pypi.python.org/pypi/stemming/1.0</a></li>
<li>Use some math to compare the two pages' keywords, e.g. term frequency–inverse document frequency: <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Tf%E2%80%93idf</a> with some of the python tools out there like these: <a href=""http://wiki.python.org/moin/InformationRetrieval"" rel=""nofollow noreferrer"">http://wiki.python.org/moin/InformationRetrieval</a></li>
</ol>
",9183,1299602873
How does AraMorph 1.2.1 work?,"<p>I have downloaded <a href=""http://sourceforge.net/projects/aramorph/"" rel=""nofollow noreferrer"">AraMorph 1.2.1</a> Perl version from SourceForge, but I do not know how to use it. Could someone explain to me how can I get it to work?</p>
","nlp, tagging",,952,1255524711
Natural language processing to recognise numerical data ,"<p>My requirement is to recognize and extract numerical data from a natural language sentence (English only) in response to queries. Platform is Java. For example if the user query is ""What is the height of mount Everest"" and we have a paragraph as:</p>

<blockquote>
  <p>In 1856, the Great Trigonometric Survey of British India established the first published height of Everest, then known as Peak XV, at 29,002 ft (8,840 m). In 1865, Everest was given its official English name by the Royal Geographical Society upon recommendation of Andrew Waugh, the British Surveyor General of India at the time, who named it after his predecessor in the post, and former chief, Sir George Everest.[4] Chomolungma had been in common use by Tibetans for centuries, but Waugh was unable to propose an established local name because Nepal and Tibet were closed to foreigners. (Pasted from wikipedia)</p>
</blockquote>

<p>For a user query ""Height of mount Everest"" from the paragraph I need to get 29002 ft or 8840 m as the answer. Can anyone please suggest any possible ways of doing it in Java?  Are there any open source libraries for the same?</p>
","java, parsing, nlp, tokenize","<p>Obviously, doing this well is extremely difficult to do. If it's an assignment though then I'm guessing the expectation is a bit lower. Here are some thoughts to hopefully get you started:</p>

<p>I'd split the problem into 2 parts; parsing the question block and then passing the answer block. From the question block, you need to know 2 pieces of information, the noun of what you're searching for, and also the type of the answer. In this case the noun is Everest and the type is height. ""Types"" of data you can build a dictionary for fairly quickly to search your input string for (e.g. ""height"", ""weight"", ""distance"", ""age""). The nouns are more difficult, so I'd say to just assume that every non-type in the question is a potential noun, perhaps removing a dictionary of known non-nouns (such as ""at"", ""the"", ""of"" etc.).</p>

<p>Once you've identified the noun and type from the question, you can begin scanning your answer block. I'd begin by breaking that up into sentences. Then scan each sentence for each of your nouns. If one is found in that sentence, you need to scan the sentence again for numbers (taking into account possible whitespace or comma delimiting). Finally, you need to look ""around"" any numbers you find for a measurement type. So in this case, your ""type"" that we parsed from the question was ""height"". You would need to create a mapping of types to measurements, so ""height"" would map ""km, ft, in, cm, m"" etc. If the number has one of these types around it, then return the number and measurement type as the answer.</p>

<p>Hope that gets you started. As stated above, this is not intended to be a robust, commercial solution. It's homework-level.</p>
",499,1299117875
RegexpTokenize Japanese sentences - python,"<p>I'm trying to split the japanese sentences up using RegexpTokenizer but it is returning null sets. can someone tell me why? and how to get split the japanese sentences up?</p>

<pre><code>#!/usr/bin/python  # -*- encoding: utf-8 -*-

import nltk
import os, sys, re, glob
from nltk.tokenize import RegexpTokenizer

jp_sent_tokenizer = nltk.RegexpTokenizer(u'[^ 「」!?。．）]*[!?。]')

print jp_sent_tokenizer.tokenize ('の各宣言を実行しておく必要があることに注意しよう。これ以下の節では、各スクリプト例の前にこれらがすでに宣言されていることを前提とする。')
</code></pre>

<p>the output to the above code is</p>

<pre><code>[]
</code></pre>
","python, nlp, nltk","<p>I think you're just missing a unicode <strong>u</strong>:</p>

<pre><code>print jp_sent_tokenizer.tokenize (u'の各宣言を実行しておく必要があることに注意しよう。これ以下の節では、各スクリプト例の前にこれらがすでに宣言されていることを前提とする。')
</code></pre>
",2504,1299052339
Efficient keyword detection / extraction. Predefined set of keywords,"<p>How can I efficiently extract keywords with relevance from a string? My list of keywords are predefined. For example, in an article about Michelle Obama that also mentions Barack Obama, I want to extract <code>Michelle Obama</code> and <code>Barack Obama</code> with the keyword <code>Michelle Obama</code> getting a higher relevance value (both <code>Michelle Obama</code> and <code>Barack Obama</code> are present in my keywords list).</p>

<p>Checking the string for the number of occurrence of each keyword doesn't seem very efficient. My application is developed in PHP, but any language is ok, if I can do this efficiently.</p>

<p>I tried OpenCalais, but it is not detecting most of my keywords. Is it possible to extract keywords using Lucene?</p>
","java, php, text-analysis, semantic-analysis","<p>The apache lucene package will suit you. However if you have title and paragraphs, you can filter out the stop words, give higher ranks for the words in the title and then match them or their forms in the paragraphs.. you can consult some text summarization articles for better programming yourself.</p>
",993,1296568379
Extracting nouns from Noun Phase in NLP,"<p>Could anyone please tell me how to extract only the nouns from the following output:</p>

<p>I have tokenized and parsed the string ""Give me the review of movie"" based on a given grammar using following procedure:-</p>

<pre><code>sent=nltk.word_tokenize(msg)
parser=nltk.ChartParser(grammar)
trees=parser.nbest_parse(sent)
for tree in trees:
    print tree
tokens=find_all_NP(tree)
tokens1=nltk.word_tokenize(tokens[0])
print tokens1
</code></pre>

<p>and obtained the following output:</p>

<pre><code>&gt;&gt;&gt; 
(S
  (VP (V Give) (Det me))
  (NP (Det the) (N review) (PP (P of) (N movie))))
(S
  (VP (V Give) (Det me))
  (NP (Det the) (N review) (NP (PP (P of) (N movie)))))
['the', 'review', 'of', 'movie']
&gt;&gt;&gt; 
</code></pre>

<p>Now I would like to only obtain the nouns. How do I do that?</p>
","python, django, nlp",,6184,1298906088
"Inter-rater agreement (Fleiss&#39; Kappa, Krippendorff&#39;s Alpha etc) Java API?","<p>I am working on building a Question Classification/Answering corpus as a part of my masters thesis. I'm looking at evaluating my expected answer type taxonomy with respect to inter-rater agreement/reliability, and I was wondering: Does anybody know of any decent (preferably free) Java API(s) that can do this?</p>

<p>I'm reasonably certain all I need is Fleiss' Kappa and Krippendorff's Alpha at this point.</p>

<p>Weka provides a kappa statistic in it's evaluation package, but I think it can only evaluate a classifier and I'm not at that stage yet (because I'm still building the data set and classes).</p>

<p>Thanks.</p>
","math, machine-learning, nlp, classification, ontology","<p>I couldn't find an existing Java API in time for my research, so I ended up implementing both Fleiss' Kappa and Krippendorff's Alpha myself. Preliminary results for our research can be found <a href=""http://portal.acm.org/citation.cfm?id=1882992.1883077&amp;coll=DL&amp;dl=GUIDE&amp;CFID=10179128&amp;CFTOKEN=61994919"" rel=""nofollow"">in this paper</a>.</p>
",3031,1266469696
put sentences into list - python,"<p>I understand that nltk can split sentences and print it out using the following code.
but how do i put the sentences into a list instead of outputing onto the screen?</p>

<pre><code>import nltk.data
from nltk.tokenize import sent_tokenize
import os, sys, re, glob
cwd = './extract_en' #os.getcwd()
for infile in glob.glob(os.path.join(cwd, 'fileX.txt')):
    (PATH, FILENAME) = os.path.split(infile)
    read = open(infile)
    for line in read:
        sent_tokenize(line)
</code></pre>

<p>the sent_tokenize(line) prints it out. how do i put it into a list?</p>
","python, nlp, nltk, tokenize","<p>Here's a simplified version that I used to test the code:</p>

<pre><code>import nltk.data
from nltk.tokenize import sent_tokenize
import sys
infile = open(sys.argv[1])
slist = []
for line in infile:
    slist.append(sent_tokenize(line))
print slist
infile.close()
</code></pre>

<p>When called like so, it prints the following:</p>

<pre><code>me@mine:~/src/ $ python nltkplay.py nltkplay.py 
[['import nltk.data\n'], ['from nltk.tokenize import sent_tokenize\n'], ['import sys\n'], ['infile = open(sys.argv[1])\n'], ['slist = []\n'], ['for line in infile:\n'], ['    slist.append(sent_tokenize(line))\n'], ['print slist\n'], ['\n']]
</code></pre>

<p>When doing something like this, a list comprehension is more concise and IMO more pleasant to read:</p>

<pre><code>slist = [sent_tokenize(line) for line in infile]
</code></pre>

<p>To clarify, the above returns a list of lists of sentences, one list of sentences for each line. If you want a flat list of sentences, do this instead, as eyquem suggests:</p>

<pre><code>slist = sent_tokenize(infile.read())
</code></pre>
",5836,1298570960
how do i script hunpos through text files?,"<p>my purpose is to parse several text files using the POS parser HunPos <a href=""http://code.google.com/p/hunpos/wiki/UserManualI"" rel=""nofollow"">http://code.google.com/p/hunpos/wiki/UserManualI</a></p>

<p>is there a way to bash script hunpos through a bunch of text files?</p>
","linux, bash, nlp, nltk","<p>Typical mechanisms look like:</p>

<pre><code>for f in glob; do command $f ; done
</code></pre>

<p>I often run commands like: <code>for f in *; do echo -n ""$f "" ; cat $f ; done</code> to see the contents of all the files in a directory. (Especially nice with <code>/proc/sys/kernel/</code>-style directories, where all the files have very short contents.)</p>

<p>or</p>

<pre><code>find . -type f -exec command {} \;
</code></pre>

<p>or</p>

<pre><code>find . -type f -print0 | xargs -0 command parameters
</code></pre>

<p>Something like <code>find . -type f -exec file {} \;</code> or <code>find . -type f -print0 | xargs -0 file</code> (only works if the command accepts multiple filenames during input).</p>

<p>Of course, if the program accepts multiple filename arguments (like <code>cat</code> or <code>more</code> or similar Unix shell tools) and all the files are in a single directory, you can very easily run: <code>cat *</code> (show contents of all files in the directory) or <code>cat *.*</code> (show contents of all files with a period in the filename).</p>

<p>If you frequently want ""all files in all [sub]*directories"", the zsh <a href=""http://zsh.sourceforge.net/Guide/zshguide05.html#l140"" rel=""nofollow""><code>**/</code></a> option can be handy: <code>ls -l **/*.c</code> would show you <code>foo/bar/baz.c</code> and <code>/blort/bleet/boop.c</code> at once. Neat tool, but I usually don't mind writing the <code>find</code> command equivalent, I just don't need it that often. (And zsh isn't installed everywhere, so relying on its features could be frustrating in the future.)</p>
",211,1298427305
Automated question answering (FAQ) in .NET,"<p>I would like to build a very simple application - Automated FAQ. I searched the internet and found some information about different approaches but there is no .Net specific example. Do you have som experience of building such application or maybe know some .Net specific examples? It would be very interesting to take a look at one. 
Here is an example <a href=""http://193.108.42.79/ikea-us/cgi-bin/ikea-us.cgi"" rel=""nofollow"">http://193.108.42.79/ikea-us/cgi-bin/ikea-us.cgi</a>
Thanks in advance.</p>
","c#, .net, artificial-intelligence, nlp","<p>What you're trying to build is a <a href=""http://en.wikipedia.org/wiki/Chatterbot"" rel=""nofollow"">chatterbot</a>.<br>
There are many ways to go about it, not many of them trivial.<br>
Probably the simplest approach would be to incrementally build an <a href=""http://en.wikipedia.org/wiki/AIML"" rel=""nofollow"">AIML</a> knowledge base together with an AIML platform of your choice (<a href=""http://rebecca-aiml.sourceforge.net/"" rel=""nofollow"">RebeccaAIML</a> support C#).</p>
",1071,1286184738
Hashtables over large natural language word sets,"<p>I'm writing a program in python to do a unigram (and eventually bigram etc) analysis of movie reviews.  The goal is to create feature vectors to feed into libsvm.  I have 50,000 odd unique words in my feature vector (which seems rather large to me, but I ham relatively sure I'm right about that).</p>

<p>I'm using the python dictionary implementation as a hashtable to keep track of new words as I meet them, but I'm noticing an enormous slowdown after the first 1000 odd documents are processed.  Would I have better efficiency (given the distribution of natural language) if I used several smaller hashtable/dictionaries or would it be the same/worse?</p>

<p>More info:</p>

<p>The data is split into 1500 or so documents, 500-ish words each.  There are between 100 and 300 unique words (with respect to all previous documents) in each document.</p>

<p>My current code:</p>

<pre><code>#processes each individual file, tok == filename, v == predefined class
def processtok(tok, v):
    #n is the number of unique words so far, 
    #reference is the mapping reference in case I want to add new data later
    #hash is the hashtable
    #statlist is the massive feature vector I'm trying to build
    global n
    global reference
    global hash
    global statlist
    cin=open(tok, 'r')
    statlist=[0]*43990
    statlist[0] = v
    lines = cin.readlines()
    for l in lines:
        line = l.split("" "")
        for word in line:
            if word in hash.keys():
                if statlist[hash[word]] == 0:
                    statlist[hash[word]] = 1
            else:
                hash[word]=n
                n+=1
                ref.write('['+str(word)+','+str(n)+']'+'\n')
                statlist[hash[word]] = 1
    cin.close()
    return statlist
</code></pre>

<p>Also keep in mind that my input data is about 6mb and my output data is about 300mb.  I'm simply startled at how long this takes, and I feel that it shouldn't be slowing down so dramatically as it's running.</p>

<p>Slowing down: the first 50 documents take about 5 seconds, the last 50 take about 5 minutes.</p>
","python, dictionary, nlp, hashtable, libsvm","<p>@ThatGuy has made the fix, but hasn't actually told you this: </p>

<p>The major cause of your slowdown is the line</p>

<p><code>if word in hash.keys():</code></p>

<p>which laboriously makes a list of all the keys so far, then laboriously searches that list for `word'. The time taken is proportional to the number of keys i.e. the number of unique words found so far. That's why it starts fast and becomes slower and slower.</p>

<p>All you need is <code>if word in hash:</code> which in 99.9999999% of cases takes time independent of the number of keys -- one of the major reasons for having a dict.</p>

<p>The faffing about with <code>statlist[hash[word]]</code> doesn't help, either. By the way, the fixed size in <code>statlist=[0]*43990</code> needs explanation.</p>

<p><strong>More problems</strong></p>

<p>Problem A: Either (1) your code suffered from indentation distortion when you published it, or (2) <code>hash</code> will never be updated by that function. Quite simply, if <code>word</code> is not in <code>hash</code> i.e it's the first time you've seen it, absolutely nothing happens. The <code>hash[word] = n</code> statement (the ONLY code that updates <code>hash</code>) is NOT executed. So no word will ever be in <code>hash</code>.</p>

<p>It looks like this block of code needs to be shifted left 4 columns, so that it's aligned with the outer <code>if</code>:</p>

<pre><code>else:
    hash[word]=n
    ref.write('['+str(word)+','+str(n)+']'+'\n')
    statlist[hash[word]] = 1
</code></pre>

<p>Problem B: There is no code at all to update <code>n</code> (allegedly the number of unique words so far).</p>

<p>I strongly suggest that you take as many of the suggestions that @ThatGuy and I have made as you care to, rip out all the <code>global</code> stuff, fix up your code, chuck in a few print statements at salient points, and run it over say 2 documents each of 3 lines with about 4 words in each. Ensure that it is working properly. THEN run it on your big data set (with the prints suppressed). In any case you may want to put out stats (like number of documents, lines, words, unique words, elapsed time, etc) at regular intervals.</p>

<p><strong>Another problem</strong></p>

<p>Problem C: I mentioned this in a comment on @ThatGuy's answer, and he agreed with me, but you haven't mentioned taking it up:</p>

<pre><code>&gt;&gt;&gt; line = ""foo bar foo\n""
&gt;&gt;&gt; line.split("" "")
['foo', 'bar', 'foo\n']
&gt;&gt;&gt; line.split()
['foo', 'bar', 'foo']
&gt;&gt;&gt;
</code></pre>

<p>Your use of .split("" "") will lead to spurious ""words"" and distort your statistics, <em>including the number of unique words that you have</em>. You may well find the need to change that hard-coded magic number.</p>

<p>I say again: <em>There is no code that updates <code>n</code> in the function</em> . Doing <code>hash[word] = n</code> seems very strange, even if <code>n</code> is updated for each document.</p>
",1244,1298258438
Guitar anaphora resolution tool,"<p>I found a tool for anophora resolution named as guitar. </p>

<p><a href=""http://cswww.essex.ac.uk/Research/nle/GuiTAR/"" rel=""nofollow"">http://cswww.essex.ac.uk/Research/nle/GuiTAR/</a></p>

<p>I can find that the tool solves anaphora for the example they have given, but ehrn i try to execute the script, it asks for ltchunk script which is nowhere available on net.. can anyone please tell me how to use this tool or any other simple anaphoora resolution tool.. please..</p>

<p>code:</p>

<pre><code>cat $1 |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\[\["" ""@|"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\]\]"" ""|@"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\(\("" ""@~"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\)\)"" ""~@"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\("" ""~~~"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\)"" ""~~"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\["" ""@@@"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\]"" ""@@"" |
ltchunk -show_tags |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\[\["" ""&lt;ne&gt;"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\]\]"" ""&lt;/ne&gt;"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\(\("" ""&lt;ve&gt;"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\)\)"" ""&lt;/ve&gt;"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""@\|"" "" [_( [_( "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\|@"" "" ]_) ]_) "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""@~"" "" (_( (_( "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""~@"" "" )_) )_) "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""~~~"" "" (_( "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""~~"" "" )_) "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""@@@"" "" [_( "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""@@"" "" ]_) "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\._\."" "" ._. "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\?_\."" "" ?_. "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\!_\."" "" !_. "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\;_:"" "" ;_: "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""\:_:"" "" :_: "" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer \""_ "" ""\""_ |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""'_"" "" '_"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.StringReplacer ""'s_"" "" 's_"" |
java -cp gtar1.1.jar uk.ac.essex.malexa.nlp.dp.GuiTAR.txtToXML.XMLConverter $2

######## COMMENTS #########
### A script that opens a file provided as a parameter
### and replaces a given pattern of characters with a replacement string.
### The replacements go as follows:
### Pre-ltchunk processing:
### A sequence '[[', ']]', '((', or '))' in the original file
### is replaced by '@|', '|@', '@~', '~@'  respectively.
### Then ltchunk is run over the resulting file.
### Post-ltchunk processing:
### A sequence '[[', ']]', '((', or '))'
### is replaced by '&lt;ne&gt;', '&lt;/ne&gt;', '&lt;ve&gt;', '&lt;/ve&gt;' respectively.
### And finally return the original symbols hided during the pre-processing phase:
### A sequence '@|', '|@', '@~', '~@'
### is replaced by '[_([_(', ']_)]_)', '(_((_(', ')_))_)' respectively.
### The original symbols are returned in a format as if they were also processed by ltchunk.
### Notes:
### ne - stands for nominal expression
### ve - stands for verbal expression
</code></pre>
",nlp,"<p>You will probably have to do a bit of reverse engineering to figure out how to fit this into the pipeline you quoted, but <code>ltchunk</code> is very likely an earlier version of the chunker available as part of the <a href=""http://www.ltg.ed.ac.uk/software/lt-ttt2"" rel=""nofollow"">LT-TTT2</a> package.  You probably need to apply the entire TTT2 pipeline to use the chunker, so check out the documentation.</p>
",1071,1298093651
RSS Reader and then Grabbing the page content,"<p>I have integrated the RSS Reader in my application. 
How do I grab the Webpage with the RSS Feed URL?
Is there any free api which does this directly in Java using the URL?</p>

<p>I have to process the content of the webpage (news articles to be precise) and do some algorithmic stuff with that.</p>

<p>The problem now is to do a small part of Crawler. Is there any free light weight api's?</p>
","java, rss, nlp, web-crawler","<p>For getting the ""content"" of any URL, look at the <code>java.net.URL</code> class. It has some useful methods to get the content, like <code>openConnection()</code> and <code>openStream()</code> to get the content.</p>
",394,1298082330
Encoding for Multilingual .py Files,"<p>I am writing a .py file that contains strings from multiple charactersets, including English, Spanish, and Russian. For example, I have something like:</p>

<pre><code>string_en = ""The quick brown fox jumped over the lazy dog.""  
string_es = ""El veloz murciélago hindú comía feliz cardillo y kiwi.""
string_ru = ""В чащах юга жил бы цитрус? Да, но фальшивый экземпляр!""
</code></pre>

<p>I am having trouble figuring out how to encode my file to avoid generating syntax errors like the one below when my file is run:</p>

<pre><code>SyntaxError: Non-ASCII character '\xc3' in file example.py on line 128, but no encoding
declared; see http://www.python.org/peps/pep-0263.html for details
</code></pre>

<p>I've tried adding <code># -*- coding: utf-8 -*-</code> to the beginning of my file, but without any luck.  I've also tried marking my strings as unicode (i.e. <code>string_en = u'The quick brown fox jumped over the lazy dog.""</code>), again unsuccessfully.  </p>

<p>Is it possible to include characters from different Python codecs in one file, or am I attempting to do something that is not allowed?</p>
","python, unicode, encoding, nlp","<p>There are two aspects to proper encoding of strings in your use case:</p>

<ol>
<li><p>For Python to understand that you are using UTF-8 encoding, you must include in the first or second line of your code, a line that looks like <code># coding=utf-8</code>. See <a href=""http://www.python.org/dev/peps/pep-0263/"">PEP 0263</a> for details.</p></li>
<li><p>Your editor also must use UTF-8. This requires to configure it, and depends on the editor you are using. Configuration of Emacs and Vim are addressed in the same PEP, Eclipse can default to the filesystem encoding, which itself can be derived from your locale settings, etc.</p></li>
</ol>
",7827,1297702810
Searching Natural Language Sentence Structure,"<p>What's the best way to store and search a database of natural language sentence structure  trees?</p>

<p>Using <a href=""http://incubator.apache.org/opennlp/"" rel=""noreferrer"">OpenNLP's</a> English Treebank Parser, I can get fairly reliable sentence structure parsings for arbitrary sentences. What I'd like to do is create a tool that can extract all the doc strings from my source code, generate these trees for all sentences in the doc strings, store these trees and their associated function name in a database, and then allow a user to search the database using natural language queries.</p>

<p>So, given the sentence <code>""This uploads files to a remote machine.""</code> for the function <code>upload_files()</code>, I'd have the tree:</p>

<pre><code>(TOP
  (S
    (NP (DT This))
    (VP
      (VBZ uploads)
      (NP (NNS files))
      (PP (TO to) (NP (DT a) (JJ remote) (NN machine))))
    (. .)))
</code></pre>

<p>If someone entered the query ""How can I upload files?"", equating to the tree:</p>

<pre><code>(TOP
  (SBARQ
    (WHADVP (WRB How))
    (SQ (MD can) (NP (PRP I)) (VP (VB upload) (NP (NNS files))))
    (. ?)))
</code></pre>

<p>how would I store and query these trees in a SQL database?</p>

<p>I've written a simple proof-of-concept script that can perform this search using a mix of regular expressions and network graph parsing, but I'm not sure how I'd implement this in a  scalable way.</p>

<p>And yes, I realize my example would be trivial to retrieve using a simple keyword search. The idea I'm trying to test is how I might take advantage of grammatical structure, so I can weed-out entries with similar keywords, but a different sentence structure. For example, with the above query, I wouldn't want to retrieve the entry associated with the sentence <code>""Checks a remote machine to find a user that uploads files.""</code> which has similar keywords, but is obviously describing a completely different behavior.</p>
","sql, artificial-intelligence, scalability, nlp, machine-learning","<p>Relational databases cannot store knowledge in a natural way, what you actually need is a <strong>knowledge base</strong> or <strong>ontology</strong> (though it may be constructed on top of relational database). It holds data in triplets <code>&lt;subject, predicate, object&gt;</code>, so your phrase will be stored as <code>&lt;upload_file(), upload, file&gt;</code>. There's a lot of tools and methods to search inside such KBs (for example, Prolog is a language that was designed to do it). So, all you have to do is to translate sentences from natural language to <em>KB triplets/ontology graph</em>, translate user query to <em>incomplete triplets</em> (your question will look like <code>&lt;?, upload, file&gt;</code>) or <em>conjunctive queries</em> and then search on your KB. OpenNLP will help you with translating, and the rest depends on concrete technique and technologies you decide to use.  </p>
",2051,1297354774
"Difference between feature selection, feature extraction, feature weights ","<p>I am slightly confused as to what ""feature selection / extractor / weights"" mean and the difference between them. As I read the literature sometimes I feel lost as I find the term used quite loosely, my primary concerns are -- </p>

<ol>
<li><p>When people talk of Feature Frequency, Feature Presence - is it feature selection?</p></li>
<li><p>When people talk of algorithms such as Information Gain, Maximum Entropy - is it still feature selection. </p></li>
<li><p>If I train the classifier - with a feature set that asks the classifier to note the position of a word within a document as an example - would one still call this feature selection?</p></li>
</ol>

<p>Thanks
Rahul Dighe</p>
","parallel-processing, nlp, nltk, stanford-nlp",,11443,1264782408
What should i use to crawl many news articles?,"<p>I've a project of natural language processing but for that i need to crawl many web articles from some sources like Yahoo news, Google news or blogs...</p>

<p>I'm a java developper (so i'd rather use java tools). I guess i can parse each source website on my own and extract the articles with HttpClient / XPath but i'm a bit lazy :) is there a way so that i won't have to make a parser per source?</p>

<p>(I'm not only interested by new articles but articles from 2000 to now too)</p>
","java, nlp, web-crawler, stanford-nlp","<p>The hardest part of NLP is getting data you can use.  Everything else is just math.</p>

<p>It may be hard to find a large collection of news articles other than on each news source's website because of all the copyright issues involved.  If you don't need recent news, your best bet is probably to look at the Linguistic Data Consortium's <a href=""http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2009T13"" rel=""nofollow noreferrer"">English Gigaword corpus</a>; if you are at a university, there may already be an existing relationship for you to use the data for free.</p>

<p>If you need to actually crawl and parse websites, for now you'll probably find you have to write specific parsers for the various news websites to make sure you get the right text.  However, once more websites start using HTML5, it will be easier to pull out the relevant text through the use of the <a href=""http://www.quackit.com/html_5/tags/html_article_tag.cfm"" rel=""nofollow noreferrer"">article tag</a>.  </p>

<p>To do the actual crawling, <a href=""https://stackoverflow.com/questions/2495289/what-is-a-good-java-web-crawler-library"">this previous question</a> can point you in some useful directions.</p>
",2554,1290990491
Get words corresponding to a match from SpanNearQuery in Lucene,"<p>I would need to retrieve the words in my text that correspond to a match of Spans returned by SpanNearQuery.getSpans(). For instance, if my text is [a b c d e f] and I use SpanNearQueries with queries 'b' and 'e' (and sufficient slop), then I get a match 'b c d e' in my text. Now, how can I most efficiently retrieve the words as they appear in the match, that is, the sequence of words 'b c d e' itself?</p>

<p>Here is an example code of what I would need:</p>

<pre><code>SpanNearQuery allNear = new SpanNearQuery(spansTermQueries, numWordsInBetween, true);
Spans allSpans = allNear.getSpans(reader);
</code></pre>

<p>Now I would like to iterate over all the matches in allSpans, and for each match retrieve the exact words between the queries 9 the text that correspond to that match.</p>

<p>One indirect way is to get the end and start position of that match, read through the text document using a file reader, and find the string of text between position 'end' and 'start'. But that does not seem a very efficient way. It seems that this information should already be stored in the Lucene Index.</p>

<p>Would anyone know of a more direct way of retrieving the words between the queries in a match?</p>

<p>Thanks.</p>
","lucene, html, text-analysis",,441,1296737647
"How would I go about categorizing sentences according to tense (present, past, future, etc.)?","<p>I want to parse a text and categorize the sentences according to their grammatical structure, but I have a very small understanding of NLP so I don't even know where to start.</p>

<p>As far as I have read, I need to parse the text and find out (or tag?) the part-of-speech of every word. Then I search for the verb clause or whatever other defining characteristic I want to use to categorize the sentences.</p>

<p>What I don't know is if there is already some method to do this more easily or if I need to define the grammar rules separately or what.</p>

<p>Any resources on NLP that discuss this would be great. Program examples are welcome as well. I have used NLTK before, but not extensively. Other parsers or languages are OK too!</p>
","python, nlp, grammar, nltk","<p>Python <a href=""http://www.nltk.org/download"" rel=""nofollow"">Natural Language Toolkit</a> is a library which is suitable for doing such a work. As with any NLP library, you will have to download the dataset for training separately and corpus(data) and scripts for training are available too.</p>

<p>There are also certain <a href=""http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/"" rel=""nofollow"">example tutorials</a> which will help you identify parts of the speech for words. By all means, I think nltk.org should be the place to go for what you are looking for. </p>

<p>Specific questions could be posted here again.</p>
",1381,1296805540
UTF-8 Encoding Problem in Java Text Output,"<p>I've been working on testing various solutions for a Khmer Unicode Wordbreaker (Khmer does not have spaces between words which makes spell checking and grammar checking difficult, as well as converting from legacy Khmer into Khmer Unicode).</p>

<p>I was given some source code which is now online ( <a href=""http://www.whitemagicsoftware.com/software/java/wordsplit/"" rel=""nofollow"">http://www.whitemagicsoftware.com/software/java/wordsplit/</a> ) that seems promising.  The author was kind enough to give the source, but he is very busy writing a book and is unable to troubleshoot.</p>

<p>I am testing the code on a very small scale, and I am having trouble with the output.</p>

<p>Here is the input:</p>

<blockquote>
  <p>ជាដែលនឹងបានមាន</p>
</blockquote>

<p>Here's the resulting output:</p>

<blockquote>
  <p>ជារ���លនឹងបានមាន,ជា រ���ល នឹង បាន
  មាន</p>
</blockquote>

<p>The words are actually split correctly, but one word is jumbled.
The output should look like this:</p>

<blockquote>
  <p>ជាដែលនឹងបានមាន, ជា ដែល នឹង បាន មាន</p>
</blockquote>

<p>Does anyone have an insight as to why the output is garbled?</p>

<p>Here's the code with a very small Khmer lexicon and words to be split: <a href=""http://www.sbbic.org/khmerwordsplit.zip"" rel=""nofollow"">http://www.sbbic.org/khmerwordsplit.zip</a></p>

<p>And here's how to run it:</p>

<blockquote>
  <p>java -jar wordsplit.jar
  khmerlexicon.csv khmercolumns.txt >>
  results.txt</p>
</blockquote>

<p>I am very grateful to the stackoverflow community for all the help you have provided with this project so far - I hope a solution is soon to be found!</p>
","java, utf-8, nlp","<p>I noticed that it works correctly when system encoding is configured as UTF-8:</p>

<pre><code>java -Dfile.encoding=UTF-8 -jar wordsplit.jar khmerlexicon.csv khmercolumns.txt &gt;&gt; results.txt
</code></pre>

<p>Perhaps input file is assumed to be in system encoding. Read <a href=""http://balusc.blogspot.com/2009/05/unicode-how-to-get-characters-right.html"" rel=""nofollow"">BalusC's post mentioned in the comments</a> to see how to perform input/output independent from system encoding.</p>
",1359,1296729709
How to find non-alphabets using Java,"<p>I am processing text corpus. It contains several characters belonging to different languages, symbols, numbers, etc. </p>

<p>-> All I need to do is to skip the symbols like arrow mark, heart symbol, etc.</p>

<p>-> I should not be spoiling any characters of different languages. </p>

<p>Any leads? </p>

<p>----UPDATE----</p>

<p>Character.isLetter('\unicode') is working for most of them, if not some. I have checked my regional languages, it seems it's working for some but not each and every.</p>

<p>Thanks.</p>
","java, nlp, special-characters, character",,468,1296650394
Text Parsing Design,"<p>Let's say I have a paragraph of text like so:</p>

<blockquote>
  <p><em>Snails <strong>can</em> be found</strong> in a very wide
  range of environments including
  ditches, deserts, and the abyssal
  depths of the sea. Numerous kinds of <em>snail <strong>can</em>
  also be</strong> found in fresh waters. <a href=""http://en.wikipedia.org/wiki/Sea_snail"" rel=""nofollow"" title=""source"">(source)</a></p>
</blockquote>

<p>I have 10,000 regex rules to match text, which can overlap. For example, the regex <code>/Snails? can/i</code> will find two matches (italicized in the text). The regex <code>/can( also)? be/i</code> has two matches (bolded).</p>

<p>After iterating through my regexes and finding matches, what is the best data structure to use, that given some place in the text, it returns all regexes that mached it? For example, if I want the matches for line 1, character 8 (0-based, which is the <code>a</code> in <code>can</code>), I would get a match for both regexes previously described.</p>

<p>I can create a hashmap: (key: character location, value: set of all matching regexes). Is this optimal? Is there a better way to parse the text with thousands of regexes (to not loop through each one)?</p>

<p>Thanks!</p>
","python, nlp","<p>Storing all of the matches in a dictionary will work, but will it means you'll have to store all of the matches in memory at the same time. If your data is small enough to easily fit into memory, don't worry about it. Just do what works and move on.</p>

<p>If you do need to reduce memory usage of increase speed it really depends on how you are using the data. For example, if you process positions starting at the beginning and going to the end, you could use re.finditer to iteratively process all of the regexes and not maintain extra matches in memory longer then needed.</p>
",164,1296503143
NLP: Language Analysis Techniques and Algorithms,"<p><strong>Situation:</strong></p>

<p>I wish to perform a Deep-level Analysis of a given text, which would mean:</p>

<ol>
<li>Ability to extract keywords and assign importance levels based on contextual usage.</li>
<li>Ability to draw conclusions on the mood expressed.</li>
<li>Ability to hint on the education level (word does this a little bit though, but something more automated)</li>
<li>Ability to mix-and match phrases and find out certain communication patterns</li>
<li>Ability to draw substantial meaning out of it, so that it can be quantified and can be processed for answering by a machine.</li>
</ol>

<p><strong>Question:</strong></p>

<p>What kind of algorithms and techniques need to be employed for this?</p>

<p>Is there a software that can help me in doing this?</p>
",nlp,"<p>When you figure out how to do this please contact DARPA, the CIA, the FBI, and all other U.S. intelligence agencies.  Contracts for projects like these are items of current research worth many millions in research grants. ;) </p>

<p>That being said you'll need to process it in layers and analyze at each of those layers.  For items 2 and 3 you'll find training an SVM on n-tuples (try, 3) words will help.  For 1 and 4 you'll want deeper analysis.  Use a tool like <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a>, or one of the many other parsers and find the subject words in sentences and related words.  Also use <a href=""http://wordnet.princeton.edu/"" rel=""nofollow"">WordNet</a> (from Princeton)
 to find the most common senses used and take those as key words.</p>

<p>5 is extremely challenging, I think intelligent use of the data above can give you what you want, but you'll need to use all your grammatical knowledge and programming knowledge, and it will still be <em>very</em> rough grained.</p>
",1115,1295876217
Ways to store and access large (~10 GB) lists in Python?,"<p>I have a large set of strings that I'm using for natural language processing research, and I'd like a nice way to store it in Python.</p>

<p>I could use pickle, but loading the entire list into memory would then be an impossibility (I believe), as it's about 10 GB large, and I don't have that much main memory. Currently I have the list stored with the shelve library... The shelf is indexed by strings, ""0"", ""1"", ..., ""n"" which is a bit clunky.</p>

<p>Are there nicer ways to store such an object in a single file, and still have random (ish) access to it?</p>

<p>It may be that the best option is to split it into multiple lists.</p>

<p>Thanks!</p>
","python, list, nlp, pickle, shelve","<p>Depending upon how you intend to get at the data, SQLite3 might be the best approach. SQLite3 is excellent at random access to relational data, but if your data is not very relational, it might not make as much sense. (Even if all your have is an 'id' number and then your string, I think SQLite3 for underlying storage of your strings might be great.)</p>

<p>If you can figure out some mechanism to group together your strings by some way that you'd use them (say, if some of your sentences have implied objects or subjects, and you'd like to do research on them specifically; or depending upon the source of your strings, whether it be formal or informal or hyperinformal) or something like that, then you could reduce the 'working set' of your data significantly by partitioning it, and potentially drastically improving throughput of your research. But if you intend on truly random access then one big pile might be best.</p>

<p>Hope this helps.</p>
",1787,1295836287
What are some software and techniques for extracting proper names from a text?,"<p>I have a large corpus of text-based documents (100,000+) from which I want to extract proper names (e.g. a person's name).</p>

<p>Could anyone recommend techniques and/or software that would be useful in accomplishing this goal. I'm not particularly interested in low-level text parsing, so much as I am in more high-level things such as recognizing and/or ranking.</p>
","parsing, nlp, information-retrieval","<p>Something like this cannot be done reliably without some form of <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow"">Natural Language Processing</a>. A few common issues:</p>

<ul>
<li><p>Names that are also common words: <code>John Black</code></p></li>
<li><p>Multiple languages and various forms of the same word.</p></li>
<li><p>Names that refer to different things. <code>Lily</code> could be a name for a person, a place, a cat or just the flower.</p></li>
</ul>

<p>NLP can use surrounding grammar constructs to tell some of these cases apart.</p>

<p>That said, a simple (and naive) technique that you could try would be to use the capitalisation of the words. If you see a capital starting letter in the middle of a sentence, it is usually a name of some sort.</p>

<p>You might be able to reasonably assume that any such word refers to the same thing within the same document. Two such words in a sequence are probably a name/surname combination etc.</p>

<p>If capitalisation in the documents cannot be trusted, you might be able to trust that of a proper wordlist, instead, in order to get a list of proper names for the applicable languages.</p>
",874,1295650685
Server-side software for translating languages?,"<p>I am searching for a server-side application (not a service, we need to host this ourselves) that can take a given string and translate it to another language. Open-source, paid, doesn't matter.</p>

<p>Can anyone provide some recommendations?</p>
","translation, nlp, server-side, machine-translation",,1424,1295540647
Natural language processing library for auto-tagging (.NET),"<p>Dose anyone know of any good libraries out there for .NET that could help pull keywords out of blocks of natural language.</p>

<p>I'm basically trying to strip out stop words and ignore tenses, plurals and generally    find words that are essentially the same. </p>

<p>Some abilities to find synonyms would be nice, especially if it includes things like business/technology/non-dictionary words. </p>
","c#, .net, parsing, nlp",,3853,1291740008
Is there an open source tool based on perl or python to generate the summary of mindmap of documents,"<p>I am really looking for a toolkit or readymade tool which will parse a given document and then generate a brief summary of better still a mindmap of the document. I know Python has ntlk and perl has quite a few  modules which will help in natural language parsing etc. 
It is even feasible to write a tool to do so, with using ntlk like tool kit, but for the lack of time. Would appreciate if you know of some such tool or has some pointer to such a tool, if you could post it here, with thanks in advance. </p>
","python, perl, nlp",,557,1295431133
How can I programmatically generate relevant tags for a database of URLs?,"<p>I'm writing an RSS reader in python as a learning exercise, and I would really like to be able to tag individual entries with keywords for searching.  Unfortunately, most real-world feeds don't include keyword metadata.  I currently have about 60,000 entries in my test database from about 600 feeds, so manually tagging is not going to be effective.  So far I have only been able to find two solutions:</p>

<p><strong>1: Use <a href=""http://code.google.com/p/nltk/"" rel=""noreferrer"">Natural Language Toolkit</a> to extract keywords:</strong></p>

<ul>
<li>Pros: flexible; no dependencies on external services;</li>
<li>Cons: can only index the article summary, not the article; non-trivial: writing a high quality keyword extraction tool is a project in itself;</li>
</ul>

<p><strong>2: Use the <a href=""http://code.google.com/apis/adwords/docs/reference/latest/TargetingIdeaService.RelatedToUrlSearchParameter.html"" rel=""noreferrer"">Google Adwords API</a> to fetch keyword suggestions from the article url:</strong></p>

<ul>
<li>Pros: Super high quality keywords; based on entire article text; easy to use;</li>
<li>Cons: Not free(?); Query rate limits unknown; I'm terrified of getting my account banned and not being able to run adwords campaigns for my commercial sites;</li>
</ul>

<p>Can anyone offer any suggestions?  Are my fears about getting my adwords account banned unfounded?</p>
","python, nlp, keyword, google-ads-api","<p>You can use <strong><a href=""http://www.delicious.com/help/api#posts_suggest"" rel=""nofollow"">delicious suggested tags API</a></strong>.</p>

<p>An example of how to use the api via python <a href=""http://www.michael-noll.com/projects/delicious-python-api/"" rel=""nofollow"">http://www.michael-noll.com/projects/delicious-python-api/</a></p>

<p>An other alternative is <strong><a href=""http://www.opencalais.com/"" rel=""nofollow"">Open Calais</a></strong></p>
",2185,1295242125
Word Base/Stem Dictionary,"<p>It seems my Google-fu is failing me.</p>

<p>Does anyone know of a freely available word base dictionary that just contains bases of words? So, for something like strawberries, it would have strawberry. But does NOT contain abbreviations or misspellings or alternate spellings (like UK versus US)? Anything quickly usable in Java would be good but just a text file of mappings or anything that could be read in would be helpful.</p>
","java, dictionary, nlp, stemming",,3217,1288106365
Apostrophes Converted to Correct Text?,"<p>Goal:  I need to be able to convert apostrophes to properly formed words. - at least for the most common words with apostrophes.  To do this ideally I'd want a list of words and their implied conterparts (i.e. ""don't"" and ""do not"").  </p>

<p>Issue: I'm creating a search algorithm based on natural language processing, but when users create content (or search) using an apostrophe, it causes issues for us.  Mostly because if we were to simply remove the apostrophe we would have (don't -> dont) (doesn't -> doesnt), which officially is not an english word, and can't be translated by the NLP system.</p>

<p>The ideal solution is simply a one to one mapping of what these items should be converted to, but I'm unaware of such a list.</p>

<p>Please let me know if you know of one, and where I might be able to find it.</p>

<p>thx</p>
","algorithm, text, nlp, text-parsing","<p>This looks like a pretty good list: 
<a href=""http://www.textfixer.com/resources/english-contractions-list.php"" rel=""nofollow"">http://www.textfixer.com/resources/english-contractions-list.php</a></p>

<p>Depends on how good you want to make your system.  Is it going to understand that ""gonna"" is ""going to"" and ""gotta"" is ... well, that's a tough one.  It could mean ""got to"" (""have to"", ""must""), or ""got a"" (""have a"").</p>

<p>Oh, the things we learn when we try to teach our computers to communicate.</p>
",1093,1294864871
How do I count words in an nltk plaintextcorpus faster?,"<p>I have a set of documents, and I want to return a list of tuples where each tuple has the date of a given document and the number of times a given search term appears in that document.  My code (below) works, but is slow, and I'm a n00b.  Are there obvious ways to make this faster?  Any help would be much appreciated, mostly so that I can learn better coding, but also so that I can get this project done faster!  </p>

<pre><code>def searchText(searchword):
    counts = []
    corpus_root = 'some_dir'
    wordlists = PlaintextCorpusReader(corpus_root, '.*')
    for id in wordlists.fileids():
        date = id[4:12]
        month = date[-4:-2]
        day = date[-2:]
        year = date[:4]
        raw = wordlists.raw(id)
        tokens = nltk.word_tokenize(raw)
        text = nltk.Text(tokens)
        count = text.count(searchword)
        counts.append((month, day, year, count))

    return counts
</code></pre>
","python, nlp, nltk, corpus","<p>If you just want a frequency of word counts, then you don't need to create <code>nltk.Text</code> objects, or even use <code>nltk.PlainTextReader</code>. Instead, just go straight to <code>nltk.FreqDist</code>.</p>

<pre><code>files = list_of_files
fd = nltk.FreqDist()
for file in files:
    with open(file) as f:
        for sent in nltk.sent_tokenize(f.lower()):
            for word in nltk.word_tokenize(sent):
                fd.inc(word)
</code></pre>

<p>Or, if you don't want to do any analysis - just use a <code>dict</code>.</p>

<pre><code>files = list_of_files
fd = {}
for file in files:
    with open(file) as f:
        for sent in nltk.sent_tokenize(f.lower()):
            for word in nltk.word_tokenize(sent):
                try:
                    fd[word] = fd[word]+1
                except KeyError:
                    fd[word] = 1
</code></pre>

<p>These could be made much more efficient with generator expressions, but I'm used for loops for readability.</p>
",7294,1286742337
What commercially-available platforms similar to OpenCalais or AlchemyAPI are there for entity extraction for Chinese and Japanese languages?,"<p>In particular, I would like to be able to extract people, places, films, music, etc. entities and have the entities available in widely used linked data IDs such as DBpedia, Freebase, or OpenCyc.</p>
","entity, nlp, semantics, freebase, opencalais",,737,1292670753
Is NER necessary for Coreference resolution?,"<p>... or is gender information enough?
More specifically, I'm interested in knowing if I can reduce the number of models loaded by the Stanford Core NLP to extract coreferences. I am not interested in actual named entity recognition.</p>

<p>Thank you</p>
","nlp, stanford-nlp, named-entity-recognition","<p>According to the EMNLP paper that describes the coref system packaged with Stanford CoreNLP, named entities tags are just used in the following coref annotation passes: <em>precise constructs</em>, <em>relaxed head matching</em>, and <em>pronouns</em> <a href=""http://cs.stanford.edu/people/nc/pubs/emnlp2010-sieve-coref.pdf"">(Raghunathan et al. 2010)</a>. </p>

<p>You can specify what passes to use with the <strong>dcoref.sievePasses</strong> configuration property. If you want coreference but you don't want to do NER, you should be able to just run the pipeline without NER and specify that the coref system should only use the annotation passes that don't require NER labels. </p>

<p>However, the resulting coref annotations will take a hit on <a href=""http://en.wikipedia.org/wiki/Recall_%28information_retrieval%29"">recall</a>. So, you might want to do some experiments to determine whether the degraded quality of the annotations is problem for whatever your are using them for downstream.</p>
",1513,1292441588
Keyword Analyser,"<p>I want to know any algorithms or php code for working out keyword competition. The keyword can be used multiple sites per website and on multiple websites. I want to know how its ranking can be worked out.</p>

<p>Thanks</p>
","php, regex, analysis, text-manipulation, text-analysis","<p>You want a search engine? That's a pretty ambitious task. Quite a lot to ask over a volunteer driven PHP thread.
Since I'm a nice guy, I'll give you the Regex for free, but you'll have to do some digging to build that search engine you want.</p>

<pre><code>/&lt;title&gt;([^&lt;])*&lt;/title&gt;/gi
</code></pre>

<p>If you really are serious about the SE, try starting with <a href=""http://onlamp.com/pub/a/php/2002/10/24/simplesearchengine.html"" rel=""nofollow"">this article from O'Rilley</a>.</p>
",340,1294594044
understanding semcor corpus structure h,"<p>I'm learning NLP.  I currently playing with Word Sense Disambiguation.  I'm planning to use the semcor corpus as training data but I have trouble understanding the xml structure.  I tried googling but did not get any resource describing the content structure of semcor.</p>

<pre><code>&lt;s snum=""1""&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;The&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""group"" lexsn=""1:03:00::"" pn=""group"" pos=""NNP"" rdf=""group"" wnsn=""1""&gt;Fulton_County_Grand_Jury&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""say"" lexsn=""2:32:00::"" pos=""VB"" wnsn=""1""&gt;said&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""friday"" lexsn=""1:28:00::"" pos=""NN"" wnsn=""1""&gt;Friday&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;an&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""investigation"" lexsn=""1:09:00::"" pos=""NN"" wnsn=""1""&gt;investigation&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""IN""&gt;of&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""atlanta"" lexsn=""1:15:00::"" pos=""NN"" wnsn=""1""&gt;Atlanta&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""POS""&gt;'s&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""recent"" lexsn=""5:00:00:past:00"" pos=""JJ"" wnsn=""2""&gt;recent&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""primary_election"" lexsn=""1:04:00::"" pos=""NN"" wnsn=""1""&gt;primary_election&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""produce"" lexsn=""2:39:01::"" pos=""VB"" wnsn=""4""&gt;produced&lt;/wf&gt;
&lt;punc&gt;``&lt;/punc&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;no&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""evidence"" lexsn=""1:09:00::"" pos=""NN"" wnsn=""1""&gt;evidence&lt;/wf&gt;
&lt;punc&gt;''&lt;/punc&gt;
&lt;wf cmd=""ignore"" pos=""IN""&gt;that&lt;/wf&gt;
&lt;wf cmd=""ignore"" pos=""DT""&gt;any&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""irregularity"" lexsn=""1:04:00::"" pos=""NN"" wnsn=""1""&gt;irregularities&lt;/wf&gt;
&lt;wf cmd=""done"" lemma=""take_place"" lexsn=""2:30:00::"" pos=""VB"" wnsn=""1""&gt;took_place&lt;/wf&gt;
&lt;punc&gt;.&lt;/punc&gt;
&lt;/s&gt;
</code></pre>

<ul>
<li>I'm assuming wnsn is 'word sense'.  Is it correct?</li>
<li>What does the attribute lexsn mean? How does it map to wordnet?</li>
<li>What does the attribute pn refer to? (third line)</li>
<li>How is the rdf attribute assigned? (again third line)</li>
<li>In general, what are the possible attributes?</li>
</ul>
","linguistics, corpus, nlp","<p>The format is described in the ""doc/cxtfile.txt"" file in the <a href=""http://lit.csci.unt.edu/~rada/downloads/semcor/semcor1.6.tar.gz"" rel=""noreferrer"">SemCor 1.6 archive</a>; for some reason, documentation is not included in later versions.</p>
",1923,1294050420
Rules for word ranking in sentences,"<p>I am looking for research and algorithms on classifying document importance based on the location of a searched keyword in the sentence. I remember seeing interesting papers on this topic before, but now that I need those I cannot find the good ones. </p>

<p>Can you please point me to recent or classic works in sentence relevance to a query?</p>

<p>Thank you very much!</p>

<p>Evgeniy</p>
",nlp,,336,1292590257
"Is there a range(&#39;a&#39;,&#39;z&#39;) for Non-English Alphabet","<p>PHP has a function <code>range('a','z')</code> which prints the English alphabet a, b, c, d, etc. </p>

<p>Is there a similar function for other alphabets? maybe a function that accepts the language as a parameter</p>
","php, string, nlp","<pre><code>$arr = range('а', 'я');

var_dump($arr);
</code></pre>

<p>But this will work with <code>cp1251</code> only and lost <code>ё</code>, since it is in the end of the ascii table</p>
",1184,1291618940
How to sift idioms and set phrases apart from other common phrases using NLP techniques?,"<p>What techniques exist that can tell the difference betwen plain common phrases such as ""to the"", ""and the"" and set phrases and idioms which have their own lexical meanings such as ""pick up"", ""fall in love"", ""red herring"", ""dead end""?</p>

<p>Are there techniques which are successful even without a dictionary, statistical methods HMMs train on large corpora for instance?</p>

<p>Or are there heuristics such as ignoring or weighting down ""promiscuous"" words which can co-occur with just about any word versus words which occur either alone or in a specific limited set of idiomatic phrases?</p>

<p>If there are such heuristics, how do we take into account set phrases and verbal phrases which do incorporate promiscuous words such as ""up"" in ""beat up"", ""eat up"", ""sit up"", ""think up""?</p>

<p><strong>UPDATE</strong></p>

<p>I've found an interesting paper online: <a href=""http://www.aclweb.org/anthology/J/J09/J09-1005.pdf"" rel=""nofollow"">Unsupervised Type and Token Identiﬁcation of Idiomatic Expressions</a></p>
","nlp, phrase, hidden-markov-models",,592,1293540396
Phonemes and Chronemes,"<p>Is there any methodology/algorithm for computing <a href=""http://en.wikipedia.org/wiki/Phoneme"" rel=""nofollow"">phonemes</a> and <a href=""http://en.wikipedia.org/wiki/Chroneme"" rel=""nofollow"">chronemes</a> of a word(text and not audio)?</p>
","python, nlp","<p><a href=""http://129.173.35.31/~pf/Linguistique/L2P-NeuralNetwork/TextToPhoneme1.pdf"" rel=""nofollow"">Here</a> you have all the explanation of how to compute text to phonemes using neural networks. It is a pretty complex task. Hope it helps.</p>
",550,1293605688
What&#39;s the best way to generate keywords from a given Text?,"<p>I want to generate Keywords for my CMS.</p>

<p>Does someone know a good PHP Script (or something else) which generates keywords?</p>

<p>I have a HTML Site like this: <a href=""http://pastebin.com/ZU8vdyeP"" rel=""nofollow"">http://pastebin.com/ZU8vdyeP</a> </p>
","algorithm, nlp, seo","<p>This is a very hard problem for a computer to solve. It would be much easier to get somebody (else?) to do it manually, or simply not do it at all.</p>

<p>If you'd really need a computer to do it, I'd head over to the excellent <strong>Python library <a href=""http://nltk.org/"" rel=""nofollow"" title=""NLTK"">NLTK</a></strong> which has many tools for this sort of thing (=natural language processing), and it's a lot of fun to work with.</p>

<p>For example, you could calculate a frequency distribution of the words, and then search for the most common hypernyms of larger (above say 5 char) words that appear most frequently and use that as a hint of what the keywords could be.</p>

<p>Again, it is much easier to get it done by a human, however.</p>
",3600,1293402321
Parsing a Syntax Tree with Perl Regex,"<p>Perhaps regex is not the best way to parse this, tell me if I it is not. Anyway, here are some examples of what the syntax tree looks like:</p>

<pre><code>(S (CC and))  
(SBARTMP (IN once) (NP otherstuff))   
(S (S (NP blah (VP blah)) (CC then) (NP blah (VP blah (PP blah))) ))   
</code></pre>

<p>Anyway, what I am trying to do is pull the connective out (and, then, once, etc) and its corresponding head (CC,IN,CC), which I already know for each syntax tree so it can act as an anchor, and I also need to retrieve its parent (in the first it is S, second SBARTMP, and third it is S), and its siblings, if there are any (in the first none, in the second left hand side sibling, and third left-hand-side and right-hand-side sibling). Anything higher than the parent is not included  </p>

<pre><code>my $pos = ""(\\\w|-)*"";  
my $sibling = qr{\s*(\\((?:(?&gt;[^()]+)|(?1))*\\))\s*};  
my $connective = ""once"";  
my $re = qr{(\(\w*\s*$sibling*\s*\\(IN\s$connective\\)\s*$sibling*\s*\))};  
</code></pre>

<p>This code works for things like:  </p>

<pre><code>my $test1 = ""(X (SBAR-TMP (IN once) (S sdf) (S sdf)))"";  
my $test2 = ""(X (SBAR-TMP (IN once))"";  
my $test3 = ""(X (SBAR-TMP (IN once) (X as))"";  
my $test4 = ""(X (SBAR-TMP (X adsf) (IN once))"";  
</code></pre>

<p>It will throw away the X on top and keep everything else, however, once the siblings have stuff embedded in them then it does not match because the regex does not go deeper.</p>

<pre><code>my $test = ""(X (SBAR-TMP (IN once) (MORE stuff (MORE stuff))))"";  
</code></pre>

<p>I am not sure how to account for this. I am kind of new to the extended patterns for Perl, just started learning it. To clarify a bit about what the regex is doing: it looks for the connective within two parentheses and the capital-letter/- combo, looks for a complete parent of the same format closing with two parentheses and then should look for any number of siblings that have all their parentheses paired off.</p>
","regex, perl, syntax, nlp","<p>To only get the nearest 'parent' to your anchor connective you can
do it as a recursive parent with a FAIL or do it directly.
(for some reason I can't edit my other posts, must be cookies being deleted).</p>

<pre><code>use strict;
use warnings;

my $connective = qr/ \((?:IN|CC)\s(?:once|and|then)\)/x;
my $sibling = qr/
  \s*
  ( 
     (?! $connective )
     \(
        (?:
            (?&gt; (?: [^()]+ ) )
          | (?-1)
        )*
     \)
  )
  \s*
 /x;

my $regex1 = qr/
      \( ( [\w-]+ \s* $sibling* \s* $connective \s* $sibling* ) \) #1
 /x;

my $regex2 = qr/
   ( #1
     \( \s*
        (  #2
           [\w-]+ \s*
           (?&gt;   $sibling* \s* $connective (?(R)(*FAIL)) \s* $sibling*
               | (?1)
           )
        )
        \s*
     \)
   )
 /x;


my $sample = qq/
 (X (SBAR-TMP (IN once) (S sdf) (S sdf)))
 (X (SBAR-TMP (IN once))
 (X (SBAR-TMP (IN once) (X as))
 (X (SBAR-TMP (X adsf) (IN once))
 (X (SBAR-TMP (IN once) (MORE stuff (MORE stuff))))
 (S (CC and))  
 (SBARTMP (IN once) (NP otherstuff))   
 (S (S (NP blah (VP blah)) (CC then) (NP blah (VP blah (PP blah))) ))
/;

while ($sample =~ /$regex1/xg) {
    print ""Found:   $1\n"";
}
print '-' x 20, ""\n"";

while ($sample =~ /$regex2/xg) {
    print ""Found:   $2\n"";
}

__END__
</code></pre>
",752,1293109418
another porter stemming algorithm implementation question?,"<p>I am trying to implement porter stemming algorithm, but i am having difficualties understanding this point </p>

<blockquote>
  <p>Step 1c</p>

<pre><code>(*v*) Y -&gt; I                    happy        -&gt;  happi
                                sky          -&gt;  sky
</code></pre>
</blockquote>

<p>Isn't that the the opposite of what we want to do , why does the algorithim convert the Y into I.</p>

<p>for the complete algorithm here <a href=""http://tartarus.org/~martin/PorterStemmer/def.txt"" rel=""nofollow"">http://tartarus.org/~martin/PorterStemmer/def.txt</a></p>

<p>Thanks</p>
","algorithm, nlp, porter-stemmer","<p>The Porter stemmer and other <a href=""http://en.wikipedia.org/wiki/Stemming"" rel=""nofollow"">stemming algorithms</a> don't always return words; they return  <a href=""http://en.wikipedia.org/wiki/Word_stem"" rel=""nofollow"">word stems</a>.  The goal is that related words should have the same stem.  As long as ""happiness"", ""happy"", and ""happily"" all reduce to the same stem, then your stemmer is doing its job, even if the stem isn't a real word.</p>
",578,1293147220
How to find out if a sentence is a question (interrogative)?,"<p>Is there an open source Java library/algorithm for finding if a particular piece of text is a question or not?
<br/>
I am working on a question answering system that needs to analyze if the text input by user is a question.
<br/> 
I think the problem can probably be solved by using opensource NLP libraries but its obviously more complicated than simple part of speech tagging. So if someone can instead tell the algorithm for it by using an existing opensource NLP library, that would be good too.
<br/>
Also let me know if you know a library/toolkit that uses data mining to solve this problem. Although it will be difficult to get sufficient data for training purposes, I will be able to use stack exchange data for training.</p>
","java, algorithm, nlp, data-mining, text-processing","<p>In a syntactic parse of a question, the correct structure will be in the form of:</p>

<pre><code>(SBARQ (WH+ (W+) ...)
       (SQ ...*
           (V+) ...*)
       (?))
</code></pre>

<p>So, using anyone of the syntactic parsers available, a tree with an SBARQ node having an embedded SQ (optionally) will be an indicator the input is a question. The WH+ node (WHNP/WHADVP/WHADJP) contains the question stem (who/what/when/where/why/how) and the SQ holds the inverted phrase.</p>

<p>i.e.: </p>

<pre><code>(SBARQ 
  (WHNP 
    (WP What)) 
  (SQ 
    (VBZ is) 
    (NP 
      (DT the) 
      (NN question)))
  (. ?))
</code></pre>

<p>Of course, having a lot of preceeding clauses will cause errors in the parse (that can be worked around), as will really poorly-written questions. For example, the title of this post ""How to find out if a sentence is a question?"" will have an SBARQ, but not an SQ.</p>
",9363,1282815677
Search many expressions in many documents using Python,"<p>I often have to search many words (1000+) in many documents (million+). I need position of matched word (if matched).</p>

<p>So slow pseudo version of code is</p>

<pre><code>for text in documents:
     for word in words:
             position = search(word, text)
             if position:
                  print word, position 
</code></pre>

<p>Is there any fast Python module for doing this? Or should I implement something myself?</p>
","python, nlp","<p>For fast exact-text, multi-keyword search, try acora - <a href=""http://pypi.python.org/pypi/acora/1.4"" rel=""nofollow"">http://pypi.python.org/pypi/acora/1.4</a></p>

<p>If you want a few extras - result relevancy, near-matches, word-rooting etc, Whoosh might be better - <a href=""http://pypi.python.org/pypi/Whoosh/1.4.1"" rel=""nofollow"">http://pypi.python.org/pypi/Whoosh/1.4.1</a></p>

<p>I don't know how well either scales to millions of docs, but it wouldn't take long to find out!</p>
",417,1292356027
"How to write an internal, natural language, DSL using Ruby + Regex?","<p>I’m looking for a simple example of how to write an internal DSL using Ruby and regex pattern matching. Similar to how Sinatra handles routes</p>

<pre><code>get '/say/*/to/*' do
   # Some Ruby code here
end
</code></pre>

<p>Also similar to how Cucumber handles step definitions:</p>

<pre><code>Given /^I have (\d+) cucumbers in my belly$/ do |cukes|
   # Some Ruby code here
end
</code></pre>

<p>I’m not interested in builders, or fluent method chaining.  Basically I want a Ruby class which looks something like this:</p>

<pre><code>class SpecVocabulary

   match ‘pattern’ do
      # Some Ruby code here
   end

   # Or, using a different keyword
   phrase ‘pattern’ do
      # Some Ruby code here
   end
end
</code></pre>

<p>What I’m struggling with is wiring-up the code which makes the SpecVocabular class automatically match patterns and fill out it’s data.</p>

<p>I’m hoping someone has a simple example of how to do this, I’m trying to avoid having to dive in the source for Sinatra and Cucumber.</p>

<p>Incidentally I already have the natural language define, though I omitted it purposely.</p>
","ruby, regex, nlp, dsl",,529,1291999850
Converting strings to numbers and back to natural language?,"<p>I'm working with a recipe database application and I need the ability to convert natural language strings to numbers and vice versa. For example, I need 1 1/4 to convert to 1.25, and I'll need to be able to convert 1.25 back into 1 1/4.</p>

<p>Is there any library or built-in functions that can do this?</p>
","java, numbers, nlp","<p>Check this <a href=""https://stackoverflow.com/questions/4270019/how-to-represent-mixed-fractions-in-java"">answer</a> for representing mixed fractions and when you need to convert from the string back to mixed fraction split the string,convert the 3 string's to numbers and pass to the the <code>MixedFraction</code> class.</p>
",717,1291872213
Use of Stanford Parser in Web Service,"<p>I need to use the Stanford Parser in a web service. As SentenceParser loads a big object, I will make sure it is a singleton, but in this case, is it thread safe (no according to <a href=""http://nlp.stanford.edu/software/parser-faq.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/parser-faq.shtml</a>). How else would it be done efficiently? One option is locking the object while being used.</p>

<p>Any idea how the people at Stanford are doing this for <a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/</a> ?</p>
","java, web-services, thread-safety, nlp, stanford-nlp",,1049,1291215702
Algorithm choice for gaining intelligence from messages,"<p>What I'm trying to do is find an algorithm that can I can implement to generate 'intelligent' suggestions to people, by comparing messages they send to messages sent by their peers.</p>

<p>For example, Person A sends a message to Person B talking about Obj1. If Person C sends a message to Person D about Obj1, it will notice they are talking about the same things, and may suggest Person A talks to person C.</p>

<p>I have implemented collecting the statistics to capture the mentions people have in common but do not know which algorithm to use to analyse this.</p>

<p>Any suggestions?
(I hope this makes enough sense)</p>
","algorithm, statistics, artificial-intelligence, text-analysis","<p>take a look at  <a href=""http://en.wikipedia.org/wiki/Data_clustering"" rel=""nofollow"">clustering algorithms</a></p>

<p>and <a href=""http://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow"">k-means</a> or
 <a href=""http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm"" rel=""nofollow"">k-nearest neighbours</a> for a quick start </p>

<p>How much data you've got? The more the better.
There are lots of approaches to this problem. You may for example take that all users, to some degree, are similar to each other and what you want to do is to find for each user the most similar ones.Vector space, cosine similarity, will give you quick results.
Give some more information on what you want to achieve.</p>
",158,1291469740
MySQL Natural Lanquage Search not working as I&#39;d hoped,"<p>I have a table of people's Full Names.  I'd like users to be able to search it by partial names and misspelled names. So a search for 'Andrew' should also return 'Andrea', etc.  I thought <code>FULLTEXT</code> search was the answer but it doesn't seem to work any differently than if I'd searched using <code>... LIKE '%Andrew%'</code>.<br>
Is there a function or feature in MySQL that will search based on string similarity?  Or will I have to roll my own on the PHP end using <code>levenshtein()</code> or something similar?  </p>

<p><strong>Given This Table:</strong>  </p>

<pre><code>CREATE TABLE `people` (
  `FullName` varchar(30) default NULL,
  `namesID` int(11) NOT NULL auto_increment,
  PRIMARY KEY  (`namesID`),
  FULLTEXT KEY `fulltext_FullName` (`FullName`)
) ENGINE=MyISAM AUTO_INCREMENT=15 DEFAULT CHARSET=utf8;

LOCK TABLES `people` WRITE;
/*!40000 ALTER TABLE `people` DISABLE KEYS */;
INSERT INTO `people` (`FullName`,`namesID`)
VALUES
    ('Mark Peters',1),
    ('Bob Jackson',2),
    ('Steve Kipp',3),
    ('Joe Runty',4),
    ('Tina Mardell',5),
    ('Tim Havers',6),
    ('Rich Beckett',7),
    ('Mary Dalson',8),
    ('Maria Grento',9),
    ('Michael Colt',10),
    ('Andrew Peters',11),
    ('Andre Bison',12),
    ('Andrea Masters',13),
    ('Marla Tool',14);

/*!40000 ALTER TABLE `people` ENABLE KEYS */;
UNLOCK TABLES;
</code></pre>

<p><strong>And This Query:</strong>  </p>

<pre><code>SELECT *
FROM people
WHERE MATCH(FullName) AGAINST('Andrew');
</code></pre>

<p><strong>I only get:</strong>  </p>

<pre><code>FullName        namesID
Andrew Peters   11
</code></pre>

<p><strong>When I'd like to also get:</strong>  </p>

<pre><code>Andre Bison
Andrea Masters
</code></pre>

<p>etc.</p>
","mysql, nlp, full-text-search","<p>FULLTEXT indices are nothing more than indices on the full text. They only allow searches on the text you actually have.</p>

<p>MySQL does have a <code>SOUNDEX()</code> function, and a shorthand <code>x SOUNDS LIKE y</code> operator, which is the same as <code>SOUNDEX(x) = SOUNDEX(y)</code>.</p>

<p>If soundex doesn't meet your needs, you would indeed need to involve a programming language like PHP to accomplish what you want.</p>
",219,1291491208
Get the word under the mouse cursor in Windows,"<p>Greetings everyone,</p>

<p>A friend and I are discussing the possibility of a new project: A translation program that will pop up a translation whenever you hover over any word in any control, even static, non-editable ones.  I know there are many browser plugins to do this sort of thing on webpages; we're thinking about how we would do it system-wide (on Windows).</p>

<p>Of course, the key difficulty is figuring out the word the user is hovering over.  I'm aware of MSAA and Automation, but as far as I can tell, those things only allow you to get the entire contents of a control, not the specific word the mouse is over.</p>

<p>I stumbled upon this (proprietary) application that does pretty much exactly what we want to do:  <a href=""http://www.gettranslateit.com/"" rel=""nofollow"">http://www.gettranslateit.com/</a></p>

<p>Somehow they are able to get the exact word the user is hovering over in almost any application (It seems to have trouble in a few apps, notably Windows Explorer).  It even grabs text out of obviously custom-drawn controls, somehow.  At first I thought it must be using OCR.  But even when I shrink the font so far down that the text becomes a completely unreadable blob, it can still recognize words perfectly.  (And yet, it doesn't recognize anything if I change the font to Wingdings.  But maybe that's by design?)</p>

<p>Any ideas as to how it's achieving this seemingly impossible task?</p>

<p>EDIT: It doesn't work with Wingdings, but it does work with some other nonsense fonts, so I've confirmed it can't be OCR.</p>
","windows, automation, accessibility, translation, nlp","<p>You could capture the GDI calls that output text to the display, and then figure out which word's bounding box the cursor falls in.</p>
",1967,1286409410
Does anyone know of a good quick and dirty text / grammar parser?,"<p>I have a ""mad lib"" scenario in which I want to </p>

<p>a) determine the parts of speech of every (or most) words in a sentence<br>
b) have the user select alternatives to those words - or replace them computationally with equivalent words</p>

<p>I looked at the Stanford parser but its a bit slow ... any suggestions?</p>
","parsing, nlp, pos-tagger","<p><strong>Use a POS tagger</strong></p>

<p>If you're just using the part-of-speech (POS) tags and not the parse trees, you don't actually need to use a parser. Instead, you can just use a standalone POS tagger. </p>

<p>POS tagging is <strong>much</strong> faster than phrase-structure parsing. On a Xeon E5520, the <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford POS tagger</a> can tag 1700 sentences in 3 seconds, while the same data takes about 10 minutes to parse using the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">Stanford Parser</a> (<a href=""http://www.lrec-conf.org/proceedings/lrec2010/pdf/730_Paper.pdf"" rel=""nofollow"">Cer et al. 2010</a>). </p>

<p>There's a fairly comprehensive list of other POS taggers <a href=""http://www-nlp.stanford.edu/links/statnlp.html#Taggers"" rel=""nofollow""><strong>here</strong></a>. </p>
",411,1291052085
Better distance metrics besides Levenshtein for ordered word sets and subsequent clustering,"<p>I am trying to solve a problem that involves comparing large numbers of word sets , each of which contains a large, ordered number of words from a set of words (totaling around 600+, very high dimensionality!) for similarity and then clustering them into distinct groupings. The solution needs to be as unsupervised as possible.</p>

<p>The data looks like</p>

<p>[Apple, Banana, Orange...]<br>
[Apple, Banana, Grape...] <br>
[Jelly, Anise, Orange...]<br>
[Strawberry, Banana, Orange...]<br>
...etc</p>

<p>The order of the words in each set matters ([Apple, Banana, Orange] is distinct from [Apple, Orange, Banana]</p>

<p>The approach I have been using so far has been to use Levenshtein distance (limited by a distance threshold) as a metric calculated in a Python script with each word being the unique identifier, generate a similarity matrix from the distances, and throwing that matrix into k-Mediods in KNIME for the groupings.</p>

<p>My questions are:</p>

<ul>
<li>Is Levenshtein the most appropriate distance metric to use for this problem?</li>
<li>Is mean/medoid prototype clustering the best way to go about the groupings?</li>
<li>I haven't yet put much thought into validating the choice for 'k' in the clustering. Would evaluating an SSE curve of the clustering be the best way to go about this?</li>
<li>Are there any flaws in my methodology? </li>
<li>As an extension to the solution in the future, given training data, would anyone happen to have any ideas for going about assigning probabilities to cluster assignments? For example, set 1 has a 80% chance of being in cluster 1, etc. </li>
</ul>

<p>I hope my questions don't seem too silly or the answers painfully obvious, I'm relatively new to data mining.</p>

<p>Thanks!</p>
","algorithm, nlp, cluster-analysis, data-mining, levenshtein-distance","<p>Yes, Levenshtein is a very suitable way to do this.  But if the sequences vary in size much, you might be better off <strong>normalising</strong> these distances by dividing by the sum of the sequence lengths -- otherwise you will find that observed distances tend to increase for pairs of long sequences whose ""average distance"" (in the sense of the average distance between corresponding k-length substrings, for some small k) is constant.</p>

<p>Example: The pair <code>([Apple, Banana], [Carrot, Banana])</code> could be said to have the same ""average"" distance as <code>([Apple, Banana, Widget, Xylophone], [Carrot, Banana, Yam, Xylophone])</code> since every 2nd item matches in both, but the latter pair's raw Levenshtein distance will be twice as great.</p>

<p>Also bear in mind that Levenshtein does not make special allowances for <strong>""block moves""</strong>: if you take a string, and move one of its substrings sufficiently far away, then the resulting pair (of original and modified strings) will have the same Levenshtein score as if the 2nd string had completely different elements at the position where the substring was moved to.  If you want to take this into account, consider using a <a href=""https://stackoverflow.com/questions/451884/similar-string-algorithm/452956#452956"">compression-based distance</a> instead.  (Although I say there that it's useful for computing distances without respect to order, it does of course favour ordered similarity to disordered similarity.)</p>
",1972,1291245149
Data mining termin &quot;fledged&quot;?,"<p>Please tell what is termin ""full fledged KI""? As i understand it is part of data mining for text analyzing. Am i right? Some interesting and useful links will be fine!</p>

<p>Thank you!!!</p>
","text, nlp, data-mining, text-parsing, turing-machines","<p>By ""full fledged"", he likely means ""fully fledged"", defined as</p>

<ol>
<li>developed or matured to the fullest degree</li>
<li>of full rank or status</li>
</ol>

<p>source: thefreedictionary.com</p>

<p>Not sure about KI, but possibly it means:</p>

<p><a href=""http://en.wikipedia.org/wiki/Knowledge_integration"" rel=""nofollow"">http://en.wikipedia.org/wiki/Knowledge_integration</a></p>
",135,1291075811
natural language question creation,"<p>I am trying to build question based on information available on about 10 variables- e.g. shape (square, circle, rectangle, paralellogram),length, width, circumference, area, diagonal length etc</p>

<p>e.g. if i want to set question to calculate area based on shape, length and width- the question gets created stating- calculate area of 'rectangle' given length='10' and width='5'. If i provide area and ask for width, the question autmatically forms as calculate area of 'rectangle' given length='10' and area='50'.</p>

<p>I am not too ambitious and am willing to be able to build this under constraints- any pointers around how I can achieve this? initial thoughts to have a question and answer fragment for each variable- but initial attempts creates very messy grammar</p>
",nlp,,176,1283666631
getting nouns and verbs from wordnet,"<p>I'm struggling to find whether a word is noun or verb etc</p>

<p>I found the MIT Java Wordnet Interface 
there was a sample code like this, but when i use this i get error that Dictionary is abstract class and cannot be instantiated</p>

<pre><code>public void testDictionary() throws IOException {


// construct the URL to the Wordnet dictionary directory

String wnhome = System.getenv(""WNHOME"");

String path = wnhome + File.separator + ""dict"";

URL url = new URL(""file"", null, path);

    // construct the dictionary object and open it

IDictionary dict = new Dictionary(url);

dict.open();


// look up first sense of the word ""dog""

IIndexWord idxWord = dict.getIndexWord(""dog"", POS.NOUN);

IWordID wordID = idxWord.getWordIDs().get(0);

IWord word = dict.getWord(wordID);

System.out.println(""Id = "" + wordID);

System.out.println(""Lemma = "" + word.getLemma());

System.out.println(""Gloss = "" + word.getSynset().getGloss());

 }
</code></pre>

<p>i also got another java interface to wordnet</p>

<p>danbikel's interface</p>

<p>but i dont get answer for the query</p>

<pre><code>WordNet wn=new WordNet(""/usr/share/wordnet"");
    Morphy m = new Morphy(wn);

    System.out.println(m.morphStr(""search"",""NOUN"").length);
</code></pre>

<p>Always the string length is 0, what is the correct arguments for this method? here is the javadoc of the method, what am i doing wrong?</p>

<pre><code>public String[] morphStr(String origstr, String pos)
Tries several techniques on origstr to find possible base forms (lemmas).

Specified by:
morphStr in interface MorphyRemote
Parameters:
origstr - word or collocation, separated either by whitespace, '_' or '-', to find lemma of
pos - part of speech of origstr
Returns:
array of possible lemmas for origstr, possibly of length 0 if no lemmas could be found
</code></pre>
","java, nlp, wordnet",,4450,1286215023
What are the &quot;-P&quot;s in the Berkeley Aligner&#39;s output format?,"<p>I want to use the Berkeley Aligner for some MT research I'm doing, since, apparently, it beats GIZA++ pretty handily (a 32% alignment error reduction in some reported results).  For the most part the outputs in the Berkeley Aligner ""examples"" directory look like what Moses does to GIZA++ output files (i.e., paired aligned word indices), but there are some funny looking ""-P""s after certain pairs.  I can't for the life of me find any documentation  of what these ""-P"" annotations are supposed to signify (certainly not in the Berkeley Aligner ""documentation"" directory).</p>

<p>For clarity, I'll give a little illustrative example.  Suppose you have the sentences: ""Jean plâit à Marie"" and ""Marie likes Jean"".  French is the source language and English is the target language. The words ""Jean"" (indices 0 and 2, resp.) and ""Marie"" (indices 3 and 0, resp.) are aligned in both sentences, and ""plâit"" and ""à "" (French indices 1 and 2, resp.) are aligned with ""like"" (English index 1).  In Moses-post-processed GIZA++ output, this would be denoted by a list of source-target index pairs:</p>

<pre><code>0-2 1-1 2-1 3-0
</code></pre>

<p>Berkeley Aligner produces files that pretty much resemble this, but some index pairs have a -P on them (e.g., you <em>might</em> see something like 1-1-P).</p>

<p>What the heck does this mean? Can I safely remove these -P annotations and get a GIZA++-via-Moses style alignment, or should I be doing something more (e.g., multiplying them out into a series of aligned index pairs, or what have you)?</p>
","nlp, machine-translation",,780,1290104265
How to compare different language String values in JAVA?,"<p>In my web application I am using <strong>two</strong> different Languages namely <strong>English</strong> and <strong>Arabic</strong>.</p>

<p>I have a <strong>search box</strong> in my web application in which if we search by name or part of the name then it will retrieve the values from DB by comparing the <strong>""Hometown""</strong> of the user</p>

<p><strong>Explanation:</strong></p>

<p>Like if a user belongs to <strong>hometown ""California""</strong> and he searches a <strong>name</strong> say <strong>""Victor""</strong> then my query will first see the people who are having the <strong>same hometown ""California""</strong> and in the list of people who have <strong>""California""</strong> as hometown the <strong>""Victor""</strong> *<em>name</em>* will be searched and it <strong>retrieve</strong> the users having <strong>""California""</strong> as their <strong>hometown</strong> and ""<strong>victor</strong>"" in their name or part of the name.</p>

<p>The <strong>problem</strong> is if the hometown <strong>""California""</strong> is <strong>saved</strong> in <strong>English</strong> it will compare and retrieve the values. But <strong>""California""</strong> will be <strong>saved as ""كاليفورنيا"" in Arabic</strong>. In this case the <strong>hometown comparison fails</strong> and it cant retrieve the values. </p>

<p>I wish that my query should find both are same hometown and retrieve the values. Is it possible?</p>

<p>What <strong>alternate</strong> I should think of for this logic for comparison. I am confused. Any suggestion please? </p>

<p><strong>EDIT:</strong>
*<em>I have an Idea such that if the <strong>hometown</strong> is got then is it possible to use Google translator or transliterator and change the hometown to another language. if it is in english then to arabic or if it is in english then to arabic and give the search results joining both. Any suggestion?</em>*</p>
","nlp, cross-language","<p>Transliterate all names into the same language (e.g. English) for searching, and use Levenstein edit distance to compute the similarity between the phonetic representations of the names. This will be slow if you simply compare your query with every name, but if you pre-index all of the place names in your database into a <a href=""http://en.wikipedia.org/wiki/BK-tree"" rel=""nofollow"">Burkhard-Keller tree</a>, then they can be efficiently searched by edit distance from the query term.</p>

<p>This technique allows you to sort names by how close they actually match. You're probably more likely to find a match this way than using metaphone or double-metaphone, though this is more difficult to implement.</p>
",3542,1290795775
how to handle misspelled words in documents for text mining tasks?,"<p>I have a set of informal documents (couple of thousands) which I want to apply topic modeling (MALLET) on. The problem is, there are a considerable number of misspelled words in the documents. Most are intentional, such as short-forms and local lingo like `'juz' -> 'just', 'alr' -> 'already'. A couple of these variations exists, due to the different authors' peculiar styles of writing.</p>

<p>After feeding them to MALLET, I kinda bothered that one of the topics generated is actually a set of misspelled stopwords. I believe these words are mostly used in the small subset of documents from the same author, hence MALLET picked it up.</p>

<p>My question is, do I spell-check and correct these sets of misspelled words, and perhaps save the corrected text somewhere, before conducting further tasks on them? I suppose this would meant that I do need to manually verify the corrections before committing right?  What would be the most ""efficient"" way to do this?</p>

<p>Or do I actually ignore these misspelled words? </p>
","nlp, spell-checking, misspelling","<p>What do you do with stopwords at the moment? If you are doing topic modelling then it would make sense to filter them out. If so, why don't you filter out these terms too?</p>

<p><strong>[Edit in response to reply]</strong></p>

<p>There is some research about handling stopwords within LDA in a more principled way. There are two papers that spring to mind:</p>

<ol>
<li><a href=""http://www.aclweb.org/anthology/N/N10/N10-1070.pdf"" rel=""nofollow"">Term Weighting Schemes for Latent Dirichlet Allocation</a></li>
<li><a href=""http://www.cs.umass.edu/~wallach/publications/wallach09rethinking.pdf"" rel=""nofollow"">Rethinking LDA: Why Priors Matter.
</a></li>
</ol>

<p>[<a href=""http://www.aclweb.org/anthology/N/N10/N10-1070.pdf"" rel=""nofollow"">1</a>] uses a term weighting scheme which apparently helps in a predictive task they set up, [<a href=""http://www.cs.umass.edu/~wallach/publications/wallach09rethinking.pdf"" rel=""nofollow"">2</a>] uses a non-symmetric prior over the word distributions which apparently leads to a few topics which contain all the stop words, and other words common to the entire corpus. </p>

<p>It seems to me that the best way to automatically infer stop words and other non-topic words in LDA is still a research question.</p>
",3714,1290684503
NLP Library for Java Programming Language,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/870460/java-is-there-a-good-natural-language-processing-library"">Java : Is there a good natural language processing library</a>  </p>
</blockquote>



<p>are there any known open source distributions that one can use like nltk for python?</p>
","java, nlp",,8319,1290693929
How can I convert between numbers and the words that represent them?,"<p>Sorry if this is a silly question, but it is useful for my current lexical analysis project. Is there any built-in and preferably culture-based (language-based) function in the .Net framework to convert back and forth from string with numbers and the words that represent them? Like ""1"" &lt;-> ""one"", ""2"" &lt;-> ""two"", ""3"" &lt;-> ""three"" and like this?</p>

<p>I know this can be made using a dictionary or a list with all the valid correspondences, but that is too much work if there is already a built-in function that one can use.</p>

<p><strong>Edit:</strong> People who voted to close may have not understood that I was not asking how to convert decimal representation to strings, but the reverse operation: from a string to it's decimal representation. The links provided do not address this problem.</p>

<p>Thank you very much.</p>
",".net, text, numbers, nlp",,559,1290507330
How to handle Out of memory with Python,"<p>I have huge dictionaries that I manipulate. More than 10 Million words are hashed. Its is too slow and some time it goes out of memory. </p>

<p>Is there a better way to handle these huge data structure ? </p>
","python, nlp","<p>Yes. It's called a database. Since a dictionary was working for you (aside from memory concerns) I would suppose that an sqlite database would work fine for you. You can use the <a href=""http://docs.python.org/library/sqlite3.html"" rel=""nofollow"">sqlite3</a> quite easily and it is very well documented.</p>

<p>Of course this will only be a good solution if you can represent the values as something like json or are willing to trust pickled data from a local file. Maybe you should post details about what you have in the values of the dictionary. (I'm assuming the keys are words, if not please correct me)</p>

<p>You might also want to look at not generating the whole dictionary and only processing it in chunks. This may not be practical in your particular use case (It often isn't with the sort of thing that dictionaries are used for unfortunately) but if you can think of a way, it may be worth it to redesign your algorithm to allow it.</p>
",1671,1290154040
algorithm to calculate similarity between texts,"<p>I am trying to score similarity between posts from social networks, but didn't find any good algorithms for that, thoughts?</p>

<p>I just tried Levenshtein, JaroWinkler, and others, but those one are more used to compare texts without sentiments. In posts we can get one text saying ""I really love dogs"" and an other saying ""I really hate dogs"", we need to classify this case as totally different.</p>

<p>Thanks</p>
","java, text, artificial-intelligence, nlp, mining","<p>You might want to have a look at <a href=""http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html"" rel=""nofollow noreferrer"">Opinion mining and sentiment analysis</a> to give you an idea of the complexity of the task.</p>

<p>Short answer: there a no ""good algorithms"" for this, only mediocre ones. And this is a very hard problem. Good luck.</p>
",2716,1282891782
Named entity recognition with preset list of names for Python / PHP,"<p>I'm trying to process a CSV file that has as in each row a text field with the name of organization and position of an individual within that organization as unstructured text.  This field is usually a mess of text like this:</p>

<pre><code>Assoc. Research Professor  Dept. Psychology  Univ. California  Santa Barbara
</code></pre>

<p>I need to pull out the position and the organization name.  For the position, I use preg_match for a series of about 60 different regular expressions for the different professions, and I think it works pretty well (my guess is that it catches about 80%).  But, I'm having trouble catching the organization name. I have a MySQL table with roughly 16,000 organization names that I can perform a simple preg_match for, but due to common misspellings and abbreviations, it's only catching about 30% of the organizations.  For example, my database has</p>

<pre><code>University of California Santa Barbara
</code></pre>

<p>But the CSV file might have any of the options:</p>

<pre><code>Univ Cal Santa Barbara
University Cal-Santa Barbara
University California-Santa Barbara
Cal University, Santa Barbara
</code></pre>

<p>I need to process several hundred thousand records, and I can't spend the time to correct 70% of the records that are currently not being processed correctly or painstakingly create multiple aliases for each organization. What I would like to be able to do is to catch small differences (such as the small misspellings, hyphens versus spaces, and common abbreviations), and, if still no matches are found, to ideally recognize an organizational name and create a new record for it.  </p>

<ul>
<li>What libraries or tools in Python or PHP would allow to perform a similarity match that would have a broader reach? </li>
<li>Would NLTK in Python catch misspellings?</li>
<li>Is it possible to use AlchemyAPI to catch misspelled organizations?  So far I've only been able to use it to catch correctly spelled organizations</li>
<li>Since I'm comparing a short string (the organization name) to a longer string (that includes the name plus extraneous information) is there any hope in using PHP's similar_text function?</li>
</ul>

<p>Any help or insight would be appreciated.</p>
","php, python, text, nlp, named-entity-recognition",,1447,1290012472
Storing tokenized text in the db?,"<p>I have a simple question. I'm doing some light crawling so new content arrives every few days. I've written a tokenizer and would like to use it for some text mining purposes. Specifically, I'm using Mallet's topic modeling tool and one of the pipe is to tokenize the text into tokens before further processing can be done. With the amount of text in my database, it takes a substantial amount of time tokenizing the text (I'm using regex here). </p>

<p>As such, is it a norm to store the tokenized text in the db so that tokenized data can be readily available and tokenizing can be skipped if I need them for other text mining purposes such as Topic modeling, POS tagging? What are the cons of this approach? </p>
","python, caching, postgresql, nlp, tokenize","<p><strong>Caching Intermediate Representations</strong></p>

<p>It's pretty normal to cache the intermediate representations created by slower components in your document processing pipeline. For example, if you needed <a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow"">dependency parse trees</a> for all the sentences in each document, it would be pretty crazy to do anything <strong>except</strong> parsing the documents once and then reusing the results.</p>

<p><strong>Slow Tokenization</strong></p>

<p>However, I'm surprise that tokenization is really slow for you, since the stuff downstream from tokenization is usually the real bottleneck.</p>

<p>What package are you using to do the tokenization? If you're using Python and you wrote your own tokenization code, you might want to try one of the tokenizers included in <strong><a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a></strong> (e.g., <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.tokenize.treebank.TreebankWordTokenizer-class.html"" rel=""nofollow"">TreebankWordTokenizer</a>). </p>

<p>Another good tokenizer, albeit one that is not written in Python, is the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html"" rel=""nofollow"">PTBTokenizer</a> included with the <strong><a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">Stanford Parser</a></strong> and the <strong><a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford CoreNLP</a></strong> end-to-end NLP pipeline.</p>
",1413,1289211456
Evaluate the content of a paragraph,"<p>We are building a database of scientific papers and performing analysis on the abstracts. The goal is to be able to say ""Interest in this topic has gone up 20% from last year"". I've already tried key word analysis and haven't really liked the results. So now I am trying to move onto phrases and proximity of words to each other and realize I'm am in over my head. Can anyone point me to a better solution to this, or at very least give me a good term to google to learn more? </p>

<p>The language used is python but I don't think that really affects your answer. Thanks in advance for the help.</p>
","string, data-mining, nlp","<p>It is a big subject, but a good introduction to NLP like this can be found with the NLTK toolkit. This is intended for teaching and works with Python - ie. good for dabbling and experimenting. Also there's a very good open source book (also in paper form from O'Reilly) on the NLTK website.</p>
",106,1289259002
MALLET tokenizer,"<p>Hi I want to use MALLET's topic modeling but can i provide my own tokenizer or tokenized version of the text documents when i import the data into mallet? I find MALLET's tokenizer inadequate for my usage...</p>
","nlp, tokenize","<p>Ok, I got it. Simply replace the default tokenizer with my own into the serial pipe and add it into the instance list.</p>
",698,1284715558
List of uninteresting words,"<p>[Caveat] This is not directly a programing question, but it is something that comes up so often in language processing that I'm sure it's of some use to the community.</p>

<p>Does anyone have a good list of <em>uninteresting</em> (English) words that have been tested by more then a casual look? This would include all prepositions, conjunctions, etc... words that may have semantic meaning, but are often frequent in <em>every</em> sentence, regardless of the subject. I've built my own lists from time to time for personal projects but they've been ad-hoc; I continuously add words that I forgotten as they come in.</p>
","nlp, text-processing, stop-words","<p>These words are usually called <a href=""http://en.wikipedia.org/wiki/Stop_words"" rel=""noreferrer"">stop words</a>. The Wikipedia article contains much more information about them, including where to find some lists.</p>
",1389,1272145753
Word-separating algorithm,"<p>What is the algorithm - seemingly in use on domain parking pages - that takes a spaceless bunch of words (eg ""thecarrotofcuriosity"") and more-or-less correctly breaks it down into the constituent words (eg ""the carrot of curiosity"") ?</p>
","algorithm, cpu-word, domain-name, nlp",,1553,1249427414
Find the words in a long stream of characters. Auto-tokenize,"<p>How would you find the correct words in a long stream of characters?</p>

<p>Input :</p>

<pre><code>""The revised report onthesyntactictheoriesofsequentialcontrolandstate""
</code></pre>

<p>Google's Output: </p>

<pre><code>""The revised report on syntactic theories sequential controlandstate""
</code></pre>

<p>(which is close enough considering the time that they produced the output)</p>

<p>How do you think Google does it? 
How would you increase the accuracy? </p>
","algorithm, computer-science, nlp, string-algorithm","<p>I would try a recursive algorithm like this:</p>

<ul>
<li>Try inserting a space at each position. If the left part is a word, then recur on the right part. </li>
<li>Count the number of valid words / number of total words in all the final outputs. The one with the best ratio is likely your answer.</li>
</ul>

<p>For example, giving it ""thesentenceisgood"" would run:</p>

<pre><code>thesentenceisgood
the sentenceisgood
    sent enceisgood
         enceisgood: OUT1: the sent enceisgood, 2/3
    sentence isgood
             is good
                go od: OUT2: the sentence is go od, 4/5
             is good: OUT3: the sentence is good, 4/4
    sentenceisgood: OUT4: the sentenceisgood, 1/2
these ntenceisgood
      ntenceisgood: OUT5: these ntenceisgood, 1/2
</code></pre>

<p>So you would pick OUT3 as the answer.</p>
",2474,1286730505
What is the default chunker for NLTK toolkit in Python?,"<p>I am using their default POS tagging and default tokenization..and it seems sufficient.  I'd like their default chunker too.</p>

<p>I am reading the NLTK toolkit book, but it does not seem like they have a default chunker?</p>
","python, nlp, nltk, chunking","<p>You can get out of the box named entity chunking with the nltk.ne_chunk() method.  It takes a list of POS tagged tuples:</p>

<p><code>nltk.ne_chunk([('Barack', 'NNP'), ('Obama', 'NNP'), ('lives', 'NNS'), ('in', 'IN'), ('Washington', 'NNP')])</code></p>

<p>results in:</p>

<p><code>Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]), ('lives', 'NNS'), ('in', 'IN'), Tree('GPE', [('Washington', 'NNP')])])</code></p>

<p>It identifies Barack as a person, but Obama as an organization.  So, not perfect.</p>
",4678,1257513020
perform classifier training on twitter data,"<p>I have just started learning about NLP and I have to do a course project this semester. what I want to do is :<br>
a.) Fetch the tweets from twitter for a particular topic ( How can I do that?? )  </p>

<p>b.) Have already trained data against which I can decide if the tweets I fetched has positive or negative sentiment ( how to train this data )   </p>

<p>c.) show the results to user  </p>

<p>I want to use Java for this, and I found that we can use <a href=""http://alias-i.com/lingpipe/index.html"" rel=""nofollow"">LingPipe</a> for this</p>

<p>I am all new to this and building the app for the first time, so guidance is very much appreciated</p>
","java, twitter, nlp","<p><a href=""http://dev.twitter.com/pages/libraries#java"" rel=""nofollow noreferrer"">Twitter APIs for Java</a></p>

<p>A survey text on <a href=""http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html"" rel=""nofollow noreferrer"">opinion mining and sentiment analysis</a>, which is what you're looking to do. I've actually read portions of this text, and thought it was decent. It isn't a ""here's an algorithm to analyze twitter"" book (perhaps <a href=""http://oreilly.com/catalog/9780596529321"" rel=""nofollow noreferrer"">Programming Collective Intelligence</a>?), but it'll have references to bajillions of research papers in the area, from which you should be able to find algorithms and analysis.</p>

<p>Also, the question ""<a href=""https://stackoverflow.com/questions/573768/sentiment-analysis-for-twitter-in-python"">Sentiment Analysis for Twitter in Python</a>"" is going to be helpful to you, as well.</p>

<p>Your questions are vague, so, sorry, you get somewhat vague answers.</p>
",616,1288888559
Synchronizing text and audio. Is there a NLP/speech-to-text library to do this?,"<p>I would like to synchronize a spoken recording against a known text.  Is there a speech-to-text / natural language processing library that would facilitate this?  I imagine I'd want to detect word boundaries and compute candidate matches from a dictionary.  Most of the questions I've found on SO concern written language.</p>

<p>Desired, but not required:</p>

<ul>
<li>Open Source</li>
<li>Compatible with American English out-of-the-box</li>
<li>Cross-platform</li>
<li>Thoroughly documented</li>
</ul>

<p>Edit: I realize this is a very broad, even naive, question, so thanks in advance for your guidance.</p>

<p>What I've found so far:</p>

<ul>
<li><a href=""http://www.politepix.com/openears/"" rel=""noreferrer"">OpenEars</a> (iOS Sphinx/Flite wrapper)</li>
</ul>
","nlp, speech-recognition, pattern-recognition","<p><strong>Forced Alignment</strong></p>

<p>It sounds like you want to do <strong><a href=""http://www.isip.piconepress.com/projects/speech/software/tutorials/production/fundamentals/v1.0/section_04/s04_04_p01.html"" rel=""noreferrer"">forced alignment</a></strong> between your audio and the known text. </p>

<p>Pretty much all research/industry grade speech recognition systems will be able to do this, since forced alignment is an important part of training a recognition system on data that doesn't have <a href=""http://en.wikipedia.org/wiki/Phoneme"" rel=""noreferrer"">phone</a> level alignments between the audio and the transcript.</p>

<p><strong>Alignment CMUSphinx</strong></p>

<p>The <a href=""http://cmusphinx.sourceforge.net/2010/09/sphinx4-1-0-beta-4-released-2/"" rel=""noreferrer"">Sphinx4-1.0 beta 5 release</a> of CMU's open source speech recognition system now includes a demo on how to do alignment between a transcript and long speech recordings.</p>
",5792,1288637181
SQL word root matching,"<p>I'm wondering whether major SQL engines out there (MS SQL, Oracle, MySQL) have the ability to understand that 2 words are related because they share the same root.</p>

<p>We know it's easy to match ""networking"" when searching for ""network"" because the latter is a substring of the former.</p>

<p>But do SQL engines have functions that can match ""network"" when searching for ""networking""?</p>

<p>Thanks a lot.</p>
","sql, nlp, stemming, lemmatization","<p>This functionality is called a <a href=""http://en.wikipedia.org/wiki/Stemming"" rel=""nofollow""><strong>stemmer</strong></a>: an algorithm that can deduce a stem from any form of the word.</p>

<p>This can be quite complex: for instance, Russian words <code>шёл</code> and <code>иду</code> are different forms of the same verb, though they have not a single common letter (ironically, this is also true for English: <code>went</code> and <code>go</code>).</p>

<p>Word breaking can also be quite a complex task for some languages that use no spaces between words.</p>

<p><code>SQL Server</code> allows using pluggable stemmers and word breakers for its fulltext search engine:</p>

<p><a href=""http://msdn.microsoft.com/en-us/library/ms142509.aspx"" rel=""nofollow"">http://msdn.microsoft.com/en-us/library/ms142509.aspx</a></p>
",2250,1288353304
Computational Linguistics project idea using Hadoop MapReduce,"<p>I need to do a project on Computational Linguistics course. Is there any interesting ""linguistic"" problem which is data intensive enough to work on using Hadoop map reduce. Solution or algorithm should try and analyse and provide some insight in ""lingustic"" domain. however it should be applicable to large datasets so that i can use hadoop for it. I know there is a python natural language processing toolkit for hadoop.</p>
","hadoop, mapreduce, nlp",,2100,1267410667
Medical information extraction using Python,"<p>I am a nurse and I know python but I am not an expert, just used it to process DNA sequences<br>
We got hospital records written in human languages and I am supposed to insert these data into a database or csv file but they are more than 5000 lines and this can be so hard. All the data are written in a consistent format let me show you an example</p>

<pre><code>11/11/2010 - 09:00am : He got nausea, vomiting and died 4 hours later
</code></pre>

<p>I should get the following data</p>

<pre><code>Sex: Male
Symptoms: Nausea
    Vomiting
Death: True
Death Time: 11/11/2010 - 01:00pm
</code></pre>

<p>Another example</p>

<pre><code>11/11/2010 - 09:00am : She got heart burn, vomiting of blood and died 1 hours later in the operation room
</code></pre>

<p>And I get</p>

<pre><code>Sex: Female
Symptoms: Heart burn
    Vomiting of blood
Death: True
Death Time: 11/11/2010 - 10:00am
</code></pre>

<p>the order is not consistent by when I say in ....... so in is a keyword and all the text after is a place until i find another keyword<br>
At the beginnning He or She determine sex, got ........ whatever follows is a group of symptoms that i should split according to the separator which can be a comma, hypen or whatever but it's consistent for the same line<br>
died ..... hours later also should get how many hours, sometimes the patient is stil alive and discharged ....etc<br>
That's to say we have a lot of conventions and I think if i can tokenize the text with keywords and patterns i can get the job done. So please if you know a useful function/modules/tutorial/tool for doing that preferably in python (if not python so a gui tool would be nice)  </p>

<p>Some few information:</p>

<pre><code>there are a lot of rules to express various medical data but here are few examples
- Start with the same date/time format followed by a space followd by a colon followed by a space followed by He/She followed space followed by rules separated by and
- Rules:
    * got &lt;symptoms&gt;,&lt;symptoms&gt;,....
    * investigations were done &lt;investigation&gt;,&lt;investigation&gt;,&lt;investigation&gt;,......
    * received &lt;drug or procedure&gt;,&lt;drug or procedure&gt;,.....
    * discharged &lt;digit&gt; (hour|hours) later
    * kept under observation
    * died &lt;digit&gt; (hour|hours) later
    * died &lt;digit&gt; (hour|hours) later in &lt;place&gt;
other rules do exist but they follow the same idea
</code></pre>
","python, parsing, machine-learning, nlp, information-extraction","<p>This uses <a href=""http://labix.org/python-dateutil"" rel=""noreferrer"">dateutil</a> to parse the date (e.g. '11/11/2010 - 09:00am'), and <a href=""http://code.google.com/p/parsedatetime/"" rel=""noreferrer"">parsedatetime</a> to parse the relative time (e.g. '4 hours later'):</p>

<pre><code>import dateutil.parser as dparser
import parsedatetime.parsedatetime as pdt
import parsedatetime.parsedatetime_consts as pdc
import time
import datetime
import re
import pprint
pdt_parser = pdt.Calendar(pdc.Constants())   
record_time_pat=re.compile(r'^(.+)\s+:')
sex_pat=re.compile(r'\b(he|she)\b',re.IGNORECASE)
death_time_pat=re.compile(r'died\s+(.+hours later).*$',re.IGNORECASE)
symptom_pat=re.compile(r'[,-]')

def parse_record(astr):    
    match=record_time_pat.match(astr)
    if match:
        record_time=dparser.parse(match.group(1))
        astr,_=record_time_pat.subn('',astr,1)
    else: sys.exit('Can not find record time')
    match=sex_pat.search(astr)    
    if match:
        sex=match.group(1)
        sex='Female' if sex.lower().startswith('s') else 'Male'
        astr,_=sex_pat.subn('',astr,1)
    else: sys.exit('Can not find sex')
    match=death_time_pat.search(astr)
    if match:
        death_time,date_type=pdt_parser.parse(match.group(1),record_time)
        if date_type==2:
            death_time=datetime.datetime.fromtimestamp(
                time.mktime(death_time))
        astr,_=death_time_pat.subn('',astr,1)
        is_dead=True
    else:
        death_time=None
        is_dead=False
    astr=astr.replace('and','')    
    symptoms=[s.strip() for s in symptom_pat.split(astr)]
    return {'Record Time': record_time,
            'Sex': sex,
            'Death Time':death_time,
            'Symptoms': symptoms,
            'Death':is_dead}


if __name__=='__main__':
    tests=[('11/11/2010 - 09:00am : He got nausea, vomiting and died 4 hours later',
            {'Sex':'Male',
             'Symptoms':['got nausea', 'vomiting'],
             'Death':True,
             'Death Time':datetime.datetime(2010, 11, 11, 13, 0),
             'Record Time':datetime.datetime(2010, 11, 11, 9, 0)}),
           ('11/11/2010 - 09:00am : She got heart burn, vomiting of blood and died 1 hours later in the operation room',
           {'Sex':'Female',
             'Symptoms':['got heart burn', 'vomiting of blood'],
             'Death':True,
             'Death Time':datetime.datetime(2010, 11, 11, 10, 0),
             'Record Time':datetime.datetime(2010, 11, 11, 9, 0)})
           ]

    for record,answer in tests:
        result=parse_record(record)
        pprint.pprint(result)
        assert result==answer
        print
</code></pre>

<p>yields:</p>

<pre><code>{'Death': True,
 'Death Time': datetime.datetime(2010, 11, 11, 13, 0),
 'Record Time': datetime.datetime(2010, 11, 11, 9, 0),
 'Sex': 'Male',
 'Symptoms': ['got nausea', 'vomiting']}

{'Death': True,
 'Death Time': datetime.datetime(2010, 11, 11, 10, 0),
 'Record Time': datetime.datetime(2010, 11, 11, 9, 0),
 'Sex': 'Female',
 'Symptoms': ['got heart burn', 'vomiting of blood']}
</code></pre>

<p>Note: Be careful parsing dates. Does '8/9/2010' mean August 9th, or September 8th? Do all the record keepers use the same convention? If you choose to use dateutil (and I really think that's the best option if the date string is not rigidly structured) be sure to read the section on ""Format precedence"" in the <a href=""http://labix.org/python-dateutil"" rel=""noreferrer"">dateutil documentation</a> so you can (hopefully) resolve '8/9/2010' properly. 
If you can't guarantee that all the record keepers use the same convention for specifying dates, then the results of this script would have be checked manually. That might be wise in any case.</p>
",7836,1287973862
implementing a dictionary,"<p>Hii ,</p>

<p>I ran across a interview question of implementing a dictionary that can implement the features of auto-completion , auto - correction , spell check etc... </p>

<p>I actually wanted to know which data structure is the best for implementing a dictionary and how one approaches the above required features...</p>

<p>Any links that guide me on this are welcome...</p>
","data-structures, autocomplete, nlp, spell-checking","<p>There is just the same answer for this kind of problem: a <strong>Trie</strong>. Take a look <a href=""http://en.wikipedia.org/wiki/Trie"" rel=""nofollow"">here</a>..</p>

<p>Also suffix trees (or <strong>Patricia Trees</strong>) can be useful for this purposes..</p>
",1669,1287937730
Open source libraries for generating automated summaries,"<p>I was looking for a open source library for generating automated summaries out of few words. For ex: if two qualities are given of a person a) good thinking skills b) bad handwriting, i need to generate a sentence like ""Bob has good thinking skills however needs to improve on his handwriting"". I need to know if any open source library could help me achieve it even partially.</p>

<p>Thanks for help!</p>

<p>-- Mohit </p>
","open-source, nlp",,173,1283430770
Online job-searching is tedious. Help me automate it,"<p>Many job sites have broken searches that don't let you narrow down jobs by experience level. Even when they do, it's usually wrong. This requires you to wade through hundreds of postings that you can't apply for before finding a relevant one, quite tedious. Since I'd rather focus on writing cover letters etc., I want to write a program to look through a large number of postings, and save the URLs of just those jobs that don't require years of experience.</p>

<p>I don't require help writing the scraper to get the html bodies of possibly relevant job posts. The issue is accurately detecting the level of experience required for the job. This should not be too difficult as job posts are usually very explicit about this (""must have 5 years experience in...""), but there may be some issues with overly simple solutions.</p>

<p>In my case, I'm looking for entry-level positions. Often they don't say ""entry-level"", but inclusion of the words probably means the job should be saved.</p>

<p>Next, I can safely exclude a job the says it requires ""5 years"" of experience in whatever, so a regex like /\d\syears/ seems reasonable to exclude jobs. But then, I realized some jobs say they'll take 0-2 years of experience, matches the exclusion regex but is clearly a job I want to take a look at. Hmmm, I can handle that with another regex. But some say ""less than 2 years"" or ""fewer than 2 years"". Can handle that too, but it makes me wonder what other patterns I'm not thinking of, and possibly excluding many jobs. That's what brings me here, to find a better way to do this than regexes, if there is one.</p>

<p>I'd like to minimize the false negative rate and save all the jobs that seem like they might not require many years of experience. Does excluding anything that matches /[3-9]\syears|1\d\syears/ seem reasonable? Or is there a better way? Training a bayesian filter maybe?</p>

<p><strong>Edit:</strong> There's a similar, but harder problem, which would probably be more useful to solve. There are lots of jobs that just require an ""engineering degree"", as you just have to understand a few technical things. But searching for ""engineering"" gives you thousands of jobs, mostly irrelevant.</p>

<p>How do I narrow this down to just those jobs that require any engineering degree, rather than particular degrees, without looking at each myself?</p>
","python, ruby, regex, perl, nlp",,619,1276629265
Word coloring and syntax analyzing,"<p>I want to colorize the words in a text according to their classification (category/declination etc). I have a fully working dictionary, but the problem is that there is a lot of ambiguity. <code>foedere</code>, for instance, can be forms of either the verb ""fornicate"" or the noun ""treaty"".</p>

<p>What the general strategies for solving these ambiguities or generating good guesses are?</p>

<p>Thanks!</p>
","algorithm, artificial-intelligence, nlp","<p>The general strategy is to first run a <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow"">part-of-speech tagger</a> on the data to determine the word category (noun, verb, etc.). That, however, requires data (context statistics) and tools. <a href=""http://gircse.marginalia.it/~passarotti/publications/Passarotti_SaLTMiL.pdf"" rel=""nofollow"">This research paper</a> may be a starting point.</p>
",134,1287324762
How to wrap words or words sequences which have not been already wrapped?,"<p>I'm trying to wrap words and words sequence from a given list with preg_replace. It almost works, but there are some use cases it doesn't and I can't figure it how.</p>

<p>For instance I do this: </p>

<pre><code>    // sort by descending length
    usort($this-&gt;_keywords, function($a,$b){return(strlen($a)&lt;strlen($b));});

    // wrapper is -%string%-
    foreach ($this-&gt;_keywords as $keyword) {
        $value = preg_replace('/((?!-)' . $keyword . '(?!-))/i', str_replace('%string%', '\1', $this-&gt;_wrapper), $value);
    }
</code></pre>

<p>From this keyword list:</p>

<ul>
<li>lorem</li>
<li>ipsum</li>
<li>sit amet</li>
<li>null</li>
<li>sed</li>
<li>sed enim</li>
</ul>

<p>I'd like to result in:</p>

<blockquote>
  <p><strong>-Lorem-</strong>  <strong>-ipsum-</strong> dolor <strong>-sit amet-</strong>,
  consectetur adipiscing elit. Phasellus
  rhoncus venenatis orci sed porta. Sed
  non dolor eros. Suspendisse a massa
  <strong>-sit amet-</strong> nulla egestas facilisis. Cras
  fringilla, leo ac ullamcorper semper,
  urna eros pretium lectus, nec rhoncus
  ligula risus eu velit. Nulla eu
  dapibus magna. Sed vehicula tristique
  lacinia. Maecenas tincidunt metus at
  urna consequat nec congue libero
  iaculis. Nulla facilisi. Phasellus <strong>-sed-</strong>
  sem ut risus mattis accumsan eu <strong>-sed
  enim-</strong>. Pellentesque habitant morbi
  tristique senectus et netus et
  malesuada fames ac turpis egestas.
  Suspendisse id est velit, eu cursus
  quam. Vivamus lacinia euismod pretium.</p>
</blockquote>

<p>Any ideas?</p>
","php, regex, cpu-word, text-analysis","<p>I finally resolved my problems by using the <code>\b</code> metacharacters which correspond to a word boundary.</p>

<pre><code>public function filter($value)
{
    usort($this-&gt;_keywords, function($a,$b){return(strlen($a)&lt;strlen($b));});

    foreach ($this-&gt;_keywords as $keyword) {
        $value = preg_replace(
            '/((?&lt;!-)('.$keyword.'\b)(?!\-))/i',
            str_replace('%string%', '\2', $this-&gt;_wrapper) . '\3',
            $value
        );
    }

    return $value;
}
</code></pre>
",320,1286974914
nltk custom tokenizer and tagger,"<p>Here is my requirement. I want to tokenize and tag a paragraph in such a way that it allows me to achieve following stuffs.</p>

<ul>
<li>Should identify date and time in the paragraph and Tag them as DATE and TIME</li>
<li>Should identify known phrases in the paragraph and Tag them as CUSTOM</li>
<li>And rest content should be tokenized should be tokenized by the default nltk's word_tokenize and pos_tag functions?</li>
</ul>

<p><strong>For example</strong>, following sentense</p>

<pre><code>""They all like to go there on 5th November 2010, but I am not interested.""
</code></pre>

<p>should be tagged and tokenized as follows in case of that custom phrase is <strong>""I am not interested""</strong>.</p>

<pre><code>[('They', 'PRP'), ('all', 'VBP'), ('like', 'IN'), ('to', 'TO'), ('go', 'VB'), 
('there', 'RB'), ('on', 'IN'), ('5th November 2010', 'DATE'), (',', ','), 
('but', 'CC'), ('I am not interested', 'CUSTOM'), ('.', '.')]
</code></pre>

<p><strong>Any suggestions would be useful.</strong></p>
","python, nlp, nltk",,4863,1287033334
"For the iPhone, can you program for different languages?","<p>For the iPhone, is it possible to program applications to translate words from a base language to any of several languages of various users. If so, how? </p>
","iphone, objective-c, nlp, ipod-touch",,125,1273413846
finding noun and verb in stanford parser,"<p>I need to find whether a word is verb or noun or it is both</p>

<p>For example, the word is ""search"" it can be both noun and a verb but stanford parser gives NN tag to  it..</p>

<p>is there any way that stanford parser will give that ""search"" is both noun and verb?</p>

<p>code that i use now</p>

<pre><code>public static String Lemmatize(String word) {
    WordTag w = new WordTag(word);
    w.setTag(POSTagWord(word));
    Morphology m = new Morphology();
    WordLemmaTag wT = m.lemmatize(w);

    return wT.lemma();
}
</code></pre>

<p>or should i use any other software to do it? please suggest me 
thanks in advance</p>
","java, nlp, stanford-nlp","<p>The Stanford Parser guesses the part-of-speech tag of a word based on context statistics. You should really pass in a complete sentence to determine whether, in that sentence, ""search"" is a noun or a verb.</p>

<p>You don't need a full parser just to get part-of-speech tags. The <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""noreferrer"">Stanford POS Tagger</a> is enough; it also includes the <code>Morphology</code> class, but it too takes context into account.</p>

<p>If you want <em>all</em> part-of-speech tags that an English word can take on, without giving context, then <a href=""http://wordnet.princeton.edu/"" rel=""noreferrer"">WordNet</a> is probably a better choice. It has several Java interfaces, including <a href=""http://sourceforge.net/apps/mediawiki/jwordnet/index.php?title=Main_Page"" rel=""noreferrer"">JWNL</a> and <a href=""http://projects.csail.mit.edu/jwi/"" rel=""noreferrer"">JWI</a>.</p>
",10189,1286192673
Person names disambiguation,"<p>I am currently doing a project on person name disambiguation. The idea behind the project, that it will be able to identify the correct person, when there are multiple people with the same name. I have used wikipedia for this. I want to evaluate my project on some standard data. I am looking for some testing data. I am not familiar with popular names in wikipedia. Any idea, where I can find this data? I am not looking for vast amounts of data. I am just looking for some 100-500 examples.</p>

<p>Thank you</p>

<p>Adding more information to the question.</p>

<p>What I am looking for is of people with same names but are actually different. For ex, Michael Jordon is a famous basketball player and there is also a statistician with that name. I am looking for examples like this.</p>

<p><a href=""http://en.wikipedia.org/wiki/Michael_Jordan"" rel=""nofollow"">http://en.wikipedia.org/wiki/Michael_Jordan</a>
http://en.wikipedia.org/wiki/Michael_I._Jordan</p>

<p>Hope, you understand the question now.</p>
","java, testing, nlp, wikipedia","<p>Datasets for testing: </p>

<ul>
<li><a href=""http://dbis.uni-trier.de/Mitarbeiter/reuther_files/private/reuther.shtml#DOWNLOAD"" rel=""nofollow"">http://dbis.uni-trier.de/Mitarbeiter/reuther_files/private/reuther.shtml#DOWNLOAD</a></li>
<li><a href=""http://nlp.uned.es/weps/weps-2/weps2-data"" rel=""nofollow"">http://nlp.uned.es/weps/weps-2/weps2-data</a></li>
<li><a href=""http://dbs.uni-leipzig.de/en/research/projects/object_matching/fever/benchmark_datasets_for_entity_resolution"" rel=""nofollow"">http://dbs.uni-leipzig.de/en/research/projects/object_matching/fever/benchmark_datasets_for_entity_resolution</a></li>
<li><a href=""http://semeval2.fbk.eu/semeval2.php"" rel=""nofollow"">http://semeval2.fbk.eu/semeval2.php</a></li>
</ul>

<p>Good luck! </p>
",705,1286162619
Justadistraction: tokenizing English without whitespaces. Murakami SheepMan,"<p>I wondered how <strong>you</strong> would go about tokenizing strings in English (or other western languages) if whitespaces were removed?</p>

<p>The inspiration for the question is the Sheep Man character in the Murakami novel '<a href=""http://en.wikipedia.org/wiki/Dance_Dance_Dance"" rel=""noreferrer"">Dance Dance Dance</a>'</p>

<p>In the novel, the Sheep Man is translated as saying things like:</p>

<blockquote>
  <p>""likewesaid, we'lldowhatwecan. Trytoreconnectyou, towhatyouwant,"" said the Sheep Man. ""Butwecan'tdoit-alone. Yougottaworktoo.""</p>
</blockquote>

<p>So, some punctuation is kept, but not all. Enough for a human to read, but somewhat arbitrary.</p>

<p>What would be your strategy for building a parser for this? Common combinations of letters, syllable counts, conditional grammars, look-ahead/behind regexps etc.?</p>

<p>Specifically, python-wise, how would you structure a (forgiving) translation flow? Not asking for a completed answer, just more how your thought process would go about breaking the problem down.</p>

<p>I ask this in a frivolous manner, but I think it's a question that might get some interesting (nlp/crypto/frequency/social) answers.
Thanks!</p>
","python, linguistics, nlp",,385,1286142226
"Dictionary-Based Named Entity Recognition with zero edit distance: LingPipe, Lucene or what?","<p>I'm trying to perform a dictionary-based NER on some documents. My dictionary, regardless of the datatype, consists of key-value pairs of strings. I want to search for all the keys in the document, and return the corresponding value for that key whenever a match occurs.</p>

<p>The problem is, my dictionary is fairly large: ~7 million key-values - average length of keys: 8 and average length of values: 20 characters.</p>

<p>I've tried LingPipe with MapDictionary but on my desired environment setup, it runs out of memory after 200,000 rows are inserted. I don't know clearly why LingPipe uses a map and not a hashmap in their algorithm.</p>

<p>So the thing is, I don't have any previous experience with Lucene and I want to know if it makes such thing with such number possible in an easier way.</p>

<p>ps. I've already tried chunking the data into several dictionaries and writing them on disk but it's relatively slow.</p>

<p>Thanks for any help.</p>

<p>Cheers 
Parsa</p>
","java, nlp","<p>I suppose if you wanted to reuse LingPipe's <a href=""http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ExactDictionaryChunker.html"" rel=""nofollow"">ExactDictionaryChunker</a> to do the NER, you could override their MapDictionary to store &amp; retrieve from your choice of <a href=""http://nosql-database.org/"" rel=""nofollow"">key/value database</a> instead of their <a href=""http://alias-i.com/lingpipe/docs/api/com/aliasi/util/ObjectToSet.html"" rel=""nofollow"">ObjectToSet</a> (which does extend HashMap, by the way). </p>

<p>Lucene/solr can be used as a key/value store, but if you don't need the extra searching capabilities, just a pure look-up, other options might be better for what you're doing.</p>
",2109,1285656889
How Can I Parse a Document and Replace the Content to Change Context from 1st or 2nd Person to 3rd Person?,"<p>Basically I need some text like:</p>

<ul>
<li>I have an ice cream cone.</li>
<li>You are in trouble.</li>
<li>You need a bath.</li>
</ul>

<p>And change it from 1st or 2nd person to 3rd person.</p>

<ul>
<li>He has an ice cream cone.</li>
<li>He is in trouble.</li>
<li>He needs a bath.</li>
</ul>

<p>I've started a js app, but it's super simple at the moment.</p>

<p>Before I waste time reinventing the wheel, I figured I'd ask: Is anyone aware of any 3rd party libraries that do this sort of thing? If not, does anyone have any advice or guidance to offer to help me get something going?</p>
","regex, parsing, nlp","<p>I don't know of any libraries that do this out-of-the-box, but I've used <a href=""http://nodebox.net/code/index.php/Linguistics"" rel=""nofollow"">Nodebox Linguistics</a>, which has a verb conjugation module, to implement some of this functionality myself. To expand on Benoit's comment, English verb conjugation is <em>mostly</em> simple, but there are a lot of nuanced exceptions, especially when changing tense.</p>
",83,1285650780
Improving entity naming with custom file/code in NLTK,"<p>We've been working with the NLTK library in a recent project where we're 
mainly interested in the named entities part. </p>

<p>In general we're getting good results using the NEChunkParser class. 
However, we're trying to find a way to provide our own terms to the 
parser, without success. </p>

<p>For example, we have a test document where my name (Shay) appears in 
several places. The library finds me as GPE while I'd like it to find 
me as PERSON... </p>

<p>Is there a way to provide some kind of a custom file/ 
code so the parser will be able to interpret the named entity as I 
want it to? </p>

<p>Thanks!</p>
","nlp, nltk","<p>The easy solution is to compile a list of entities that you know are misclassified, then filter the <code>NEChunkParser</code> output in a postprocessing module and replace these entities' tags with the tags you want them to have.</p>

<p>The proper solution is to retrain the NE tagger. If you look at the <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.chunk.named_entity-pysrc.html"" rel=""nofollow"">source code</a> for NLTK, you'll see that the <code>NEChunkParser</code> is based on a MaxEnt classifier, i.e. a machine learning algorithm. You'll have to compile and annotate a corpus (dataset) that is representative for the kind of data you want to work with, then retrain the NE tagger on this corpus. (This is hard, time-consuming and potentially expensive.)</p>
",637,1285225077
How can I make this Python2.6 function work with Unicode?,"<p>I've got this function, which I modified from material in chapter 1 of the online NLTK book. It's been very useful to me but, despite reading the chapter on Unicode, I feel just as lost as before.</p>

<pre><code>def openbookreturnvocab(book):
    fileopen = open(book)
    rawness = fileopen.read()
    tokens = nltk.wordpunct_tokenize(rawness)
    nltktext = nltk.Text(tokens)
    nltkwords = [w.lower() for w in nltktext]
    nltkvocab = sorted(set(nltkwords))
    return nltkvocab
</code></pre>

<p>When I tried it the other day on Also Sprach Zarathustra, it clobbered words with an umlat over the o's and u's. I'm sure some of you will know why that happened. I'm also sure that it's quite easy to fix. I know that it just has to do with calling a function that re-encodes the tokens into unicode strings. If so, that it seems to me it might not happen inside that function definition at all, but here, where I prepare to write to file:</p>

<pre><code>def jotindex(jotted, filename, readmethod):
    filemydata = open(filename, readmethod)
    jottedf = '\n'.join(jotted)
    filemydata.write(jottedf)
    filemydata.close()
    return 0
</code></pre>

<p>I heard that what I had to do was encode the string into unicode after reading it from the file. I tried amending the function like so:</p>

<pre><code>def openbookreturnvocab(book):
    fileopen = open(book)
    rawness = fileopen.read()
    unirawness = rawness.decode('utf-8')
    tokens = nltk.wordpunct_tokenize(unirawness)
    nltktext = nltk.Text(tokens)
    nltkwords = [w.lower() for w in nltktext]
    nltkvocab = sorted(set(nltkwords))
    return nltkvocab
</code></pre>

<p>But that brought this error, when I used it on Hungarian. When I used it on German, I had no errors.</p>

<pre><code>&gt;&gt;&gt; import bookroutines
&gt;&gt;&gt; elles1 = bookroutines.openbookreturnvocab(""lk1-les1"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""bookroutines.py"", line 9, in openbookreturnvocab
    nltktext = nltk.Text(tokens)
  File ""/usr/lib/pymodules/python2.6/nltk/text.py"", line 285, in __init__
    self.name = "" "".join(map(str, tokens[:8])) + ""...""
UnicodeEncodeError: 'ascii' codec can't encode character u'\xe1' in position 4: ordinal not in range(128)
</code></pre>

<p>I fixed the function that files the data like so:</p>

<pre><code>def jotindex(jotted, filename, readmethod):
    filemydata = open(filename, readmethod)
    jottedf = u'\n'.join(jotted)
    filemydata.write(jottedf)
    filemydata.close()
    return 0
</code></pre>

<p>However, that brought this error, when I tried to file the German:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""bookroutines.py"", line 23, in jotindex
    filemydata.write(jottedf)
UnicodeEncodeError: 'ascii' codec can't encode character u'\xf6' in position 414: ordinal not in range(128)
&gt;&gt;&gt; 
</code></pre>

<p>...which is what you get when you try to write the u'\n'.join'ed data.</p>

<pre><code>&gt;&gt;&gt; jottedf = u'/n'.join(elles1)
&gt;&gt;&gt; filemydata.write(jottedf)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
UnicodeEncodeError: 'ascii' codec can't encode character u'\xf6' in position 504: ordinal not in range(128)
</code></pre>
","python, unicode, nlp, python-2.6, nltk","<p>For each string that you read from your file, you can convert them to unicode by calling <code>rawness.decode('utf-8')</code>, if you have the text in UTF-8.  You will end up with unicode objects.  Also, I don't know what ""jotted"" is, but you may want to make sure it's a unicode object and use <code>u'\n'.join(jotted)</code> instead.</p>

<h3>Update:</h3>

<p>It appears that the NLTK library doesn't like unicode objects.  Fine, then you have to make sure that you are using str instances with UTF-8 encoded text.  Try using this:</p>

<pre><code>tokens = nltk.wordpunct_tokenize(unirawness)
nltktext = nltk.Text([token.encode('utf-8') for token in tokens])
</code></pre>

<p>and this:</p>

<pre><code>jottedf = u'\n'.join(jotted)
filemydata.write(jottedf.encode('utf-8'))
</code></pre>

<p>but if jotted is really a list of UTF-8-encoded str, then you don't need this and this should be enough:</p>

<pre><code>jottedf = '\n'.join(jotted)
filemydata.write(jottedf)
</code></pre>

<p>By the way, it looks as though NLTK isn't very cautious with respect to unicode and encoding (at least, the demos).  Better be careful and check that it has processed your tokens correctly.  Also, and this may have caused the fact that you get errors with Hungarian text and not German text, <strong>check your encodings</strong>.</p>
",1826,1285151739
Parser Generator or Library that Supports Suffix Agreement,"<p>I'm working on a syntactic parser for some language. But this language requires suffix agreement highly. For example in English a verb must agree with pronoun as I,we,you-do or he,she,it,this-does etc. In this language a verb has different forms for each pronoun. I know in literature this is handled by unification method. But I couldn't find any implementation of it in Java. I also researched Stanford parser and ANTLR but I couldn’t find any evidence that they support suffix agreement. </p>

<p>So which tool or lib. would you offer me in this situation?</p>

<p>Thanks in advance.</p>
","java, parsing, nlp, parser-generator","<p>I haven't seen a parser than can do this directly, though we did use a unification parser in a grad school class I had. Unfortunately, the name escapes me and it was really old even then.  I'm 99% sure it wasn't open source.</p>

<p>You could try the <a href=""http://www.sil.org/pckimmo/"" rel=""nofollow noreferrer"">KIMMO</a> parser, though I have never used it, so can't attest to its applicability to your problem.</p>
",91,1285228643
How to guess out the grammars of a list of sentences generated by some way?,"<p>I have a lost of sentences generated from <a href=""http://www.ywing.net/graphicspaper.php"" rel=""nofollow noreferrer"">http://www.ywing.net/graphicspaper.php</a>, a random computer graphics paper title generator, some of example sentences sorted are as following:</p>

<hr>

<ul>
<li>Abstract Ambient Occlusion using Texture Mapping</li>
<li>Abstract Ambient Texture Mapping</li>
<li>Abstract Anisotropic Soft Shadows</li>
<li>Abstract Approximation</li>
<li>Abstract Approximation of Adaptive Soft Shadows using Culling</li>
<li>Abstract Approximation of Ambient Occlusion using Hardware-accelerated Clustering</li>
<li>Abstract Approximation of Distributed Surfaces using Estimation</li>
<li>Abstract Approximation of Geometry for Texture-mapped Ambient Occlusion</li>
<li>Abstract Approximation of Mipmaps for Opacity</li>
<li>Abstract Approximation of Occlusion Fields for Subsurface Scattering</li>
<li>Abstract Approximation of Soft Shadows using Reflective Texturing</li>
<li>Abstract Arbitrary Rendering</li>
<li>Abstract Attenuation and Displacement Mapping of Geometry</li>
<li>Abstract Attenuation of Ambient Occlusion using View-dependent Texture Mapping</li>
<li>Abstract Attenuation of Light Fields for Mipmaps</li>
<li>Abstract Attenuation of Non-linear Ambient Occlusion</li>
<li>Abstract Attenuation of Pre-computed Mipmaps using Re-meshing

<h2> - ...</h2></li>
</ul>

<p>I would like to try reverse engineering the grammar behind and learn how to do it in some sort of ways, like in common lisp way or NLTK way. Any ideas about that?</p>

<p>-- Drake</p>
","python, lisp, nlp","<p>This seems to be an interesting problem. How ever, I was under the impression that it is not easy to guess a generator from it's generated sequence of bits. What you can get is a model that may be or may not be a close approximation of the original generator. The approximation will be closer when a large number of sequences generated is processed. </p>

<p>A simple technique would be to create a parse tree and create a vocabulary in each portion of the tree.</p>

<p>Some thing like this:</p>

<pre><code>  Abstract
  |--------|
           |Ambient , Anisotropic,(Approximation, Attenuation)
                                        |
                                        of
                                        |
                                   xxxx      yyyy
                                     |         |
                                   using       for
</code></pre>

<p>xxxx -> list of vocabularies</p>

<p>yyyy -> list of vocabularies</p>
",280,1284181301
Regular expression for counting sentences in a block of text,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/2158296/php-how-to-split-a-paragraph-into-sentences"">PHP - How to split a paragraph into sentences.</a>  </p>
</blockquote>



<p>I have a block of text that I would like to separate into sentences, what would be the best way of doing this? I thought of looking for '.','!','?' characters, but I realized there were some problems with this, such as when people use acronyms, or end a sentence with something like !?.  What would be the best way to handle this? I figured there would be some regex that could handle this, but I'm open to a non-regex solution if that fits the problem better.</p>
","php, regex, nlp","<p>Regex isn't the best solution for this problem.  You'd be served better by creating a parsing library.  Something where you an easily create logic blocks to distinguish one thing from another.  You'll need to come up with a set of rules breaking up the text into the chunks you'd like to see.</p>

<pre><code>""Are you sure?"" he asked.
</code></pre>

<p>Doesn't that mess things up when using regex?  However, with a parser you could actually see</p>

<pre><code>&lt;start quote&gt;&lt;capitalization&gt;are you sure&lt;question&gt;&lt;end quote&gt;he asked&lt;period&gt;
</code></pre>

<p>that with simple rules could say ""that's one sentence.""</p>
",688,1284045098
Transforming early modern English into 20th century spelling using the NLTK,"<p>I have a list of strings that are all early modern English words ending with 'th.' These include hath, appointeth, demandeth, etc. -- they are all conjugated for the third person singular.</p>

<p>As part of a much larger project (using my computer to convert the Gutenberg etext of Gargantua and Pantagruel into something more like 20th century English, so that I'll be able to read it more easily) I want to remove the last two or three characters from all of those words and replace them with an 's,' then use a slightly modified function on the words that still weren't modernized, both included below.</p>

<p>My main problem is that I just never manage to get my typing right in Python. I find that part of the language really confusing at this point.</p>

<p>Here's the function that removes th's:</p>

<pre><code>from __future__ import division
import nltk, re, pprint

def ethrema(word):
    if word.endswith('th'):
        return word[:-2] + 's'
</code></pre>

<p>Here's the function that removes extraneous e's:</p>

<pre><code>def ethremb(word):
    if word.endswith('es'):
        return word[:-2] + 's'
</code></pre>

<p>hence the words 'abateth' and 'accuseth' would pass through ethrema but not through ethremb(ethrema), while the word 'abhorreth' would need to pass through both.</p>

<p>If anyone can think of a more efficient way to do this, I'm all ears.</p>

<p>Here's the result of my very amateurish attempt to use these functions on a tokenized list  of words that need modernizing:</p>

<pre><code>&gt;&gt;&gt; eth1 = [w.ethrema() for w in text]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'str' object has no attribute 'ethrema'
</code></pre>

<p>So, yeah, it's really an issue of typing. These are the first functions I've ever written in Python, and I have no idea how to apply them to actual objects.</p>
","python, text, nlp, nltk","<p><code>ethrema()</code> is not a method of the type <code>str</code>, you have to use the following :</p>

<pre><code>eth1 = [ethrema(w) for w in text]
#AND
eth2 = [ethremb(w) for w in text]
</code></pre>

<p>EDIT (to answer comment) : </p>

<p><code>ethremb(ethrema(word))</code> wouldn't work until you made some little changes to your functions : </p>

<pre><code>def ethrema(word):
    if word.endswith('th'):
        return word[:-2] + 's'
    else
        return word

def ethremb(word):
    if word.endswith('es'):
        return word[:-2] + 's'
    else
        return word

#OR

def ethrema(word):
    if word.endswith('th'):
        return word[:-2] + 's'
    elif word.endswith('es'):
        return word[:-2] + 's'
    else
        return word
</code></pre>
",684,1283015771
"Text mining: when to use parser, tagger, NER tool?","<p>I'm doing a project on mining blog contents and I need help differentiating on which tool to uses. When do I use a parser, when do I use a tagger, and when do I need to use a NER tool?</p>

<p>For instance, I want to find out the most talked about topics/subjects between several blogs; do I use a part-of-speech tagger to grab the nouns and do a frequency count? That would probably be insufficient because very generic terms can pop up right? Or do I have a list of categories and these synonyms that I can match on?</p>

<p>BTW, I'm using nltk, but am looking at stanford tagger or parser since a couple of dudes said that it was good. </p>
","python, nlp, nltk","<p>Instead of trying to reinvent the wheel, you might want to read up on Topic Models, which basically creates clusters of words that frequently occur together.  Mallet has a readily available toolkit for doing such a task: <a href=""http://mallet.cs.umass.edu/topics.php"" rel=""nofollow noreferrer"">http://mallet.cs.umass.edu/topics.php</a> .</p>

<p>To answer your original question, POS tagger, parsers, and NER tools are not typically used for topic identification, but are more heavily used for tasks like information extraction where the goal is to identify within a document the specific actors, events, locations, times, etc...  For example if you had a simple sentence like ""John gave the apple to Mary."" you might use a dependency parser to figure out that John is the subject, the apple is the object, and Mary is the prepositional object; thus you know John is the giver and Mary is the receiver and not vice-versa.</p>
",1162,1277371116
Looking for artificial intelligence (AI) cookbook reader research,"<p>I am looking for research (published) on AI techniques for reading cookbook recipes. Recipes are a very limited domain that might be doable in a natural language recognition engine with some degree of accuracy.</p>

<p>I have in mind writing a program that would allow copy/pasting a recipe from a web browser into the AI and having it determine the title, author, ingredients, instructions, nutritional information, etc. by ""reading"" the recipe. I would also like to be able to process PDF files (I have a large collection), maybe also just using copy/paste.</p>

<p>The output will be some kind of (standard) XML-based format that can be read by a recipe organizer.</p>

<p>I have in mind PhD or Masters-level work.</p>
","artificial-intelligence, nlp","<p>One subfield of AI that you might find relevant is <strong><a href=""http://en.wikipedia.org/wiki/Information_extraction"" rel=""nofollow noreferrer"">information extraction</a></strong>.</p>

<p>Information extraction algorithms often work by using rules (e.g. regular expressions) to identify entities and relations in text.  These rules can either be defined by hand (i.e. the Suiseki algorithm) or learned with supervised machine learning algorithms (i.e. RAPIER, Wrapper Induction, Conditional Random Fields).</p>

<hr>

<p>For example, an information extraction algorithm might grab data from a job posting:</p>

<p><code>Job Title</code>: Senior DBMS Consultant<br/>
<code>Location</code>: Dallas,TX<br/>
<code>Responsibilities</code>: DBMS Applications consultant works with project teams to define DBMS based solutions that support the enterprise deployment of Electronic Commerce, Sales Force Automation, and Customer Service applications.<br/>
<code>Desired Requirements</code>: 3-5 years exp. developing Oracle or SQL Server apps using Visual Basic, C/C++, Powerbuilder, Progress, or similar. Recent experience related to installing and configuring Oracle or SQL Server in both dev. and deployment environments.<br/>
<code>Desired Skills</code>: Understanding of UNIX or NT, scripting language. Know principles of structured software engineering and project management<br/></p>

<p>...and distill it into this template:</p>

<p><code>title</code>: Senior DBMS Consultant<br/>
<code>state</code>: TX<br/>
<code>city</code>: Dallas<br/>
<code>country</code>: US<br/>
<code>language</code>: Powerbuilder, Progress, C, C++, Visual Basic<br/>
<code>platform</code>: UNIX, NT<br/>
<code>application</code>: SQL Server, Oracle<br/>
<code>area</code>: Electronic Commerce, Customer Service<br/>
<code>required years of experience</code>: 3<br/>
<code>desired years of experience</code>: 5<br/></p>

<hr>

<p><a href=""http://userweb.cs.utexas.edu/~mooney/"" rel=""nofollow noreferrer"">Ray Mooney</a> and his group at the University of Texas at Austin have done some great work in information extraction.  Here are some references that might make good jumping-off points:</p>

<ul>
<li>Raymond J. Mooney and Razvan Bunescu, <a href=""http://www.cs.utexas.edu/~ml/papers/text-kddexplore-05.pdf"" rel=""nofollow noreferrer""><strong>Mining Knowledge from Text Using Information Extraction</strong></a>.  <em>SIGKDD Explorations, 7:1 (2005), pp 3-10.</em></li>
<li>Stephen Soderland, <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.8809&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer""><strong>Learning Information Extraction Rules for Semi-Structured and Free Text</strong></a>.  <em>Machine Learning, 34:1 (1999), pp 233-272.</em></li>
<li>C. Blaschke and A. Valencia. <a href=""http://www.computer.org/portal/web/csdl/doi/10.1109/MIS.2002.999215"" rel=""nofollow noreferrer""><strong>The frame-based module of the Suiseki information extraction system</strong></a>. <em>IEEE Intelligent Systems, 17:14–20 (2002).</em></li>
</ul>
",309,1282738366
Tools for getting intent from Twitter statuses?,"<p>I am considering a project in which a publication's content is augmented by relevant, publicly available tweets from people in the area. But how could I programmatically find the relevant Tweets? I know that generating a structure representing the meaning of natural language is pretty much the holy grail of NLP, but perhaps there's some tool I can use to at least narrow it down a bit?</p>

<p>Alternatively, I could just use hashtags. But that requires more work on behalf of the users. I'm not super familiar with Twitter - do most people use hashtags (even for smaller scale issues), or would relying on them cut off a large segment of data?</p>

<p>I'd also be interested in grabbing Facebook statuses (with permission from the poster, of course), and hashtag use is pretty rare on Facebook.</p>

<p>I could use simple keyword search to crudely narrow the field, but that's more likely to require human intervention to determine which tweets should actually be posted alongside the content.</p>

<p>Ideas? Has this been done before?</p>
","facebook, twitter, nlp, social-media",,1164,1281815624
Binarization in Natural Language Processing,"<p>Binarization is the act of transforming colorful features of of an entity into vectors of numbers, most often binary vectors, to make good examples for classifier algorithms.</p>

<p>If we where to binarize the sentence ""The cat ate the dog"", we could start by assigning every word an ID (for example cat-1, ate-2, the-3, dog-4) and then simply replace the word by it's ID giving the vector &lt;3,1,2,3,4>. </p>

<p>Given these IDs we could also create a binary vector by giving each word four possible slots, and setting the slot corresponding to a specific word with to one, giving the vector &lt;0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1>. The latter method is, as far as I know, is commonly referred to as the bag-of-words-method.</p>

<p>Now for my question, what is the <i>best</i> binarization method when it comes to describe features for natural language processing in general, and transition-based <i>dependency parsing</i> (with Nivres algorithm) in particular? </p>

<p>In this context, we do not want to encode the whole sentence, but rather the current state of the parse, for example the top word on the stack en the first word in the input queue. Since order is highly relevant, this rules out the bag-of-words-method. </p>

<p>With <i>best</i>, I am referring to the method that makes the data the most intelligible for the classifier, without using up unnecessary memory. For example I don't want a word bigram to use 400 million features for 20000 unique words, if only 2% the bigrams actually exist.</p>

<p>Since the answer is also depending on the particular classifier, I am mostly interested in maximum entropy models (liblinear), support vector machines (libsvm) and perceptrons, but answers that apply to other models are also welcome.</p>
","machine-learning, nlp, classification, libsvm","<p>This is actually a really complex question. The first decision you have to make is whether to <a href=""http://en.wikipedia.org/wiki/Lemmatization"" rel=""noreferrer"">lemmatize</a> your input tokens (your words). If you do this, you dramatically decrease your type count, and your syntax parsing gets a lot less complicated. However, it takes a lot of work to lemmatize a token. Now, in a computer language, this task gets greatly reduced, as most languages separate keywords or variable names with a well defined set of symbols, like whitespace or a period or whatnot.</p>

<p>The second crucial decision is what you're going to do with the data post-facto. The ""bag-of-words"" method, in the binary form you've presented, ignores word order, which is completely fine if you're doing <a href=""http://en.wikipedia.org/wiki/Automatic_summarization"" rel=""noreferrer"">summarization of a text</a> or maybe a Google-style search where you don't care <em>where</em> the words appear, as long as they appear. If, on the other hand, you're building something like a compiler or parser, order is very much important. You can use the token-vector approach (as in your second paragraph), or you can extend the bag-of-words approach such that each non-zero entry in the bag-of-words vector contains the linear index position of the token in the phrase.</p>

<p>Finally, if you're going to be building <a href=""http://en.wikipedia.org/wiki/Parse_tree"" rel=""noreferrer"">parse trees</a>, there are obvious reasons why you'd want to go with the token-vector approach, as it's a big hassle to maintain sub-phrase ids for every word in the bag-of-words vector, but very easy to make ""sub-vectors"" in a token-vector. In fact, Eric Brill used a token-id sequence for his <a href=""http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/parsing/taggers/brill/0.html"" rel=""noreferrer"">part-of-speech tagger</a>, which is really neat.</p>

<p>Do you mind if I ask what specific task you're working on?</p>
",3329,1235421115
Is there something like automatic writing or surrealist automatism in programming?,"<p>Is there something like <a href=""http://en.wikipedia.org/wiki/Automatic_writing"" rel=""nofollow noreferrer"">automatic writing</a> or <a href=""http://en.wikipedia.org/wiki/Surrealist_automatism"" rel=""nofollow noreferrer"">surrealist automatism</a> in programming?</p>
","text, automation, nlp, markov-chains",,246,1279032223
"Extract inconsistently formatted date from string (date parsing, NLP)","<p>I have a large list of files, some of which have dates embedded in the filename.  The format of the dates is inconsistent and often incomplete, e.g. ""Aug06"", ""Aug2006"", ""August 2006"", ""08-06"", ""01-08-06"", ""2006"", ""011004"" etc.  In addition to that, some filenames have unrelated numbers that look somewhat like dates, e.g. ""20202010"".</p>

<p>In short, the dates are normally incomplete, sometimes not there, are inconsistently formatted and are embedded in a string with other information, e.g. ""Report Aug06.xls"".</p>

<p>Are there any Perl modules available which will do a decent job of guessing the date from such a string?  It doesn't have to be 100% correct, as it will be verified by a human manually, but I'm trying to make things as easy as possible for that person and there are thousands of entries to check :)</p>
","perl, date, nlp","<p>Date::Parse is definitely going to be part of your answer - the bit that works out a randomly formatted date-like string and make an actual useable date out of it.</p>

<p>The other part of your problem - the rest of the characters in your filenames - is unusual enough that you're unlikely to find someone else has packaged up a module for you.</p>

<p>Without seeing more of your sample data, it's really only possible to guess, but I'd start by identifying possible or likely ""date section"" candidates.</p>

<p>Here's a nasty brute-force example using Date::Parse (a smarter approach would use a list of regex-en to try and identify dates-bits - I'm happy to burn cpu cycles to not think quite so hard though!)</p>

<pre><code>!/usr/bin/perl
use strict;
use warnings;
use Date::Parse;

my @files=(""Report Aug06.xls"", ""ReportAug2006"", ""Report 11th September 2006.xls"", 
           ""Annual Report-08-06"", ""End-of-month Report01-08-06.xls"", ""Report2006"");

# assumption - longest likely date string is something like '11th September 2006' - 19 chars
# shortest is ""2006"" - 4 chars.
# brute force all strings from 19-4 chars long at the end of the filename (less extension)
# return the longest thing that Date::Parse recognises as a date



foreach my $file (@files){
  #chop extension if there is one
  $file=~s/\..*//;
  for my $len (-19..-4){
    my $string = substr($file, $len);
    my $time = str2time($string);
    print ""$string is a date: $time = "",scalar(localtime($time)),""\n"" if $time;
    last if $time;
    }
  }
</code></pre>
",1893,1281402965
What&#39;s needed for NLP?,"<p>assuming that I know nothing about everything and that I'm starting in programming TODAY what do you say would be necessary for me to learn in order to start working with Natural Language Processing?</p>

<p>I've been struggling with some string parsing methods but so far it is just annoying me and making me create ugly code. I'm looking for some fresh new ideas on how to create a Remember The Milk API like to parse user's input in order to provide an input form for fast data entry that are not based on fields but in simple one line phrases instead.</p>

<p><strong>EDIT</strong>: RTM is todo list system. So in order to enter a task you don't need to type in each field to fill values (task name, due date, location, etc). You can simply type in a phrase like ""Dentist appointment monday at 2PM in WhateverPlace"" and it will parse it and fill all fields for you.</p>

<p>I don't have any kind of technical constraints since it's going to be a personal project but I'm more familiar with .NET world. Actually, I'm not sure this is a matter of language but if it's necessary I'm more than willing to learn a new language to do it.</p>

<p>My project is related to personal finances so the phrases are more like ""Spent 10USD on Coffee last night with my girlfriend"" and it would fill location, amount of $$$, tags and other stuff.</p>

<p>Thanks a lot for any kind of directions that you might give me!</p>
","algorithm, language-agnostic, nlp","<p>Have a look at NLTK, its a good resource for beginner programmers interested in NLP. 
<a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">http://www.nltk.org/</a><br>
It is written in python which is one of the easier programming languages.</p>

<p>Now that I understand your problem, here is my solution:</p>

<p>You can develop a kind of restricted vocabulary, in which all amounts must end witha $ sign or  any time must be in form of 00:00 and/or end with AM/PM, regarding detecting items, you can use list of objects from ontology such as Open Cyc. Open Cyc can provide you with list of all objects such beer, coffee, bread and milk etc. this will help you to detect objects in the short phrase. Still it would be a very fuzzy approach.</p>
",347,1281390714
Natural Language Processing Solution in Java?,"<p>Are there any equally great packages like Python's NTLK in Java world ?</p>
","java, nlp","<p>Two popular ones that I know of are:</p>

<p><a href=""http://gate.ac.uk/"" rel=""noreferrer"">Gate</a></p>

<p><a href=""http://opennlp.sourceforge.net/"" rel=""noreferrer"">OpenNLP</a></p>
",1576,1276732897
Voice Form Matching in Visual C++,"<p>Are there SDK's for voice-form matching / comparison for Visual C++? Or, possibly converting sounds to phonetics.</p>

<ul>
<li><p>Usage: Program will do different things from input from certain command words given in a made-up foreign language. (Klingon)</p></li>
<li><p>Analysis - comparison of user's voice with existing pre-recorded voice segment</p></li>
</ul>

<p>Rather than using existing text to speech SDK's, I believe I have to opt for a more general version since the language I am dealing with isn't widely supported.</p>
","visual-c++, sdk, nlp, voice, voice-recognition",,193,1281283505
effective way to determine if a message is spam?,"<p>Is there a way to determine if the given message is a spam? For example those who posts on forums and advertise their own sites for various products.</p>
","nlp, spam-prevention","<p>There are many ways, non of which are fool proof.</p>

<p>The current technology that achieves the best results is <a href=""http://en.wikipedia.org/wiki/Bayesian_inference"" rel=""nofollow noreferrer"">bayesian inference</a> for statistical analysis.</p>
",112,1281173631
"Extracting a set of words with the Python/NLTK, then comparing it to a standard English dictionary","<p>I have:</p>

<pre><code>from __future__ import division
import nltk, re, pprint
f = open('/home/a/Desktop/Projects/FinnegansWake/JamesJoyce-FinnegansWake.txt')
raw = f.read()
tokens = nltk.wordpunct_tokenize(raw)
text = nltk.Text(tokens)
words = [w.lower() for w in text]

f2 = open('/home/a/Desktop/Projects/FinnegansWake/catted-several-long-Russian-novels-and-the-NYT.txt')
englishraw = f2.read()
englishtokens = nltk.wordpunct_tokenize(englishraw)
englishtext = nltk.Text(englishtokens)
englishwords = [w.lower() for w in englishwords]
</code></pre>

<p>which is straight from the NLTK manual. What I want to do next is to compare <code>vocab</code> to an exhaustive set of English words, like the OED, and extract the difference -- the set of Finnegans Wake words that have not, and probably never will, be in the OED. I'm much more of a verbal person than a math-oriented person, so I haven't figured out how to do that yet, and the manual goes into way too much detail about stuff I don't actually want to do. I'm assuming it's just one or two more lines of code, though. </p>
","python, text, set, nlp, nltk","<p>If your English dictionary is indeed a set (hopefully of lowercased words),</p>

<pre><code>set(vocab) - english_dictionary
</code></pre>

<p>gives you the set of words which are in the <code>vocab</code> set but not in the <code>english_dictionary</code> one.  (It's a pity that you turned <code>vocab</code> into a list by that <code>sorted</code>, since you need to turn it back into a set to perform operations such as this set difference!).</p>

<p>If your English dictionary is in some different format, not really a set or not comprised only of lowercased words, you'll have to tell us what that format is for us to be able to help!-)</p>

<p><strong>Edit</strong>: given the OP's edit shows that both <code>words</code> (what was previously called <code>vocab</code>) and <code>englishwords</code> (what I previously called <code>english_dictionary</code>) are in fact lists of lowercased words, then</p>

<pre><code>newwords = set(words) - set(englishwords)
</code></pre>

<p>or</p>

<pre><code>newwords = set(words).difference(englishwords)
</code></pre>

<p>are two ways to express ""the set of words that are not englishwords"".  The former is slightly more concise, the latter perhaps a bit more readable (since it uses the word ""difference"" explicitly, instead of a minus sign) and perhaps a bit more efficient (since it doesn't explicitly transform the list <code>englishwords</code> into a set -- though, if speed is crucial this needs to be checked by measurement, since ""internally"" <code>difference</code> still needs to do some kind of ""transformation-to-set""-like operation).</p>

<p>If you're keen to have a list as the result instead of a set, <code>sorted(newwords)</code> will give you an alphabetically sorted list (<code>list(newwords)</code> would give you a list a bit faster, but in totally arbitrary order, and I suspect you'd rather wait a tiny extra amount of time and get, in return, a nicely alphabetized result;-).</p>
",3349,1281132240
How to find common phrases in a large body of text,"<p>I'm working on a project at the moment where I need to pick out the most common phrases in a huge body of text. For example say we have three sentences like the following:</p>

<ul>
<li><strong>The dog jumped</strong> over the woman.</li>
<li><strong>The dog jumped</strong> into the car.</li>
<li><strong>The dog jumped</strong> up the stairs.</li>
</ul>

<p>From the above example I would want to extract ""<em>the dog jumped</em>"" as it is the most common phrase in the text. At first I thought, ""oh lets use a directed graph [with repeated nodes]"":</p>

<p><a href=""http://img.skitch.com/20091218-81ii2femnfgfipd9jtdg32m74f.png"">directed graph http://img.skitch.com/20091218-81ii2femnfgfipd9jtdg32m74f.png</a></p>

<p><strong>EDIT</strong>: Apologies, I made a mistake while making this diagram ""over"", ""into"" and ""up"" should all link back to ""the"".</p>

<p>I was going to maintain a count of how many times a word occurred in each node object (""the"" would be 6; ""dog"" and ""jumped"", 3; etc.) but despite many other problems the main one came up when we add a few more examples like (please ignore the bad grammar :-)): </p>

<ul>
<li>Dog jumped up and down.</li>
<li>Dog jumped like no dog had ever jumped before.</li>
<li>Dog jumped happily.</li>
</ul>

<p>We now have a problem since ""<em>dog</em>"" would start a new root node (at the same level as ""the"") and we would not identify ""<em>dog jumped</em>"" as now being the most common phrase. So now I am thinking maybe I could use an undirected graph to map the relationships between all the words and eventually pick out the common phrases but I'm not sure how this is going to work either, as you lose the important relationship of order between the words. </p>

<p>So does anyone have any general ideas on how to identify common phrases in a large body of text and what data structure I would use.</p>

<p>Thanks,
Ben</p>
","data-structures, graph, data-mining, text-analysis","<p>Check out this related question: <a href=""https://stackoverflow.com/questions/1426383/what-techniques-tools-are-there-for-discovering-common-phrases-in-chunks-of-text"">What techniques/tools are there for discovering common phrases in chunks of text?</a>  Also related to <a href=""http://en.wikipedia.org/wiki/Longest_common_substring_problem"" rel=""noreferrer"">the longest common substring problem</a>.</p>

<p>I've posted this before, but I use <a href=""http://www.r-project.org"" rel=""noreferrer"">R</a> for all of my data-mining tasks and it's well suited to this kind of analysis.  In particular, look at the <code>tm</code> package.  Here are some relevant links:</p>

<ul>
<li>Paper about the package in the Journal of Statistical Computing: <a href=""http://www.jstatsoft.org/v25/i05/paper"" rel=""noreferrer""><a href=""http://www.jstatsoft.org/v25/i05/paper"" rel=""noreferrer"">http://www.jstatsoft.org/v25/i05/paper</a></a>.  The paper includes a nice example of an analysis of the R-devel
mailing list (<a href=""https://stat.ethz.ch/pipermail/r-devel/"" rel=""noreferrer"">https://stat.ethz.ch/pipermail/r-devel/</a>) newsgroup postings from 2006.</li>
<li>Package homepage: <a href=""http://cran.r-project.org/web/packages/tm/index.html"" rel=""noreferrer"">http://cran.r-project.org/web/packages/tm/index.html</a></li>
<li>Look at the introductory vignette: <a href=""http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf"" rel=""noreferrer"">http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf</a></li>
</ul>

<p>More generally, there are a large number of text mining packages <a href=""http://cran.r-project.org/web/views/NaturalLanguageProcessing.html"" rel=""noreferrer"">on the Natural Language Processing view on CRAN</a>.  </p>
",16502,1261151545
how to create exclamations for a particular sentence,"<p>I would like to create exclamations for a particular sentence using the java API?</p>

<p>e.g. It's surprising == Isn't it surprising!<br>
e.g. It's cold == Isn't it cold!  </p>

<p>Are there any vendors or tools which help you generate exclamations, provided you give a sentence (i.e. the left hand side in the above example). Note: The sentences will be provided by the user and we should be able to get the correct sentence.</p>

<p>I am not sure, if this needs to be tagged under other categories</p>

<p><strong>EDIT1</strong></p>

<p>Some more examples, I would like this to be as generic as possible</p>

<p>e.g. They're late == Aren't they late!<br>
e.g. He looks tired == Doesn't he look tired!<br>
e.g. That child is dirty == Isn't that child dirty!<br>
e.g. It's hot == Isn't it hot!  </p>
","java, regex, nlp, text-manipulation","<p>This question is not about exclamations. You can just add '!' to all your input examples and get valid exclamatory sentences.</p>

<p>You are after grammar transformations, like <a href=""http://www.towson.edu/ows/SentPattTrans.htm"" rel=""nofollow noreferrer"">these</a>.</p>

<p><a href=""http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html"" rel=""nofollow noreferrer"">LingPipe</a> looks like it has some intersting stuff that you could use (it's java), particularly if you are developing a learning system, and need to recognise '<a href=""http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html"" rel=""nofollow noreferrer"">parts of speach</a>' (like e.g. subject and verb phrase, as per your examples).</p>
",493,1277292758
Plural of words using Open Office API for Python (UNO),"<p>I would like to retrieve the plural words in different languages in Python.</p>

<p>I know that openoffice has an API called uno (<code>import uno</code>) and it should give me this ability using openoffice's language dictionaries, but I could not find any reference to it.</p>

<p>As a concrete example, I would something like this:</p>

<pre><code>&gt;&gt;&gt; print getPluralOf('table')

tables
</code></pre>

<p>One possibility is to download the dictionary files though this <a href=""http://wiki.services.openoffice.org/wiki/Dictionaries"" rel=""nofollow noreferrer"">link</a> and write a method to read the dictionary and form the plurals. But i can't believe that this is not available already using uno.</p>

<p>I appreciate any help</p>
","python, openoffice.org, nlp, pyuno",,539,1281011739
Splitting a Domain name into constituent words (if possible)?,"<p>I want to break a domain name into constituent words and numbers e.g.</p>

<p>iamadomain11.com = ['i', 'am', 'a', 'domain', '11']</p>

<p>How do i do this? I am aware that there may be multiple sets possible, however, i am currently even ok, just getting 1 set of possibilities. </p>
","nlp, algorithm, dynamic-programming, np-complete",,2198,1280288586
stanford tagger - tagging speed,"<p>regarding the stanford tagger, I've provided my own labelled corpus for training the model for the stanford tagger. However, I've realised that the tagging speed of my model for the tagger is much less slower than the default wsjleft3 tagger model. What might contribute to this? And how do I improve the speed of my model? (I've added 3 or 4 custom tags in addition to the Penn treebank tagsets)  </p>
","nlp, stanford-nlp","<p>While adding more features (in arch) makes it a bit slower in general (as feature extraction is one of the main runtime costs), the two big determinants of speed are:</p>

<ul>
<li>Number of context tags used in
features:  left3words uses the
previous and second previous tag (2)
and so is fairly fast, bidirectional
uses 4 (two on each side) and so is
very slow.  A tagger that uses just 1
or 0 context tags is much faster
again.</li>
<li>Size of the tag set in general, and in particular the size of the set of open class tags that can be applied to unknown words.  (But adding 3 or 4 should make almost no difference -- it's problematic when you have a tag set with hundreds of tags.)</li>
</ul>
",597,1279081050
Where can I find get a dump of raw text on the web?,"<p>I am looking to do some text analysis in a program I am writing. I am looking for alternate sources of text in its raw form similar to what is provided in the Wikipedia dumps (download.wikimedia.com).</p>

<p>I'd rather not have to go through the trouble of crawling websites, trying to parse the html , extracting text etc.. </p>
","parsing, text, nlp, wikipedia",,670,1280756681
convert 2010-04-15 23:59:59 to 15th Apr 2010,"<p>I have the following date format: <code>2010-04-15 23:59:59</code></p>

<p>How would I go about converting this into: <code>15th Apr 2010</code></p>

<p>using javascript</p>
","javascript, datetime, nlp",,110,1280743147
How would you group up articles by context? - Natural Language,"<p><strong>I have lists of articles made of:
title, subtitle and body.</strong></p>

<p>Now I need to parse all these articles and <strong>group them up under different context categories</strong> or sub categories based on their possible <strong>keywords</strong>.</p>

<blockquote>
  <p><strong>e.g.</strong> if the article is likely to be related to sports cars then the article would be associated with the car or/and vehicle context</p>
</blockquote>

<hr>

<p>Now I understand that this is a vast ocean, but this is also why I have put up this question. Because the ocean of solutions might be too big for me, and I would most likely get lost and adopt some bad thought solution. </p>

<p>There are probably some popular and standardized ways of doing this that I do not know, and it would be very useful if someone pointed me in the right direction. </p>

<p>Help would be great. =)</p>
","python, data-mining, nlp","<p>The <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">Natural Lanugage Toolkit</a> but don't expect that there is a magic bullet in there which will keep you having to learn a fair bit about linguistics, as the problem you describe cannot be solved wholly mechanically.</p>
",102,1280593411
Generating RDF From Natural Language,"<p>Are there any tools available for generating RDF from natural language? A <a href=""http://simile.mit.edu/wiki/RDFizers"" rel=""nofollow noreferrer"">list of RDFizers</a> compiled by the SIMILE project only mentions one, the Monrai Cypher. Unfortunately, it seems to have been a proprietary tool developed by Monrai Technologies, which has since disappeared, and I can't find any download links. Has anyone seen anything similar?</p>
","machine-learning, nlp, rdf, semantic-web, ontology","<p>You want some ontology learning and population tools. </p>

<p><a href=""http://ontogenesis.knowledgeblog.org/331"" rel=""nofollow noreferrer"">This online article</a> lists 4 different systems:</p>

<ul>
<li>Text2Onto, </li>
<li>Abraxas, </li>
<li>KnowItAll, </li>
<li>OntoLearn</li>
</ul>

<p>You may want to check out the book; it reviews several ontology learning tools as well:
<em>Ontology learning from text: methods, evaluation and applications</em>, by Paul Buitelaar, Philipp Cimiano, Bernardo Magnini</p>
",2010,1280164865
Parsing bulk text with Hadoop: best practices for generating keys,"<p>I have a 'large' set of line delimited full sentences that I'm processing with Hadoop.  I've developed a mapper that applies some of my favorite NLP techniques to it.  There are several different techniques that I'm mapping over the original set of sentences, and my goal during the reducing phase is to collect these results into groups such that all members in a group share the same original sentence.</p>

<p>I feel that using the entire sentence as a key is a bad idea.  I felt that generating some hash value of the sentence may not work because of a limited number of keys (unjustified belief).  </p>

<p><em>Can anyone recommend the best idea/practice for generating unique keys for each sentence?</em>  Ideally, I would like to preserve order.  However, this isn't a main requirement.</p>

<p>Aντίο,</p>
","nlp, hadoop","<p>Standard hashing should work fine. Most hash algorithms have a value space far greater than the number of sentences you're likely to be working with, and thus the likelihood of a collision will still be extremely low.</p>
",238,1280287371
Adding documents to a scored TF-IDF collection?,"<p>I have a large collection of documents that already have their TF-IDF computed.  I'm getting ready to add some more documents to the collection, and I am wondering if there is a way to add TF-IDF scores to the new documents without re-processing the entire database?</p>
","machine-learning, nlp, similarity, tf-idf","<p>Basically there are two options:</p>

<ol>
<li><p>Compute your tf-idf scores only when you need them. Adding a new document is now trivial. All you'll have to do is to update the number of all documents, the number of documents in which a token occurs and to store the token occurence vector for the new document. </p></li>
<li><p>Periodically recalc your tf-idf vectors, maybe after adding 100K documents or something like that. In between, just work with the old values (number of all documents, number of documents a token occurs in).</p></li>
</ol>

<p>If your collection is really large, you'll probably want to take the second approach, because new documents won't change the global distribution of words much anyway. That said, it's better to test both methods and settle for the one that fits your problem best.</p>
",1198,1279818824
getting text that will be displayed to user from html,"<p>Bit of a random one, i am wanting to have a play with some NLP stuff and I would like to:</p>

<p><strong>Get all the text that will be displayed to the user in a browser from HTML</strong>.</p>

<p>My ideal output would not have any tags in it and would only have fullstops (and any other punctuation used) and new line characters, though i can tolerate a fairly reasonable amount of failure in this (random other stuff ending up in output).</p>

<p>If there was a way of inserting a newline or full stop in situations where the content was likely not to continue on then that would be considered an added bonus. e.g:</p>

<p>items in an ul or option tag could be separated by full stops (or to be honest just ignored).</p>

<p>I am working Java, but would be interested in seeing any code that does this.</p>

<p>I can (and will if required) come up with something to do this, just wondered if there was anything out there like this already, as it would probably be better than what I come up with in an afternoon ;-).</p>

<p>An example of the code I might write if I do end up doing this would be to use a SAX parser to find content in p tags, strip it of any span or strong etc tags, and add a full stop if I hit a div or another p without having had a fullstop.</p>

<p>Any pointers or suggestions very welcome.</p>
","java, html, nlp, screen-scraping","<p>HTML parsers seem to be a reasonable starting point for this.</p>

<p>there are a number of them for example: <a href=""http://htmlcleaner.sourceforge.net/"" rel=""nofollow noreferrer"">HTMLCleaner</a> and <a href=""http://nekohtml.sourceforge.net/"" rel=""nofollow noreferrer"">Nekohtml</a> seem to work fine.</p>

<p>They are good as they fix the tags to allow you to more consistently process them, even if you are just removing them.</p>

<p>But as it turns out you probably want to get rid of script tags meta data etc. And in that case you are better working with well formed XML which these guy get for you from ""wild"" html.</p>

<p>there are many SO questions relating to this (like <a href=""https://stackoverflow.com/questions/2168610/which-html-parser-is-best"">this</a> one) you should search for ""HTML parsing"" though ;-)</p>
",170,1276423181
Unstructured Text to Structured Data,"<p>I am looking for references (tutorials, books, academic literature) concerning structuring unstructured text in a manner similar to the google calendar quick add button.</p>

<p>I understand this may come under the NLP category, but I am interested only in the process of going from something like ""Levi jeans size 32 A0b293""</p>

<p>to: Brand: Levi, Size: 32, Category: Jeans, code: A0b293</p>

<p>I imagine it would be some combination of lexical parsing and machine learning techniques.</p>

<p>I am rather language agnostic but if pushed would prefer python, Matlab or C++ references</p>

<p>Thanks</p>
","python, nlp, structured-data",,7822,1278028134
"Perl and NLP, parse Names out of Biographies","<p>I'm pretty new to NLP in general, but getting really good at Perl, and I was wondering what kind of powerful NLP modules are out there. Basically, I have a file with a bunch of paragraphs, and some of them are people's biographies. So, first I need to look for a person's name, and that helps with the rest of the process later.</p>

<p>So I was roughly starting with something like this:</p>

<pre><code>foreach $PPid (0 .. $PPscalar) {
$paragraph = @PP[$PPid];
if ($paragraph =~ /^(\w+ \w\. \w+|\w+ \w+)( also|)( has served| served| worked| joined| currently serves| has| was| is|, )/){
    $possibleName = $1;
    $badName = 0;
    foreach $piece (@pieces){
    if ($possibleName =~ /$piece/){
        $badName = 1;
    }
    }
    if ($badName == 0){
    push @namePile, $possibleName;
    }
}

}
</code></pre>

<p>Because most of the names start at the beginning of the paragraphs. And then I'm looking for keywords that denote action or possession, but right now, that picks up extra junk that is not a name. There has to be a module to do this, right?</p>
","perl, module, nlp","<p>Extracting names from data is hard.  There are a variety of solutions.  For named entity extraction you've got the following</p>

<ol>
<li><a href=""http://search.cpan.org/dist/Lingua-EN-NamedEntity/"" rel=""nofollow noreferrer"">The naive approach</a>.  I remember looking at this and being unimpressed with the output.</li>
<li><a href=""http://namefind.sourceforge.net/"" rel=""nofollow noreferrer"">The dictionary approach</a>.  I've used this, but lots of false negatives, and I'm not too fond of the code underneath it.</li>
<li><a href=""http://search.cpan.org/perldoc?Text::NLP::Stanford::EntityExtract"" rel=""nofollow noreferrer"">An open source binary with a perl interface</a> (not recommended, and I'm the author of this cpan library - and setting it up is fiddly too).</li>
<li>Best solution is the propietary web service with the <a href=""http://search.cpan.org/perldoc?Net::Calais"" rel=""nofollow noreferrer"">Net::Calais</a> perl wrapper</li>
</ol>

<p>Net::Calais is by far the best bet for speed and accuracy.  Go with the Stanford library if you need the underlying implementation to be open source.</p>
",483,1279220837
Identifying collocation in Stanford POS Tagger?,"<p>Is the Stanford POS tagger able to detect collocation? If so, how do I use it?</p>

<p>If I want to provide my own training file for the Stanford POS Tagger, do I have to tag the words according to the 
<a href=""http://images.freshmeat.net/editorials/python_linguistics/wsj_tagged/wsj_0099.pos"" rel=""nofollow noreferrer"">one like the WSJ</a></p>

<p>This means that I have to 'bracket"" the words into Entities and collocation right?</p>

<p>If so, how do I find collocations from the tagger? </p>

<p>I am avoiding the need of using a parser.</p>
","nlp, stanford-nlp","<p>No, the Stanford tagger neither needs nor provides collocations.  It just puts part of speech labels on individual words.  (If you are training a tagger, you don't have to use WSJ tags, but you do have to provide training data with a tag for each word.)</p>
",780,1278398724
Evaluating the &quot;Value&quot; Attribute,"<p>I'm attempting to use the <a href=""http://www.openamplify.com/"" rel=""nofollow noreferrer"">OpenAmplify</a> <a href=""http://community.openamplify.com/content/2dot0docs.aspx"" rel=""nofollow noreferrer"">API</a> to evaluate the content of a URI. The point is to draw out the topics that are truly relevant to the article. Unfortunately, the topical analysis I'm getting back is:</p>

<ol>
<li>Huge, and</li>
<li>Varied</li>
</ol>

<p>Neither quality is terribly useful for what I'm trying to do because the signal to noise ratio is being heavily skewed towards noise. I'm analyzing web content, so there is a certain amount (perhaps a large amount) of irrelevant content (ads, etc.) involved. I get that. </p>

<p>Nonetheless, many of the topics being returned are either useless (utterly non-sensical, not even words), irrelevant (as in, where did that come from?) or too granular to provide any meaning or insight. I can probably filter out most of this noise using the <strong>value</strong>, um, value that is returned for each domain, subdomain, topic, et al, but I don't really know what it means. </p>

<p>Certainly I understand that the <strong>value</strong> it's a measure of ""the prominence of the word in the text,"" but the number itself appears entirely arbitrary in a way that I prevents me saying something like ""ignore any terms with a value less than 50"" and have it carry any real meaning.</p>

<p>Are there any range criteria that I can use to help me understand how to use a topic's value score as a filtering threshold? Alternatively, is there another field that I should be using for this sort of filtration?</p>

<p>Thanks for your help.</p>
",nlp,"<p>From other channels, I've learned that the <code>value</code> attribute can't be evaluated the way I was hoping. It means different things for different signals and none are defined in such a way that are meaningful for this kind of requirement.</p>
",42,1278421492
Probabilistic Generation of Semantic Networks,"<p>I've studied some simple semantic network implementations and basic techniques for parsing natural language. However, I haven't seen many projects that try and bridge the gap between the two.</p>

<p>For example, consider the dialog:</p>

<pre><code>""the man has a hat""
""he has a coat""
""what does he have?"" =&gt; ""a hat and coat""
</code></pre>

<p>A simple semantic network, based on the grammar tree parsing of the above sentences, might look like:</p>

<pre><code>the_man = Entity('the man')
has = Entity('has')
a_hat = Entity('a hat')
a_coat = Entity('a coat')
Relation(the_man, has, a_hat)
Relation(the_man, has, a_coat)
print the_man.relations(has) =&gt; ['a hat', 'a coat']
</code></pre>

<p>However, this implementation assumes the prior knowledge that the text segments ""the man"" and ""he"" refer to the same network entity.</p>

<p>How would you design a system that ""learns"" these relationships between segments of a semantic network? I'm used to thinking about ML/NL problems based on creating a simple training set of attribute/value pairs, and feeding it to a classification or regression algorithm, but I'm having trouble formulating this problem that way.</p>

<p>Ultimately, it seems I would need to overlay probabilities on top of the semantic network, but that would drastically complicate an implementation. Is there any prior art along these lines? I've looked at a few libaries, like NLTK and OpenNLP, and while they have decent tools to handle symbolic logic and parse natural language, neither seems to have any kind of proabablilstic framework for converting one to the other.</p>
","machine-learning, data-mining, nlp","<p>There is quite a lot of history behind this kind of task. Your best start is probably by looking at <a href=""http://en.wikipedia.org/wiki/Question_answering"" rel=""nofollow noreferrer"">Question Answering</a>. </p>

<p>The general advice I always give is that if you have some highly restricted domain where you know about all the things that might be mentioned and all the ways they interact then you can probably be quite successful. If this is more of an 'open-world' problem then it will be extremely difficult to come up with something that works acceptably.</p>

<p>The task of extracting relationship from natural language is called 'relationship extraction' (funnily enough) and sometimes fact extraction. This is a pretty large field of research, <a href=""http://ace.cs.ohiou.edu/~razvan/papers/thesis-white.pdf"" rel=""nofollow noreferrer"">this guy</a> did a PhD thesis on it, as have many others. There are a large number of challenges here, as you've noticed, like entity detection, anaphora resolution, etc. This means that there will probably be a lot of 'noise' in the entities and relationships you extract.</p>

<p>As for representing facts that have been extracted in a knowledge base, most people tend not to use a probabilistic framework. At the simplest level, entities and relationships are stored as triples in a flat table. Another approach is to use an ontology to add structure and allow reasoning over the facts. This makes the knowledge base vastly more useful, but adds a lot of scalability issues. As for adding probabilities, I know of the <a href=""http://www.pr-owl.org/"" rel=""nofollow noreferrer"">Prowl</a> project that is aimed at creating a probabilistic ontology, but it doesn't look very mature to me.</p>

<p>There is some research into probabilistic relational modelling, mostly into <a href=""http://en.wikipedia.org/wiki/Markov_logic_network"" rel=""nofollow noreferrer"">Markov Logic Networks</a> at the University of Washington and <a href=""http://dags.stanford.edu/PRMs/"" rel=""nofollow noreferrer"">Probabilstic Relational Models</a> at Stanford and other places. I'm a little out of touch with the field, but this is is a difficult problem and it's all early-stage research as far as I know. There are a lot of issues, mostly around efficient and scalable inference.</p>

<p>All in all, it's a good idea and a very sensible thing to want to do. However, it's also very difficult to achieve. If you want to look at a slick example of the state of the art, (i.e. what is possible with a bunch of people and money) maybe check out <a href=""http://www.powerset.com/"" rel=""nofollow noreferrer"">PowerSet</a>.</p>
",769,1278708436
Natural language query processing libraries,"<p>I am looking for Natural language query processing libraries to convert plain english query to sql like statements. For ex, show the list of employees whose age is 30 should be converted to select * from employees where age = 30.</p>

<p>Can you provide pointers/references?</p>

<p>Thanks,
Mani</p>
","java, nlp",,709,1277296313
How do you think the &quot;Quick Add&quot; feature in Google Calendar works?,"<p>Am thinking about a project which might use similar functionality to how ""Quick Add"" handles parsing natural language into something that can be understood with some level of semantics. I'm interested in understanding this better and wondered what your thoughts were on how this might be implemented.</p>

<hr>

<p>If you're unfamiliar with what ""Quick Add"" is, check out <a href=""http://www.google.com/support/calendar/bin/answer.py?hl=en&amp;answer=36604#text"" rel=""noreferrer"">Google's KB</a> about it.</p>

<hr>

<p><strong>6/4/10 Update</strong><br>
Additional research on ""Natural Language Parsing"" (NLP) yields results which are MUCH broader than what I feel is actually implemented in something like ""Quick Add"". Given that this feature expects specific types of input rather than the true free-form text, I'm thinking this is a much more narrow implementation of NLP. If anyone could suggest more narrow topic matter that I could research rather than the entire breadth of NLP, it would be greatly appreciated.</p>

<p>That said, I've found a nice <a href=""http://www.aaai.org/AITopics/pmwiki/pmwiki.php/AITopics/NaturalLanguage"" rel=""noreferrer"">collection of resources about NLP</a> including this great <a href=""http://www.faqs.org/faqs/natural-lang-processing-faq/"" rel=""noreferrer"">FAQ</a>.</p>
","parsing, nlp, google-calendar-api","<p>It would seem that there's really no narrow approach to this problem. I wanted to avoid having to pull along the entirety of NLP to figure out a solution, but I haven't found any alternative. I'll update this if I find a really great solution later.</p>
",1386,1275494875
Can we brainstorm for an automated tagging system?,"<p>I am interested to do automatic tagging for bodies of text. I am pretty new to NLP so I would like to hear some methods which you guys are familiar with in this context.</p>

<p>Any recommendations will be appreciated.</p>
",nlp,,213,1276804953
Ngram IDF smoothing,"<p>I am trying to use IDF scores to find interesting phrases in my pretty huge corpus of documents.<br>
I basically need something like Amazon's Statistically Improbable Phrases, i.e. phrases that distinguish a document from all the others<br>
The problem that I am running into is that some (3,4)-grams in my data which have super-high idf actually consist of component unigrams and bigrams which have really low idf..<br>
For example, ""you've never tried"" has a very high idf, while each of the component unigrams have very low idf..<br>
I need to come up with a function that can take in document frequencies of an n-gram and all its component (n-k)-grams and return a more meaningful measure of how much this phrase will distinguish the parent document from the rest.<br>
If I were dealing with probabilities, I would try interpolation or backoff models.. I am not sure what assumptions/intuitions those models leverage to perform well, and so how well they would do for IDF scores.<br>
Anybody has any better ideas?</p>
","machine-learning, nlp, information-retrieval, tf-idf",,1997,1276195678
"I have a list of names, some of them are fake, I need to use NLP and Python 3.1 to keep the real names and throw out the fake names","<p>I have no clue of where to start on this. I've never done any NLP and only programmed in Python 3.1, which I have to use. I'm looking at the site <a href=""http://www.linkedin.com"" rel=""noreferrer"">http://www.linkedin.com</a> and I have to gather all of the public profiles and some of them have very fake names, like 'aaaaaa k dudujjek' and I've been told I can use NLP to find the real names, where would I even start?</p>
","python-3.x, nlp","<p>This is a difficult problem to solve, and one which starts with acquiring valid given name &amp; surname lists.</p>

<p>How large is the set of names that you're evaluating, and where do they come from? These are both important things for you to consider. If you're evaluating a small set of ""American"" names, your valid name lists will differ greatly from lists of Japanese or Indian names, for instance.</p>

<p>Your idea of scraping LinkedIn is on the right track, but you were right to catch the fake profile/name flaw. A better website would probably be something like IMDB (perhaps scraping names <a href=""http://uk.imdb.com/search/name?birth_date=1946-01-01,1946-12-31"" rel=""nofollow noreferrer"">by iterating over different birth years</a>), or Wikipedia's lists of <a href=""http://en.wikipedia.org/wiki/List_of_most_popular_given_names"" rel=""nofollow noreferrer"">most popular given names</a> and <a href=""http://en.wikipedia.org/wiki/List_of_most_common_surnames"" rel=""nofollow noreferrer"">most common surnames</a>.</p>

<p>When it comes down to it, this is a precision vs. recall problem: in order to miss fewer fakes, you're inevitably going to throw out some real names. If you loosen up your restrictions, you'll get more fakes, but you'll also throw out fewer real names.</p>
",2184,1268023790
PyParsing: Not all tokens passed to setParseAction(),"<p>I'm parsing sentences like ""CS 2110 or INFO 3300"". I would like to output a format like:</p>

<pre><code>[[(""CS"" 2110)], [(""INFO"", 3300)]]
</code></pre>

<p>To do this, I thought I could use <code>setParseAction()</code>. However, the <code>print</code> statements in <code>statementParse()</code> suggest that only the last tokens are actually passed:</p>

<pre><code>&gt;&gt;&gt; statement.parseString(""CS 2110 or INFO 3300"")
Match [{Suppress:(""or"") Re:('[A-Z]{2,}') Re:('[0-9]{4}')}] at loc 7(1,8)
string CS 2110 or INFO 3300
loc: 7 
tokens: ['INFO', 3300]
Matched [{Suppress:(""or"") Re:('[A-Z]{2,}') Re:('[0-9]{4}')}] -&gt; ['INFO', 3300]
(['CS', 2110, 'INFO', 3300], {'Course': [(2110, 1), (3300, 3)], 'DeptCode': [('CS', 0), ('INFO', 2)]})
</code></pre>

<p>I expected all the tokens to be passed, but it's only <code>['INFO', 3300]</code>. Am I doing something wrong? Or is there another way that I can produce the desired output?</p>

<p>Here is the pyparsing code:</p>

<pre><code>from pyparsing import *

def statementParse(str, location, tokens):
    print ""string %s"" % str
    print ""loc: %s "" % location
    print ""tokens: %s"" % tokens

DEPT_CODE = Regex(r'[A-Z]{2,}').setResultsName(""DeptCode"")
COURSE_NUMBER = Regex(r'[0-9]{4}').setResultsName(""CourseNumber"")

OR_CONJ = Suppress(""or"")

COURSE_NUMBER.setParseAction(lambda s, l, toks : int(toks[0]))

course = DEPT_CODE + COURSE_NUMBER.setResultsName(""Course"")

statement = course + Optional(OR_CONJ + course).setParseAction(statementParse).setDebug()
</code></pre>
","python, parsing, nlp, pyparsing","<p>In order to keep the token bits from ""CS 2110"" and ""INFO 3300"", I suggest you wrap your definition of course in a Group:</p>

<pre><code>course = Group(DEPT_CODE + COURSE_NUMBER).setResultsName(""Course"")
</code></pre>

<p>It also looks like you are charging head-on at parsing out some kind of search expression, like ""x and y or z"".  There is some subtlety to this problem, and I suggest you check out some of the examples at the pyparsing wiki on how to build up these kinds of expressions.  Otherwise you will end up with a bird's nest of <code>Optional(""or"" + this)</code> and <code>ZeroOrMore(
""and"" + that)</code> pieces.  As a last-ditch, you may even just use something with <code>operatorPrecedence</code>, like:</p>

<pre><code>DEPT_CODE = Regex(r'[A-Z]{2,}').setResultsName(""DeptCode"")        
COURSE_NUMBER = Regex(r'[0-9]{4}').setResultsName(""CourseNumber"")
course = Group(DEPT_CODE + COURSE_NUMBER)

courseSearch = operatorPrecedence(course, 
    [
    (""not"", 1, opAssoc.RIGHT),
    (""and"", 2, opAssoc.LEFT),
    (""or"", 2, opAssoc.LEFT),
    ])
</code></pre>

<p>(You may have to download the latest 1.5.3 version from the SourceForge SVN for this to work.)</p>
",592,1275257018
PyParsing: What does Combine() do?,"<p>What is the difference between:</p>

<pre><code>foo = TOKEN1 + TOKEN2
</code></pre>

<p>and</p>

<pre><code>foo = Combine(TOKEN1 + TOKEN2)
</code></pre>

<p>Thanks. </p>

<p><strong>UPDATE</strong>: Based on my experimentation, it seems like <code>Combine()</code> is for terminals, where you're trying to build an expression to match on, whereas plain <code>+</code> is for non-terminals. But I'm not sure.</p>
","python, parsing, nlp, pyparsing","<p>Combine has 2 effects:</p>

<ul>
<li><p>it concatenates all the tokens into a single string</p></li>
<li><p>it requires the matching tokens to all be adjacent with no intervening whitespace</p></li>
</ul>

<p>If you create an expression like </p>

<pre><code>realnum = Word(nums) + ""."" + Word(nums)
</code></pre>

<p>Then <code>realnum.parseString(""3.14"")</code> will return a list of 3 tokens: the leading '3', the '.', and the trailing '14'.  But if you wrap this in Combine, as in:</p>

<pre><code>realnum = Combine(Word(nums) + ""."" + Word(nums))
</code></pre>

<p>then <code>realnum.parseString(""3.14"")</code> will return '3.14' (which you could then convert to a float using a parse action).  And since Combine suppresses pyparsing's default whitespace skipping between tokens, you won't accidentally find ""3.14"" in ""The answer is 3. 14 is the next answer.""</p>
",4241,1275256264
Python/PyParsing: Difficulty with setResultsName,"<p>I think I'm making a mistake in how I call <code>setResultsName()</code>:</p>

<pre><code>from pyparsing import *

DEPT_CODE = Regex(r'[A-Z]{2,}').setResultsName(""Dept Code"")
COURSE_NUMBER = Regex(r'[0-9]{4}').setResultsName(""Course Number"")

COURSE_NUMBER.setParseAction(lambda s, l, toks : int(toks[0]))

course = DEPT_CODE + COURSE_NUMBER

course.setResultsName(""course"")

statement = course
</code></pre>

<p>From IDLE:</p>

<pre><code>&gt;&gt;&gt; myparser import *
&gt;&gt;&gt; statement.parseString(""CS 2110"")
(['CS', 2110], {'Dept Code': [('CS', 0)], 'Course Number': [(2110, 1)]})
</code></pre>

<p>The output I hope for:</p>

<pre><code>&gt;&gt;&gt; myparser import *
&gt;&gt;&gt; statement.parseString(""CS 2110"")
(['CS', 2110], {'Course': ['CS', 2110], 'Dept Code': [('CS', 0)], 'Course Number': [(2110, 1)]})
</code></pre>

<p>Does <code>setResultsName()</code> only work for terminals?</p>
","python, nlp, pyparsing","<p>If you change the definition of <code>course</code> to </p>

<pre><code>course = (DEPT_CODE + COURSE_NUMBER).setResultsName(""Course"")
</code></pre>

<p>you get the following behavior:</p>

<pre><code>x=statement.parseString(""CS 2110"")
print(repr(x))
# (['CS', 2110], {'Course': [((['CS', 2110], {'Dept Code': [('CS', 0)], 'Course Number': [(2110, 1)]}), 0)], 'Dept Code': [('CS', 0)], 'Course Number': [(2110, 1)]})
print(x['Dept Code'])
# CS
print(x['Course Number'])
# 2110
print(x['Course'])
# ['CS', 2110]
</code></pre>

<p>That's not exactly the <code>repr</code> you wanted, but does it suffice?</p>

<p>Note, <a href=""http://crpppc19.epfl.ch/doc/python-pyparsing/htmldoc/pyparsing.pyparsing.ParserElement-class.html#setResultsName"" rel=""nofollow noreferrer"">from the docs</a>:</p>

<blockquote>
  <p>[setResultsName] returns a <em>copy</em> of
  the original ParserElement object;
  this is so that the client can define
  a basic element, such as an integer,
  and reference it in multiple places
  with different names.</p>
</blockquote>

<p>So <code>course.setResultsName(""Course"")</code> does not work because it doesn't affect <code>course</code>. You would instead have to say <code>course=course.setResultsName(""Course"")</code>. That's an alternative way to do what I did above.</p>
",1188,1275251041
Are there any well known algorithms to detect the presence of names?,"<p>For example, given a string:</p>

<p>""Bob went fishing with his friend Jim Smith.""</p>

<p>Bob and Jim Smith are both names, but bob and smith are both words. Weren't for them being uppercase, there would be less indication of this outside of our knowledge of the sentence. Are there any well known algorithms for detecting the presence of names, at least Western names?</p>
","algorithm, nlp, text-processing","<p>Take a look at Named Entity Recognition.</p>

<p><a href=""http://en.wikipedia.org/wiki/Named_entity_recognition"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Named_entity_recognition</a> The article links to two good implementations.</p>

<p>Im not sure if this falls under your definition of grammar analysis though.</p>
",277,1275183392
Sentiment analysis with NLTK python for sentences using sample data or webservice?,"<p>I am embarking upon a NLP project for sentiment analysis.</p>

<p>I have successfully installed NLTK for python (seems like a great piece of software for this). However,I am having trouble understanding how it can be used to accomplish my task.</p>

<p>Here is my task:</p>

<ol>
<li>I start with one long piece of data (lets say several hundred tweets on the subject of the UK election from their webservice)</li>
<li>I would like to break this up into sentences (or info no longer than 100 or so chars) (I guess i can just do this in python??)</li>
<li>Then to search through all the sentences for specific instances within that sentence e.g. ""David Cameron""</li>
<li>Then I would like to check for positive/negative sentiment in each sentence and count them accordingly</li>
</ol>

<p>NB: I am not really worried too much about accuracy because my data sets are large and also not worried too much about sarcasm. </p>

<p>Here are the troubles I am having:</p>

<ol>
<li><p>All the data sets I can find e.g. the corpus movie review data that comes with NLTK arent in webservice format. It looks like this has had some processing done already. As far as I can see the processing (by stanford) was done with WEKA. Is it not possible for NLTK to do all this on its own? Here all the data sets have already been organised into positive/negative already e.g. polarity dataset <a href=""http://www.cs.cornell.edu/People/pabo/movie-review-data/"" rel=""noreferrer"">http://www.cs.cornell.edu/People/pabo/movie-review-data/</a> How is this done? (to organise the sentences by sentiment, is it definitely WEKA? or something else?)</p></li>
<li><p>I am not sure I understand why WEKA and NLTK would be used together. Seems like they do much the same thing. If im processing the data with WEKA first to find sentiment why would I need NLTK? Is it possible to explain why this might be necessary?</p></li>
</ol>

<p>I have found a few scripts that get somewhat near this task, but all are using the same pre-processed data. Is it not possible to process this data myself to find sentiment in sentences rather than using the data samples given in the link?</p>

<p>Any help is much appreciated and will save me much hair!</p>

<p>Cheers Ke</p>
","nlp, nltk, weka, classification","<p>The movie review data has already been marked by humans as being positive or negative (the person who made the review gave the movie a rating which is used to determine polarity).  These <em>gold standard</em> labels allow you to train a classifier, which you could then use for other movie reviews.  You could train a classifier in NLTK with that data, but applying the results to election tweets might be less accurate than randomly guessing positive or negative.  Alternatively, you can go through and label a few thousand tweets yourself as positive or negative and use this as your training set.</p>

<p>For a description of using Naive Bayes for sentiment analysis with NLTK: <a href=""http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/"" rel=""noreferrer"">http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/</a></p>

<p>Then in that code, instead of using the movie corpus, use your own data to calculate word counts (in the <code>word_feats</code> method).</p>
",17077,1273820659
Algorithm for sentence analysis and tokenization,"<p>I need to analyze a document and compile statistics as to how many times each a sequence of words is used (so the analysis is not on single words but of batch of recurring words).  I read that compression algorithms do something similar to what I want - creating dictionaries of blocks of text with a piece of information reporting its frequency.
It should be something similar to <a href=""http://www.codeproject.com/KB/recipes/Patterns.aspx"" rel=""nofollow noreferrer"">http://www.codeproject.com/KB/recipes/Patterns.aspx</a>
Do you have anything written in C#?</p>
","c#, nlp",,1835,1275006421
Which OSS can extract a synopsis from a text?,"<p>Is there an OSS which can compress a text to a synopsis?</p>

<p>My goal is to build an editor for SciFi novels which can either automatically create a synopsizes for chapters or at least make a suggestion for one.</p>
","artificial-intelligence, nlp, text-processing","<p>I checked the comprehensive list <a href=""http://alias-i.com/lingpipe/web/competition.html"" rel=""nofollow noreferrer"">here</a>, and the <a href=""http://dragon.ischool.drexel.edu/default.asp"" rel=""nofollow noreferrer"">Dragon Toolkit</a> looked like one of the few to offer <a href=""http://dragon.ischool.drexel.edu/textsum.asp"" rel=""nofollow noreferrer"">this feature</a>. My experience is mainly with the commercial tools in this area.</p>
",104,1260348030
A StringToken Parser which gives Google Search style &quot;Did you mean:&quot; Suggestions,"<h2>Seeking a method to:</h2>

<h2>Take whitespace separated tokens in a String; return a suggested Word</h2>

<p><br>
<strong>ie:</strong><br>
Google Search can take <em>""fonetic wrd nterpreterr""</em>,<br>
and atop of the result page it shows <em>""Did you mean: phonetic word interpreter""</em></p>

<p><em>A solution in any of the C* languages or Java would be preferred.</em></p>

<p><br>
<strong>Are there any existing Open Libraries which perform such functionality?</strong></p>

<p><strong>Or is there a way to Utilise a Google API to request a suggested word?</strong></p>
","language-agnostic, parsing, nlp","<p>In his article <a href=""http://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">How to Write a Spelling Corrector</a>, Peter Norvig discusses how a Google-like spellchecker could be implemented. The article contains a 20-line implementation in Python, as well as links to several reimplementations in C, C++, C# and Java. Here is an excerpt:</p>

<blockquote>
  <p>The full details of an
  industrial-strength spell corrector
  like Google's would be more confusing
  than enlightening, but I figured that
  on the plane flight home, in less than
  a page of code, I could write a toy
  spelling corrector that achieves 80 or
  90% accuracy at a processing speed of
  at least 10 words per second.</p>
</blockquote>

<p>Using Norvig's code and <a href=""http://www.phon.ucl.ac.uk/home/johnm/ptlc2005/pdf/ptlcp56.pdf"" rel=""nofollow noreferrer"">this text</a> as training set, i get the following results:</p>

<pre><code>&gt;&gt;&gt; import spellch
&gt;&gt;&gt; [spellch.correct(w) for w in 'fonetic wrd nterpreterr'.split()]
['phonetic', 'word', 'interpreters']
</code></pre>
",1904,1222373983
Theory: &quot;Lexical Encoding&quot;,"<p><strong>I am using the term ""Lexical Encoding"" for my lack of a better one.</strong></p>

<p>A Word is arguably the fundamental unit of communication as opposed to a Letter.  Unicode tries to assign a numeric value to each Letter of all known Alphabets.  What is a Letter to one language, is a Glyph to another.  Unicode 5.1 assigns more than 100,000 values to these Glyphs currently.  Out of the approximately 180,000 Words being used in Modern English, it is said that with a vocabulary of about 2,000 Words, you should be able to converse in general terms. A ""Lexical Encoding"" would encode each Word not each Letter, and encapsulate them within a Sentence.</p>

<pre><code>// An simplified example of a ""Lexical Encoding""
String sentence = ""How are you today?"";
int[] sentence = { 93, 22, 14, 330, QUERY };
</code></pre>

<p>In this example each Token in the String was encoded as an Integer. The Encoding Scheme here simply assigned an int value based on generalised statistical ranking of word usage, and assigned a constant to the question mark.</p>

<p>Ultimately, a Word has both a Spelling &amp; Meaning though.  Any ""Lexical Encoding"" would preserve the meaning and intent of the Sentence as a whole, and not be language specific.  An English sentence would be encoded into <a href=""https://stackoverflow.com/questions/170452/linguistics-lexical-encoding#174249"">""...language-neutral atomic elements of meaning ...""</a> which could then be reconstituted into any language with a structured Syntactic Form and Grammatical Structure.</p>

<p>What are other examples of ""Lexical Encoding"" techniques?</p>

<hr>

<p>If you were interested in where the word-usage statistics come from :<br>
<a href=""http://www.wordcount.org"" rel=""nofollow noreferrer"">http://www.wordcount.org</a></p>
","encoding, theory, nlp, linguistics","<p>Their are several major problems with this idea. In most languages, the meaning of a word, and the word associated with a meaning change very swiftly.</p>

<p>No sooner would you have a number assigned to a word, before the meaning of the word would change. For instance, the word ""gay"" used to only mean ""happy"" or ""merry"", but it is now used mostly to mean homosexual. Another example is the morpheme ""thank you"" which originally came from German ""danke"" which is just one word. Yet another example is ""Good bye"" which is a shortening of ""God bless you"".</p>

<p>Another problem is that even if one takes a snapshot of a word at any point of time, the meaning and usage of the word would be under contention, even within the same province. When dictionaries are being written, it is not uncommon for the academics responsible to argue over a single word.</p>

<p>In short, you wouldn't be able to do it with an existing language. You would have to consider inventing a language of your own, for the purpose, or using a fairly static language that has already been invented, such as Interlingua or Esperanto. However, even these would not be perfect for the purpose of defining static morphemes in an ever-standard lexicon.</p>

<p>Even in Chinese, where there is rough mapping of character to meaning, it still would not work. Many characters change their meanings depending on both context, and which characters either precede or postfix them.</p>

<p>The problem is at its worst when you try and translate between languages. There may be one word in English, that can be used in various cases, but cannot be directly used in another language. An example of this is ""free"". In Spanish, either ""libre"" meaning ""free"" as in speech, or ""gratis"" meaning ""free"" as in beer can be used (and using the wrong word in place of ""free"" would look very funny).</p>

<p>There are other words which are even more difficult to place a meaning on, such as the word beautiful in Korean; when calling a girl beautiful, there would be several candidates for substitution; but when calling food beautiful, unless you mean the food is good looking, there are several other candidates which are completely different.</p>

<p>What it comes down to, is although we only use about 200k words in English, our vocabularies are actually larger in some aspects because we assign many different meanings to the same word. The same problems apply to Esperanto and Interlingua, and every other language meaningful for conversation. Human speech is not a well-defined, well oiled-machine. So, although you could create such a lexicon where each ""word"" had it's own unique meaning, it would be very difficult, and nigh on impossible for machines using current techniques to translate from any human language into your special standardised lexicon.</p>

<p>This is why machine translation still sucks, and will for a long time to come. If you can do better (and I hope you can) then you should probably consider doing it with some sort of scholarship and/or university/government funding, working towards a PHD; or simply make a heap of money, whatever keeps your ship steaming.</p>
",2498,1223131686
Building dictionary of words from large text,"<p>I have a text file containing posts in English/Italian. I would like to read the posts into a data matrix so that each row represents a post and each column a word. The cells in the matrix are the counts of how many times each word appears in the post. The dictionary should consist of all the words in the whole file or a non exhaustive English/Italian dictionary. </p>

<p>I know this is a common essential preprocessing step for NLP. And I know it's pretty trivial to code it, sill I'd like to use some NLP domain specific tool so I get stop-words trimmed etc..</p>

<p>Does anyone know of a tool\project that can perform this task?</p>

<p>Someone mentioned apache lucene, do you know if lucene index can be serialized to a data-structure similar to my needs?</p>
","lucene, nlp","<p>Thanks to @Mikos' comment, I googled the term ""term-document matrix' and found <a href=""http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page"" rel=""nofollow noreferrer"">TMG</a> (Text to Matrix Generator).</p>

<p>I found it suitable for my needs.</p>
",1630,1270583005
Keyword sorting algorithm,"<p>I have over 1000 surveys, many of which contains open-ended replies. </p>

<p>I would like to be able to 'parse' in all the words and get a ranking of the most used words (disregarding common words) to spot a trend.</p>

<p>How can I do this? Is there a program I can use?</p>

<p><strong>EDIT</strong> If a 3rd party solution is not available, it would be great if we can keep the discussion to microsoft technologies only. Cheers.</p>
","c#, sorting, nlp, data-mining, ranking","<p>Divide and conquer.  Split up your problem into many smaller problems and solve each of them.</p>

<p>First problem: turn a paragraph into a list of words.</p>

<p>You are fortunate because you don't have to worry about being perfect. Actually parsing natural languages to determine exactly what ""a word"" is can be very difficult, but frankly you probably don't really care whether ""lightbulb"" has the the same semantics as ""light bulb"". Since you are in particular looking for common words (for now, more on that later) the interesting ones are precisely those that <em>are</em> easy to identify because they come up a lot.</p>

<p>So, break this problem down further. You want a list of words. Start by getting a string with the text in it:</p>

<pre><code>StreamReader streamReader = new StreamReader(@""c:\survey.txt"");
string source = streamReader.ReadToEnd();
</code></pre>

<p>Great, you've got a string somehow. Now turn that into an array of words.  Because you probably want to count ""Frog"" and ""frog"" as the same word, make everything lowercase. How to do all that that? Split the lowercase string up based on spaces, newlines, tabs and punctuation:</p>

<pre><code>char[] punctuation = new char[] {' ', '\n', '\r', '\t', '(', ')', '""'};
string[] tokens = source.ToLower().Split(punctuation, true); 
</code></pre>

<p>Now examine the output. That was terrible. There's all kinds of stuff we forget. Periods and commas and colons and semicolons and so on. Figure out which punctuation you care about and add it to the list.</p>

<p>Is ToLower the right thing to do? What about ToLowerInvariant? There are times you want to stress about it; this isn't one of them. The fact that ToLower doesn't necessarily canoncialize the Turkish lowercase I in a manner that consistently round-trips is unlikely to throw off your summary statistics. We're not going for pinpoint accuracy here. If someone says ""luxury-yacht"", and someone says ""luxury yacht"", the former might be one word if you forget to break on hyphens. Who cares? Hyphenated words are unlikely to be in your top ten anyway.</p>

<p>Next problem: count all the occurrences of each word:</p>

<pre><code>var firstPass = new Dictionary&lt;string, int&gt;();
foreach(string token in tokens)
{
    if (!firstPass.ContainsKey(token))
        firstPass[token] = 1;
    else
        ++firstPass[token];
} 
</code></pre>

<p>Great. We now have a dictionary that maps words to integers. Trouble is, that's backwards. What you want to know is what are all the words that have the same number of occurrences. A dictionary is a sequence of key/value pairs, so group it:</p>

<pre><code>var groups = from pair in firstPass
             group pair.Key by pair.Value;
</code></pre>

<p>OK, now we have a sequence of groups of words, each one associated with its count of occurrences. Order it. Remember, the key of the group is the value of the dictionary, the count:</p>

<pre><code>var sorted = from group in groups
             orderby group.Key
             select group;
</code></pre>

<p>And you want the top hundred, let's say:</p>

<pre><code>foreach(var g in sorted.Take(100))
{
  Console.WriteLine(""Words with count {0}:"", g.Key);
  foreach(var w in g)
    Console.WriteLine(w);
}
</code></pre>

<p>And you're done.</p>

<p>Now, is this really what you're interested in?  I think it might be more interesting to look for <em>unusual</em> words, or pairs of words. If the words ""yacht"" and ""racing"" show up together a lot, not a surprise. If ""tomato"" and ""ketchup"" show up a lot together, not surprising. If ""tomato"" and ""racing"" start showing up together, then maybe something noteworthy is going on.</p>

<p>That requires much deeper analysis; read up on Bayes' Theorem if that's the sort of thing you're interested in.</p>

<p>Also note that this tracks the <em>raw count</em> of words, not their <em>frequency</em> -- the number of times they appear <em>per thousand words</em>. That might also be an interesting metric to measure: not just how many times did this word appear, period, but how many times did it appear as a percentage of the text.</p>
",849,1274364709
Java Stanford NLP: Spell checking,"<p>I'm trying to check spelling accuracy of text samples using the Stanford NLP. It's just a metric of the text, not a filter or anything, so if it's off by a bit it's fine, as long as the error is uniform.</p>

<p>My first idea was to check if the word is known by the lexicon:</p>

<pre><code>private static LexicalizedParser lp = new LexicalizedParser(""englishPCFG.ser.gz"");

@Analyze(weight=25, name=""Spelling"")
    public double spelling() {
        int result = 0;

        for (List&lt;? extends HasWord&gt; list : sentences) {
            for (HasWord w : list) {
                if (! lp.getLexicon().isKnown(w.word())) {
                    System.out.format(""misspelled: %s\n"", w.word());
                    result++;
                }
            }
        }

        return result / sentences.size();
    }
</code></pre>

<p>However, this produces quite a lot of false positives:</p>

<pre><code>misspelled: Sincerity
misspelled: Sisyphus
misspelled: Sisyphus
misspelled: fidelity
misspelled: negates
misspelled: gods
misspelled: henceforth
misspelled: atom
misspelled: flake
misspelled: Sisyphus
misspelled: Camus
misspelled: foandf
misspelled: foandf
misspelled: babby
misspelled: formd
misspelled: gurl
misspelled: pregnent
misspelled: babby
misspelled: formd
misspelled: gurl
misspelled: pregnent
misspelled: Camus
misspelled: Sincerity
misspelled: Sisyphus
misspelled: Sisyphus
misspelled: fidelity
misspelled: negates
misspelled: gods
misspelled: henceforth
misspelled: atom
misspelled: flake
misspelled: Sisyphus
</code></pre>

<p>Any ideas on how to do this better?</p>
","java, nlp, stanford-nlp, spell-checking","<p>Using the parser's lexicon's isKnown(String) method as a spellchecker isn't a viable use case of the parser.  The method is correct: ""false"" means that this word was not seen (with the given capitalization) in the approximately 1 million words of text the parser is trained from.  But 1 million words just isn't enough text to train a comprehensive spellchecker from in a data-driven manner. People would typically use at least two orders of magnitude of text more, and might well add some cleverness to handle capitalization.  The parser includes some of this cleverness to handle words that were unseen in the training data, but this isn't reflected in what the isKnown(String) method returns.</p>
",4572,1260045374
Java Stanford NLP: ArrayIndexOutOfBounds after loading second lexicon,"<p>I am using the Stanford Natural Language processing toolkit. I've been trying to find spelling errors with <code>Lexicon</code>'s <code>isKnown</code> method, but it produces quite a few false positives. So I thought I'd load a second lexicon, and check that too. However, that causes a problem.</p>

<pre><code>private static LexicalizedParser lp = new LexicalizedParser(Constants.stdLexFile);
private static LexicalizedParser wsjLexParse = new LexicalizedParser(Constants.wsjLexFile);

    static {
        lp.setOptionFlags(Constants.lexOptionFlags);        
        wsjLexParse.setOptionFlags(Constants.lexOptionFlags);       
    }

public ParseTree(String input) throws IllegalArgumentException, IllegalAccessException, InvocationTargetException {
    initialInput = input;
    DocumentPreprocessor process = new DocumentPreprocessor();
    sentences = process.getSentencesFromText(new StringReader(input));

    for (List&lt;? extends HasWord&gt; sent : sentences) {
        if(lp.parse(sent)) { // line 65
            forest.add(lp.getBestParse()); //non determinism?
        }
    }

    partsOfSpeech = pos();
    runAnalysis();
}
</code></pre>

<p>The following fail trace is produced:</p>

<pre><code>java.lang.ArrayIndexOutOfBoundsException: 45547
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.initRulesWithWord(BaseLexicon.java:300)
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.isKnown(BaseLexicon.java:160)
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.ruleIteratorByWord(BaseLexicon.java:212)
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.initializeChart(ExhaustivePCFGParser.java:1299)
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:388)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:234)
    at nth.compling.ParseTree.&lt;init&gt;(ParseTree.java:65)
    at nth.compling.ParseTreeTest.constructor(ParseTreeTest.java:33)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at org.junit.internal.runners.BeforeAndAfterRunner.invokeMethod(BeforeAndAfterRunner.java:74)
    at org.junit.internal.runners.BeforeAndAfterRunner.runBefores(BeforeAndAfterRunner.java:50)
    at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:33)
    at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
</code></pre>

<p>If I comment out this line: (and other references to wsjLexParse)</p>

<pre><code>private static LexicalizedParser wsjLexParse = new LexicalizedParser(Constants.wsjLexFile);
</code></pre>

<p>then everything works fine. What am I doing wrong here?</p>
","java, nlp, stanford-nlp",,474,1260233254
Is there a list of language only character regions for UTF-8 somewhere?,"<p>I'm trying to analyze some UTF-8 encoded documents in a way that recognizes different language characters. For my approach to work I need to ignore non-language characters, such as control characters, mathematical symbols etc. Just trying to dissect the basic Latin section of the UTF standard has resulted in multiple regions, with characters like the division symbol being right in the middle of a range of valid Latin characters.</p>

<p>Is there a list somewhere that identifies these regions? Or better yet, a Regex that defines the regions or something in C# that can identify the different characters?</p>
","utf-8, character-encoding, nlp","<p>Look at the Unicode <a href=""http://www.fileformat.info/info/unicode/category/index.htm"" rel=""nofollow noreferrer"">character categories</a>.  You can match these in C# regular expressions with the character class syntax <code>\p{catname}</code>.  So to match a lower-case letter, you would use <code>\p{Ll}</code>.  You can combine these.   <code>[\p{Ll}\p{Lu}]</code> matches characters in either the Ll or Lu class.</p>
",421,1274066136
Java or Python distributed compute job (on a student budget)?,"<p>I have a large dataset (c. 40G) that I want to use for some NLP (largely embarrassingly parallel) over a couple of computers in the lab, to which i do <em>not</em> have root access, and only 1G of user space.
I experimented with hadoop, but of course this was dead in the water-- the data is stored on an external usb hard drive, and i cant load it on to the dfs because of the 1G user space cap. 
I have been looking into a couple of python based options (as I'd rather use NLTK instead of Java's lingpipe if I can help it), and it seems distributed compute options look like:</p>

<ul>
<li>Ipython</li>
<li>DISCO</li>
</ul>

<p>After my hadoop experience, i am trying to make sure i try and make an informed choice -- any help on what might be more appropriate would be greatly appreciated.</p>

<p>Amazon's EC2 etc not really an option, as i have next to no budget.</p>
","java, python, nlp, hadoop, nltk",,347,1274020114
machine learning and code generator from strings,"<p>The problem: Given a set of hand categorized strings (or a set of ordered vectors of strings) generate a categorize function to categorize more input. In my case, that data (or most of it) is not natural language.</p>

<p>The question: are there any tools out there that will do that? I'm thinking of some kind of reasonably polished,  download, install and go kind of things, as opposed to to some library or a brittle academic program.</p>

<hr>

<p>(Please don't get stuck on details as the <em>real</em> details would restrict answers to less generally useful responses AND are under NDA.)</p>

<p>As an example of what I'm looking at; the input I'm wanting to filter is computer generated status strings pulled from logs. Error messages (as an example) being filtered based on who needs to be informed or what action needs to be taken.</p>
","code-generation, machine-learning, nlp, classification, decision-tree",,984,1273775840
"Given a document, select a relevant snippet","<p>When I ask a question here, the tool tips for the question returned by the auto search given the first little bit of the question, but a decent percentage of them don't give any text that is any more useful for understanding the question than the title. Does anyone have an idea about how to make a filter to trim out useless bits of a question?</p>

<p>My first idea is to trim any leading sentences that contain only words in some list (for instance, stop words, plus words from the title, plus words from the SO corpus that have very weak correlation with tags, that is that are equally likely to occur in any question regardless of it's tags)</p>
","statistics, nlp, text-processing, heuristics",,3267,1273775420
Splitting string on probable English word boundaries,"<p>I recently used Adobe Acrobat Pro's OCR feature to process a Japanese kanji dictionary.  The overall quality of the output is generally quite a bit better than I'd hoped, but word boundaries in the English portions of the text have often been lost.  For example, here's one line from my file:</p>

<pre><code>softening;weakening(ofthemarket)8 CHANGE [transform] oneselfINTO,takethe form of; disguise oneself
</code></pre>

<p>I could go around and insert the missing word boundaries everywhere, but this would be adding to what is already a substantial task.  I'm hoping that there might exist software which can analyze text like this, where some of the words run together, and split the text on probable word boundaries.  Is there such a package?</p>

<p>I'm using Emacs, so it'd be extra-sweet if the package in question were already an Emacs package or could be readily integrated into Emacs, so that I could simply put my cursor on a line like the above and repeatedly invoke some command that splits the line on word boundaries in decreasing order of probable correctness.</p>
",text-analysis,"<p>I couldn't find anything either, and ended up going with a more <a href=""http://groups.google.com/group/gnu.emacs.help/browse_thread/thread/1b1ff620d89ee6db/910452e11493c735?q=eefacm+emacs+ocr#910452e11493c735"" rel=""nofollow noreferrer"">interactive approach</a>.</p>
",237,1266085612
entity set expansion python,"<p>Do you know of any existing implementation in any language (preferably python) of any entity set expansion algorithms, such that the one from Google sets ? ( <a href=""http://labs.google.com/sets"" rel=""nofollow noreferrer"">http://labs.google.com/sets</a> )</p>

<p>I couldn't find any library implementing such algorithms and I'd like to play with some of those to see how they would perform on some specific task I would like to implement.</p>

<p>Any help is welcome !</p>

<p>Thanks a lot for your help,</p>

<p>Regards,</p>

<p>Nicolas.</p>
","java, python, nlp, information-retrieval","<p>I'm not aware of any ready to use open source libraries that implement the sort of <strong>clustering on demand</strong> of named entities provided by Google Sets. However, there are a few academic papers that describe in detail how to build similar systems, e.g.:</p>

<ul>
<li><p><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6493&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer""><strong>Language-Independent Set Expansion of Named Entities using the Web</strong></a> <br/>Wang and Cohen, in EMNLP 2009 <br/>
<a href=""http://www.boowa.com/"" rel=""nofollow noreferrer"">Online Demo</a> </p></li>
<li><p><a href=""http://www.gatsby.ucl.ac.uk/~heller/bsets.pdf"" rel=""nofollow noreferrer""><strong>Bayesian Sets</strong></a><br/> Ghahramani and Heller, in NIPS, 2005</p></li>
</ul>

<p>Below is a brief summary of Wang and Cohen's method. If you do end up implementing something like this yourself, it might be good to start with their method. I suspect most people will find it more intuitive than Ghahramani and Heller's formulation.</p>

<p><strong>Wang and Cohen 2009</strong></p>

<p>Wang and Cohen start by describing a method for automatically constructing <strong>extraction patterns</strong> that allow them to find lists of named entities in any sort of structured document. The method looks at the prefixes and suffixes bracketing known occurrences of named entities. These prefix and suffixes are then used to identify other named entities within the same document. </p>

<p>To complete a clusters of entities, they <strong>build a graph</strong> consisting of the interconnections between named entities, the extraction patterns associated with them, and the documents. Using this graph and starting at the nodes for the cluster's seed entities (i.e., the initial set of entities in the set to be completed), they perform numerous <strong>random walks</strong> on the graph up to 10 steps in length. They count how many times they reach the nodes corresponding to non-seed entities. Non-seed entities with high counts can then be used to complete the cluster.</p>
",521,1272926280
Stanford Parser - Traversing the typed dependencies graph,"<p>Basically I want to find a path between two NP tokens in the dependencies graph. However, I can't seem to find a good way to do this in the Stanford Parser. Any help?</p>

<p>Thank You Very Much</p>
","java, graph, nlp, stanford-nlp","<p>The Stanford Parser just returns a list of dependencies between word tokens.  (We do this to avoid external library dependencies.)  But if you want to manipulate the dependencies, you'll almost certainly want to put them in a graph data structure.  We usually use jgrapht: <a href=""http://jgrapht.sourceforge.net/"" rel=""noreferrer"">http://jgrapht.sourceforge.net/</a></p>
",5082,1272277083
How to estimate the quality of a web page?,"<p>I'm doing a university project, that must gather and combine data on a user provided topic. The problem I've encountered is that Google search results for many terms are polluted with low quality autogenerated pages and if I use them, I can end up with wrong facts. How is it possible to estimate the quality/trustworthiness of a page? </p>

<p>You may think ""nah, Google engineers are working on the problem for 10 years and he's asking for a solution"", but if you think about it, SE must provide up-to-date content and if it marks a good page as a bad one, users will be dissatisfied. I don't have such limitations, so if the algorithm accidentally marks as bad some good pages, that wouldn't be a problem.</p>

<p>Here's an example:
Say the input is <code>buy aspirin in south la</code>. Try to Google search it. The first 3 results are already deleted from the sites, but the fourth one is interesting: <code>radioteleginen.ning.com/profile/BuyASAAspirin</code> (I don't want to make an active link)</p>

<p>Here's the first paragraph of the text:</p>

<blockquote>
  <p>The bare of purchasing prescription drugs from Canada is big
  in the U.S. at this moment. This is
  because in the U.S. prescription drug
  prices bang skyrocketed making it
  arduous for those who bang limited or
  concentrated incomes to buy their much
  needed medications. Americans pay more
  for their drugs than anyone in the
  class.</p>
</blockquote>

<p>The rest of the text is similar and then the list of related keywords follows. This is what I think is a low quality page. While this particular text seems to make sense (except it's horrible), the other examples I've seen (yet can't find now) are just some rubbish, whose purpose is to get some users from Google and get banned 1 day after creation.</p>
","machine-learning, nlp, spam, information-retrieval","<p><strong>N-gram Language Models</strong></p>

<p>You could try training one <strong><a href=""http://en.wikipedia.org/wiki/N-gram"" rel=""nofollow noreferrer"">n-gram language model</a></strong> on the autogenerated spam pages and one on a collection of other non-spam webpages.</p>

<p>You could then simply score new pages with both language models to see if the text looks more similar to the spam webpages or regular web content. </p>

<p><strong>Better Scoring through Bayes Law</strong></p>

<p>When you score a text with the spam language model, you get an estimate of the probability of finding that text on a spam web page, <code>P(Text|Spam)</code>. The notation reads as the probability of <code>Text</code> given <code>Spam (page)</code>. The score from the non-spam language model is an estimate of the probability of finding the text on a non-spam web page, <code>P(Text|Non-Spam)</code>. </p>

<p>However, the term you probably really want is <code>P(Spam|Text)</code> or, equivalently <code>P(Non-Spam|Text)</code>. That is, you want to know <strong>the probability that a page is <code>Spam</code> or <code>Non-Spam</code> given the text that appears on it</strong>.</p>

<p>To get either of these, you'll need to use <a href=""http://en.wikipedia.org/wiki/Bayes%27_theorem"" rel=""nofollow noreferrer""><strong>Bayes Law</strong></a>, which states</p>

<pre><code>           P(B|A)P(A)
P(A|B) =  ------------
              P(B)
</code></pre>

<p>Using Bayes law, we have</p>

<pre><code>P(Spam|Text)=P(Text|Spam)P(Spam)/P(Text)
</code></pre>

<p>and</p>

<pre><code>P(Non-Spam|Text)=P(Text|Non-Spam)P(Non-Spam)/P(Text)
</code></pre>

<p><code>P(Spam)</code> is your <strong>prior belief</strong> that a page selected at random from the web is a spam page. You can estimate this quantity by counting how many spam web pages there are in some sample, or you can even use it as a parameter that you manually <strong>tune to trade-off <a href=""http://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow noreferrer"">precision and recall</a></strong>. For example, giving this parameter a high value will result in fewer spam pages being mistakenly classified as non-spam, while given it a low value will result in fewer non-spam pages being accidentally classified as spam.</p>

<p>The term <code>P(Text)</code> is the overall probability of finding <code>Text</code> on any webpage. If we ignore that <code>P(Text|Spam)</code> and <code>P(Text|Non-Spam)</code> were determined using different models, this can be calculated as <code>P(Text)=P(Text|Spam)P(Spam) + P(Text|Non-Spam)P(Non-Spam)</code>. This sums out the binary variable <code>Spam</code>/<code>Non-Spam</code>. </p>

<p><strong>Classification Only</strong></p>

<p>However, if you're not going to use the probabilities for anything else, you don't need to calculate <code>P(Text)</code>. Rather, you can just compare the numerators <code>P(Text|Spam)P(Spam)</code> and <code>P(Text|Non-Spam)P(Non-Spam)</code>. If the first one is bigger, the page is most likely a spam page, while if the second one is bigger the page is mostly likely non-spam. This works since the equations above for both <code>P(Spam|Text)</code> and <code>P(Non-Spam|Text)</code> are normalized by the <strong>same</strong> <code>P(Text)</code> value. </p>

<p><strong>Tools</strong></p>

<p>In terms of software toolkits you could use for something like this, <a href=""http://www-speech.sri.com/projects/srilm/download.html"" rel=""nofollow noreferrer"">SRILM</a> would be a good place to start and it's free for non-commercial use. If you want to use something commercially and you don't want to pay for a license, you could use <a href=""http://sourceforge.net/projects/irstlm/"" rel=""nofollow noreferrer"">IRST LM</a>, which is distributed under the LGPL.  </p>
",397,1272697271
"How to implement a SIMPLE &quot;You typed ACB, did you mean ABC?&quot;","<p><em>I know this is not a straight up question, so if you need me to provide more information about the scope of it, let me know. There are a bunch of questions that address almost the same issue (they are linked here), but never the exact same one with the same kind of scope and objective - at least as far as I know.</em> </p>

<p><strong>Context:</strong> </p>

<ul>
<li>I have a MP3 file with ID3 tags for
artist name and song title.</li>
<li>I have two tables Artists and Songs</li>
<li>The ID3 tags might be slightly off (e.g. Mikaell Jacksonne)</li>
<li>I'm using ASP.NET + C# and a MSSQL database</li>
</ul>

<p>I need to synchronize the MP3s with the database. Meaning:</p>

<ol>
<li>The user launches a script</li>
<li>The script browses through all the MP3s</li>
<li>The script says ""<em>Is 'Mikaell Jacksonne' 'Michael Jackson' YES/NO</em>""</li>
<li>The user pick and we start over</li>
</ol>

<p>Examples of what the system could find:</p>

<p><em>In the database...</em></p>

<pre><code>SONGS = {""This is a great song title"", ""This is a song title""}
ARTISTS = {""Michael Jackson""}
</code></pre>

<p><em>Outputs...</em></p>

<pre><code>""This is a grt song title"" did you mean ""This is a great song title"" ?
""This is song title"" did you mean ""This is a song title"" ?
""This si a song title""  did you mean ""This is a song title"" ?
""This si song a title""  did you mean ""This is a song title"" ?
""Jackson, Michael"" did you mean ""Michael Jackson"" ?
""JacksonMichael"" did you mean ""Michael Jackson"" ?
""Michael Jacksno"" did you mean ""Michael Jackson"" ?
</code></pre>

<p><em>etc.</em></p>

<p>I read some documentation from this <a href=""https://stackoverflow.com/questions/41424/how-do-you-implement-a-did-you-mean"">/how-do-you-implement-a-did-you-mean</a> and this is not exactly what I need since I don't want to check an entire dictionary. I also can't really use a web service since it's depending a lot on what I already have in my database. If possible I'd also like to avoid dealing with <a href=""http://en.wikipedia.org/wiki/Edit_distance"" rel=""nofollow noreferrer"">distances</a> and other <a href=""https://rads.stackoverflow.com/amzn/click/com/0262133601"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">complicated things</a>.</p>

<hr>

<p>I could use the <a href=""http://code.google.com/apis/soapsearch/reference.html#1_3"" rel=""nofollow noreferrer"">google api</a> (or something similar) to do this, meaning that the script will try spell checking and test it with the database, but I feel there could be a better solution since my database might end up being really specific with weird songs and artists, making spell checking useless.</p>

<p>I could also try something like what has been explained <a href=""https://stackoverflow.com/questions/39240/similar-posts-like-functionality-using-ms-sql-server"">on this post</a>, using <a href=""http://en.wikipedia.org/wiki/Soundex"" rel=""nofollow noreferrer"">Soundex</a> <a href=""http://www.csharphelp.com/archives2/archive394.html"" rel=""nofollow noreferrer"">for c#</a>.</p>

<p>Using a regular spell checker won't work because I won't be using words but names and 'titles'. </p>

<hr>

<p>So my question is: is there a <em>relatively</em> simple way of doing this, and if so, what is it?</p>

<p>Any kind of help would be appreciated.</p>

<p>Thanks!</p>
","nlp, spell-checking","<p>What you want is a similarity factor.  Essentially, you want to compare your inputs (""Micheal Jackson"", for example) to your expected values (""Michael Jackson""); if you score a very high similarity value to one of your expected values, you can ask the user.</p>

<p>One way of doing this is to hash the expected values into a fully packed hashtable.  If you get your hashing algorithm right (and yes, this is the tricky bit), each input will hash to the closest expected value; once you've found the closest expected value, you can run a similarity evaluation against the input and that expected value; if you're above a certain threshold, ask the user.</p>
",1016,1246988114
Java: remove-common-words-method in the API?,"<p><strong>Related:</strong></p>

<ol>
<li><a href=""http://www.velocityreviews.com/forums/t126618-remove-common-words-from-a-string.html"" rel=""nofollow noreferrer"">Forum post</a></li>
</ol>

<p>Before reinventing the wheel, I need to know whether such method exists. Stripping words according to a list such as <a href=""http://www.world-english.org/english500.htm"" rel=""nofollow noreferrer"">list</a> does not sound challenging but there are linguistic aspects, such as which words to stress the most in stripping, how about context?</p>
","java, parsing, nlp","<p>What it sounds like you are trying to do is remove the ""<a href=""http://en.wikipedia.org/wiki/Stop_words"" rel=""nofollow noreferrer"">stop words</a>"" from the text. You can find a <a href=""http://www.ranks.nl/resources/stopwords.html"" rel=""nofollow noreferrer"">list of English stopwords</a> at the link. Depending on how many stop words you use, it may be more efficient to create a <a href=""http://java.sun.com/j2se/1.5.0/docs/api/java/util/HashSet.html"" rel=""nofollow noreferrer"">HashSet</a> of words,so that you can tell whether a word is a stop-word in constant-time (by using the <a href=""http://java.sun.com/j2se/1.5.0/docs/api/java/util/HashSet.html#contains%28java.lang.Object%29"" rel=""nofollow noreferrer"">contains()</a> function), which would imply that filtering the entire text would take linear time in the number of words. This is such a simple operation that I doubt you will find some library to do it, but it shouldn't take long.</p>

<p>In terms of choosing which words to use... it really depends on what you are trying to do. If you are performing some sort of machine learning algorithm on the <a href=""http://en.wikipedia.org/wiki/Bag_of_words_model"" rel=""nofollow noreferrer"">bag of words model</a>, then you really have to try different selections of words and see which ones lead to the least validation error. In terms of the context, a lot of words really aren't needed. Anyone who speaks English well can tell you when you've dropped a ""the"" or ""a"" or ""an"". There may be common words that are important for certain disambiguation, but depending on your application, they may or may not be necessary. For example, if you want to know who did something, then eliminating ""he"", ""she"", etc. might be a problem, but if you only care about whether such-and-such an action occured and you don't really care who did it, then eliminating pronouns would be just fine.</p>
",1585,1272162238
Classification of relationships in words?,"<p>I'm not sure whats the best algorithm to use for the classification of relationships in words. For example in the case of a sentence such as ""The yellow sun"" there is a relationship between yellow and sun. THe machine learning techniques I have considered so far are Baynesian Statistics, Rough Sets, Fuzzy Logic, Hidden markov model and Artificial Neural Networks. </p>

<p>Any suggestions please?</p>

<p>thank you :)</p>
","machine-learning, nlp, linguistics",,1483,1268066070
Open Source Library for Linguistic Inquiry and Word Count (LIWC),"<p>I am looking for an open source library for Linguistic Inquiry and Word Count <a href=""http://liwc.net/index.php"" rel=""noreferrer"">(LIWC)</a>. Something in java or python will be good, though I am open to use other language.</p>

<p>Does anyone know where I can get one ?</p>

<p>Cheers,</p>
","java, python, open-source, nlp",,12274,1269469461
details on the following Natural Language Processing terms?,"<pre><code>Named Entity Extraction (extract ppl, cities, organizations)
Content Tagging (extract topic tags by scanning doc)
Structured Data Extraction
Topic Categorization (taxonomy classification by scanning doc....bayesian )
Text extraction (HTML page cleaning)
</code></pre>

<p>are there libraries that i can use to do any of the above functions of NLP ?</p>

<p>dont really feel like forking out cash to AlchemyAPI</p>
","nlp, libraries, text-processing","<p>There are actually plenty of freely available open-source natural language processing packages out there. Here's a brief list, organized by what language the toolkit is implemented in:</p>

<ul>
<li><strong>Python</strong>: <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">Natural Language Toolkit NLTK</a></li>
<li><strong>Java</strong>: <a href=""http://opennlp.sourceforge.net/"" rel=""nofollow noreferrer"">OpenNLP</a>, <a href=""http://gate.ac.uk/"" rel=""nofollow noreferrer"">Gate</a>, and <a href=""http://nlp.stanford.edu/software/"" rel=""nofollow noreferrer"">Stanford's JavaNLP</a></li>
<li><strong>.NET</strong>: <a href=""http://sharpnlp.codeplex.com/Wikipage"" rel=""nofollow noreferrer"">Sharp NLP</a></li>
</ul>

<p>If you're uncertain which one to go with, I would recommend starting with  <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">NLTK</a>. The package is reasonably easy to use and has great documentation online, including <a href=""http://www.nltk.org/book"" rel=""nofollow noreferrer"">a free book</a>.</p>

<p>You should be able to use NLTK to easily accomplish the NLP tasks you've listed, e.g. <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html#named-entity-recognition"" rel=""nofollow noreferrer"">named entity recognition (NER)</a>, <a href=""https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content"">extracting tags for documents</a>, and <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html#document-classification"" rel=""nofollow noreferrer"">document categorization</a>. </p>

<p>What the Alchemy people call <a href=""http://www.alchemyapi.com/api/scrape/"" rel=""nofollow noreferrer"">structured data extraction</a> looks like it's just HTML scrapping that is robust against changes to the underlying HTML as long as the page still visually renders the same way. So, it's not really a NLP task.</p>

<p>For the extraction of text from HTML, just use <a href=""http://code.google.com/p/boilerpipe/"" rel=""nofollow noreferrer"">boilerpipe</a>. It's fast, good, and free.</p>
",477,1271812946
Does knowing a Natural Language well help with Programming?,"<p>We all hear that math at least helps a little bit with programming. My question though, does English or other natural language skills help with programming? I know it has to help with technical documentation, but what about actual programming? Are certain constructs in a programming language also there in natural languages? Does knowing how to write a 20 page research paper help with writing a 20k loc programming project? </p>
","language-agnostic, nlp","<p>Dijkstra went so far as to say: ""Besides a mathematical inclination, an exceptionally good mastery of one's native tongue is the most vital asset of a competent programmer.""</p>

<p>Edit: yes, I'm reasonably certain he was talking about the programming part of the job. Here's a bit more complete quote:</p>

<p>The problems of business administration in general and database management in particular are much too difficult for people who think in IBMerese, compounded by sloppy English.</p>

<p>About the use of language: it is impossible to sharpen a pencil with a blunt axe. It is equally vain to try to do it with ten blunt axes instead.</p>

<p>Besides a mathematical inclination, an exceptionally good mastery of one's native tongue is the most vital asset of a competent programmer.</p>

<p>From <a href=""http://userweb.cs.utexas.edu/users/EWD/ewd04xx/EWD498.PDF"" rel=""nofollow noreferrer"">EWD498</a>.</p>

<p>I certainly can't speak for Dijkstra, but I think it's impossible to cleanly separate the part where you're doing actual programming from the part where you're interacting with people. Just for example, even when you're working alone, it's crucial that you're able to understand (clearly and unambiguously) notes you wrote down about what to do, the nature of a bug, etc. A good command of English is necessary even when nobody else is involved at all (and, of course, that's unusual except on trivial tasks).</p>
",323,1271535287
Generating easy-to-remember random identifiers,"<p>As all developers do, we constantly deal with some kind of identifiers as part of our daily work. Most of the time, it's about bugs or support tickets. Our software, upon detecting a bug, creates a package that has a name formatted from a timestamp and a version number, which is a cheap way of creating reasonably unique identifiers to avoid mixing packages up. Example: ""<em>Bug Report 20101214 174856 6.4b2</em>"".</p>

<p>My brain just isn't that good at remembering numbers. What I would love to have is a simple way of <strong>generating alpha-numeric identifiers that are easy to remember</strong>.</p>

<p>It takes about 5 minutes to whip up an algorithm like the following in python, which produces halfway usable results:</p>

<pre><code>import random

vowels = 'aeiuy' # 0 is confusing
consonants = 'bcdfghjklmnpqrstvwxz'
numbers = '0123456789'

random.seed()

for i in range(30):
    chars = list()
    chars.append(random.choice(consonants))
    chars.append(random.choice(vowels))
    chars.append(random.choice(consonants + numbers))
    chars.append(random.choice(vowels))
    chars.append(random.choice(vowels))
    chars.append(random.choice(consonants))
    print ''.join(chars)
</code></pre>

<p>The results look like this:</p>

<pre><code>re1ean
meseux
le1ayl
kuteef
neluaq
tyliyd
ki5ias
</code></pre>

<p>This is already quite good, but I feel it is still easy to forget how they are spelled exactly, so that if you walk over to a colleagues desk and want to look one of those up, there's still potential for difficulty.</p>

<p>I know of algorithms that perform trigram analysis on text (say you feed them a whole book in German) and that can generate strings that look and feel like German words and are thus easier to handle generally. This requires lots of data, though, and makes it slightly less suitable for embedding in an application just for this purpose.</p>

<p>Do you know of any published algorithms that solve this problem?</p>

<p>Thanks!</p>

<p>Carl</p>
","random, nlp, mnemonics",,651,1271419713
are there any c# libraries for Named Entity Recognition?,"<p>I am looking for any free libraries for Named Entity Recognition in c# or any other .net language.</p>
","c#, dll, nlp","<p><a href=""http://sharpnlp.codeplex.com/"" rel=""nofollow noreferrer"">SharpNLP</a>, a port of the Java based OpenNLP, supports named entity tagging.</p>
",3761,1270581931
Constructing human readable sentences based on a survey,"<p>The following is a survey given to course attendees to assess an instructor at the end of the course.</p>

<pre><code>Communication Skills
1. The instructor communicated course material clearly and accurately.
Yes No
2. The instructor explained course objectives and learning outcomes.
Yes No
3. In the event of not understanding course materials the instructor was available outside of class.
Yes No
4. Was instructor feedback and grading process clear and helpful?
Yes No
5. Do you feel that your oral and written skills have improved while in this course?
Yes No
</code></pre>

<p>We would like to summarize each attendees selection based on the choices chosen by him.</p>

<p>If the provided answers were [No, No, Yes, Yes, Yes]. Then we would summarize this as ""The instructor was not able to summarize course objectives and learning outcomes clearly, but was available and usually helpful outside of class. The instructor feedback and grading process was clear and helpful and I feel that my oral and written skills have improved because of this course.</p>

<p>Based on the selections chosen by the attendee the summary would be quite different. This leads to many answers based on the choices selected and the number of such questions in the survey. The questions are usually provided by the training organization. How do you come up with a generic solution so that this can be effectively translated into a human readable form. I am looking for tools or libraries (java based), suggestions which will help me create such human readable output. I would like to hide the complexity from the end users as much as possible.</p>
","java, parsing, nlp, semantics",,434,1269668783
Dictionary of English Words for a J2ME app,"<p>I intend to develop a J2ME application, that should be able to read words from the English Dictionary. How do I interface to/and store a Dictionary ? Will I have to create the Dictionary myself, by inserting words, or is there a third party Dictionary available with APIs?</p>
","dictionary, nlp, java-me","<p>There are definitely free dictionary files, so you don't need to make one yourself!</p>

<p>A good starting point is looking at a Java SE implementation of a spell checker, such as <a href=""http://sourceforge.net/projects/jazzy/"" rel=""nofollow noreferrer"">Jazzy on Sourceforge</a>. It has dictionary files included in the project, and by inspecting the code you can get an idea of how to integrate it into your own J2ME app.</p>

<p>Your major challenge will be to be able to search the dictionary in an efficient manner using J2ME since it can be slow in comparison to its desktop coutnerpart.</p>
",5603,1270331411
Searching text for geonames,"<p>which part of huge package nltk I must study and use, if I need mark geonames in text?</p>
","python, nlp, nltk","<p>You'll want to use their named entity recognizer <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html#named-entity-recognition"" rel=""nofollow noreferrer"">nltk.ne_chunk</a>. </p>

<p>Once the text is tagged you'll want to look for phrases labeled LOC (location) and GPE (Geo-political Entity).</p>
",600,1270239590
Hierarchy of meaning,"<p>I am looking for a method to build a hierarchy of words. </p>

<p>Background: I am a ""amateur"" natural language processing enthusiast and right now one of the problems that I am interested in is determining the hierarchy of word semantics from a group of words.</p>

<p>For example, if I have the set which contains a ""super"" representation of others, i.e.</p>

<pre><code>[cat, dog, monkey, animal, bird, ... ]
</code></pre>

<p>I am interested to use any technique which would allow me to extract the word 'animal' which has the most meaningful and accurate representation of the other words inside this set.</p>

<p>Note: they are NOT the same in meaning. cat != dog != monkey != animal
BUT cat is a subset of animal and dog is a subset of animal.</p>

<p>I know by now a lot of you will be telling me to use wordnet. Well, I will try to but I am actually interested in doing a very domain specific area which WordNet doesn't apply because:
1) Most words are not found in Wordnet
2) All the words are in another language; translation is possible but is to limited effect.</p>

<p>another example would be:</p>

<pre><code>[ noise reduction, focal length, flash, functionality, .. ]
</code></pre>

<p>so functionality includes everything in this set. </p>

<p>I have also tried crawling wikipedia pages and applying some techniques on td-idf etc but wikipedia pages doesn't really do much either.</p>

<p>Can someone possibly enlighten me as to what direction my research should go towards? (I could use anything)</p>
","machine-learning, nlp, wordnet",,623,1269449367
Defining the context of a word - Python,"<p>I think this is an interesting question, at least for me.</p>

<hr>

<p>I have a <strong>list of words</strong>, let's say: </p>

<blockquote>
  <p>photo, free, search, image, css3, css, tutorials, webdesign, tutorial, google, china, censorship, politics, internet</p>
</blockquote>

<p>and I have a <strong>list of contexts</strong>:</p>

<ul>
<li>Programming</li>
<li>World news</li>
<li>Technology</li>
<li>Web Design</li>
</ul>

<hr>

<p>I need to try and match words with the appropriate context/contexts if possible.</p>

<p>Maybe discovering word relationships in some way.</p>

<p><img src=""https://i.sstatic.net/44UvG.png"" alt=""alt text""></p>

<hr>

<p>Any ideas?</p>

<p>Help would be much appreciated!</p>
","python, django, dictionary, nlp",,1437,1269355050
String chunking algorithm with natural language context,"<p>I have a arbitrarily large string of text from the user that needs to be split into 10k chunks (potentially adjustable value) and sent off to another system for processing. </p>

<ul>
<li>Chunks cannot be longer than 10k (or other arbitrary value)</li>
<li>Text should be broken with natural language context in mind 

<ul>
<li>split on punctuation when possible</li>
<li>split on spaces if no punction exists</li>
<li>break a word as a last resort</li>
</ul></li>
</ul>

<p>I'm trying not to re-invent the wheel with this, any suggestions before I roll this from scratch?</p>

<p>Using C#.</p>
","c#, string, nlp","<p>This may not handle every case as you need, but it should get you on your way.</p>

<pre><code>    public IList&lt;string&gt; ChunkifyText(string bigString, int maxSize, char[] punctuation)
    {
        List&lt;string&gt; results = new List&lt;string&gt;();

        string chunk;
        int startIndex = 0;

        while (startIndex &lt; bigString.Length)
        {
            if (startIndex + maxSize + 1 &gt; bigString.Length)
                chunk = bigString.Substring(startIndex);
            else
                chunk = bigString.Substring(startIndex, maxSize);

            int endIndex = chunk.LastIndexOfAny(punctuation);

            if (endIndex &lt; 0)
                endIndex = chunk.LastIndexOf("" "");

            if (endIndex &lt; 0)
                endIndex = Math.Min(maxSize - 1, chunk.Length - 1);

            results.Add(chunk.Substring(0, endIndex + 1));

            startIndex += endIndex + 1;
        }

        return results;
    }
</code></pre>
",596,1269283070
How to make concept representation with the help of bag of words,"<p>Thanks for stoping to read my question :) this is very sweet place full of GREAT peoples !</p>

<p>I have a question about ""creating sentences with words"". NO NO it is not about english grammar :)</p>

<p>Let me explain, If I have bag of words like</p>

<pre><code>""person apple apple person person a eat person will apple eat hungry apple hungry""
</code></pre>

<p>and it can generate some kind of following sentence</p>

<pre><code>""hungry person eat apple""
</code></pre>

<p>I don't in which field this topic will relate. Where should I try to find an answer. I tried to search google but I only found english grammar stuff :)</p>

<p>Any body there who can tell me which algo can work in this problem? or any program </p>

<p>Thanks</p>

<p>P.S: It is not an assignment :) if it would be i would ask for source code ! I don't even know in which field I should look for :)</p>
","java, algorithm, nlp, linguistics",,905,1267672855
Dependency parsing,"<p>I particularly like the transduce feature offered by agfl in their EP4IR 
<a href=""http://www.agfl.cs.ru.nl/EP4IR/english.html"" rel=""nofollow noreferrer"">http://www.agfl.cs.ru.nl/EP4IR/english.html</a></p>

<p>The download page is here:
<a href=""http://www.agfl.cs.ru.nl/download.html"" rel=""nofollow noreferrer"">http://www.agfl.cs.ru.nl/download.html</a></p>

<p>Is there any way i can make use of this in a c# program? Do I need to convert classes to c#?</p>

<p>Thanks
:)</p>
","c#, nlp, dependencies, tree-grammar",,1046,1268261802
TDD and the Bayesian Spam Filter problem,"<p>It's well known that Bayesian classifiers are an effective way to filter spam. These can be fairly concise (our one is only a few hundred LoC) but all core code needs to be written up-front before you get any results at all.</p>

<p>However, the TDD approach mandates that only the minimum amount of code to pass a test can be written, so given the following method signature:</p>

<pre><code>bool IsSpam(string text)
</code></pre>

<p>And the following string of text, which is clearly spam:</p>

<pre><code>""Cheap generic viagra""
</code></pre>

<p>The minimum amount of code I could write is:</p>

<pre><code>bool IsSpam(string text)
{
    return text == ""Cheap generic viagra""
}
</code></pre>

<p>Now maybe I add another test message, e.g.</p>

<pre><code>""Online viagra pharmacy""
</code></pre>

<p>I could change the code to:</p>

<pre><code>bool IsSpam(string text)
{
    return text.Contains(""viagra"");
}
</code></pre>

<p>...and so on, and so on. Until at some point the code becomes a mess of string checks, regular expressions, etc. because we've <em>evolved</em> it instead of thinking about it and writing it in a different way from the start.</p>

<p>So how is TDD supposed to work with this type of situation where evolving the code from the simplest possible code to pass the test is not the right approach? (Particularly if it is known in advance that the best implementations cannot be trivially evolved).</p>
","tdd, machine-learning, nlp, classification",,730,1240217117
How can I create my own corpus in the Python Natural Language Toolkit?,"<p>I have recently expanded the names corpus in nltk and would like to know how I can turn the two files I have (male.txt, female.txt) in to a corpus so I can access them using the existing nltk.corpus methods. Does anyone have any suggestions?</p>

<p>Many thanks,
James.</p>
","python, nlp, nltk",,7344,1264873725
python - syntax error,"<p>Hi:) I am not able to figure out what the error in the program is could you please help me out with it. Thank you..:)</p>

<p>The input file contains the following:</p>

<pre><code>3.  भारत का इतिहास काफी समृद्ध एवं विस्तृत है।
57. जैसे आज के झारखंड प्रदेश से, उन दिनों, बहुत से लोग चाय बागानों में मजदूरी करने के उद्देश्य से असम आए।
</code></pre>

<p>( its basically sample sentences for which i need to get word positions in the output appended to each word in hindi)</p>

<p>for e.g the output for the first sentence would look like this:</p>

<pre><code>3.  भारत(1) का(2) इतिहास(3) काफी(4) समृद्ध(5) एवं(6) विस्तृत(7) है(8) ।(9)
</code></pre>

<p>I should get a similar op for the following sentence(s)</p>

<p>The code looks like this:</p>

<pre><code>#!/usr/bin/python
# -*- coding: UTF-8 -*-
# encoding: utf-8
separators = [u'।', ',', '.']
text = open(""hinstest1.txt"").read()
#This converts the encoded text to an internal unicode object, where
# all characters are properly recognized as an entity:
text = text.decode(""UTF-8"")
#this breaks the text on the white spaces, yielding a list of words:
words = text.split()

counter = 1

output = """"
#if the last char is a separator, and is joined to the word:
for word in words:
    if word[-1] in separators and len(word) &gt; 1:
        #word up to the second to last char:
        output += word[:-1] + u'(%d) ' % counter
        counter += 1
        #last char
        output += word[-1] +  u'(%d) ' % counter
    else:
        output += word + u'(%d) ' % counter
        counter += 1

    print output
</code></pre>

<p>The error I am getting is:</p>

<pre><code>  File ""pyth_hinwp.py"", line 22
    output += word[-1] +  u'(%d) ' % counter
                         ^
SyntaxError: invalid syntax
</code></pre>

<p>I know this question is something similar to what I have asked earlier, but since I am not able to successfully execute some of the answers given to me earlier hence I am kinda restructuring the question to the place where I am currently getting stuck.</p>
","python, nlp","<p>What is posted here does not have the error. Note that what is posted has TWO space characters between the + and the u in <code>output += word[-1] +  u'(%d) ' % counter</code>. What is probably happening is that you have a whitespace character other than a space in there. A possibility is NBSP (U+00A0) aka ""no-break space"". What SO does to format your code is likely to scrub away such things.</p>

<p>Diagnosis: At the Python interactive prompt, type</p>

<pre><code>open(""pyth_hinwp.py"").readlines()[22-1]
</code></pre>

<p>What do you see between the + and the u?</p>

<p>Fix: in your editor, delete both characters between the + and the u. Insert a single space.</p>

<p>By the way, with a syntax error, the problem is entirely within the named SOURCE file; the code has not been run (because it couldn't be compiled) and so what is in your INPUT file has no bearing on the problem.</p>
",245,1266644529
Build a natural language model that fixes misspellings,"<p>What are books about how to build a natural language parsing program like this:</p>

<pre>
input: I got to TALL you
output: I got to TELL you

input: Big RAT box
output: Big RED box

in: hoo un thum zend three
out: one thousand three

</pre>

<p>It must have the language model that allows to predict what words are misspelled !</p>

<p>What are the best books on how to build such a tool??</p>

<p>p.s. Are there free webservices to spell-check? From Google maybe?..</p>
","java, parsing, nlp, linguistics","<p>Peter Norvig has written a terrific <a href=""http://norvig.com/spell-correct.html"" rel=""noreferrer"">spell checker</a>.  Maybe that can help you.</p>
",1310,1265806435
Python - letter frequency count and translation,"<p>I am using Python 3.1, but I can downgrade if needed.</p>

<p>I have an ASCII file containing a short story written in one of the languages the alphabet of which can be represented with upper and or lower ASCII. I wish to:</p>

<p>1) Detect an encoding to the best of my abilities, get some sort of confidence metric (would vary depending on the length of the file, right?)</p>

<p>2) Automatically translate the whole thing using some free online service or a library.</p>

<p>Additional question: What if the text is written in a language where it takes 2 or more bytes to represent one letter and the byte order mark is not there to help me?</p>

<p>Finally, how do I deal with punctuation and misc characters such as space? It will occur more frequently than some letters, right? How about the fact that punctuation and characters can be sometimes mixed - there might be two representations of a comma, two representations for what looks like an ""a"", etc.?</p>

<p>Yes, I have read <a href=""http://www.joelonsoftware.com/articles/Unicode.html"" rel=""nofollow noreferrer"">the article by Joel Spolsky on Unicode</a>. Please help me with at least some of these items.</p>

<p>Thank you!</p>

<p>P.S. This is not a homework, but it is for self-educational purposes. I prefer using a letter frequency library that is open-source and readable as opposed to the one that is closed, efficient, but gets the job done well.</p>
","python, character-encoding, translation, nlp","<p>Essentially there are three main tasks to implement the described application:</p>

<ul>
<li>1a) Identify the character encoding of the input text</li>
<li>1b) Identify the language of the input text</li>
<li>2) Get the text translated the text, by way of one of the online services' API</li>
</ul>

<p>For 1a, you may want to take a look at <a href=""http://gizmojo.org/code/decodeh/"" rel=""nofollow noreferrer""><strong>decodeh.py</strong></a>, aside from the script itself, it provides many very useful resources regarding character sets and encoding at large. <strong>CharDet</strong>, mentioned in other answer also seems to be worthy of consideration.</p>

<p>Once the character encoding is known, as you suggest, you may solve 1b) by calculating the character frequency profile of the text, and matching it with known frequencies.  While simple, this approach typically provide a decent precision ratio, although it may be weak on shorter texts and also on texts which follow particular patterns; for example a text in French with many references to units in the metric system will have an unusually high proportion of the letters M, K and C.</p>

<p>A complementary and very similar approach, use bi-grams (sequences of two letters) and tri-grams (three letters) and the corresponding tables of frequency distribution references in various languages.</p>

<p>Other language detection methods involve tokenizing the text, i.e. considering the words within the text.  NLP resources include tables with the most used words in various languages. Such words are typically articles, possessive adjectives, adverbs and the like.</p>

<p>An alternative solution to the language detection is to rely on the online translation service to figure this out for us.  What is important is to supply the translation service with text in a character encoding it understands, providing it the language may be superfluous.</p>

<p>Finally, as many practical NLP applications, you may decide to implement multiple solutions.  By using a strategy design pattern, one can apply several filters/classifiers/steps in a particular order, and exit this logic at different points depending on the situation.  For example, if a simple character/bigram frequency matches the text to English (with a small deviation), one may just stop there.  Otherwise, if the guessed language is French or German, perform another test, etc. etc.</p>
",3582,1265758665
Elegant command-parsing in an OOP-based text game,"<p>I'm playing with writing a MUD/text adventure (please don't laugh) in Ruby.  Can anyone give me any pointers towards   an elegant, oop-based solution to parsing input text?  </p>

<p>We're talking about nothing more complex than ""put wand on table"", here.  But everything needs to be soft; I want to extend the command set painlessly, later.  </p>

<p>My current thoughts, slightly simplified:  </p>

<ol>
<li><p>Each item class (box, table, room, player) knows how to recognise a command that  'belongs' to it.</p></li>
<li><p>The game class understands a sort of a domain-specific language involving actions such as ""move object X inside object Y"", ""show description of object X"", etc.  </p></li>
<li><p>The game class asks each item in the room if it recognises the input command.  First to say yes wins.  </p></li>
<li><p>It then passes control to a method in the item class that handles the command.  This method rephrases the command in the DSL, passes it back to the game object to make it happen.</p></li>
</ol>

<p>There must be well-worn, elegant ways of doing this stuff.  Can't seem to google anything, though.</p>
","ruby, language-agnostic, oop, nlp","<p>The <a href=""http://en.wikipedia.org/wiki/Interpreter_pattern"" rel=""nofollow noreferrer"">Interpreter design pattern</a> is the most <em>object-oriented</em> approach to parsing that I'm aware of, but I'm sure compiler experts will point out algorithms that are more powerful.</p>
",2292,1265373438
Am I passing the string correctly to the python library?,"<p>I'm using a python library called Guess Language: <a href=""http://pypi.python.org/pypi/guess-language/0.1"" rel=""nofollow noreferrer"">http://pypi.python.org/pypi/guess-language/0.1</a></p>

<p>""justwords"" is a string with unicode text. I stick it in the package, but it always returns English, even though the web page is in Japanese. Does anyone know why? Am I not encoding correctly?</p>

<pre><code>§ç©ºéå
¶ä»æ¡å°±æ²æéç¨®å¾                                é¤ï¼æä»¥ä¾éè£¡ç¶ç
éäºï¼åæ­¤ç°å¢æ°£æ°¹³åèµ·ä¾åªè½ç®âå¾å¥½âéå¸¸å¥½âåå                 ¶æ¯è¦é»é¤ï¼é¨ä¾¿é»çé»ãé£²æãä¸ææ²»ç­åä¸å                                     ä¾¿å®ï¼æ¯æ´è¥ç   äºï¼æ³æ³éè£¡ä»¥å°é»ãæ¯è§ä¾èªªä¹è©²æpremiumï¼åªæ±é¤é»å¥½åå°±å¥½äºã&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;é¦åç¾ï¼æä»¥å°±é»åå®æ´ç         æ­£è¦åä¸ä¸å
ä¸ç                           å¥é¤å§ï¼å



justwords = justwords.encode('utf-8')
true_lang =  str(guess_language.guessLanguage(justwords))
print true_lang
</code></pre>

<p>Edit: THanks guys for your help. This is an update of the problem.</p>

<p>I am trying to ""guess"" the language of this: <strong><a href=""http://feeds.feedburner.com/nchild"" rel=""nofollow noreferrer"">http://feeds.feedburner.com/nchild</a></strong></p>

<p>Basically, in Python, I get the htmlSource. Then, I strip the tags using BeautifulSoup.  Then, I pass it to the library to get the language. If I do not do encode('utf-8'), then ASCII-errors will come up. So , this is a must.</p>

<pre><code>soup = BeautifulStoneSoup(htmlSource)
justwords = ''.join(soup.findAll(text=True))
justwords = justwords.encode('utf-8')
true_lang =  str(guess_language.guessLanguage(justwords))
</code></pre>
","python, unicode, encoding, nlp","<p>Looking at the main page, it says """"""Detects over 60 languages; Greek (el), Korean (ko), Japanese (ja), Chinese (zh) and all the languages listed in the trigrams directory. """"""</p>

<p>It doesn't use trigrams for those 4 languages; it relies on what script blocks are present in the input text. Looking at the source code:</p>

<pre><code>if ""Katakana"" in scripts or ""Hiragana"" in scripts or ""Katakana Phonetic Extensions"" in scripts:
    return ""ja""

if ""CJK Unified Ideographs"" in scripts or ""Bopomofo"" in scripts \
        or ""Bopomofo Extended"" in scripts or ""KangXi Radicals"" in scripts:
    return ""zh""
</code></pre>

<p>For a script name like Katakana or Hiragana to appear in <code>scripts</code>, such characters must comprise 40% or more of the input text (after normalisation which removes non-alphabetic characters etc). It may be possible that some Japanese text needs a threshold of less than 40%. HOWEVER if that was the problem with your text, I would expect it to have more than 40% kanji (CJK Unified Ideographs) and thus should return ""zh"" (Chinese).</p>

<p><strong>Update</strong> after some experimentation, including inserting a print statement to show what script blocks were detected with what percentages:</p>

<p>A presumably typical news item from the Asahi newspaper website:</p>

<pre><code> 49.3 Hiragana
  8.7 Katakana
 42.0 CJK Unified Ideographs
result ja
</code></pre>

<p>A presumably atypical ditto:</p>

<pre><code> 35.9 Hiragana
 49.2 CJK Unified Ideographs
 13.3 Katakana
  1.6 Halfwidth and Fullwidth Forms
result zh
</code></pre>

<p>(Looks like it might be a good idea to base the test on the total (Hiragana + Katakana) content)</p>

<p>Result of shoving the raw front page (XML, HTML, everything) through the machinery:</p>

<pre><code>  2.4 Hiragana
  6.1 CJK Unified Ideographs
  0.1 Halfwidth and Fullwidth Forms
  3.7 Katakana
 87.7 Basic Latin
result ca
</code></pre>

<p>The high percentage of Basic Latin is of course due to the markup. I haven't investigated what made it choose ""ca"" (Catalan) over any other language which uses Basic Latin, including English. However the gobbledegook that you printed doesn't show any sign of including markup.</p>

<p><strong>End of update</strong></p>

<p><strong>Update 2</strong></p>

<p>Here's an example (2 headlines and next 4 paragraphs from <a href=""http://www2.toyota.co.jp/jp/news/10/01/nt10_0102.html"" rel=""nofollow noreferrer"">this link</a>) where about 83% of the characters are East Asian and the rest are Basic Latin but the result is <strong>en</strong> (English).</p>

<pre><code> 29.6 Hiragana
 18.5 Katakana
 34.9 CJK Unified Ideographs
 16.9 Basic Latin
result en
</code></pre>

<p>The Basic Latin Characters are caused by the use of the English names of organisations etc in the text. The Japanese rule fails because neither Katakana nor Hiragana score 40% (together they score 48.1%). The Chinese rule fails because CJK Unified Ideographs scores less than 40%. So the 83.1% East Asian characters are ignored, and the result is decided by the 16.9% minority. These ""<a href=""http://en.wikipedia.org/wiki/Rotten_and_pocket_boroughs"" rel=""nofollow noreferrer"">rotten borough</a>"" rules need some reform. In generality, it could be expressed like:</p>

<p>If (total of script blocks used by only language X) >= X-specific threshold, then select language X.</p>

<p>As suggested above, Hiragana + Katakana >= 40% will probably do the trick for Japanese. A similar rule may well be needed for Korean.</p>

<p>Your gobbledegook did actually contain a few characters of markup (I didn't scroll far enough to the right to see it) but certainly not enough to depress all the East Asian scores below 40%. So we're still waiting to see what your actual input is and how you got it from where.</p>

<p><strong>End of update2</strong></p>

<p>To aid with diagnosis of your problem, please don't print gobbledegook; use</p>

<pre><code>print repr(justwords)
</code></pre>

<p>That way anyone who is interested in actually doing debugging has got something to work on. It would help if you gave the URL of the webpage, and showed the Python code that you used to get your unicode <code>justwords</code>. Please edit your answer to show those 3 pieces of information.</p>

<p><strong>Update 3</strong> Thanks for the URL. Visual inspection indicates that the language is overwhelmingly Chinese. What gave you the impression that it is Japanese?</p>

<p>Semithanks for supplying some of your code. To avoid your correspondents having to do your work for you, and to avoid misunderstandings due to guessing, you should always supply (without being asked) a self-contained script that will reproduce your problem. Note that you say you got ""ASCII errors"" (no exact error message! no traceback!) if you didn't do .encode('utf8') -- my code (see below) doesn't have this problem.</p>

<p>No thanks for not supplying the result of <code>print repr(justwords)</code> (even after being asked). Inspecting what intermediate data has been created is a very elementary and very effective debugging technique. This is something you should always do before asking a question. Armed with this knowledge you can ask a better question.</p>

<p>Using this code:</p>

<pre><code># coding: ascii
import sys
sys.path.append(r""C:\junk\wotlang\guess-language\guess_language"")
import guess_language
URL = ""http://feeds.feedburner.com/nchild""
from BeautifulSoup import BeautifulStoneSoup
from pprint import pprint as pp
import urllib2
htmlSource = urllib2.urlopen(URL).read()
soup = BeautifulStoneSoup(htmlSource)
fall = soup.findAll(text=True)
# pp(fall)
justwords = ''.join(fall)
# justwords = justwords.encode('utf-8')
result = guess_language.guessLanguage(justwords)
print ""result"", result
</code></pre>

<p>I got these results:</p>

<pre><code> 29.0 CJK Unified Ideographs
  0.0 Extended Latin
  0.1 Katakana
 70.9 Basic Latin
result en
</code></pre>

<p>Note that the URL content is not static; about an hour later I got:</p>

<pre><code> 27.9 CJK Unified Ideographs
  0.0 Extended Latin
  0.1 Katakana
 72.0 Basic Latin
</code></pre>

<p>The statistics were obtained by fiddling around line 361 of <code>guess_language.py</code> so that it reads:</p>

<pre><code>for key, value in run_types.items():
    pct = (value*100.0) / totalCount # line changed so that pct is a float
    print ""%5.1f %s"" % (pct, key) # line inserted
    if pct &gt;=40:
        relevant_runs.append(key)
</code></pre>

<p>The statistics are symptomatic of Chinese with lots of HTML/XML/Javascript stuff (see previous example); this is confirmed by looking at the output of the pretty-print obtained by un-commenting <code>pp(fall)</code> -- lots of stuff like:</p>

<pre><code>&amp;lt;img style=""float:left; margin:0 10px 0px 10px;cursor:pointer; cursor:hand
;"" width=""60px"" src=""http://2.bp.blogspot.com/_LBJ4udkQZag/Rm6sTn1b7NI/AAAAAAAAA
FA/bYkSJZ3i2bg/s400/hepinge169.gif"" border=""0"" alt=""""id=""BLOGGER_PHOTO_ID_507518
3283203730642"" alt=""\u548c\u5e73\u6771\u8def\u4e00\u6bb5169\u865f"" title=""\u548c
\u5e73\u6771\u8def\u4e00\u6bb5169\u865f""/&amp;gt;\u4eca\u5929\u4e2d\u5348\u8d70\u523
0\u516c\u53f8\u5c0d\u9762\u76847-11\u8cb7\u98f2\u6599\uff0c\u7a81\u7136\u770b\u5
230\u9019\u500b7-11\u602a\u7269\uff01\u770b\u8d77\u4f86\u6bd4\u6a19\u6e96\u62db\
u724c\u6709\u4f5c\u7528\u7684\u53ea\u6709\u4e2d\u9593\u7684\u6307\u793a\u71c8\u8
00c\u5df2\uff0c\u53ef\u537b\u6709\u8d85\u7d1a\u5927\u7684footprint\uff01&amp;lt;br /
&amp;gt;&amp;lt;br /&amp;gt;&amp;lt;a href=""http://4.bp.blogspot.com/_LBJ4udkQZag/Rm6wHH1b7QI/AA
</code></pre>

<p>You need to do something about the markup. Steps: Look at your raw ""htmlSource"" in an XML browser. Is the XML non-compliant? How can you avoid having untranslated <code>&amp;lt;</code> etc? What elements have text content that is ""English"" only by virtue of it being a URL or similar? Is there a problem in Beautiful[Stone]Soup? Should you be using some other functionality of Beautiful[Stone]Soup? Should you use lxml instead?</p>

<p>I'd suggest some research followed by a new SO question.</p>

<p><strong>end of update 3</strong></p>
",784,1264796592
English Lexicon for Search Query Correction,"<p>I'm building a spelling corrector for search engine queries by implementing the method described in ""<a href=""http://research.microsoft.com/en-us/people/silviu/emnlp04.pdf"" rel=""nofollow noreferrer"">Spelling correction as an iterative process that exploits the collective knowledge of web users</a>"".  </p>

<p>The high-level approach is as follows:  for a given query, come up with possible correction candidates (words in the query log within a certain edit distance) of each unigram and bigram, then perform a modified Viterbi search to find the most likely sequence of candidates given bigram frequencies.  Repeat this process until the sequence is of maximum probability.</p>

<p>The modification to the Viterbi search is such that if two adjacent words are both found in a trusted lexicon, at most one can be corrected.  This is especially important for avoiding correction of properly-spelled single-word queries to words of higher frequency.</p>

<p>My question is where to find such a lexicon.  It should be in English and contain proper nouns (first/last names, places, brand names, etc) likely to show up in search queries as well as common and uncommon English words.  Even a push in the right direction would be useful.</p>

<p>Also, if anyone is reading this and has any suggestions for improvement on the methodology supplied in the paper, I am open to those as well given that this is my first foray into NLP.</p>
","search, dictionary, nlp, lexicon","<p>The best lexicon for this purpose is probably the Google Web 1T 5-gram data set.</p>

<p><a href=""http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13"" rel=""nofollow noreferrer"">http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13</a></p>

<p>Unfortunately, it is not free unless your university is a member of LDC.</p>

<p>You could also try the corpora in packages like Python NLTK, but the Google one seems to be the best for your purpose since it is related to search queries already.</p>
",350,1264699896
What a single sentence consist of? How to name it?,"<p>I'm designing architecture of a text parser. Example sentence: <code>Content here, content here.</code></p>

<p>Whole sentence is a... sentence, that's obvious. <code>The</code>, <code>quick</code> etc are words; <code>,</code> and  <code>.</code> are punctuation marks. But what are words and punctuation marks all together  in general? Are they just symbols? I simply don't know how to name what a single sentence consists of in the most reasonable abstract way (because one may write it consists of letters/vowels etc).</p>

<p>Thanks for any help :)</p>
","oop, nlp, abstraction, linguistics","<p>What you're doing is technically lexical analysis (""lexing""), which takes a sequence of input symbols and generates a series of tokens or lexemes. So word, punctuation and white-space are all tokens.</p>

<p>In (E)BNF terms, lexemes or tokens are synonymous with ""terminal symbols"". If you think of the set of parsing rules as a tree the terminal symbols are the leaves of the tree.</p>

<p>So what's the atom of your input? Is it a word or a sentence? If it's words (and white-space) then a sentence is more akin to a parsing rule. In fact the term ""sentence"" can itself be misleading. It's not uncommon to refer to the entire input sequence as a sentence.</p>

<p>A semi-common term for a sequence of non-white-space characters is a ""textrun"".</p>
",483,1264565445
Optimizing a Recursive Method In PHP,"<p>I'm writing a text tag parser and I'm currently using this recursive method to create tags of <em>n</em> words. Is there a way that it can be done non-recursively or at least be optimized? Assume that $this->dataArray could be a very large array.</p>

<pre><code>/**
 * A recursive function to add phrases to the tagTracker array
 * @param string $data
 * @param int $currentIndex
 * @param int $depth
 */ 
protected function compilePhrase($data, $currentIndex, $depth){
    if (!empty($data)){
        if ($depth &gt;= $this-&gt;phraseStart){
            $this-&gt;addDataCount($data, $depth);
        }
        if ($depth &lt; $this-&gt;phraseDepth){
            $currentIndex = $currentIndex + 1;
            //$this-&gt;dataArray is an array containing all words in the text
            $data .= ' '.$this-&gt;dataArray[$currentIndex]; 
            $depth += 1;
            $this-&gt;compilePhrase($data, $currentIndex, $depth);
        }
    }
}
</code></pre>
","php, recursion, nlp","<p>See if you can use <strong><a href=""http://en.wikipedia.org/wiki/Tail_recursion"" rel=""nofollow noreferrer"">tail recursion</a></strong> rather than call-based recursion. Some rewriting may be required but a cursory looks says it is fine to do.</p>

<p>Tail recursion is great for a subset of recursive functions, and good practice to spot when loops can replace recursion, and how to rewrite.</p>

<p>Saying that, I don't know what the overhead inside PHP is of the call. Might just be one return-pointer type setup rather than a real stack wind.</p>

<blockquote>
  <p>Turns out about the same. Does PHP optimize tail recursive calls out itself?</p>
</blockquote>

<p>Here is my rewrite, but beware, my brain is currently sleep deprived!</p>

<pre><code>protected function compilePhrase($data, $currentIndex, $depth){
    /* Loop until break condition */
    while(true) {
        if (!empty($data)){
            if ($depth &gt;= $this-&gt;phraseStart){
                $this-&gt;addDataCount($data, $depth);
            }
            if ($depth &lt; $this-&gt;phraseDepth){
                $currentIndex = $currentIndex + 1;
                // A test here might be better than the !empty($data)
                // in the IF condition. Check array bounds, assuming
                // array is not threaded or anything
                $data .= ' '.$this-&gt;dataArray[$currentIndex]; 
                $depth += 1;
            }else{
               break;
            }
        }else{
           break;
        }
    }
    /* Finish up here */
    return($this-&gt;dataArray);
}
</code></pre>
",3081,1264052046
Morphophoneme processing library in Java,"<p>Are there any good Java libraries with prebuilt dictionaries that I can use to try and extract word roots from input words?</p>

<p>I asked a more general question which supersedes this question.  It is <a href=""https://stackoverflow.com/questions/2061881/natural-language-parsing-tools-what-is-out-there-and-what-is-not"">here</a>.  Please vote to close this question.  </p>
","nlp, morphological-analysis","<p>Work with WordNet for Java in the form of <a href=""http://sourceforge.net/projects/jwordnet/"" rel=""nofollow noreferrer"">JWNL</a>, which includes a port of Wordnet's ""Morphy"" lemmatizer.</p>
",125,1263436153
Algorithm to determine how positive or negative a statement/text is,"<p>I need to implement sentiment analysis.  Can anyone point me to examples/reference implementations?</p>
","c#, algorithm, nlp",,5556,1251177064
How to choose a Feature Selection Algorithm? - advice,"<p>Is there a research paper/book that I can read which can tell me for the problem at hand what sort of feature selection algorithm would work best. </p>

<p>I am trying to simply identify twitter messages as pos/neg (to begin with). I started out with Frequency based feature selection (having started with NLTK book) but soon realised that for a similar problem various individuals have choosen different algorithms</p>

<p>Although I can try Frequency based, mutual information, information gain and various other algorithms the list seems endless.. and was wondering if there an efficient way then trial and error. </p>

<p>any advice</p>
","nlp, nltk, semantic-analysis",,1633,1263487906
Language related -What does Client-server application mean?,"<p>Well</p>

<p>It's not a big question, obviously.</p>

<p>But you see, an application that is using a database on the servers, and is installed on multiple clients. Is called Client/Server application.</p>

<p>And an application that is constituted by two parts: Host (or server) part, and the client part.</p>

<p>They are both called client/server apps</p>

<p>How can we distinguish them, and what's the proper name of each type.</p>

<p>P.S. I'm talking about English, you know. I want to say that this application is Client/Server(only database on server) and not a Client/Server(as real client and server).</p>
",nlp,"<p>As others have said, they're both client/server applications.  If you want to be more explicit about what kind of client and what kind of server, you can use adjectives or nouns.  Please don't go inventing new terms when we've already got plenty of terms that can be combined - computer science &amp; software engineering are already overloaded with unnecessary terms.</p>

<ul>
<li>web client/web server   (note that the web includes more protocols than just HTTP)</li>
<li>http client/http server</li>
<li>ftp client/ftp server</li>
<li>soap client/soap server</li>
<li><p>client/application server (the app server may include a db, or the appserver may be the db client)
<br><br></p></li>
<li><p>database client/database server</p></li>
<li>JDBC client/database server</li>
<li>sqlplus/oracle</li>
<li>application server/database server</li>
</ul>
",833,1263482210
Training set - proportion of pos / neg / neutral sentences,"<p>I am hand tagging twitter messages as Positive, Negative, Neutral. I am try to appreciate is there some logic one can use to identify of the training set what proportion of message should be positive / negative and neutral ? </p>

<p>So for e.g. if I am training a Naive Bayes classifier with 1000 twitter messages should the proportion of pos : neg : neutral be 33 % : 33% : 33% or should it be 25 % : 25 % : 50  %</p>

<p>Logically in my head it seems that I i train (i.e. give more samples for neutral) that the system would be better at identifying neutral sentences then whether they are positive or negative - is that true ? or I am missing some theory here  ? </p>

<p>Thanks
Rahul </p>
","nlp, semantic-markup, nltk, semantic-analysis","<p>The problem you're referring to is known as the imbalance problem. Many machine learning algorithms perform badly when confronted with imbalanced training data, i.e. when the instances of one class heavily outnumber those of the other class. Read <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.7757&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">this article</a> to get a good overview of the problem and how to approach it. For techniques like naive bayes or decision trees it is always a good idea to balance your data somehow, e.g. by random oversampling (explained in the references paper). I disagree with mjv's suggestion to have a training set match the proportions in the real world. This may be appropriate in some cases but I'm quite confident it's not in your setting. For a classification problem like the one you describe, the more the sizes of the class sets differ, the more most ML algorithms will have problems discriminating the classes properly. However, you can always use the information about which class is the largest in reality by taking it as a fallback such that when the classifier's confidence for a particular instance is low or this instance couldn't be classified at all, you would assign it the largest class. </p>

<p>One further remark: finding the positivity/negativity/neutrality in Twitter messages seems to me to be a question of degree. As such, it may be viewes as a regression rather than a classification problem, i.e. instead of a three class scheme you perhaps may want calculate a score which tells you <em>how</em> positive/negative the message is. </p>
",1476,1263403388
Matching substrings from a dictionary to other string: suggestions?,"<p>Hellow Stack Overflow people. I'd like some suggestions regarding the following problem. I am using Java.</p>

<p>I have an array #1 with a number of Strings. For example, two of the strings might be: ""An apple fell on Newton's head"" and ""Apples grow on trees"".</p>

<p>On the other side, I have another array #2 with terms like (Fruits => Apple, Orange, Peach; Items => Pen, Book; ...). I'd call this array my ""dictionary"".</p>

<p>By comparing items from one array to the other, I need to see in which ""category"" the items from #1 fall into from #2. E.g. Both from #1 would fall under ""Fruits"".</p>

<p>My most important consideration is speed. I need to do those operations fast. A structure allowing constant time retrieval would be good.</p>

<p>I considered a Hashset with the contains() method, but it doesn't allow substrings. I also tried running regex like (apple|orange|peach|...etc) with case insensitive flag on, but I read that it will not be fast when the terms increase in number (minimum 200 to be expected). Finally, I searched, and am considering using an ArrayList with indexOf() but I don't know about its performance. I also need to know which of the terms actually matched, so in this case, it would be ""Apple"".</p>

<p>Please provide your views, ideas and suggestions on this problem.</p>

<p>I saw Aho-Corasick algorithm, but the keywords/terms are very likely to change often. So I don't think I can use that. Oh, I'm no expert in text mining and maths, so please elaborate on complex concepts.</p>

<p>Thank you, Stack Overflow people, for your time! :)</p>
","java, nlp",,771,1262791825
How to recognize words in text with non-word tokens?,"<p>I am currently parsing a bunch of mails and want to get words and other interesting tokens out of mails (even with spelling errors or combination of characters and letters, like ""zebra21"" or ""customer242""). But how can I know that ""0013lCnUieIquYjSuIA"" and ""anr5Brru2lLngOiEAVk1BTjN"" are not words and not relevant?  How to extract words and discard tokens that are encoding errors or parts of pgp signature or whatever else we get in mails and know that we will never be interested in those?</p>
","algorithm, nlp, lexical-analysis","<p>You need to decide on a good enough criteria for a word and write a regular expression or a manual to enforce it.<br>
A few rules that can be extrapolated from your examples:</p>

<ul>
<li>words can start with a captial letter or be all capital letters but if you have more than say, 2 uppercase letters and more than 2 lowercase letters inside a word, it's not a word</li>
<li>If you have numbers inside the word, it's not a word</li>
<li>if it's longer than say, 20 characters</li>
</ul>

<p>There's no magic trick. you need to decide what you want the rules to be and make them happen.</p>

<p>Al alternative way is to train some kind of Hidden Markov-Models system to recognize things that sound like words but I think this is an overkill for what you want to do.</p>
",1496,1262523238
How does twitter&#39;s trending topics algorithm decide which words to extract from tweets?,"<p>I saw <a href=""https://stackoverflow.com/questions/787496/what-is-the-best-way-to-compute-trending-topics-or-tags"">this question</a>, which focuses on the ""Brittney Spears"" problem.  But I have a bit of a different question.  How does the algorithm determine which words or phrases need to be ranked?  For instance, if I send out a tweet that says ""Michael Jackson died"", how does it know to pull out ""Michael Jackson"" but not ""died""?</p>

<p>Or suppose that Alec Baldwin and Steven Baldwin were in the news that day and thus were both mentioned in a lot of tweets.  How would it know to treat both names differently instead of just pulling out ""Baldwin""?</p>

<p>Done naively, I could see this problem as being NP-complete (you'd have to compare all potential phrases in the tweet with all potential phrases in everyone else's tweets).</p>
","algorithm, twitter, nlp, ranking","<p>A general solution to this problem is with <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">""term frequency, inverse document frequency"" (tf-idf)</a>.</p>

<p>It is a statistical approach which finds words/terms that are more relevant than others because they're not seen very often.  In this case, the name ""Michael Jackson"" may have very low frequency compared to a common English word ""died"".</p>

<p>As for the Alec Baldwin vs. Steven Baldwin - these would be identified as separate during <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow noreferrer"">part-of-speech tagging</a> - they would tagged as individual proper nouns.</p>
",2612,1262547164
Training Hidden Markov Models without Tagged Corpus Data,"<p>For a linguistics course we implemented Part of Speech (POS) tagging using a hidden markov model, where the hidden variables were the parts of speech. We trained the system on some tagged data, and then tested it and compared our results with the gold data.</p>

<p>Would it have been possible to train the HMM without the tagged training set? </p>
","artificial-intelligence, machine-learning, nlp, linguistics, markov-models","<p>In theory you can do that. In that case you would use the Baum-Welch-Algorithm. It is described very well in <a href=""http://www.google.de/url?sa=t&amp;source=web&amp;ct=res&amp;cd=1&amp;ved=0CAcQFjAA&amp;url=http%3A%2F%2Fwww.cs.ubc.ca%2F~murphyk%2FBayes%2Frabiner.pdf&amp;ei=W9AqS5L3FMnG_gaQ_7SECQ&amp;usg=AFQjCNHeXLhTHmuKUXKKCHYSs58TxVGfZg&amp;sig2=WbhBGy_vxbfdRXicyae0Rg"" rel=""nofollow noreferrer"">Rabiner's HMM Tutorial</a>.</p>

<p>However, having applied HMMs to part of speech, the error you get with the standard form will not be so satisfying. It is a form of expectation maximization which only converges to local maxima. Rule based approaches beat HMMs hands down, iirc. </p>

<p>I believe the natural language toolkit NLTK for python has an HMM implementation for that exact purpose.</p>
",1259,1260990119
Calculating context-sensitive text correlation,"<p>Suppose I want to match address records (or person names or whatever) against each other to merge records that are most likely referring to the same address. Basically, I guess I would like to calculate some kind of correlation between the text values and merge the records if this value is over a certain threshold.</p>

<p>Example: 
""West Lawnmower Drive 54 A"" is probably the same as ""W. Lawn Mower Dr. 54A"" but different from ""East Lawnmower Drive 54 A"".</p>

<p>How would you approach this problem? Would it be necessary to have some kind of context-based dictionary that knows, in the address case, that ""W"", ""W."" and ""West"" are the same? What about misspellings (""mover"" instead of ""mower"" etc)?</p>

<p>I think this is a tricky one - perhaps there are some well-known algorithms out there?</p>
","algorithm, string, text, nlp","<p>A good <em>baseline</em>, probably an impractical one in terms of its relatively high computational cost and more importantly its production of many false positive, would be generic string distance algorithms such as</p>

<ul>
<li><a href=""http://www.itl.nist.gov/div897/sqg/dads/HTML/Levenshtein.html"" rel=""noreferrer""><strong>Edit distance</strong></a> (aka Levenshtein distance)</li>
<li><a href=""http://www.itl.nist.gov/div897/sqg/dads/HTML/ratcliffObershelp.html"" rel=""noreferrer""><strong>Ratcliff/Obershelp</strong></a></li>
</ul>

<p>Depending on the level of accuracy required (which, BTW, should be specified both in terms of its <a href=""http://en.wikipedia.org/wiki/Recall_%28information_retrieval%29"" rel=""noreferrer""><em>recall</em> and <em>precision</em></a>, i.e. generally expressing whether it is more important to miss a correlation than to falsely identify one), <strong>a home-grown process based on [some of] the following heuristics and ideas could do the trick</strong>:</p>

<ul>
<li>tokenize the input, i.e. see the input as an array of words rather than a string</li>
<li>tokenization should also keep the line number info</li>
<li>normalize the input with the use of a short dictionary of common substituions (such as ""dr"" at the end of a line = ""drive"", ""Jack"" = ""John"", ""Bill"" = ""William""..., ""W."" at the begining of a line is ""West"" etc.</li>
<li>Identify (a bit like tagging, as in POS tagging) the nature of some entities (for example ZIP Code, and Extended ZIP code, and also city</li>
<li>Identify (lookup) some of these entities (for example a relative short database table can include all the Cities / town in the targeted area</li>
<li>Identify (lookup) some domain-related entities  (if all/many of the address deal with say folks in the legal profession, a lookup of law firm names or of federal buildings may be of help.</li>
<li>Generally, put more weight on tokens that come from the last line of the address</li>
<li>Put more (or less) weight on tokens with a particular entity type (ex: ""Drive"", ""Street"", ""Court"" should with much less than the tokens which precede them.</li>
<li>Consider a modified <a href=""http://en.wikipedia.org/wiki/Soundex"" rel=""noreferrer""><strong>SOUNDEX</strong></a> algorithm to help with normalization of </li>
</ul>

<p>With the above in mind, implement <strong><em>a rule-based evaluator</em></strong>.  Tentatively, the rules could be implemented as visitors to a tree/array-like structure where the input is parsed initially (<a href=""http://sourcemaking.com/design_patterns/visitor"" rel=""noreferrer"">Visitor design pattern</a>).<br>
The advantage of the rule-based framework, is that each heuristic is in its own function and rules can be prioritized, i.e. placing some rules early in the chain, allowing to abort the evaluation early, with some strong heuristics  (eg: different City => Correlation = 0, level of confidence = 95% etc...).</p>

<p>An important consideration with search for correlations is the <strong>need to <em>a priori</em> compare every single item (here address) with every other item</strong>, hence requiring as many as <code>1/2 n^2</code> item-level comparisons.  Because of this, it may be useful to store the reference items in a way where they are pre-processed (parsed, normalized...) and also to maybe have a <strong>digest/key of sort</strong> that can be used as [very rough] indicator of a possible correlation  (for example a key made of the 5 digit ZIP-Code followed by the SOUNDEX value of the ""primary"" name). </p>
",3839,1259851991
Natural language command language,"<p>I'm interested in developing a natural language command language for a domain with existing rules. I was very impressed when Terry Winograd's <a href=""http://en.wikipedia.org/wiki/Shrdlu"" rel=""noreferrer"">SHRDLU</a> showed the way (the conversation below is 40 years old! Astonishing). Can we do better now and if so where can I get examples?</p>

<blockquote>
  <p>Person: Pick up a big red block.<br>
  Computer: OK.<br>
  Person: Grasp the pyramid.<br>
  Computer: I don't understand which pyramid you mean.<br>
  Person (changing their mind): Find a block which is taller than the one you are holding and put it into the box.<br>
  Computer: By ""it"", I assume you mean the block which is taller than the one I am holding.<br>
  Computer: OK.<br>
  Person: What does the box contain?<br>
  Computer: The blue pyramid and the blue block.<br>
  Person: What is the pyramid supported by?<br>
  Computer: The box.<br>
  Person: How many blocks are not in the box?<br>
  Computer: Four of them.<br>
  Person: Is at least one of them narrower than the one which I told you to pick up?<br>
  Computer: Yes, the red cube.</p>
</blockquote>

<p>I have an ontology for the domain so can reason over common problems.</p>
","command-line, nlp","<p>I have also just encountered Regulus (<a href=""http://en.wikipedia.org/wiki/Regulus_Grammar_Compiler"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Regulus_Grammar_Compiler</a>) which has been extensively used by NASA.</p>
",1482,1256195174
Natural language rendering,"<p>Do you know any frameworks that implement natural language rendering concept ?<br/>
I've found several NLP oriented frameworks like <a href=""http://www.proxem.com/Antelope/WhatisAntelope/tabid/67/Default.aspx"" rel=""nofollow noreferrer"">Anthelope</a> or <a href=""http://opennlp.sourceforge.net/"" rel=""nofollow noreferrer"">Open NLP</a> but they have only parsers but not renderers or builders. For example I want to render a question about smth. I'm constructing sentence object, setting it's properties, specify it's language and then render as a plain text. <br/>
Please advice. Thanks !</p>
","frameworks, artificial-intelligence, nlp",,143,1258367885
find some sentences,"<p>I'd like to find good way to find some (let it be two) sentences in some text. What will be better - use regexp or split-method? Your ideas?</p>

<p>As requested by Jeremy Stein - there are some examples</p>

<p><strong>Examples:</strong></p>

<p><strong>Input:</strong></p>

<blockquote>
  <p>The first thing to do is to create the Comment model. We’ll create this in the normal way, but with one small difference. If we were just creating comments for an Article we’d have an integer field called article_id in the model to store the foreign key, but in this case we’re going to need something more abstract.</p>
</blockquote>

<p><strong>First two sentences:</strong></p>

<blockquote>
  <p>The first thing to do is to create the Comment model. We’ll create this in the normal way, but with one small difference.</p>
</blockquote>

<p><strong>Input:</strong></p>

<blockquote>
  <p>Mr. T is one mean dude.  I'd hate to get in a fight with him.</p>
</blockquote>

<p><strong>First two sentences:</strong></p>

<blockquote>
  <p>Mr. T is one mean dude.  I'd hate to get in a fight with him.</p>
</blockquote>

<p><strong>Input:</strong></p>

<blockquote>
  <p>The D.C. Sniper was executed was executed by lethal injection at a Virginia prison. Death was pronounced at 9:11 p.m. ET.</p>
</blockquote>

<p><strong>First two sentences:</strong></p>

<blockquote>
  <p>The D.C. Sniper was executed was executed by lethal injection at a Virginia prison. Death was pronounced at 9:11 p.m. ET.</p>
</blockquote>

<p><strong>Input:</strong></p>

<blockquote>
  <p>In her concluding remarks, the opposing attorney said that ""...in this and so many other instances, two wrongs won’t make a right.""  The jury seemed to agree.</p>
</blockquote>

<p><strong>First two sentences:</strong></p>

<blockquote>
  <p>In her concluding remarks, the opposing attorney said that ""...in this and so many other instances, two wrongs won’t make a right.""  The jury seemed to agree.</p>
</blockquote>

<p>Guys, as you can see - it's not so easy to determine two sentences from text. :(</p>
","ruby, regex, nlp",,1383,1257939680
Is there a fairly simple way for a script to tell (from context) whether &quot;her&quot; is a possessive pronoun?,"<p>I am writing a script to reverse all genders in a piece of text, so all gendered words are swapped - ""man"" is swapped with ""woman"", ""she"" is swapped with ""he"", etc. But there is an ambiguity as to whether ""her"" should be replaced with ""him"" or ""his"".</p>
","regex, nlp, linguistics",,1072,1257604902
"Natural language parsing, practical example","<p>I am looking to use a natural language parsing library for a simple chat bot.  I can get the Parts of Speech tags, but I always wonder.  What do you do with the POS.  If I know the parts of the speech, what then?</p>

<p>I guess it would help with the responses.  But what data structures and architecture could I use.</p>
","java, nlp","<p>A part-of-speech tagger assigns labels to the words in the input text. For example, the popular Penn Treebank tagset has some 40 labels, such as ""plural noun"", ""comparative adjective"", ""past tense verb"", etc. The tagger also resolves some ambiguity. For example, many English word forms can be either nouns or verbs, but in the context of other words, their part of speech is unambiguous.
So, having annotated your text with POS tags you can answer questions like: how many nouns do I have?, how many sentences do not contain a verb?, etc.</p>

<p>For a chatbot, you obviously need much more than that. You need to figure out the subjects and objects in the text, and which verb (predicate) they attach to; you need to resolve anaphors (which individual does a <em>he</em> or <em>she</em> point to), what is the scope of negation and quantifiers (e.g. <em>every</em>, <em>more than 3</em>), etc.</p>

<p>Ideally, you need to map you input text into some logical representation (such as first-order logic), which would let you bring in reasoning to determine if two sentences are equivalent in meaning, or in an entailment relationship, etc.</p>

<p>While a POS-tagger would map the sentence</p>

<pre><code>Mary likes no man who owns a cat.
</code></pre>

<p>to such a structure</p>

<pre><code>Mary/NNP likes/VBZ no/DT man/NN who/WP owns/VBZ a/DT cat/NN ./.
</code></pre>

<p>you would rather need something like this:</p>

<pre><code>SubClassOf(
   ObjectIntersectionOf(
      Class(:man)
      ObjectSomeValuesFrom(
         ObjectProperty(:own)
         Class(:cat)
      )
   )
   ObjectComplementOf(
      ObjectSomeValuesFrom(
         ObjectInverseOf(ObjectProperty(:like))
         ObjectOneOf(
            NamedIndividual(:Mary)
         )
      )
   )
)
</code></pre>

<p>Of course, while POS-taggers get precision and recall values close to 100%, more complex automatic processing will perform much worse.</p>

<p>A good Java library for NLP is <a href=""http://alias-i.com/lingpipe/"" rel=""nofollow noreferrer"">LingPipe</a>. It doesn't, however, go much beyond POS-tagging, chunking, and named entity recognition.</p>
",3592,1236434819
Algorithms to detect phrases and keywords from text,"<p>I have around 100 megabytes of text, without any markup, divided to approximately 10,000 entries. I would like to automatically generate a 'tag' list. The problem is that there are word groups (i.e. phrases) that only make sense when they are grouped together.</p>

<p>If I just count the words, I get a large number of really common words (is, the, for, in, am, etc.). I have counted the words and the number of other words that are before and after it, but now I really cannot figure out what to do next The information relating to the 2 and 3 word phrases is present, but how do I extract this data?</p>
","algorithm, nlp, text-processing","<p>Before anything, <strong>try to preserve the info about ""boundaries"" which comes in the input text.</strong><br>
(if such info has not readily be lost, your question implies that maybe the tokenization has readily been done)<br>
During the tokenization (word parsing, in this case) process, look for patterns that may define <strong>expression boundaries</strong> (such as punctuation, particularly periods, and also multiple LF/CR separation,  use these.  Also words like ""the"", can often be used as boundaries.   Such expression boundaries are typically ""negative"", in a sense that they separate two token instances which are sure to <em>not</em> be included in the same expression.  A few positive boundaries are quotes, particularly double quotes.   This type of info may be useful to filter-out some of the n-grams (see next paragraph).  Also word sequencces such as ""for example"" or ""in lieu of"" or ""need to"" can be used as expression boundaries as well (but using such info is edging on using ""priors"" which I discuss later).</p>

<p><em>Without using external data</em> (other than the input text), you can have a relative success with this by running statistics on <strong>the text's digrams and trigrams</strong> (sequence of 2 and 3 consecutive words).  Then [most] the sequences with a significant (*) number of instances will likely be the type of ""expression/phrases"" you are looking for.<br>
This somewhat crude method will yield a few false positive, but on the whole may be workable.  Having filtered the n-grams known to cross ""boundaries"" as hinted in the first paragraph, may help significantly because in natural languages sentence ending and sentence starts tend to draw from a limited subset of the message space and hence produce combinations of token that may appear to be statistically well represented, but which are typically not semantically related. </p>

<p><strong>Better methods</strong> (possibly more expensive, processing-wise, and design/investment-wise), will make the use of extra ""priors"" relevant to the domain and/or national languages of the input text. </p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""noreferrer""><strong>POS (Part-Of-Speech) tagging</strong></a> is quite useful, in several ways (provides additional, more objective expression boundaries, and also ""noise"" words classes, for example all articles, even when used in the context of entities are typically of little in tag clouds such that the OP wants to produce.</li>
<li><strong>Dictionaries, lexicons</strong> and the like can be quite useful too.  In particular, these which identify ""entities"" (aka instances in <a href=""http://wordnet.princeton.edu"" rel=""noreferrer"">WordNet</a> lingo) and their alternative forms.  Entities are very important for tag clouds (though they are not the only class of words found in them), and by identifying them, it is also possible to normalize them  (the many different expressions which can be used for say,""Senator T. Kennedy""), hence eliminate duplicates, but also increase the frequency of the underlying entities.</li>
<li>if the corpus is structured as a document collection, it may be useful to use various tricks related to the TF (Term Frequency) and IDF (Inverse Document Frequency)</li>
</ul>

<p>[Sorry, gotta go, for now (plus would like more detail from your specific goals etc.).  I'll try and provide more detail and pointes later]</p>

<p>[BTW, I want to plug here <strong>Jonathan Feinberg and Dervin Thunk responses</strong> from this post, as they provide excellent pointers, in terms of methods and tools for the kind of task at hand.  In particular, <strong>NTLK</strong> and <strong>Python-at-large</strong> provide an excellent framework for experimenting]</p>
",35154,1256821867
extract grammar features from sentence on Google App Engine,"<p>For my GAE app I need to do some natural language processing to extract the subject and object from an input sentence. </p>

<p>Apparently <a href=""https://stackoverflow.com/questions/1286301/using-the-python-nltk-2-0b5-on-the-google-app-engine"">NLTK can't be installed</a> (easily) on GAE so I am looking for another solution.
I noticed GAE comes with <a href=""http://code.google.com/appengine/docs/python/tools/libraries.html#Antlr"" rel=""nofollow noreferrer"">Antlr3</a> but from browsing their documentation it solves a different kind of grammar problem.</p>

<p>Any ideas?</p>
","python, google-app-engine, antlr3, nlp","<p>You can easily build and NTLK RPC server on some machine and access it.</p>

<p>Another option is to find another web based service that already does that (such as opencalais).</p>
",476,1256791005
Integrating my program with a web2.0 website,"<p>I'm creating an ELIZA-like chatterbot, and I'd like to calibrate it with Omegle, using what the other person type as the input. </p>

<p>If it was a regular HTML page, I could parse it and send back the response to some script, but checking the source code, I've noticed that the entire page is created using Javascript, but obfuscates the entire thing.</p>

<p>Any idea on how to do that? </p>

<p>Thanks!</p>
","javascript, web-applications, chat, nlp, chatbot","<p>Well you should ask the people that made Omegle about an API. From what I've seen, it doesn't have an open API, but maybe they can give you some extra info.</p>

<p>You can also try Firebug. It lets you see all the requests that are made and their target. The site is AJAX driven, that's why you can't see the desination scripts directly in the page.</p>

<p>You can (1) build a program that sends post requests to the targets show in the Firebug net panel and passes the data to the chat-bot or (2) build a browser plugin that does that (it's pretty easy to do in Firefox). You may need the browser plugin because of cross-domain request limitations and because you may not see the received messages from an external program/web-app - it's not very clear to me how they implemented that, i guess it's something Comet-like.</p>

<p>Hope this helps :).</p>
",149,1255792526
Natural language automation?,"<p>I remember reading about an automation program for windows that would accept a list of commands like this:</p>

<pre><code>press the ok button
put ""hello world"" into the text control
press the add button
</code></pre>

<p>etc etc. Can anyone name this program? A thousand thankyous.</p>
","automation, nlp","<p>Take a look at watin. <a href=""http://watin.sourceforge.net/"" rel=""nofollow noreferrer"">http://watin.sourceforge.net/</a>
Combined with Microsoft Oslo DSL. You can achieve this easily</p>
",299,1226961589
How to automatically excerpt user generated content?,"<p>I run a website that allows users to write blog-post, I would really like to summarize the written content and use it to fill the <code>&lt;meta name=""description"".../&gt;</code>-tag for example.</p>

<p><em>What methods can I employ to automatically summarize/describe the contents of user generated content?<br>
Are there any (preferably free) methods out there that have solved this problem?</em></p>

<p>(I've seen other websites just copy the first 100 or so words but this strikes me as a sub-optimal solution.)</p>
","artificial-intelligence, nlp, user-generated-content","<p>Think of the task of summarization as a challenge to 'select the most important sentences' from the document.</p>

<p>The method described in <a href=""http://www.di.ubi.pt/~jpaulo/competence/general/(1958)Luhn.pdf"" rel=""nofollow noreferrer"">The Automatic Creation of Literature Abstracts</a> by H.P. Luhn (1958) describes a naive method that actually performs quite well.  Try giving it a shot.  </p>

<p>If your website is in Python coding this algorithm using the <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">NLTK</a> (Natural Language Toolkit) is a fun task.</p>
",605,1253741713
Finding type of break in icu::BreakIterator,"<p>I'm trying to understang how to use <code>icu::BreakIterator</code> to find specific words.</p>

<p>For example I have following sentence:</p>

<blockquote>
  <p>To be or not to be? That is the question...</p>
</blockquote>

<p>Word instance of break iterator would put breaks there:</p>

<blockquote>
  <p>|To| |be| |or| |not| |to| |be|?| |That| |is| |the| |question|.|.|.|</p>
</blockquote>

<p>Now, not every pair of break points is actual word.</p>

<p>In derived class <code>icu::RuleBasedBreakIterator</code> there is a ""getRuleStatus()"" that returns some kind of information about break, and it gives ""Word status at following points (marked ""/"")""</p>

<blockquote>
  <p>|To/ |be/ |or/ |not/ |to/ |be/?| |That/ |is/ |the/ |question/.|.|.|</p>
</blockquote>

<p>But... It all depends on specific rules, and there is absolutely no documentation to understand it (unless I just try), but what would happend with different locales and languages where dictionaries are used? what happens with backware iteration?</p>

<p>Is there any way to get ""Begin of Word"" or ""End of Word"" information like in Qt QTextBoundaryFinder: <a href=""http://qt.nokia.com/doc/4.5/qtextboundaryfinder.html#BoundaryReason-enum"" rel=""nofollow noreferrer"">http://qt.nokia.com/doc/4.5/qtextboundaryfinder.html#BoundaryReason-enum</a>?</p>

<p>How should I solve such problem in ICU correctly?</p>
","c++, unicode, nlp, localization, icu","<p>Have you tried the <a href=""http://userguide.icu-project.org/boundaryanalysis"" rel=""nofollow noreferrer"">ICU documentation</a>? It appears to explain everything you are asking about including handling of internationalisation, reverse iteration, and the rules, both default and how to create your own custom set. They also have code snippets to help.</p>
",1144,1253800135
What is a fast and unsupervised way of checking quality of pdf-extracted text?,"<p>I am working on a somewhat large corpus with articles numbering the tens of thousands. I am currently using PDFBox to extract with various success, and I am looking for a way to programatically check each file to see if the extraction was moderately successful or not. I'm currently thinking of running a spellchecker on each of them, but the language can differ, I am not yet sure which languages I'm dealing with. Natural language detection with scores may also be an idea.</p>

<p>Oh, and any method also has to play nice with Java, be fast and relatively quick to integrate.</p>
","java, pdf, text, nlp","<p>Try an automatically learning spell checker. That's not as scary as it sounds: Start with a big dictionary containing all the words you're likely to encounter. This can be from several languages.</p>

<p>When scanning a PDF, allow for a certain number of unknown words (say 5%). If any of these words are repeated often enough (say 5 times), add them to the dictionary. If the PDF contains more than 5% unknown words, it's very likely something that couldn't be processed.</p>

<p>The scanner will learn over time allowing you to reduce the amount of unknown words if that should be necessary. If that is too much hazzle, a very big dictionary should work well, too.</p>

<p>If you don't have a dictionary, manually process a couple of documents and have the scanner learn. After a dozen files or so, your new dictionary should be large enough for a reasonable water level.</p>
",443,1253524281
Vista speech recognition in multiple languages,"<p>my primary language is spanish, but I use all my software in english, including windows; however I'd like to use speech recognition in spanish.</p>

<p>Do you know if there's a way to use vista's speech recognition in other language than the primary os language?</p>
","windows-vista, nlp, speech-recognition, multilingual","<p>Citation from Vista <a href=""http://blogs.msdn.com/speech/archive/2007/09/01/windows-speech-recognition-language-support-in-windows-vista.aspx"" rel=""nofollow noreferrer"">speech recognition blog</a>:</p>

<blockquote>
  <p>In Windows Vista, Windows Speech
  Recognition works in the current
  language of the OS.  That means that
  in order to use another language for
  speech recognition, you have to have
  the appropriate language pack
  installed.  Language packs are
  available as free downloads through
  Windows Update for the Ultimate and
  Enterprise versions of Vista.  Once
  you have the language installed,
  you’ll need to change the display
  language of the OS to the language you
  want to use.  Both of these are
  options on the “Regional and Language
  Options” control panel.  You can look
  in help for “Install a display
  language” or “Change the display
  language”.</p>
</blockquote>
",5661,1220144928
Natural Language CFG builder Algorithm,"<p>I am working in a natural language processing project. It aims to build libraries for Arabic language. We working on a POS tagger and now I am thinking in grammar phase. Since Arabic language and many others have complicated grammar, so it is very hard to build their context free grammar (CFG). For this reason I had an idea for an algorithm to build a CFG (with probability PCFG) for any language from a tagger corpora using unsupervised learning. To explain the algorithm suppose I have these three tagged statements as an input:
1- Verb Noun
2- Verb Noun Subject
3- Verb Noun Subject adverb
The algorithm gives:
1) A--> Verb Noun
2) B-->A Subject
3) C-->B adverb.<br/>
We repeat this methodology for each statement such that we can finish with a specific PCFG. The main power of the algorithm lies beyond the fact of seeing the whole statement, so the probabilities can be conditional and they are specific. After that CKY algorithm can be applied to choose the best tree for new statements using probabilities.
Do you expect that this algorithm is good or not and does it worth to continue improving it.</p>
","algorithm, nlp",,732,1252702406
Google Wave Context-Aware Spell Checker,"<p>Is it possible to use the <a href=""http://googlesystem.blogspot.com/2009/05/googles-context-sensitive-spell-checker.html"" rel=""nofollow noreferrer"">Google Wave Context-Aware Spell Checker</a> via web services?</p>

<p>If yes, can anyone please be kind enough to post a simple example?</p>
","web-services, nlp, spell-checking, google-wave","<p>Google Wave hasn't been released yet.</p>
",642,1252548579
Natural language processing / text structure analysis starting point,"<p>I need to parse &amp; process a big set of semi-structured text (basically, legal documents - law texts, addendums to them, treaties, judge's decisions, ...). The most fundamental thing I'm trying to do is extract information on how subparts are structured - chapters, articles, subheadings, ... plus some metadata. My question is if anyone can point me to starting points for this type of text processing, because I'm sure there has been a lot of research into this but what I find is mostly on either parsing something with a strict grammar (like code) or completely free-form text (like google tries to do on webpages). I think if I get hold of the right keywords, I would have more success in google and my journal databases. Thanks.</p>
","parsing, nlp, text-processing",,1339,1251734785
How to find Title case phrases from a passage or bunch of paragraphs,"<p>How do I parse sentence case phrases from a passage.</p>

<p>For example from this passage</p>

<p>Conan Doyle said that the character of Holmes was inspired by Dr. Joseph Bell, for whom Doyle had worked as a clerk at the Edinburgh Royal Infirmary. Like Holmes, Bell was noted for drawing large conclusions from the smallest observations.[1] Michael Harrison argued in a 1971 article in Ellery Queen's Mystery Magazine that the character was inspired by Wendell Scherer, a ""consulting detective"" in a murder case that allegedly received a great deal of newspaper attention in England in 1882.</p>

<p>We need to generate stuff like Conan Doyle, Holmes, Dr Joseph Bell, Wendell Scherr etc.</p>

<p>I would prefer a Pythonic Solution if possible</p>
","python, parsing, nlp, text-parsing","<p>This kind of processing can be very tricky.  This simple code does almost the right thing:</p>

<pre><code>for s in re.finditer(r""([A-Z][a-z]+[. ]+)+([A-Z][a-z]+)?"", text):
    print s.group(0)
</code></pre>

<p>produces:</p>

<pre><code>Conan Doyle
Holmes
Dr. Joseph Bell
Doyle
Edinburgh Royal Infirmary. Like Holmes
Bell
Michael Harrison
Ellery Queen
Mystery Magazine
Wendell Scherer
England
</code></pre>

<p>To include ""Dr. Joseph Bell"", you need to be ok with the period in the string, which allows in ""Edinburgh Royal Infirmary. Like Holmes"".</p>

<p>I had a similar problem: <a href=""http://nedbatchelder.com/blog/200804/separating_sentences.html"" rel=""noreferrer"">Separating Sentences</a>.</p>
",1365,1251403447
"How to determine subject, object and other words?","<p>I'm trying to implement application that can determine meaning of sentence, by dividing it to smaller pieces. So I need to know what words are subject, object etc. so that my program can know how to handle this sentence.</p>
","artificial-intelligence, nlp","<p>This is an open research problem. You can get an overview on Wikipedia, <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Natural_language_processing</a>. Consider phrases like ""Time flies like an arrow, fruit flies like a banana"" - unambiguously classifying words is not easy.</p>
",6323,1251021802
How would you interpret these dates?,"<p>I need to interpret relative date string like:</p>

<ul>
<li>last Friday</li>
<li>this Tuesday</li>
<li>next Wednesday</li>
</ul>

<p>The ""Last Friday"" form is easy (take the most recent Friday that is not today) but what about ""this"" vs. ""next""? Could ""this Wednesday"" be yesterday on a Thursday? Could ""this"" and ""next"" Friday be the same day in some cases and a week apart in others?</p>

<hr>

<p>p.s. Given that my target audience is American, I'm primarily interested in the US English vernacular use of the term and slightly less interested in other non-US English (for instance <code>en-gb</code>) usages so if you are non-US please say where you are from.</p>

<hr>

<p>My current thinking:</p>

<ul>
<li><strong>Last X</strong>: the most recent X not including today.</li>
<li><strong>This X</strong>: the immediate next X not including today.</li>
<li><strong>Next X</strong>: the X in the next week (with the start of the week being a bit arbitrary).</li>
</ul>

<p><a href=""http://smplsite.com/NaturalDate/Default.aspx"" rel=""nofollow noreferrer"">Try it out here</a> (be sure to check <em>allow relative</em>)</p>
","nlp, survey","<p>In my experience, in American English ""this""  always means the next immediate occurence of the day.  If it is Monday, ""this Wednesday"" means the day after tomorrow.  Typically, if it is Monday, ""this Tuesday"" is preferentially referred to as ""tomorrow"" -- I cannot remember anyone ever saying ""this Tuesday"" on a Monday, unless they thought it was currently Sunday.</p>

<p>If I say ""next Wednesday"" on a Wednesday, I mean a week from today.  If I say ""next Wednesday"" on a Tuesday, however, I might mean a week from tomorrow, or I might mean tomorrow -- it would depend upon context and is thus rather ""squishy"".  Most Yanks would interpret that as a week from tomorrow.  I think.  I would, anyway.  </p>
",587,1250100720
LingPipe Text Processing API,"<p>This is a question only to those who 
already used the LingPipe.
My question is how to load up the GENIA corpus
for Part of Speech tagging. 
When I start parsing it I get an error saying
that I got out of memory heap.
Thnx.</p>
","java, text, nlp","<p>You need to run the JVM with more memory.  See the answer to <a href=""https://stackoverflow.com/questions/235047/why-do-i-get-an-outofmemoryerror-when-inserting-50-000-objects-into-hashmap"">Why do I get an OutOfMemoryError when inserting 50,000 objects into HashMap?</a>.</p>
",994,1247491675
How to use Wordnet in SQL,"<p>How to use Wordnet in SQL database. Does it exists anywhere can someone give me step by step procedure</p>
","sql, nlp","<p>Yes, actually, you can use the WordNet SQL Builder (<a href=""http://wnsqlbuilder.sourceforge.net/"" rel=""noreferrer"">http://wnsqlbuilder.sourceforge.net/</a>).  Instructions for performing the conversion are straightforward: <a href=""http://wnsqlbuilder.sourceforge.net/howto.html"" rel=""noreferrer"">http://wnsqlbuilder.sourceforge.net/howto.html</a>.  Basically you download the code, decompress it, and run the script.</p>
",2509,1249465114
Different levels in speech recognition software,"<p>There are phonetic level, syntactic level, semantic level, phonological level, acoustic level,
linguistic level, language level. </p>

<p>Are there any other levels? </p>

<p>What's the order from the bottom up?</p>

<p>And what are they really about?</p>
","nlp, speech-recognition","<p>Language admits a lot of diversity, but it also obeys a lot of rules (though often loose ones, with tons of exceptions).  So in a particular language, certain sounds are more likely to follow other sounds, certain words are more likely to follow others, and so on. The levels are basically the level of modeling.  </p>

<p>Acoustic level is trying to determine which acoustic signals are useful for distinguishing human speech.  It tries to answer questions like, ""Is this background noise or a speech sound?""</p>

<p>Phonological level is based on what sounds are most likely to combine together when it is trying to reconstruct an acoustic signal into a sequence of phonemes.  I think this is essentially the same as the phonetic level.</p>

<p>The language level determines what kind of accent the user has, the dialect, etc.</p>

<p>At the syntactic level, you're looking at which words are likely to appear together based on the syntax of the sentence.  This gets rid of words that it would have guessed based on the phonological level but would construct ungrammatical sentences.</p>

<p>The linguistic level as I understand it is more a matter of picking the right word (for example, which homonym <em>our</em> versus <em>hour</em>) based on the context. </p>

<p>At the semantic level, it attempts to model the meaning of the sentence and get rid of things that don't conform to the grammatical relations of verbs and prepositions.  For example, the verb disappear takes no direct object, so if there is something in that semantic slot, there probably is an error.</p>

<p>The order will depend on the application really, some of them may be collapsed into each other, some may not be used at all.  A conceptual hierarchy that makes sense to me is acoustic &lt; phonological = phonetic &lt; language &lt; syntactic &lt; linguistic &lt; semantic.</p>
",976,1249142915
Natural language dates in ruby/rails?,"<p>I need to show natural dates like</p>

<ol>
<li>""few seconds ago""</li>
<li>""21 minutes ago""</li>
</ol>

<p>Is there something built in to the rails? Or may be third party? This is not hard to implement, but I do not want to invent the wheel.</p>
","ruby-on-rails, ruby, datetime, nlp","<p>I think what you are looking for is <a href=""http://apidock.com/rails/ActionView/Helpers/DateHelper/time_ago_in_words"" rel=""nofollow noreferrer""><code>time_ago_in_words</code></a></p>
",902,1248816780
Scope ambiguity in natural language,"<p>I feel it is bit curious to understand the Natural language processing.
I have the following questions..</p>

<p><li>What is meant by Scope ambiguity in natural language?</li>
<li>How can done Statistical resolution of scope ambiguity?</li>
<li>Which is the best language can I use for the Statistical resolution?</li></p>
",nlp,"<p>Scope ambiguity refers to the order of precedence of quantifiers (words like ""a"", ""the"", ""each"", ""some"", ""every"", ""all"", ""one"" etc.) in a natural language sentence.</p>

<p>For example, consider this sentence: ""The dog brings me the newspaper every morning"".</p>

<p>You know that the sentence parses as: ""Exists DOG d ( Foreach MORNING m ( Exists NEWSPAPER n ( d brings n during m ) ) )"". In other words, every morning, the newspaper is different.</p>

<p>But a computer program might instead interpret the sentence to mean ""Exists DOG d ( Exists NEWSPAPER n ( Foreach MORNING m ( d brings n during m ) ) )"" - in other words, there is one old newspaper that you haven't thrown away, and every morning, the dog brings it to you.</p>

<p>Resolving scope ambiguity, as far as I know, is very much an unsolved problem.</p>
",2448,1248237633
Localizing and Globalization of WinForms applications,"<p>We've developed a WinForms application (targeting .NET 2.0 with VS2008), we've just found out that we need to localize it for use in another language (other than english) :( What are the guidelines for developing multi-lingual languages in .NET?</p>

<p>Another application borrows Paint.NET's idea of globalization (using resources) but I was wondering if there are tools out there than can automate this for us - free would be nice but commercial is OK too.</p>

<p>Any ideas?</p>

<p>How do people normally work on projects that require multi-lingual interfaces? We're talking WiNForms apps. Do you just use IsLocalized = true and let .NET handle it?</p>
","winforms, internationalization, globalization, nlp",,2247,1248084970
How can I create relative/approximate dates in Perl?,"<p>I'd like to know if there are any libraries (preferably <a href=""http://search.cpan.org/~drolsky/DateTime-0.50/lib/DateTime.pm"" rel=""nofollow noreferrer"">DateTime</a>-esque) that can take a normal date time and create an appropriate relative human readable date.
Essentially the exact opposite of the more common question: <a href=""https://stackoverflow.com/questions/296738/how-can-i-parse-relative-dates-with-perl"">How can I parse relative dates with Perl?</a>.</p>

<p>Obviously, the exact wording/interpretation is up to the actual implementation, but I'm looking to provide a consistent way to specify dates in the future. Knowing an apporximation like ""<code>due in 2 weeks</code>"" is (to me) more helpful in getting a grasp of how much time I have remaining than something ""<code>due on 2009-07-30</code>"".</p>

<p>Examples:</p>

<pre><code>2009-07-06        =&gt; ""in 1 year""
2009-07-30        =&gt; ""in 2 weeks""
2009-07-09        =&gt; ""tomorrow""
2009-07-09 12:32  =&gt; ""tomorrow at 12:32""
2009-07-12 05:43  =&gt; ""monday morning""
2009-07-03 05:74  =&gt; ""6 days ago""
</code></pre>
","perl, datetime, nlp, relative-date","<p><strong>Update:</strong> It looks like this functionality is implemented in a <a href=""http://search.cpan.org/dist/Template-Plugin-DtFormatter-RelativeDate/"" rel=""noreferrer"">Template Toolkit Plugin</a>. I am leaving the rest of my answer here for reference, but <a href=""http://search.cpan.org/dist/Template-Plugin-DtFormatter/"" rel=""noreferrer"">Template::Plugin::DtFormatter</a> might be the best place to look.</p>

<p>Looking at the source code of that module, I was lead to <a href=""http://search.cpan.org/perldoc?DateTime::Format::Natural"" rel=""noreferrer"">DateTime::Format::Natural</a> which seems related to what you want.</p>

<p><strong>Previous Answer:</strong></p>

<p>Look into <a href=""http://search.cpan.org/perldoc?Date::Calc"" rel=""noreferrer"">Date::Calc</a> to give you deltas using <code>Delta_DHMS</code>. You should be able to use those deltas to choose how you are going to phrase the date.</p>

<p>Here is a very rudimentary starting point. It is buggy but illustrates the basic idea. Add logic to taste.</p>

<pre><code>#!/usr/bin/perl

use strict;
use warnings;

use Date::Calc qw(:all);
use Lingua::EN::Inflect qw( NO );

my @dates = (
    [ 2009, 7,  6 ],
    [ 2009, 7, 30 ],
    [ 2009, 7,  9 ],
    [ 2009, 7,  9, 12, 32 ],
    [ 2009, 7, 12,  5, 43 ],
    [ 2009, 7,  3,  5, 14 ],
    [ 2010, 8, 9 ],
    [ 2012, 8, 9 ],
    [ 2013, 8, 9 ],
);

for my $date ( @dates ) {
    print ""@$date: "", relative_when( $date ), ""\n"";
}

sub relative_when {
    my ($year, $month, $day, $hour, $min, $sec) = @{ $_[0] };
    my ($Dyear, $Dmon, $Dday, $Dhr, $Dmin, $Dsec) = Delta_YMDHMS(
        Today_and_Now(),
        $year, $month, $day, $hour || 0, $min || 0, $sec || 0
    );
    return NO('year',  $Dyear )     if $Dyear &gt; 0;
    return NO('month', $Dmon )      if $Dmon  &gt; 0;
    return NO('week',  int($Dday/7) if $Dday  &gt; 6;
    return NO('day',   $Dday)       if $Dday  &gt; 1;
    return 'tomorrow' if $Dday == 1;
    return 'today'    if $Dday == 0;
    return """";
}

__END__
</code></pre>

<p>Output:</p>

<pre><code>C:\Temp&gt; dfg
2009 7 6:
2009 7 30: 2 weeks
2009 7 9: today
2009 7 9 12 32: today
2009 7 12 5 43: 2 days
2009 7 3 5 14:
2010 8 9: 1 year
2012 8 9: 3 years
2013 8 9: 4 years
</code></pre>
",970,1247151524
Very basic English grammar parser,"<p>I'm writing a very basic parser(mostly just to better understand how they work) that takes a user's input of a select few words, detects whether the sentence structure is OK or Not OK, and outputs the result. The grammar is:</p>

<p>Sentence:
Noun Verb</p>

<p>Article Sentence</p>

<p>Sentence Conjunction Sentence</p>

<p>Conjunction:
""and""
""or""
""but""</p>

<p>Noun:
""birds""
""fish""
""C++""</p>

<p>Verb:
""rules""
""fly""
""swim""</p>

<p>Article:
""the""</p>

<p>Writing the grammar was simple. It's implementing the code that is giving me some trouble. My psuedocode for it is:</p>

<pre><code>main()
get user input (string words;)
while loop (cin &gt;&gt; words)
call sentence()
end main()

sentence()
call noun()
if noun() call verb() (if verb is true return ""OK"" ???)(else ""not ok""???)
else if not noun() call article()
                if article() call sentence() (if sentence is true ""OK""???)(else ""not""?)
else if not noun() call conjunction()
                   if sentence() conjunction() sentence() - no idea how to implement
                                                             return ""OK""
else ""not ok""
</code></pre>

<p>So there is my extremely sloppy psuedo code. I have a few questions on implementing it.</p>

<ol>
<li><p>For the word functions (noun, verb, etc.) how should I go about checking if they are true? (as in checking if the user's input has birds, fish, fly, swim, etc.)</p></li>
<li><p>How should I handle the conjunction call and the output?</p></li>
<li><p>Should I handle the output from the main function or the call functions?</p></li>
<li><p>None of the above questions matter if my psuedo code is completely wrong. Is there anything wrong with the basics?</p></li>
</ol>

<p>As an added note, I'm on a Chapter 6 exercise of Programming: Practice and Principles Using C++ so I'd prefer to use language syntax that I've already learned, so anything that falls into the category of advanced programming probably isn't very helpful. (The exercise specifically says not to use tokens, so count those out.)</p>

<p>Thanks in advance</p>

<p>Last Edit: In the book's public group I asked the same question and Bjarne Stroustrup commented back saying he put the exercise solution online. He basically had the input read into the sentence function and used if statements to return true or false. However, he didn't use articles so mine was much more complex. I guess if I've learned anything from this exercise its that when dealing with a lot of user input, tokenization is key (from what I know so far.) Here is my code for now. I may go back to it later because it is still very buggy and basically only returns if the sentence is OK and can't handle things like (noun, conjunction, sentence), but for now I'm moving on.</p>

<pre><code>#include ""std_lib_facilities.h""

bool article(string words)
{
               if (words == ""the"")
               return true;
               else return false;        
}

bool verb(string words)
{
               if (words == ""rules"" || words == ""fly"" || words == ""swim"")
               return true;
               else return false;                   
}

bool noun(string words)
{
               if (words == ""birds"" || words == ""fish"" || words == ""c++"")
               return true;
               else return false;                   
}

bool conjunction(string words)
{
              if (words == ""and"" || words == ""but"" || words == ""or"")
              return true;
              else return false;                  
}

bool sentence()
{
string w1;
string w2;
string w3;
string w4;

cin &gt;&gt; w1;
if (!noun(w1) &amp;&amp; !article(w1)) return false; // grammar of IFS!

cin &gt;&gt; w2;
if (noun(w1) &amp;&amp; !verb(w2)) return false;
if (article(w1) &amp;&amp; !noun(w2)) return false;

cin &gt;&gt; w3;
if (noun(w1) &amp;&amp; verb(w2) &amp;&amp; (w3 == ""."")) return true;
if (verb(w2) &amp;&amp; !conjunction(w3)) return false;
if (noun(w2) &amp;&amp; !verb(w3)) return false;
if (conjunction(w3)) return sentence();

cin &gt;&gt; w4;
if (article(w1) &amp;&amp; noun(w2) &amp;&amp; verb(w3) &amp;&amp; (w4 == ""."")) return true;
if (!conjunction(w4)) return false;
if (conjunction(w4)) return sentence();
}


int main()
{                                   
cout &lt;&lt; ""Enter sentence. Use space then period to end.\n"";
            bool test = sentence();
            if (test)
               cout &lt;&lt; ""OK\n"";
            else
               cout &lt;&lt; ""not OK\n"";
</code></pre>

<p>keep_window_open();
    }</p>
","c++, parsing, nlp",,10416,1245773821
Finding bigram in a location index,"<p>I have a table which indexes the locations of words in a bunch of documents.
I want to identify the most common bigrams in the set.</p>

<p>How would you do this in <strong>MSSQL 2008</strong>?
the table has the following structure:</p>

<pre><code>LocationID -&gt; DocID -&gt; WordID -&gt; Location
</code></pre>

<p>I have thought about trying to do some kind of complicated join... and i'm just doing my head in.</p>

<p>Is there a simple way of doing this?</p>

<p>I think I better edit this on monday inorder to bump it up in the questions</p>

<p><strong>Sample Data</strong></p>

<pre><code>LocationID  DocID   WordID  Location
21952       534     27  155
21953       534         109     156
21954       534       4     157
21955       534     45      158
21956       534     37      159
21957       534     110     160
21958       534     70      161
</code></pre>
","sql-server, algorithm, nlp","<p>It's been years since I've written SQL, so my syntax may be a bit off; however, I believe the logic is correct.</p>

<pre><code>SELECT CONCAT(i.WordID, ""|"", j.WordID) as bigram, count(*) as freq
FROM index as i, index as j
WHERE j.Location = i.Location+1 AND 
      j.DocID = i.DocID
GROUP BY bigram
ORDER BY freq DESC
</code></pre>

<p>You can also add the actual word IDs to the select list if that's useful, and add a join to whatever table you've got that dereferences WordID to actual words.</p>
",635,1244878265
Multi layer perceptron for OCR,"<p>I intend to use a multi layer perceptron network trained with backpropagation (one hidden layer, inputs served as 8x8 bit matrices containing the B/W pixels from the image). The following questions arise:</p>

<ol>
<li>which type of learning should I use: batch or on-line?</li>
<li>how could I estimate the right number of nodes in the hidden layer? I intend to process the 26 letter of english alphabet.</li>
<li>how could I stop the training process, to avoid overfitting?</li>
<li>(not quite related) is there another better NN prved to perform better than MLP? I know about MLP stucking in local minima, overfitting and so on, so is there a better (soft computing-based) approach?</li>
</ol>

<p>Thanks</p>
","nlp, ocr, backpropagation, neural-network",,1742,1238266902
Produce a sentence from a grammar with a given number of terminals,"<p>Say you've got a toy grammar, like: <em>(updated so the output looks more natural)</em></p>

<pre><code>S -&gt; ${NP} ${VP} | ${S} and ${S} | ${S}, after which ${S}

NP -&gt; the ${N} | the ${A} ${N} | the ${A} ${A} ${N}

VP -&gt; ${V} ${NP}

N -&gt; dog | fish | bird | wizard

V -&gt; kicks | meets | marries

A -&gt; red | striped | spotted
</code></pre>

<p>e.g., ""the dog kicks the red wizard"", ""the bird meets the spotted fish or the wizard marries the striped dog""</p>

<p>How can you produce a sentence from this grammar according to the constraint that it must contain a total of <em>n</em> Vs + As + Ns. Given an integer the sentence must contain that many terminals. (note of course in this grammar the minimum possible <em>n</em> is 3).</p>
","algorithm, language-agnostic, parsing, nlp, grammar","<p>The following Python code will generate a random sentence with the given number of terminals.
It works by counting the number of ways to produce a sentence of a given length, generating a large random number, and computing the indicated sentence.
The count is done recursively, with memoization.
An empty right hand side produces 1 sentence if n is 0 and 0 sentences otherwise.
To count the number of sentences produced by a nonempty right hand side, sum over i, the number of terminals used by the first symbol in the right hand side.
For each i, multiply the number of possibilities for the rest of the right hand side by the number of possibilities for the first symbol.
If the first symbol is a terminal, there is 1 possibility if i is 1 and 0 otherwise.
If the first symbol is a nonterminal, sum the possibilities over each alternative.
To avoid infinite loops, we have to be careful to prune the recursive calls when a quantity is 0.
This may still loop infinitely if the grammar has infinitely many derivations of one sentence.
For example, in the grammar</p>

<pre><code>S -&gt; S S
S -&gt;
</code></pre>

<p>there are infinitely many derivations of the empty sentence: S => , S => S S => , S => S S => S S S => , etc.
The code to find a particular sentence is a straightforward modification of the code to count them.
This code is reasonably efficient, generating 100 sentences with 100 terminals each in less than a second.</p>

<pre><code>import collections
import random

class Grammar:
    def __init__(self):
        self.prods = collections.defaultdict(list)
        self.numsent = {}
        self.weight = {}

    def prod(self, lhs, *rhs):
        self.prods[lhs].append(rhs)
        self.numsent.clear()

    def countsent(self, rhs, n):
        if n &lt; 0:
            return 0
        elif not rhs:
            return 1 if n == 0 else 0
        args = (rhs, n)
        if args not in self.numsent:
            sym = rhs[0]
            rest = rhs[1:]
            total = 0
            if sym in self.prods:
                for i in xrange(1, n + 1):
                    numrest = self.countsent(rest, n - i)
                    if numrest &gt; 0:
                        for rhs1 in self.prods[sym]:
                            total += self.countsent(rhs1, i) * numrest
            else:
                total += self.countsent(rest, n - self.weight.get(sym, 1))
            self.numsent[args] = total
        return self.numsent[args]

    def getsent(self, rhs, n, j):
        assert 0 &lt;= j &lt; self.countsent(rhs, n)
        if not rhs:
            return ()
        sym = rhs[0]
        rest = rhs[1:]
        if sym in self.prods:
            for i in xrange(1, n + 1):
                numrest = self.countsent(rest, n - i)
                if numrest &gt; 0:
                    for rhs1 in self.prods[sym]:
                        dj = self.countsent(rhs1, i) * numrest
                        if dj &gt; j:
                            j1, j2 = divmod(j, numrest)
                            return self.getsent(rhs1, i, j1) + self.getsent(rest, n - i, j2)
                        j -= dj
            assert False
        else:
            return (sym,) + self.getsent(rest, n - self.weight.get(sym, 1), j)

    def randsent(self, sym, n):
        return self.getsent((sym,), n, random.randrange(self.countsent((sym,), n)))

if __name__ == '__main__':
    g = Grammar()
    g.prod('S', 'NP', 'VP')
    g.prod('S', 'S', 'and', 'S')
    g.prod('S', 'S', 'after', 'which', 'S')
    g.prod('NP', 'the', 'N')
    g.prod('NP', 'the', 'A', 'N')
    g.prod('NP', 'the', 'A', 'A', 'N')
    g.prod('VP', 'V', 'NP')
    g.prod('N', 'dog')
    g.prod('N', 'fish')
    g.prod('N', 'bird')
    g.prod('N', 'wizard')
    g.prod('V', 'kicks')
    g.prod('V', 'meets')
    g.prod('V', 'marries')
    g.prod('A', 'red')
    g.prod('A', 'striped')
    g.prod('A', 'spotted')
    g.weight.update({'and': 0, 'after': 0, 'which': 0, 'the': 0})
    for i in xrange(100):
        print ' '.join(g.randsent('S', 3))
</code></pre>
",644,1244489798
Extract small relevant bits text (as Google does) from the full text search results,"<p>I have implemented a full text search in a discussion forum database and I want to display
the search results in a way Google does. Even for a very long html page only a two or three
lines of the texts displayed in a search result list. Usually these are the lines
which contain a search terms. </p>

<p>What would be the good algorithm of how to extract a few lines of the text based on the text itself and a search terms. I could think of something as easy as just using one line of text before the search term occurrence in a text and a line after - but that seems to be too simple to work.</p>

<p>Would like to get a few directions, ideas and insights. </p>

<p>Thank you.</p>
","algorithm, search, full-text-search, nlp, data-mining",,485,1244178712
Natural language date parser for ruby/rails,"<p>Does anybody know of something similar to <a href=""http://www.datejs.com/"" rel=""nofollow noreferrer"">Date.js</a> in Ruby? Something that would be able to return a date object from something like: ""two weeks from today"". The Remember the Milk webapp incorporates this feature into their system and it is incredibly easy to use.</p>

<p>I would use the Date.js library itself but because it is on the client side it has its limitations. If the user doesn't have javascript enabled the functionality would be lost. This would affect mobile phone users who would, ideally, use our system via text message (sms).</p>

<p>I would love to use a solution that's already out there but if not how hard would it be to port this code into Ruby? I really don't know much about natural language interpretation but it seems like it would take some time.</p>

<p>Thanks.</p>
","ruby-on-rails, ruby, datetime, nlp","<p>Do not forget that everything being an object in Ruby, you have already some pretty readable statement from the language itself:</p>

<pre><code>2.weeks.from_now.utc
</code></pre>

<p>would be the equivalent of ""two weeks from today"".</p>

<p>However, for <em>real</em> natural language, may be <strong><a href=""http://www.yup.com/articles/2006/09/10/a-natural-language-date-time-parser-for-ruby-chronic"" rel=""noreferrer"">chronic</a></strong> might be a more specialized library at <a href=""http://chronic.rubyforge.org/"" rel=""noreferrer"">rubyforge</a>.</p>

<pre><code>Chronic.parse('this tuesday 5:00')
#=&gt; Tue Aug 29 17:00:00 PDT 2006
</code></pre>
",1835,1236063350
Appropriate article (a/an) in String.Format,"<p>I'm looking for a culturally-sensitive way to properly insert a noun into a sentence while using the appropriate article (a/an). It could use String.Format, or possibly something else if the appropriate way to do this exists elsewhere.</p>

<p>For example:</p>

<p>Base Sentence: ""You are looking at a/an {0}""</p>

<p>This should format to: ""You are looking at a carrot"" or ""You are looking at an egg.""</p>

<p>I'm currently doing this by manually checking the first character of the word to be inserted and then manually inserting ""a"" or ""an."" But I'm concerned that this might limit me when the application is localized to other languages.</p>

<p>Is there a best practice for approaching this problem?</p>

<p>RESOLUTION: It appears that the problem is complicated to the point that there does not exist a utility or framework to solve this problem in the way I originally phrased. It appears that the best solution (in my situation) is to store the article in the database along with the noun so that the translators can have the level of control they need. Thanks for all of the suggestions!</p>
","language-agnostic, localization, grammar, nlp","<p>Besides the problem noted by lc (an hour, a hat) the grammar rules in different languages vary widely.  For instance, many Latin based languages switch articles of nouns based on the 'gender' and 'number' of the noun, which can sometimes be inferred from the last few characters of the word but has the same problem as English...there are many exceptions.</p>

<p>If you are talking about localizing an interface I would store the article with the noun for the interface element in each language.  If you are processing user input I don't see an easy way to do this.</p>
",664,1241542041
Rhyme in PHP,"<p>I am having a hard time to find a way to detect if <strong>two words has the same rhyme in English</strong>. It has not to be the same syllabic ending but something closer to <strong>phonetically similarity</strong>. </p>

<p>I can not believe in 2009 the only way of doing it is using those old fashioned rhyme dictionaries. Do you know any resources (in PHP would be a plus) to help me in this painful task?</p>

<p>Thank you.</p>

<p>Your hints were all really hepful. I will take some time to investigate it. Anyway, more info about DoubleMetaPhone can be found <a href=""http://swoodbridge.com/DoubleMetaPhone/"" rel=""nofollow noreferrer"">here in a proper PHP code</a> (the other one is an extension).
There are interesting information about MethaPhone function and doublemetaphone <a href=""http://es2.php.net/metaphone"" rel=""nofollow noreferrer"">in Php.net</a>.</p>

<p>They specially alert about how slow double metaphone is compared with metaphone (something like 100 times slower).</p>
","php, nlp","<p>Soundex won't help you. Soundex focuses on the beginning of the word, not its ending. Generally it think you'll have hard time finding any tool to do this. Even to the linguist the root of the word is more interesting, than it's ending. </p>

<p>Generally what you'll have to do is to divide words in syllables and compare their last syllable. Even better if you could divide it in phonemes, reverse their order and do comparison on reversed word. <br/> You might trying comparing last part of <a href=""http://pecl.php.net/package/doublemetaphone"" rel=""nofollow noreferrer"">metaphone keys</a>.</p>
",1895,1240303410
"The lines that stand out in a file, but aren&#39;t exact duplicates","<p>I'm combing a webapp's log file for statements that stand out.</p>

<p>Most of the lines are similar and uninteresting. I'd pass them through Unix <code>uniq</code>, however that filters nothing, as all the lines are slightly different: they all have a different timestamp, similar statements might print a different user ID, etc.</p>

<p>What's a way and/or tool to get just the lines that are notably different from any other? (But, again, not precise duplicates)</p>

<p>I was thinking about playing with Python's <a href=""http://docs.python.org/library/difflib.html"" rel=""nofollow noreferrer"">difflib</a> but that seems geared toward diffing two files, rather than all pairs of lines in the same file.</p>

<p>[EDIT]</p>

<p>I assumed the solution would give a uniqueness score for each line. So by ""notably different"" I meant, I choose a threshold that the uniqueness score must exceed for any line to be included in the output.</p>

<p>Based on that, if there are other viable ways to define it, please discuss. Also, the method doesn't have to have 100% accuracy and recall.</p>

<p>[/EDIT]</p>

<p>Examples:</p>

<p>I'd prefer answers that are as general purpose as possible. I know I can strip away the timestamp at the beginning. Stripping the end is more challenging, as its language may be absolutely unlike anything else in the file. These sorts of details are why I shied from concrete examples before, but because some people asked...</p>

<p>Similar 1:</p>

<pre><code>2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234
2009-04-20 00:04:02 INFO  com.foo.Bar - URL:/graph?id=asdfghjk
</code></pre>

<p>Similar 2:</p>

<pre><code>2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses
2009-04-20 00:06:00 INFO  com.baz.abc.Accessor - Cache /path/to/some/different/dir hits: 4352685 / 271315, 0.004423% misses
</code></pre>

<p>Different 1:</p>

<pre><code>2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234
2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses
</code></pre>

<p>In the Different 1 case, I'd like both lines returned but not other lines like them. In other words, those 2 lines are distinct types (then I can later ask for only statistically rare line types). The edit distance is much bigger between those two, for one thing.</p>
","python, algorithm, unix, grep, nlp","<p>I don't know a tool for you but if <em>I</em> were going to roll my own, I'd approach it like this:</p>

<p>Presumably the log lines have a well defined structure, no? So</p>

<ul>
<li>parse the lines on that structure</li>
<li>write a number of very basic relevance filters (functions that just return a simple number from the parsed structure)</li>
<li>run the parsed lines through a set of filters, and cut on the basis of the total score</li>
<li>possibly sort the remaining lines into various bins by the results of more filters</li>
<li>generate reports, dump bins to files, or other output</li>
</ul>

<p>If you are familiar with the unix tool <code>procmail</code>, I'm suggesting a similar treatment customized for your data.</p>

<hr>

<p>As zacherates notes in the comments, your filters will typically ignore time stamps (and possibly IP address), and just concentrate on the content: for example <em>really</em> long http requests might represent an attack...or whatever applies to your domain.</p>

<p>Your binning filters might be as simple as a hash on a few selected fields, or you might try to do something with <a href=""https://stackoverflow.com/questions/769775/the-lines-that-stand-out-in-a-file-but-arent-exact-duplicates/769790#769790"">Charlie Martin's suggestion</a> and used edit distance measures.</p>
",351,1240255837
Pulling stats out of a text,"<p>I'd like to know what are the most recurrent in a given text or group of text (pulled from a database) in ruby.</p>

<p>Does anyone know what are the best practices?</p>
","ruby, nlp","<p>You might start with <a href=""http://en.wikipedia.org/wiki/Stochastic_grammar"" rel=""nofollow noreferrer"">statistical natural language processing</a>. Also, you may be able to leverage one or more of the libraries mentioned on the <a href=""http://web.media.mit.edu/~dustin/rubyai.html"" rel=""nofollow noreferrer"">AI Ruby Plugins</a> page.</p>
",70,1240060802
How to analyze simple English sentences,"<p>Is there any library that can be used for analyzing (nlp) simple english text. For example it would be perfect if it can do that;
Input: ""I am going""
Output: I, go, present continuous tense</p>
","java, nlp","<p>How about the <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">Natural Language Toolkit</a>?</p>
",1960,1239655468
Match rows containing a word with permutations,"<p>Say you've got a big table that contains a varchar column.</p>

<p>How would you match rows that contain the word 'preferred' in the varchar col BUT the data is somewhat noisy and contains occasional spelling errors, e.g.:</p>

<pre><code>['$2.10 Cumulative Convertible Preffered Stock, $25 par value',
'5.95% Preferres Stock',
'Class A Preffered',
'Series A Peferred Shares',
'Series A Perferred Shares',
'Series A Prefered Stock',
'Series A Preffered Stock',
'Perfered',
'Preffered  C']
</code></pre>

<p>The permutations of the word 'preferred' in the spelling errors above appear to exhibit a <a href=""http://en.wikipedia.org/wiki/Family_resemblance"" rel=""nofollow noreferrer"">family resemblance</a> but there's very little that they all have in common. Note that splitting out every word and running <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">levenshtein</a> on every word in every row is going to be prohibitively expensive.</p>

<p>UPDATE:</p>

<p>There are a couple of other examples like this, e.g. with 'restricted':</p>

<pre><code>['Resticted Stock Plan',
'resticted securities',
'Ristricted Common Stock',
'Common stock (restrticted, subject to vesting)',
'Common Stock (Retricted)',
'Restircted Stock Award',
'Restriced Common Stock',]
</code></pre>
","nlp, information-retrieval",,267,1239228737
Non regular context-free language and infinite regular sublanguages,"<p>I had a work for the university which basically said:  </p>

<p>""Demonstrates that the non-regular language L={0^n 1^n : n natural} had no infinite regular sublanguages.""</p>

<p>I demonstrated this by contradiction.  I basically said that there is a language S which is a sublanguage of L and it is a regular language.  Since the possible Regular expressions for S are 0*, 1*, (1+0)* and (0o1)*.  I check each grammar and demonstrate that none of them are part of the language L.  </p>

<p>However, how I could prove that ANY non regular context free language could not contain any regular infinite sublanguages?</p>

<p>I don't want the prove per se, I just want to be pointed in the right direction.</p>
","regex, nlp, context-free-grammar","<p>L = {0^n 1^n : n natural} is non-regular context free.</p>

<p>M = 2*3* is infinite regular.</p>

<p>N = L∪M is non-regular context free.  N contains M.</p>
",4394,1237423888
Can anyone point me at a good example of pretty printing rules to &quot;english&quot;,"<p>I've got the equivalent of an AST that a user has built using a rule engine.  But when displaying a list of the rules, I'd like to be able to ""pretty print"" each rule into something that looks nice**.  Internally when represented as a string they look like s-expressions so imagine something like:</p>

<pre><code>(and (contains ""foo"" ""foobar"") (equals 4 (plus 2 2 )))
</code></pre>

<p>Can anyone point me at a program that has done a good job of displaying rules in a readable fashion?</p>

<p>** Needs to be localizable too, but I guess we'll leave that for extra credit.</p>
","nlp, pretty-print, rule-engine",,385,1225823584
NLP: Morphological manipulations,"<p>I am trying to build an NLP system for an assignment, for which I am allowed to use external libraries.<br>
I am using parse trees to break down sentences into their constituent parts down to nouns, verbs, etc.<br>
I am looking for a library or software that would let me identify which lexical form a word is in, and possibly translate it to some other form for me.<br>
Basically, I need something with functions like isPlural, singularize, getInfinitive, etc.<br>
I have considered the Ruby Linguistics package and a simple Porter Stemmer (for infinitives) but neither is very good.<br>
This does not seem like a very hard problem, just very tedious.<br>
Does anyone know of a good package/library/software that could do things like that?</p>
",nlp,"<p>Typically, in order to build a parse tree of a sentence, one needs to first determine the part-of-speech and lemma information of the words in the sentence. So, you should have this information already.</p>

<p>But in any case, in order to map wordforms to their lemmas, and synthesize wordforms from lemmas, take a look at <a href=""http://www.informatics.sussex.ac.uk/research/groups/nlp/carroll/morph.html"" rel=""nofollow noreferrer""><code>morpha</code> and <code>morphg</code></a>, and also the Java version of (or front-end to) <code>morphg</code> contained in the <a href=""http://www.csd.abdn.ac.uk/~ereiter/simplenlg/"" rel=""nofollow noreferrer"">SimpleNLG package</a>. There are methods like <code>getInfinitive</code>, <code>getPastParticiple</code>, etc. See e.g. the <a href=""http://www.csd.abdn.ac.uk/~ereiter/simplenlg/api/simplenlg/lexicon/lexicalitems/Verb.html"" rel=""nofollow noreferrer"">API for the Verb class</a>.</p>
",559,1237325753
Why some countries have dot as a decimal separator and some have comma?,"<p>Why in some countries there is a comma separator and in some dot? Do you know what is the reason of that? It's very annoying to check every time if you should use this or this.</p>
","internationalization, nlp","<p>I would suggest reading the <a href=""http://en.wikipedia.org/wiki/Decimal_separator#History"" rel=""noreferrer"">Wikipedia entry</a> on the history of the decimal separator. Basically, it boils down to different choices made when typesetting what was previously written by hand.</p>
",1614,1237284718
Parsing expressions with an undefined number of arguments,"<p>I'm trying to parse a string in a self-made language into a sort of tree, e.g.:</p>

<pre><code># a * b1 b2 -&gt; c * d1 d2 -&gt; e # f1 f2 * g
</code></pre>

<p>should result in:</p>

<pre><code># a
  * b1 b2
    -&gt; c
  * d1 d2
    -&gt; e
# f1 f2
  * g
</code></pre>

<p>#, * and -> are symbols. a, b1, etc. are texts.</p>

<p>Since the moment I know only rpn method to evaluate expressions, and my current solution is as follows. If I allow only a single text token after each symbol I can easily convert expression first into RPN notation (b = b1 b2; d = d1 d2; f = f1 f2) and parse it from here:</p>

<p>a b c -> * d e -> * # f g * #</p>

<p>However, merging text tokens and whatever else comes seems to be problematic. My idea was to create marker tokens (M), so RPN looks like:</p>

<p>a M b2 b1 M c -> * M d2 d1 M e -> * # f2 f1 M g * #</p>

<p>which is also parseable and seems to solve the problem.</p>

<p>That said:</p>

<ol>
<li>Does anyone have experience with something like that and can say it is or it is not a viable solution for the future?</li>
<li>Are there better methods for parsing expressions with undefined arity of operators?</li>
<li>Can you point me at some good resources?</li>
</ol>

<p>Note. Yes, I know this example very much resembles Lisp prefix notation and maybe the way to go would be to add some brackets, but I don't have any experience here. However, the source text must not contain any artificial brackets and also I'm not sure what to do about potential infix mixins like # a * b -> [if value1 =  value2] c -> d.</p>

<p>Thanks for any help.</p>

<p>EDIT: It seems that what I'm looking for are sources on postfix notation with a variable number of arguments.</p>
","algorithm, parsing, nlp, rpn","<p><s>I couldn't fully understand your question, but it seems what you want is a grammar definition and a parser generator. I suggest you take a look at <a href=""http://www.antlr.org/"" rel=""nofollow noreferrer"">ANTLR</a>, it should be pretty straightforward with it to define a grammar for either your original syntax or the RPN.</s></p>
<p><strong>Edit:</strong> (After exercising self-criticism, and making some effort to understand the question details.) Actually, the language grammar is unclear from your example. However, it seems to me, that the advantages of the prefix/postfix notations (i.e. that you need neither parentheses nor a precedence-aware parser) stem from the fact that you <em>know the number of arguments</em> every time you encounter an operator, therefore you know exactly how many elements to read (for prefix notation) or to pop from the stack (for postfix notation). OTOH, I beleive that having operators which can have variable number of arguments makes prefix/postfix notations not simply difficult to parse but outright ambiguous.  Take the following expression for example:</p>
<pre><code># a * b c d
</code></pre>
<p>Which of the following three is the canonical form?</p>
<ol>
<li><h1>(a, *(b, c, d))</h1>
</li>
<li><h1>(a, *(b, c), d)</h1>
</li>
<li><h1>(a, *(b), c, d)</h1>
</li>
</ol>
<p>Without knowing more about the operators, it is impossible to tell. Of course you could define some sort of greedyness of the operators, e.g. * is greedier than #, so it gobbles up all the arguments. But this would beat the purpose of a prefix notation, because you simply wouldn't be able to write down the second variant from the above three; not without additinonal syntactic elements.</p>
<p>Now that I think of it, it is probably not by sheer chance that none of the programming languages I know support <em>operators</em> with a variable number of arguments, only <em>functions/procedures</em>.</p>
",1001,1237370346
Online (preferably) lookup API of a word&#39;s class,"<p>I have a list of words and I want to filter it down so that I only have the nouns from that list of words (Using Java).  To do this I am looking for an easy way to query a database of words for their type.</p>

<p>My question is does anybody know of a free, easy word lookup API that would enable me to find the <em>class</em> of a word, not necessarily its semantic definition.</p>

<p>Thanks!</p>

<p>Ben.  </p>

<p>EDIT: By class of the word I meant 'part-of-speech' thanks for clearing this up</p>
","java, web-services, nlp","<p>Word type? Such as verb, noun, adjective, etc? If so, you might run into the issue that some words can be used in more than one way. For example: ""Can you trade me that card?"", ""That was a bad trade.""</p>

<p>See <a href=""https://stackoverflow.com/questions/216612/dictionary-api-or-library"">this thread</a> for some suggestions.</p>

<p>Have a look at <a href=""http://wordnetweb.princeton.edu/perl/webwn"" rel=""nofollow noreferrer"">this</a> as well, seems like it might do exactly what you're looking for.</p>
",2676,1236195433
Stemming - code examples or open source projects?,"<p>Stemming is something that's needed in tagging systems.  I use delicious, and I don't have time to manage and prune my tags.  I'm a bit more careful with my blog, but it isn't perfect.  I write software for embedded systems that would be much more functional (helpful to the user) if they included stemming.</p>

<p>For instance:<br>
Parse<br>
Parser<br>
Parsing  </p>

<p>Should all mean the same thing to whatever system I'm putting them into.</p>

<p>Ideally there's a BSD licensed stemmer somewhere, but if not, where do I look to learn the common algorithms and techniques for this?</p>

<p>Aside from BSD stemmers, what other open source licensed stemmers are out there?</p>

<p>-Adam</p>
","algorithm, tags, nlp, stemming","<p><a href=""http://snowball.tartarus.org/"" rel=""noreferrer"">Snowball</a> stemmer (C &amp; Java)
I've used it's Python binding, <a href=""http://pypi.python.org/pypi/PyStemmer"" rel=""noreferrer"">PyStemmer</a></p>
",8329,1235746806
NLP classify sentences/paragraph as funny,"<p>Is there a way to classify a particular sentence/paragraph as funny. There are very few pointers as to where one should go further on this.</p>
","nlp, classification","<p>There is research on this, it's called <a href=""http://en.wikipedia.org/wiki/Computational_Humour"" rel=""noreferrer"">Computational Humor</a>. It's an interdisciplinary area that takes elements from computational linguistics, psycholinguistics, artificial intelligence, machine learning etc. They are trying to find out what it is that makes stories or jokes funny (e.g. the unexpected connection, or using a taboo topic in a surprising way etc) and apply it to text (either to generate a funny story or to measure the 'funniness' of text). </p>

<p>There are <a href=""http://www.springerlink.com/content/d1m5v3341gp7w674/"" rel=""noreferrer"">books</a> and articles about it (e.g. by <a href=""http://www.csd.abdn.ac.uk/~gritchie/papers/index.html"" rel=""noreferrer"">Graeme Ritchie</a>).</p>
",2897,1234610041
Algorithm for separating nonsense text from meaningful text,"<p>I provided some of my programs with a feedback function. Unfortunately I forgot to include some sort of spam-protection - so users could send  anything they wanted to my server - where every feedback is stored in a huge db.  </p>

<p>In the beginning I periodically checked those feedbacks - I filtered out what was usable and deleted garbage. The problem is: I get 900 feedbacks per day. Only 4-5 are really useful, the other messages are mostly 2 type of gibberish:</p>

<ul>
<li>nonsense: jfvgasdjkfahs kdlfjhasdf  (People smashing their heads on the keyboard)</li>
<li>language i don't understand</li>
</ul>

<p>What I did so far: </p>

<ol>
<li><p>I installed a filter to delete any feedback containing ""asdf"", ""qwer"" etc... -> only 700 per day</p></li>
<li><p>I installed a word filter to delte anything containing bad language -> 600 per day (don't ask - but there are many strange people out there)</p></li>
<li>I filter out any messages containing letters not being used in my language -> 400 per day</li>
</ol>

<p>But 400 per day is still way too much. So I'm wondering if anybody has dealt with such a problem before and knows some sort of algorithm to filter out senseless messages.</p>

<p>Any help would really be appreciated!</p>
","algorithm, filter, cpu-word, nlp, spam","<p>How about just using some existing implementation of a bayesian spam filter instead of implementing your own. I have had good results with DSpam</p>
",4235,1233525948
"Contextual Natural Language Resources, Where Do I Start?","<p>Where can i find some .Net or conceptual resources to start working with Natural Language where I can pull context and subjects from text. I wish not to work with word frequency algorithms.</p>
",".net, nlp","<p>To find resources in part of speech tagging (a natural language processing task) look at this: </p>

<ul>
<li><a href=""http://nltk.sourceforge.net/index.php/Book"" rel=""nofollow noreferrer"">Natural Language Toolkit</a></li>
<li><a href=""http://search.cpan.org/dist/Lingua-EN-Tagger/Tagger.pm"" rel=""nofollow noreferrer"">PoS tagger in perl</a></li>
<li><a href=""http://www.lsi.upc.es/~nlp/SVMTool/"" rel=""nofollow noreferrer"">SVM Tool</a></li>
<li><a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer"">NLP Group at Stanford</a></li>
</ul>

<p>Hope it helps.</p>
",323,1222264104
Tool or methods for automatically creating contextual links within a large corpus of content?,"<p>Here's the basic scenario - I have a corpus of say 100,000 newspaper-like articles.  Minimally they will all have a well-defined title, and some amount of body content.  </p>

<p>What I want to do is find runs of text in articles that ought to link to other articles.</p>

<p>So, if article Foo has a run of text like ""Students in 8th grade are being encouraged to read works by John-Paul Sartre"" and article Bar is titled (and about) ""The important works of John-Paul Sartre"", I'd like to automagically create that HTML link from Foo to Bar within the text of Foo.  </p>
","seo, nlp",,248,1231788800
How does Google&#39;s In Quotes work?,"<p>I find Google's <a href=""http://labs.google.com/inquotes/"" rel=""nofollow noreferrer"">In Quotes</a> a really nifty application, and as a CS guy, I have to understand how it works. How do you think it turns news articles into a list of quotes attributed to specific persons?
Sure, there are some mistakes, but their algorithm seems to be smarter than just a simple heuristic or multiple regular expressions. For example, a quote can be attributed to someone even though his/her name was only mentioned in the last paragraph.</p>

<p>Any ideas? Any known paper on the subject?</p>
","algorithm, nlp",,611,1224955538
End user tool for generating a regular expression,"<p>We have a SaaS application requirement to allow a user responsible for building a CMS site to define up to 10 custom fields in a form. 
As part of this field definition we want to add a field validation option which we will store (and apply at runtime) as a reg-ex.</p>

<p>Are there any tools, code samples or similar that offer a wizard style front end for building a reg-ex. We are looking to embed a control or code into our .NET site that will generate the reg-ex from (pseudo) user friendly terms (close to natural language if possible).</p>

<p>e.g.
Field 1 = (5 alphanumerics) followed-by (1 to 3 numerics) followed by ""-"" followed by 1 alpha</p>
",".net, regex, nlp",,570,1228412131
Shell script to find bigrams,"<p>I'm making a shell script to find bigrams, which works, sort of.</p>

<pre><code>#tokenise words
tr -sc 'a-zA-z0-9.' '\012' &lt; $1 &gt; out1
#create 2nd list offset by 1 word
tail -n+2 out1 &gt; out2
#paste list together
paste out1 out2 
#clean up
rm out1 out2
</code></pre>

<p>The only problem is that it pairs words from the end and start of the previous sentence.</p>

<p>eg for the two sentences 'hello world.' and 'foo bar.' i'll get a line with ' world. foo'. Would it be possible to filter these out with grep or something?</p>

<p>I know i can find all bigrams containing a full stop with grep [.] but that also finds the legitimate bigrams.</p>
","shell, grep, nlp","<p>Just replace the paste line with this:</p>

<pre><code>paste out1 out2 | grep -v '\..'
</code></pre>

<p>This will filter out any lines that contain a period which is not the last character of a line.</p>
",3029,1225232197
Natural Language/Text Mining and Reddit/social news site,"<p>I think there is a wealth of natural language data associated with sites like reddit or digg or news.google.com.</p>

<p>I have done a little bit of research with text mining, but can't find how I could use those tools to parse something like reddit.</p>

<p>What kind of applications can you come up with?</p>
","nlp, information-retrieval, text-mining","<p>I have found in the past that the best way to mine data on sites like Reddit or Digg is to first use the developer API that they provide. Typically you have a focused interest in either a topic or trend, and the only way to get that data is through an established public interface. You can also parse feeds, and combine them both to uncover 90% of what you would want to know. If you want to do deep research on data not available through an API, then you should be prepared to spend a significant amount of time writing custom wrappers around a tool like cURL. If you have the budget you can also call them and ask if they offer paid research data on users.</p>
",1052,1224721969
