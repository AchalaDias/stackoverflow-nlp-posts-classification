Title,Description,category,combined_text
semantic similarity for mix of languages,"<p>I have a database of several thousands of utterances. Each record (utterance) is a text representing a problem description, which a user has submitted to a service desk. Sometimes also the service desk agent's response is included. The language is highly technical, and it contains three types of tokens:</p>
<ol>
<li>words and phrases in Language 1 (e.g. English)</li>
<li>words and phrases in Language 2 (e.g. French, Norwegian, or Italian)</li>
<li>machine-generated output (e.g. listing of files using unix command ls -la)</li>
</ol>
<p>These languages are densely mixed. I often see that in one conversation, a sentence in Language 1 is followed by Language 2. So it is impossible to divide the data into two separate sets, corresponding to utterances in two languages.</p>
<p>The task is to find similarities between the records (problem descriptions). The purpose of this exercise is to understand whether some bugs submitted by users are similar to each other.</p>
<p><strong>Q: What is the most effective way to proceed in such a situation?</strong></p>
<p>In particular, the problem lies in the fact that the words come from two different corpora (corpuses), while in addition, some technical words (like filenames, OS paths, or application names) will not be found in any of those.</p>
<p>So the difficulty is in finding similarities between words and utterances in situation when the mix of words does not correspond to known corpus, but rather a mix of corpora.</p>
",Multilingual Language Processing & Language Identification,semantic similarity mix language database several thousand utterance record utterance text representing problem description user ha submitted service desk sometimes also service desk agent response included language highly technical contains three type token word phrase language e g english word phrase language e g french norwegian italian machine generated output e g listing file using unix command l la language densely mixed often see one conversation sentence language followed language impossible divide data two separate set corresponding utterance two language task find similarity record problem description purpose exercise understand whether bug submitted user similar q effective way proceed situation particular problem lie fact word come two different corpus corpus addition technical word like filename path application name found difficulty finding similarity word utterance situation mix word doe correspond known corpus rather mix corpus
Why are weight matrices shared between embedding layers in &#39;Attention is All You Need&#39; paper?,"<p>I am using the Transformer module in pytorch from the paper &quot;Attention is All You Need&quot;. On page 5, the authors state that</p>
<blockquote>
<p>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. (page 5)</p>
</blockquote>
<p>The embedding layer, at least in pytorch, is a learnable tensor whose columns are the embedding vectors corresponding to each word. My confusion stems from the fact that in the paper, the Transformer learns a translation task between languages (i.e. English to German). <strong>Thus, how could the embedding weights be shared for the English and German embedding vectors?</strong></p>
<p><strong>In addition, how could the weights be shared between the output embedding (which goes from word index to embedding vector) and the linear layer (which goes from embedding vector to word probabilities)?</strong> As far as I can tell there is no constraint requiring the embedding tensor must be orthogonal (so that its inverse is its transpose).</p>
",Multilingual Language Processing & Language Identification,weight matrix shared embedding layer attention need paper using transformer module pytorch paper attention need page author state model share weight matrix two embedding layer pre softmax linear transformation similar page embedding layer least pytorch learnable tensor whose column embedding vector corresponding word confusion stem fact paper transformer learns translation task language e english german thus could embedding weight shared english german embedding vector addition could weight shared output embedding go word index embedding vector linear layer go embedding vector word probability far tell constraint requiring embedding tensor must inverse transpose
Natural Language Processing Tag definitions,"<p>When parsing and tagging the words with OpenNLP i was wondering if the tags (eg S, NP, VP, ADJP) actually mean, i found a few by researching the web, but some of them are still missing,
which i am unable to find, currently my code outputs this:</p>
<pre><code>The movie was really good

\-S - S
 |-NP - {Unknown}
 |  |-DT - Determiner
 |  | \- The - The
 |  \- NN - Noun, Singular or mass
 |    \- movie - movie
 \-VP - {Unknown}
   |-VBD - Verb, past tense
   | \- was - was
   \- ADJP - {Unknown}
      |-RB - Adverb
      | \-really - really
      \-JJ - Adjective
        \- good - good
</code></pre>
<p>As you can see I have managed to map some of them such as NN as &quot;Noun, Singular or mass&quot; but i am unable to find any references to S, NP, VP, ADJP</p>
",Multilingual Language Processing & Language Identification,natural language processing tag definition parsing tagging word opennlp wa wondering tag eg np vp adjp actually mean found researching web still missing unable find currently code output see managed map nn noun singular mass unable find reference np vp adjp
Natural Language Processing Model,"<p>I'm making a project to parse, and understand the intentions of input lines by a user in english.</p>
<p>Here is what I think I should do:</p>
<ol>
<li>Create a text of sentences with POS tagging &amp; marked intentions for every sentence by hand.</li>
<li>Create a model say: decision tree and train it on the above sentences.</li>
<li>Try the model on user input:</li>
<li>Do basic tokenizing and POS tagging on user input sentence and testing it on the above model for knowing the intention of this sentence.</li>
</ol>
<p>It all may be completely wrong or silly but I'm determined to learn how to do it. I don't want to use ready-made solutions and the programming language is not a concern.</p>
<p>How would you guys do this task? Which model to choose and why? Normally to make NLP parsers, what steps are done.</p>
",Multilingual Language Processing & Language Identification,natural language processing model making project parse understand intention input line user english think create text sentence po tagging marked intention every sentence hand create model say decision tree train sentence try model user input basic tokenizing po tagging user input sentence testing model knowing intention sentence may completely wrong silly determined learn want use ready made solution programming language concern would guy task model choose normally make nlp parser step done
Natural language processing - Tamil,"<p>I'm building a voice-based system where the end users speak in Tamil, but they may include English keywords (e.g., &quot;WiFi&quot;, &quot;OTP&quot;, &quot;camera&quot;) or numerical values (e.g., &quot;2024&quot;, &quot;1234&quot;) in their speech.</p>
<p>I'm planning to use AI4Bharat's Speech-to-Text model because it is well-trained for Indian languages and is free and open-source. The output from the ASR model is in pure Tamil script (e.g., &quot;‡Æµ‡Øà‡ÆÉ‡Æ™‡Øà&quot; for &quot;WiFi&quot;).</p>
<p>‚úÖ My Current Approach:
Use AI4Bharat to transcribe Tamil speech to text.</p>
<p>Tokenize the Tamil output.</p>
<p>Check for certain target English words or numbers.</p>
<p>Replace or trigger an action when these are detected.</p>
<p>ü§î My Question:
Is this a good approach for reliably detecting specific English words or numerical values within Tamil speech?</p>
<p>Are there better or more robust free and long-term alternatives or enhancements to this method?</p>
<p>Any suggestions for improving accuracy (e.g., handling Tamil transliterations like &quot;‡Æµ‡Øà‡ÆÉ‡Æ™‡Øà&quot; for &quot;WiFi&quot;)?</p>
<p>üîß Constraints:
It must be free of cost.</p>
<p>It should work reliably for the long term (no paid APIs or licenses).</p>
<p>Should work offline or with minimal dependencies if possible.</p>
",Multilingual Language Processing & Language Identification,natural language processing tamil building voice based system end user speak tamil may include english keywords e g wifi otp camera numerical value e g speech planning use ai bharat speech text model well trained indian language free open source output asr model pure tamil script e g wifi current approach use ai bharat transcribe tamil speech text tokenize tamil output check certain target english word number replace trigger action detected question good approach reliably detecting specific english word numerical value within tamil speech better robust free long term alternative enhancement method suggestion improving accuracy e g handling tamil transliteration like wifi constraint must free cost work reliably long term paid apis license work offline minimal dependency possible
simpler gmail Filter syntax for &quot;word family&quot; [verif +(y/ied/ification] + similar loanwords [term +(s/es/a)]?,"<p>Is there simpler filter that I can use for below cases?
Google has a very smart AI gemini, I hope there is a shortcut for this as I am receiving bilingual emails and loan words in Malay/Indonesia are quite similar to English.</p>
<p>a. variations of the same word?</p>
<pre><code>subject:{+updates +updated +updating +update}
subject:{+verify +verification +verified}
</code></pre>
<p>b. similar transliteration of words</p>
<pre><code>subject:{+terms +termes +terma}
subject:{+invois +invoice}
subject:{+privacy +privasi +privacidad}
</code></pre>
<p>c. words with different character(s)/prefix/suffix</p>
<pre><code>subject:{+e-statement +estatement +statement}
subject:{+&quot;log in&quot; +login +&quot;log-ins&quot; &quot;logged in&quot;}
</code></pre>
",Multilingual Language Processing & Language Identification,simpler gmail filter syntax word family verif ied ification similar loanword term e simpler filter use case google ha smart ai gemini hope shortcut receiving bilingual email loan word malay indonesia quite similar english variation word b similar transliteration word c word different character prefix suffix
Word frequency algorithm for natural language processing,"<p>Without getting a degree in information retrieval, I'd like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.  The goal is to get a ""general feel"" of what people are saying over a set of textual comments.  Along the lines of <a href=""http://wordle.net/"" rel=""noreferrer"">Wordle</a>.</p>

<p>What I'd like:</p>

<ul>
<li>ignore articles, pronouns, etc ('a', 'an', 'the', 'him', 'them' etc)</li>
<li>preserve proper nouns</li>
<li>ignore hyphenation, except for soft kind</li>
</ul>

<p>Reaching for the stars, these would be peachy:</p>

<ul>
<li>handling stemming &amp; plurals (e.g. like, likes, liked, liking match the same result)</li>
<li>grouping of adjectives (adverbs, etc) with their subjects (""great service"" as opposed to ""great"", ""service"")</li>
</ul>

<p>I've attempted some basic stuff using Wordnet but I'm just tweaking things blindly and hoping it works for my specific data.  Something more generic would be great.</p>
",Multilingual Language Processing & Language Identification,word frequency algorithm natural language processing without getting degree information retrieval like know exists algorithm counting frequency word occur given body text goal get general feel people saying set textual comment along line wordle like ignore article pronoun etc etc preserve proper noun ignore hyphenation except soft kind reaching star would peachy handling stemming plural e g like like liked liking match result grouping adjective adverb etc subject great service opposed great service attempted basic stuff using wordnet tweaking thing blindly hoping work specific data something generic would great
"How to handle German language specific characters like (&#228;, &#246;, &#252;, &#223;) while tokenizing using GPT2Tokenizer?","<p>I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer.</p>
<p>To tokenize the text, I wrote the implementation as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Tokenizer

text = &quot;z√ºgiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA&quot;
text = text.encode(&quot;utf-8&quot;).decode(&quot;utf-8&quot;)  # Re-encode to fix encoding issues

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)

# Tokenize the text
tokens = tokenizer.tokenize(text)

print(tokens)  # Should properly tokenize &quot;z√ºgiger&quot; instead of splitting &quot;√º&quot;
</code></pre>
<p>Now, when I execute this code snippet I get output as follows:</p>
<pre><code>['z', '√É¬º', 'g', 'iger', 'ƒ†Transport', 'ƒ†des', 'ƒ†ABC', 'D', 'ƒ†stabil', 'en', 'ƒ†Kind', 'es', 'ƒ†in', 'ƒ†die', 'ƒ†Not', 'au', 'fn', 'ah', 'me', 'ƒ†UK', 'A']
</code></pre>
<p>After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below.</p>
<pre class=""lang-markdown prettyprint-override""><code>| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |
|-----------|-------------|-----------------------|------------------|
| √§         | C3 A4       | √É + ¬§                 | √É¬§               |
| √∂         | C3 B6       | √É + ¬∂                 | √É¬∂               |
| √º         | C3 BC       | √É + ¬º                 | √É¬º               |
| √ü         | C3 9F       | √É + ≈∏                 | √É≈∏               |
</code></pre>
<p>Now, how I can keep German language specific characters like (√§, √∂, √º, √ü) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. &quot;z√ºgiger&quot; becomes something like ['z', '√º', 'g', 'iger'].</p>
",Multilingual Language Processing & Language Identification,handle german language specific character like tokenizing using gpt tokenizer working german text need tokenize text using gpt tokenizer tokenize text wrote implementation follows execute code snippet get output follows bit analysis found german language specific character mi decoded latin see table keep german language specific character like inside token tokenization process avoiding unintentional misdecodeding e z giger becomes something like z g iger
How do I remove escape characters from output of nltk.word_tokenize?,"<p>How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output.</p>
<p>Here's my code:</p>
<pre><code>from __future__ import division
import nltk, re, pprint
from urllib.request import urlopen

url = &quot;https://www.gutenberg.org/cache/epub/75394/pg75394.txt&quot;
raw = urlopen(url).read()
raw = raw.decode('utf-8')
tokens = nltk.word_tokenize(raw)
print(type(tokens))
print(len(tokens))
print(tokens[:10])
</code></pre>
<p>And the output, with the escapes visible in the first list item:
<a href=""https://i.sstatic.net/L1QJ1Mdr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/L1QJ1Mdr.png"" alt=""enter image description here"" /></a></p>
<p>I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong?</p>
<p>I'm using Python 3.12.1 on Windows 11.</p>
<p>Thanks in advance - please do let me know if I can provide any further helpful information.</p>
",Multilingual Language Processing & Language Identification,remove escape character output nltk word tokenize get rid non printing escaped character output nltk word tokenize method working book natural language processing python following code example inform output consist word punctuation however still getting escape output code output escape visible first list item poked around online suspicion may fact book sample code wa written python ha already caused encoding issue needed add line convert output byte string right track wrong using python window thanks advance please let know provide helpful information
OpenNLP POSTaggerME and ChunkerME synergy,"<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=""https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api"" rel=""nofollow noreferrer"">TokenizerME</a>, then I tagged it with <a href=""https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api"" rel=""nofollow noreferrer"">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=""https://opennlp.apache.org/models.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>For the sentence ‚ÄúIvo viu a uva‚Äù, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=""https://universaldependencies.org/u/pos/"" rel=""nofollow noreferrer"">UD POS Tags</a>.</p>
<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=""https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas"" rel=""nofollow noreferrer"">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>
<p>But, for more complex sentences, it hasn't produced such good results.</p>
<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=""https://www.linguateca.pt/Floresta/corpus.html#download"" rel=""nofollow noreferrer"">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>
<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>
<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>
<ol>
<li>this does indeed have an impact,</li>
<li>there is a tool that already does this translation and</li>
<li>there are any other suggestions for improving the chunker precision/recall.</li>
</ol>
",Multilingual Language Processing & Language Identification,opennlp postaggerme chunkerme synergy trying use opennlp chunking api chunk portuguese sentence first tokenized sentence using tokenizerme tagged postaggerme used ready made model provided project sentence ivo viu uva postaggerme return tag propn verb det noun model seems using ud po tag ready made model chunkerme portuguese followed instruction training first using chunkerconverter tool convert arvore deitada conll generating model chunkertrainerme tool everything worked well sentence chunker produced correct tag b np b vp b np np complex sentence produced good result wa trying identify could improve chunker training one thing noticed difference type tag portuguese corpus bosque seems using portuguese tag example instead propn corpus us prop instead det us art seems could lead problem especially since one parameter chunker receives array ud tag ha trained another type tag writing code creating routine convert portuguese notation ud penn wanted ask doe indeed impact tool already doe translation suggestion improving chunker precision recall
Document Analysis and Tagging,"<p>Let's say I have a bunch of essays (thousands) that I want to tag, categorize, etc.  Ideally, I'd like to train <em>something</em> by manually categorizing/tagging a few hundred, and then let the thing loose.</p>
<p>What resources (books, blogs, languages) would you recommend for undertaking such a task?  Part of me thinks this would be a good fit for a <a href=""http://www.process.com/precisemail/bayesian_filtering.htm"" rel=""nofollow noreferrer"">Bayesian Classifier</a> or even <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">Latent Semantic Analysis</a>, but I'm not really familiar with either other than what I've found from a few <a href=""http://rubyforge.org/projects/bishop/"" rel=""nofollow noreferrer"">ruby</a> <a href=""http://classifier.rubyforge.org/"" rel=""nofollow noreferrer"">gems</a>.</p>
<p>Can something like this be solved by a bayesian classifier?  Should I be looking more at semantic analysis/natural language processing?  Or, should I just be looking for keyword density and mapping from there?</p>
",Multilingual Language Processing & Language Identification,document analysis tagging let say bunch essay thousand want tag categorize etc ideally like train something manually categorizing tagging hundred let thing loose resource book blog language would recommend undertaking task part think would good fit bayesian classifier even latent semantic analysis really familiar either found ruby gem something like solved bayesian classifier looking semantic analysis natural language processing looking keyword density mapping
PunktTokenizer does not work with Russian `—è.`,"<p>When tokenizing paragraphs to sentences in the Russian language, I am observing the special case when the sequence is not treated as the end of the sentence. The case is with the <code>—è.</code> at the end of the sentence. See the working example:</p>
<pre><code>import nltk

tok = nltk.tokenize.PunktTokenizer('russian')

print('-----------------')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å —è. –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å —è. - –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')
</code></pre>
<p><a href=""https://i.sstatic.net/yrj2ukn0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yrj2ukn0.png"" alt=""the output"" /></a></p>
<p>The second case works as expected. It differs only in adding the dash (kind of introduction of the speaker's note [sorry for my lack of terms]).</p>
<p>The <code>—è</code> is not present in the Russian abbreviations (<code>c:\nltk_data\tokenizers\punkt_tab\russian\abbrev_types.txt</code>). However, even when added to the file with abbreviations, it does not make a difference.</p>
<p>How the situation should be fixed?</p>
<p>The nltk is of the version 3.9.1, the nltk_data are shared -- stored in c:\nltk_data; fresh download (30. 1. 2025). Python 3.12 on Windows 10 was used.</p>
<p><strong>Update:</strong></p>
<p>(... observing donwnvotes). Please, I am very new to nltk, and I tried my best to get the answer by myself. When down-voting, write the comment why. I am searching for the answer in various sources without success.</p>
<p>I am using UTF-8 encoding for both the test script here, and also in the file that is actually being processed (the image take from notepad++).
<a href=""https://i.sstatic.net/bmEd4NOU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bmEd4NOU.png"" alt=""UTF-8 encoding used"" /></a></p>
<p><strong>2nd Update:</strong>
As Joop Eggen suggested, I have tried with other single letters:</p>
<pre><code>import nltk

tok = nltk.tokenize.PunktTokenizer('russian')

print('\n================= —è in the original question (—è not in the abbrev_types.txt)\n')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å —è. –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å —è. - –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('\n================= –≥ is in the abbrev_types.txt\n')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å –≥. –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å –≥. - –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('\n================= –∞ IS NOT the abbrev_types.txt\n')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å –∞. –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = '–†–æ–¥–∏–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ, —Å—Ç–∞–ª —Ä–∞–∑–º—ã—à–ª—è—Ç—å –∞. - –û–Ω–∞ –Ω–µ –∑–∞—Å—Ç—Ä–µ–ª–µ–Ω–∞, –∞ —ç—Ç–æ –¥–µ–ª–æ —É–ø—Ä–æ—â–∞–µ—Ç.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')
</code></pre>
<p><a href=""https://i.sstatic.net/9dezvzKN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9dezvzKN.png"" alt=""other single letters"" /></a></p>
<p>The <code>–∞</code> is also <strong>not</strong> in the abbreviations; however, it is treated as if it was (differently from <code>—è</code>). The <code>–≥</code> is in the abbreviations, so here the behavior is expected.</p>
",Multilingual Language Processing & Language Identification,punkttokenizer doe work russian tokenizing paragraph sentence russian language observing special case sequence treated end sentence case end sentence see working example second case work expected differs adding dash kind introduction speaker note sorry lack term present russian abbreviation however even added file abbreviation doe make difference situation fixed nltk version nltk data shared stored c nltk data fresh download python window wa used update observing donwnvotes please new nltk tried best get answer voting write comment searching answer various source without success using utf encoding test script also file actually processed image take notepad nd update joop eggen suggested tried single letter also abbreviation however treated wa differently abbreviation behavior expected
How To Detect Is Text Human Readable?,"<p>I am wondering if there's a way to tell a given text is human readable. By human readable, I mean: it has some meanings, format like an article written by somebody, or at least generated by a software translator that is intended to be read by a human.</p>

<p>Here's the background story: recently I am making an app that allows user to upload a short text to a database. At the early stage of deployment I noticed some user always uploaded corrupted text due to a problem with encoding. This problem is fixed later, but leaves me wonder if there's a way to pick up non human readable text before serving the text back to users.</p>

<p>Any advice will be appreciated. The scope might be too large to include other languages, so at the moment let's limit the discussion to English only.</p>
",Multilingual Language Processing & Language Identification,detect text human readable wondering way tell given text human readable human readable mean ha meaning format like article written somebody least generated software translator intended read human background story recently making app allows user upload short text database early stage deployment noticed user always uploaded corrupted text due problem encoding problem fixed later leaf wonder way pick non human readable text serving text back user advice appreciated scope might large include language moment let limit discussion english
Why is the vocab size of Byte level BPE smaller than Unicode&#39;s vocab size?,"<p>I recently read GPT2 and the paper says:</p>
<blockquote>
<p>This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added.  This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. In contrast, a byte-level version of BPE only requires a base vocabulary of size 256.</p>
</blockquote>
<p>I really don't understand the words. The number of characters that Unicode represents is 130K but how can this be reduced to 256? Where's the rest of approximately 129K characters? What am I missing? Does byte-level BPE allow duplicating of representation between different characters?</p>
<p>I don't understand the logic. Below are my questions:</p>
<ul>
<li>Why the size of vocab is reduced? (from 130K to 256)</li>
<li>What's the logic of the BBPE (Byte-level BPE)?</li>
</ul>
<hr />
<h1>Detail question</h1>
<p>Thank you for your answer but I really don't get it. Let's say we have 130K unique characters. What we want (and BBPE do) is to reduce this basic (unique) vocabulary. Each Unicode character can be converted 1 to 4 bytes by utilizing UTF-8 encoding. The original paper of BBPE says (Neural Machine Translation with Byte-Level Subwords):</p>
<blockquote>
<p>Representing text at the level of bytes and <strong>using the 256 bytes</strong> set as vocabulary is a potential solution to this issue.</p>
</blockquote>
<p>Each byte can represent 256 characters (bits, 2^8), we only need 2^17 (131072) bits for representing the unique Unicode characters. In this case, where did the <strong>256 bytes in the original paper</strong> come from? I don't know both the logic and how to derive this result.</p>
<p>I arrange my questions again, more detail:</p>
<ul>
<li>How does BBPE work?</li>
<li>Why the size of vocab is reduced? (from 130K to 256 bytes)
<ul>
<li>Anyway, we always need 130K space for a vocab. What's the difference between representing unique characters as Unicode and Bytes?</li>
</ul>
</li>
</ul>
<p>Since I have little knowledge of computer architecture and programming, please let me know if there's something I missed.</p>
<p>Sincerely, thank you.</p>
",Multilingual Language Processing & Language Identification,vocab size byte level bpe smaller unicode vocab size recently read gpt paper say would result base vocabulary multi symbol token added prohibitively large compared token vocabulary often used bpe contrast byte level version bpe requires base vocabulary size really understand word number character unicode represents k reduced rest approximately k character missing doe byte level bpe allow duplicating representation different character understand logic question size vocab reduced k logic bbpe byte level bpe detail question thank answer really get let say k unique character want bbpe reduce basic unique vocabulary unicode character converted byte utilizing utf encoding original paper bbpe say neural machine translation byte level subwords representing text level byte using byte set vocabulary potential solution issue byte represent character bit need bit representing unique unicode character case byte original paper come know logic derive result arrange question detail doe bbpe work size vocab reduced k byte anyway always need k space vocab difference representing unique character unicode byte since little knowledge computer architecture programming please let know something missed sincerely thank
Removing various symbols from a text,"<p>I am trying to <strong>clean</strong> some texts that are very different from one another. I would like to remove the headlines, quotation marks, abbreviations, special symbols and points that don't actually end sentences.</p>
<p>Example input:</p>
<pre><code>This is a headline

And inside the text there are 'abbreviations', e.g. &quot;bzw.&quot; in German or some German dates, like 2. Dezember 2017. Sometimes there are even enumerations, that I might just eliminate completely.
‚Ä¢ they have
‚ó¶ different bullet points
- or even equations and 
Sometimes there are special symbols. ‚úì
</code></pre>
<p>Example output:</p>
<pre><code>And inside the text there are abbreviations, for example beziehungsweise in German or some German dates, like 2 Dezember 2017. Sometimes there are even enumerations, that I might just eliminate completely. Sometimes there are special symbols.
</code></pre>
<p><strong>What I did:</strong></p>
<pre><code>with open(r'C:\\Users\me\\Desktop\\ex.txt', 'r', encoding=&quot;utf8&quot;) as infile: 
    data = infile.read()
    data = data.replace(&quot;'&quot;, '')
    data = data.replace(&quot;e.g.&quot;, 'for example') 
    #and so on
with open(r'C:\\Users\me\\Desktop\\ex.txt', 'w', encoding=&quot;utf8&quot;) as outfile:
    outfile.write(data)
</code></pre>
<p><strong>My problems (although number 2 is the most important):</strong></p>
<ol>
<li><p>I just want a string with this input, but it obviously breaks because of the quotation marks, is there any way to do this other than working with files like I did? In reality, I'm copy-pasting a text and want an app to clean it.</p>
</li>
<li><p>The code seems very inefficient because I just manually write the things that I remember to delete/clean, but I don't know all the abbreviations by heart. How do I clean it in one go, so to say?</p>
</li>
<li><p>Is there any way to eliminate the headline and enumeration, and the point <code>.</code> that appears in that German date? My code doesn't do that.</p>
</li>
</ol>
<p>Edit: I just remembered stuff like <code>text = re.sub(r&quot;(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?&quot;, &quot;&quot;, text)</code>, but regex is inefficient for huge texts, isn't it?</p>
",Multilingual Language Processing & Language Identification,removing various symbol text trying clean text different one another would like remove headline quotation mark abbreviation special symbol point actually end sentence example input example output problem although number important want string input obviously break quotation mark way working file like reality copy pasting text want app clean code seems inefficient manually write thing remember delete clean know abbreviation heart clean one go say way eliminate headline enumeration point appears german date code edit remembered stuff like regex inefficient huge text
"Is there a database, API, or parsable text for getting verb conjugations?","<p>This isn't directly a programming question, so I apologize in advance. I've been working on a grammar-free random sentence generator for a typing game I'd like to make, and I've been having a difficult time finding any parsable (or callable) data for getting verb conjugations. Ultimately, if I can't find anything like this, I'm going to have to go through the dictionary I've created and add first-person singular and plural, second-person singular and plural, third-person singular and plural, simple past, past participle, and present participle forms for every irregular verb.</p>

<p>This wouldn't be a problem in many languages, but there are so many irregular English verbs that this could take a long, long time to do manually. I'm not against the worse option, but I want to make sure I'm not going to be wasting obscene hours doing it myself when there is some database I can use instead.</p>

<p>I've seen <a href=""http://www.scientificpsychic.com/verbs1.html"" rel=""nofollow noreferrer"">http://www.scientificpsychic.com/verbs1.html</a> and spoken with the creator, but he doesn't release his exact dictionary (just the classes for it). I've also seen sites like <a href=""http://www.verbix.com/webverbix/English/find.html"" rel=""nofollow noreferrer"">http://www.verbix.com/webverbix/English/find.html</a>, which would be great for scraping, but that's a bit of a pain as well.</p>

<p>This question has been asked here before ( <a href=""https://stackoverflow.com/questions/8424806/verb-conjugations-database"">Verb Conjugations Database</a> ), but the question was left unanswered, and the asker alluded to solving the problem but never said what the solution was.</p>
",Multilingual Language Processing & Language Identification,database api parsable text getting verb conjugation directly programming question apologize advance working grammar free random sentence generator typing game like make difficult time finding parsable callable data getting verb conjugation ultimately find anything like going go dictionary created add first person singular plural second person singular plural third person singular plural simple past past participle present participle form every irregular verb problem many language many irregular english verb could take long long time manually worse option want make sure going obscene hour database use instead seen spoken creator release exact dictionary class also seen site like would great scraping bit pain well question ha asked href conjugation database question wa left unanswered asker alluded solving problem never said solution wa
Extracting English imperative mood from verb tags with spaCy,"<p>I would like to detect the imperative mood of verbs in English sentences. From <a href=""https://stackoverflow.com/questions/53755559/how-to-extract-tag-attributes-using-spacy"">this question</a> I'm aware that spaCy has access to extended morphological features, but I don't see them when I use it, using spaCy version 2.1.8, Python 3.7.1:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load('en_core_web_sm')
&gt;&gt;&gt; doc = nlp(""Show me the money!"")
&gt;&gt;&gt; token = doc[0]
&gt;&gt;&gt; print(token, token.tag_, nlp.vocab.morphology.tag_map[token.tag_], token.pos_, token.dep_)
Show VB {74: 100, 'VerbForm': 'inf'} VERB ROOT
</code></pre>

<p>Looks like spaCy correctly inferred that ""show"" is a verb, but it appears that it thinks it's an infinitive, and I see no mood information from the <code>tag_map</code>. I realize that this extended information is available in my linked question because Italian was being parsed there and determining the mood may be simpler for Romance-inflected verbs. </p>

<p>In any case, does any extended information about mood exist for verbs in English sentences?</p>
",Multilingual Language Processing & Language Identification,extracting english imperative mood verb tag spacy would like detect imperative mood verb english sentence case doe extended information mood exist verb english sentence
Arabic lemmatization and Stanford NLP,"<p>I try to make lemmatization, ie identifying the lemma and possibly the Arabic root of a verb, for example:
Ÿäÿ™ÿµŸÑ ==> lemma (infinitive of the verb) ==> ÿßÿ™ÿµŸÑ ==> root (triliteral root / Jidr thoulathi)
==> Ÿà ÿµ ŸÑ</p>

<p>Do you think Stanford NLP can do that?</p>

<p>Best Regards,</p>
",Multilingual Language Processing & Language Identification,arabic lemmatization stanford nlp try make lemmatization ie identifying lemma possibly arabic root verb example lemma infinitive verb root triliteral root jidr thoulathi think stanford nlp best regard
Fetch proximity between two languages through Python,"<p>Input 1 - &quot;English&quot;</p>
<p>Input 2 - &quot;French&quot;</p>
<p>Expected Output - 46.9 (Based on this website - <a href=""http://www.elinguistics.net/Compare_Languages.aspx"" rel=""nofollow noreferrer"">http://www.elinguistics.net/Compare_Languages.aspx</a>)</p>
<p>Is there any Python library that supports such a language similarity request? There are plenty of options for checking similarity of two sentences, but what about that of two whole languages.</p>
<p>The website in the question is the best one I could find but I'm unable to use it through HTTP request. Selenium is not an option here either.</p>
",Multilingual Language Processing & Language Identification,fetch proximity two language python input english input french expected output based website python library support language similarity request plenty option checking similarity two sentence two whole language website question best one could find unable use http request selenium option either
How to split and spelling correct arabic text without spaces into list of words,"<p>I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues:</p>
<pre><code>ÿ™ÿ¥ÿ±ÿßÿ®ÿßŸÑŸÇÿ∑ÿ∑ ÿßŸÑÿ≠ŸÑŸäÿ® =&gt; [ÿ™ÿ¥ÿ±ÿ®ÿå ÿßŸÑŸÇÿ∑ÿ∑ÿå ÿßŸÑÿ≠ŸÑŸäÿ®]

ŸÖÿÆŸÖÿØŸäŸàÿ≥ŸÅ =&gt; [&quot;ŸÖÿ≠ŸÖÿØ&quot;, &quot;ŸäŸàÿ≥ŸÅ&quot;]

ÿßŸÜÿß ÿßÿ±ŸäŸäÿØÿßŸÜ ÿßÿ¥ÿ±ÿßÿ® =&gt; [&quot;ÿßŸÜÿß&quot;, &quot;ÿ£ÿ±ŸäÿØ&quot;, &quot;ÿ£ŸÜ&quot;, &quot;ÿ£ÿ¥ÿ±ÿ®&quot;]

ÿ¨ŸÖŸÑÿ© ÿµÿ≠Ÿäÿ≠ÿ© =&gt; [&quot;ÿ¨ŸÖŸÑÿ©&quot;, &quot;ÿµÿ≠Ÿäÿ≠ÿ©&quot;]
</code></pre>
<p>And if there are multiple correct splitting ways, return them all:</p>
<pre><code>ŸÖÿÆŸÖÿØŸäŸàÿ≥ŸÅ =&gt; [ [&quot;ŸÖÿ≠ŸÖÿØ&quot;, &quot;ŸäŸàÿ≥ŸÅ&quot;] , [&quot;ÿßÿ≠ŸÖÿØ&quot;, &quot;ŸäŸàÿ≥ŸÅ&quot;] ]
</code></pre>
<p>If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement?</p>
",Multilingual Language Processing & Language Identification,split spelling correct arabic text without space list word looking way split arabic text correct spelling whitespace first splitting criterion maybe based dictionary correct word splitting done considering spelling issue multiple correct splitting way return library otherwise custom algorithm code implement
How to make an AI bot of Natural Language Processing?,"<p>I want to make AI bot which can understand only 4 words &quot;Up&quot;, &quot;Down&quot;, &quot;Left&quot;, &quot;Right&quot;.</p>
<p>as my friend make a python script which executes some task by the voice like to open youtube just say &quot;Youtube&quot; and Chrome browser will open with youtube.com URL. But the system was slow as they were using google assistant/ai to process the voice which makes me feel impatient.</p>
<p>Then I got an idea what if an AI system offline which Understand only a few words and we can get some desired result and will be super fast.</p>
<p>for example:- I have a remote control car I want to make voice-activated as when I say &quot;Up&quot; car should move forward, similarly for &quot;Down&quot; -&gt; Backward, &quot;Left&quot; -&gt; Left and &quot;Right&quot; -&gt; Right &amp; &quot;{Any other voice}&quot; -&gt; blink the led to tell that the system didn't understand</p>
<p>so:</p>
<p>how should i start?<br>
how should i train the AI Bot?<br>
what should be my requirements?<br>
and other thing that i should know.</p>
",Multilingual Language Processing & Language Identification,make ai bot natural language processing want make ai bot understand word left right friend make python script executes task voice like open youtube say youtube chrome browser open youtube com url system wa slow using google assistant ai process voice make feel impatient got idea ai system offline understand word get desired result super fast example remote control car want make voice activated say car move forward similarly backward left left right right voice blink led tell system understand start train ai bot requirement thing know
Benepar for syntactic segmentation,"<p>I want to use Benepar with a French model to do a syntactic segmentation.</p>
<p>I followed the tutorial but I have always have this error</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for ChartParser:
Unexpected key(s) in state_dict: &quot;pretrained_model.embeddings.position_ids&quot;.</p>
</blockquote>
<p>I tried to do the following:</p>
<pre><code>import benepar
benepar.download('benepar_fr2')
import spacy

nlp = spacy.load('fr_core_news_sm')
parser = benepar.Parser(&quot;benepar_fr2&quot;)

def parse_sentence(text):
    doc = nlp(text)
    parsed_sents = [parser.parse(sent.text) for sent in doc.sents]
    return parsed_sents

sentence = &quot;Le chat mange une souris.&quot;
parsed_sentence = parse_sentence(sentence)
print(parsed_sentence)
</code></pre>
",Multilingual Language Processing & Language Identification,benepar syntactic segmentation want use benepar french model syntactic segmentation followed tutorial always error runtimeerror error loading state dict chartparser unexpected key state dict pretrained model embeddings position id tried following
Determining most popular words in the English dictionary within a dictionary of words,"<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>
<p>For example:
A sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>
<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>
<p>Any and all help is welcome and appreciated.</p>
<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>
",Multilingual Language Processing & Language Identification,determining popular word english dictionary within dictionary word forgive wording awful trying figure determine used word english language set word dictionary made done research nltk seem find function within library matter help need example sentence enjoy cold glass water hot day would return water used word day day conversation sentence essentially need returned value frequently used word conversation figure likely involve ai time tried use ai wind copy pasting code understand trying avoid going route help welcome appreciated context decided start project would essentially guess predetermined word based character user say ha computer guess
Fine-tune BERT for a specific domain on a different language?,"<p>I want to fine-tune on a pre-trained BERT model.
However, my task uses data within a specific domain (say biomedical data).
Additionally, my data is also in a language different from English (say Dutch).</p>
<p>Now I could fine-tune the Dutch bert-base-dutch-cased pre-trained model.
However, how would I go about fine-tuning a Biomedical BERT model, like BioBERT,
which is in the correct domain, but wrong language?</p>
<p>I have thought about using NMT, but don't think it's viable and worth the effort.
If I fine-tune without any alterations to the model, I fear that the model will not learn the task well
since it was pre-trained on a completely different language.</p>
",Multilingual Language Processing & Language Identification,fine tune bert specific domain different language want fine tune pre trained bert model however task us data within specific domain say biomedical data additionally data also language different english say dutch could fine tune dutch bert base dutch cased pre trained model however would go fine tuning biomedical bert model like biobert correct domain wrong language thought using nmt think viable worth effort fine tune without alteration model fear model learn task well since wa pre trained completely different language
Strategy for parsing natural language descriptions into structured data,"<p>I have a set of requirements and I'm looking for the best <strong>Java-based</strong> strategy / algorthm / software to use.  Basically, I want to take a set of recipe ingredients entered by real people in natural english and parse out the meta-data into a structured format (see requirements below to see what I'm trying to do).</p>
<p>I've looked around here and other places, but have found nothing that gives a high-level advice on what direction follow.  So, I'll put it to the smart people :-):</p>
<p>What's the best / simplest way to solve this problem?  Should I use a natural language parser, dsl, lucene/solr, or some other tool/technology?  NLP seems like it may work, but it looks really complex.  I'd rather not spend a whole lot of time doing a deep dive just to find out it can't do what I'm looking for or that there is a simpler solution.</p>
<h1>Requirements</h1>
<p>Given these recipe ingredient descriptions....</p>
<ol>
<li>&quot;8 cups of mixed greens (about 5 ounces)&quot;</li>
<li>&quot;Eight skinless chicken thighs (about 1¬º lbs)&quot;</li>
<li>&quot;6.5 tablespoons extra-virgin olive oil&quot;</li>
<li>&quot;approximately 6 oz. thinly sliced smoked salmon, cut into strips&quot;</li>
<li>&quot;2 whole chickens (3 .5 pounds each)&quot;</li>
<li>&quot;20 oz each frozen chopped spinach, thawed&quot;</li>
<li>&quot;.5 cup parmesan cheese, grated&quot;</li>
<li>&quot;about .5 cup pecans, toasted and finely ground&quot;</li>
<li>&quot;.5 cup Dixie Diner Bread Crumb Mix, plain&quot;</li>
<li>&quot;8 garlic cloves, minced (4 tsp)&quot;</li>
<li>&quot;8 green onions, cut into 2 pieces&quot;</li>
</ol>
<p>I want to turn it into this....</p>
<pre>
|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|
|     | Measure |             |                         | weight | weight    |                                |             |
| #   | value   | Measure     | ingredient              | value  | measure   | preparation                    | Brand Name  |
|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|
| 1.  | 8       | cups        | mixed greens            | 5      | ounces    | -                              | -           |
| 2.  | 8       | -           | skinless chicken thigh  | 1.5    | pounds    | -                              | -           |
| 3.  | 6.5     | tablespoons | extra-virgin olive oil  | -      | -         | -                              | -           |
| 4.  | 6       | ounces      | smoked salmon           | -      | -         | thinly sliced, cut into strips | -           |
| 5.  | 2       | -           | whole chicken           | 3.5    | pounds    | -                              | -           |
| 6.  | 20      | ounces      | forzen chopped spinach  | -      |           | thawed                         | -           |
| 7.  | .5      | cup         | parmesean cheese        | -      | -         | grated                         | -           |
| 8.  | .5      | cup         | pecans                  | -      | -         | toasted, finely ground         | -           |
| 9.  | .5      | cup         | Bread Crumb Mix, plain  | -      | -         | -                              | Dixie Diner |
| 10. | 8       | -           | garlic clove            | 4      | teaspoons | minced                         | -           |
| 11. | 8       | -           | green onions            | -      | -         | cut into 2 pieces              | -           |
|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|
</pre>
<p>Note the diversity of the descriptions.  Some things are abbreviated, some are not.  Some numbers are numbers, some are spelled out.</p>
<p>I would love something that does a perfect parse/translation.  But, would settle for something that does reasonably well to start.</p>
<p>Bonus question:  after suggesting a strategy / tool, how would you go about it?</p>
<p>Thanks!</p>
<p>Joe</p>
",Multilingual Language Processing & Language Identification,strategy parsing natural language description structured data set requirement looking best java based strategy algorthm software use basically want take set recipe ingredient entered real people natural english parse meta data structured format see requirement see trying looked around place found nothing give high level advice direction follow put smart people best simplest way solve problem use natural language parser dsl lucene solr tool technology nlp seems like may work look really complex rather spend whole lot time deep dive find looking simpler solution requirement given recipe ingredient description cup mixed green ounce eight skinless chicken thigh lb tablespoon extra virgin olive oil approximately thinly sliced smoked salmon cut strip whole chicken pound frozen chopped spinach thawed cup parmesan cheese grated cup pecan toasted finely ground cup dixie diner bread crumb mix plain garlic clove minced tsp green onion cut piece want turn measure weight weight value measure ingredient value measure preparation brand name cup mixed green ounce skinless chicken thigh pound tablespoon extra virgin olive oil ounce smoked salmon thinly sliced cut strip whole chicken pound ounce forzen chopped spinach thawed cup parmesean cheese grated cup pecan toasted finely ground cup bread crumb mix plain dixie diner garlic clove teaspoon minced green onion cut piece note diversity description thing abbreviated number number spelled would love something doe perfect parse translation would settle something doe reasonably well start bonus question suggesting strategy tool would go thanks joe
Understanding byte-pair encoding tokenization for Greek characters,"<p>I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.add_tokens(list(new_tokens)).
</code></pre>
<p>However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this:</p>
<pre class=""lang-py prettyprint-override""><code>\['Translate', 'ƒ†from', 'ƒ†Greek', 'ƒ†to', 'ƒ†Spanish', ':', 'ƒ†√éƒø√é¬±', 'ƒ†√é≈É√èƒ•√é¬π', 'ƒ†√é¬ø√é¬≥√é¬Ø', 'ƒ†√é¬≥√é≈É√èƒ£√é¬ø√èƒß'\]
</code></pre>
<p>When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in</p>
<pre class=""lang-py prettyprint-override""><code>extender_tokenizer.add_tokens(['ŒëœÖœÑœå', 'ŒµŒØŒΩŒ±Œπ'])
</code></pre>
<p>it does work.</p>
<p>I assume this is an encoding issue or it is related to BPE inner workings.
Why are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer?</p>
<p>Reference code:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers import Tokenizer, models, trainers, pre_tokenizers

tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
trainer = trainers.BpeTrainer(vocab_size = 2000, min_frequency = 3, show_progress = True)
tokenizer.train_from_iterator(training_corpus, trainer = trainer)
</code></pre>
",Multilingual Language Processing & Language Identification,understanding byte pair encoding tokenization greek character trying train new tokenizer greek text later add new token llama tokenizer using however upon training byte pair encoding tokenizer greek spanish text result look something like extending token vocabulary tokenizer seems encoded token passed literally encoding greek character recognized tokenizer encode sentence however using method new token hardcoded doe work assume encoding issue related bpe inner working greek character shown way related encoding bpe obtain list greek character token added tokenizer reference code
A language model for machine translation between a low-resource language and Portuguese using Tensorflow,"<p>I'm trying to train a language model for machine translation between a low-resource language and Portuguese using Tensorflow. unfortunately, I'm getting the following error:</p>
<pre><code>PS C:\Users\myuser\PycharmProjects\teste&gt; python .\tensorflow_model.py                   
2024-08-23 21:29:50.839647: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File &quot;.\tensorflow_model.py&quot;, line 52, in &lt;module&gt;
    dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;, line 831, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;, line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;, line 45, in __init__
    batch_dim.assert_is_compatible_with(
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\framework\tensor_shape.py&quot;, line 300, in assert_is_compatible_with
    raise ValueError(&quot;Dimensions %s and %s are not compatible&quot; %
ValueError: Dimensions 21 and 22 are not compatible
</code></pre>
<p>How can I overcome this error?</p>
<pre><code>import tensorflow as tf
import numpy as np
import re
import os

# Clean data
def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()
    sentence = re.sub(r&quot;([?.!,¬ø])&quot;, r&quot; \1 &quot;, sentence)
    sentence = re.sub(r'[&quot; &quot;]+', &quot; &quot;, sentence)
    sentence = re.sub(r&quot;[^a-zA-Z?.!,¬ø]+&quot;, &quot; &quot;, sentence)
    sentence = sentence.strip()
    sentence = '&lt;start&gt; ' + sentence + ' &lt;end&gt;'
    return sentence

#Function to load data
def load_data(file_path_src, file_path_tgt):
    src_sentences = open(file_path_src, 'r', encoding='utf-8').read().strip().split('\n')
    tgt_sentences = open(file_path_tgt, 'r', encoding='utf-8').read().strip().split('\n')

    src_sentences = [preprocess_sentence(sentence) for sentence in src_sentences]
    tgt_sentences = [preprocess_sentence(sentence) for sentence in tgt_sentences]

    return src_sentences, tgt_sentences

#load data
src_sentences, tgt_sentences = load_data('src_language.txt', 'portuguese.txt')

#Tokenization
src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
tgt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')

src_tokenizer.fit_on_texts(src_sentences)
tgt_tokenizer.fit_on_texts(tgt_sentences)

src_tensor = src_tokenizer.texts_to_sequences(src_sentences)
tgt_tensor = tgt_tokenizer.texts_to_sequences(tgt_sentences)

src_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_tensor, padding='post')
tgt_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_tensor, padding='post')

BUFFER_SIZE = len(src_tensor)

#Creating the Dataset
dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE) 
</code></pre>
",Multilingual Language Processing & Language Identification,language model machine translation low resource language portuguese using tensorflow trying train language model machine translation low resource language portuguese using tensorflow unfortunately getting following error overcome error
Integrate Python Model to Power Bi,"<p>In power BI  I have to create a NLP based chatbot kind of thing, like we have QnA visual in power bi which can answer the question asked in Natural Language Processing but it due to some of it's limitations I can't use.</p>
<p>I want to train my dataset using python and create a model which can answer any question asked in natural language.
However the problem is I can't run python script into Power bi , since there is no gateway connection for it so it will not refresh, I do not have to do in my local system only it should be reached out to end user. Anything if I load into power bi will eventually look like a table  and I have to create a visual of it.</p>
<p>I want to know in what ways it could be possible to use the QnA model written in python to inculcate the python model into power bi, so that a user can go search about the question they want to ask in a  natural language.</p>
<p>Is there a way to use python so that the end user can type their question and get their answer like a chat bot?</p>
",Multilingual Language Processing & Language Identification,integrate python model power bi power bi create nlp based chatbot kind thing like qna visual power bi answer question asked natural language processing due limitation use want train dataset using python create model answer question asked natural language however problem run python script power bi since gateway connection refresh local system reached end user anything load power bi eventually look like table create visual want know way could possible use qna model written python inculcate python model power bi user go search question want ask natural language way use python end user type question get answer like chat bot
Semantic Role Labeling for English Newspaper,"<p>I am doing reserch on Newspaper headlines that are written in English. As part of my reserch, I want to labeling the semantic roles (SRL) of the entities in the headlines, and I would like to use some-sort of NLP technique to do this.</p>
<p>What are some <strong>modern</strong> sematic role labeling NLP models or ways to do this (using Python).</p>
<p>I looked into using AllenNLP and other resourcces, but many of the models seem **outdated. **</p>
",Multilingual Language Processing & Language Identification,semantic role labeling english newspaper reserch newspaper headline written english part reserch want labeling semantic role srl entity headline would like use sort nlp technique modern sematic role labeling nlp model way using python looked using allennlp resourcces many model seem outdated
Why do Transformers in Natural Language Processing need a stack of encoders?,"<p>I am following this blog on transformers</p>

<p><a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">http://jalammar.github.io/illustrated-transformer/</a></p>

<p>The only thing I don't understand is why there needs to be a stack of encoders or decoders. I understand that the multi-headed attention layers capture different representation spaces of the problem. I don't understand why there needs to be a vertical stack of encoders and decoders. Wouldn't one encoder/decoder layer work?</p>
",Multilingual Language Processing & Language Identification,transformer natural language processing need stack encoders following blog transformer thing understand need stack encoders decoder understand multi headed attention layer capture different representation space problem understand need vertical stack encoders decoder one encoder decoder layer work
facebook/m2m100_418M model - how to translate longer sequences of text,"<p>I have this extracted the following text from Wikipedia's Wiki (<a href=""https://en.wikipedia.org/wiki/Wiki"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Wiki</a>),</p>
<pre><code>A wiki is a form of hypertext publication on the internet which is collaboratively edited and managed by its audience directly through a web browser. A typical wiki contains multiple pages that can either be edited by the public or limited to use within an organization for maintaining its internal knowledge base.

Wikis are powered by wiki software, also known as wiki engines. Being a form of content management system, these differ from other web-based systems such as blog software or static site generators in that the content is created without any defined owner or leader. Wikis have little inherent structure, allowing one to emerge according to the needs of the users. Wiki engines usually allow content to be written using a lightweight markup language and sometimes edited with the help of a rich-text editor. There are dozens of different wiki engines in use, both standalone and part of other software, such as bug tracking systems. Some wiki engines are free and open-source, whereas others are proprietary. Some permit control over different functions (levels of access); for example, editing rights may permit changing, adding, or removing material. Others may permit access without enforcing access control. Further rules may be imposed to organize content. In addition to hosting user-authored content, wikis allow those users to interact, hold discussions, and collaborate.
</code></pre>
<p>and tried to translate it to French, in the way shown in the model card and found some part of the text to be discarded from the translation.</p>
<p>This is the French translation I got,</p>
<pre><code>Un wiki est une forme de publication hypertexte sur Internet qui est collaborativement √©dit√© et g√©r√© par son public directement par le biais d'un navigateur Web. Un wiki typique contient plusieurs pages qui peuvent soit √™tre √©dit√© par le public ou limit√© √† utiliser dans une organisation pour maintenir sa base de connaissances interne. Wikis sont aliment√©es par le logiciel wiki, √©galement connu sous le nom de moteurs wiki. √ätre une forme de syst√®me de gestion du contenu, ces derniers diff√®rent d'autres syst√®mes web tels que le logiciel de blog ou les g√©n√©rateurs de site statiques dans lequel le contenu est cr√©√© sans propri√©taire ou leader d√©fini. Wikis ont peu de structure inh√©rente, permettant √† l'un d'√©merger en fonction des besoins des utilisateurs. Les moteurs wiki permettent g√©n√©ralement le contenu
</code></pre>
<p>and English version of that according to google-translate is, (I wanted to verify the translation since I don't know French)</p>
<pre><code>A wiki is a form of hypertext publication on the Internet that is collaboratively edited and managed by its audience directly through a web browser. A typical wiki contains multiple pages that can either be edited by the public or restricted to use within an organization to maintain its internal knowledge base. Wikis are powered by wiki software, also known as wiki engines. Being a form of content management system, these differ from other web systems such as blogging software or static site generators in which content is created without a defined owner or leader. Wikis have little inherent structure, allowing one to emerge based on user needs. Wiki engines typically allow content
</code></pre>
<p>So, as can be seen from both (from facebook/m2m100_418M and google-translate) the translation from the m2m100_418M is shorter and I presume because the input was truncated because of model's ability.</p>
<p>How to translate longer sequences using this pretrained model? and if the option is to feed shorter sequences in a loop, how to find the threshold at which sentences end or to not let the context get lost, not at a character limit?</p>
<p>Here is my code:</p>
<pre><code>&gt;&gt;&gt; # translate Hindi to French
&gt;&gt;&gt; tokenizer.src_lang = &quot;eng&quot;
&gt;&gt;&gt; # `inp_text` variable has the text from above
&gt;&gt;&gt; inp_encoded = tokenizer(inp_text, return_tensors=&quot;pt&quot;)
&gt;&gt;&gt; generated_tokens = model.generate(**inp_encoded, forced_bos_token_id=tokenizer.get_lang_id(&quot;fr&quot;))
&gt;&gt;&gt; tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
</code></pre>
",Multilingual Language Processing & Language Identification,facebook model translate longer sequence text extracted following text wikipedia wiki tried translate french way shown model card found part text discarded translation french translation got english version according google translate wanted verify translation since know french seen facebook google translate translation shorter presume input wa truncated model ability translate longer sequence using pretrained model option feed shorter sequence loop find threshold sentence end let context get lost character limit code
"In NLTK, Can I do morphological analysis for specific language","<p>I am trying to add some arabic features into the NLTK,
but some tasks such as stemming need a morphological analysis. Is there any way to define the morphological features of specific language such as Arabic to NLTK or I must to customize the analyzer?</p>
",Multilingual Language Processing & Language Identification,nltk morphological analysis specific language trying add arabic feature nltk task stemming need morphological analysis way define morphological feature specific language arabic nltk must customize analyzer
How to plot Japanese string-based data in Python,"<p><strong>I have a csv like this:</strong></p>

<pre><code>Date, i, eat, chicken, you, fish, banana
2014-9-14, 1, 2, 1, 1, 1, 0
2014-10-15, 1, 1, 1, 0, 0, 0
2014-11-13, 0, 1, 0, 1, 0, 1
</code></pre>

<p>Forget about upper/lowercase and stemming because I will be morphological analyzing Japanese texts.</p>

<p><em>Ultimate Goal:</em> </p>

<p><a href=""http://imgur.com/uyTRQXR"" rel=""nofollow"">http://imgur.com/uyTRQXR</a> (I do not have enough reputation to post images.)</p>

<p><em>Note:</em> Y-axis is the word counts. It doesn't have to be a dot, x, square and some random shapes, just dots/x with different colors will be fine.</p>

<p>I want to use ggplot instead of matplotlib if possible.</p>
",Multilingual Language Processing & Language Identification,plot japanese string based data python csv like forget upper lowercase stemming morphological analyzing japanese text ultimate goal enough reputation post image note axis word count dot x square random shape dot x different color fine want use ggplot instead matplotlib possible
Is there a free library for morphological analysis of the German language?,"<p>I'm looking for a library which can perform a morphological analysis on German words, i.e. it converts any word into its root form and providing meta information about the analysed word.</p>

<p>For example:</p>

<pre><code>gegessen -&gt; essen
wurde [...] gefasst -&gt; fassen
H√§user -&gt; Haus
Hunde -&gt; Hund
</code></pre>

<p>My wishlist:</p>

<ul>
<li>It has to work with both nouns and verbs.</li>
<li>I'm aware that this is a very hard task given the complexity of the German language, so I'm also looking for libaries which provide only approximations or may only be 80% accurate.</li>
<li>I'd prefer libraries which don't work with dictionaries, but again I'm open to compromise given the cirumstances.</li>
<li>I'd also prefer C/C++/Delphi Windows libraries, because that would make them easier to integrate but .NET, Java, ... will also do.</li>
<li>It has to be a free library. (L)GPL, MPL, ...</li>
</ul>

<p><strong>EDIT:</strong> I'm aware that there is no way to perform a morphological analysis without any dictionary at all, because of the irregular words. 
When I say, I prefer a library without a dictionary I mean those full blown dictionaries which map each and every word:</p>

<pre><code>arbeite -&gt; arbeiten
arbeitest -&gt; arbeiten
arbeitet -&gt; arbeiten
arbeitete -&gt; arbeiten
arbeitetest -&gt; arbeiten
arbeiteten -&gt; arbeiten
arbeitetet -&gt; arbeiten
gearbeitet -&gt; arbeiten
arbeite -&gt; arbeiten
... 
</code></pre>

<p>Those dictionaries have several drawbacks, including the huge size and the inability to process unknown words.</p>

<p>Of course all exceptions can only be handled with a dictionary:</p>

<pre><code>esse -&gt; essen
isst -&gt; essen
e√üt -&gt; essen
a√ü -&gt; essen
a√üt -&gt; essen
a√üen -&gt; essen
...
</code></pre>

<p>(My mind is spinning right now :) )</p>
",Multilingual Language Processing & Language Identification,free library morphological analysis german language looking library perform morphological analysis german word e convert word root form providing meta information analysed word example wishlist ha work noun verb aware hard task given complexity german language also looking libaries provide approximation may accurate prefer library work dictionary open compromise given cirumstances also prefer c c delphi window library would make easier integrate net java also ha free library l gpl mpl edit aware way perform morphological analysis without dictionary irregular word say prefer library without dictionary mean full blown dictionary map every word dictionary several drawback including huge size inability process unknown word course exception handled dictionary mind spinning right
Creating tree data structure,"<p>i have some data:</p>

<pre><code>A
AXNHJNEHWXNOECMEJK
DNFJNXYEEQWhsdbchjsxs
XMJQWsdsEOJdfsKMDJE
</code></pre>

<p>....</p>

<p>Each row is array and each letter is object. I have comparer function which could say that letter A is equavalent of letter a(actually it is not letter. It's russian words and comparer function use morphology to let me know that word are equal for example –º–∞—Ç—Ä–µ—à–∫–∞==–º–∞—Ç—Ä–µ—à–∫–∏==–º–∞—Ç—Ä–µ—à–∫–∏–Ω—ã and arrays are russian sentences. For example: ""–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É""). I want to create tree data structure which looks like:</p>

<pre><code>1) A
2.1) BA
2.2) DHBAFH
3.1) BEDMEWA
etc...
</code></pre>

<p>Otherwise child nodes must contain letters from parent nodes. If you know how to work google adwords i think you can understand me. My question is how to do that FAST. I need to create tree with thousands arrays. Compare function works very slow(it use big dictionary) that's why speed is real problem.</p>

<p>Some simple data(sorry for russian):</p>

<p>here is set of sentences</p>

<pre><code>—Å–∞–π—Ç—ã        
—Å–∞–π—Ç—ã –Ω–µ–¥–æ—Ä–æ–≥–æ
—Å–∞–π—Ç—ã –¥–µ—à–µ–≤–æ
—Å–∞–π—Ç—ã –¥–µ—à–µ–≤–æ –∏ –±—ã—Å—Ç—Ä–æ
–∫—Ä–∞—Å–∏–≤—ã–π —Å–∞–π—Ç –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ü–µ–Ω–∞–º 
—Ö–æ—á—É –∫—É–ø–∏—Ç—å —Ö–æ—Ä–æ—à–∏–π —Å—Ç—É–ª 
—Å—Ç—É–ª –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ü–µ–Ω–∞–º
</code></pre>

<p>we must create following tree data structure</p>

<pre><code>1) —Å–∞–π—Ç—ã
1-&gt;2.1) —Å–∞–π—Ç—ã –Ω–µ–¥–æ—Ä–æ–≥–æ
1-&gt;2.2) —Å–∞–π—Ç—ã –¥–µ—à–µ–≤–æ
1-&gt;2.3) –∫—Ä–∞—Å–∏–≤—ã–π —Å–∞–π—Ç –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ü–µ–Ω–∞–º 
1-&gt;2.2-&gt;3) —Å–∞–π—Ç—ã –¥–µ—à–µ–≤–æ –∏ –±—ã—Å—Ç—Ä–æ
</code></pre>

<p>other parent nodes:</p>

<pre><code>1) —Ö–æ—á—É –∫—É–ø–∏—Ç—å —Ö–æ—Ä–æ—à–∏–π —Å—Ç—É–ª 
1) —Å—Ç—É–ª –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ü–µ–Ω–∞–º
</code></pre>

<p>Child nodes must contain more words then parent.</p>
",Multilingual Language Processing & Language Identification,creating tree data structure data row array letter object comparer function could say letter equavalent letter actually letter russian word comparer function use morphology let know word equal example array russian sentence example want create tree data structure look like otherwise child node must contain letter parent node know work google adwords think understand question fast need create tree thousand array compare function work slow use big dictionary speed real problem simple data sorry russian set sentence must create following tree data structure parent node child node must contain word parent
No such file or directory &#39;nltk_data/corpora/stopwords/English&#39; when using colab,"<p>First of all I am using Google colab for the work and
I have downloaded nltk stopwords for English with following:</p>

<pre><code>nltk.download('stopwords')
</code></pre>

<p>The download was successful</p>

<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
</code></pre>

<p>but when I run <code>stop = stopwords.words('English')</code></p>

<p>I am getting <code>OSError: No such file or directory: '/root/nltk_data/corpora/stopwords/English'</code></p>
",Multilingual Language Processing & Language Identification,file directory nltk data corpus stopwords english using colab first using google colab work downloaded nltk stopwords english following download wa successful run getting
How to extract subtitles from Youtube videos in varied languages,"<p>I have used the code below to extract subtitles from YouTube videos, but it only works for videos in English. I have some videos in Spanish, so I would like to know how I can modify the code to extract Spanish subtitles too?</p>
<pre class=""lang-py prettyprint-override""><code>from pytube import YouTube
from youtube_transcript_api import YouTubeTranscriptApi

# Define the video URL or ID of the YouTube video you want to extract text from
video_url = 'https://www.youtube.com/watch?v=xYgoNiSo-kY'

# Download the video using pytube
youtube = YouTube(video_url)
video = youtube.streams.get_highest_resolution()
video.download()

# Get the downloaded video file path
video_path = video.default_filename

# Get the video ID from the URL
video_id = video_url.split('v=')[-1]

# Get the transcript for the specified video ID
transcript = YouTubeTranscriptApi.get_transcript(video_id)

# Extract the text from the transcript
captions_text = ''
for segment in transcript:
    caption = segment['text']
    captions_text += caption + ' '

# Print the extracted text
print(captions_text)
</code></pre>
",Multilingual Language Processing & Language Identification,extract subtitle youtube video varied language used code extract subtitle youtube video work video english video spanish would like know modify code extract spanish subtitle
Efficient implementation of BPE using priority queue,"<p>I think that it is not strictly BPE (<a href=""https://en.wikipedia.org/wiki/Byte_pair_encoding"" rel=""nofollow noreferrer"">byte pair encoding</a>), but there is a similar idea applied to strings.</p>

<p>Suppose there are three Chinese words in the dictionary (I will use a huge dictionary like <a href=""https://cc-cedict.org/editor/editor.php?handler=Download"" rel=""nofollow noreferrer"">CEDICT</a> for practical use.)</p>

<ul>
<li>Êàë</li>
<li>ÂñúÊ¨¢</li>
<li>Ê∞¥Êûú</li>
</ul>

<p>Then take an input like this below.</p>

<p>ÊàëÂñúÊ¨¢Ê∞¥Êûú (I like fruit)</p>

<p>Since Chinese texts are not splitted by white spaces, it's difficult to process.</p>

<p>We can decompose the input string into multiple single characters.</p>

<p>Êàë Âñú Ê¨¢ Ê∞¥ Êûú</p>

<p>Then lookup new symbol pair at [left, right] and combine them. If the combined word is in the dictionary, we can replace the combined word with a new symbol.</p>

<ul>
<li>ÊàëÂñú</li>
<li>ÂñúÊ¨¢ &lt;- in the dic</li>
<li>Ê¨¢Ê∞¥</li>
<li>Ê∞¥Êûú &lt;- in the dic</li>
</ul>

<p>We found two new symbols, so the input text becomes </p>

<p>Êàë ÂñúÊ¨¢ Ê∞¥Êûú</p>

<p>We should iterate until we cannot find any combined word in the dictionary. In this case, we cannot find a new symbol in the dictionary.</p>

<ul>
<li>ÊàëÂñúÊ¨¢ Ê∞¥Êûú</li>
<li>ÂñúÊ¨¢Ê∞¥Êûú</li>
</ul>

<p>It's not difficult to implement this naively but we need to scan adjoining two words many times. Some said we can implement BPE efficiently with a priority queue. I'm not familiar with compression algorithms. I would be grateful if someone could tell me the implementation or useful documentations.</p>

<p>In this method, out of vocabulary words are decomposed into single characters, so we can avoid unknown words problems.</p>

<p>Best regards,</p>

<p>Reference: <a href=""https://arxiv.org/abs/1508.07909"" rel=""nofollow noreferrer"">Neural Machine Translation of Rare Words with Subword Units</a> He had to start with pre-tokenized words because of computational complexity.</p>
",Multilingual Language Processing & Language Identification,efficient implementation bpe using priority queue think strictly bpe byte pair encoding similar idea applied string suppose three chinese word dictionary use huge dictionary like cedict practical use take input like like fruit since chinese text splitted white space difficult process decompose input string multiple single character lookup new symbol pair left right combine combined word dictionary replace combined word new symbol dic dic found two new symbol input text becomes iterate find combined word dictionary case find new symbol dictionary difficult implement naively need scan adjoining two word many time said implement bpe efficiently priority queue familiar compression algorithm would grateful someone could tell implementation useful documentation method vocabulary word decomposed single character avoid unknown word problem best regard reference neural machine translation rare word subword unit start pre tokenized word computational complexity
Poor lemmatization in Apple&#39;s Natural Language framework,"<p>I did a quick experiment to check the accuracy of lemmatization in Apple's Natural Language framework and the results are quite poor.</p>

<p>I wonder if I am doing something wrong or if the framework is really that bad.</p>

<p>For the experiment, I used code straight from Apple's documentation (which also gets repeated in the few online examples I could find).</p>

<pre><code>let tagger = NLTagger(tagSchemes: [.lemma])
tagger.string = text
let options: NLTagger.Options = [.omitPunctuation, .omitWhitespace]
tagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .word, scheme: .lemma, options: options) { tag, tokenRange in
    print(""\(text[tokenRange]): \(tag?.rawValue ?? ""NO LEMMA"")"")
    return true
}
</code></pre>

<p>To test the output, I took a paragraph from a Euronews article, which is available in multiple languages.</p>

<p>The English version seems accurate, but in English, most words coincide with their lemma, so it's not a great benchmark.</p>

<p>I'm running the code in an Xcode Playground, on macOS 10.14.6. I tried both <em>macOS</em> and <em>iOS</em> as the platform for the playground, which makes no difference.</p>

<pre><code>let text = ""For the possible necessity of short-time work I want to make sure to build an incentive with connecting it to training. And I want Germany to be able to implement short-time work faster in case of a fast recession of the economic situation because of global economic risks.""

// Output

For: for
the: the
possible: possible
necessity: necessity
of: of
short: short
time: time
work: work
I: I
want: want
to: to
make: make
sure: sure
to: to
build: build
an: an
incentive: incentive
with: with
connecting: connect
it: it
to: to
training: training
And: and
I: I
want: want
Germany: Germany
to: to
be: be
able: able
to: to
implement: implement
short: short
time: time
work: work
faster: fast
in: in
case: case
of: of
a: a
fast: fast
recession: recession
of: of
the: the
economic: economic
situation: situation
because: because
of: of
global: global
economic: economic
risks: risk
</code></pre>

<p>I then tried Italian, which is my native language, so I can verify it easily. Here I started to see some problems.</p>

<pre><code>let text = ""Voglio assicurare che siano creati degli incentivi nel settore del lavoro a orario ridotto, collegati con periodi di training. E voglio che la Germania sia in grado di incrementare questo tipo di offerta lavorativa in modo veloce, in caso di recessione dell'economia per non farsi travolgere dai rischi che a livello globale subiremmo.""

// Output

Voglio: volersi
assicurare: assicurare
che: che
siano: essersi
creati: crearsi
degli: degli
incentivi: incentivo
nel: nel
settore: settore
del: del
lavoro: lavoro
a: a
orario: orario
ridotto: ridotto
collegati: collegarsi
con: con
periodi: periodo
di: di
training: training
E: e
voglio: volersi
che: che
la: la
Germania: Germania
sia: essersi
in: in
grado: grado
di: di
incrementare: incrementare
questo: questo
tipo: tipo
di: di
offerta: offerta
lavorativa: lavorativo
in: in
modo: modo
veloce: veloce
in: in
caso: caso
di: di
recessione: recessione
dell'economia: economia
per: per
non: non
farsi: farsi
travolgere: travolgere
dai: dai
rischi: rischio
che: che
a: a
livello: livello
globale: globale
subiremmo: subire
</code></pre>

<p>Here some verbs get strange lemmas: ""volersi"",  ""essersi"", ""crearsi"" are not the correct infinitive version of these verbs. For some reason, they are in a reflexive form. The problem though is that in many parts of the sample sentence they are not used reflexively.</p>

<p>But it's when I try with Russian (which I speak at an intermediate level) that things really fall apart.</p>

<pre><code>let text = ""–ß—Ç–æ–±—ã –±—ã—Ç—å –≥–æ—Ç–æ–≤—ã–º–∏ –∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–º—É –ø–æ—è–≤–ª–µ–Ω–∏—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π —Ä–∞–±–æ—Ç–µ, —è —Ö–æ—á—É —Å–æ–∑–¥–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –Ø —Ö–æ—á—É, —á—Ç–æ–±—ã –ì–µ—Ä–º–∞–Ω–∏—è –º–æ–≥–ª–∞ –±—ã—Å—Ç—Ä–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–±–æ—Ç—É –≤ —Å–∞–º—ã–µ —Å–∂–∞—Ç—ã–µ —Å—Ä–æ–∫–∏ –≤ —Å–ª—É—á–∞–µ –±—ã—Å—Ç—Ä–æ–≥–æ —Å–ø–∞–¥–∞ —ç–∫–æ–Ω–æ–º–∏–∫–∏ –∏–∑-–∑–∞ –Ω–∞–≤–∏—Å—à–∏—Ö –Ω–∞–¥ –Ω–µ–π –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤.""

// Output

–ß—Ç–æ–±—ã: —á—Ç–æ–±—ã
–±—ã—Ç—å: –±—ã—Ç—å
–≥–æ—Ç–æ–≤—ã–º–∏: NO LEMMA
–∫: –∫
–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–º—É: NO LEMMA
–ø–æ—è–≤–ª–µ–Ω–∏—é: NO LEMMA
–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏: NO LEMMA
–≤: –≤
–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π: NO LEMMA
—Ä–∞–±–æ—Ç–µ: NO LEMMA
—è: —è
—Ö–æ—á—É: NO LEMMA
—Å–æ–∑–¥–∞—Ç—å: NO LEMMA
–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: NO LEMMA
–¥–ª—è: –¥–ª—è
–æ–±—É—á–µ–Ω–∏—è: NO LEMMA
–Ø: —è
—Ö–æ—á—É: NO LEMMA
—á—Ç–æ–±—ã: —á—Ç–æ–±—ã
–ì–µ—Ä–º–∞–Ω–∏—è: –ì–µ—Ä–º–∞–Ω–∏—è
–º–æ–≥–ª–∞: –º–æ—á—å
–±—ã—Å—Ç—Ä–æ: NO LEMMA
–≤—ã–ø–æ–ª–Ω—è—Ç—å: NO LEMMA
—Ä–∞–±–æ—Ç—É: NO LEMMA
–≤: –≤
—Å–∞–º—ã–µ: —Å–∞–º—ã–π
—Å–∂–∞—Ç—ã–µ: NO LEMMA
—Å—Ä–æ–∫–∏: NO LEMMA
–≤: –≤
—Å–ª—É—á–∞–µ: —Å–ª—É—á–∞–π
–±—ã—Å—Ç—Ä–æ–≥–æ: NO LEMMA
—Å–ø–∞–¥–∞: —Å–ø–∞–¥
—ç–∫–æ–Ω–æ–º–∏–∫–∏: NO LEMMA
–∏–∑: –∏–∑
–∑–∞: –∑–∞
–Ω–∞–≤–∏—Å—à–∏—Ö: NO LEMMA
–Ω–∞–¥: –Ω–∞–¥
–Ω–µ–π: –Ω–µ–π
–≥–ª–æ–±–∞–ª—å–Ω—ã—Ö: NO LEMMA
—Ä–∏—Å–∫–æ–≤: —Ä–∏—Å–∫
</code></pre>

<p>Here most words produce no lemma. These are not rare words, and in my opinion, they should not be hard to lemmatize either (but I am no NLP expert).</p>

<p>For example, the word ""–±—ã—Å—Ç—Ä–æ"" (fast, quickly) is among the top 300 most common Russian words and is an adverb of the adjective ""–±—ã—Å—Ç—Ä—ã–π"". That's definitely a word I would expect a lemmatizer to recognize. Like the word ""—Ö–æ—á—É"" which is ""I want"".</p>

<p>The Italian output already leaves me perplexed, but the Russian one is definitely unusable.</p>

<p>Am I doing something wrong, or is Apple's framework really that bad? </p>
",Multilingual Language Processing & Language Identification,poor lemmatization apple natural language framework quick experiment check accuracy lemmatization apple natural language framework result quite poor wonder something wrong framework really bad experiment used code straight apple documentation also get repeated online example could find test output took paragraph euronews article available multiple language english version seems accurate english word coincide lemma great benchmark running code xcode playground macos tried macos io platform playground make difference tried italian native language verify easily started see problem verb get strange lemma volersi essersi crearsi correct infinitive version verb reason reflexive form problem though many part sample sentence used reflexively try russian speak intermediate level thing really fall apart word produce lemma rare word opinion hard lemmatize either nlp expert example word fast quickly among top common russian word adverb adjective definitely word would expect lemmatizer recognize like word want italian output already leaf perplexed russian one definitely unusable something wrong apple framework really bad
NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation),"<p>I'm attempting to fine-tune the NLLB model <code>&quot;facebook/nllb-200-distilled-600M&quot;</code> for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.</p>
<p>Documentation: <a href=""https://github.com/facebookresearch/fairseq/tree/nllb?tab=readme-ov-file"" rel=""nofollow noreferrer"">link</a></p>
<p>This is the code block which is giving error:</p>
<pre><code>DATA_CONFIG = &quot;/content/sample_data/data_config.json&quot;
OUTPUT_DIR = &quot;/content/outputs&quot;
MODEL_FOLDER = &quot;/content/drive/MyDrive/Thesis/nllb-checkpoints&quot;
DROP = 0.1
SRC = &quot;eng_Latn&quot;
TGT = &quot;deu_Latn&quot;
!python /content/fairseq/examples/nllb/modeling/train/train_script.py \
    cfg=nllb200_dense3.3B_finetune_on_fbseed \
    cfg/dataset=default \
    cfg.dataset.lang_pairs=&quot;$SRC-$TGT&quot; \
    cfg.fairseq_root=$(pwd) \
    cfg.output_dir=$OUTPUT_DIR \
    cfg.dropout=$DROP \
    cfg.warmup=10 \
    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt
</code></pre>
<p>This is the error:</p>
<pre><code>/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path=&quot;conf&quot;, config_name=&quot;base_config&quot;)
/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
TRAINING DIR:  /content/outputs
Error executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']
Traceback (most recent call last):
  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 289, in main
    train_module = TrainModule(config)
  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 122, in __init__
    assert cluster_name in cfg.dataset.data_prefix
omegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct
    full_key: cfg.dataset.data_prefix
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
</code></pre>
<p>So far, I understand there is a <code>Missing data_prefix configuration</code>. I created a demo custom data_config.json. Which looks like this:</p>
<pre><code>{
    &quot;data_prefix&quot;: &quot;/content/sample_data&quot;,
    &quot;train_data&quot;: &quot;train_demo.json&quot;,
    &quot;test_data&quot;: &quot;test_demo.json&quot;,
    &quot;lang_pairs&quot;: &quot;eng_Latn-deu_Latn&quot;
}
</code></pre>
<p>While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?</p>
",Multilingual Language Processing & Language Identification,nllb fine tuning error missing data prefix configuration english german translation attempting fine tune nllb model scientific translation task english eng latn german deu latn followed official guideline fine tuning author nllb documentation link code block giving error error far understand created demo custom data config json look like official documentation provides information encountering difficulty applying specific use case someone share detailed guide point helpful resource fine tuning nllb
ROUGE score metric for non-English (Arabic) language is not working,"<p>ROUGE score metric is not working for Arabic evaluation, what should I do?</p>
<pre><code>!pip install rouge_score
from datasets import load_metric
metric= load_metric(&quot;rouge&quot;)

pred_str =['ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉ']
label_str=['ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ ÿµÿØŸäŸÇŸä ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉ']

metric.add_batch(predictions=pred_str, references=label_str)
metric.compute()
</code></pre>
<p>Output:</p>
<pre><code>{‚Äòrouge1‚Äô: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)),
‚Äòrouge2‚Äô: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)),
‚ÄòrougeL‚Äô: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)),
‚ÄòrougeLsum‚Äô: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}
</code></pre>
",Multilingual Language Processing & Language Identification,rouge score metric non english arabic language working rouge score metric working arabic evaluation output
How to deal with word counts of zero when calculating Pointwise Mutual Information (PMI) for word cooccurrences in Natural Language Processing,"<p>I have a co-occurrence matrix of words in a text (two words x and y are considered co-occurring, if they both occur in a context window of w words). I want to calculate the Pointwise Mutual Information for two words x and y which I do using the common formula</p>
<p><code>PMI(x, y) = log2(P(x, y) / P(x)*P(y))</code>.</p>
<p>There may be cases, where <code>P(x)*P(y) = 0</code>, e.g.:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>X</th>
<th>Not X</th>
</tr>
</thead>
<tbody>
<tr>
<td>Y</td>
<td>30</td>
<td>0</td>
</tr>
<tr>
<td>Not Y</td>
<td>0</td>
<td>1500</td>
</tr>
</tbody>
</table></div>
<p>or</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>X</th>
<th>Not X</th>
</tr>
</thead>
<tbody>
<tr>
<td>Y</td>
<td>30</td>
<td>0</td>
</tr>
<tr>
<td>Not Y</td>
<td>1000</td>
<td>100</td>
</tr>
</tbody>
</table></div>
<p>How do I handle such cases in order to avoid a math error in my Python Script (division by zero) as well as avoiding messing up the data?</p>
<p>I tried to find information on websites explaining PMI, but they don't mention this special case. Either this does not happen often (which I cannot believe, since there must be something like &quot;perfect&quot; PMI) or the solution to this is so trivial, that everyone knows it, but no one speaks about it. What can be done to handle the problem?</p>
<p>My ideas so far:</p>
<ol>
<li>Define what should happen in such a case and catch it with an if-clause, then manually assign the desired value. But this seems inexact to me and depends on many non-binary factors. E.g., in table one there is a total correlation, in table two the correlation is rather coincidental, given that nearly the whole corpus consists of x and y is bound to occur with it.</li>
<li>Use some kind of additive smoothing as suggested in a comment on <a href=""https://stackoverflow.com/a/13492808/14393183"">this thread</a>, i.e. adding a positive value to all values involved in the calculation. But what should this value be in order to not skew the frequency distribution even for small corpora - 1, 0.1, 0.001, something totally different?</li>
</ol>
<p>I would be glad about any hint for procedures usually accepted when working with PMI.</p>
<hr />
<p><strong>EDIT</strong></p>
<p>It turns out that the problem I had was due to a misunderstanding of PMI on my side. In the above examples, P(x)*P(y) is not 0, because each word occurs at least 30 times.
Not the probability of x and y occurring without the other word respectively is relevant, but the probability of them occurring at all, which includes the times that they occur with the other word.</p>
<p>In case that x and/or y <em>never</em> occur in the entire corpus, manually catching this problem (e.g., like @Mustafa suggest in their answer) might help.</p>
",Multilingual Language Processing & Language Identification,deal word count zero calculating pointwise mutual information pmi word cooccurrences natural language processing co occurrence matrix word text two word x considered co occurring occur context window w word want calculate pointwise mutual information two word x using common formula may case e g x x x x handle case order avoid math error python script division zero well avoiding messing data tried find information website explaining pmi mention special case either doe happen often believe since must something like perfect pmi solution trivial everyone know one speaks done handle problem idea far define happen case catch clause manually assign desired value seems inexact depends many non binary factor e g table one total correlation table two correlation rather coincidental given nearly whole corpus consists x bound occur use kind additive smoothing suggested comment href thread e adding positive value value involved calculation value order skew frequency distribution even small corpus something totally different would glad hint procedure usually accepted working pmi edit turn problem wa due misunderstanding pmi side example p x p word occurs least time probability x occurring without word respectively relevant probability occurring includes time occur word case x never occur entire corpus manually catching problem e g like mustafa suggest answer might help
"Calling Asr model in BHasini throws - {&quot;code&quot;:&quot;something went wrong&quot;,&quot;message&quot;:null,&quot;timestamp&quot;:&quot;2024-07-03T06:08:52.407+00:00&quot;}","<p>I am trying to call the asr model of bhasini  api to get transcription of my audio.</p>
<p>I tried the following code.</p>
<pre><code>import requests
import json

url = &quot;https://meity-auth.ulcacontrib.org/ulca/apis/v0/model/getModelsPipeline&quot;

payload = {
  &quot;pipelineTasks&quot;: [
    {
      &quot;taskType&quot;: &quot;asr&quot;,
      &quot;config&quot;: {
        &quot;language&quot;: {
          &quot;sourceLanguage&quot;: &quot;kn&quot;
        }
      }
    }
  ],
  &quot;pipelineRequestConfig&quot;: {
            &quot;pipelineId&quot; : &quot;64392f96daac500b55c543cd&quot;
        }
  }
headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;userID&quot;: &quot;&quot;,
        &quot;ulcaApiKey&quot;: &quot;1&quot;
    }

response = requests.post('https://meity-auth.ulcacontrib.org/ulca/apis/v0/model/getModelsPipeline', json=payload, headers=headers)
if response.status_code == 200:
    response_data = response.json()

    service_id = response_data[&quot;pipelineResponseConfig&quot;][0][&quot;config&quot;][0][&quot;serviceId&quot;]
    print(service_id)

payload = {
  &quot;pipelineTasks&quot; : [
    {
        &quot;taskType&quot; : &quot;asr&quot;,
        &quot;language&quot;: {
                    &quot;sourceLanguage&quot;: &quot;hi&quot;
                },
        &quot;serviceId&quot;: service_id,
        &quot;audioFormat&quot;: &quot;flec&quot;,
        &quot;samplingRate&quot;: 16000
    }
],
  &quot;inputData&quot;: {
    &quot;audio&quot;: [
      {
        &quot;audioContent&quot;:&quot;ZkxhQ4AAACISABIAABnpAClACsRBcAADIKeTSPVYJt2ij/I04MboBPQk//hZDABoTv2jnP3pA/3LkP3zRf12d/16m/1inf0vDue2C+Us+mdYWxe+CSdyNfeq6WaNLipP61DryhyMTt4EMcTJE8lmn8jdmZG/LT87FbE4Y6r0G10enfExlS7mt0D3U/dDZt/vcIP+Cyj1Icx0dGAKOXFWhZH9tTMqG/jWZE8/dKAcNNSoSTDq2LkHRMnZZ+zC8Wnwxdiv0qzVdxvuxZGu2yOGtgWHTJIGyik8c6+gC8nbp6rFayA+eEVOZxWfIzwudaXDJdhmnnH9Rr/e9wym813+0c5TpbcKHh42ROTqzT1TRSS6kcgavT/xKq9Q2N3xWDu0dULrmjGUp4rTj1pvqLtTxMY6Ut8tkJeuRj4zsMmJ7H1ld60jQqta8+PRh/ESK/wTjXBo1o3dTjLLjEpRc7E1AMSxmaz6z89vK5JEGZZL4rCLLqEB8JwmgRPT+v6dmVRspAdDWAO9ECkJIG4ewD6ygDITAqFKARicAUfoFfMwJXcguiNBA/eApbEB7e4PLJQImXgtqXBaRCAJREDwLoL7lwRDggNgrAL/GBrmUE8vYM2RQecNgozxApwGBQzcDLToQ7BwqSUg9gzApgKAefkClF4HBMQjLlhAy9BCf2DEnkBtZYIp0wb2Cghy3CKG6D0msCz7oHw9QQmng1ZBAMDOB8iMDPQIXnzQLEtgnRhB/P2AhOUBxcomZNwQ2gg/YTGIv6CYvMZlU4xUJQcjDgGaXAfLqB1F4n5k=&quot;
      }
    ]
  }
}




response = requests.request(&quot;POST&quot;, url, headers=headers, json=payload)

print(response.text)

</code></pre>
<h4>Output</h4>
<p>ai4bharat/conformer-multilingual-dravidian-gpu--t4
{&quot;code&quot;:&quot;something went wrong&quot;,&quot;message&quot;:null,&quot;timestamp&quot;:&quot;2024-07-03T06:08:52.407+00:00&quot;}</p>
",Multilingual Language Processing & Language Identification,calling asr model bhasini throw code something went wrong message null timestamp trying call asr model bhasini api get transcription audio tried following code output ai bharat conformer multilingual dravidian gpu code something went wrong message null timestamp
"Python, using pdfplumber, pdfminer packages extract text from pdf, bolded characters duplicates","<p>Goal: extract <strong>Chinese</strong> financial report text</p>
<p>Implementation: Python pdfplumber/pdfminer package to extract PDF text to txt</p>
<p>problem: for PDF <em>text in bold</em>, corresponding extracted <em>text in txt duplicates</em></p>
<p>Examples are as follows:</p>
<p>Such as the following PDF text:
<a href=""https://i.sstatic.net/uESiY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uESiY.png"" alt=""pdf text"" /></a>
Python extracts to txt as:
<a href=""https://i.sstatic.net/A1eNI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A1eNI.png"" alt=""pdfplumber result"" /></a></p>
<p>And I don't need to repeat the text, just normal text.</p>
<p>How should I do it, should I change the package or add a new function?</p>
<p>Please see the code and original pdf text below.</p>
<p>Additional: pdfplumber code:</p>
<pre><code>import pdfplumber
 
def pdf2txt(filename, delLinebreaker=True):
    pageContent = ''
    showplace = ''
    try:    
        with pdfplumber.open(  filename  ) as pdf:
            page_count = len(pdf.pages)
            for page in pdf.pages:
                if delLinebreaker==True:
                    pageContent += page.extract_text().replace('\n', &quot;&quot;)   
                else:
                    pageContent += page.extract_text()  
    except Exception as e:
        print( &quot;file: &quot;, filename, ', reason: ', repr(e) )
    return pageContent
 
pdf2txt(r&quot;report.pdf&quot;, delLinebreaker=False)
</code></pre>
<p>pdfminer code:</p>
<pre><code>from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.pdfpage import PDFPage
 
rsrcmgr = PDFResourceManager()
outfp = open(r&quot;report.txt&quot;, 'w', encoding='utf-8')
device = TextConverter(rsrcmgr, outfp)
with open(r&quot;Report.pdf&quot;, 'rb') as fp:
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    for page in PDFPage.get_pages(fp):
        interpreter.process_page(page)
device.close()
outfp.close()
</code></pre>
<p>Result of pdfminer is:
<a href=""https://i.sstatic.net/AbNHh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AbNHh.png"" alt=""pdfminer result"" /></a></p>
<p>the pdf file can download here in Shenzhen Stock Exchange official website
<a href=""http://www.szse.cn/disclosure/listed/bulletinDetail/index.html?9324ce3c-6072-499d-8798-b25d641b52ec"" rel=""nofollow noreferrer"">http://www.szse.cn/disclosure/listed/bulletinDetail/index.html?9324ce3c-6072-499d-8798-b25d641b52ec</a></p>
",Multilingual Language Processing & Language Identification,python using pdfplumber pdfminer package extract text pdf bolded character duplicate goal extract chinese financial report text implementation python pdfplumber pdfminer package extract pdf text txt problem pdf text bold corresponding extracted text txt duplicate example follows following pdf text python extract txt need repeat text normal text change package add new function please see code original pdf text additional pdfplumber code pdfminer code result pdfminer pdf file download shenzhen stock exchange official website
Classify data on unstructured texts using python,"<p>I'm going to give an introduction to the project surroundings so you have some context for helping me out.</p>
<p>I'm trying to parse out information of german organizational charts in pdf format. Right now I can reliably read out the content of the rectangles from the PDF and get all textblocks that are contained in that rectangle. For each rectangle, I want to sort the information inside it into several categories. These are:</p>
<ul>
<li><p>Type (the type of the suborganization. e.g. Referat)</p>
</li>
<li><p>Title (the specific name of the suborganization. e.g. Referat XI A 9)</p>
</li>
<li><p>Description (the responsibilities. e.g. Personalien der Zollverwaltung und der Bundesmonopolverwaltung f√ºr Branntwein)</p>
</li>
<li><p>Person in charge (e.g. MR Hoffmann)</p>
</li>
</ul>
<p>I looked into several different approaches using python but I couldn't find a fitting solution. With spacy I found it hard to get any meaningful results because the text inside a rectangle isn't a real sentence and, as far as I understand, spacy relies on identifying the part-of-speech a token has in a sentence.<br />
I read into some other approaches using bag-of-words for example but this also doesn't seem to make sense because I again don't have full sentences. I'm currently implementing a really large but simple pattern matching solution where I check for keywords like &quot;Referat&quot; or the prefix of a person &quot;MR&quot; and categorize based on that. This regex-approach obviously fails pretty quickly as soon as I don't have a specified prefix or keyword to search for in the given text.</p>
<p>I also tried taking into account at which positions the textblocks are placed inside the content node and parse the information based on that. But this approach fails as soon as the order differs between the pdfs.</p>
<p>I'm looking for general advice that points me to a solution for this problem using python. The questions regarding this topic seems to be very old or focus only on a subset of the problems that I have so I think it would be helpful to have another discussion.</p>
",Multilingual Language Processing & Language Identification,classify data unstructured text using python going give introduction project surroundings context helping trying parse information german organizational chart pdf format right reliably read content rectangle pdf get textblocks contained rectangle rectangle want sort information inside several category type type suborganization e g referat title specific name suborganization e g referat xi description responsibility e g personalien der zollverwaltung und der bundesmonopolverwaltung f r branntwein person charge e g mr hoffmann looked several different approach using python find fitting solution spacy found hard get meaningful result text inside rectangle real sentence far understand spacy relies identifying part speech token ha sentence read approach using bag word example also seem make sense full sentence currently implementing really large simple pattern matching solution check keywords like referat prefix person mr categorize based regex approach obviously fails pretty quickly soon specified prefix keyword search given text also tried taking account position textblocks placed inside content node parse information based approach fails soon order differs pdfs looking general advice point solution problem using python question regarding topic seems old focus subset problem think would helpful another discussion
How is transformers loss calculated for blank token predictions?,"<p>I'm currently trying to implement a transformer and have trouble understanding its loss calculation.</p>
<p>My encoders input looks for batch_size=1  and max_sentence_length=8 like:</p>
<pre><code>[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>My decoders input looks like (german to english):</p>
<pre><code>[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Let's say my transformer predicted those class probabilities (only showing the word for the class with the highest class probability):</p>
<pre><code>[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Now I calculate the loss using:</p>
<pre><code>loss = categorical_crossentropy(
   [[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],
   [[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
)
</code></pre>
<p>Is this the correct way to calculate the loss? My transformer always predicts the blank token for the next word and I thought that's because I have a mistake in my loss calculation and have to do something with the blank tokens before calculating the loss.</p>
",Multilingual Language Processing & Language Identification,transformer loss calculated blank token prediction currently trying implement transformer trouble understanding loss calculation encoders input look batch size max sentence length like decoder input look like german english let say transformer predicted class probability showing word class highest class probability calculate loss using correct way calculate loss transformer always predicts blank token next word thought mistake loss calculation something blank token calculating loss
Pretrained Translation model(french to english) for food related data,"<p>I am working on a project that involves mapping between two food-related datasets. One dataset is in French, and the other is in English. Both datasets have a &quot;food name&quot; field, but I am not fluent in French and would prefer not to use Google Translate for this task. I have tried translating a few food items manually, but the translations are often inaccurate, as food names can have specific meanings that are not easily translated.</p>
<p>Is there a pre-trained language model or tool specifically designed for accurately translating food names between French and English? Any recommendations or advice would be greatly appreciated.</p>
<p>Thank you!</p>
<p>any links for suggestion to handle this task?</p>
",Multilingual Language Processing & Language Identification,pretrained translation model french english food related data working project involves mapping two food related datasets one dataset french english datasets food name field fluent french would prefer use google translate task tried translating food item manually translation often inaccurate food name specific meaning easily translated pre trained language model tool specifically designed accurately translating food name french english recommendation advice would greatly appreciated thank link suggestion handle task
save and load keras transformer model,"<p>i am still new to deep learning right now i follow this keras tutorial to make an translation model using transformer <a href=""https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"" rel=""nofollow noreferrer"">here the link</a>. Everything works just fine but i have no idea how to save the model, currently i have this implementation</p>
<pre><code>transformer.save(&quot;models/transformer.h5&quot;)
</code></pre>
<p>and load the model with this</p>
<pre><code>new_model = tf.keras.models.load_model(
    &quot;models/transformer.h5&quot;,
    custom_objects={
        &quot;PositionalEmbedding&quot;: PositionalEmbedding,
        &quot;TransformerEncoder&quot;: TransformerEncoder,
        &quot;TransformerDecoder&quot;: TransformerDecoder,
    },
)
new_model.summary()
</code></pre>
<p>i also attempt to save the vectorization and load it with pickle following <a href=""https://stackoverflow.com/questions/77774499/how-to-save-keras-textvectorization-layer-configuration-with-custom-standardizat"">this</a>
but when i use the model outside the notebook the model is worsen, when i load the model in the same notebook it just fine (without reloading the vectorization)</p>
<p>can someone guide me how to correctly how to save this model?</p>
",Multilingual Language Processing & Language Identification,save load kera transformer model still new deep learning right follow kera tutorial make translation model using transformer link everything work fine idea save model currently implementation load model also attempt save vectorization load pickle following href use model outside notebook model worsen load model notebook fine without reloading vectorization p someone guide correctly save model
How can i make a transformer output a translation relative to a specific context,"<p>I am working a machine-translation-like project, where i have a transformer with the encoder-decoder structure, which is supposed to generate SQL queries from natural language commands, example:</p>
<p><strong>Input:</strong> Count the number of aircrafts produced by the company XYZ</p>
<p><strong>Output:</strong> SELECT COUNT(*) FROM aircrafts WHERE manufacturer='XYZ';</p>
<p>Now i have kind of achieved my goal and my model can generate decent looking queries in most of cases, but there's still one problem to solve, which is that the queries it could generate are all SYNTACTICALLY correct but it doesn't nail the attributes/tables names, for example:</p>
<p><strong>Input:</strong> Count the number of aircrafts produced by the company XYZ</p>
<p><strong>Output:</strong> SELECT COUNT(*) FROM aircrafts WHERE produced='XYZ';</p>
<p>or:</p>
<p><strong>Input:</strong> Show all the employees older than 40</p>
<p><strong>Output:</strong> SELECT * FROM employee WHERE country &gt; 40;</p>
<p>So like i said, syntactically-speaking the query has no errors but it sure cannot be executed correctly on the database itself.</p>
<p>My model is trained on a json dataset that contains a list of samples who follows the following form:</p>
<pre><code>[
  [
    &quot;Count the number of aircraft produced by company XYZ&quot;,
    &quot;SELECT COUNT(*) FROM aircraft WHERE manufacturer = 'XYZ';&quot;
  ],
  [
    &quot;How many marine species are found in the Atlantic Ocean?&quot;,
    &quot;SELECT COUNT(*) FROM marine_species WHERE location = 'Atlantic Ocean';&quot;
  ]
]
</code></pre>
<p>Although i found some datasets that contains the schema of the database as a set of CREATE queries like this one:</p>
<pre><code>{&quot;instruction&quot;: &quot;CREATE TABLE table_72445 (
    \&quot;County\&quot; text,
    \&quot;Population\&quot; real,
    \&quot;Per capita income\&quot; text,
    \&quot;Median household income\&quot; text,
    \&quot;Median family income\&quot; text
)
-- Name the median family income for riverside&quot;,
&quot;output&quot;: &quot;SELECT \&quot;Median family income\&quot; FROM table_72445 WHERE \&quot;County\&quot; = 'Riverside'&quot;}
</code></pre>
<p>So is there anyway i can leverage this kind of dataset so i can give my transformer the database context it should work on, or is there any other way to achieve this task goal?</p>
<p>NOTE: i cannot make the previous corpus as a whole my input, because it will result in a huge performance-overhead. Imagine defining the very SAME set that contains tens of tables every single time that i want to execute, so this is no choice to take.</p>
",Multilingual Language Processing & Language Identification,make transformer output translation relative specific context working machine translation like project transformer encoder decoder structure supposed generate sql query natural language command example input count number aircraft produced company xyz output select count aircraft manufacturer xyz kind achieved goal model generate decent looking query case still one problem solve query could generate syntactically correct nail attribute table name example input count number aircraft produced company xyz output select count aircraft produced xyz input show employee older output select employee country like said syntactically speaking query ha error sure executed correctly database model trained json dataset contains list sample follows following form although found datasets contains schema database set create query like one anyway leverage kind dataset give transformer database context work way achieve task goal note make previous corpus whole input result huge performance overhead imagine defining set contains ten table every single time want execute choice take
How can I build an accurate dataset in a short span of time?,"<p>We are developing an iOS app that lets users send customizable digital cards. Users can choose from various card templates, enter their own text, and make edits to the card as they like. We also have a feature where users can provide a short message, like &quot;happy birthday mom,&quot; and receive an expanded version of the text, such as &quot;happy birthday to my special mother! I love you and hope you have an amazing day.&quot;</p>
<p>I'm conducting research to figure out how to accomplish this and planning to create a model using Natural Language Processing (NLP) and CoreML. However, I've encountered an issue in finding a suitable dataset for this particular task. As a result, I'm interested in building an accurate dataset specifically tailored to this purpose. However, I'm unsure where I can obtain the necessary data or if there are alternative data sources available for quick use.</p>
<p>If you have any insights or alternative methods to achieve this feature, please share them.</p>
",Multilingual Language Processing & Language Identification,build accurate dataset short span time developing io app let user send customizable digital card user choose various card template enter text make edits card like also feature user provide short message like happy birthday mom receive expanded version text happy birthday special mother love hope amazing day conducting research figure accomplish planning create model using natural language processing nlp coreml however encountered issue finding suitable dataset particular task result interested building accurate dataset specifically tailored purpose however unsure obtain necessary data alternative data source available quick use insight alternative method achieve feature please share
process_tweet and build_freqs,"<p>I am unable to import process_tweet and build_freqs into JupyterLab notebook for use in natural language processing. I have checked previous recommendations on stackoverflow for the resolution of this issue. None seems to work. Kindly assist.</p>
",Multilingual Language Processing & Language Identification,process tweet build freqs unable import process tweet build freqs jupyterlab notebook use natural language processing checked previous recommendation stackoverflow resolution issue none seems work kindly assist
FastText language_identification in R returns too many arguments - how to match to texts?,"<p>FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document.</p>
<p>There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts?</p>
<p>Code:</p>
<pre><code>DF = data.frame(doc_id = seq(1, 5),
speechtext = c(&quot;Hello. Fake text entry 1.&quot;, &quot;Fake text entry 2&quot;, &quot;more text&quot;, &quot;Text in a
different language&quot;, &quot;Hola&quot;))

library(fastText)
# download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html
file_ftz = system.file(&quot;language_identification/lid.176.ftz&quot;, package = &quot;fastText&quot;)
lang1 = language_identification(DF$speechtext,
                                pre_trained_language_model_path = file_ftz,
                                verbose = T)
</code></pre>
<p>I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with.</p>
<p>Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug.</p>
<p>(I tried adding intern = T as an argument per <a href=""https://stackoverflow.com/questions/65130621/r-fasttext-how-to-load-output-into-a-dataframe-from-command-line"">R - fasttext how to load output into a dataframe from command line</a> -- this is not recognized as an argument).</p>
",Multilingual Language Processing & Language Identification,fasttext language identification r return many argument match text fasttext language identification return multiple prediction per original text also fails indicate belong original document differing number prediction per original document github forum closed doe anyone know match output original text code wa expecting one prediction per original text least consistent number way marking document prediction align really could guess based largest number per series element outputted seem optimal doe seem like bug tried adding intern argument per href fasttext load output dataframe command line recognized argument
Trying to cluster short survey answers (1 to 10 words). Am I on the right track?,"<p>Here's the explanation of what i want to fully make (its a project for school).</p>
<ol>
<li>A user just puts in a file with just the answers to whatever question was asked in the survey.</li>
</ol>
<p>2.The machine finds similar answers and groups them under one label or cluster that is unnamed (thinking of using MeanShift, GMM, KMeans)</p>
<ol start=""3"">
<li>If possible I would also like it to generate a label for the cluster.</li>
</ol>
<p>4.Write the clustered and labeled answers back into a file to be checked and used for whatever purpose.</p>
<p>Some context on the data: lots of short (with some long, 10+ words) answers like &quot;i dont know&quot;, &quot;??&quot;, &quot;helpful&quot;, &quot;red&quot;, etc. and each has anywhere from 200 to 2000 answers.The answers are in dutch or french, would it be recomanded I translate them to english for better performance? Usually there are around 7 to 20 (high amounts are rare) clusters. I also have the right labels for the answers so I can check if the algorithm has clustered correctly.</p>
<p>I have tried looking into it and I need to vectorize my texts first, for this I tried TF-IDF and Count vectorizer in scikit. I also found their cheat sheet and It recommands i use MeanShift.</p>
<p>I havent tried looking for the best parameters yet, but the performance seem very poor (close to random). I used Adjusted Rand Index, Normalized Mutual Information and Silhouette Score to evbaluate.</p>
<p>Am I on the right track or are there better things out there? Vectorization methods, embeddings, clustering algorithms?</p>
<p>Edit: I just realized something that may overthrow the algorithm. In each survey there is a category named ¬¥others¬¥. It is a category with actual answers that are just irrelevant.</p>
<p>Possible workaroud i think way work: The gategory is ¬¥small¬¥ with around 8%. My clustering algorithm always makes way too many classes, but i could try to figure out a balance between combining classes of 1 or 2 answers into ¬¥others¬¥.</p>
",Multilingual Language Processing & Language Identification,trying cluster short survey answer word right track explanation want fully make project school user put file answer whatever question wa asked survey machine find similar answer group one label cluster unnamed thinking using meanshift gmm kmeans possible would also like generate label cluster write clustered labeled answer back file checked used whatever purpose context data lot short long word answer like dont know helpful red etc ha anywhere answer answer dutch french would recomanded translate english better performance usually around high amount rare cluster also right label answer check algorithm ha clustered correctly tried looking need vectorize text first tried tf idf count vectorizer scikit also found cheat sheet recommands use meanshift havent tried looking best parameter yet performance seem poor close random used adjusted rand index normalized mutual information silhouette score evbaluate right track better thing vectorization method embeddings clustering algorithm edit realized something may overthrow algorithm survey category named others category actual answer irrelevant possible workaroud think way work gategory small around clustering algorithm always make way many class could try figure balance combining class answer others
Transformer-based Yor&#249;b&#225; to English language processor : requirements issues,"<p>Following the successful implementation of the first transduction model that leveraged attention mechanism, I want to implement similar model applying it to a Nigerian language e.g. Yor√πb√°. However, I do not have a corpus that can help actualize this experiment. Is there a way of getting it considering the idea in Vaswani et al (2017) in their article 'Attention Is All You Need'? It is expected that the model achieves high Bilingual Evaluation Understudy metric. Help is needed here regarding the corpus as mentioned.</p>
<p>#Yor√πb√°-to-English translation task</p>
<p>Attempts had been made to browse the internet for it but all to no avail. Equally, I tried to check the WMT 2014 site, all to  no avail.</p>
",Multilingual Language Processing & Language Identification,transformer based yor b english language processor requirement issue following successful implementation first transduction model leveraged attention mechanism want implement similar model applying nigerian language e g yor b however corpus help actualize experiment way getting considering idea vaswani et al article attention need expected model achieves high bilingual evaluation understudy metric help needed regarding corpus mentioned yor b english translation task attempt made browse internet avail equally tried check wmt site avail
How to remove negation from japanese sentence and extract keywords (tfidf search )?,"<p>I want to remove negation from user query and extarct keywords from it for a search engine, for example .</p>
<p>`NISA„ÇíÈô§„ÅÑ„Å¶„É™„Çπ„Éà„ÇíË¶ã„Åõ</p>
<p>English translation : Show me the list excluding NISA</p>
<p>For this I don't want nisa as a keyword as user is excluding it.</p>
<p>I am using tfidf for keyword search .I am not able to handle negation in user sentences.</p>
<p>I tried using llm to manipulate my query but is not giving proper result below is the prompt I am using.</p>
<pre><code>[
{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Analyze user query and return only relevant keywords from it in . Give answer in this format 'keyword, keyword, keyword'. Give keywords in same language as the user query. Skip verbs like show, display. Also change the keywords to lowercase if they are in english language. Whenever query mentions 'not nisa' , 'excluding nisa' for this keyword would be 'tsumitate'. If a keyword is preceded by negation then don't return that keyword.Keep the tense of the keywords as it is. Do not skip any keyword&quot;}, 
{{#session}}
{{^last}}
{&quot;role&quot;:&quot;{{{role}}}&quot;, &quot;content&quot;:&quot;{{{content}}}&quot;},
{{/last}}
{{#last}}
{&quot;role&quot;:&quot;{{{role}}}&quot;, &quot;content&quot;:&quot; user query : '{{content}} ' &quot;}
{{/last}}
{{/session}}
]



</code></pre>
<p>Model I am using : gpt35-turbo chat model</p>
",Multilingual Language Processing & Language Identification,remove negation japanese sentence extract keywords tfidf search want remove negation user query extarct keywords search engine example nisa english translation show list excluding nisa want nisa keyword user excluding using tfidf keyword search able handle negation user sentence tried using llm manipulate query giving proper result prompt using model using gpt turbo chat model
Semantic search with NLP and elasticsearch,"<p>I am experimenting with elasticsearch as a search server and my task is to build a ""semantic"" search functionality. From a short text phrase like ""I have a burst pipe"" the system should infer that the user is searching for a plumber and return all plumbers indexed in elasticsearch. </p>

<p>Can that be done directly in a search server like elasticsearch or do I have to use a natural language processing (NLP) tool like e.g. Maui Indexer. What is the exact terminology for my task at hand, text classification? Though the given text is very short as it is a search phrase.</p>
",Multilingual Language Processing & Language Identification,semantic search nlp elasticsearch experimenting elasticsearch search server task build semantic search functionality short text phrase like burst pipe system infer user searching plumber return plumber indexed elasticsearch done directly search server like elasticsearch use natural language processing nlp tool like e g maui indexer exact terminology task hand text classification though given text short search phrase
Why sentencepiece tokenizer in nllb returns id&#39;s that differ by 1 when I specify model name or file?,"<p>When I'm using hf tokenizers and specify the model by name <code>facebook/nllb-200-distilled-600M</code> I get such tokens: <code>256047, 30311, 104, 253990</code> and so on. But when I take the <code>sentencepiece.bpe.model</code> from the repo and load it as a file I get this: <code>30310, 103, 253989</code>. I understand that the first token is the language that this model requires for knowing the translation, but why the next tokens differ by 1? The input string is &quot;English (<code>eng_Latn</code>) is set as the default language from which to translate.&quot; And this difference is consistent on the whole string.</p>
",Multilingual Language Processing & Language Identification,sentencepiece tokenizer nllb return id differ specify model name file using hf tokenizers specify model name get token take repo load file get understand first token language model requires knowing translation next token differ input string english set default language translate difference consistent whole string
How to get up and running with spaCy for Vietnamese?,"<p>I success with English</p>
<pre><code>python -m spacy download en_core_web_lg
python -m spacy download en_core_web_sm

python -m spacy download en
</code></pre>
<p>I read <a href=""https://spacy.io/models/xx"" rel=""nofollow noreferrer"">https://spacy.io/models/xx</a> . How to use with Vietnamese use <code>xx</code>?</p>
",Multilingual Language Processing & Language Identification,get running spacy vietnamese success english read use vietnamese use
Python Counter() function to count words in documents with more then one occurrence,"<p>I am working on an NLP (Natural Language Processing) project where I used the Python Counter() function from collections library. I am getting the results in the following form:</p>

<p><strong>OUTPUT</strong>:</p>

<pre><code>Counter({'due': 23, 'support': 20, 'ATM': 16, 'come': 12, 'case': 11, 'Sallu': 10, 'tough,': 9, 'team': 8, 'evident': , 'likely': 6, 'rupee': 4, 'depreciated': 2, 'senior': 1, 'neutral': 1, 'told': 1, 'tour\n\nRussia‚Äôs': 1, 'Vladimir': 1, 'indeed,': 1, 'welcome,‚Äù': 1, 'player': 1, 'added': 1, 'Games,': 1, 'Russia': 1, 'arrest': 1, 'system.\nBut': 1, 'rate': 1, 'Tuesday': 1, 'February,': 1, 'idea': 1, 'ban': 1, 'data': 1, 'consecutive': 1, 'interbank': 1, 'man,': 1, 'involved': 1, 'aggressive': 1, 'took': 1, 'sure': 1, 'market': 1, 'custody': 1, 'gang.\nWithholding': 1, 'cricketer': 1})
</code></pre>

<p>The problem is, I want to extract the words having count more than 1. In other words, I am trying to get only those words whose count is greater than 1 or 2.</p>

<p>I want to use the output to make a vocabulary list after reducing the words with low frequency.</p>

<p><strong>PS</strong>: I have more than 100 documents to test my data with almost 2000 distinct words.</p>

<p><strong>PPS</strong>: I have tried everything to get the results but unable to do so. I only need a logic and will be able to implement. </p>
",Multilingual Language Processing & Language Identification,python counter function count word document one occurrence working nlp natural language processing project used python counter function collection library getting result following form output problem want extract word count word trying get word whose count greater want use output make vocabulary list reducing word low frequency p document test data almost distinct word pps tried everything get result unable need logic able implement
Automatic Word Boundary Detection for German,"<p>I want to rephrase that: I need a corpus of German words so that I can check if a segment is a word. My solution so far is to take the string, check if it's in the dictionary and if not, delete the last character and check again, and so on. I just need a German word list now. Does anyone know something?</p>
<p>I have a bunch of German texts, but lost all whitespaces. Now I need to perform some kind of word boundary detection to get from &quot;Namens√§nderungimNamenderIntegration&quot; to [&quot;Namens√§nderung&quot;, &quot;im&quot;, &quot;Namen&quot;, &quot;der&quot;, &quot;Integration&quot;].</p>
<p>I found the python package wordsegment and it works okay, but not ideally. I also found the german_compound_splitter, but that would also split &quot;Namens√§nderung&quot; in &quot;Namens&quot; &quot;√§nderung&quot;. Does anyone have any experience with that or knows how I could build a solution?</p>
",Multilingual Language Processing & Language Identification,automatic word boundary detection german want rephrase need corpus german word check segment word solution far take string check dictionary delete last character check need german word list doe anyone know something bunch german text lost whitespaces need perform kind word boundary detection get namens nderungimnamenderintegration namens nderung im namen der integration found python package wordsegment work okay ideally also found german compound splitter would also split namens nderung namens nderung doe anyone experience know could build solution
Can I monitor progress of spacy parsing?,"<p>I have a simple program to process English text with spacy and output some of the info about the tokens. For a big text it takes a long time for spacy to process it. Is there a way to see how far the processing has progressed ideally as a percentage? I'm not using my own models, just ones provided by spacy.</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

// load big text file into `text` variable

nlp = spacy.load(&quot;en_core_web_sm&quot;)
nlp.max_length = len(text)+1
doc = nlp(text)

// output info
</code></pre>
",Multilingual Language Processing & Language Identification,monitor progress spacy parsing simple program process english text spacy output info token big text take long time spacy process way see far processing ha progressed ideally percentage using model one provided spacy
removing paywall language from piece of text (pandas),"<p>I'm trying to do some preprocessing on my dataset. Specifically, I'm trying to remove paywall language from the text (in bold below) but I keep getting an empty string as my output.</p>
<p>Here is the sample text:</p>
<blockquote>
<p>In order to put a stop to the invasive bush honeysuckle or Lonicera
Maackii currently taking over forests in Missouri and Kansas,
according to Debbie Neff of Excelsior Springs has organized an‚Ä¶
Premium Content is available to subscribers only. <strong>Please login here to
access content or go here to purchase a subscription.</strong></p>
</blockquote>
<p>and my custom function:</p>
<pre class=""lang-py prettyprint-override""><code>import re
import string
import nltk
from nltk.corpus import stopwords

# function to detect paywall-related text
def detect_paywall(text):
    paywall_keywords = [&quot;login&quot;, &quot;subscription&quot;, &quot;purchase a subscription&quot;, &quot;subscribers&quot;]
    for keyword in paywall_keywords:
        if re.search(r'\b{}\b'.format(keyword), text, flags=re.IGNORECASE):
            return True
    return False

# function for text preprocessing
def preprocess_text(text):
    # Check if the text contains paywall-related content
    if detect_paywall(text):
        # Remove paywall-related sentences or language from the text
        sentences = nltk.sent_tokenize(text)
        cleaned_sentences = [sentence for sentence in sentences if not detect_paywall(sentence)]
        cleaned_text = ' '.join(cleaned_sentences)
        return cleaned_text.strip()  # Remove leading/trailing whitespace

    # Tokenization
    tokens = nltk.word_tokenize(text)
    # Convert to lowercase
    tokens = [token.lower() for token in tokens]
    # Remove punctuation
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in stripped if word.isalpha() and word not in stop_words]
    return ' '.join(words)
</code></pre>
<p>I've tried modifying the list of words to detect but to no avail. However, I found that removing &quot;subscribers&quot; from the list does remove the second sentence of the paywall language. But that's not really ideal because there still remains the other half.</p>
<p>The function is also inconsistent because it works on this piece of text (as it will remove the paywall language), but not the one above.</p>
<blockquote>
<p>Of the hundreds of thousands of high school wrestlers, only a small percentage know what it‚Äôs like to win a state title. is part of that percentage. The Richmond junior joined that group by winning‚Ä¶ <strong>Premium Content is available to subscribers only. Please login here to access content or go here to purchase a subscription.</strong></p>
</blockquote>
",Multilingual Language Processing & Language Identification,removing paywall language piece text panda trying preprocessing dataset specifically trying remove paywall language text bold keep getting empty string output sample text order put stop invasive bush honeysuckle lonicera maackii currently taking forest missouri kansa according debbie neff excelsior spring ha organized premium content available subscriber please login access content go purchase subscription custom function tried modifying list word detect avail however found removing subscriber list doe remove second sentence paywall language really ideal still remains half function also inconsistent work piece text remove paywall language one hundred thousand high school wrestler small percentage know like win state title part percentage richmond junior joined group winning premium content available subscriber please login access content go purchase subscription
Remove everything from text except Hindi and English letters and numbers and punctuation,"<p>I have a text which is mixed in English and Hindi and I want to remove all the characters except Hindi and English characters and numbers and punctuation. That way, I can get rid of &quot;(&quot;, &quot;)&quot;,&quot;@&quot;, etc. Please consider the text below.</p>
<pre><code>text = ‡§®‡§à ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä‡•§ Navjot Singh Sidhu Resigns. ‡§™‡§Ç‡§ú‡§æ‡§¨ ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§ö‡•Å‡§®‡§æ‡§µ ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§™‡§Ç‡§ú‡§æ‡§¨ ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§Æ‡•á‡§Ç ‡§ï‡§à ‡§¨‡§°‡§º‡•á ‡§¨‡§¶‡§≤‡§æ‡§µ ‡§¶‡•á‡§ñ‡§®‡•á ‡§ï‡•ã ‡§Æ‡§ø‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ ‡§™‡§π‡§≤‡•á ‡§ï‡•à‡§™‡•ç‡§ü‡§® ‡§Ö‡§Æ‡§∞‡§ø‡§Ç‡§¶‡§∞ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡§æ ‡§™‡§Ç‡§ú‡§æ‡§¨ ‡§ï‡•á ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§™‡§¶ ‡§∏‡•á ‡§á‡§∏‡•ç‡§§‡•Ä‡§´‡§æ ‡§¶‡§ø‡§Ø‡§æ, ‡§â‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ö‡§∞‡§£‡§ú‡•Ä‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§ö‡§®‡•ç‡§®‡•Ä ‡§ï‡•ã ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§ï‡§æ ‡§®‡§Ø‡§æ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ ‡§µ‡§π‡•Ä‡§Ç ‡§Ö‡§¨ ‡§®‡§µ‡§ú‡•ã‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§∏‡§ø‡§¶‡•ç‡§ß‡•Ç ‡§®‡•á ‡§™‡§Ç‡§ú‡§æ‡§¨ ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑ ‡§™‡§¶ ‡§∏‡•á ‡§á‡§∏‡•ç‡§§‡•Ä‡§´‡§æ ‡§¶‡•á ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§I told you so‚Ä¶he is not a stable man and not fit for the border state of punjab.‚Äî Capt.Amarinder Singh (@capt_amarinder) September 28, 2021‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§ï‡•ã ‡§≤‡§ø‡§ñ‡§æ ‡§™‡§§‡•ç‡§∞ ‡§¨‡§§‡§æ ‡§¶‡•á‡§Ç ‡§ï‡§ø ‡§®‡§µ‡§ú‡•ã‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§∏‡§ø‡§¶‡•ç‡§ß‡•Ç ‡§®‡•á ‡§ï‡§æ‡§ó‡•ç‡§∞‡•á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§ï‡•ã ‡§è‡§ï ‡§™‡§§‡•ç‡§∞ ‡§≤‡§ø‡§ñ‡§ï‡§∞ ‡§á‡§∏ ‡§∏‡§Ç‡§¨‡§Ç‡§ß ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡•Ä ‡§π‡•à‡•§ ‡§™‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§∏‡§ø‡§¶‡•ç‡§ß‡•Ç ‡§®‡•á ‡§Ø‡§π ‡§≠‡•Ä ‡§ï‡§π ‡§ï‡§ø ‡§µ‡•á ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§¨‡§®‡•á ‡§∞‡§π‡•á‡§Ç‡§ó‡•á‡•§pic.twitter.com/L5wdRql5t3‚Äî Navjot Singh Sidhu (@sherryontopp) September 28, 2021
</code></pre>
",Multilingual Language Processing & Language Identification,remove everything text except hindi english letter number punctuation text mixed english hindi want remove character except hindi english character number punctuation way get rid etc please consider text
"logits and labels must have the same first dimension, got logits shape [100,5930] and labels shape [1900]","<p><a href=""https://i.sstatic.net/1opcU.png"" rel=""nofollow noreferrer"">Visual Representation of Model</a></p>
<p>I am working on a Machine Translation Task and I have used attention machenism for better results. It is english to urdu conversion task. My english dataset has longest sequence on length 14 and and urdu has 19 i have padded them and make sequences of equal length. X has shape i.e english train set (19620,14) and y i.e urdu target sequence has (19620,19). I have used embedding layer for my input and my target sequences are not one hot encoded as my target vocab size is 5930 so there is no benefit in getting this much sparse vectors. One more thing output layer has 5930 neurons which is equal to number of classes as it is size of my target vocab.</p>
<p>Now the issue is i am using sparse_categorical_crossentropy loss and i am getting this error:</p>
<pre><code>logits and labels must have the same first dimension, got logits shape [100,5930] and labels shape [1900]
</code></pre>
<p>i am also getting shape mismatch error in case of categorical cross entropy:
But when I change the number of classes in out put layer to 19 which is the length my target sequence it runs but loss is to much high and is overshooting approx in thousands. I f i one hot my target sequences to 5930 it also runs but same issue of loss. Because documentation says categorical cross entropy takes one hot representation but i can't do this.</p>
<p>In case of correct input no loss is working.</p>
<p>Here is the whole code</p>
<pre><code>English vocab Size:  5679
Urdu vocab Size:  5930
Max English sequence:  14
Max Urdu sequence:  19
X.shape=(19620,14)
Y.shape=(19620,19)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Preprocessing of Training Data
train_eng_seq,train_eng_vocab,train_eng_tok=Tokenize_fn(train_data['English-Sentences'])
train_urdu_seq,train_urdu_vocab,train_urdu_tok=Tokenize_fn(train_data['Urdu-Sentences'])
# Padding
train_eng_seq=pad_fn(train_eng_seq,length=english_length)
train_urdu_seq=pad_fn(train_urdu_seq,length=urdu_length)

# Preprocessing of Testing Data
test_eng_seq,test_eng_vocab,test_eng_tok=Tokenize_fn(test_data['English-Sentences'])
test_urdu_seq,test_urdu_vocab,test_urdu_tok=Tokenize_fn(test_data['Urdu-Sentences'])
# Padding
test_eng_seq=pad_fn(test_eng_seq,length=english_length)
test_urdu_seq=pad_fn(test_urdu_seq,length=urdu_length)

# It is because Our each english sequence has max laength of 14 and urdu has 19
Tx=english_length
Ty=urdu_length
repeator = RepeatVector(Tx)
concatenator = Concatenate(axis=-1)
densor1 = Dense(10, activation = &quot;tanh&quot;)
densor2 = Dense(1, activation = &quot;relu&quot;)
activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook
dotor = Dot(axes = 1)


def one_step_attention(a,s_prev):
  # We done this to change s_prev to shape of(m,Tx,n_s) for cocatination with a
  s_prev=repeator(s_prev)
  # We will here concatenate a and s_prev
  concat=concatenator([a,s_prev])
  # here i will calculate energies with 2 dense layers
  e=densor1(concat)
  energies=densor2(e)
  # we know alpha is softmax of this energy
  alpha=activator(energies)
  # to calculate context vector we take dot product of alpha and a
  context_vector=dotor([alpha,a])
  return context_vector


n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'
n_s = 64 # number of units for the post-attention LSTM's hidden state &quot;s&quot;

# this is the post attention LSTM cell.
post_activation_LSTM_cell = LSTM(n_s, return_state = True) 
output_layer = Dense(total_urdu_vocab, activation='softmax')


def modelf(Tx,Ty,n_a, n_s, total_eng_vocab, total_urdu_vocab):
  X=Input(shape=(english_length,)) # because embedding layer only demands the sequence length if i give full shape like (m,Tx) the
  # ouput of embedding layer will be 4D which can not be fed into BILSTM
  # hidden state for post LSTM
  s0 = Input(shape=(n_s,), name='s0')
  # cell state for post lstm
  # because we know From CampusX that shape of hidden and cell state of lstm are equal
  c0 = Input(shape=(n_s,), name='c0')
  s=s0
  c=c0
  outputs = []
  embedding_layer=tf.keras.layers.Embedding(total_eng_vocab,64,input_length=english_length)(X)
  a = Bidirectional(LSTM(n_a,return_sequences=True))(embedding_layer)

  for t in range(Ty):
    context=one_step_attention(a,s)
    _,s,c=post_activation_LSTM_cell(context,initial_state = [s,c] )
    out = output_layer(s)
    outputs.append(out)

    ''' Above the scene is like this:
        First we have initialized the hidden and cell state of post LSTM with zeros than our input goes through
        embedding layer than BiLSTM which return a which is a list of all the hidden states of BILSTM. Attention machenism works in a
        way that we take hidden state s which we have initialized with 0 and list of hidden ststes of BILSTM concat them and compute context
        vector as in one_step_attention function. we pass this context vector to one node of post LSTM to get hidden state s which passes
        through the output layer to give y1 same goes for 2nd word 3rd word etc.'''
  print(outputs)
  model=tf.keras.models.Model(inputs=[X,s0,c0],outputs=outputs)

  return model


model = modelf(Tx, Ty, n_a, n_s, total_eng_vocab, total_urdu_vocab)
opt = tf.keras.optimizers.Adam(learning_rate=0.005,beta_1=0.9,beta_2=0.999) # Adam(...)
model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])
m=train_eng_seq.shape[0]
s0 = np.zeros((m, n_s))
c0 = np.zeros((m, n_s))
model.fit([train_eng_seq, s0, c0], train_urdu_seq, epochs=50, batch_size=100)
</code></pre>
",Multilingual Language Processing & Language Identification,logits label must first dimension got logits shape label shape visual representation model working machine translation task used attention machenism better result english urdu conversion task english dataset ha longest sequence length urdu ha padded make sequence equal length x ha shape e english train set e urdu target sequence ha used embedding layer input target sequence one hot encoded target vocab size benefit getting much sparse vector one thing output layer ha neuron equal number class size target vocab issue using sparse categorical crossentropy loss getting error also getting shape mismatch error case categorical cross entropy change number class put layer length target sequence run loss much high overshooting approx thousand f one hot target sequence also run issue loss documentation say categorical cross entropy take one hot representation case correct input loss working whole code
Convert numbers to English strings,"<p>Websites like <a href=""http://www.easysurf.cc/cnvert18.htm"" rel=""nofollow"">http://www.easysurf.cc/cnvert18.htm</a> and <a href=""http://www.calculatorsoup.com/calculators/conversions/numberstowords.php"" rel=""nofollow"">http://www.calculatorsoup.com/calculators/conversions/numberstowords.php</a> tries to convert a numerical string into an english strings, but they are giving natural sounding output.</p>

<p>For example, on <a href=""http://www.easysurf.cc/cnvert18.htm"" rel=""nofollow"">http://www.easysurf.cc/cnvert18.htm</a>:</p>

<pre><code>[in]: 100456
[out]:  one hundred  thousand four hundred fifty-six
</code></pre>

<p>this website is a little better, <a href=""http://www.calculator.org/calculate-online/mathematics/text-number.aspx"" rel=""nofollow"">http://www.calculator.org/calculate-online/mathematics/text-number.aspx</a>:</p>

<pre><code>[in]: 100456
[out]: one hundred thousand, four hundred and fifty-six

[in]: 10123124001
[out]: ten billion, one hundred and twenty-three million, one hundred and twenty-four thousand, one 
</code></pre>

<p>but it breaks at some point:</p>

<pre><code>[in]: 10000000001
[out]: ten billion, , , one 
</code></pre>

<p>I've wrote my own version but it involves lots of rules and it caps at one billion, from <a href=""http://pastebin.com/WwFCjYtt"" rel=""nofollow"">http://pastebin.com/WwFCjYtt</a>:</p>

<pre><code>import codecs

def num2word (num):
  ones = {1:""one"",2:""two"",3:""three"",4:""four"",
          5:""five"",6:""six"",7:""seven"",8:""eight"",
          9:""nine"",0:""zero"",10:""ten""}
  teens = {11:""eleven"",12:""twelve"",13:""thirteen"",
           14:""fourteen"",15:""fifteen""}
  tens = {2:""twenty"",3:""thirty"",4:""forty"",
          5:""fifty"",6:""sixty"",7:""seventy"",
          8:""eighty"",9:""ninety""}
  lens = {3:""hundred"",4:""thousand"",6:""hundred"",7:""million"",
          8:""million"", 9:""million"",10:""billion""#,13:""trillion"",11:""googol"",
          }

  if num &gt; 999999999:
    return ""Number more than 1 billion""

  # Ones
  if num &lt; 11:
    return ones[num]
  # Teens
  if num &lt; 20:
    word = ones[num%10] + ""teen"" if num &gt; 15 else teens[num]
    return word
  # Tens
  if num &gt; 19 and num &lt; 100:
    word = tens[int(str(num)[0])]
    if str(num)[1] == ""0"":
      return word
    else:
      word = word + "" "" + ones[num%10]
      return word

  # First digit for thousands,hundred-thousands.
  if len(str(num)) in lens and len(str(num)) != 3:
    word = ones[int(str(num)[0])] + "" "" + lens[len(str(num))]
  else:
    word = """"

  # Hundred to Million  
  if num &lt; 1000000:
    # First and Second digit for ten thousands.  
    if len(str(num)) == 5:
      word = num2word(int(str(num)[0:2])) + "" thousand""
    # How many hundred-thousand(s).
    if len(str(num)) == 6:
      word = word + "" "" + num2word(int(str(num)[1:3])) + \
            "" "" + lens[len(str(num))-2]
    # How many hundred(s)?
    thousand_pt = len(str(num)) - 3
    word = word + "" "" + ones[int(str(num)[thousand_pt])] + \
            "" "" + lens[len(str(num))-thousand_pt]
    # Last 2 digits.
    last2 = num2word(int(str(num)[-2:]))
    if last2 != ""zero"":
      word = word + "" and "" + last2
    word = word.replace("" zero hundred"","""")
    return word.strip()

  left, right = '',''  
  # Less than 1 million.
  if num &lt; 100000000:
    left = num2word(int(str(num)[:-6])) + "" "" + lens[len(str(num))]
    right = num2word(int(str(num)[-6:]))
  # From 1 million to 1 billion.
  if num &gt; 100000000 and num &lt; 1000000000:
    left = num2word(int(str(num)[:3])) +  "" "" + lens[len(str(num))]
    right = num2word(int(str(num)[-6:]))
  if int(str(num)[-6:]) &lt; 100:
    word = left + "" and "" + right
  else:  
    word = left + "" "" + right
  word = word.replace("" zero hundred"","""").replace("" zero thousand"","" thousand"")
  return word

print num2word(int(raw_input(""Give me a number:\n"")))
</code></pre>

<p><strong>How can I make the script i've wrote accept <code>&gt; billion</code>?</strong></p>

<p><strong>Is there any other way to get the same output?</strong></p>

<p><strong>Can my code be written in a less verbose way?</strong></p>
",Multilingual Language Processing & Language Identification,convert number english string website like try convert numerical string english string giving natural sounding output example website little better break point wrote version involves lot rule cap one billion make script wrote accept way get output code written le verbose way
"How to load all non-digit letters of a particular language, let&#39;s say Russian?","<p>So, a function is returning some garbage as text output, and all the characters in that output are in Russian Cyrillic. To avoid this, I need to do some checks, for which a list of all characters in Russian Cyrillic is required.</p>
<p>Now, the question is can I generalize this to any given language? Given a language, can I get a list of all its characters?</p>
",Multilingual Language Processing & Language Identification,load non digit letter particular language let say russian function returning garbage text output character output russian cyrillic avoid need check list character russian cyrillic required question generalize given language given language get list character
Create a multilingual chatbot,"<p>I created a chatbot using PyTorch an I want to make it support the French language. Note that I want to train the chatbot so that it can respond to technical questions.</p>
<p>One of the things that came to my mind is to use translation APIs but since the chatbot is expected to respond to technical questions translation APIs might provide inaccurate information</p>
",Multilingual Language Processing & Language Identification,create multilingual chatbot created chatbot using pytorch want make support french language note want train chatbot respond technical question one thing came mind use translation apis since chatbot expected respond technical question translation apis might provide inaccurate information
Matching entities &amp; individuals using entity recognition in R(spacy),"<p>I am relatively new to entity recognition but I have been using this useful <a href=""https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html"" rel=""nofollow noreferrer"">guide</a> and I have a large text corpus of Congress policy debates in a Latin American country translated into English.</p>
<p>My goal is to explore which Congress members mention a specific free trade agreement called &quot;NAFTA&quot; the most and their political affiliation. I will then be analyzing sentiment or views about this agreement by political affiliation, but I am now working on the first task and I am not sure if entity recognition would be helpful.
The main political parties are abbreviated as &quot;WP&quot;, &quot;PRD&quot;, and &quot;PAN&quot;</p>
<p>Here is my current attempt:</p>
<pre><code>#Install and load required packages
# install.packages(&quot;pdftools&quot;)
# install.packages(&quot;spacyr&quot;)
library(pdftools)
library(&quot;spacyr&quot;)
library(quanteda)
library(dplyr)
# spacy_initialize(model = &quot;en_core_web_sm&quot;)
## successfully initialized (spaCy Version: 3.7.2, language model: en_core_web_sm)
</code></pre>
<pre><code>pdf_files &lt;- c(&quot;df1.pdf&quot;, &quot;df2.pdf&quot;, &quot;df3.pdf&quot;)
</code></pre>
<pre><code>#Function to extract text from PDFs
extract_text_from_pdf &lt;- function(pdf_files) {
  texts &lt;- lapply(pdf_files, function(file) {
    # Extract text from each PDF file
    pdf_text(file)
  })
  return(unlist(texts))
}

# Extract text from PDFs
pdf_texts &lt;- extract_text_from_pdf(pdf_files)

# Process text using SpaCy
parsed_texts &lt;- spacy_parse(pdf_texts)
parsed_texts
</code></pre>
<p>Here is a data example:</p>
<pre><code>dput(parsed_texts[1:25,(1:7)])
</code></pre>
<p>output:</p>
<pre><code>structure(list(doc_id = c(&quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, 
&quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, 
&quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, 
&quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;, &quot;text1&quot;
), sentence_id = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), token_id = 1:25, 
    token = c(&quot;12/19/23&quot;, &quot;,&quot;, &quot;4:41&quot;, &quot;PM&quot;, &quot;                                                &quot;, 
    &quot;about&quot;, &quot;:&quot;, &quot;blank&quot;, &quot;\n\n\n\n &quot;, &quot;Parliament&quot;, &quot;No&quot;, &quot;:&quot;, 
    &quot;               &quot;, &quot;14&quot;, &quot;\n\n &quot;, &quot;Session&quot;, &quot;No&quot;, &quot;:&quot;, &quot;                  &quot;, 
    &quot;1&quot;, &quot;\n\n &quot;, &quot;Volume&quot;, &quot;No&quot;, &quot;:&quot;, &quot;                   &quot;), 
    lemma = c(&quot;12/19/23&quot;, &quot;,&quot;, &quot;4:41&quot;, &quot;pm&quot;, &quot;                                                &quot;, 
    &quot;about&quot;, &quot;:&quot;, &quot;blank&quot;, &quot;\n\n\n\n &quot;, &quot;Parliament&quot;, &quot;no&quot;, &quot;:&quot;, 
    &quot;               &quot;, &quot;14&quot;, &quot;\n\n &quot;, &quot;Session&quot;, &quot;no&quot;, &quot;:&quot;, &quot;                  &quot;, 
    &quot;1&quot;, &quot;\n\n &quot;, &quot;volume&quot;, &quot;no&quot;, &quot;:&quot;, &quot;                   &quot;), 
    pos = c(&quot;NUM&quot;, &quot;PUNCT&quot;, &quot;NUM&quot;, &quot;NOUN&quot;, &quot;SPACE&quot;, &quot;ADP&quot;, &quot;PUNCT&quot;, 
    &quot;ADJ&quot;, &quot;SPACE&quot;, &quot;PROPN&quot;, &quot;NOUN&quot;, &quot;PUNCT&quot;, &quot;SPACE&quot;, &quot;NUM&quot;, 
    &quot;SPACE&quot;, &quot;PROPN&quot;, &quot;NOUN&quot;, &quot;PUNCT&quot;, &quot;SPACE&quot;, &quot;NUM&quot;, &quot;SPACE&quot;, 
    &quot;NOUN&quot;, &quot;NOUN&quot;, &quot;PUNCT&quot;, &quot;SPACE&quot;), entity = c(&quot;CARDINAL_B&quot;, 
    &quot;&quot;, &quot;TIME_B&quot;, &quot;TIME_I&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;ORG_B&quot;, &quot;ORG_I&quot;, &quot;ORG_I&quot;, 
    &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;CARDINAL_B&quot;, &quot;&quot;, &quot;&quot;, 
    &quot;&quot;, &quot;&quot;, &quot;&quot;)), row.names = c(NA, 25L), class = c(&quot;spacyr_parsed&quot;, 
&quot;data.frame&quot;))
</code></pre>
<p>Ideally, I would like an outcome that shows something as follows, while currently both the political party and the trade agreement are below the &quot;entity&quot; column in my df.</p>
<pre><code>token    entity       political affiliation      congress_member_share_of_NAFTA_mentions   
Rafael   NAFTA            WP                               3%
Martinez   NAFTA          WP                               7%
Martinez   NAFTA          WP                               7%
Alberto   NAFTA          PAN                               36%
Alberto   NAFTA          PAN                               36%
Rafael   NAFTA          PAN                                24%
Rafael   NAFTA          PAN                                24%
Alberto   NAFTA          PAN                               36%
</code></pre>
",Multilingual Language Processing & Language Identification,matching entity individual using entity recognition r spacy relatively new entity recognition using useful guide large text corpus congress policy debate latin american country translated english goal explore congress member mention specific free trade agreement called nafta political affiliation analyzing sentiment view agreement political affiliation working first task sure entity recognition would helpful main political party abbreviated wp prd pan current attempt data example output ideally would like outcome show something follows currently political party trade agreement entity column df
How does the model.resize_token_embeddings() function refactor the embeddings for newly added tokens in the tokenizer?,"<p>I am new to Natural Language Processing and currently working on machine translation using ALMA-7B model from Hugging Face. I wanted to create custom tokenizer based on the tokens that I have in my Word2Vec Embeddings and I also have their corresponding Embeddings (weights) with me. I am adding the tokens to tokenizers using following code:</p>
<p><code>alma_tokenizer.add_tokens(word_chunks)</code></p>
<p>Where <code>alma_tokenizer</code> is the Tokenizer for ALMA-7B model and <code>word_chunks</code> is a list of words I want to add. I want to update the model with its corresponding word embeddings as well, in the model and I was suggested to use <code>resize_token_embeddings()</code> function of <code>AutoModelForCausalLM</code>. When used it actually created new embeddings for the tokens I had added and I confirmed it as well. But my question is how are these embeddings created? Are they created randomly (as they are not a tensor of zeroes)? Can I insert my embeddings instead of the embeddings created by them?</p>
<p>Any kind of help will be appreciated!</p>
<pre><code>embeddings=model.resize_token_embeddings(len(tokenizer))
</code></pre>
",Multilingual Language Processing & Language Identification,doe model resize token embeddings function refactor embeddings newly added token tokenizer new natural language processing currently working machine translation using alma b model hugging face wanted create custom tokenizer based token word vec embeddings also corresponding embeddings weight adding token tokenizers using following code tokenizer alma b model list word want add want update model corresponding word embeddings well model wa suggested use function used actually created new embeddings token added confirmed well question embeddings created created randomly tensor zero insert embeddings instead embeddings created kind help appreciated
Unable to tag the POS of the text file,"<p>I want to tag the parts of speech of a sentence. For this task I am using <a href=""https://huggingface.co/flair/pos-english-fast"" rel=""nofollow noreferrer"">pos-english-fast</a> model. If there was one sentence the model identified the tags for the pos. I created a data file where I kept all my sentences. The name of the data file is 'data1.txt'. Now if I try to tag the sentences on the data file it does not work.</p>
<p>My code</p>
<pre><code>from flair.models import SequenceTagger
model = SequenceTagger.load(&quot;flair/pos-english&quot;)
#Read the data from the data.txt 
with open('data1.txt') as f:
  data = f.read().splitlines()
#Create a list of sentences from the data 
sentences = [sentence.split() for sentence in data]
#Tag each sentence using the model
tagged_sentences = []
for sentence in sentences:
  tagged_sentences.append(model.predict(sentence))
for sentence in tagged_sentences:
  print(sentence)
</code></pre>
<p>The error I received</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-16-03268ee0d9c9&gt; in &lt;cell line: 10&gt;()
      9 tagged_sentences = []
     10 for sentence in sentences:
---&gt; 11   tagged_sentences.append(model.predict(sentence))
     12 for sentence in tagged_sentences:
     13   print(sentence)

1 frames
/usr/local/lib/python3.10/dist-packages/flair/data.py in set_context_for_sentences(cls, sentences)
   1116         previous_sentence = None
   1117         for sentence in sentences:
-&gt; 1118             if sentence.is_context_set():
   1119                 continue
   1120             sentence._previous_sentence = previous_sentence

AttributeError: 'str' object has no attribute 'is_context_set'
</code></pre>
<p>The snapshot of the errors
<a href=""https://i.sstatic.net/0h0ZC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0h0ZC.png"" alt=""enter image description here"" /></a></p>
<p>How could I resolve it?</p>
",Multilingual Language Processing & Language Identification,unable tag po text file want tag part speech sentence task using po english fast model wa one sentence model identified tag po created data file kept sentence name data file data txt try tag sentence data file doe work code error received snapshot error could resolve
Add custom prompts to Llama 2 for RAG,"<p>I have downloaded Llama 2 locally and it works. Now I want to adjust my prompts/change the default prompt to force Llama 2 to anwser in a different language like German. Here is my code:</p>
<pre><code>from langchain.llms import LlamaCpp
from langchain.chains import LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.prompts import PromptTemplate
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import Chroma
# embeddings are numerical representations of the question and answer text
from langchain.embeddings import HuggingFaceEmbeddings

# use a common text splitter to split text into chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter

# for token-wise streaming so you'll see the answer gets generated token by token when Llama is answering your question
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

llm = LlamaCpp(
    model_path=&quot;MYPATH/llama.cpp/models/7B/ggml-model-q4_0.bin&quot;,
    temperature=0.0,
    top_p=1,
    n_ctx=6000,
    callback_manager=callback_manager, 
    verbose=True
)

# Load Pdf-File
loader = PyPDFLoader(&quot;myfile.pdf&quot;)
documents = loader.load()

# split the loaded documents into chunks 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
all_splits = text_splitter.split_documents(documents)

# create the vector db to store all the split chunks as embeddings
embeddings = HuggingFaceEmbeddings()
vectordb = Chroma.from_documents(
    documents=all_splits,
    embedding=embeddings,
)

# use another LangChain's chain, RetrievalQA, to associate Llama with the loaded documents stored in the vector db
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)

question = &quot;What is the biggest city in Germany?&quot;
result = qa_chain({&quot;query&quot;: question})
</code></pre>
<p>At which part do I have to insert my own prompt? As I understood correctl, now the default Llama 2 prompt is being used.  I tried to insert a prompt at the following part, but the model kept answering in English language:</p>
<pre><code>qa_chain = RetrievalQA.from_chain_type(
    llm(prompt=&quot;Please answer only in the German language!&quot;),
    retriever=vectordb.as_retriever()
)
</code></pre>
<p>I saw that the prompt template for Llama 2 looks as follows:</p>
<pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
&lt;&lt;/SYS&gt;&gt;

There's a llama in my garden üò± What should I do? [/INST]
</code></pre>
<p>Thanks in advance!</p>
",Multilingual Language Processing & Language Identification,add custom prompt llama rag downloaded llama locally work want adjust prompt change default prompt force llama anwser different language like german code part insert prompt understood correctl default llama prompt used tried insert prompt following part model kept answering english language saw prompt template llama look follows thanks advance
How to assign multiple tags to a token using OpenNLP?,"<p>I'm using OpenNLP and it works fine for detecting parts of speech and such when doing this:</p>
<pre><code>try (InputStream modelIn = new FileInputStream(&quot;en-pos-maxent.bin&quot;){
  POSModel model = new POSModel(modelIn);
  POSTaggerME tagger = new POSTaggerME(model);
  String tags[] = tagger.tag(tokenList);
}
</code></pre>
<p>so if tokens = [Test, Recipe, of, Incredible, Goodness, .]
then tags    = [ADJ, NOUN, ADP, ADJ, NOUN, PUNCT]</p>
<p>can I further add even more tags than just those defined as parts of speech?   what if I want to add a tag for short words, products, food, etc...</p>
<p>would i need to add a custom POS model with my definitions, run it in addition to the english POS model, and just have additional tag arrays for each POS model that I run the sentence through??</p>
<p>I have tried what I described, defining my own model and running it so that I have multiple arrays.   I was just wondering if there was some other way to do this that might be better than what I tried.</p>
",Multilingual Language Processing & Language Identification,assign multiple tag token using opennlp using opennlp work fine detecting part speech token test recipe incredible goodness tag adj noun adp adj noun punct add even tag defined part speech want add tag short word product food etc would need add custom po model definition run addition english po model additional tag array po model run sentence tried described defining model running multiple array wa wondering wa way might better tried
Is there any part of speech tagger and tokenizer of Tamil language?,"<p>I am a beginner in natural language processing. I have to work on different languages that Tamil is one of them. Could I ask from experts whether there is any Tamil language tokenizer code (java,c,python or etc.) and part of speech tagger codes that I use it for my research? </p>

<p>I really appreciate if I can get some experts' opinion here. Any help is appreciated.</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,part speech tagger tokenizer tamil language beginner natural language processing work different language tamil one could ask expert whether tamil language tokenizer code java c python etc part speech tagger code use research really appreciate get expert opinion help appreciated thanks
How to make stanza lemmatizer to return just the lemma instead of a dictionary?,"<p>I'm implementing stanza's lemmatizer because it works well with spanish texts but the lemmatizer retuns a whole dictionary with ID and other characteristics I don't care about for the time being. I checked the &quot;processors&quot; in the pipeline but I don't seem to find and example where I just get the sence with the lemmatized text instead of the dictionary.</p>
<p>This is what I have:</p>
<pre><code>stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True)
stNLP('me hubiera gustado mas ‚Äúsincronia‚Äù con la primaria')
</code></pre>
<p>Output:</p>
<pre><code>[
  [
    {
      &quot;id&quot;: 1,
      &quot;text&quot;: &quot;me&quot;,
      &quot;lemma&quot;: &quot;yo&quot;,
      &quot;upos&quot;: &quot;PRON&quot;,
      &quot;xpos&quot;: &quot;pp1cs000&quot;,
      &quot;feats&quot;: &quot;Case=Dat|Number=Sing|Person=1|PrepCase=Npr|PronType=Prs&quot;,
      &quot;start_char&quot;: 0,
      &quot;end_char&quot;: 2
    },
....
</code></pre>
<p>Of course when I try to lemmatize my document it returns a lot of text I don't need at the moment, how can I just obtain the lemma? I'm aware I could possibly extract the word from the dictionary but it takes a lot of time as it is, what I want to avoid is giving the fuction extra work.</p>
<p>Thank you in advance.</p>
",Multilingual Language Processing & Language Identification,make stanza lemmatizer return lemma instead dictionary implementing stanza lemmatizer work well spanish text lemmatizer retuns whole dictionary id characteristic care time checked processor pipeline seem find example get sence lemmatized text instead dictionary output course try lemmatize document return lot text need moment obtain lemma aware could possibly extract word dictionary take lot time want avoid giving fuction extra work thank advance
Most similar words within a given context,"<p>I want to create a deep learning model that can generate context-aware synonyms. I've been thinking about using BERT since it is bidirectional and creates good representations, and my idea was to use an approach where I provide the model with both the original sentence (e.g. &quot;It is a beautiful house&quot;) and the same sentence but where the word I want to find synonyms for is masked (e.g. &quot;It is a [MASK] house&quot;, if I want to find synonyms for beautiful).</p>
<p>A normal fill-mask obviously wouldn't work since it would not provide the model with the actual word we want to find synonyms for. I was thinking about using a machine translation-model (e.g. T5) instead, where you don't translate the sentence from one language to another but make an e.g. English-to-English translation where you provide the original sentence (&quot;It is a beautiful house&quot;) as input to the encoder and the masked sentence (&quot;It is a [MASK] house&quot;) as another input - this sentence would so to speak be the equivalent of an almost completed translation of the original sentence, and instead of simply translating the missing word, it would give me the top k most probable logits as synonyms.</p>
<p>However, I'm not sure how I can make this work at all... Another approach would be to train BERT on a domain-specific corpus and then get the k-nearest neighbors of the word I want to find synonyms for, but from what I've read it's not possible to get the word representations from a model like BERT the same way you would from Word2Vec and GloVe.</p>
<p>Any suggestions on how I can solve this challenge? Any help would be greatly appreciated...</p>
",Multilingual Language Processing & Language Identification,similar word within given context want create deep learning model generate context aware synonym thinking using bert since bidirectional creates good representation idea wa use approach provide model original sentence e g beautiful house sentence word want find synonym masked e g mask house want find synonym beautiful normal fill mask obviously work since would provide model actual word want find synonym wa thinking using machine translation model e g instead translate sentence one language another make e g english english translation provide original sentence beautiful house input encoder masked sentence mask house another input sentence would speak equivalent almost completed translation original sentence instead simply translating missing word would give top k probable logits synonym however sure make work another approach would train bert domain specific corpus get k nearest neighbor word want find synonym read possible get word representation model like bert way would word vec glove suggestion solve challenge help would greatly appreciated
Pretraining BERT Models from scratch vs Further Pretraining,"<p>I want to pretrain an Arabic BERT model on domain-specific data to make it suitable for a specific domain problem, which is the classification of citizen reviews about government services into relevant government sectors. My plan is to pretrain the model on freely available Arabic newspaper articles that specifically tackle the same sectors as the government ones, including education, healthcare, etc. I know these articles are not considered too specific to the target domain, but they are the only suitable data available. I plan to pretrain the model on around 20K articles only since I am limited with time and computational resources. Also, the target dataset contains about 2K citizen reviews provided in Modern Standard Arabic.</p>
<p>So, I have several questions concerning this project:</p>
<ol>
<li><p>Would it be beneficial to pretrain the Arabic BERT model from scratch using this small dataset of 20K samples? or would it be too small to tackle my problem?</p>
</li>
<li><p>Would it be better to apply further pretraining for Arabic BERT model, which means starting with the model initial knowledge (weights) and then further pretraining it on the 20K samples? I am afraid this will lead to model forgetting for the previously learnt knowledge. Also, the combination of general and specific knowledge might affect the model performance on the target dataset of citizen reviews.</p>
</li>
<li><p>Whichever method I choose from above, should I pretrain the model on unlabeled data (unsupervised learning)? or is it better to train it on labeled data to be useful for text classification?</p>
</li>
<li><p>After pretraining the model, should I apply feature extraction or fine-tuning on the target dataset of citizen reviews?</p>
</li>
</ol>
<p>After extensive research, I found that domain-specific models outperform the general ones. Also, it is advised to use pretraining from scartch to make the model specific to the target domain. However, this requires large amount of data and computational power. So, I am not sure if 20K samples are enough. Furthermore, I am not sure if further pretraining will be beneficial for the specific domain target data.</p>
",Multilingual Language Processing & Language Identification,pretraining bert model scratch v pretraining want pretrain arabic bert model domain specific data make suitable specific domain problem classification citizen review government service relevant government sector plan pretrain model freely available arabic newspaper article specifically tackle sector government one including education healthcare etc know article considered specific target domain suitable data available plan pretrain model around k article since limited time computational resource also target dataset contains k citizen review provided modern standard arabic several question concerning project would beneficial pretrain arabic bert model scratch using small dataset k sample would small tackle problem would better apply pretraining arabic bert model mean starting model initial knowledge weight pretraining k sample afraid lead model forgetting previously learnt knowledge also combination general specific knowledge might affect model performance target dataset citizen review whichever method choose pretrain model unlabeled data unsupervised learning better train labeled data useful text classification pretraining model apply feature extraction fine tuning target dataset citizen review extensive research found domain specific model outperform general one also advised use pretraining scartch make model specific target domain however requires large amount data computational power sure k sample enough furthermore sure pretraining beneficial specific domain target data
Which component in a transformer architecture is actually responsible form mapping a given word into the most likely next word?,"<p>I've been trying to gain intuition on how transformers work behind the scenes for language translation. I implemented in <a href=""https://docs.google.com/spreadsheets/d/1HdBN9cM0v7SWPZk_KJrpfMLncDw_ENJ7yFJobevJnak/edit?usp=sharing"" rel=""nofollow noreferrer"">a spreadsheet</a> in order to visualize the math behind and how the embeddings are transformed. But, there's one component that still is not clear to me which is: where the &quot;next word mapping&quot; is really occurring.</p>
<p>For example, let's consider a small inference task from Spanish &quot;yo estoy bien&quot; into English as &quot;I am fine&quot;. Given an input sequence that starts with <code>[&lt;BOS&gt;, 'I', 'am']</code>, which component of the transformer model is tasked with transforming the embedding of 'am' into the embedding for the subsequent word 'fine'? I know that all of them are somewhat involved, but which one is the key one?</p>
<p>Here's my current understanding of possible roles:</p>
<ul>
<li><p>Masked Causal Self-Attention: Maps relationships within words in the target language (e.g., between 'I' and 'am'), but does it aid in choosing 'fine' as the next word?</p>
</li>
<li><p>Cross-Attention: Finds correlations between source and target language words (e.g., 'I' with 'yo' and 'am' with 'sono'), but is it responsible for producing the next word in the sequence?</p>
</li>
<li><p>Feed-Forward Layers: Execute linear transformations of embeddings without considering the context from other words in the sentence. How could they be responsible for selecting 'fine' following 'am' without context from other embeddings?</p>
</li>
</ul>
<p>In short, what I see in the attention mechanisms is that they operate by adjusting an embedding to align more closely with the embeddings they share the greatest similarity with. But, what I would expect in the cross-attention is actually to see the word 'am' having a high similarity with most likely next work, 'bien'. So the new embedding from 'am' would be mapped into 'fine' which is the English equivalent embedding to 'bien'.</p>
<p>I hope I was able to make my question clear. It is hard for me to explain it. But I appreciate anyone who could give me some directions on the right path. Thanks</p>
",Multilingual Language Processing & Language Identification,component transformer architecture actually responsible form mapping given word likely next word trying gain intuition transformer work behind scene language translation implemented spreadsheet order visualize math behind embeddings transformed one component still clear next word mapping really occurring example let consider small inference task spanish yo estoy bien english fine given input sequence start component transformer model transforming embedding embedding subsequent word fine know somewhat involved one key one current understanding possible role masked causal self attention map relationship within word target language e g doe aid choosing fine next word cross attention find correlation source target language word e g yo sono responsible producing next word sequence feed forward layer execute linear transformation embeddings without considering context word sentence could responsible selecting fine following without context embeddings short see attention mechanism operate adjusting embedding align closely embeddings share greatest similarity would expect cross attention actually see word high similarity likely next work bien new embedding would mapped fine english equivalent embedding bien hope wa able make question clear hard explain appreciate anyone could give direction right path thanks
Can mT5 model on Huggingface be used for machine translation?,"<p>The <code>mT5</code> model is pretrained on the mC4 corpus, covering 101 languages:</p>
<blockquote>
<p>Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.</p>
</blockquote>
<h3>Can it do machine translation?</h3>
<p>Many users have tried something like this but it fails to generate a translation:</p>
<pre><code>from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained(&quot;google/mt5-small&quot;)

tokenizer = T5Tokenizer.from_pretrained(&quot;google/mt5-small&quot;)

article = &quot;translate to french: The capital of France is Paris.&quot;

batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], return_tensors=&quot;pt&quot;)
output_ids = model.generate(input_ids=batch.input_ids, num_return_sequences=1, num_beams=8, length_penalty=0.1)

tokenizer.decode(output_ids[0])
</code></pre>
<p>[out]:</p>
<pre><code>&gt;&gt;&gt; &lt;pad&gt; &lt;extra_id_0&gt;&lt;/s&gt;

</code></pre>
<h3>How do we make the mt5 model do machine translation?</h3>
",Multilingual Language Processing & Language Identification,mt model huggingface used machine translation model pretrained mc corpus covering language afrikaans albanian amharic arabic armenian azerbaijani basque belarusian bengali bulgarian burmese catalan cebuano chichewa chinese corsican czech danish dutch english esperanto estonian filipino finnish french galician georgian german greek gujarati haitian creole hausa hawaiian hebrew hindi hmong hungarian icelandic igbo indonesian irish italian japanese javanese kannada kazakh khmer korean kurdish kyrgyz lao latin latvian lithuanian luxembourgish macedonian malagasy malay malayalam maltese maori marathi mongolian nepali norwegian pashto persian polish portuguese punjabi romanian russian samoan scottish gaelic serbian shona sindhi sinhala slovak slovenian somali sotho spanish sundanese swahili swedish tajik tamil telugu thai turkish ukrainian urdu uzbek vietnamese welsh west frisian xhosa yiddish yoruba zulu machine translation many user tried something like fails generate translation make mt model machine translation
Language detection for short user-generated string,"<p>I need to detect the language of text sent in chat, and I am faced with 2 problems:</p>
<ul>
<li>the length of the message</li>
<li>the errors that may be in it and the noise (emoji etc...)</li>
</ul>
<p>For the noise, I clean the message and that works fine, but the length of the message is a problem.</p>
<p>For example, if a user writes &quot;hi&quot;, Fasttext detects the language as Dutch text, but Google Translate detects it as English. And most likely it is a message in English.</p>
<p>I try to train my own Fasttext model, but how can I adjust the model to have better results with short strings? Do I need to train the model with the dictionary of a lot of languages to get a better result?</p>
<p>I use Fasttext because it's the most accurate language detector.</p>
<p>Here is an exemple of the problem with Fasttext:</p>
<pre class=""lang-py prettyprint-override""><code># wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin

import fasttext

text = &quot;Hi&quot;

pretrained_lang_model = &quot;lid.176.bin&quot;
model = fasttext.load_model(pretrained_lang_model)

predictions = model.predict(text, k=2)
print(predictions)
# (('__label__de', '__label__en'), array([0.51606238, 0.31865335]))
</code></pre>
",Multilingual Language Processing & Language Identification,language detection short user generated string need detect language text sent chat faced problem length message error may noise emoji etc noise clean message work fine length message problem example user writes hi fasttext detects language dutch text google translate detects english likely message english try train fasttext model adjust model better result short string need train model dictionary lot language get better result use fasttext accurate language detector exemple problem fasttext
"Referring to &quot;short texts&quot; in topic modelling and natural language processing, what is the definition of the length of a short text?","<p>When it comes to &quot;short texts&quot; in topic modelling and natural language processing, what exactly is the definition of a short text? I have not been able to find a definitive answer. Could anyone provide a clear definition of the length of a &quot;short text&quot; in these two areas?</p>
<p>I've tried searching a lot of papers and I haven't seen anyone define short text clearly. I'm using <strong>Biterm</strong> for short texts, but how long a text can be considered a short text? The thesis in this <a href=""https://stackoverflow.com/questions/62280471/short-text-in-the-context-of-topic-modeling"">Similar answers</a>, which I also researched, but gave some examples to state that it was a short text and did not give a definition. I checked some other blogs and someone said that as long as it is less than 160 characters it is a short text. But I didn't find any academic basis for this.</p>
",Multilingual Language Processing & Language Identification,referring short text topic modelling natural language processing definition length short text come short text topic modelling natural language processing exactly definition short text able find definitive answer could anyone provide clear definition length short text two area tried searching lot paper seen anyone define short text clearly using biterm short text long text considered short text thesis href answer also researched gave example state wa short text give definition checked blog someone said long le character short text find academic basis
Best way to identify and extract dates from text Python?,"<p>As part of a larger personal project I'm working on, I'm attempting to separate out inline dates from a variety of text sources.</p>

<p>For example, I have a large list of strings (that usually take the form of English sentences or statements) that take a variety of forms:</p>

<blockquote>
  <p>Central design committee session Tuesday 10/22 6:30 pm</p>
  
  <p>Th 9/19 LAB: Serial encoding (Section 2.2)</p>
  
  <p>There will be another one on December 15th for those who are unable to make it today.</p>
  
  <p>Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm</p>
  
  <p>He will be flying in Sept. 15th.</p>
</blockquote>

<p>While these dates are in-line with natural text, none of them are in specifically natural language forms themselves (e.g., there's no ""The meeting will be two weeks from tomorrow""‚Äîit's all explicit).  </p>

<p>As someone who doesn't have too much experience with this kind of processing, what would be the best place to begin? I've looked into things like the <code>dateutil.parser</code> module and <a href=""https://github.com/bear/parsedatetime"">parsedatetime</a>, but those seem to be for <em>after</em> you've isolated the date.</p>

<p>Because of this, is there any good way to extract the date and the extraneous text </p>

<pre><code>input:  Th 9/19 LAB: Serial encoding (Section 2.2)
output: ['Th 9/19', 'LAB: Serial encoding (Section 2.2)']
</code></pre>

<p>or something similar? It seems like this sort of processing is done by applications like Gmail and Apple Mail, but is it possible to implement in Python?</p>
",Multilingual Language Processing & Language Identification,best way identify extract date text python part larger personal project working attempting separate inline date variety text source example large list string usually take form english sentence statement take variety form central design committee session tuesday pm th lab serial encoding section another one december th unable make today workbook minimum due wednesday pm flying sept th date line natural text none specifically natural language form e g meeting two week tomorrow explicit someone much experience kind processing would best place begin looked thing like module href seem em isolated date good way extract date extraneous text something similar seems like sort processing done application like gmail apple mail possible implement python
Add new entity recognizer in Presidio,"<p>I'm testing a data anonymizer that I found on <a href=""https://github.com/akazah/prompt-anonymizer/"" rel=""nofollow noreferrer"">github</a>. But I want to add a new entity, because the author of the repository only uses <code>[&quot;PERSON&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;LOCATION&quot;, &quot;PHONE_NUMBER&quot;]</code> as entities. So based on the presidio <a href=""https://microsoft.github.io/presidio/samples/python/presidio_notebook/"" rel=""nofollow noreferrer"">documentation</a> I add the method <code>add_entity</code> into PresidioHandler class:</p>
<pre><code>from presidio_analyzer import AnalyzerEngine, PatternRecognizer
from presidio_analyzer.nlp_engine import NlpEngineProvider
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig
import yaml

LANGUAGES = [&quot;en&quot;, &quot;es&quot;]
ENTITIES = [&quot;PERSON&quot;,&quot;EMAIL_ADDRESS&quot;,&quot;LOCATION&quot;,&quot;PHONE_NUMBER&quot;]
NLP_ENGINE=&quot;spacy&quot;
LANGUAGE_LABELS_PATH = &quot;i18n/&quot;
NLP_MODELS=[
    {&quot;lang_code&quot;: &quot;en&quot;, &quot;model_name&quot;: &quot;en_core_web_lg&quot;},
    {&quot;lang_code&quot;: &quot;es&quot;, &quot;model_name&quot;: &quot;es_core_news_lg&quot;},
]

class PresidioHandler:
    def __init__(self):
        self.languages = LANGUAGES
        self.entities = ENTITIES
        self.entities_language_labels = self._build_entities_language_labels(LANGUAGE_LABELS_PATH)

        self.analyzer = AnalyzerEngine(
            nlp_engine=NlpEngineProvider(
                nlp_configuration={
                    &quot;nlp_engine_name&quot;: NLP_ENGINE,
                    &quot;models&quot;: NLP_MODELS}
            ).create_engine(),
            supported_languages=self.languages
        )
        self.anonymizer = AnonymizerEngine()
        self.operators = {entity: OperatorConfig(&quot;hash&quot;) for entity in self.entities}

    def _build_entities_language_labels(self, language_labels_path):
        return {
            entity: {
                lang: self._load_language_labels(language_labels_path, lang)[entity] 
                for lang in self.languages
            } 
            for entity in self.entities
        }

    def _load_language_labels(self, language_labels_path, language):
        with open(f'{language_labels_path}/{language}.yaml', 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    @staticmethod
    def _int_to_alphabet(num):
        if num &lt; 26:
            return chr(65 + num)
        elif num &lt; 52:
            return chr(97 + num - 26)
        else:
            return chr(48 + num - 52)

    def _create_entities_hash_dict(self, items, language):
        entities_hash_dict = {}
        for entity in self.entities:
            order = 0
            for item in items:
                if item.entity_type == entity:
                    if item.text not in entities_hash_dict:
                        entities_hash_dict[item.text] = {&quot;replace&quot;: self.entities_language_labels[entity][language] + &quot;_&quot; + self._int_to_alphabet(order), &quot;count&quot;: 1}
                        order += 1
                    else:
                        entities_hash_dict[item.text][&quot;count&quot;] += 1
        return entities_hash_dict

    @staticmethod
    def _replace_hash(text, entities_hash_dict):
        for key, value in entities_hash_dict.items():
            text = text.replace(key, f&quot;{value['replace']}&quot;)
        return text

    def anonymize_text(self, text, language):
        anonymizer_result = self.anonymizer.anonymize(
            text=text,
            analyzer_results=self.analyzer.analyze(text=text, entities=self.entities, language=language),
            operators=self.operators
        )

        items = anonymizer_result.items
        items.sort(key=lambda x: x.start)
        entities_hash_dict = self._create_entities_hash_dict(items=items, language=language)

        return self._replace_hash(text=anonymizer_result.text, entities_hash_dict=entities_hash_dict)
    
    def add_entity(self, entity_name, entity_list):
        self.entities.append(entity_name)
        entity_recognizer = PatternRecognizer(supported_entity=entity_name, deny_list=entity_list)
        self.analyzer.registry.add_recognizer(entity_recognizer)
</code></pre>
<p>Also add in the yaml files the label for each language of the new entity.
<a href=""https://i.sstatic.net/apXDq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/apXDq.png"" alt=""enter image description here"" /></a>
I tested as follows:</p>
<pre><code>from presidio_handler import PresidioHandler

if __name__ == '__main__':
    presidio_handler = PresidioHandler()
    presidio_handler.add_entity('COMPANY', [&quot;Microsoft&quot;, &quot;Apple&quot;])
    text = &quot;Daniel y Alvaro trabajan en Microsoft y Apple&quot;
    language = &quot;es&quot;
    result = presidio_handler.anonymize_text(text, language)
    print(result)
</code></pre>
<p>When I test it in English everything works fine. But when I try in spanish I get:</p>
<pre><code>Nombre_A y Nombre_B trabajan en Microsoft y Apple
</code></pre>
<p>The desired output is:</p>
<pre><code>Nombre_A y Nombre_B trabajan en Compa√±ia_A y Compa√±ia_B
</code></pre>
<p>Does anyone know what I am doing wrong? Or what do you suggest me to create new entities? The problem is the model_name for spanish in NLP_MODELS?</p>
",Multilingual Language Processing & Language Identification,add new entity recognizer presidio testing data anonymizer found github want add new entity author repository us entity based presidio documentation add method presidiohandler class also add yaml file label language new entity tested follows test english everything work fine try spanish get desired output doe anyone know wrong suggest create new entity problem model name spanish nlp model
MarianMTModel stops translating on encountering ‚Äú-‚Äù character,"<p>I‚Äôm trying to translate simple text from Polish to English:</p>
<p>‚Äú≈ªycie nigdy siƒô nie ko≈Ñczy ‚Äì przygotuj siƒô zatem na ciƒÖg dalszy. Zasilany twojƒÖ energiƒÖ zegarek z widocznym mechanizmem dopasuje siƒô do ciebie..‚Äù</p>
<p>The model behaves strangely, when it encounters the <code>-</code> char it stops translating only returning the translation of what precedes the <code>-</code> char.
When I move this char the translation always ends before it.</p>
<p>After further investigation, the model returns the generated ids:
<code>tensor([[63429, 7157, 522, 10126, 15, 0]])</code></p>
<p>when decoded:
<code>'&lt;pad&gt; Life never ends &lt;/s&gt;'</code>.</p>
<p>Surprisingly, when I use <code>num_beams</code> set to 2 instead of 1 I get a good result. The problem is that because of time constraints I can't use <code>num_beams=2</code></p>
<p>Does anyone know what is happening?</p>
",Multilingual Language Processing & Language Identification,marianmtmodel stop translating encountering character trying translate simple text polish english ycie nigdy si nie ko czy przygotuj si zatem na ci g dalszy zasilany twoj energi zegarek z widocznym mechanizmem dopasuje si ciebie model behaves strangely encounter char stop translating returning translation precedes char move char translation always end investigation model return generated id decoded surprisingly use set instead get good result problem time constraint use doe anyone know happening
Documentation of Moses (statistical machine translation) mose.ini file format?,"<p>Is there any documentation of the moses.ini format for Moses? Running moses at the command line without arguments returns available feature names but not their available arguments. Additionally, the structure of the .ini file is not specified in the manual that I can see.</p>
",Multilingual Language Processing & Language Identification,documentation moses statistical machine translation mose ini file format documentation moses ini format moses running moses command line without argument return available feature name available argument additionally structure ini file specified manual see
Transformer train a new tokenizer base on existing one,"<p>In the following code</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)
tokenizer_new = tokenizer.train_new_from_iterator(training_corpus, 50000, new_special_tokens = ['ÂÅ•Â∫∑','ÂåªÂ≠¶','ËØïÂâÇÁõí',....])
</code></pre>
<p>where training_corpus is an iterator generating lines of text from hard drive chinese_medical.txt file</p>
<p>For readers who may not familiar with &quot;bert-base-chinese&quot;, it is a single character tokenizer with &quot;wordpiece&quot; model as default</p>
<p>My question is</p>
<p><strong>tokenizer_new</strong> has totally different indexing from <strong>tokenizer</strong>, for example</p>
<pre><code>tokenizer.vocab['ÂÖà'] #1044
tokenizer_new.vocab['ÂÖà'] #5151
</code></pre>
<p>this makes continue training (i.e. train a Chinese medical specific BERT model) on model</p>
<pre><code>model = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-chinese&quot;)
</code></pre>
<p>impossible. Because the indexing id for the same character is totally different in <strong>tokenizer_new</strong></p>
<p>How to make the indexing for the same character fixed?</p>
",Multilingual Language Processing & Language Identification,transformer train new tokenizer base existing one following code training corpus iterator generating line text hard drive chinese medical txt file reader may familiar bert base chinese single character tokenizer wordpiece model default question tokenizer new ha totally different indexing tokenizer example make continue training e train chinese medical specific bert model model impossible indexing id character totally different tokenizer new make indexing character fixed
Is there a language detection that detects Arabic and Persian languages?,"<p>I have a dataset of twitter texts. Most of the tweets in this dataset are in Persian and some of them are in Arabic.
I want to find Arabic tweets.
Is there an API or a tool that can do it for me?
If I want to explain more, I want a language detection that classifies tweets in Persian and Arabic languages.
Thanks.</p>
",Multilingual Language Processing & Language Identification,language detection detects arabic persian language dataset twitter text tweet dataset persian arabic want find arabic tweet api tool want explain want language detection classifies tweet persian arabic language thanks
Is there such thing as dataset improvement?,"<p>I know that we can use explained machine learning to find why a model chose a certain classification.</p>
<p>I wonder if there is a way I can find which features are going to improve my current model.</p>
<p>I will explain what I mean by this.</p>
<p>Case: NLP classification of sports, there is a paragraph talking about Ronaldo scores against Uruguay...</p>
<p>Is there a method that can ask which Ronaldo you mean (Ronaldo de Lime the Brazilian player or Cristiano Ronaldo the Portuguese)?</p>
<p>so the model can get a higher accuracy result to classify the paragraph about Brazilian Team or about Portugal Team?</p>
",Multilingual Language Processing & Language Identification,thing dataset improvement know use explained machine learning find model chose certain classification wonder way find feature going improve current model explain mean case nlp classification sport paragraph talking ronaldo score uruguay method ask ronaldo mean ronaldo de lime brazilian player cristiano ronaldo portuguese model get higher accuracy result classify paragraph brazilian team portugal team
How to know if two words are the same base?,"<p>I want to make an app where a user inputs some word, and then that word is detected and highlighted on the page. The difficult part is that I want all forms of the word to be detected. This would ultimately be for translation purposes.</p>
<p>So if the user enters <code>eat</code> I want to have that match with <code>eat</code>, <code>eats</code>, <code>eating</code>, and <code>ate</code>.</p>
<p>Ideally this supports multiple languages as well.</p>
<p>Is there a library for this, preferably in JavaScript? So far I've found <a href=""https://stackoverflow.com/questions/8856347/how-to-know-if-two-words-have-the-same-base"">this decade old question</a> and stuff like the Porter Stemming algorithm, but haven't come across a good, modern solution so far.</p>
",Multilingual Language Processing & Language Identification,know two word base want make app user input word word detected highlighted page difficult part want form word detected would ultimately translation purpose user enters want match ideally support multiple language well library preferably javascript far found href decade old question stuff like porter stemming algorithm come across good modern solution far
"python replace Ô∏∞ (unicode - 0xFE30) with : (unicode - 0x3a) in python (programmatically, rather than replace it using dictionary mapping)","<p>I am cleaning text in python, and I want to replace Ô∏∞ (unicode - 0xFE30) with : (unicode - 0x3a) in python automatically (programmatically, rather than replace it using dictionary mapping), since that affects the further results in text analysis</p>
<p>find some similarity between the above 2 symbols, could not find any.</p>
<p>unicodedata, the text is in chinese</p>
",Multilingual Language Processing & Language Identification,python replace unicode xfe unicode x python programmatically rather replace using dictionary mapping cleaning text python want replace unicode xfe unicode x python automatically programmatically rather replace using dictionary mapping since affect result text analysis find similarity symbol could find unicodedata text chinese
How to improve the flair NER-model results?,"<p>I am new to NER and NLP in general and I want to know if I understood the material right.
So for example I have pre-trained model &quot;ner-english-large&quot;.
I am using a model and it turns out that the model is not recognizing the right entities.</p>
<p>(In this example 'Dsl' wasn't marked as ORG)</p>
<pre><code>from flair.data import Sentence
from flair.models import SequenceTagger
from flair.data import Corpus
from flair.trainers import ModelTrainer

tagger = SequenceTagger.load(&quot;flair/ner-english-large&quot;)
sentence = Sentence(&quot;Dsl hit 100% success in sales across all the world&quot;)
tagger.predict(sentence)
sentence.get_spans('ner')
</code></pre>
<pre><code>#output
[]
</code></pre>
<p>So I want to improve my model.
I am uploading this sentence to the corpus in the appropriate format.</p>
<pre><code>columns = {0 : 'text', 1 : 'ner'}
data_folder = &quot;train&quot;
corpus: Corpus = ColumnCorpus(data_folder, columns)
label_type = 'ner'
label_dict = corpus.make_label_dictionary(label_type=label_type)
</code></pre>
<p>Then I am initializing the <code>trainer</code></p>
<pre><code>tagger = SequenceTagger.load(&quot;flair/ner-english-large&quot;)
trainer = ModelTrainer(tagger, corpus)
</code></pre>
<p>But after this I am a little bit confused about what I should do next.
Before that, I tried <code>train</code> method and it worked. Data (that was previously recognized incorrectly) was recognized as needed.</p>
<pre><code>trainer.train('fine/taggers/continued_model',
              learning_rate=0.01,
              mini_batch_size=32,
              max_epochs=6)
</code></pre>
<p>But I don't know if this is the correct method or I need to use <code>fine-tune</code>? Could you explain which approach is more correct?</p>
",Multilingual Language Processing & Language Identification,improve flair ner model result new ner nlp general want know understood material right example pre trained model ner english large using model turn model recognizing right entity example dsl marked org want improve model uploading sentence corpus appropriate format initializing little bit confused next tried method worked data wa previously recognized incorrectly wa recognized needed know correct method need use could explain approach correct
How can I enhance morphological information for English models in spaCy?,"<p>I am trying to detect verbs that are in the imperative mood using English models in spaCy but I am seeing morphological features that are inconsistent with the examples found in the <a href=""https://spacy.io/usage/linguistic-features#morphology"" rel=""nofollow noreferrer"">Morphology</a> documentation. This issue is similar to this unanswered <a href=""https://stackoverflow.com/q/59008046/72992"">Extracting English imperative mood from verb tags with spaCy</a> question. Specifically, there seems to very few <a href=""https://universaldependencies.org/u/feat/Mood.html"" rel=""nofollow noreferrer"">mood</a> features identified.</p>
<p>I am not sure if I am missing some configuration or if I need to somehow train the model to better identify morphological features. Before I go down the path of training, I'd like to understand why what I am doing is not matching the documentation.</p>
<p>I have written a small example that demonstrates the discrepancy.</p>
<pre><code>'''
Prerequisites

pip install spacy
python -m spacy download en_core_web_lg
'''
import spacy

nlp = spacy.load(&quot;en_core_web_lg&quot;)

def show_morph_as_markdown_table(doc):
    print(&quot;|Context|Token|Lemma|POS|TAG|MORPH|&quot;)
    print(&quot;|----|----|----|----|----|----|&quot;)
    for token in doc:
        print(f'|{doc}|{token.text}|{token.lemma_}|{token.pos_}|{token.tag_}|{token.morph.to_dict()}|')

def show_morph_for_sentences_as_markdown_table(sentences):
    sentence_docs = list(nlp.pipe(sentences))
    for sentence_doc in sentence_docs:
        show_morph_as_markdown_table(sentence_doc)

example_sentences = [
    &quot;I was reading the paper&quot;,
    &quot;I don‚Äôt watch the news, I read the paper&quot;,
    &quot;I read the paper yesterday&quot;
]

show_morph_for_sentences_as_markdown_table(example_sentences)
</code></pre>
<p>I have trimmed the output to include only rows shown in the <a href=""https://spacy.io/usage/linguistic-features#morphology"" rel=""nofollow noreferrer"">Morphology</a> documentation.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Context</th>
<th>Token</th>
<th>Lemma</th>
<th>POS</th>
<th>TAG</th>
<th>MORPH</th>
</tr>
</thead>
<tbody>
<tr>
<td>I was reading the paper</td>
<td>reading</td>
<td>read</td>
<td>VERB</td>
<td>VBG</td>
<td>{'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}</td>
</tr>
<tr>
<td>I don‚Äôt watch the news, I read the paper</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBD</td>
<td>{'Tense': 'Past', 'VerbForm': 'Fin'}</td>
</tr>
<tr>
<td>I read the paper yesterday</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBP</td>
<td>{'Tense': 'Pres', 'VerbForm': 'Fin'}</td>
</tr>
</tbody>
</table>
</div>
<p>This is very different from the expected output of:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Context</th>
<th>Token</th>
<th>Lemma</th>
<th>POS</th>
<th>TAG</th>
<th>MORPH</th>
</tr>
</thead>
<tbody>
<tr>
<td>I was reading the paper</td>
<td>reading</td>
<td>read</td>
<td>VERB</td>
<td>VBG</td>
<td>{'VerbForm': 'Ger'}</td>
</tr>
<tr>
<td>I don‚Äôt watch the news, I read the paper</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBD</td>
<td>{'VerbForm': 'Fin', 'Mood': 'Ind', 'Tense': 'Pres'}</td>
</tr>
<tr>
<td>I read the paper yesterday</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBP</td>
<td>{'VerbForm': 'Fin', 'Mood': 'Ind', 'Tense': 'Past'}</td>
</tr>
</tbody>
</table>
</div>
<p>I've tried adding a morphologizer to the pipeline using the DEFAULT_MORPH_MODEL but was met with an initialization error. I don't know enough about the pipeline yet to understand why.</p>
<pre><code>from spacy.pipeline.morphologizer import DEFAULT_MORPH_MODEL

config = {&quot;model&quot;: DEFAULT_MORPH_MODEL}
nlp.add_pipe(&quot;morphologizer&quot;, config=config)

# ValueError: [E109] Component 'morphologizer' could not be run. Did you forget to call `initialize()`?

# trying to fix above error with the following
nlp.initialize()

# [E955] Can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.

</code></pre>
<p>In researching further, it appears that spaCy version 3 <a href=""https://spacy.io/usage/v3#migrating-training-mappings-exceptions"" rel=""nofollow noreferrer"">manages tag_map and morph_rules with AttributeRuler</a>. Could it be possible that the downloadable models aren't including the same information that the documentation is using?</p>
<p>I'm hoping for an easy configuration fix that I am missing or a pointer to the right rabbit hole (I've been down many).</p>
",Multilingual Language Processing & Language Identification,enhance morphological information english model spacy trying detect verb imperative mood using english model spacy seeing morphological feature inconsistent example found morphology documentation issue similar unanswered mood feature identified sure missing configuration need somehow train model better identify morphological feature go path training like understand matching documentation written small example demonstrates discrepancy trimmed output include row shown morphology documentation context token lemma po tag morph wa reading paper reading read verb vbg aspect prog tense pres verbform part watch news read paper read read verb vbd tense past verbform fin read paper yesterday read read verb vbp tense pres verbform fin different expected output context token lemma po tag morph wa reading paper reading read verb vbg verbform ger watch news read paper read read verb vbd verbform fin mood ind tense pres read paper yesterday read read verb vbp verbform fin mood ind tense past tried adding morphologizer pipeline using default morph model wa met initialization error know enough pipeline yet understand researching appears spacy version manages tag map morph rule attributeruler could possible downloadable model including information documentation using hoping easy configuration fix missing pointer right rabbit hole many
"Python NLP: identifying the tense of a sentence using TextBlob, StanfordNLP or Google Cloud","<p>(Note: I am aware that there have been previous posts on this question (e.g. <a href=""https://stackoverflow.com/questions/30016904/determining-tense-of-a-sentence-python"">here</a> or <a href=""https://stackoverflow.com/questions/22139866/finding-tense-of-a-sentence-using-stanford-nlp/22146151#22146151"">here</a>, but they are rather old and I think there has been quite some progress in NLP in the past few years.)</p>

<p>I am trying to determine the tense of a sentence, using natural language processing in Python. </p>

<p>Is there an easy-to-use package for this? If not, how would I need to implement solutions in TextBlob, StanfordNLP or Google Cloud Natural Language API?</p>

<p>TextBlob seems easiest to use, and I manage to get the POS tags listed, but I am not sure how I can turn the output into a 'tense prediction value' or simply a best guess on the tense. Moreover, my text is in Spanish, so I would prefer to use GoogleCloud or StanfordNLP (or any other easy to use solution) which support Spanish.</p>

<p>I have not managed to work with the Python interface for StanfordNLP. </p>

<p>Google Cloud Natural Language API seems to offer exactly what I need (see <a href=""https://cloud.google.com/natural-language/docs/reference/rest/v1/Token#tense"" rel=""nofollow noreferrer"">here</a>, but I have not managed to find out how I would get to this output. I have used Google Cloud NLP for other analysis (e.g. entity sentiment analysis) and it has worked, so I am confident I could set it up if I find the right example of use.</p>

<p>Example of textblob: </p>

<pre><code>from textblob import TextBlob
from textblob.taggers import NLTKTagger
nltk_tagger = NLTKTagger()
blob = TextBlob(""I am curious to see whether NLP is able to predict the tense of this sentence., pos_tagger=nltk_tagger)
print(blob.pos_tags)
</code></pre>

<p>-> this prints the pos tags, how would I convert them into a prediction of the tense of this sentence?</p>

<p>Example with Google Cloud NLP (after setting up credentials):</p>

<pre><code>from google.cloud import language
from google.cloud.language import enums
from google.cloud.language import types
text = ""I am curious to see how this works""
client = language.LanguageServiceClient()
document = types.Document(
    content=text,
    type=enums.Document.Type.PLAIN_TEXT)

tense = (WHAT NEEDS TO COME HERE?)
print(tense)
</code></pre>

<p>-> I am not sure about the code that needs to be entered to predict the tense (indicated in the code)</p>

<p>I am quite a newbie to Python so any help on this topic would be highly appreciated! Thanks!</p>
",Multilingual Language Processing & Language Identification,python nlp identifying tense sentence using textblob stanfordnlp google cloud note aware previous post question e g trying determine tense sentence using natural language processing python easy use package would need implement solution textblob stanfordnlp google cloud natural language api textblob seems easiest use manage get po tag listed sure turn output tense prediction value simply best guess tense moreover text spanish would prefer use googlecloud stanfordnlp easy use solution support spanish managed work python interface stanfordnlp google cloud natural language api seems offer exactly need see managed find would get output used google cloud nlp analysis e g entity sentiment analysis ha worked confident could set find right example use example textblob print po tag would convert prediction tense sentence example google cloud nlp setting credential sure code need entered predict tense indicated code quite newbie python help topic would highly appreciated thanks
Latin to English alphabet hashing,"<p>I have to convert all the latin characters to their corresponding English alphabets. Can I use Python to do it? Or is there a mapping available? </p>

<p>Unicode values to non-unicode characters </p>

<p><code>Ram√≠rez S√°nchez</code> should be converted to <code>Ramirez Sanchez</code>.</p>
",Multilingual Language Processing & Language Identification,latin english alphabet hashing convert latin character corresponding english alphabet use python mapping available unicode value non unicode character converted
How to concatenate a split word using NLP caused by tokenizers after machine translation?,"<p>Russian translation produces the following result, is there a NLP function which we can use to concatenate as &quot;Europe's&quot; in the following string?</p>
<p>&quot;Nitzchia Protector Todibo can go to one of Europe ' s top clubs&quot;</p>
",Multilingual Language Processing & Language Identification,concatenate split word using nlp caused tokenizers machine translation russian translation produce following result nlp function use concatenate europe following string nitzchia protector todibo go one europe top club
&#39;utf-8&#39; decode error when loading a word2vec module,"<p>I have to use a word2vec module containing tons of Chinese characters. The module was trained by my coworkers using Java and is saved as a bin file. </p>

<p>I installed <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">gensim</a> and tries to load the module, but following error occurred: </p>

<pre><code>In [1]: import gensim  

In [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True)

UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 96-97: unexpected end of data
</code></pre>

<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>
",Multilingual Language Processing & Language Identification,utf decode error loading word vec module use word vec module containing ton chinese character module wa trained coworkers using java saved bin file installed gensim try load module following error occurred tried load module python failed way load module gensim thanks
what&#39;s the best way to evaluate Helsinki model with the Huggingface Trainer,"<p>I am trying to finetune Helsinki model with the Huggingface trainer based on the documentation found under <a href=""https:////https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#:%7E:text=The%20score%20can%20go%20from,100%2C%20and%20higher%20is%20better."" rel=""nofollow noreferrer"">link 1</a> and <a href=""https://huggingface.co/docs/transformers/tasks/translation"" rel=""nofollow noreferrer"">link 2</a> for a translation task from German to French. I am using cross validation for performance check. I am finetuning the model in aws Sagemaker notebook. I am noticing that the performance while training is working just fine but during evaluation the performance decreases dramatically from 2.7 iter/s (during training/finetuning) to 0.07 iter/s (during validation).</p>
<p>This is the code I am trying to finetune with:</p>
<pre><code>#checkpoint
checkpoint = &quot;Helsinki-NLP/opus-mt-de-fr&quot;
source_lang = &quot;de&quot;
target_lang = &quot;fr&quot;
prefix = &quot;√úbersetzen Deutsch ins Franz√∂sisch: &quot;

#dataset
dataset_opus100 = load_dataset(&quot;opus100&quot;, &quot;de-fr&quot;, split=&quot;test&quot;)
final_corpus_dataset= dataset_opus100 
# preprocess
def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples[&quot;translation&quot;]]
    targets = [example[target_lang] for example in examples[&quot;translation&quot;]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=128, truncation=True
    )
    return model_inputs
# metric
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {&quot;bleu&quot;: result[&quot;score&quot;]}

    prediction_lens = [
        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds
    ]
    result[&quot;gen_len&quot;] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    # print(result)
    return result

# config_params
exp_name = &quot;de-fr-helsinki&quot;


seed = 42
kfold = KFold(n_splits=5, shuffle=True, random_state=seed)
# K-fold Cross Validation model evaluation
for fold, (train_ids, val_ids) in enumerate(kfold.split(final_corpus_dataset)):
    # if fold==0 or fold==1:
    #     continue
    # metric
    metric = evaluate.load(&quot;sacrebleu&quot;)
    # arguments for training
    training_args = Seq2SeqTrainingArguments(
        output_dir=f&quot;test/huggingface_exps/{exp_name}/fold{fold}&quot;,
        evaluation_strategy=&quot;epoch&quot;,
        logging_strategy=&quot;epoch&quot;,
        # logging_steps=4,
        learning_rate=2e-5,
        per_device_train_batch_size=64,
        per_device_eval_batch_size=64,
        weight_decay=0.01,
        num_train_epochs=10,
        save_total_limit=2,
        save_strategy=&quot;epoch&quot;,
        load_best_model_at_end=True,
        predict_with_generate=True,
        fp16=False,
        push_to_hub=False,
        # tensorboard log directory
        logging_dir=f&quot;test/huggingface_exps/{exp_name}/fold{fold}/runs&quot;,
        report_to=[&quot;tensorboard&quot;],
        # dataloader_num_workers=2,
    )
    print(&quot;fold&quot;, fold)
    # reset tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
    # select splits
    train_dataset = final_corpus_dataset.select(train_ids)
    eval_dataset = final_corpus_dataset.select(val_ids)

    # preprocess
    train_dataset = train_dataset.map(preprocess_function, batched=True)
    eval_dataset = eval_dataset.map(preprocess_function, batched=True)




    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        # train_dataset=Dataset.from_dict(train_dataset[:40]),
        # eval_dataset=Dataset.from_dict(eval_dataset[:20]),
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        # callbacks=[TensorBoardCallback]
        callbacks=[
            CombinedTensorBoardCallback,
            EarlyStoppingCallback(early_stopping_patience=3),
        ],
    )

    train_result = trainer.train()

    # compute train results
    metrics = train_result.metrics

    # print metrics history
    import os

    # Create the directory if it doesn't exist
    os.makedirs(f&quot;test/huggingface_exps/{exp_name}/fold{fold}/&quot;, exist_ok=True)

    with open(f&quot;test/huggingface_exps/{exp_name}/fold{fold}/console.json&quot;, &quot;w&quot;) as f:
        json.dump(trainer.state.log_history, f)
    #next


</code></pre>
<p><strong>Things I considered:</strong></p>
<ol>
<li>GPU memory is enough to fit the process (14.5GB/15.5GB)</li>
<li>Reducing the batch size for the evaluation process didn't help</li>
<li>While training on sagemmaker I get this error <code> valueerror: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer</code> in comparaison to the training process with <code>mt5-small</code>. I had to install sentencepiece with <code> ! pip install transformers[sentencepiece]</code>. I'm not sure if it reduces the performance during the evaluation process ( nothing regarding this was mentioned in the documentation)</li>
</ol>
<p><strong>Packages I installed in this order:</strong></p>
<pre><code>! pip install transformers==4.28.0
! pip install datasets
! pip install evaluate
! pip install torch
! pip install sklearn
! pip install tensorboard
! pip install sacrebleu
! pip install accelerate -U
! pip install transformers[sentencepiece]
</code></pre>
",Multilingual Language Processing & Language Identification,best way evaluate helsinki model huggingface trainer trying finetune helsinki model huggingface trainer based documentation found link link translation task german french using cross validation performance check finetuning model aws sagemaker notebook noticing performance training working fine evaluation performance decrease dramatically iter training finetuning iter validation code trying finetune thing considered gpu memory enough fit process gb gb reducing batch size evaluation process help training sagemmaker get error comparaison training process install sentencepiece sure reduces performance evaluation process nothing regarding wa mentioned documentation package installed order
Machine translation transformer with context,"<p>I'm working on a transformer for seq2seq translation that has the typical encoder-decoder structure. I want to include examples of similar sentences and their translations in the prompt (few-shot) but I could only find guides that just translate one sentence to another without other context. Are there any best practices for including examples as context for translation transformers? Should they go in the decoder or encoder?</p>
<p>I currently feed an input sentence <code>source_sentence</code> to the encoder and start generating the translation by feeding a start token <code>&lt;start&gt;</code> to the decoder until an end token <code>&lt;end&gt;</code> is produced.</p>
<p>Should my encoder input be something like this:</p>
<pre><code>example_sentence1
&lt;start&gt; example_translation1 &lt;end&gt;

example_sentence2
&lt;start&gt; example_translation2 &lt;end&gt;

source_sentence
</code></pre>
<p>Or alternatively should my decoder input be:</p>
<pre><code>example_sentence1
&lt;start&gt; example_translation1 &lt;end&gt;

example_sentence2
&lt;start&gt; example_translation2 &lt;end&gt;

source_sentence
&lt;start&gt;
</code></pre>
<p>Neither really feels right. Are there other options?</p>
",Multilingual Language Processing & Language Identification,machine translation transformer context working transformer seq seq translation ha typical encoder decoder structure want include example similar sentence translation prompt shot could find guide translate one sentence another without context best practice including example context translation transformer go decoder encoder currently feed input sentence encoder start generating translation feeding start token decoder end token produced encoder input something like alternatively decoder input neither really feel right option
Apple&#39;s NLTagger Sentiment analysis doesn&#39;t work with other languages,"<p>I'm building a project with Apple's NaturalLanguage framework, I tried to execute Sentiment analysis with French language, but it surely doesn't recognise any word, but tokeniser recognises the language. Here is the code:</p>
<pre><code>func emotionClassification(text: String) -&gt; EmotionalStates {
        var numericScore: Double = 0
        let tagger = NLTagger(tagSchemes: [.tokenType, .sentimentScore])
        tagger.string = text
        
        tagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .paragraph, scheme: .sentimentScore, options: []) { sentiment, _ in
            
            if let sentimentScore = sentiment {
                print(sentimentScore.rawValue)
                
                numericScore = Double(sentimentScore.rawValue)  ?? 0
            }
            
            return true
        }
        
        emotionScore = numericScore
        
        if numericScore &lt;= -0.9 {
            return .bad
        }
        else if numericScore &lt;= -0.5 {
            return .worried
        }
        else if numericScore &lt;= 0.3 {
            return .calm
        }
        else {
            return .happy
        }
    }
</code></pre>
<p>&quot;sentimentScore.rawValue&quot; is always 0.0</p>
<p>Apple doesn't claim anywhere that other languages are not supported, so please help me resolve this issue.</p>
<p>I tried to tokenise string to detect language in this code and it defines it correctly</p>
",Multilingual Language Processing & Language Identification,apple nltagger sentiment analysis work language building project apple naturallanguage framework tried execute sentiment analysis french language surely recognise word tokeniser recognises language code sentimentscore rawvalue always apple claim anywhere language supported please help resolve issue tried tokenise string detect language code defines correctly
How to remove English and Spanish stop words,"<p>I am trying to delete stop words for English and Spanish. My code is working for English but not Spanish:</p>
<pre><code>stopword = nltk.corpus.stopwords.words('english', 'spanish')

def remove_stopwords(text):
    text = [word for word in text if word not in stopword]
    return text
    
df['Tweet_nonstop'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))
</code></pre>
",Multilingual Language Processing & Language Identification,remove english spanish stop word trying delete stop word english spanish code working english spanish
mbart50 having trouble translating long texts/documents?,"<p>I'm new to NLP and MBART and sorry if my question sounds stupid. I'm having trouble with translating Korean long texts into English using MBart50.</p>
<p>I realized that it works fine with shorter texts (for example, a sentence). But when it comes to longer texts such as news, it always give me an error of &quot;index out of range in self&quot;.</p>
<p>Here's my code:</p>
<pre><code>from transformers import MBartForConditionalGeneration, MBart50Tokenizer
import streamlit as st
import csv


@st.cache_resource
def download_model():
    model_name = &quot;facebook/mbart-large-50-many-to-many-mmt&quot;
    model = MBartForConditionalGeneration.from_pretrained(model_name)
    tokenizer = MBart50Tokenizer.from_pretrained(model_name, src_lang=&quot;ko_KR&quot;)
    return model, tokenizer


model, tokenizer = download_model()

model_name = &quot;facebook/mbart-large-50-many-to-many-mmt&quot;
tokenizer.src_lang = &quot;ko_KR&quot;

with open('Korean_Translation.csv', 'w', newline='', encoding='UTF-8') as korean_translation:
    translation_writer = csv.writer(korean_translation)

    with open('original_text.txt', mode='r', encoding='UTF-8') as korean_original:
        original_lines = korean_original.readlines()
        for lines in original_lines:
            print(lines)
            encoded_korean_text = tokenizer(lines, return_tensors=&quot;pt&quot;)
            generated_tokens = model.generate(**encoded_korean_text,
                                              forced_bos_token_id=tokenizer.lang_code_to_id[&quot;en_XX&quot;],
                                              max_length=99999999999999)
            out2 = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
            print(out2)
            translation_writer.writerow(out2)
</code></pre>
<p>The error it gives me looks like this:</p>
<pre><code>2023-03-10 14:15:04.182 Uncaught app exception
Traceback (most recent call last):
  File &quot;E:\Python 3.10.5\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py&quot;, line 565, in _run_script
    exec(code, module.__dict__)
  File &quot;D:\Study\NLP\Multilingual_news_analysis\pythonProject\test.py&quot;, line 36, in &lt;module&gt;
    generated_tokens = model.generate(**encoded_korean_text,
  File &quot;E:\Python 3.10.5\lib\site-packages\torch\autograd\grad_mode.py&quot;, line 27, in decorate_context
    return func(*args, **kwargs)
  File &quot;E:\Python 3.10.5\lib\site-packages\transformers\generation\utils.py&quot;, line 1252, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File &quot;E:\Python 3.10.5\lib\site-packages\transformers\generation\utils.py&quot;, line 617, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs[&quot;encoder_outputs&quot;]: ModelOutput = encoder(**encoder_kwargs)
  File &quot;E:\Python 3.10.5\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;E:\Python 3.10.5\lib\site-packages\transformers\models\mbart\modeling_mbart.py&quot;, line 794, in forward
    embed_pos = self.embed_positions(input)
  File &quot;E:\Python 3.10.5\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;E:\Python 3.10.5\lib\site-packages\transformers\models\mbart\modeling_mbart.py&quot;, line 133, in forward
    return super().forward(positions + self.offset)
  File &quot;E:\Python 3.10.5\lib\site-packages\torch\nn\modules\sparse.py&quot;, line 160, in forward
    return F.embedding(
  File &quot;E:\Python 3.10.5\lib\site-packages\torch\nn\functional.py&quot;, line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>Why is this happening? Is it because the text is too long? (about 600 characters) Because this won't happen with shorter texts (&lt; 200 characters). How can I fix this?
Thanks!</p>
",Multilingual Language Processing & Language Identification,mbart trouble translating long text document new nlp mbart sorry question sound stupid trouble translating korean long text english using mbart realized work fine shorter text example sentence come longer text news always give error index range self code error give look like happening text long character happen shorter text character fix thanks
Pegasus tokenizer for batch processing ( TypeError: TextEncodeInput must be Union ),"<p>i'm trying to play with some trasformers for learn the basics of this amazing world, but in the last days i got stuck with pegasus model. I'm trying to summarize text feature from a dataset for make a summarize enought short for Bert tokenizer, using pegasus as the summary model. When i run the code using the function map with batched =  False  evrething is working fine, but if i turn to batched = True i get :</p>
<blockquote>
<p>TypeError: TextEncodeInput must be Union[TextInputSequence,
Tuple[InputSequence, InputSequence]]</p>
</blockquote>
<p>I already:</p>
<ul>
<li>checked if there are none values and removed</li>
<li>checked if i was giving a list of string as input of tokenizer</li>
<li>tried to generate a list of string from batch_samples and give it as input</li>
</ul>
<p>Check function:</p>
<pre><code>    for value in input_dict[&quot;text&quot;]:
        if value is None:  # Check for None or NaN
            result_list.append(&quot;is None&quot;)
        elif value != value :
            result_list.append(&quot;is NaN&quot;)
        elif isinstance(value, str):
            result_list.append(&quot;is String&quot;)
        else: 
            result_list.append(False)

    print(result_list)


process_dictionary(batch_samples)
</code></pre>
<p>This is my code, while you can find the dataset <a href=""https://www.kaggle.com/datasets/shivamb/legal-citation-text-classification?resource=download"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>from datasets import Dataset, DatasetDict, concatenate_datasets
import pandas as pd
from transformers import AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM
import torch

def preprocess_data(df):
    #adding token_lenght column
    df[&quot;lb_num_token&quot;] = d_len
    
    #Dropping Nan values
    df = df.dropna(subset=['case_text'])

    # Dropping unused features and renaming columns
    df = df.drop(columns =['case_id', 'case_title'])
    df.rename(columns={&quot;case_text&quot;:&quot;text&quot;, &quot;case_outcome&quot;:&quot;label&quot;}, inplace= True)

    # Get the number of unique labels
    labels_list = df[&quot;label&quot;].unique().tolist()
    
    # Splitting Dataset
    df = Dataset.from_pandas(df)
    df = df.map(lambda example: {'text': str(example['text'])})
    train_valid = df.train_test_split(test_size= 0.2, seed= 42)
    valid_test  = train_valid[&quot;test&quot;].train_test_split(test_size= 0.5, seed= 42)
    
    df_split = DatasetDict({
    'train': train_valid['train'],
    'valid': valid_test['train'],
    'test': valid_test['test']
    })
    
    return df_split, labels_list

#Loading Dataset
df = pd.read_csv(&quot;./datasets/legal_text_classification.csv&quot;)

# number of bert token for each sample
model_ckpt = &quot;nlpaueb/legal-bert-small-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)  
d_len = [len(tokenizer.encode(str(s))) for s in df[&quot;case_text&quot;]]

# preprocessing dataset
df, labels_list = preprocess_data(df)

train = Dataset.from_dict(df[&quot;train&quot;][0:9])

def pegasus_summary(batch_samples, model, tokenizer):
    # This function take in input a batch of samples and return the summary of each sample.
    # The summary length is set to 400 token length, because the output summary will be used as bert tokenizer input
    # LLM used: legal-pegasus
    # It will be better to call this function with model anf tokenizer already define inside the main code

    summary = &quot;&quot;
    # summary
    input_tokenized = tokenizer.encode(batch_samples[&quot;text&quot;], return_tensors='pt', max_length=1024, truncation=True).to(device)
    with torch.no_grad():
        summary_ids = model.generate(input_tokenized,
                                     num_beams=9,
                                     no_repeat_ngram_size=3,
                                     length_penalty=2.0,
                                     min_length=150,
                                     max_length=400,
                                     early_stopping=True)

    summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]
    return {&quot;text&quot;: summary}

def summarizing_samples(df):
    model_ckpt_sum = &quot;nsi319/legal-pegasus&quot;
    tokenizer_sum = AutoTokenizer.from_pretrained(model_ckpt_sum)
    model_sum = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt_sum).to(device)
    
    df_long = df.filter(lambda example: example[&quot;lb_num_token&quot;] &gt; 512)
    df_short= df.filter(lambda example: example[&quot;lb_num_token&quot;] &lt;= 512)

    df_long = df_long.map(lambda example: pegasus_summary(example, model_sum, tokenizer_sum), batched = True)
                                                                                          
    df = concatenate_datasets([df_long, df_short])
    return df

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
train = summarizing_samples(train)

for it in train[&quot;text&quot;]:
    print(it, &quot;\n\n\n&quot;)
</code></pre>
<p>Thanks a lot for your time and i hope that my english is understandable.</p>
",Multilingual Language Processing & Language Identification,pegasus tokenizer batch processing typeerror textencodeinput must union trying play trasformers learn basic amazing world last day got stuck pegasus model trying summarize text feature dataset make summarize enought short bert tokenizer using pegasus summary model run code using function map batched false evrething working fine turn batched true get typeerror textencodeinput must union textinputsequence tuple inputsequence inputsequence already checked none value removed checked wa giving list string input tokenizer tried generate list string batch sample give input check function code find dataset thanks lot time hope english understandable
Customize word embeddings to your own vocabulary,"<p>I have a vocabulary related to restaurant stuff in Spanish and I am using predefined word embeddings in Spanish with FastText and Bert, however, I see that there are a lot of out-of-vocabulary (oov) words that are not recognized by the predefined word embeddings. Also, my vocabulary is very limited, so it does not make sense to train word embeddings just for that from scratch.
Is there any approach I could follow to expand the predefined word embeddings to include word embeddings for most oov?</p>
<p>Thank you</p>
",Multilingual Language Processing & Language Identification,customize word embeddings vocabulary vocabulary related restaurant stuff spanish using predefined word embeddings spanish fasttext bert however see lot vocabulary oov word recognized predefined word embeddings also vocabulary limited doe make sense train word embeddings scratch approach could follow expand predefined word embeddings include word embeddings oov thank
How to get the inflections from a lemma in English preferably with Python,"<p>I realize many people have asked this question before and that the standard answer is to use <code>lemminflect</code>.  However, I just want to make sure there is not a better solution.  I have a list of about 230K lemmas in the Oxford English Dictionary and only about 37K of them can be inflected by <code>lemminflect</code>.  For example, 'website', 'gallery', 'smartphone','calorie' cannot be inflected among a slew of others. I find it very hard to believe that no one else has worked on this problem.</p>
",Multilingual Language Processing & Language Identification,get inflection lemma english preferably python realize many people asked question standard answer use however want make sure better solution list k lemma oxford english dictionary k inflected example website gallery smartphone calorie inflected among slew others find hard believe one else ha worked problem
How do I prepare to use entire wikipedia for natural language processing?,"<p>I am a bit new here. I have a project where I have to download and use Wikipedia for NLP. The questions I am facing are as follows:
I have RAM of only 12 GB, but the English wiki dump is over 15 GB compressed. Does this limit my processing of wiki? I do not need any picture from the wiki. Do I need to uncompress the dump before processing? Can someone just tell me the steps required or point to me related content for it?
Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,prepare use entire wikipedia natural language processing bit new project download use wikipedia nlp question facing follows ram gb english wiki dump gb compressed doe limit processing wiki need picture wiki need uncompress dump processing someone tell step required point related content thanks advance
NLP Pytorch python - [enforce fail at .path] data. DefaultCPUAllocator: not enough memory: you tried to allocate 157079520 bytes,"<p>This post links directly to this post: <a href=""https://stackoverflow.com/questions/67522026/defaultcpuallocator-not-enough-memory-you-tried-to-allocate-364742208-bytes-b"">DefaultCPUAllocator: not enough memory: you tried to allocate 364742208 bytes. Buy new RAM</a> which an ex-colleague and friend posted on my behalf.</p>
<p>I wanted to follow on from this and give more detail and an example of where this happens as it's a problem I am constantly coming across whilst running NLP models in python, with still no solution.</p>
<p>It has occurred whilst using BERT models for sentiment analysis and the latest problem has come whilst doing backtranslation using the MarianMT model library on transformers library. It always occurs when using a Pytorch based architecture. Link to model: <a href=""https://huggingface.co/docs/transformers/main/model_doc/marian"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main/model_doc/marian</a></p>
<p>I am trying to translate a list of around 130 English sentences to <code>kwn</code> and then back to <code>en</code>. My sentence data are private but I'm sure a list of any 130 sentences would give the same result. Here is my code:</p>
<pre><code>from transformers import MarianMTModel, MarianTokenizer


def format_sentence(target_lang, s):
    text = f'&gt;&gt;{target_lang}&lt;&lt; {s}'
    return text

def translate(current, target, text):
    model_name = f'Helsinki-NLP/opus-mt-{current}-{target}'
    tokenizer_current = MarianTokenizer.from_pretrained(model_name)
    model_current = MarianMTModel.from_pretrained(model_name)
    tr = model_current.generate(**tokenizer_current(text, return_tensors=&quot;pt&quot;, padding=True))
    translated = [tokenizer_current.decode(t, skip_special_tokens=True) for t in tr]
    return translated

def simple_translate(current, target, text):
    text = [format_sentence(target, s) for s in text]
    try:
        first = translate(current, target, text)
        back = translate(target, current, first)
        return back
    except Exception as e:
        pass


sents = [YOUR LIST OF 130 ENGLISH SENTS HERE]
simple_translate('en', 'kwn', sents)
</code></pre>
<p>As the question shows once again I get this error:</p>
<pre><code>[enforce fail at ..\c10\core\impl\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 157079520 bytes.
</code></pre>
<p>It is trying to allocate 0.146292 GB which is nothing and I can't understand why this is failing. I have also had it before where it has tried to allocated 14 GB for the same medium sized sentences which seems bizarre.</p>
<p>The error isn't helpful and doesn't really point to the problem. Is it an underlying architecture problem? Is it a cacheing/memory problem? Is it an issue with threading? Can it be solved by batching the sentences into a Dataloader?</p>
<p>I'd really like to try and pin point the problem here and looking at my original (very bare so apologies) post it seems to be a popular issue.</p>
<p>Any help would be appreciated. Thanks.</p>
",Multilingual Language Processing & Language Identification,nlp pytorch python enforce fail path data defaultcpuallocator enough memory tried allocate byte post link directly post trying translate list around english sentence back sentence data private sure list sentence would give result code question show get error trying allocate gb nothing understand failing also ha tried allocated gb medium sized sentence seems bizarre error helpful really point problem underlying architecture problem cacheing memory problem issue threading solved batching sentence dataloader really like try pin point problem looking original bare apology post seems popular issue help would appreciated thanks
Feature selection and unsupervised learning for multilingual data + machine learning algorithm selection,"<p><strong>Questions</strong></p>

<p>I want to classify/categorize/cluster/group together a set of several thousand websites. There's data that we can train on, so we can do supervised learning, but it's not data that we've gathered and we're not adamant about using it -- so we're also considering unsupervised learning.</p>

<ul>
<li><p>What features can I use in a machine learning algorithm to deal with multilingual data? Note that some of these languages might not have been dealt with in the Natural Language Processing field.</p></li>
<li><p>If I were to use an unsupervised learning algorithm, should I just partition the data by language and deal with each language differently? Different languages might have different relevant categories (or not, depending on your psycholinguistic theoretical tendencies), which might affect the decision to partition.</p></li>
<li><p>I was thinking of using decision trees, or maybe Support Vector Machines (SVMs) to allow for more features (from my understanding of them). <a href=""https://stackoverflow.com/questions/3789856/newbie-where-to-start-given-a-problem-to-predict-future-success-or-not/3791608#3791608"">This post</a> suggests random forests instead of SVMs. Any thoughts?</p></li>
</ul>

<p>Pragmatical approaches are welcome! (Theoretical ones, too, but those might be saved for later fun.)</p>

<p><strong>Some context</strong></p>

<p>We are trying to classify a corpus of many thousands of websites in 3 to 5 languages (maybe up to 10, but we're not sure).</p>

<p>We have training data in the form of hundreds of websites already classified. However, we may choose to use that data set or not -- if other categories make more sense, we're open to not using the training data that we have, since it is not something we gathered in the first place. We are on the final stages of scraping data/text from websites.</p>

<p>Now we must decide on the issues above. I have done some work with the Brown Corpus and the Brill tagger, but this will not work because of the multiple-languages issue.</p>

<p>We intend to use the <a href=""http://orange.biolab.si/"" rel=""nofollow noreferrer"">Orange</a> machine learning package.</p>
",Multilingual Language Processing & Language Identification,feature selection unsupervised learning multilingual data machine learning algorithm selection question want classify categorize cluster group together set several thousand website data train supervised learning data gathered adamant using also considering unsupervised learning feature use machine learning algorithm deal multilingual data note language might dealt natural language processing field use unsupervised learning algorithm partition data language deal language differently different language might different relevant category depending psycholinguistic theoretical tendency might affect decision partition wa thinking using decision tree maybe support vector machine svms allow feature understanding orange machine learning package
Python API documentation for FAISS,"<p>I am searching for a Python API documentation for FAISS, unable to find it. The wiki page says the python translation is very close to the C++ classes whose documentation can be found <a href=""https://faiss.ai/index.html"" rel=""nofollow noreferrer"">here</a></p>
",Multilingual Language Processing & Language Identification,python api documentation faiss searching python api documentation faiss unable find wiki page say python translation close c class whose documentation found
Preprocessing Arabic text,"<p>when I apply this code:
re.sub(r'\sŸà(\w+)', r' Ÿà \1', text)
it deletes the letter &quot;Ÿà&quot; that locate at the front of the word. I just want to separate it from the word
for example:
&quot; ÿ£ŸÉŸÑÿ™ ŸÖŸàÿ≤ÿ© Ÿàÿ™ŸÅÿßÿ≠ÿ© ŸÅŸä ÿßŸÑÿ≠ÿØŸäŸÇÿ© &quot;
I want it to be:
&quot; ÿ£ŸÉŸÑÿ™ ŸÖŸàÿ≤ÿ© Ÿà ÿ™ŸÅÿßÿ≠ÿ© ŸÅŸä ÿßŸÑÿ≠ÿØŸäŸÇÿ©&quot;
but it be like this:
&quot; ÿ£ŸÉŸÑÿ™ ŸÖŸàÿ≤ÿ© ÿ™ŸÅÿßÿ≠ÿ© ŸÅŸä ÿßŸÑÿ≠ÿØŸäŸÇÿ© &quot;</p>
<p>this is the code:
class Arabic_preprocessing:</p>
<pre><code>def __init__(self):

    #preparing punctuations list
    arabic_punctuations = '''`√∑√óÿõ&lt;&gt;_()*&amp;^%][ŸÄÿå/:&quot;ÿü.,'{}~¬¶+|!‚Äù‚Ä¶‚Äú‚ÄìŸÄ'''
    english_punctuations = string.punctuation
    self.all_punctuations = set(arabic_punctuations + english_punctuations)

    # initializing the stemmer
    #self.stemmer = ARLSTem()  # requires minimum NLTK version of 3.2.5

    self.arabic_diacritics = re.compile(&quot;&quot;&quot;
                                     Ÿë    | # Tashdid
                                     Ÿé    | # Fatha
                                     Ÿã    | # Tanwin Fath
                                     Ÿè    | # Damma
                                     Ÿå    | # Tanwin Damm
                                     Ÿê    | # Kasra
                                     Ÿç    | # Tanwin Kasr
                                     Ÿí    | # Sukun
                                     ŸÄ     # Tatwil/Kashida

                                 &quot;&quot;&quot;, re.VERBOSE)


def normalize_arabic(self, text):
    text = re.sub(&quot;[ÿ•ÿ£ÿ¢ÿßŸ±]&quot;, &quot;ÿß&quot;, text)
    text = re.sub(&quot;Ÿâ&quot;, &quot;Ÿä&quot;, text)
    #text = re.sub(&quot;ÿ§&quot;, &quot;ÿ°&quot;, text)
    #text = re.sub(&quot;ÿ¶&quot;, &quot;ÿ°&quot;, text)
    text = re.sub(&quot;ÿ©&quot;, &quot;Ÿá&quot;, text)  # replace ta2 marboota by ha2
    text = re.sub(&quot;⁄Ø&quot;, &quot;ŸÉ&quot;, text)
    text = re.sub(r'\bÿßŸÑ(\w\w+)', r'\1', text)    # remove al ta3reef
    text = re.sub(r'\sŸà(\w+)', r' Ÿà \1', text)

    text = re.sub(&quot;\u0640&quot;, '', text)  # remove tatweel
    return text
</code></pre>
",Multilingual Language Processing & Language Identification,preprocessing arabic text apply code sub r w r text deletes letter locate front word want separate word example want like code class arabic preprocessing
Training a NLP Model to Translate User Descriptions to Predefined Part Numbers,"<p>I am working on mapping user input descriptions to specific part number categories. I have a large dataset of part numbers and their associated descriptions. Here's an example of what the data looks like:</p>
<pre class=""lang-js prettyprint-override""><code>&quot;52119A&quot;: &quot;COVER, FRONT BUMPER&quot;,
&quot;81110&quot;: &quot;HEADLAMP ASSY, RH&quot;,
&quot;52134I&quot;: &quot;REINFORCEMENT, FRONT BUMPER BAR, LH&quot;
&quot;53301F&quot;: &quot;PANEL SUB-ASSY, FRONT&quot;,
&quot;87050&quot;: &quot;RADIATOR ASSY, AIR CONDITIONER&quot;
</code></pre>
<p>The challenge is to develop a system that can accurately map a user's description of the part to the appropriate part number. For instance, if a user inputs &quot;FRONT BUMPER COVER&quot;, the system should be able to translate this to &quot;52119A&quot;: &quot;COVER, FRONT BUMPER&quot;.</p>
<p>I'm considering using Natural Language Processing (NLP) or some sort of AI for this problem, but I'm unsure of the best approach. Specifically, I would appreciate guidance on:</p>
<ol>
<li><p>What type of NLP or AI model would be most appropriate for this task?</p>
</li>
<li><p>How should I go about training the model? What kind of training data will I need and how much of it?</p>
</li>
<li><p>What libraries or tools would be useful? I have a little bit of familiarity with Python-based libraries (Ive used NLTK, and SpaCy - I'm open to using others if they're better).</p>
</li>
<li><p>Are there any similar projects or resources that I could reference to get a better understanding of how to tackle this problem?</p>
</li>
</ol>
",Multilingual Language Processing & Language Identification,training nlp model translate user description predefined part number working mapping user input description specific part number category large dataset part number associated description example data look like challenge develop system accurately map user description part appropriate part number instance user input front bumper cover system able translate cover front bumper considering using natural language processing nlp sort ai problem unsure best approach specifically would appreciate guidance type nlp ai model would appropriate task go training model kind training data need much library tool would useful little bit familiarity python based library ive used nltk spacy open using others better similar project resource could reference get better understanding tackle problem
Customized preprocessor in SVM model for text classification,"<p>I'm implementing a <code>SVM</code> model for a <strong>text classification</strong> problem. I use <code>SVC</code> classifier with the vectorizer: <code>CountVectorizer</code> which has a <code>preprocessor</code> parameter which can take a customized function.</p>
<p><code>CountVectorizer(preprocessor=mp.prepro,analyzer=&quot;word&quot;,max_df=0.6,min_df=3,ngram_range=(1,7))</code></p>
<p>Trying to reduce the number of features I have built my own <strong>preprocessor</strong> in which I do the following tasks:</p>
<ul>
<li>Assign all <strong>months names</strong> the label &quot;month_exp&quot;</li>
<li>Assign all <strong>week days names</strong> the label &quot;week_exp&quot;</li>
<li>Assign all <strong>time expression</strong> <em>(12:10a.m.)</em> the label &quot;time_exp&quot;</li>
<li>Assign all <strong>digit expression</strong> with <em>n</em> digits the label &quot;dig_i&quot;</li>
<li>Apply a Stemmer.</li>
</ul>
<p>Moreover I have imported a dictionary and using it I have checked if each word belongs to English language. If it doesn't hold I assign to this word the label &quot;uknw_exp&quot;.</p>
<p>I have done all this because I think that is important that my model is capable to recognize these expressions for the classification problem.
So I want help because I don't know if my reasoning is right or not.</p>
<ul>
<li><p>If is right, how can I improve it?</p>
</li>
<li><p>Is there any better preprocessor pre-cooked that I can Implement?</p>
</li>
<li><p>Could it be a good idea try to join this method with word embeddings?</p>
</li>
</ul>
<p>Thanks for the help!</p>
",Multilingual Language Processing & Language Identification,customized preprocessor svm model text classification implementing model text classification problem use classifier vectorizer ha parameter take customized function trying reduce number feature built preprocessor following task assign month name label month exp assign week day name label week exp assign time expression label time exp assign digit expression n digit label dig apply stemmer moreover imported dictionary using checked word belongs english language hold assign word label uknw exp done think important model capable recognize expression classification problem want help know reasoning right right improve better preprocessor pre cooked implement could good idea try join method word embeddings thanks help
"If an FST transition is based on a given context, how can it be called as &#39;non deterministic&#39;?","<p>I am going through the paper 'SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS' (hbka.pdf - <a href=""https://cs.nyu.edu/%7Emohri/pub/hbka.pdf"" rel=""nofollow noreferrer"">https://cs.nyu.edu/~mohri/pub/hbka.pdf</a>)</p>
<p><a href=""https://i.sstatic.net/AvhNK.jpg"" rel=""nofollow noreferrer"">At page 9, and figure 6</a></p>
<p>I do not understand how 6 a) figure be considered for 'non-determinism'.</p>
<p>[<strong>Here ae/k t represents the triphonic model for ae with left context k and right context t. This transition is considered as non-deterministic. Why?</strong>]</p>
<p>The differences between transitions that are termed 'non-deterministic' and 'deterministic' are</p>
<ol>
<li>Nondeterministic transitions allow (epsilon transitions) a transition to happen between states without any input symbol consumption and/or any output symbol generation.</li>
</ol>
<p>This is especially useful where the transducer is supposed to output English word morphological description from the input English word, like in cities -&gt; city -PL. Here there are transitions between states where you input and output the same alphabet. But when you input i or e you need not output anything and in the final transition for input s you would output y -PL</p>
<p>So, I understand the need for epsilon transitions. I also understand that non-deterministic transitions are when multiple transitions for same input label can exist. This could lead to ambiguity and hence supports the name 'non-deterministic'.</p>
<ol start=""2"">
<li>Deterministic transitions are the very opposite of the above. No multiple transitions for same input label; hence every transition is unique. No epsilon transitions.</li>
</ol>
<p>With this limited knowing, I could not decipher why a transition could ever be called 'non-deterministic' when you have provided the context. Here ae/k t represents the triphonic model for ae with left context k and right context t. This transition is considered as non-deterministic. Why?</p>
<p>The main idea of providing context is to remove the ambiguity.</p>
",Multilingual Language Processing & Language Identification,fst transition based given context called non deterministic going paper speech recognition weighted finite state transducer hbka pdf page figure understand figure considered non determinism ae k represents triphonic model ae left context k right context transition considered non deterministic difference transition termed non deterministic deterministic nondeterministic transition allow epsilon transition transition happen state without input symbol consumption output symbol generation especially useful transducer supposed output english word morphological description input english word like city city pl transition state input output alphabet input e need output anything final transition input would output pl understand need epsilon transition also understand non deterministic transition multiple transition input label exist could lead ambiguity hence support name non deterministic deterministic transition opposite multiple transition input label hence every transition unique epsilon transition limited knowing could decipher transition could ever called non deterministic provided context ae k represents triphonic model ae left context k right context transition considered non deterministic main idea providing context remove ambiguity
Parsing Italian CONLLU files to remove lemmas,"<p>I am working with Italian Universal Dependency data in CONLLU format, like this:</p>
<blockquote>
<pre><code>sent_id = VIT-4006
text = &quot;grazie dell'informazione, la metter√≤ nella memoria del mio Macintosh&quot;.
1 &quot;   &quot;   PUNCT   FB  _   2   punct   _   SpaceAfter=No
2 grazie  grazie  NOUN    S   _   0   root    _   _
3-4   dell'   _   _   _   _   _   _   _   SpaceAfter=No
3 di  di  ADP E   _   5   case    _   _
4 l'  il  DET RD  Definite=Def|Number=Sing|PronType=Art   5   det _   _
5 informazione    informazione    NOUN    S   Gender=Fem|Number=Sing  2   nmod    _   SpaceAfter=No
6 ,   ,   PUNCT   FF  _   2   punct   _   _
7 la  la  PRON    PC  Clitic=Yes|Gender=Fem|Number=Sing|Person=3|PronType=Prs 8   obj _   _
8 metter√≤ mettere VERB    V   Mood=Ind|Number=Sing|Person=1|Tense=Fut|VerbForm=Fin    2   parataxis   _   _
9-10  nella   _   _   _   _   _   _   _   _
9 in  in  ADP E   _   11  case    _   _
10    la  il  DET RD  Definite=Def|Gender=Fem|Number=Sing|PronType=Art    11  det _   _
11    memoria memoria NOUN    S   Gender=Fem|Number=Sing  8   obl _   _
12-13 del _   _   _   _   _   _   _   _
12    di  di  ADP E   _   15  case    _   _
13    il  il  DET RD  Definite=Def|Gender=Masc|Number=Sing|PronType=Art   15  det _   _
14    mio mio DET AP  Gender=Masc|Number=Sing|Poss=Yes|PronType=Prs   15  det:poss    _   _
15    Macintosh   Macintosh   PROPN   SP  _   11  nmod    _   SpaceAfter=No
16    &quot;   &quot;   PUNCT   FB  _   2   punct   _   SpaceAfter=No
17    .   .   PUNCT   FS  _   2   punct   _   _
</code></pre>
</blockquote>
<p>In this example, I want to remove the lines 3, 4, 9, 10, 12, 13 since they are the component pieces of the lines immediately above them (3-4, 9-10, 12-13).</p>
<p>I want my output to look like:</p>
<blockquote>
<pre><code>sent_id = VIT-4006
text = &quot;grazie dell'informazione, la metter√≤ nella memoria del mio Macintosh&quot;.
1 &quot;   &quot;   PUNCT   FB  _   2   punct   _   SpaceAfter=No
2 grazie  grazie  NOUN    S   _   0   root    _   _
3-4   dell'   _   _   _   _   _   _   _   SpaceAfter=No
5 informazione    informazione    NOUN    S   Gender=Fem|Number=Sing  2   nmod    _   SpaceAfter=No
6 ,   ,   PUNCT   FF  _   2   punct   _   _
7 la  la  PRON    PC  Clitic=Yes|Gender=Fem|Number=Sing|Person=3|PronType=Prs 8   obj _   _
</code></pre>
</blockquote>
<p>...```</p>
<p>thanks</p>
<p>The conllu library TokenList object includes every word, like &quot;grazie, dell', di, l', informazione...&quot; so I am not able to use it.</p>
",Multilingual Language Processing & Language Identification,parsing italian conllu file remove lemma working italian universal dependency data conllu format like example want remove line since component piece line immediately want output look like thanks conllu library tokenlist object includes every word like grazie dell di l informazione able use
Library to lemmatize German compound verbs,"<p>I'm trying to lemmatize a German text using HannoverTagger (same results were achieved with SpaCy).</p>
<pre><code>from HanTa import HanoverTagger as ht

tagger = ht.HanoverTagger('morphmodel_ger.pgz')

print(tagger.analyze('Wir geben niemals auf.'))
</code></pre>
<p>What I get is 'Wir geben niemals auf', but what I would expect is 'Wir aufgeben niemals' or 'Wir aufgeben niemals aufgeben'.</p>
<p>Any ideas what kind of lemmatizer I could use to achieve the desired result?</p>
",Multilingual Language Processing & Language Identification,library lemmatize german compound verb trying lemmatize german text using hannovertagger result achieved spacy get wir geben niemals auf would expect wir aufgeben niemals wir aufgeben niemals aufgeben idea kind lemmatizer could use achieve desired result
Dropping non-English text with langdetect,"<p>I am trying to use langdetect to drop all the languages which are not English in my text.</p>
<pre><code>def det(x):
    try:
        language = detect(x)
    except:
        language = 'Other'
    return language

df['langue'] = df['Tweet'].apply(det)
filtered_for_english = df.loc[df['langue'] == 'en']
</code></pre>
<p>The above code is what I have tried. It detects the language used in each tweet but does not drop the non-English tweets from my data frame.</p>
<p>The resulting data frame:</p>
<pre><code>0        es
1        es
2        es
3        en
4        en
         ..
14272    en
14273    en
14274    en
14275    it
14276    en
Name: langue, Length: 14277, dtype: object
</code></pre>
<p>How can I fix this code?</p>
",Multilingual Language Processing & Language Identification,dropping non english text langdetect trying use langdetect drop language english text code tried detects language used tweet doe drop non english tweet data frame resulting data frame fix code
How to add comment in CSV while using scikit-learn,"<p>I am currently doing Natural Language Processing project.</p>

<p>I need to transform the word into vector format for computer to process.</p>

<p>For debugging I need to know the relation between vector and word, so there must be some comment after each row of data.</p>

<p>How to add comment in CSV while using scikit-learn?</p>
",Multilingual Language Processing & Language Identification,add comment csv using scikit learn currently natural language processing project need transform word vector format computer process debugging need know relation vector word must comment row data add comment csv using scikit learn
Translation between different tokenizers,"<p>Sorry if this question is too basic to be asked here. I tried but I couldn't find solutions.</p>
<p>I'm now working on an NLP project that requires using two different models (BART for summarization and BERT for QA). I want to concatenate the two models so that I can train them at the same time instead of separately. However, the two models use different tokenizers. Is there a way to create a layer mapping the output of the first layer to the second one? Would creating a dictionary by hand (mapping every single available value of the tokenizer) work?  Thanks a lot!</p>
",Multilingual Language Processing & Language Identification,translation different tokenizers sorry question basic asked tried find solution working nlp project requires using two different model bart summarization bert qa want concatenate two model train time instead separately however two model use different tokenizers way create layer mapping output first layer second one would creating dictionary hand mapping every single available value tokenizer work thanks lot
Natural Language Processing for an optimal solution to Hangman,"<p>I have a dictionary of 250,000 words and am supposed to design an algorithm to play the game of Hangman as follows:</p>
<ol>
<li>We are given a random word, not necessarily from the dictionary.</li>
<li>We have N attempts to guess the correct word by guessing each letter individually. Each incorrect guess leads to a penalty leading to losing an attempt.</li>
<li>Unlimited correct guesses are allowed.</li>
<li>The game stops when we either make N incorrect guesses or we have guessed the correct word.</li>
</ol>
<p>A common algorithm I came across is as follows:</p>
<ol>
<li>The algorithm starts with a masked string, replacing some letters with underscores.</li>
<li>It searches a dictionary to find all possible words matching the pattern. (The length of the word, say)</li>
<li>Then, it analyzes the frequency of letters appearing in these potential words and creates a table to keep track of these frequencies.</li>
<li>Next, it selects the letter with the highest frequency among the remaining unguessed letters as the best guess.</li>
</ol>
<p>I understood the algorithm given above. However, my question is, if I get a random word, <em><strong>not necessarily from the dictionary of words</strong></em>, won't this algorithm fail to work? What is a more efficient method to tackle this broader problem?</p>
",Multilingual Language Processing & Language Identification,natural language processing optimal solution hangman dictionary word supposed design algorithm play game hangman follows given random word necessarily dictionary n attempt guess correct word guessing letter individually incorrect guess lead penalty leading losing attempt unlimited correct guess allowed game stop either make n incorrect guess guessed correct word common algorithm came across follows algorithm start masked string replacing letter underscore search dictionary find possible word matching pattern length word say analyzes frequency letter appearing potential word creates table keep track frequency next selects letter highest frequency among remaining unguessed letter best guess understood algorithm given however question get random word necessarily dictionary word algorithm fail work efficient method tackle broader problem
Multilingual text spam detection,"<p>Many technologies can be used to detect spam in a specific language, and if proper technology is adopted, it can make a system be able to detect spams in multiple languages, but this requires a single text be in a specific language.</p>

<p>So my question is how to detect a text that composed of multiple languages? this is not only about language detection. I'd like to know some best practices to do multilingual text spam detection.</p>
",Multilingual Language Processing & Language Identification,multilingual text spam detection many technology used detect spam specific language proper technology adopted make system able detect spam multiple language requires single text specific language question detect text composed multiple language language detection like know best practice multilingual text spam detection
Is there any code can fix arabic words display problem?,"<p>i have a string and i want to fix it with python code, I tried to use reshape and it didn't work</p>
<pre><code>string = &quot;ÔªïÔ∫éÔªßÔªÆÔªß&quot;
</code></pre>
<p>the word must be like this</p>
<pre><code>ŸÇÿßŸÜŸàŸÜ
</code></pre>
<p>this is the lib I use and didn't work
arabic-reshaper</p>
<pre><code>text_to_be_reshaped = &quot;ÔªïÔ∫éÔªßÔªÆÔªß&quot;
reshaped_text = arabic_reshaper.reshape(text_to_be_reshaped)
</code></pre>
",Multilingual Language Processing & Language Identification,code fix arabic word display problem string want fix python code tried use reshape work word must like lib use work arabic reshaper
"Fine-tuning of multilingual translation models (Huggingface Transformers, Helsinki)","<p>I want to fine-tune pre-trained multilingual Models from the Huggingface transformers library (MarianMT in this case) for domain-specific translation. I want the models to be able to translate between 5 different languages. I have domain-specific datasets for every sentence pair (e.g. de-en, en-de, de-es, es-de and so on). In the available tutorials for fine-tuning I only could find fine-tuning for single language pairs (e.g. only the pretrained ‚ÄúHelsinki-NLP/opus-mt-en-roa‚Äù model is downloaded) which then needs to be fine-tuned on en-roa datasets. What I want to do is to train the whole multilingual model (not just en-roa). I want to mix the sentences of all sentence pairs of my datasets into one dataset and fine-tune the whole multilingual model on this large dataset. How can I achieve this task? Is it possible to download the ‚Äúwhole‚Äù model and not just the language pair models like en-roa? I hope someone can help me :)</p>
<p>Best regards,</p>
<p>Simon</p>
",Multilingual Language Processing & Language Identification,fine tuning multilingual translation model huggingface transformer helsinki want fine tune pre trained multilingual model huggingface transformer library marianmt case domain specific translation want model able translate different language domain specific datasets every sentence pair e g de en en de de e e de available tutorial fine tuning could find fine tuning single language pair e g pretrained helsinki nlp opus mt en roa model downloaded need fine tuned en roa datasets want train whole multilingual model en roa want mix sentence sentence pair datasets one dataset fine tune whole multilingual model large dataset achieve task possible download whole model language pair model like en roa hope someone help best regard simon
Expanding English language contractions in Python,"<p>The English language has <a href=""http://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions"">a couple of contractions</a>. For instance:</p>

<pre><code>you've -&gt; you have
he's -&gt; he is
</code></pre>

<p>These can sometimes cause headache when you are doing natural language processing. Is there a Python library, which can expand these contractions?</p>
",Multilingual Language Processing & Language Identification,expanding english language contraction python english language ha href couple contraction instance sometimes cause headache natural language processing python library expand contraction
Categorize rows per their similarity in Python,"<p>I am here to look for input for a data manipulation problem related to natural language processing.</p>
<p>To make life easier, I am using a mock dataset posted several years ago from <a href=""https://stackoverflow.com/questions/47159996/how-to-group-text-data-based-on-document-similarity"">How to group text data based on document similarity?</a>.</p>
<pre><code>import pandas as pd
from difflib import SequenceMatcher

df = pd.DataFrame({'Questions': ['What are you doing?','What are you doing tonight?','What are you doing now?','What is your name?','What is your nick name?','What is your full name?','Shall we meet?',
                             'How are you doing?' ]})

def similarity_score(s1, s2):
    return SequenceMatcher(None, s1, s2).ratio()

def similarity(x,df):
    sim_score = []
    for i in df['Questions']:
        sim_score.append(similarity_score(x,i))
    return sim_score

df['similarity'] = df['Questions'].apply(lambda x : similarity(x, df)).astype(str)
print(df)
</code></pre>
<p>The output is as following</p>
<pre><code>Questions  \
0          What are you doing?   
1  What are you doing tonight?   
2      What are you doing now?   
3           What is your name?   
4      What is your nick name?   
5      What is your full name?   
6               Shall we meet?   
7           How are you doing?   

                                          similarity  
0  [1.0, 0.8260869565217391, 0.9047619047619048, ...  
1  [0.8260869565217391, 1.0, 0.84, 0.533333333333...  
2  [0.9047619047619048, 0.84, 1.0, 0.585365853658...  
3  [0.6486486486486487, 0.5333333333333333, 0.585...  
4  [0.5714285714285714, 0.52, 0.5217391304347826,...  
5  [0.5714285714285714, 0.52, 0.5652173913043478,...  
6  [0.36363636363636365, 0.34146341463414637, 0.3...  
7  [0.8108108108108109, 0.6666666666666666, 0.731...  
</code></pre>
<p>The logic is that I go through each row in the data frame to compare it to all over rows (including itself) in order to compute their similarity. I then store the similarity score as a list in another column called &quot;similarity&quot;.</p>
<p>Next, I want to categorize the questions in the first column. If the similarity score &gt; 0.9, then those rows should be assigned to the same group. How can I achieve this?</p>
",Multilingual Language Processing & Language Identification,categorize row per similarity python look input data manipulation problem related natural language processing make life easier using mock dataset posted several year ago href group text data based document similarity output following logic go row data frame compare row including order compute similarity store similarity score list another column called similarity next want categorize question first column similarity score row assigned group achieve
Google Translate for nlp Dataset,"<p>I have been trying to work on a NLP dataset and want to use the google translate for oversampling purpose. I have reduced the length of text in each row to 4000 characters but when I try to translate it shows error. I have also used the latest google translate
<code>pip install googletrans==4.0.0-rc1</code></p>
<pre><code>df['Sentence'] = df['sentence'].str.slice(0,4000)
df['translation_text'] = df['Sentence'].apply(lambda x: translator.translate(x, src='en', dest='de').text )
df['sentence2'] = df['translation_text'].apply(lambda x: translator.translate(x, src='de', dest='en').text )
</code></pre>
<p>Error shows up every time - TypeError: the JSON object must be str, bytes or bytearray, not NoneType
Now, I am confused as to what I am missing that these codes are not working</p>
",Multilingual Language Processing & Language Identification,google translate nlp dataset trying work nlp dataset want use google translate oversampling purpose reduced length text row character try translate show error also used latest google translate error show every time typeerror json object must str byte bytearray nonetype confused missing code working
Unable to detect gibberish names using Python,"<p>I am trying to build Python model that could classify account names as either legitimate or gibberish. Capitalization is not important in this particular case as some legitimate account names could be comprised of all upper-case or all lower-case letters. </p>

<p><strong>Disclaimer: this is just a internal research/experiment and no real action will be taken on the classifier outcome.</strong> </p>

<p>In my particular, there are 2 possible characteristics that can reveal an account name as suspicious, gibberish or both:</p>

<ol>
<li><p>Weird/random spelling in name or name consists of purely or mostly numbers. Examples of account names that fit these criteria are: <strong>128, 127, h4rugz4sx383a6n64hpo, tt, t66, t65, asdfds</strong>.</p></li>
<li><p>The name has 2 components (let's assume that no name will ever have more than 2 components) and the spelling and pronounciation of the 2 components are very similar. Examples of account names that fit these criteria are: <strong>Jala Haja, Hata Yaha, Faja Kaja</strong>.</p></li>
</ol>

<p>If an account name meets both of the above criteria (i.e. 'asdfs lsdfs', '332 333') it should also be considered suspicious.  </p>

<p>On the other hand, a legitimate account name doesn't need to have both first name and last name. They are usually names from popular languages such as Roman/Latin (i.e. Spanish, German, Portuguese, French, English), Chinese, and Japanese. </p>

<p>Examples of legitimate account names include (these names are made up but do reflect similar styles of legitimate account names in real world): <strong>Michael, sara, jose colmenares, Dimitar, Jose Rafael, Morgan, Eduardo Medina, Luis R. Mendez, Hikaru, SELENIA, Zhang Ming, Xuting Liu, Chen Zheng</strong>.</p>

<p>I've seen some slightly similar questions on Stackoverflow that asks for ways to detect gibberish texts. But those don't fit my situation because legitimate texts and words actually have meanings, whereas human names usually don't. I also want to be able to do it just based on account names and nothing else.  </p>

<p>Right now my script takes care of finding the 2nd characteristic of suspicious account names (similar components in name) using Python's Fuzzy Wuzzy package and using 50% as the similarity threshold. The script is listed below:</p>

<pre><code>from fuzzywuzzy import fuzz
from fuzzywuzzy import process

import pandas as pd
import numpy as np

accounts = pd.read_csv('dataset_with_names.csv', encoding = 'ISO-8859-1', sep=None, engine='python').replace(np.nan, 'blank', regex=True)

pd.options.mode.chained_assignment = None

accounts.columns = ['name', 'email', 'akon_id', 'acct_creation_date', 'first_time_city', 'first_time_ip', 'label']

accounts['name_simplified']=accounts['name'].str.replace('[^\w\s]','')
accounts['name_simplified']=accounts['name_simplified'].str.lower()

sim_name = []

for index, row in accounts.iterrows():        
    if ' ' in row['name_simplified']:
        row['name_simplified']=row['name_simplified'].split()
        if len(row['name_simplified']) &gt; 1:
            #print(row['name_simplified'])
            if fuzz.ratio(row['name_simplified'][0], row['name_simplified'][1]) &gt;= 50:
                sim_name.append('True')
            else:
                sim_name.append('False')
        else:
            sim_name.append('False')
    else:
        sim_name.append('False')        

accounts['are_name_components_similar'] = sim_name 
</code></pre>

<p>The result has been reliable for what the script was designed to do, but I also want to be able to surface gibberish account names with the 1st characteristic (weird/random spelling or name consists of purely or mostly numbers). So far I have not found a solution to that yet. </p>

<p>Can anyone help? Any feedback/suggestion will be greatly appreciated! </p>
",Multilingual Language Processing & Language Identification,unable detect gibberish name using python trying build python model could classify account name either legitimate gibberish capitalization important particular case legitimate account name could comprised upper case lower case letter disclaimer internal research experiment real action taken classifier outcome particular possible characteristic reveal account name suspicious gibberish weird random spelling name name consists purely mostly number example account name fit criterion h rugz sx n hpo tt asdfds name ha component let assume name ever component spelling pronounciation component similar example account name fit criterion jala haja hata yaha faja kaja account name meet criterion e asdfs lsdfs also considered suspicious hand legitimate account name need first name last name usually name popular language roman latin e spanish german portuguese french english chinese japanese example legitimate account name include name made reflect similar style legitimate account name real world michael sara jose colmenares dimitar jose rafael morgan eduardo medina luis r mendez hikaru selenia zhang ming xuting liu chen zheng seen slightly similar question stackoverflow asks way detect gibberish text fit situation legitimate text word actually meaning whereas human name usually also want able based account name nothing else right script take care finding nd characteristic suspicious account name similar component name using python fuzzy wuzzy package using similarity threshold script listed result ha reliable script wa designed also want able surface gibberish account name st characteristic weird random spelling name consists purely mostly number far found solution yet anyone help feedback suggestion greatly appreciated
Transliteration from a source language to Roman (English) script,"<p>We need the Romanization feature badly. Can someone please help? We want to transliterate (<strong>not translate</strong>) from Hindi (Devanagiri script) language to English (Roman script) language.</p>
<p><strong>Input</strong><br />
<code>romanize_text('‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à')</code></p>
<p><strong>Expected Output</strong><br />
<code>'antim lakshya kya hai'</code></p>
<p>As per the <a href=""https://cloud.google.com/translate/docs/advanced/romanize-text#romanize_text"" rel=""nofollow noreferrer"">Google Romanize text docs</a>, I wrote the following Python code to transliterate from some language script to Roman script.</p>
<pre class=""lang-py prettyprint-override""><code># Authenticate using credentials.
import os
os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;translate.json&quot;

PROJECT_ID = &quot;project-id&quot;
LOCATION = &quot;global&quot;

# Imports the Google Cloud Translation library
from google.cloud import translate_v3

# Transliteration.
def romanize_text(text, src_lang=&quot;hi&quot;, tgt_lang=&quot;en&quot;):

    client = translate_v3.TranslationServiceClient()
    parent = f&quot;projects/{PROJECT_ID}/locations/{LOCATION}&quot;

    response = client.romanize_text(
        request={
            &quot;parent&quot;: parent,
            &quot;contents&quot;: [text],
            &quot;source_language_code&quot;: src_lang,
            &quot;target_language_code&quot;: tgt_lang,
        }
    )

    # Display the romanized for each input text provided
    for romanization in response.romanizations:
        print(f&quot;Romanized text: {romanization.romanized_text}&quot;)

romanize_text('‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à')
</code></pre>
<p>Running the above code, gives the following error:</p>
<pre><code>AttributeError: 'TranslationServiceClient' object has no attribute 'romanize_text'
</code></pre>
<p>Also, in the <a href=""https://cloud.google.com/translate/docs/reference/rest/v3/projects/romanizeText"" rel=""nofollow noreferrer"">Google's API reference of romanizeText</a>, the right-hand side API Explorer is broken. Whereas, if you select any other method from the left-hand side - its API Explorer works correctly.</p>
<p>We need the Romanization feature badly: so either a solution to the aforementioned problem, or an alternative non-Google solution for romanization would be fine.</p>
",Multilingual Language Processing & Language Identification,transliteration source language roman english script need romanization feature badly someone please help want transliterate translate hindi devanagiri script language english roman script language input expected output per google romanize text doc wrote following python code transliterate language script roman script running code give following error also google api reference romanizetext right hand side api explorer broken whereas select method left hand side api explorer work correctly need romanization feature badly either solution aforementioned problem alternative non google solution romanization would fine
How to download spaCy models in a Poetry managed environment,"<p>I am writing a Python Jupyter notebook that does some NLP processing on Italian texts.</p>
<p>I have installed spaCy 3.5.3 via Poetry and then attempt to run the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
load_model = spacy.load('it_core_news_sm')
</code></pre>
<p>The <code>import</code> line works as expected, but running <code>spacy.load</code> produces the following error:</p>
<blockquote>
<p>OSError: [E050] Can't find model 'it_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.
The model name is correct as shown on <a href=""https://spacy.io/models/it"" rel=""noreferrer"">https://spacy.io/models/it</a></p>
</blockquote>
<p>After a web search, I see that a solution is to issue the following command:</p>
<pre class=""lang-none prettyprint-override""><code>python3 -m spacy download it_core_news_sm
</code></pre>
<p>After running this command the above code works as expected, however, is there a more 'kosher' way of doing this via Poetry?</p>
",Multilingual Language Processing & Language Identification,download spacy model poetry managed environment writing python jupyter notebook doe nlp processing italian text installed spacy via poetry attempt run following code line work expected running produce following error oserror e find model core news sm seem python package valid path data directory model name correct shown web search see solution issue following command running command code work expected however kosher way via poetry
How to turn an adjective into a noun in python (pymorphy2)?,"<p>I have a word in Russian (which is actually where the difficulty arises). It's an adjective. And I need to make it in a noun form.</p>
<p>I found an interesting library that can parse words, inflect and normalize them. Thiw is library: <code>pymorphy2</code>, but no matter what I do, I can't get the expected result. I want to receive:</p>
<p>These are the names of cities. On the left is the form of the word that I get from the data, on the right, the form that I need.</p>
<pre><code>–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π ---&gt; –∫–∏—Å–µ–ª–µ–≤—Å–∫
—é—Ä–≥–∏–Ω—Å–∫–∏–π ---&gt; —é—Ä–≥–∞
</code></pre>
<p>So far, the words on the left are defined only as different forms of the adjective, if you look at their analysis. Is there any way to convert them to nouns?</p>
<p>Small word parsing code:</p>
<pre><code>import pymorphy2
morph = pymorphy2.MorphAnalyzer()
word = '–ö–∏—Å–µ–ª–µ–≤—Å–∫–∏–π'

test = morph.parse(word)[0]
test.tag.POS

&gt;&gt;'ADJF'

test_2 = morph.parse(word)[0].normal_form
test_2

&gt;&gt;'–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π'

test.lexeme
&gt;&gt; [Parse(word='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', tag=OpencorporaTag('ADJF masc,sing,nomn'), normal_form='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', score=1.0, methods_stack=((FakeDictionary(), '–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', 16, 0), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), '–≤—Å–∫–∏–π'))),
 Parse(word='–∫–∏—Å–µ–ª–µ–≤—Å–∫–æ–≥–æ', tag=OpencorporaTag('ADJF masc,sing,gent'), normal_form='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', score=1.0, methods_stack=((FakeDictionary(), '–∫–∏—Å–µ–ª–µ–≤—Å–∫–æ–≥–æ', 16, 1), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), '–≤—Å–∫–∏–π'))),
 Parse(word='–∫–∏—Å–µ–ª–µ–≤—Å–∫–æ–º—É', tag=OpencorporaTag('ADJF masc,sing,datv'), normal_form='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', score=1.0, methods_stack=((FakeDictionary(), '–∫–∏—Å–µ–ª–µ–≤—Å–∫–æ–º—É', 16, 2), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), '–≤—Å–∫–∏–π'))),
 Parse(word='–∫–∏—Å–µ–ª–µ–≤—Å–∫–æ–≥–æ', tag=OpencorporaTag('ADJF anim,masc,sing,accs'), normal_form='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', score=1.0, methods_stack=((FakeDictionary(), '–∫–∏—Å–µ–ª–µ–≤—Å–∫–æ–≥–æ', 16, 3), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), '–≤—Å–∫–∏–π'))),
 Parse(word='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', tag=OpencorporaTag('ADJF inan,masc,sing,accs'), normal_form='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', score=1.0, methods_stack=((FakeDictionary(), '–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', 16, 4), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), '–≤—Å–∫–∏–π'))),
 Parse(word='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–º', tag=OpencorporaTag('ADJF masc,sing,ablt'), normal_form='–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–π', score=1.0, methods_stack=((FakeDictionary(), '–∫–∏—Å–µ–ª–µ–≤—Å–∫–∏–º', 16, 5), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), '–≤—Å–∫–∏–π')))...]
</code></pre>
",Multilingual Language Processing & Language Identification,turn adjective noun python pymorphy word russian actually difficulty arises adjective need make noun form found interesting library parse word inflect normalize thiw library matter get expected result want receive name city left form word get data right form need far word left defined different form adjective look analysis way convert noun small word parsing code
keeping certain stopwords when natural language processing in R,"<p>I'm doing natural language processing in R with the code below. I noticed the line that remove stopwords, removes the word 'no'. Can I have it keep that word? Is there a way to view all the words it removes?</p>
<pre><code># Pre-processing chain
corpus &lt;- tm_map(corpus, tolower)
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, removeNumbers)
cleanset &lt;- tm_map(corpus, removeWords, stopwords('english')) # do not remove the word 'no'
cleanset &lt;- tm_map(cleanset, stemDocument)
cleanset &lt;- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:25])
</code></pre>
",Multilingual Language Processing & Language Identification,keeping certain stopwords natural language processing r natural language processing r code noticed line remove stopwords remove word keep word way view word remove
Natural Language Processing for Smart Homes,"<p>I'm writing up a Smart Home software for my bachelor's degree, that will only simulate the actual house, but I'm stuck at the NLP part of the project. The idea is to have the client listen to voice inputs (already done), transform it into text (done) and send it to the server, which does all the heavy lifting / decision making.</p>
<p>So all my inputs will be fairly short (like &quot;please turn on the porch light&quot;). Based on this, I want to take the decision on which object to act, and how to act. So I came up with a few things to do, in order to write up something somewhat efficient.</p>
<ol>
<li>Get rid of unnecessary words (in the previous example &quot;please&quot; and &quot;the&quot; are words that don't change the meaning of what needs to be done; but if I say &quot;turn off <em>my</em> lights&quot;, &quot;my&quot; does have a fairly important meaning).</li>
<li>Deal with synonyms (&quot;turn on lights&quot; should do the same as &quot;enable lights&quot; -- I know it's a stupid example). I'm guessing the only option is to have some kind of a dictionary (XML maybe), and just have a list of possible words for one particular object in the house.</li>
<li>Detecting the verb and subject. &quot;turn on&quot; is the verb, and &quot;lights&quot; is the subject. I need a good way to detect this.</li>
<li>General implementation. How are these things usually developed in terms of algorithms? I only managed to find one article about NLP in Smart Homes, which was very vague (and had bad English). Any links welcome.</li>
</ol>
",Multilingual Language Processing & Language Identification,natural language processing smart home writing smart home software bachelor degree simulate actual house stuck nlp part project idea client listen voice input already done transform text done send server doe heavy lifting decision making input fairly short like please turn porch light based want take decision object act act came thing order write something somewhat efficient get rid unnecessary word previous example please word change meaning need done say turn light doe fairly important meaning deal synonym turn light enable light know stupid example guessing option kind dictionary xml maybe list possible word one particular object house detecting verb subject turn verb light subject need good way detect general implementation thing usually developed term algorithm managed find one article nlp smart home wa vague bad english link welcome
Using Google&#39;s T5 for translation from German to English,"<p>I am trying to use Google's T5 for language translation. However, it is not working for German to English.</p>
<p>English to German works fine:</p>
<pre><code>self.tokenizer = AutoTokenizer.from_pretrained(&quot;t5-small&quot;)
self.model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-small&quot;)
inputs = self.tokenizer.encode(&quot;translate English to German: &quot; + text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
summary_ids = self.model.generate(inputs, max_length=512, min_length=5, length_penalty=5., num_beams=2)
summary = self.tokenizer.decode(summary_ids[0])
</code></pre>
<p>However, changing encoding to &quot;German to English&quot; is not working.</p>
<p>Is this model not intended to be able to translate German to English, or am I using it wrong?</p>
",Multilingual Language Processing & Language Identification,using google translation german english trying use google language translation however working german english english german work fine however changing encoding german english working model intended able translate german english using wrong
Nonsensical Distance queries from weaviate,"<p>I am using weaviate with the vectorizer <code>semitechnologies/transformers-inference:sentence-transformers-paraphrase-multilingual-mpnet-base-v2</code>, which is supposedly the best multi language vectorizer available. I am using it sort through German text:</p>
<pre><code>import weaviate from 'weaviate-ts-client';

const client = weaviate.client({
    scheme: 'http',
    host: 'localhost:8080',
});



const schemaConfig = {
    class: 'Person',
    properties: [
        {
            name: 'name',
            dataType: ['string'],
        }
    ]
};
try {
    await client.schema.classDeleter().withClassName('Person').do();
} catch (e) {
    console.log(e)
}

await client.schema
    .classCreator()
    .withClass(schemaConfig)
    .do();


const schemaRes = await client.schema.getter().do();



await client.data.creator()
  .withClassName('Person')
  .withProperties({
           //night receptionist at a hotel
      name:&quot;Nachtportier: (ca. 50%)&quot; 
  })
  .do();

await client.data.creator()
  .withClassName('Person')
  .withProperties({
      //Mandate manager for agricultural fiduciary services e.g. fiduciary with federal certificate or equivalent training
      name:&quot;Mandatsleiter Agrotreuhand z.B. Treuh√§nder mit eidg. Fachausweis oder gleichwertiger Ausbildung&quot;
  })
  .do();


            //night receptionist at a hotel
const query=&quot;Nachtportier&quot;

const resImage = await client.graphql.get()
  .withClassName('Person')
  .withFields(['name _additional{distance certainty id}'])
  .withNearText({concepts: [query]})
  .do();

console.log(resImage.data.Get.Person)
</code></pre>
<p>Executing which gives this result</p>
<pre><code>[
  {
    _additional: {
      certainty: 0.8617309927940369,
      distance: 0.276538,
      id: '20b34dc7-d7d7-4d00-8c1d-93022960f224'
    },
    name: 'Mandatsleiter Agrotreuhand z.B. Treuh√§nder mit eidg. Fachausweis oder gleichwertiger Ausbildung'
  },
  {
    _additional: {
      certainty: 0.7965770363807678,
      distance: 0.40684593,
      id: '0b072dab-2c93-4189-b713-101fec7248b3'
    },
    name: 'Nachtportier: (ca. 50%)'
  }
]
</code></pre>
<p>Even though the first text has nothing to do with night or hotels, it is still much higher than the literal query word. In the full application there is a lot of text that also has nothing to do with the input query being scored much higher than the second text. This is also the case with many other text queries in the final app. The vectorizer sometimes works and suggests the right jobs, but theres also many, many cases like this one where it scores unrelated texts unreasonably. High. Am I using the system wrong or is it just not mature technology yet?</p>
",Multilingual Language Processing & Language Identification,nonsensical distance query weaviate using weaviate vectorizer supposedly best multi language vectorizer available using sort german text executing give result even though first text ha nothing night hotel still much higher literal query word full application lot text also ha nothing input query scored much higher second text also case many text query final app vectorizer sometimes work suggests right job also many many case like one score unrelated text unreasonably high using system wrong mature technology yet
Python translate large texts to English,"<p>I am searching for a Python library that translates very large texts to English. I have already used <code>TextBlob</code> (which at some point just stops translating, API limits I suppose), <code>googletrans</code> (which at some point also just stops translating, it also doesn't translate very large texts and I have to split them into pieces and then merge). I am looking for a solution that I can be sure that it won't stop working, since I will be running this code regularly on around 100K texts with average word length of 10K. If anyone has done something similar, I would appreciate your help!</p>
",Multilingual Language Processing & Language Identification,python translate large text english searching python library translates large text english already used point stop translating api limit suppose point also stop translating also translate large text split piece merge looking solution sure stop working since running code regularly around k text average word length k anyone ha done something similar would appreciate help
Seq2Seq Neural Machine Translation step for aligning right to left languages with English (Or any LTR language),"<p>I've so far worked with left to right languages and NLTK worked fine for tokenization. But while working on a research paper focused on several languages including RTL languages, the normal procedure has been giving me completely inaccurate translations. Could anyone please let me know what is the norm in neural machine translation when working with languages like Persian or Hebrew?</p>
<p>I've tried following the steps mentioned in <a href=""https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb?hl=ja#scrollTo=chTF5N885F0P"" rel=""nofollow noreferrer"">nmt with attention</a>, where I changed the two regex to fit the Farsi and Urdu scripts along with other languages and seperating the punctuations,</p>
<pre><code>def lowerSplitPunct(text):
  # Split accented characters.
  text = tf_text.normalize_utf8(text, 'NFKC')
  text = tf.strings.lower(text)
  # Keep space, a to z, and select punctuation.
  text = tf.strings.regex_replace(text, '[^\u0600-\u06FF\uFB8A\u067E\u0686\u06AF\u200C\u200F\u0980-\u09FFa-z€îÿüÿå¬´¬ª‡•§ÿß.?!,]', '')
  # Add spaces around punctuation.
  text = tf.strings.regex_replace(text, '[€îÿüÿå¬´¬ªÿß‡•§.?!,]', r' \0 ')
  # Strip whitespace.
  text = tf.strings.strip(text)

  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')
  return text
</code></pre>
<p>and it still doesn't solve my problem.</p>
",Multilingual Language Processing & Language Identification,seq seq neural machine translation step aligning right left language english ltr language far worked left right language nltk worked fine tokenization working research paper focused several language including rtl language normal procedure ha giving completely inaccurate translation could anyone please let know norm neural machine translation working language like persian hebrew tried following step mentioned nmt attention changed two regex fit farsi urdu script along language seperating punctuation still solve problem
bert-case from portugese to english,"<p>I'm trying to build bert question-answering model.
This is from where I'm trying to build it:
<a href=""https://nbviewer.org/github/piegu/language-models/blob/master/question_answering_BERT_large_cased_squad_v11_pt.ipynb"" rel=""nofollow noreferrer"">https://nbviewer.org/github/piegu/language-models/blob/master/question_answering_BERT_large_cased_squad_v11_pt.ipynb</a></p>
<pre><code>model_name_or_path = &quot;neuralmind/bert-large-portuguese-cased&quot;
dataset_name = &quot;squad11pt&quot;
</code></pre>
<p>The above is the path for portugese. I want to change it to English. I have tried using path name as</p>
<pre><code>bert-large-cased
</code></pre>
<p>but it's not working. Can someone tell me how I can change the dataset to English?</p>
",Multilingual Language Processing & Language Identification,bert case portugese english trying build bert question answering model trying build path portugese want change english tried using path name working someone tell change dataset english
How i get the occurrence of a sentence with google ngram viewer and python?,"<p>short backround: i try to enhance the spelling corrector by <a href=""https://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">Peter Norvig</a> in python. In this sense i need the occurrence of a sentence (up to 3-4 words)... The <a href=""https://books.google.com/ngrams"" rel=""nofollow noreferrer"">Ngram viewer from Google</a> would help me a lot but i don't know how i get the value with an API or something else.</p>
<p>pseudocode:</p>
<pre><code># Sentence without meaning but word for word correct.
&gt;&gt; occurrence(&quot;were are you&quot;)
0.0000000978

# Sentence that makes sense
&gt;&gt; occurrence(&quot;where are you&quot;)
0.000148

# Then my method should return the sentence with the highest value. (But thats not the problem)
</code></pre>
<p>sorry for my english :-D
Thank you!</p>
",Multilingual Language Processing & Language Identification,get occurrence sentence google ngram viewer python short backround try enhance spelling corrector peter norvig python sense need occurrence sentence word ngram viewer google would help lot know get value api something else pseudocode sorry english thank
How to check if a python string is a valid Bengali word using regular expression?,"<p>I am trying to test if a word is a valid Bengali word which may contain Bengali letters, vowel markers, 'Hasanta' (&quot;‡ßç&quot;), Bengali digits, all punctuation symbols including Bengali &quot;‡•§&quot;. We can test this easily for English using regex patter <code>&quot;\w+&quot;</code>, but I cannot find any way to do this in Bengali.</p>
<p>For example, these strings: <code>&quot;‡¶Ü‡¶Æ‡¶ø&quot;</code>, <code>&quot;‡¶≠‡¶æ‡¶§&quot;</code>, <code>&quot;‡¶ñ‡¶æ‡¶á‡•§&quot;</code>, <code>&quot;‡ßß‡ß®‡ß©&quot;</code> should be detected as valid Bengali words, but <code>&quot;I&quot;</code>, <code>&quot;eat&quot;</code>, <code>&quot;rice&quot;</code>, <code>&quot;123&quot;</code> should be detected as invalid words.</p>
",Multilingual Language Processing & Language Identification,check python string valid bengali word using regular expression trying test word valid bengali word may contain bengali letter vowel marker hasanta bengali digit punctuation symbol including bengali test easily english using regex patter find way bengali example string detected valid bengali word detected invalid word
Finding words that indicate time order using NLP in python,"<p>I want to find all words that indicate time using NLP in python. Also known as Time-order words in english. This includes works like 'evening' 'morning', 'first', '5 o'clock ', etc. I can't find a way to do this without having to make a list of every time order word in english. I need this for an english to American sign language sentence translator that should convert an English sentence to the correct grammatical structure of an ASL sentence. In ASL, timeorder words should be at the start of the sentence in most cases.</p>
<p>I can't find anything on spaCy or NLTK that does this for me, if anyone knows any functionalities i missed please let me know. This is my current code:</p>
<pre><code>import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import string
import spacy

nltk.download('punkt', )
nltk.download('averaged_perceptron_tagger')

nlp = spacy.load('en_core_web_sm')
# nlp = spacy.load('en_core_web_lg')  #takess longer to load but is a larger dictionary

sentences=[&quot;The big black cat stared at the small dog.&quot;,
           &quot;I didn't watch her brother in the evenings.&quot;,
           &quot;Which car did Jane buy?&quot;]
doc = nlp(sentences[1])
# for word in doc
tokens_list = []
signs_list = []
for token in doc:
  # remove auxilaries, punctuation, determiners, prepositions
    if token.pos_ not in ['AUX', 'PUNCT'] and token.tag_ not in ['DT','IN']:
        tokens_list.append([token.text, token.lemma_, token.pos_, token.tag_,])
        signs_list.append(token.lemma_)
for sign in signs_list:
  signs_list[signs_list.index(sign)] = sign.lower()
signs_list

if 'not' in signs_list: 
  ind = signs_list.index('not')
  signs_list[ind], signs_list[ind+1] = signs_list[ind+1], signs_list[ind]
# not comes after the verb it negates in ASL

if 'i' in signs_list:
  ind = signs_list.index('i')
  signs_list[ind] = 'me'

# 'me' is used instead of 'i' in ASL

signs_list
</code></pre>
<p>SOLUTION:</p>
<pre><code>timeOrderWords = [synset.lemma_names() for synset in wn.synsets('time_of_day')[0].hyponyms()]
for x in wn.synsets('time_of_day')[0].hyponyms():
  if len(x.hyponyms()) &gt; 0:
    timeOrderwords = [timeOrderWords.append(y.lemma_names()) for y in x.hyponyms()]
timeOrderWords = list(itertools.chain(*timeOrderWords))
timeOrderWords = [timeOrderWords[x].replace('_', &quot; &quot;) for x in range(len(timeOrderWords))]
timeOrderWords = [timeOrderWords[x].replace('-', &quot; &quot;) for x in range(len(timeOrderWords))]
timeOrderWords
</code></pre>
<p>The code above returns a flat list of all the words related (synsets) to 'time_of_day' and all the hyponyms of those related words. Then all the _ and - are removed from the words</p>
<p>The list looks like this: (I had to add some words myself like afternoon)</p>
<pre><code>timeOrderWords = ['aurora', 'afternoon','bedtime', 'break of day', 'break of the day', 'breakfast time','canonical hour','closing time',
 'cockcrow','complin','compline','crepuscle','crepuscule','dawn','dawning','daybreak','dayspring','dinnertime',
 'dusk','early morning hour','evenfall','evening','evensong','fall','first light','gloam','gloaming','happy hour','high noon','late night hour',
 'lights out','lunch period','lunchtime','matins','mealtime','midday','midnight','morning','morning prayer','night','nightfall','none','nones','noon',
 'noonday','noontide','prime','rush hour','sext','small hours','sundown','sunrise','sunset','sunup','suppertime','terce','tierce','twelve noon','twilight','vespers','zero hour']
</code></pre>
",Multilingual Language Processing & Language Identification,finding word indicate time order using nlp python want find word indicate time using nlp python also known time order word english includes work like evening morning first clock etc find way without make list every time order word english need english american sign language sentence translator convert english sentence correct grammatical structure asl sentence asl timeorder word start sentence case find anything spacy nltk doe anyone know functionality missed please let know current code solution code return flat list word related synset time day hyponym related word removed word list look like add word like afternoon
re.findall does not find some dots,"<p>I have a file with preprocessed german text <a href=""https://i.sstatic.net/1XRBP.png"" rel=""nofollow noreferrer"">all 39lines have a dot at the end</a></p>
<p>In order to get rid of some nulls in text I use this code:</p>
<pre><code>text_with_nulls = open('lemmatizeAFD', 'r')
text_without_nulls = open('lemmatizeAFD_without_nulls', 'w')
for i in text_with_nulls:
    res = re.findall(r'[a-zA-Z0-9√§√∂√º√Ñ√ñ√ú√ü\.]+', i)
    for i in res:
        text_without_nulls.write(i)
        text_without_nulls.write(' ')
</code></pre>
<p>the output file, however, has only 33 dots, but all lemmatized words are in placed</p>
<p><a href=""https://i.sstatic.net/Kr8X1.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Why did some dots disappear? I need them to split sentenses in separate lines later.</p>
<p>I am not very pofessional in <code>re</code>, so I assume that something is wrong with <code>res = re.findall(r'[a-zA-Z0-9√§√∂√º√Ñ√ñ√ú√ü\.]+', i)</code></p>
",Multilingual Language Processing & Language Identification,findall doe find dot file preprocessed german text line dot end order get rid null text use code output file however ha dot lemmatized word placed enter image description dot disappear need split sentenses separate line later pofessional assume something wrong
What is the most efficient way to identify text similarity between items in large lists of strings in Python?,"<p>The following piece of code achieves the results I'm trying to achieve. There is a list of strings called 'lemmas' that contains the accepted forms of a specific class of words. The other list, called 'forms' contains a lot of spelling variations of words found in a large amount of texts from different periods and different dialects of a specific language. For each one of the words in 'forms', I want to get the string in 'lemmas' that is the closest match.</p>
<p>The script, as I said, seems to work well with some test lists I've constructed. The problem I have, though, is that when I use the real lists, which are rather large, it takes forever to produce the results. In fact, I have had to stop the execution of the program because it was taking already more than two hours and the computer was becoming very slow and I couldn't do anything else.</p>
<p>What could I do to make this more efficient? How would I have to modify the code using other Python tools or libraries to make this faster? Thanks in advance.</p>
<pre><code>    import textdistance
    from textdistance import hamming
    from textdistance import cosine
    from textdistance import jaro_winkler
    import heapq
    
    # 'lemmas' is a list containing a huge amount of words, basically dictionary entries
    # 'forms' is a huge list of spelling variations of words found in hundreds of texts
    
    distances = {}
    
    processed_pairs = set() # keep track of processed pairs
    for lemma in lemmas:
        if lemma is None:
            continue
        lemma_lower = lemma.lower()
        for form in forms:
            if form is None:
                continue        
            form_lower = form.lower()
            pair = (lemma_lower, form_lower) # create a tuple with the lowercase pair
            if pair not in processed_pairs: # check if the pair has been processed before
                processed_pairs.add(pair)
                if textdistance.hamming.normalized_similarity(lemma_lower, form_lower) &gt; 0.34 and textdistance.jaro_winkler(lemma_lower, form_lower) &gt; 0.7 and textdistance.cosine(lemma_lower, form_lower) &gt; 0.5:             
                    dist = hamming.normalized_similarity(lemma_lower, form_lower)
                    distances.setdefault(form_lower, []).append((dist, lemma_lower))
    
    # Find the closest pairs
    closest_pairs = {}
    for form, dist_lemmas in distances.items():
        closest_pairs[form] = heapq.nsmallest(2, dist_lemmas)
    
    with open(ROOT / 'potential_lemmas.txt', 'w') as f:
        for form, pairs in closest_pairs.items():
            for dist, lemma in pairs:
                f.write(f&quot;{form} ‚ûù  {lemma}: {dist}\n&quot;)
             

</code></pre>
<p>EDIT:</p>
<p>In the end, the solution that worked the best was an integration of @Kyle F Hartzenberg's proposal with @Jamie_B suggestion of using joblib to parallelize (see comments after code, though):</p>
<pre><code>from itertools import zip_longest
from bisect import insort
from joblib import Parallel, delayed
import line_profiler

profile = line_profiler.LineProfiler()

emmas = ['gran', 'vermell', 'groc', 'atens', 'Do', 'dOne', 'PUrpose', 'can', 'be', 'use', 'for', 'cannon', 'amuse', 'useful', 'user', 'become', 'downtown', 'develop', 'fulminate', 'deduce', 'de', 'bezant']

forms = ['preriarenos', 'Marinara', 'Grand', 'Gran', 'Grans', 'Grands', 'Grandeses', 'Grandullons', 'grand', 'grandissisimus', 'gran', 'grans', 'grands', 'grandeses', 'grandullons', 'grandullon', 'grandullones', 'uermell', 'uermells', 'vermell', 'vermells', 'vermella', 'vermelles', 'varmell√≠ssimes', 'uarmell√≠ssimes', 'uermell√≠ssimes', 'uarnell√≠ssimes', 'varmell√≠ssima', 'uermella', 'uarmella', 'uarnella', 'varnella', 'uarnellas', 'varnellas', 'varmella', 'uermelles', 'grog', 'grogues', 'doNE', 'donE', 'doIng', 'purposeful', 'canonical', 'becareful', 'being', 'berate', 'best', 'bezant', 'full', 'fulmination', 'predict', 'downgrade', 'down', 'developing', 'deduct', 'deducing']

distances = {}

@delayed
def calc_distances(form, lemmas_low):
    form_distances = []
    for lemma in lemmas_low:
        char_matches = [c1 != c2 for c1, c2 in zip_longest(lemma, form)]
        dist = 1 - (sum(char_matches)/len(char_matches))
        if dist &gt; 0.25:
            insort(form_distances, (dist, lemma))
    return (form, form_distances)

@profile
def profile_distance_calcs():
    lemmas_low = [lemma.lower() for lemma in lemmas]
    forms_low = [form.lower() for form in forms]
    results = Parallel(n_jobs=-1, prefer=&quot;threads&quot;)(calc_distances(form, lemmas_low) for form in forms_low)
    for form, form_distances in results:
        distances[form] = form_distances

    with open(&quot;potential_lemmas_hamming-like.txt&quot;, &quot;w&quot;) as f:
        for form, form_distances in distances.items():
            for dist, lemma in reversed(form_distances[-2:]):
                f.write(f&quot;{form} ‚ûù  {lemma}: {dist}\n&quot;)

if __name__ == &quot;__main__&quot;:
    profile_distance_calcs()
    profile.print_stats()

</code></pre>
<p>This was a HUGE improvement over everything I had tried before. Besides the test with the short lists in the example, I ran it with the actual lists containing around 190,000 strings and the processing time was 118 minutes. While I'm pretty sure this could be improved (one might look for ways to do it using some kind of vectorization - someone suggested using arrays from numpy or AI oriented libraries), for the time being, this is quite manageable. There is still a problem that doesn't have to do with efficiency.</p>
<p>I mention this in my comment to @jqurious below but I will explain it here in more detail. Running the script above with the test list, one gets results like the following:</p>
<pre><code>berate ‚ûù  bezant: 0.5
berate ‚ûù  become: 0.5
</code></pre>
<p>From a linguistic point of view, any English speaker would know that these pairs of words are not related (OK, unless you know about the history of the language and know that be- used to be a productive prefix). What I'm trying to do with this script is to determine what would be the appropriate lemma (the dictionary form or representative word) for all the variants of a particular word found in the texts of a corpus.</p>
<p>This is a diachronic corpus containing many texts from many different authors and from many different dialects of a language writen over a period of more than 5 centuries. A 'u' could often be used instead of 'v' or a 'y' instead of an 'i'. An 'h' can als be often be missing from a word that is spelt with 'h' even in the same text by the same author. The variation is huge and yet even a modern speaker of the languate can usually detect whether the words are related quite easily. Of course, the speaker of the language is knowledgeable about the word structure and the morphology and so can immediately see that, for instance, 'uermell√≠ssima' is related to 'vermell' despite the fact that a lot of characters are different.</p>
<p>Using Kyle's suggestion with the actual lists, I got results like the following:</p>
<pre><code>beato ‚ûù  beat: 0.8
beatri√ß ‚ûù  tectriu: 0.5714285714285714
beatri√ß ‚ûù  teatral: 0.5714285714285714
beatte ‚ûù  beats: 0.6666666666666667
beatus ‚ûù  certus: 0.6666666666666667
beat√≠ssim ‚ûù  nequ√≠ssim: 0.6666666666666667
beat√≠ssim ‚ûù  grav√≠ssim: 0.6666666666666667
</code></pre>
<p>Even if you don't know the language (medieval Catalan in case anybody is interested), you can see how this is very wrong (using other algorithms like the Levenshtein or the cosine distance it is just hopeless). The lemmas 'beat' or 'beats' should ideally be the ones selected as being the &quot;closest&quot; in all these cases. Yet the algorithm does what it does.</p>
<p>Perhaps I haven't looked hard enough, but with all the work in NLP, I'm surprised there aren't other algorithms that could do better in this kind of scenario. I know this deviates a little bit from the main point in the original question but if anybody can give me some useful advice, I would greatly appreciate it.</p>
",Multilingual Language Processing & Language Identification,efficient way identify text similarity item large list string python following piece code achieves result trying achieve list string called lemma contains accepted form specific class word list called form contains lot spelling variation word found large amount text different period different dialect specific language one word form want get string lemma closest match script said seems work well test list constructed problem though use real list rather large take forever produce result fact stop execution program wa taking already two hour computer wa becoming slow anything else could make efficient would modify code using python tool library make faster thanks advance edit end solution worked best wa integration kyle f hartzenberg proposal jamie b suggestion using joblib parallelize see comment code though wa huge improvement everything tried besides test short list example ran actual list containing around string processing time wa minute pretty sure could improved one might look way using kind vectorization someone suggested using array numpy ai oriented library time quite manageable still problem efficiency mention comment jqurious explain detail running script test list one get result like following linguistic point view english speaker would know pair word related ok unless know history language know used productive prefix trying script determine would appropriate lemma dictionary form representative word variant particular word found text corpus diachronic corpus containing many text many different author many different dialect language writen period century u could often used instead v instead h al often missing word spelt h even text author variation huge yet even modern speaker languate usually detect whether word related quite easily course speaker language knowledgeable word structure morphology immediately see instance uermell ssima related vermell despite fact lot character different using kyle suggestion actual list got result like following even know language medieval catalan case anybody interested see wrong using algorithm like levenshtein cosine distance hopeless lemma beat beat ideally one selected closest case yet algorithm doe doe perhaps looked hard enough work nlp surprised algorithm could better kind scenario know deviate little bit main point original question anybody give useful advice would greatly appreciate
How to generate a merge file and a vocab file in NLP field,"<p>I want to use the Megatron framework for Chinese NLP pre-training tasks. Currently, I have Chinese corpus resources and a vocab.txt file. However, for most frameworks, it seems that vocab.json and merge.txt are needed. Can I generate the above two files from Chinese corpus resources? If so, how can I generate them? Sorry, I haven't found a particularly suitable tutorial on Google.</p>
<p>I have tried to search for relevant tutorials and answers through Google, but have not found suitable results. I am hoping to obtain a method for generating a vocab file and merge file.</p>
",Multilingual Language Processing & Language Identification,generate merge file vocab file nlp field want use megatron framework chinese nlp pre training task currently chinese corpus resource vocab txt file however framework seems vocab json merge txt needed generate two file chinese corpus resource generate sorry found particularly suitable tutorial google tried search relevant tutorial answer google found suitable result hoping obtain method generating vocab file merge file
How to resolve No module named &#39;nltk.translate.meteor_score&#39; on google colab,"<p><a href=""https://i.sstatic.net/0FUZQ.png"" rel=""nofollow noreferrer"">No module named 'nltk.translate.meteor_score'</a></p>
",Multilingual Language Processing & Language Identification,resolve module named nltk translate meteor score google colab module named nltk translate meteor score
Using spaCy to Lemmatize Korean?,"<p>I'm trying to lemmatize some Korean sentences using some pretrained models. I'm very much a beginner with this sort of thing so I'm sure I could be missing something obvious but following examples I found for other languages and the Korean model's docs (<a href=""https://spacy.io/models/ko#ko_core_news_sm"" rel=""nofollow noreferrer"">https://spacy.io/models/ko#ko_core_news_sm</a>) I tried:</p>
<pre><code># loading model
nlp = spacy.load(&quot;ko_core_news_sm&quot;)

# test on first sentence 
doc = nlp(sentences[0])
print(doc)
for token in doc:
print(token.lemma_)
</code></pre>
<p>I would expect that it would provide the base form of the word, like if it were English for example something like apples-&gt;apple.</p>
<p>For the Korean however, the output of this code is providing WORD+affix. I cannot post with Korean due to anti-spam measures but basically it appears to be rather than providing the lemma simply telling me how the word is composed. Am I doing something wrong is this simply how the model works? Is there any way to get the actual base word? Sorry if it's obvious and thanks everyone for the help.</p>
",Multilingual Language Processing & Language Identification,using spacy lemmatize korean trying lemmatize korean sentence using pretrained model much beginner sort thing sure could missing something obvious following example found language korean model doc tried would expect would provide base form word like english example something like apple apple korean however output code providing word affix post korean due anti spam measure basically appears rather providing lemma simply telling word composed something wrong simply model work way get actual base word sorry obvious thanks everyone help
How to Properly Fine-Tune Translational Transformer Models,"<p>I want to fine-tune Helsinki-NLP/opus-mt-en-ar for english to arabic translation, but the results are so bad, although I previously used Hugging Face Autotrain tool on smaller dataset, and got better results. My code is:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

import pandas as pd

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-ar&quot;)


model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-ar&quot;)

dff = pd.read_csv('/content/data.csv')



&quot;&quot;&quot;
# Formatting Data
&quot;&quot;&quot;

source = dff['eng_Latn'].values.tolist()
target = dff['arb_Arab'].values.tolist()


from sklearn.model_selection import train_test_split
max = 512
X_train, X_val, y_train, y_val = train_test_split(source, target, test_size=0.2)
X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=max, return_tensors=&quot;pt&quot;)
y_train_tokenized = tokenizer(y_train, padding=True, truncation=True, max_length=max, return_tensors=&quot;pt&quot;)
X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=max, return_tensors=&quot;pt&quot;)
y_val_tokenized = tokenizer(y_val, padding=True, truncation=True, max_length=max, return_tensors=&quot;pt&quot;)

import torch
class ForDataset(torch.utils.data.Dataset):
    def __init__(self, inputs, targets):
        self.inputs = inputs
        self.targets = targets
    
    def __len__(self):
        return len(self.targets)
    
    def __getitem__(self, index):
        input_ids = torch.tensor(self.inputs[&quot;input_ids&quot;][index]).squeeze()
        target_ids = torch.tensor(self.targets[&quot;input_ids&quot;][index]).squeeze()
        
        return {&quot;input_ids&quot;: input_ids, &quot;labels&quot;: target_ids}

train_dataset = ForDataset(X_train_tokenized, y_train_tokenized)
test_dataset = ForDataset(X_val_tokenized, y_val_tokenized)


from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=&quot;pt&quot;)

&quot;&quot;&quot;# Metrics&quot;&quot;&quot;

import evaluate

metric = evaluate.load(&quot;sacrebleu&quot;)

import numpy as np


def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {&quot;bleu&quot;: result[&quot;score&quot;]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result[&quot;gen_len&quot;] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result

&quot;&quot;&quot;# Training&quot;&quot;&quot;

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer



training_args = Seq2SeqTrainingArguments(

    output_dir=&quot;mymodel&quot;,

    evaluation_strategy=&quot;epoch&quot;,

    save_strategy = 'epoch',

    learning_rate=2e-5,

    per_device_train_batch_size=16,

    per_device_eval_batch_size=16,

    weight_decay=0.01,

    save_total_limit=3,

    num_train_epochs=20,

    predict_with_generate=True,

    load_best_model_at_end=True

)

trainer = Seq2SeqTrainer(

    model=model,

    args=training_args,

    train_dataset=train_dataset,

    eval_dataset=test_dataset,

    tokenizer=tokenizer,

    data_collator=data_collator,

    compute_metrics=compute_metrics,

)

trainer.train()

&quot;&quot;&quot; # Testing &quot;&quot;&quot;

trainer.save_model('finalmodel')

from transformers import pipeline

mymodel = AutoModelForSeq2SeqLM.from_pretrained(&quot;./finalmodel&quot;)

translator =  pipeline ('translation' , model = mymodel, tokenizer=tokenizer)

print (translator('suddenly, we were attacked '))

&gt;&gt;&gt; [Some Bad translation]

</code></pre>
<p>A snippet of the metrics, where bleu ends up 0 after 10 epochs!:
<a href=""https://i.sstatic.net/vyRsF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vyRsF.png"" alt=""enter image description here"" /></a></p>
<p>The csv data looks like this:
<a href=""https://i.sstatic.net/O6jRn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/O6jRn.png"" alt=""enter image description here"" /></a></p>
<p>I'm suspecting issues with the tokenizer maybe? Or is it the data not well formatted? Could it be the training arguments?</p>
<p>I'm new to NLP, thus, I copied a lot of code from SO and Hugging Face.</p>
",Multilingual Language Processing & Language Identification,properly fine tune translational transformer model want fine tune helsinki nlp opus mt en ar english arabic translation result bad although previously used hugging face autotrain tool smaller dataset got better result code snippet metric bleu end epoch csv data look like suspecting issue tokenizer maybe data well formatted could training argument new nlp thus copied lot code hugging face
Got the &quot;Unable to load vocabulary from file.&quot; while using pipelines,"<p>I have been trying to use the &quot;csebuetnlp/mT5_multilingual_XLSum&quot; model for summarization purposes.<br />
The code I tried is listed as below:</p>
<pre class=""lang-py prettyprint-override""><code>
!pip install transformers
!pip install sentencepiece
import transformers
text_example = &quot;&quot;&quot; 
En d√º≈ü√ºk emekli aylƒ±ƒüƒ±nƒ±n 5 bin 500 liradan 7 bin 500 liraya y√ºkseltilmesi i√ßin TBMM'de yasal d√ºzenleme yapƒ±lacak; ardƒ±ndan zamlƒ± aylƒ±klarƒ±n nisan ayƒ±nda hesaplara aktarƒ±lmasƒ± planlanƒ±yor.

AKP'li Cumhurba≈ükanƒ± Recep Tayyip Erdoƒüan'ƒ±n d√ºn katƒ±ldƒ±ƒüƒ± televizyon programƒ±nda en d√º≈ü√ºk emekli aylƒ±ƒüƒ±nƒ±n 7 bin 500 liraya y√ºkseltildiƒüi y√∂n√ºndeki a√ßƒ±klamasƒ±, emekliler tarafƒ±ndan memnuniyetle kar≈üƒ±landƒ±.

Bu m√ºjdenin ardƒ±ndan g√∂zler, s√∂z konusu kararƒ±n uygulanmasƒ± i√ßin TBMM'de yapƒ±lacak yasal d√ºzenlemeye √ßevrildi.

En d√º≈ü√ºk emekli aylƒ±ƒüƒ±nƒ±n 7 bin 500 liraya y√ºkseltilmesi y√∂n√ºndeki kararƒ±n ilerleyen g√ºnlerde yasala≈ümasƒ± ve zamlƒ± aylƒ±klarƒ±n nisan ayƒ±nda hesaplara yatƒ±rƒ±lmasƒ± bekleniyor.

S√∂z konusu artƒ±≈ütan, EYT d√ºzenlemesiyle emekli olanlarla beraber emeklilerin yarƒ±sƒ±ndan fazlasƒ± yararlanacak.
&quot;&quot;&quot;

from transformers import pipeline
summarizer = pipeline(&quot;summarization&quot;, model= &quot;csebuetnlp/mT5_multilingual_XLSum&quot;)
summarizer(text_example)
</code></pre>
<p>The output I got is listed as below:</p>
<pre class=""lang-tsx prettyprint-override""><code>Requirement already satisfied: transformers in d:\anaaac\lib\site-packages (4.24.0)
Requirement already satisfied: regex!=2019.12.17 in d:\anaaac\lib\site-packages (from transformers) (2023.3.23)
Requirement already satisfied: pyyaml&gt;=5.1 in d:\anaaac\lib\site-packages (from transformers) (6.0)
Requirement already satisfied: requests in d:\anaaac\lib\site-packages (from transformers) (2.28.1)
Requirement already satisfied: tqdm&gt;=4.27 in d:\anaaac\lib\site-packages (from transformers) (4.65.0)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.10.0 in d:\anaaac\lib\site-packages (from transformers) (0.11.0)
Requirement already satisfied: packaging&gt;=20.0 in d:\anaaac\lib\site-packages (from transformers) (23.0)
Requirement already satisfied: tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 in d:\anaaac\lib\site-packages (from transformers) (0.13.2)
Requirement already satisfied: filelock in d:\anaaac\lib\site-packages (from transformers) (3.9.0)
Requirement already satisfied: numpy&gt;=1.17 in d:\anaaac\lib\site-packages (from transformers) (1.24.2)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in d:\anaaac\lib\site-packages (from huggingface-hub&lt;1.0,&gt;=0.10.0-&gt;transformers) (4.4.0)
Requirement already satisfied: colorama in d:\anaaac\lib\site-packages (from tqdm&gt;=4.27-&gt;transformers) (0.4.6)
Requirement already satisfied: certifi&gt;=2017.4.17 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (2022.12.7)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (2.0.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (3.4)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in d:\anaaac\lib\site-packages (from requests-&gt;transformers) (1.26.14)
Requirement already satisfied: sentencepiece in d:\anaaac\lib\site-packages (0.1.97)
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1932, in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)
   1931 try:
-&gt; 1932     tokenizer = cls(*init_inputs, **init_kwargs)
   1933 except OSError:

File D:\anaaac\lib\site-packages\transformers\models\t5\tokenization_t5.py:155, in T5Tokenizer.__init__(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, **kwargs)
    154 self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)
--&gt; 155 self.sp_model.Load(vocab_file)

File D:\anaaac\lib\site-packages\sentencepiece\__init__.py:905, in SentencePieceProcessor.Load(self, model_file, model_proto)
    904   return self.LoadFromSerializedProto(model_proto)
--&gt; 905 return self.LoadFromFile(model_file)

File D:\anaaac\lib\site-packages\sentencepiece\__init__.py:310, in SentencePieceProcessor.LoadFromFile(self, arg)
    309 def LoadFromFile(self, arg):
--&gt; 310     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)

OSError: Not found: &quot;C:\Users\ist/.cache\huggingface\hub\models--csebuetnlp--mT5_multilingual_XLSum\snapshots\2437a524effdbadc327ced84595508f1e32025b3\spiece.model&quot;: No such file or directory Error #2

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
Cell In[4], line 17
      4 text_example = &quot;&quot;&quot; 
      5 En d√º≈ü√ºk emekli aylƒ±ƒüƒ±nƒ±n 5 bin 500 liradan 7 bin 500 liraya y√ºkseltilmesi i√ßin TBMM'de yasal d√ºzenleme yapƒ±lacak; ardƒ±ndan zamlƒ± aylƒ±klarƒ±n nisan ayƒ±nda hesaplara aktarƒ±lmasƒ± planlanƒ±yor.
      6 
   (...)
     13 S√∂z konusu artƒ±≈ütan, EYT d√ºzenlemesiyle emekli olanlarla beraber emeklilerin yarƒ±sƒ±ndan fazlasƒ± yararlanacak.
     14 &quot;&quot;&quot;
     16 from transformers import pipeline
---&gt; 17 summarizer = pipeline(&quot;summarization&quot;, model= &quot;csebuetnlp/mT5_multilingual_XLSum&quot;)
     18 summarizer(text_example)

File D:\anaaac\lib\site-packages\transformers\pipelines\__init__.py:801, in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    798             tokenizer_identifier = tokenizer
    799             tokenizer_kwargs = model_kwargs
--&gt; 801         tokenizer = AutoTokenizer.from_pretrained(
    802             tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs
    803         )
    805 if load_feature_extractor:
    806     # Try to infer feature extractor from model or config name (if provided as str)
    807     if feature_extractor is None:

File D:\anaaac\lib\site-packages\transformers\models\auto\tokenization_auto.py:619, in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    615     if tokenizer_class is None:
    616         raise ValueError(
    617             f&quot;Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.&quot;
    618         )
--&gt; 619     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    621 # Otherwise we have to be creative.
    622 # if model is an encoder decoder, the encoder tokenizer class is used by default
    623 if isinstance(config, EncoderDecoderConfig):

File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1777, in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
   1774     else:
   1775         logger.info(f&quot;loading file {file_path} from cache at {resolved_vocab_files[file_id]}&quot;)
-&gt; 1777 return cls._from_pretrained(
   1778     resolved_vocab_files,
   1779     pretrained_model_name_or_path,
   1780     init_configuration,
   1781     *init_inputs,
   1782     use_auth_token=use_auth_token,
   1783     cache_dir=cache_dir,
   1784     local_files_only=local_files_only,
   1785     _commit_hash=commit_hash,
   1786     **kwargs,
   1787 )

File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1807, in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)
   1805 has_tokenizer_file = resolved_vocab_files.get(&quot;tokenizer_file&quot;, None) is not None
   1806 if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class is not None:
-&gt; 1807     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
   1808         copy.deepcopy(resolved_vocab_files),
   1809         pretrained_model_name_or_path,
   1810         copy.deepcopy(init_configuration),
   1811         *init_inputs,
   1812         use_auth_token=use_auth_token,
   1813         cache_dir=cache_dir,
   1814         local_files_only=local_files_only,
   1815         _commit_hash=_commit_hash,
   1816         **(copy.deepcopy(kwargs)),
   1817     )
   1818 else:
   1819     slow_tokenizer = None

File D:\anaaac\lib\site-packages\transformers\tokenization_utils_base.py:1934, in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)
   1932     tokenizer = cls(*init_inputs, **init_kwargs)
   1933 except OSError:
-&gt; 1934     raise OSError(
   1935         &quot;Unable to load vocabulary from file. &quot;
   1936         &quot;Please check that the provided vocabulary is accessible and not corrupted.&quot;
   1937     )
   1939 # Save inputs and kwargs for saving and re-loading with ``save_pretrained``
   1940 # Removed: Now done at the base class level
   1941 # tokenizer.init_inputs = init_inputs
   1942 # tokenizer.init_kwargs = init_kwargs
   1943 
   1944 # If there is a complementary special token map, load it
   1945 special_tokens_map_file = resolved_vocab_files.pop(&quot;special_tokens_map_file&quot;, None)

OSError: Unable to load vocabulary from file. Please check that the provided vocabulary is accessible and not corrupted.
</code></pre>
<p>The text is about the <em>raise for the retirement wage in Turkey</em>.</p>
<p>This part of the output is really weird considering the <em>spiece.model</em> file exist in the exact same directory.</p>
<pre><code>OSError: Not found: &quot;C:\Users\ist/.cache\huggingface\hub\models--csebuetnlp--mT5_multilingual_XLSum\snapshots\2437a524effdbadc327ced84595508f1e32025b3\spiece.model&quot;: No such file or directory Error #2
</code></pre>
<p><strong>Version info:</strong></p>
<pre><code>Server Information:

You are using Jupyter Notebook.

The version of the notebook server is: **6.5.3**  
The server is running on this version of Python:

`Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]`

</code></pre>
<ol>
<li>I tried upgrading transformers sentencepiece, it didnt help.</li>
<li>I tried using the models named <code>sshleifer/distilbart-cnn-12-6</code> and <code>flax-community/t5-base-cnn-dm</code>. They both worked as expected but i need a multilingual model.</li>
<li>I tried running the same code in Google Colaboratory it worked as expected and the output is:</li>
</ol>
<pre class=""lang-tsx prettyprint-override""><code>Downloading (‚Ä¶)lve/main/config.json: 100%
730/730 [00:00&lt;00:00, 14.0kB/s]
Downloading pytorch_model.bin: 100%
2.33G/2.33G [00:23&lt;00:00, 109MB/s]
Downloading (‚Ä¶)okenizer_config.json: 100%
375/375 [00:00&lt;00:00, 9.51kB/s]
Downloading spiece.model: 100%
4.31M/4.31M [00:00&lt;00:00, 14.0MB/s]
Downloading (‚Ä¶)cial_tokens_map.json: 100%
65.0/65.0 [00:00&lt;00:00, 2.70kB/s]
/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[{'summary_text': &quot;Cumhurba≈ükanƒ± Recep Tayyip Erdoƒüan'ƒ±n d√ºn a√ßƒ±kladƒ±ƒüƒ± en d√º≈ü√ºk emekli aylƒ±ƒüƒ±nƒ±n 7 bin 500 liraya y√ºkseltilmesi y√∂n√ºndeki a√ßƒ±klamasƒ±, emekliler tarafƒ±ndan memnuniyetle kar≈üƒ±landƒ±.&quot;}]
</code></pre>
<p>The output is not weird and the summary is understandable.</p>
<hr />
<p>I belive the error is more about sentencepiece than pipelines. I checked some similar issues in github, stackoverflow and some chineese forums. None of the issues helped. I need some help with the code. Thanks.</p>
",Multilingual Language Processing & Language Identification,got unable load vocabulary file using pipeline trying use csebuetnlp mt multilingual xlsum model summarization purpose code tried listed output got listed text raise retirement turkey part output really weird considering spiece model file exist exact directory version info tried upgrading transformer sentencepiece didnt help tried using model named worked expected need multilingual model tried running code google colaboratory worked expected output output weird summary understandable belive error sentencepiece pipeline checked similar issue github stackoverflow chineese forum none issue helped need help code thanks
Problem trimming Japanese string in java,"<p>I have the following string (japanese) ""„ÄÄ„É¶„Éº„Ç∂„ÉºÂêç"" , the first character is ""like"" whitespace but its number in unicode is 12288, so if I do ""„ÄÄ„É¶„Éº„Ç∂„ÉºÂêç"".trim() I get the same string (trim doesn't work).
If i do trim in c++ it works ok.
Does anyone know how to solve this issue in java?
Is there a special trim method for unicode?</p>
",Multilingual Language Processing & Language Identification,problem trimming japanese string java following string japanese first character like whitespace number unicode trim get string trim work trim c work ok doe anyone know solve issue java special trim method unicode
How to generate word embeddings in Portuguese using Gensim?,"<p>I have the following problem:</p>

<p>In English language my code generates successful word embeddings with Gensim, and similar phrases are close to each other considering cosine distance:</p>

<p>The angle between ""Response time and error measurement"" and ""Relation of user perceived response time to error measurement"" is very small, thus they are the most similar phrases in the set.</p>

<p><a href=""https://i.sstatic.net/Seuzy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Seuzy.png"" alt=""enter image description here""></a></p>

<p>However, when I use the same phrases in Portuguese, it doesn't work:</p>

<p><a href=""https://i.sstatic.net/0l2Sz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0l2Sz.png"" alt=""enter image description here""></a></p>

<p>My code as follows:</p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
import matplotlib.pyplot as plt
from gensim import corpora
documents = [""Interface m√°quina humana para aplica√ß√µes computacionais de laborat√≥rio abc"",
          ""Um levantamento da opini√£o do usu√°rio sobre o tempo de resposta do sistema inform√°tico"",
           ""O sistema de gerenciamento de interface do usu√°rio EPS"",
           ""Sistema e testes de engenharia de sistemas humanos de EPS"",
           ""Rela√ß√£o do tempo de resposta percebido pelo usu√°rio para a medi√ß√£o de erro"",
           ""A gera√ß√£o de √°rvores n√£o ordenadas bin√°rias aleat√≥rias"",
           ""O gr√°fico de interse√ß√£o dos caminhos nas √°rvores"",
           ""Gr√°fico de menores IV Largura de √°rvores e bem quase encomendado"",
           ""Gr√°ficos menores Uma pesquisa""]

stoplist = set('for a of the and to in on'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]
texts

from collections import defaultdict
frequency = defaultdict(int)

for text in texts:
    for token in text:
        frequency[token] += 1
frequency

from nltk import tokenize  
texts=[tokenize.word_tokenize(documents[i], language='portuguese') for i in range(0,len(documents))]

from pprint import pprint
pprint(texts)

dictionary = corpora.Dictionary(texts)
dictionary.save('/tmp/deerwester.dict')
print(dictionary)

print(dictionary.token2id)


# VECTOR
new_doc = ""Tempo de resposta e medi√ß√£o de erro""
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)

## VETOR OF PHRASES
corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  
print(corpus)

from gensim import corpora, models, similarities
tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model

### PHRASE COORDINATES
frase=tfidf[new_vec]
print(frase)

corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
    print(doc)

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lsi = lsi[corpus_tfidf]

lsi.print_topics(2)

## TEXT COORDINATES
todas=[]
for doc in corpus_lsi:
    todas.append(doc)
todas

from gensim import corpora, models, similarities
dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')
corpus = corpora.MmCorpus('/tmp/deerwester.mm')
print(corpus)

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

doc = new_doc
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
print(vec_lsi)

p=[]
for i in range(0,len(documents)):
    doc1 = documents[i]
    vec_bow2 = dictionary.doc2bow(doc1.lower().split())
    vec_lsi2 = lsi[vec_bow2]
    p.append(vec_lsi2)

p

index = similarities.MatrixSimilarity(lsi[corpus])

index.save('/tmp/deerwester.index')
index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')

sims = index[vec_lsi]
print(list(enumerate(sims)))

sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims) 

#################

import gensim
import numpy as np
import matplotlib.colors as colors
import matplotlib.cm as cmx
import matplotlib as mpl

matrix1 = gensim.matutils.corpus2dense(p, num_terms=2)
matrix3=matrix1.T
matrix3[0]
ss=[]
for i in range(0,9):
    ss.append(np.insert(matrix3[i],0,[0,0]))
matrix4=ss
matrix4

matrix2 = gensim.matutils.corpus2dense([vec_lsi], num_terms=2)
matrix2=np.insert(matrix2,0,[0,0])
matrix2

DATA=np.insert(matrix4,0,matrix2)
DATA=DATA.reshape(10,4)
DATA

names=np.array(documents)
names=np.insert(names,0,new_doc)
new_doc
cmap = plt.cm.jet

cNorm  = colors.Normalize(vmin=np.min(DATA[:,3])+.2, vmax=np.max(DATA[:,3]))

scalarMap = cmx.ScalarMappable(norm=cNorm,cmap=cmap)
len(DATA[:,1])

plt.subplots()
plt.figure(figsize=(12,9))
plt.scatter(matrix1[0],matrix1[1],s=60)
plt.scatter(matrix2[2],matrix2[3],color='r',s=95)
for idx in range(0,len(DATA[:,1])):
    colorVal = scalarMap.to_rgba(DATA[idx,3])
    plt.arrow(DATA[idx,0],
          DATA[idx,1], 
          DATA[idx,2], 
          DATA[idx,3], 
          color=colorVal,head_width=0.002, head_length=0.001)
for i,names in enumerate (names):
    plt.annotate(names, (DATA[i][2],DATA[i][3]),va='top')
plt.title(""PHRASE SIMILARITY - WORD2VEC with GENSIM library"")
plt.xlim(min(DATA[:,2]-.2),max(DATA[:,2]+1))
plt.ylim(min(DATA[:,3]-.2),max(DATA[:,3]+.3))
plt.show()
</code></pre>

<p>My question is: is there any additional set up for Gensim to generate proper word embeddings in Portuguese language or Gensim does not support this language?</p>
",Multilingual Language Processing & Language Identification,generate word embeddings portuguese using gensim following problem english language code generates successful word embeddings gensim similar phrase close considering cosine distance angle response time error measurement relation user perceived response time error measurement small thus similar phrase set however use phrase portuguese work code follows question additional set gensim generate proper word embeddings portuguese language gensim doe support language
How to properly split streaming text into paragraphs?,"<p>I am using okhttp subscribing to a <code>SSE</code>( <a href=""https://en.wikipedia.org/wiki/Server-sent_events"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Server-sent_events</a>) stream, the <code>onEvent</code> callback is use to receive new data from upstream and passed by <code>data</code> param as <code>String</code></p>
<p>It takes too long to wait for all data to arrive and is too frequent to send on every single char arriving, and I need to send proper data segment intermitently.</p>
<pre><code>// initiate an SSE request and receive new data by `onEvent`
eventSourceFactory.newEventSource(request, new EventSourceListener() {
    // onEvent called every time remote send new data, `data` is new data
    public void onEvent(EventSource eventSource, String id, String type, String data) {
        // pseudo code below
        String stringToSend = process(data, minSplitLength);
        if (!stringToSend.isEmpty()) {
            send(stringToSend);
        }
    }
})
</code></pre>
<p><strong>So I want to impl <code>process(String data, int minSplitLength);</code> method</strong></p>
<p><strong>It should properly store stream data and emits string <code>stringToSend</code>  with length larger than <code>minSplitLength</code> e.g. when a paragraph or a sentence is constructed from <code>data</code>s with length &gt;= <code>minSplitLength</code> or empty string</strong></p>
<p>The <code>data</code> param can contain any mainstream natural language, e.g. <strong>en-US</strong>, <strong>zh-CN</strong>, <strong>Russian</strong>, <strong>Arabic</strong>, <strong>ja-JP</strong> , etc</p>
<p>I can accept occasionally non-perfect splitting.</p>
<p>Any other ideas, for example split base both on text content and time is also welcome.</p>
<p>I want to impl this in Java.</p>
<p>Thanks</p>
",Multilingual Language Processing & Language Identification,properly split streaming text paragraph using okhttp subscribing stream callback use receive new data upstream passed param take long wait data arrive frequent send every single char arriving need send proper data segment intermitently want impl method properly store stream data emits string length larger e g paragraph sentence constructed length empty string param contain mainstream natural language e g en u zh cn russian arabic ja jp etc accept occasionally non perfect splitting idea example split base text content time also welcome want impl java thanks
Transformer is only predicting End of Sequence token during inference,"<h1>Question</h1>
<hr />
<p>I am trying to code a translator from English to Kannada (a South Indian language). The training seems to happen just fine. However, when trying to infer / translate a given English sentence to Kannada, I just get the <code>END_TOKEN</code> as the translation no matter what the actual English sentence input is. I need assistance to see why this is the case.</p>
<h1>Details</h1>
<hr />
<p>I have prodded different parts of the network, but have not seen anything unusual. The dataset I used is from <a href=""https://paperswithcode.com/paper/samanantar-the-largest-publicly-available"" rel=""nofollow noreferrer"">Samanatar</a>. This is a collection of sentence pairs of 11 Indian languages along with their English translations.The following is the code for the transformer stored in a file called <code>transformer.py</code></p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import math
from torch import nn
import torch.nn.functional as F

def get_device():
    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

def scaled_dot_product(q, k, v, mask=None):
    d_k = q.size()[-1]
    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)
    if mask is not None:
        scaled = scaled.permute(1, 0, 2, 3) + mask
        scaled = scaled.permute(1, 0, 2, 3)
    attention = F.softmax(scaled, dim=-1)
    values = torch.matmul(attention, v)
    return values, attention

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_sequence_length):
        super().__init__()
        self.max_sequence_length = max_sequence_length
        self.d_model = d_model

    def forward(self):
        even_i = torch.arange(0, self.d_model, 2).float()
        denominator = torch.pow(10000, even_i/self.d_model)
        position = (torch.arange(self.max_sequence_length)
                          .reshape(self.max_sequence_length, 1))
        even_PE = torch.sin(position / denominator)
        odd_PE = torch.cos(position / denominator)
        stacked = torch.stack([even_PE, odd_PE], dim=2)
        PE = torch.flatten(stacked, start_dim=1, end_dim=2)
        return PE

class SentenceEmbedding(nn.Module):
    &quot;For a given sentence, create an embedding&quot;
    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):
        super().__init__()
        self.vocab_size = len(language_to_index)
        self.max_sequence_length = max_sequence_length
        self.embedding = nn.Embedding(self.vocab_size, d_model)
        self.language_to_index = language_to_index
        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)
        self.dropout = nn.Dropout(p=0.1)
        self.START_TOKEN = START_TOKEN
        self.END_TOKEN = END_TOKEN
        self.PADDING_TOKEN = PADDING_TOKEN
    
    def batch_tokenize(self, batch, start_token, end_token):

        def tokenize(sentence, start_token, end_token):
            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]
            if start_token:
                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])
            if end_token:
                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])
            for _ in range(len(sentence_word_indicies), self.max_sequence_length):
                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])
            return torch.tensor(sentence_word_indicies)

        tokenized = []
        for sentence_num in range(len(batch)):
           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )
        tokenized = torch.stack(tokenized)
        return tokenized.to(get_device())
    
    def forward(self, x, start_token, end_token): # sentence
        x = self.batch_tokenize(x, start_token, end_token)
        x = self.embedding(x)
        pos = self.position_encoder().to(get_device())
        x = self.dropout(x + pos)
        return x


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.qkv_layer = nn.Linear(d_model , 3 * d_model)
        self.linear_layer = nn.Linear(d_model, d_model)
    
    def forward(self, x, mask):
        batch_size, sequence_length, d_model = x.size()
        qkv = self.qkv_layer(x)
        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)
        qkv = qkv.permute(0, 2, 1, 3)
        q, k, v = qkv.chunk(3, dim=-1)
        values, attention = scaled_dot_product(q, k, v, mask)
        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)
        out = self.linear_layer(values)
        return out


class LayerNormalization(nn.Module):
    def __init__(self, parameters_shape, eps=1e-5):
        super().__init__()
        self.parameters_shape=parameters_shape
        self.eps=eps
        self.gamma = nn.Parameter(torch.ones(parameters_shape))
        self.beta =  nn.Parameter(torch.zeros(parameters_shape))

    def forward(self, inputs):
        dims = [-(i + 1) for i in range(len(self.parameters_shape))]
        mean = inputs.mean(dim=dims, keepdim=True)
        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)
        std = (var + self.eps).sqrt()
        y = (inputs - mean) / std
        out = self.gamma * y + self.beta
        return out

  
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, hidden, drop_prob=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, hidden)
        self.linear2 = nn.Linear(hidden, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=drop_prob)

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x


class EncoderLayer(nn.Module):
    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):
        super(EncoderLayer, self).__init__()
        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
        self.norm1 = LayerNormalization(parameters_shape=[d_model])
        self.dropout1 = nn.Dropout(p=drop_prob)
        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)
        self.norm2 = LayerNormalization(parameters_shape=[d_model])
        self.dropout2 = nn.Dropout(p=drop_prob)

    def forward(self, x, self_attention_mask):
        residual_x = x
        x = self.attention(x, mask=self_attention_mask)
        x = self.dropout1(x)
        x = self.norm1(x + residual_x)
        residual_x = x
        x = self.ffn(x)
        x = self.dropout2(x)
        x = self.norm2(x + residual_x)
        return x
    
class SequentialEncoder(nn.Sequential):
    def forward(self, *inputs):
        x, self_attention_mask  = inputs
        for module in self._modules.values():
            x = module(x, self_attention_mask)
        return x

class Encoder(nn.Module):
    def __init__(self, 
                 d_model, 
                 ffn_hidden, 
                 num_heads, 
                 drop_prob, 
                 num_layers,
                 max_sequence_length,
                 language_to_index,
                 START_TOKEN,
                 END_TOKEN, 
                 PADDING_TOKEN):
        super().__init__()
        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)
                                      for _ in range(num_layers)])

    def forward(self, x, self_attention_mask, start_token, end_token):
        x = self.sentence_embedding(x, start_token, end_token)
        x = self.layers(x, self_attention_mask)
        return x


class MultiHeadCrossAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.kv_layer = nn.Linear(d_model , 2 * d_model)
        self.q_layer = nn.Linear(d_model , d_model)
        self.linear_layer = nn.Linear(d_model, d_model)
    
    def forward(self, x, y, mask):
        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention
        kv = self.kv_layer(x)
        q = self.q_layer(y)
        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)
        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)
        kv = kv.permute(0, 2, 1, 3)
        q = q.permute(0, 2, 1, 3)
        k, v = kv.chunk(2, dim=-1)
        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!
        values = values.reshape(batch_size, sequence_length, d_model)
        out = self.linear_layer(values)
        return out


class DecoderLayer(nn.Module):
    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):
        super(DecoderLayer, self).__init__()
        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])
        self.dropout1 = nn.Dropout(p=drop_prob)

        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)
        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])
        self.dropout2 = nn.Dropout(p=drop_prob)

        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)
        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])
        self.dropout3 = nn.Dropout(p=drop_prob)

    def forward(self, x, y, self_attention_mask, cross_attention_mask):
        _y = y
        y = self.self_attention(y, mask=self_attention_mask)
        y = self.dropout1(y)
        y = self.layer_norm1(y + _y)

        _y = y
        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)
        y = self.dropout2(y)
        y = self.layer_norm2(y + _y)

        _y = y
        y = self.ffn(y)
        y = self.dropout3(y)
        y = self.layer_norm3(y + _y)
        return y


class SequentialDecoder(nn.Sequential):
    def forward(self, *inputs):
        x, y, self_attention_mask, cross_attention_mask = inputs
        for module in self._modules.values():
            y = module(x, y, self_attention_mask, cross_attention_mask)
        return y

class Decoder(nn.Module):
    def __init__(self, 
                 d_model, 
                 ffn_hidden, 
                 num_heads, 
                 drop_prob, 
                 num_layers,
                 max_sequence_length,
                 language_to_index,
                 START_TOKEN,
                 END_TOKEN, 
                 PADDING_TOKEN):
        super().__init__()
        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])

    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):
        y = self.sentence_embedding(y, start_token, end_token)
        y = self.layers(x, y, self_attention_mask, cross_attention_mask)
        return y


class Transformer(nn.Module):
    def __init__(self, 
                d_model, 
                ffn_hidden, 
                num_heads, 
                drop_prob, 
                num_layers,
                max_sequence_length, 
                kn_vocab_size,
                english_to_index,
                kannada_to_index,
                START_TOKEN, 
                END_TOKEN, 
                PADDING_TOKEN
                ):
        super().__init__()
        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.linear = nn.Linear(d_model, kn_vocab_size)
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

    def forward(self, 
                x, 
                y, 
                encoder_self_attention_mask, 
                decoder_self_attention_mask, 
                decoder_cross_attention_mask,
                enc_start_token=False,
                enc_end_token=False,
                dec_start_token=False, # We should make this true
                dec_end_token=False): # x, y are batch of sentences
        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)
        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)
        out = self.linear(out)
        return out
</code></pre>
<p>The following is the code to get the dataset and initialize the model.</p>
<pre class=""lang-py prettyprint-override""><code>from transformer import Transformer
english_file = 'drive/MyDrive/translation_en_kn/train.en'
kannada_file = 'drive/MyDrive/translation_en_kn/train.kn'

def is_valid_tokens(sentence, vocab):
    for token in list(set(sentence)):
        if token not in vocab:
            return False
    return True

def is_valid_length(sentence, max_sequence_length):
    return len(list(sentence)) &lt; (max_sequence_length - 1) # need to re-add the end token so leaving 1 space


# Generated this by filtering Appendix code

START_TOKEN = '&lt;START&gt;'
PADDING_TOKEN = '&lt;PADDING&gt;'
END_TOKEN = '&lt;END&gt;'

kannada_vocabulary = [START_TOKEN, ' ', '!', '&quot;', '#', '$', '%', '&amp;', &quot;'&quot;, '(', ')', '*', '+', ',', '-', '.', '/', 
                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '&lt;', '=', '&gt;', '?', 'Àå', 
                      '‡§Å', '‡∞Ü', '‡∞á', '‡∞æ', '‡∞ø', '‡±Ä', '‡±Å', '‡±Ç', 
                      '‡≤Ö', '‡≤Ü', '‡≤á', '‡≤à', '‡≤â', '‡≤ä', '‡≤ã', '‡≥†', '‡≤å', '‡≤é', '‡≤è', '‡≤ê', '‡≤í', '‡≤ì', '‡≤î', 
                      '‡≤ï', '‡≤ñ', '‡≤ó', '‡≤ò', '‡≤ô', 
                      '‡≤ö', '‡≤õ', '‡≤ú', '‡≤ù', '‡≤û', 
                      '‡≤ü', '‡≤†', '‡≤°', '‡≤¢', '‡≤£', 
                      '‡≤§', '‡≤•', '‡≤¶', '‡≤ß', '‡≤®', 
                      '‡≤™', '‡≤´', '‡≤¨', '‡≤≠', '‡≤Æ', 
                      '‡≤Ø', '‡≤∞', '‡≤±', '‡≤≤', '‡≤≥', '‡≤µ', '‡≤∂', '‡≤∑', '‡≤∏', '‡≤π', 
                      '‡≤º', '‡≤Ω', '‡≤æ', '‡≤ø', '‡≥Ä', '‡≥Å', '‡≥Ç', '‡≥É', '‡≥Ñ', '‡≥Ü', '‡≥á', '‡≥à', '‡≥ä', '‡≥ã', '‡≥å', '‡≥ç', '‡≥ï', '‡≥ñ', '‡≥û', '‡≥£', '‡≤Ç', '‡≤É', 
                      '‡≥¶', '‡≥ß', '‡≥®', '‡≥©', '‡≥™', '‡≥´', '‡≥¨', '‡≥≠', '‡≥Æ', '‡≥Ø', PADDING_TOKEN, END_TOKEN]

english_vocabulary = [START_TOKEN, ' ', '!', '&quot;', '#', '$', '%', '&amp;', &quot;'&quot;, '(', ')', '*', '+', ',', '-', '.', '/', 
                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
                        ':', '&lt;', '=', '&gt;', '?', '@', 
                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 
                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 
                        'Y', 'Z',
                        '[', '\\', ']', '^', '_', '`', 
                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 
                        'y', 'z', 
                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]

index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}
kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}
index_to_english = {k:v for k,v in enumerate(english_vocabulary)}
english_to_index = {v:k for k,v in enumerate(english_vocabulary)}

with open(english_file, 'r') as file:
    english_sentences = file.readlines()
with open(kannada_file, 'r') as file:
    kannada_sentences = file.readlines()

# Limit Number of sentences
TOTAL_SENTENCES = 100000
english_sentences = english_sentences[:TOTAL_SENTENCES]
kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]
english_sentences = [sentence.rstrip('\n') for sentence in english_sentences]
kannada_sentences = [sentence.rstrip('\n') for sentence in kannada_sentences]

max_sequence_length = 200
valid_sentence_indicies = []
for index in range(len(kannada_sentences)):
    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]
    if is_valid_length(kannada_sentence, max_sequence_length) \
      and is_valid_length(english_sentence, max_sequence_length) \
      and is_valid_tokens(kannada_sentence, kannada_vocabulary):
        valid_sentence_indicies.append(index)

kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]
english_sentences = [english_sentences[i] for i in valid_sentence_indicies]

d_model = 512
batch_size = 30
ffn_hidden = 2048
num_heads = 8
drop_prob = 0.1
num_layers = 1
max_sequence_length = 200
kn_vocab_size = len(kannada_vocabulary)

transformer = Transformer(d_model, 
                          ffn_hidden,
                          num_heads, 
                          drop_prob, 
                          num_layers, 
                          max_sequence_length,
                          kn_vocab_size,
                          english_to_index,
                          kannada_to_index,
                          START_TOKEN, 
                          END_TOKEN, 
                          PADDING_TOKEN)
</code></pre>
<p>We then create the dataset loader</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):

    def __init__(self, english_sentences, kannada_sentences):
        self.english_sentences = english_sentences
        self.kannada_sentences = kannada_sentences

    def __len__(self):
        return len(self.english_sentences)

    def __getitem__(self, idx):
        return self.english_sentences[idx], self.kannada_sentences[idx]


criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN],
                    reduction='none')

# When computing the loss, we are ignoring cases when the label is the padding token
for params in transformer.parameters():
    if params.dim() &gt; 1:
        nn.init.xavier_uniform_(params)

optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
</code></pre>
<p>Here is the function to create the look ahead and padding masks for the encoder and decoder. I use <code>-1e9</code> instead of negative infinity so the softmax performed during scaled dot product attention is stable.</p>
<pre class=""lang-py prettyprint-override""><code>
NEG_INFTY = -1e9

def create_masks(eng_batch, kn_batch):
    num_sentences = len(eng_batch)
    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)
    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)
    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)
    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)
    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)

    for idx in range(num_sentences):
      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])
      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)
      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)
      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True
      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True
      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True
      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True
      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True
      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True

    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)
    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)
    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)
    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask
</code></pre>
<p>Here is the trainer code. Note for every 100 iterations, we print an example English Sentence from the current batch, it's actual Kannada Translation and the corresponding prediction the model made at the time.</p>
<pre class=""lang-py prettyprint-override""><code>transformer.train()
transformer.to(device)
total_loss = 0
num_epochs = 10

for epoch in range(num_epochs):
    print(f&quot;Epoch {epoch}&quot;)
    iterator = iter(train_loader)
    for batch_num, batch in enumerate(iterator):
        eng_batch, kn_batch = batch
        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)
        optim.zero_grad()
        kn_predictions = transformer(eng_batch,
                                     kn_batch,
                                     encoder_self_attention_mask.to(device), 
                                     decoder_self_attention_mask.to(device), 
                                     decoder_cross_attention_mask.to(device),
                                     enc_start_token=False,
                                     enc_end_token=False,
                                     dec_start_token=True,
                                     dec_end_token=True)
        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)
        loss = criterian(
            kn_predictions.view(-1, kn_vocab_size).to(device),
            labels.view(-1).to(device)
        ).to(device)
        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)
        loss = loss.sum() / valid_indicies.sum()
        loss.backward()
        optim.step()
        #train_losses.append(loss.item())
        if batch_num % 100 == 0:
            print(f&quot;Iteration {batch_num} : {loss.item()}&quot;)
            print(f&quot;English: {eng_batch[0]}&quot;)
            print(f&quot;Kannada Translation: {kn_batch[0]}&quot;)
            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)
            predicted_sentence = &quot;&quot;
            for idx in kn_sentence_predicted:
              if idx == kannada_to_index[END_TOKEN]:
                break
              predicted_sentence += index_to_kannada[idx.item()]
            print(f&quot;Kannada Prediction: {predicted_sentence}&quot;)
            print(&quot;-------------------------------------------&quot;)
</code></pre>
<p>You'll notice the the loss certainly decreases and the prediction though not amazing, are starting to actually take shape. I ran this for about 2,000 iterations for now.</p>
<pre><code>Epoch 0
Iteration 0 : 5.444823265075684
English: Hes a scientist.
Kannada Translation: ‡≤á‡≤µ‡≤∞‡≥Å ‡≤∏‡≤Ç‡≤∂‡≥ã‡≤ß‡≤ï ‡≤∏‡≥ç‡≤µ‡≤≠‡≤æ‡≤µ‡≤¶‡≤µ‡≤∞‡≥Å.
Kannada Prediction: ‡≤¨‡§Å‡≤ó‡≤ó‡≤ó‡≤ó‡≤ó‡≥´‡≤ó‡≥´‡±Å :‡≥¨‡≥¨‡≥¨‡≥¨‡≥¨‡≤ó‡≥É‡≤ó‡≤•‡≤• ‡≥à‡≥à$$‡≥†‡≥†‡≥†‡≥†‡≥†‡≥†‡≥†‡≥†‡≥†‡≥†‡≥†‡≤∞‡≥†‡≥†‡≥†‡±Ä‡≥†‡≤∞‡≥†‡≥†‡≥†‡≥†---+‡≤¨+++‡≤¨‡≤¨‡≤è+‡≤¨‡≤¨‡≤ø+‡≥Æ‡≤¨‡≥ß‡≥Æ‡≤≥ +‡≤≥+‡∞á‡∞á‡§Å‡≥ï‡≥å‡≥å‡∞á‡∞á‡∞á‡≥å‡≤ö‡≤•‡≥ç‡∞á‡≤ì‡≥´‡≤£‡≥ï4‡∞á‡≥Ä‡∞á‡∞á‡≥´‡≤î‡≤åÀå‡∞æ‡≤ó‡≤∏‡≤ú‡≤∂‡≤∂‡∞æ‡≤∏‡≤∏‡≤¶‡≤∂+‡≤∑‡∞æ‡∞æ‡≤∏‡≤∑Àå‡≥¶‡≥´‡≤∑‡≤™‡≤∑‡≥≠‡≥û#‡≥ß‡§Å?‡≤∑‡≤∑‡≤ö‡≤•‡≤£‡≤•‡≤•?‡≤∑?‡≤•‡≤•‡≤ö‡≤ö‡≤•‡≤ö%‡≤ö‡≤ö*‡≤¨‡≤´‡≤ó‡≤ó‡≤¨Àå‡≤ó‡≥ß&amp;‡≤ó‡≤¨‡≤ó‡≤¨‡≥Æ‡≥Æ.‡≤ó‡≤óÀå‡≤ó‡≤ó‡≤ó‡≤ó‡≤ó‡≤å%%‡≤∑‡≤∑‡≤∑‡≤ö‡≤∑‡≤ö?+0‡≤£++‡≥ã‡≥ã‡≤ó‡≤ó‡≤ò‡≤ö‡≥ã&lt;‡≤á‡≤ö
-------------------------------------------
Iteration 100 : 3.5337600708007812
English: She ate it.
Kannada Translation: ‡≤Ö‡≤µ‡≤≥‡≥Å ‡≤Ö‡≤µ‡≤®‡≤ø‡≤ó‡≥Ü ‡≤ä‡≤ü ‡≤π‡≤æ‡≤ï‡≤ø‡≤¶‡≤≥‡≥Ç.
Kannada Prediction: ‡≤Ü‡≤¶‡≥ç       ‡≥ç            ‡≥ç   ‡≥ç‡≥ç‡≥ç  ‡≥ç     
-------------------------------------------
Iteration 200 : 3.3461103439331055
English: Caste and religion were unknown.
Kannada Translation: ‡≤ú‡≤æ‡≤§‡≤ø, ‡≤¨‡≥á‡≤ß ‡≤é‡≤Ç‡≤¨‡≥Å‡≤¶‡≥á ‡≤ó‡≥ä‡≤§‡≥ç‡≤§‡≤ø‡≤∞‡≤≤‡≤ø‡≤≤‡≥ç‡≤≤.
Kannada Prediction: ‡≤á‡≤¶‡≤∞‡≥ç  ‡≥Ü  ‡≤ø   ‡≤ø    ‡≤ø ‡≥ç‡≥ç‡≥ç ‡≥ç
-------------------------------------------
Iteration 300 : 3.1086130142211914
English: Seeing this, ruler was elated and told his son that the strength of the rabbit is due to the valour of the region's citizenry.
Kannada Translation: ‡≤á‡≤¶‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≥ã‡≤°‡≤ø, ‡≤Ü‡≤°‡≤≥‡≤ø‡≤§‡≤ó‡≤æ‡≤∞‡≤®‡≥Å ‡≤â‡≤§‡≥ç‡≤∏‡≤æ‡≤π‡≤¶‡≤ø‡≤Ç‡≤¶ ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Æ‡≥ä‡≤≤‡≤¶ ‡≤¨‡≤≤‡≤µ‡≥Å ‡≤™‡≥ç‡≤∞‡≤¶‡≥á‡≤∂‡≤¶ ‡≤®‡≤æ‡≤ó‡≤∞‡≤ø‡≤ï‡≤∞ ‡≤∂‡≥å‡≤∞‡≥ç‡≤Ø‡≤¶ ‡≤ï‡≤æ‡≤∞‡≤£ ‡≤é‡≤Ç‡≤¶‡≥Å ‡≤§‡≤®‡≥ç‡≤® ‡≤Æ‡≤ó‡≤®‡≤ø‡≤ó‡≥Ü ‡≤§‡≤ø‡≤≥‡≤ø‡≤∏‡≤ø‡≤¶‡≤®‡≥Å.
Kannada Prediction: ‡≤Ü‡≤¶‡≥ç‡≥Å‡≤¶  ‡≤∏‡≥ç‡≤¶   ‡≤Æ ‡≤ø‡≤∞ ‡≥ç‡≥ç‡≤¶‡≥ç‡≥ç ‡≤∏‡≤∏‡≥ç‡≤∞‡≥Å‡≤∞‡≤ø  ‡≤¶‡≥ç ‡≤ø‡≥ç ‡≥ç ‡≤ï‡≤æ ‡≥ç‡≥Å‡≤∏‡≤ó‡≥ç  ‡≤∏‡≤æ‡≤§‡≥Å‡≥ç  ‡≤ø‡≤Æ‡≤ø‡≤®‡≥ç ‡≤¶‡≤æ‡≤ø‡≤Æ‡≥ç ‡≥Å‡≤® ‡≥Å‡≤∏‡≥ç ‡≥ç‡≥ç‡≤¶‡≤∞‡≤∞‡≤ø ‡≤§‡≥ç‡≤ø‡≤¶‡≥Å‡≤Æ‡≤æ‡≤æ‡≥ç‡≤¶‡≤ø ‡≥Å‡≤ø‡≤∞‡≥ç ‡≤ø    
-------------------------------------------
Iteration 400 : 3.0160765647888184
English: I also had such a feeling.
Kannada Translation: ‡≤®‡≤®‡≤ó‡≤Ç‡≤§‡≥Ç ‡≤Ö‡≤Ç‡≤• ‡≤Ö‡≤®‡≥Å‡≤≠‡≥Ç‡≤§‡≤ø‡≤Ø‡≥á ‡≤Ü‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å.
Kannada Prediction: ‡≤á‡≤æ‡≥ç‡≥Ü‡≤§‡≥ç ‡≤Æ‡≤≤‡≤¶ ‡≤Æ‡≤µ‡≥ç ‡≤æ ‡≥ç   ‡≤µ‡≤ó‡≤ø ‡≥Å‡≤≤‡≥Å 
-------------------------------------------
Iteration 500 : 3.020336627960205
English: What if its too late?
Kannada Translation: ‡≤¶‡≥Ä‡≤∞‡≥ç‡≤ò‡≤ï‡≤æ‡≤≤ ‡≤á‡≤¶‡≥ç‡≤¶‡≤∞‡≥Ü ‡≤®‡≤æ‡≤ü‡≥ç ‡≤è‡≤®‡≤æ‡≤ó‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü?
Kannada Prediction: ‡≤á‡≤æ‡≤® ‡≤ø‡≤≤ ‡≥ç‡≤∞‡≤æ‡≤™  ‡≤∞‡≥Å‡≥Å ‡≤π‡≥Ü‡≤ó‡≥ç‡≤∞‡≤Æ‡≤®‡≥Å‡≤∞‡≤≥ ‡≥ç‡≤§‡≥ç‡≥Ü.
-------------------------------------------
Iteration 600 : 2.7896459102630615
English: I am happy that our principals and teachers are enthusiastically participating in this campaign to implement the National Education Policy.
Kannada Translation: ‡≤∞‡≤æ‡≤∑‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Ø ‡≤∂‡≤ø‡≤ï‡≥ç‡≤∑‡≤£ ‡≤®‡≥Ä‡≤§‡≤ø‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤ú‡≤æ‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤§‡≤∞‡≥Å‡≤µ ‡≤à ‡≤Ö‡≤≠‡≤ø‡≤Ø‡≤æ‡≤®‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø ‡≤®‡≤Æ‡≥ç‡≤Æ ‡≤™‡≥ç‡≤∞‡≤æ‡≤Ç‡≤∂‡≥Å‡≤™‡≤æ‡≤≤‡≤∞‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∂‡≤ø‡≤ï‡≥ç‡≤∑‡≤ï‡≤∞‡≥Å ‡≤â‡≤§‡≥ç‡≤∏‡≤æ‡≤π‡≤¶‡≤ø‡≤Ç‡≤¶ ‡≤≠‡≤æ‡≤ó‡≤µ‡≤π‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤ø‡≤∞‡≥Å‡≤µ‡≥Å‡≤¶‡≥Å ‡≤®‡≤®‡≤ó‡≥Ü ‡≤∏‡≤Ç‡≤§‡≥ã‡≤∑‡≤µ‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü.
Kannada Prediction: ‡≤Ü‡≤ø‡≤∞‡≥ç ‡≥ç‡≤≤‡≥ç‡≤Ø ‡≤™‡≥ç ‡≥ç‡≤∞ ‡≥ç‡≤™‡≥ç‡≤∞‡≥ç‡≤Ø‡≤ø‡≥ç‡≤Ø‡≥ç ‡≤™‡≥ç‡≤∞‡≥ç ‡≤ø ‡≤Æ‡≥ç‡≤ø  ‡≤Æ ‡≤Æ‡≤®   ‡≤∞‡≥ç‡≥ç‡≥ç‡≤∞‡≥ç‡≤¶‡≤π‡≤ø  ‡≤æ‡≤®‡≥ç‡≤∞‡≥ç ‡≤¶‡≥ç ‡≥ç‡≤ó‡≥ç‡≥Å ‡≤π‡≤æ‡≥ç‡≤®‡≥ç ‡≤Æ ‡≤∏‡≤æ‡≤Ø‡≥ç‡≥ç‡≤ø ‡≤ï‡≤Ç‡≥ç‡≤Ø‡≥Å‡≤ó‡≥Å‡≥Ü‡≤¶‡≤¶‡≥Ü‡≤™‡≥ç‡≤∞‡≤ø‡≤ø‡≥ç ‡≤ø ‡≤ø ‡≥ç‡≤Ø‡≥ç ‡≤ø ‡≥ç ‡≤∏‡≤ø‡≥ç‡≤ø ‡≤™‡≤ø‡≤§‡≥ç‡≤§‡≥ç‡≤ø‡≤ó‡≤ø ‡≥ç 
-------------------------------------------
Iteration 700 : 2.8551084995269775
English: This will cause heartburn.
Kannada Translation: ‡≤á‡≤¶‡≥Å ‡≤é‡≤¶‡≥Ü‡≤Ø‡≥Å‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤ï‡≤æ‡≤∞‡≤£‡≤µ‡≤æ‡≤ó‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü.
Kannada Prediction: ‡≤Ö‡≤¶‡≥ç ‡≤ï‡≤Ç  ‡≤æ ‡≥ç ‡≤≥ ‡≤™‡≥ç‡≤∞‡≥ç  ‡≤∞‡≤≥ ‡≥ç‡≤≤‡≥ç .
-------------------------------------------
Iteration 800 : 2.7453389167785645
English: The government is sinking into debt.
Kannada Translation: ‡≤∏‡≤æ‡≤≤‡≤¶ ‡≤¨‡≤≤‡≥Ü‡≤ó‡≥Ü ‡≤∏‡≤∞‡≥ç‡≤ï‡≤æ‡≤∞ ‡≤∏‡≤ø‡≤ï‡≥ç‡≤ï‡≤ø‡≤ï‡≥ä‡≤Ç‡≤°‡≤ø‡≤¶‡≥Ü.
Kannada Prediction: ‡≤Ü‡≥Å‡≤∞‡≥ç‡≥Å‡≤ï‡≥ç‡≥ç ‡≤ø‡≤ó‡≤ï‡≤ø‡≥Å‡≤≤‡≥ç‡≤∞‡≥ç‡≤Æ‡≤ø  ‡≤≤‡≥ç‡≤¶‡≥ç‡≤Ç‡≤¶‡≤ø‡≤¶‡≥Ü.
-------------------------------------------
Iteration 900 : 2.7385761737823486
English: Rajeshwari Devi and others participated in this lively programme.
Kannada Translation: ‡≤∞‡≤æ‡≤ú‡≥á‡≤∂‡≥ç‡≤µ‡≤∞‡≤ø ‡≤¶‡≥á‡≤µ‡≤ø ‡≤Æ‡≥ä‡≤¶‡≤≤‡≤æ‡≤¶‡≤µ‡≤∞‡≥Å ‡≤â‡≤™‡≤∏‡≥ç‡≤•‡≤ø‡≤§‡≤∞‡≤ø‡≤¶‡≥ç‡≤¶‡≤∞‡≥Å.
Kannada Prediction: ‡≤Æ‡≤ø‡≤µ‡≥ç ‡≥ç‡≤≤‡≤ø‡≥ç ‡≤∏ ‡≤ï‡≤æ ‡≤∏‡≤æ‡≤∞‡≤∞‡≥ç‡≤∞ ‡≥Å‡≥Ü ‡≤Æ‡≤§‡≥ç‡≥ç‡≤®‡≤ø‡≤¶‡≥ç‡≥ç‡≤∏ ‡≤§‡≥Å‡≤ø.
-------------------------------------------
Iteration 1000 : 2.6645731925964355
English: The virus can be passed on through contact with contaminated surfaces.
Kannada Translation: ‡≤ï‡≤≤‡≥Å‡≤∑‡≤ø‡≤§ ‡≤Æ‡≥á‡≤≤‡≥ç‡≤Æ‡≥à‡≤ó‡≤≥ ‡≤Æ‡≥Ç‡≤≤‡≤ï ‡≤µ‡≥à‡≤∞‡≤∏‡≥ç ‡≤π‡≤∞‡≤°‡≤¨‡≤π‡≥Å‡≤¶‡≥Å.
Kannada Prediction: ‡≤∏‡≤æ‡≥ç ‡≥ç ‡≥ç‡≤∏‡≤æ ‡≥ç‡≤≤‡≤æ ‡≤≥‡≤ø‡≤∏  ‡≤ø‡≥ç‡≤∏  ‡≥ç‡≤ø‡≤∞‡≤®‡≤æ‡≤æ‡≥Ü‡≥ç‡≤æ ‡≤ø.
-------------------------------------------
Iteration 1100 : 2.7321813106536865
English: &quot;&quot;&quot;I don't know too much more.&quot;
Kannada Translation: &quot;&quot;&quot;‡≤®‡≤®‡≤ó‡≥Ü ‡≤π‡≥Ü‡≤ö‡≥ç‡≤ö‡≥á‡≤®‡≥Å ‡≤§‡≤ø‡≤≥‡≤ø‡≤¶‡≤ø‡≤≤‡≥ç‡≤≤.&quot;
Kannada Prediction: ‡≤Ö&quot;‡≤®‡≤ï‡≥Å‡≤ø‡≥Ü ‡≤π‡≤æ ‡≥ç‡≤∞‡≥ç‡≤®‡≥ç ‡≤®‡≥ç‡≤¶‡≤ø‡≤¶‡≥Ü‡≤¶‡≤ø‡≤§‡≥ç
-------------------------------------------
</code></pre>
<p>The issue however creeps in the model inference time. I pass an empty Kannada sentence (which should be encoded to a <code>START_TOKEN</code>), but all I get is the <code>END_TOKEN</code> for the translation.</p>
<pre class=""lang-py prettyprint-override""><code>transformer.eval()
kn_sentence = (&quot;&quot;,)
eng_sentence = (&quot;Should we go to the mall?&quot;,)

for word_counter in range(max_sequence_length):
    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)
    predictions = transformer(eng_sentence,
                              kn_sentence,
                              encoder_self_attention_mask.to(device), 
                              decoder_self_attention_mask.to(device), 
                              decoder_cross_attention_mask.to(device),
                              enc_start_token=False,
                              enc_end_token=False,
                              dec_start_token=True,
                              dec_end_token=False)
    next_token_prob_distribution = predictions[0][word_counter] # not actual probs
    next_token_index = torch.argmax(next_token_prob_distribution).item()
    next_token = index_to_kannada[next_token_index]
    kn_sentence = (kn_sentence[0] + next_token, )
    print(f&quot;next_token: {next_token}&quot;)
    if next_token == END_TOKEN:
      break
 
print(f&quot;translation: {kn_sentence}&quot;)
</code></pre>
<p>Running the code above, I get this</p>
<pre><code>next_token: &lt;END&gt;
translation: ('&lt;END&gt;',)
</code></pre>
<p>Thanks for reading until this point. Any assistance is appreciated.</p>
",Multilingual Language Processing & Language Identification,transformer predicting end sequence token inference question trying code translator english kannada south indian language training seems happen fine however trying infer translate given english sentence kannada get translation matter actual english sentence input need assistance see case detail prodded different part network seen anything unusual dataset used samanatar collection sentence pair indian language along english translation following code transformer stored file called following code get dataset initialize model create dataset loader function create look ahead padding mask encoder decoder use instead negative infinity softmax performed scaled dot product attention stable trainer code note every iteration print example english sentence current batch actual kannada translation corresponding prediction model made time notice loss certainly decrease prediction though amazing starting actually take shape ran iteration issue however creep model inference time pas empty kannada sentence encoded get translation running code get thanks reading point assistance appreciated
"Find n-grams especially common in one corpus vs. another one, and compared to English as a whole","<p>I have two corpuses/documents of survey responses (as columns in Pandas dataframes). I would like to understand which n-grams distinguish one corpus from another, among those n-grams which distinguish those corpuses from the English language as a whole. I would like to find the weird words (jargon) which show up in these documents more often than English.
That is, how did changing the prompt which generated the survey responses, or the week the participants were surveyed, change the content of what they responded? This would be a similar concept to NLP data drift detection. Another way to look at it would be to detect trending n-grams over time.</p>
<p>I would like to do this in Python or R, or perhaps in AWS. Deep learning approaches are fine.</p>
<p>This is not cosine similarity.
This is not the same problem as taking the top 10 words in the corpuses by TF-IDF. That still gives me very common English words, even with stopword removal. Also, that does not compare the two corpuses.</p>
",Multilingual Language Processing & Language Identification,find n gram especially common one corpus v another one compared english whole two corpus document survey response column panda dataframes would like understand n gram distinguish one corpus another among n gram distinguish corpus english language whole would like find weird word jargon show document often english changing prompt generated survey response week participant surveyed change content would similar concept nlp data drift detection another way look would detect trending n gram time would like python r perhaps aws deep learning approach fine cosine similarity problem taking top word corpus tf idf still give common english word even stopword removal also doe compare two corpus
How to detect Natural Language dates in strings in Swift iOS,"<p>I'm trying to detect dates written in natural language in a string in Swift using NSDataDetector with <code>.date</code> CheckingType but it doesn't work for all natural language dates: for instance last week, next month, last year etc. but works if the string has more specific date components.</p>
<p>Here is some sample code:</p>
<pre><code>let string = &quot;I will be free next Monday at 3 PM&quot; // match found - worked corrected
// let string = &quot;we had the meeting last month&quot; // no match found does not work - returns nil

let types: NSTextCheckingResult.CheckingType = [.date]
let detector = try! NSDataDetector(types: types.rawValue)

let matches = detector.matches(in: string, options: [], range: NSRange(location: 0, length: string.utf16.count))
for match in matches {
    if let date = match.date {
        print(date.formatted(date: .complete, time: .shortened))
    }
}
</code></pre>
<p>Also tried using <code>RelativeDateTimeFormatter()</code> but it's only supports Date object to String and not the other way round.</p>
<p>What would be the best approach to detect natural language dates in strings that include week, month, year, name of month etc.</p>
",Multilingual Language Processing & Language Identification,detect natural language date string swift io trying detect date written natural language string swift using nsdatadetector checkingtype work natural language date instance last week next month last year etc work string ha specific date component sample code also tried using support date object string way round would best approach detect natural language date string include week month year name month etc
"In Natural language processing, what is the purpose of chunking?","<p>In Natural language processing, what is the purpose of chunking?</p>
",Multilingual Language Processing & Language Identification,natural language processing purpose chunking natural language processing purpose chunking
Using google translate in android application,"<p>I want to use google translate API on my android project. Below are my codes. It works as java project but when I try it on android device, project stopped. What is the reason?
 I added <a href=""https://code.google.com/p/java-google-translate-text-to-speech/downloads/detail?name=gtranslateapi-1.0.jar&amp;can=2&amp;q="" rel=""nofollow"">googletranslateapi-1.0.jar</a>, </p>

<p><strong>translator.java</strong></p>

<pre><code>package com.example.calendar;

import com.gtranslate.Language;
import com.gtranslate.Translator;

public class translator {

    public String translate(String text) {
        Translator translate = Translator.getInstance();
        String text2 = translate.translate(text, Language.TURKISH,
                Language.ENGLISH);
        return text2;
    }

}
</code></pre>

<p>In mainactivity</p>

<pre><code>public void onActivityResult(int requestCode, int resultCode, Intent data) 
{

if (requestCode == VR_REQUEST &amp;&amp; resultCode == RESULT_OK) {

            ArrayList&lt;String&gt; suggestedWords = data
                    .getStringArrayListExtra(RecognizerIntent.EXTRA_RESULTS);
            Context context = getApplicationContext();
            text = suggestedWords.get(0);
            int duration = Toast.LENGTH_SHORT;
            ClassificationActivity classificationActivity = new ClassificationActivity();
            String category = classificationActivity.control(text);
            translator trans = new translator();
            String ingtext = trans.translate(text);

            CalendarEvent cv = new CalendarEvent();
            Date date = new Date();
            ContentResolver cr = ctx.getContentResolver();
            Uri uri = cr.insert(Events.CONTENT_URI, CalendarEvent
                    .toICSContentValues(cv, date.getTime(), category));
            Toast toast = Toast.makeText(context, ingtext, duration);
            toast.show();

        }
</code></pre>
",Multilingual Language Processing & Language Identification,using google translate android application want use google translate api android project code work java project try android device project stopped reason added googletranslateapi jar translator java mainactivity
I have developed a software for nlp but it always returns the same output,"<p>The software he developed a software for natural language processing is able to predict the first 10/20 characters, but it always returns the same output for the remaining 180 characters</p>
<pre><code>text_prompt = 'I‚Äôve used these features of Git for years and I have no idea why they are not used more often.'
</code></pre>
<pre><code>input_tokens = tokenizer(text_prompt, return_tensors=&quot;pt&quot;).to(0)
</code></pre>
<pre><code>result_sample = model_lm.generate(**input_tokens, max_length=200, top_k=0, temperature=0.2)

</code></pre>
<pre><code>tokenizer.decode(result_sample[0], truncate_before_pattern=[r&quot;\n\n^#&quot;,&quot;^''&quot;, '\n\n\n'])
</code></pre>
<p>output:</p>
<pre><code>I‚Äôve used these features of Git for years and I have no idea why they are not used more often. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for developers and a great tool for the user. I think they are a great tool for
</code></pre>
<p>How can I break this cycle? Or can I have the returned characters in the loop only be returned once?</p>
",Multilingual Language Processing & Language Identification,developed software nlp always return output software developed software natural language processing able predict first character always return output remaining character output break cycle returned character loop returned
Person Name Detection using SpaCy in English Lang. Looking for Answer,"<p>I am using Spacy and trying to detect names in the text. For example, text = 'Keras is a good package. Adam Smith uses a car of black colour. I hope Katrina is doing well in her job.'</p>

<p>The answer should like this: Adam Smith and Katrina. </p>

<p>Can anyone recommend </p>
",Multilingual Language Processing & Language Identification,person name detection using spacy english lang looking answer using spacy trying detect name text example text kera good package adam smith us car black colour hope katrina well job answer like adam smith katrina anyone recommend
NLP general English to action,"<p>I am working on automating task flow of application using text based Natural Language Processing.</p>

<p>It is something like chatting application where the user can type in the text area. At same time python code interprets what user wants and it performs the corresponding action. </p>

<p>Application has commands/actions like:</p>

<ol>
<li>Create Task</li>
<li>Give Name to as t1</li>
<li>Add time to task</li>
<li>Connect t1 to t2</li>
</ol>

<p>The users can type in chat (natural language). It will be like a general English conversation, for example: </p>

<ol>
<li><strong>Can you create a task with name t1 and assign time to it. Also, connect t1 to t2</strong></li>
</ol>

<p>I could write a rule drive parser, but it would be limited to few rules only.</p>

<p>Which approach or algorithm can I use to solve this task?
How can I map general English to command or action?    </p>
",Multilingual Language Processing & Language Identification,nlp general english action working automating task flow application using text based natural language processing something like chatting application user type text area time python code interprets user want performs corresponding action application ha command action like create task give name add time task connect user type chat natural language like general english conversation example create task name assign time also connect could write rule drive parser would limited rule approach algorithm use solve task map general english command action
How to find correlation of two set of software requirements,"<p>I have a thought to find correlation between given set of high level software requirements to customer requirement specification both of which are represented by English text. I want to do this to find which high level requirement corresponds to which customer requirement specification. Can anyone guide me how to do this? Are there any open source libraries to use?</p>
<p>I don't know much on machine learning or natural language processing, so have not tried anything yet</p>
",Multilingual Language Processing & Language Identification,find correlation two set software requirement thought find correlation given set high level software requirement customer requirement specification represented english text want find high level requirement corresponds customer requirement specification anyone guide open source library use know much machine learning natural language processing tried anything yet
Identify and correct mistakes in Q&amp;A datasets for natural language processing (NLP),"<p>How do you identify and correct mistakes in Q&amp;A datasets that contain errors, such as incorrect answers or missing information, and ensure the accuracy of the dataset?
Let's say I got thousands of questions+answers that are formed like the Stanford Question Answering Dataset (SQuAD) and I want to double-check every single of them.
What are some common methods or best practices for adjusting or correcting Q&amp;A datasets?</p>
<p>For example, if the input text of the context looks like this:</p>
<p>text = &quot;Albert Einstein, (born March 14, 1879, Ulm, W√ºrttemberg, Germany‚Äîdied April 18, 1955, Princeton, New Jersey, U.S.), German-born physicist who developed the special and general theories of relativity and won the Nobel Prize for Physics in 1921 for his explanation of the photoelectric effect. Einstein is generally considered the most influential physicist of the 20th century.&quot;
``
Output:</p>
<p>Q: Who is generally considered to be the most influential? (miss <strong>physicist</strong>)
A: Albert Einstein
Q: What is the photoelectric effect?
A: Albert Einstein (<strong>Wrong answer</strong>)</p>
<p>I examined the QA-dataset as a json-file and attempted to correct it directly, but it is very slow and it is easy to lose track of updating everything in a file.</p>
",Multilingual Language Processing & Language Identification,identify correct mistake q datasets natural language processing nlp identify correct mistake q datasets contain error incorrect answer missing information ensure accuracy dataset let say got thousand question answer formed like stanford question answering dataset squad want double check every single common method best practice adjusting correcting q datasets example input text context look like text albert einstein born march ulm w rttemberg germany died april princeton new jersey u german born physicist developed special general theory relativity nobel prize physic explanation photoelectric effect einstein generally considered influential physicist th century output q generally considered influential miss physicist albert einstein q photoelectric effect albert einstein wrong answer examined qa dataset json file attempted correct directly slow easy lose track updating everything file
Apply python package (spaCy) word list only covering the specific language vocabulary,"<p>I need to filter out non-core German words from a text using spaCy. However, I couldn't find a suitable approach or word list that covers only the essential vocabulary of the German language.</p>
<p>I have tried different approaches using the spacy tools <code>nlp(word).has_vector</code> and <code>nlp(word).vector_norm == 0</code> and using a list of words like <code>list(nlp.vocab.strings)</code> from 'de_core_news_sm' or 'de_core_news_lg', but they either recognize irrelevant words as part of the German language or fail to recognize basic German words.
I'm looking for recommendations on how to obtain or create a word list that accurately covers only the core vocabulary of the German language, and can be used with (preferably) spaCy or other NLP packages. I would prefer using a universal, not german specific, language package, so that I can extend to other languages as easily.</p>
",Multilingual Language Processing & Language Identification,apply python package spacy word list covering specific language vocabulary need filter non core german word text using spacy however find suitable approach word list cover essential vocabulary german language tried different approach using spacy tool using list word like de core news sm de core news lg either recognize irrelevant word part german language fail recognize basic german word looking recommendation obtain create word list accurately cover core vocabulary german language used preferably spacy nlp package would prefer using universal german specific language package extend language easily
python: AI language processing to extract information from short but variated texts?,"<p>I'm trying to scrape a reddit site and parse the titles. But, the titles are not in a very specific format and that things like regex don't work. In fact, there has to be some level of understanding of English.</p>
<p>The goal is to get the requested amount, repayment amount(s), repayment date(s), and repayment method(s).</p>
<p>[REQ] ($500)(#Brisbane, Queensland, Australia)(Repay $550 March 20)(PayPal)</p>
<p>[REQ] ($300) (#Scottsdale, AZ, USA) (Repay $360 3/17/23) (CashApp)</p>
<p>[Req] (400) - (#Los Angeles, Ca, USA), (Repay $440 on 3/17/23), (Cash app)</p>
<p>[REQ] ($150) - (#Phoenix, Arizona, USA), (REPAY $175 03/6/23), (PayPal)</p>
<p>[REQ] - (#Tooele, UT, US), ($300), (Payback $200 3/31/23, $200 4/30/23)</p>
<p>Notice how there can be a dash between [REQ] and location. And also sometimes the amount goes before the location and sometimes vice versa. For repayment, some used terms are &quot;Repay&quot;, &quot;payback&quot;, &quot;pay back&quot;. For dates, some use 3/6, some say March.</p>
<p>The point is that there's a ton of subtle variations in these titles. We as humans can parse them, but I'm not sure how complicated a NLP model would need to be.</p>
",Multilingual Language Processing & Language Identification,python ai language processing extract information short variated text trying scrape reddit site parse title title specific format thing like regex work fact ha level understanding english goal get requested amount repayment amount repayment date repayment method req brisbane queensland australia repay march paypal req scottsdale az usa repay cashapp req los angeles ca usa repay cash app req phoenix arizona usa repay paypal req tooele ut u payback notice dash req location also sometimes amount go location sometimes vice versa repayment used term repay payback pay back date use say march point ton subtle variation title human parse sure complicated nlp model would need
How to extract a non-English address from a string,"<p>It would be a big help if someone here knows about a library for python that can extract from a whole message only an address in Turkey (the text is originally in Turkish).</p>
<p>A translated example would be:</p>
<blockquote>
<p>&quot;Hi, my name is Salem and I have information about a crash site at
.....(Address goes here), Many Thanks.&quot;</p>
</blockquote>
<p>I tried looking online and didn't find a library that had functionality on Turkish addresses, only some NLP projects for the US.
The input is plain text. I have already translated it to English but I don't know how to specifically extract the address from the whole message.</p>
",Multilingual Language Processing & Language Identification,extract non english address string would big help someone know library python extract whole message address turkey text originally turkish translated example would hi name salem information crash site address go many thanks tried looking online find library functionality turkish address nlp project u input plain text already translated english know specifically extract address whole message
What is the best way to do Natural Language Processing in Rails app?,"<p>I have a Rails app. I need to implement automatic text categorization algorithms and possibly more NLP capabilities in app. I believe Ruby does not have good NLP tools available as Python has. I am using a separate resque server for process background jobs. I believe I have following </p>

<ol>
<li>Run python scripts using resque jobs</li>
<li>Run a flask application on a separate server which can either talk to resque job or can automatically update the app database with processed results. </li>
<li>Use Ruby tools mentioned in <a href=""https://stackoverflow.com/questions/999410/natural-language-processing-in-ruby"">this thread</a></li>
<li>Any other suggestions welcome</li>
</ol>

<p>Please let me know what is the best way to do it. Are there any similar working examples? </p>
",Multilingual Language Processing & Language Identification,best way natural language processing rail app rail app need implement automatic text categorization algorithm possibly nlp capability app believe ruby doe good nlp tool available python ha using separate resque server process background job believe following run python script using resque job run flask application separate server either talk resque job automatically update app database processed result use ruby tool mentioned href thread suggestion welcome please let know best way similar working example
Random spaces when creating txt-File from PDF,"<p>This is my code:</p>
<pre><code>import PyPDF2

# open the pdf file
pdfFileObj = open('pdfFile.pdf', 'rb')

# creating a pdf reader object
pdfReader = PyPDF2.PdfFileReader(pdfFileObj)

# looping through all the pages of the pdf
for page in range(pdfReader.numPages):

    # creating a page object
    pageObj = pdfReader.getPage(page)

    # extracting text from page
    text = pageObj.extractText()

    # writing text to a txt file
    file = open('txtFile.txt', 'a+', encoding='utf-8')
    file.write(text)
    file.close()

# closing the pdf file object
pdfFileObj.close()
</code></pre>
<p>The problem is that i get &quot;random&quot; spaces when converting from pdf to txt.</p>
<p><strong>Question:</strong></p>
<p><strong>Why do i get these spaces when writing pdf to txt? Is it a problem of PyPDF2?</strong></p>
<p>An example of this is: &quot;The apple is red.&quot;(PDF) and the text in the txt file is: &quot;The <strong>ap ple</strong> is red.&quot;</p>
<p><a href=""https://i.sstatic.net/JvVHw.png"" rel=""nofollow noreferrer"">PDF1</a> to <a href=""https://i.sstatic.net/7JZwC.png"" rel=""nofollow noreferrer"">txt1</a></p>
<p>or another ex.:</p>
<p><a href=""https://i.sstatic.net/VkPLW.png"" rel=""nofollow noreferrer"">PDF2</a> to <a href=""https://i.sstatic.net/iNszc.png"" rel=""nofollow noreferrer"">txt2</a></p>
<p>Furthermore, its not always inbetween the &quot;apple&quot;. It could be from &quot;Apple&quot; to &quot;A pple&quot; or &quot;Banana&quot; to &quot;Bana na&quot;, with no clear reason for these spaces.</p>
<p>I tried looking for recuring patterns, like always getting spaces between chars of a &amp; p or others, but sometimes the same chars that get spaced apart, seem normal on other occasions.</p>
<p>FYI:</p>
<p>-The language i want to convert is German</p>
<p>-The pdf file is computer generated</p>
<p>Thanks a lot for helping me :)</p>
",Multilingual Language Processing & Language Identification,random space creating txt file pdf code problem get random space converting pdf txt question get space writing pdf txt problem pypdf example apple red pdf text txt file ap ple red pdf txt another ex pdf txt furthermore always inbetween apple could apple pple banana bana na clear reason space tried looking recuring pattern like always getting space char p others sometimes char get spaced apart seem normal occasion fyi language want convert german pdf file computer generated thanks lot helping
Getting the adjectives and plurals of lemma&#39;s in various languages,"<p>Could anyone point me to a solution/lib to instead of lemmatise, to do inflection(?). And for multiple languages (English, Dutch, German and French).</p>
<p>Or to give an example. I have the lemma 'science' for which I need the words 'sciences', 'scientific', 'scientifically'... returned. So plural and adjectives.</p>
<p>I looked into NLTK (cf Wordnet and Spacy), but did not find a solution.</p>
",Multilingual Language Processing & Language Identification,getting adjective plural lemma various language could anyone point solution lib instead lemmatise inflection multiple language english dutch german french give example lemma science need word science scientific scientifically returned plural adjective looked nltk cf wordnet spacy find solution
How to detect language of user entered text?,"<p>I am dealing with an application that is accepting user input in different languages (currently 3 languages fixed). The requirement is that users can enter text and dont bother to select the language via a provided checkbox in the UI.</p>

<p>Is there an <strong>existing Java library</strong> to detect the language of a text?</p>

<p>I want something like this:</p>

<pre><code>text = ""To be or not to be thats the question.""

// returns ISO 639 Alpha-2 code
language = detect(text);

print(language);
</code></pre>

<p>result:</p>

<pre><code>EN
</code></pre>

<p><strong>I dont want to know how to create a language detector by myself</strong> (i have seen plenty of blogs trying to do that). The library should provide a simple APi and also work completely offline. Open-source or commercial closed doesn't matter.</p>

<p>i also found this questions on SO (and a few more):</p>

<p><a href=""https://stackoverflow.com/questions/3173005"">How to detect language</a><br>
<a href=""https://stackoverflow.com/questions/2752691"">How to detect language of text?</a></p>
",Multilingual Language Processing & Language Identification,detect language user entered text dealing application accepting user input different language currently language fixed requirement user enter text dont bother select language via provided checkbox ui existing java library detect language text want something like result dont want know create language detector seen plenty blog trying library provide simple api also work completely offline open source commercial closed matter also found question
spacy Arabic word2vector,"<p>i try to arabic vector with spacy, so i try code from this github
<a href=""https://github.com/bakrianoo/aravec/blob/master/aravec-with-spacy.ipynb"" rel=""nofollow noreferrer"">https://github.com/bakrianoo/aravec/blob/master/aravec-with-spacy.ipynb</a></p>
<p>when i use this code</p>
<pre><code>!python -m spacy  init-model ar spacy.aravec.model --vectors-loc ./spacyModel/aravec.txt.gz
</code></pre>
<p><strong>it apear this error</strong></p>
<pre><code>Error: No such command 'init-model'.
</code></pre>
<p>so i try to fix it by change <code>init-model</code> to <code>init</code> and add <code>vector</code> but this error apear:</p>
<pre><code>!python -m spacy init vectors ar ./spacyModel/spacy.aravec.model --vectors-loc ./spacyModel/aravec.txt.gz
</code></pre>
<p><strong>error</strong> :</p>
<pre><code>Usage: python -m spacy init vectors [OPTIONS] LANG VECTORS_LOC OUTPUT_DIR
Try 'python -m spacy init vectors --help' for help.

Error: No such option: --vectors-loc
</code></pre>
<p>how can i fix it ?</p>
",Multilingual Language Processing & Language Identification,spacy arabic word vector try arabic vector spacy try code github use code apear error try fix change add error apear error fix
How to train FLAN-T5 to summarization task with a custom dataset of legal documents in pt-br?,"<p>So, I would like to create a small proof-of-concept using (already extracted in txt files) +- 4.000 legal text divided in:</p>
<ol>
<li>2.000 initial petitions / complaints *.txt files</li>
<li>2.000 summaries of each initial petition (txt files too)</li>
</ol>
<p>PS.: <strong>all text files are in brazilian portuguese (pt-br)</strong></p>
<p>So how can I use these txt files to train a new transformer able to generate new summaries (using flan-t5) ?</p>
",Multilingual Language Processing & Language Identification,train flan summarization task custom dataset legal document pt br would like create small proof concept using already extracted txt file legal text divided initial petition complaint txt file summary initial petition txt file p text file brazilian portuguese pt br use txt file train new transformer able generate new summary using flan
"ValueError: Found input variables with inconsistent numbers of samples: [28332, 24]","<p>I am currently working with multilabel text classification in the Arabic language using binary relevance and label power set, after I make all preprocessing that I need when I need to combine chi and mutual feature selection based on their weights, I am facing this problem
<code>Found input variables with inconsistent numbers of samples: [28332, 24]</code>
where my dataset has one column have the text and 24 columns as a target as shown in the image :
<a href=""https://i.sstatic.net/gJ4Yk.png"" rel=""nofollow noreferrer"">enter image description here</a>
I am writing this code</p>
<pre><code>`class Classifier:
    def __init__(self):
        self.merged_df = pd.read_csv(r&quot;D:\project\Ymal.csv&quot;, encoding='utf-8')
        self.train_df, self.test_df = train_test_split(self.merged_df,test_size=0.2,random_state=42) 
        self.vectorizer = CountVectorizer()
        self.ModelsPerformance = {}
    def train(self):
        self.train_text = self.train_df['text']
        self.test_text = self.test_df['text']
        self.train_labels = self.train_df.drop(columns=['text'])
        self.test_labels = self.test_df.drop(columns=['text'])
        self.mlb = MultiLabelBinarizer()
        self.train_labels = self.mlb.fit_transform(self.train_labels)
        self.test_labels = self.mlb.transform(self.test_labels)
        self.train_text_bow = self.vectorizer.fit_transform(self.train_text)
        self.test_text_bow = self.vectorizer.transform(self.test_text)
        self.chi2_selector = SelectKBest(chi2, k='all',)
        self.mi_selector = SelectKBest(mutual_info_classif, k='all',)
        self.chi2_features = self.chi2_selector.fit_transform(self.train_text_bow,self.train_labels)     
        self.mi_features = self.mi_selector.fit_transform(self.train_text_bow,self.train_labels)
        self.weights_chi2 = self.chi2_selector.scores_ 
        self.weights_mi = self.mi_selector.scores_
        self.weights = (self.weights_chi2 + self.weights_mi ) / 2
        self.top_features = np.argsort(self.weights)[-4000:]  #[::-1]
        self.train_combined_features = self.train_text_bow[:,self.top_features]
        self.test_text_bow = self.vectorizer.transform(self.test_text)
        self.test_combined_features = self.test_text_bow[:, self.top_features]
    def metricsReport(self,modelName, test_labels, predictions):
        hamLoss = hamming_loss(test_labels, predictions)
        print(&quot;------&quot; + modelName + &quot; Model Metrics-----&quot;)
        accuracy = accuracy_score(test_labels, predictions)
        macroPrecision = precision_score(test_labels, predictions, average='macro')
        macroRecall = recall_score(test_labels, predictions, average='macro')
        macroF1 = f1_score(test_labels, predictions, average='macro')
        microPrecision = precision_score(test_labels, predictions, average='micro')
        microRecall = recall_score(test_labels, predictions, average='micro')
        microF1 = f1_score(test_labels, predictions, average='micro')
        weightedF1 = f1_score(test_labels, predictions, average='weighted')
            # print metrics
        print(&quot;Hamming Loss: {:.4f}&quot;.format(hamLoss))
        print('Accuracy: {0:.4f}'.format(accuracy))
        print('Macro Precision: {0:.4f}'.format(macroPrecision))
        print('Macro Recall: {0:.4f}'.format(macroRecall))
        print('Macro F1-measure: {0:.4f}'.format(macroF1))
        print('Micro Precision: {0:.4f}'.format(microPrecision))
        print('Micro Recall: {0:.4f}'.format(microRecall))
        print('Micro F1-measure: {0:.4f}\n'.format(microF1)) 
        print('Weighted F1-measure: {0:.4f}\n'.format(weightedF1))         
    def fitAlgorithms(self):
        algorithms = [{'name': 'LinearSVC', 'model': LinearSVC(max_iter=12000, dual=False),
             'params': {'C': [0.1, 1, 10]}},
            {'name': 'KNN', 'model': KNeighborsClassifier(),
             'params': {'n_neighbors': [5, 10, 15]}},
            {'name': 'RandomForest', 'model': RandomForestClassifier(),
             'params': {'n_estimators': [100, 300, 500]}},
            {'name': 'LogisticRegression', 'model': LogisticRegression(),
             'params': {'C': [0.1, 1, 10]}},
            {'name': 'DecisionTree', 'model': DecisionTreeClassifier(),
             'params': {'max_depth': [5, 10, 15]}},
            {'name': 'MultinomialNB', 'model': MultinomialNB(),
             'params': {'alpha': [0.1, 1, 10]}}
        ]
        for algorithm in algorithms:
            model = algorithm['model']
            name = algorithm['name']
            params = algorithm['params']
        # Fit the binary relevance and label powerset classifiers before the grid search
            binary_relevance_classifier = BinaryRelevance(model)
            binary_relevance_classifier.fit(self.train_combined_features, self.train_labels)
            labelPowerSet_classifier = LabelPowerset(model)
            labelPowerSet_classifier.fit(self.train_combined_features, self.train_labels)
            print(f&quot;Performing GridSearchCV for {name}...&quot;)
            clf = GridSearchCV(model, params, scoring='accuracy', cv=5)
            clf.fit(self.train_combined_features, self.train_labels)
            best_params = clf.best_params_
            print(f&quot;Best parameters for {name}: {best_params}&quot;)
            model.set_params(**best_params)
            binary_relevance_preds = binary_relevance_classifier.predict(self.test_combined_features)
            self.metricsReport(f&quot;Binary Relevance with {name}&quot;, self.test_labels, binary_relevance_preds)
            labelPowerSet_preds = labelPowerSet_classifier.predict(self.test_combined_features)
            self.metricsReport(f&quot;Label Powerset with {name}&quot;, self.test_labels, labelPowerSet_preds)
            self.ModelsPerformance[name] = clf.best_score_
        return self.ModelsPerformance
# Create an instance of the Classifier
classifier = Classifier()
# Invoke the training method
classifier.train()
# Invoke the fitAlgorithms() method
classifier.fitAlgorithms()
</code></pre>
<p>but this basic problem is this error I referee it above
please any one can help me and if any one can optimize this ?<br />
I believe that error is clear but I cant avoid this , also i tried the do this to sure the shape but it fine
<code>print(&quot;train_text_bow shape:&quot;, train_text_bow.shape) print(&quot;train_labels shape:&quot;, train_labels.shape) train_text_bow shape: (28332, 121714) train_labels shape: (28332, 24)t</code></p>
<p>I need just to avoid this error</p>
",Multilingual Language Processing & Language Identification,valueerror found input variable inconsistent number sample currently working multilabel text classification arabic language using binary relevance label power set make preprocessing need need combine chi mutual feature selection based weight facing problem dataset ha one column text column target shown image enter image description writing code basic problem error referee please one help one optimize believe error clear cant avoid also tried sure shape fine need avoid error
Tf-IDF vectorized data won&#39;t work with naive bayes classifier,"<p>I have the following python code that I am using after preprocessing the data where data has to columns, one is the label either positive or negative and the other has tweet texts.</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(data['Tweet'], data['Label'], test_size=0.20, random_state=0)
tf_idf = TfidfVectorizer()

x_traintf = tf_idf.fit_transform(X_train)
x_traintf = tf_idf.transform(X_train)
x_testtf = tf_idf.fit_transform(X_test)
x_testtf = tf_idf.transform(X_test)

naive_bayes_classifier = MultinomialNB()
naive_bayes_classifier.fit(x_traintf, y_train)
y_pred = naive_bayes_classifier.predict(x_testtf)
print(metrics.classification_report(y_test, y_pred, target_names=['pos', 'neg']))
</code></pre>
<p>Here is the full error:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File &quot;C:\Users\Lenovo\PycharmProjects\pythonProject1\main.py&quot;, line 72, in &lt;module&gt;
    naive_bayes_classifier.fit(x_traintf, y_train)
  File &quot;C:\Users\Lenovo\PycharmProjects\pythonProject1\venv\Lib\site-packages\sklearn\naive_bayes.py&quot;, line 749, in fit
    X, y = self._check_X_y(X, y)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Lenovo\PycharmProjects\pythonProject1\venv\Lib\site-packages\sklearn\naive_bayes.py&quot;, line 583, in _check_X_y
    return self._validate_data(X, y, accept_sparse=&quot;csr&quot;, reset=reset)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Lenovo\PycharmProjects\pythonProject1\venv\Lib\site-packages\sklearn\base.py&quot;, line 565, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Lenovo\PycharmProjects\pythonProject1\venv\Lib\site-packages\sklearn\utils\validation.py&quot;, line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Lenovo\PycharmProjects\pythonProject1\venv\Lib\site-packages\sklearn\utils\validation.py&quot;, line 1144, in _check_y
    _assert_all_finite(y, input_name=&quot;y&quot;, estimator_name=estimator_name)
  File &quot;C:\Users\Lenovo\PycharmProjects\pythonProject1\venv\Lib\site-packages\sklearn\utils\validation.py&quot;, line 111, in _assert_all_finite
    raise ValueError(&quot;Input contains NaN&quot;)
ValueError: Input contains NaN
</code></pre>
<p>I've tried this as well but got similar results:</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2, random_state=42) 
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
clf = MultinomialNB() 
clf.fit(X_train_tfidf, y_train)
X_test_tfidf = vectorizer.transform(X_test)
y_pred = clf.predict(X_test_tfidf)
print(metrics.accuracy_score(y_test, y_pred))
</code></pre>
<p>edit: I've used <code>data.dropna(inplace=True)</code> and it appears to think that my strings are null because they are in Arabic.</p>
",Multilingual Language Processing & Language Identification,tf idf vectorized data work naive bayes classifier following python code using preprocessing data data ha column one label either positive negative ha tweet text full error tried well got similar result edit used appears think string null arabic
NLTK and language detection,"<p>How do I detect what language a text is written in using NLTK?</p>

<p>The examples I've seen use <code>nltk.detect</code>, but when I've installed it on my mac, I cannot find this package.</p>
",Multilingual Language Processing & Language Identification,nltk language detection detect language text written using nltk example seen use installed mac find package
How can I extract text using regex on a string that containts both English and Hebrew?,"<p>I have strings in the following format and am trying to extract in python the word(s) before the file extension but before the word <code>◊©◊†◊î</code>. In the first example I want the word <code>◊ë◊®◊ê◊©◊ô◊™</code> and in the second <code>◊ó◊ô◊ô ◊©◊®◊î</code></p>
<p><code>1. ◊ë◊®◊ê◊©◊ô◊™ ◊©◊†◊î ◊î_ - ◊©◊§◊™ ◊î◊™◊ï◊®◊î.pdf</code></p>
<p><code>5. ◊ó◊ô◊ô ◊©◊®◊î ◊©◊†◊î ◊î_ - ◊õ◊™◊ô◊ë◊™ ◊õ◊™◊ï◊ë◊î.pdf</code></p>
<p>I've tried a few regexs, including
<code>regex = '\d\..+◊©◊†◊î (. +)\.pdf</code> but can't seem to figure it out</p>
",Multilingual Language Processing & Language Identification,extract text using regex string containts english hebrew string following format trying extract python word file extension word first example want word second tried regexs including seem figure
Facing CUDA Error while training MNMT model using fairseq,"<p>I was trying to use fairseq to train a model for English-Russian,English-French,English-Spanish,English-German data but have been getting a CUDA Error which prevents me from running the model.
I have tried using multiple batch sizes,learning rate but am unable to run .</p>
<pre><code>fairseq-train pre \
--arch transformer_wmt_en_de \
--task translation_multi_simple_epoch \
--encoder-langtok src --decoder-langtok --lang-pairs en-ru,en-fr,en-es,en-de \
--criterion label_smoothed_cross_entropy --label-smoothing 0.1 \
--optimizer adam --adam-betas '(0.9, 0.98)' \
--lr-scheduler inverse_sqrt --lr 1e-03 --warmup-updates 4000 --max-update 100000 \
--dropout 0.3 --weight-decay 0.0001 \
--max-tokens 4096 --max-epoch 20 --update-freq 8 \
--save-interval 10 --save-interval-updates 5000 --keep-interval-updates 20 \
--log-format simple --log-interval 100 \
--save-dir checkpoints --validate-interval-updates 5000 \
--fp16 --num-workers 0 --batch-size 64
</code></pre>
<p>The above code is what I have used with various different parameters for batch size, learning rate, etc., but all seem to amount to a CUDA Error.</p>
<p><code>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.57 GiB (GPU 0; 15.74 GiB total capacity; 5.29 GiB already allocated; 9.50 GiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>
<p>Any kind of help would be appreciated.</p>
",Multilingual Language Processing & Language Identification,facing cuda error training mnmt model using fairseq wa trying use fairseq train model english russian english french english spanish english german data getting cuda error prevents running model tried using multiple batch size learning rate unable run code used various different parameter batch size learning rate etc seem amount cuda error kind help would appreciated
Regex - How can you identify strings which are not words?,"<p>Got an interesting one, and can't come up with any solid ideas, so thought maybe someone else may have done something similar.</p>
<p>I want to be able to identify strings of letters in a longer sentence that are not words and remove them.  Essentially things like <code>kuashdixbkjshakd</code></p>
<p>Everything annoyingly is in lowercase which makes it more difficult, but since I only care about English, I'm essentially looking for the opposite of consonant clusters, groups of them that don't make phonetically pronounceable sounds.</p>
<p>Has anyone heard of/done something like this before?</p>
<p>EDIT: this is what ChatGpt tells me</p>
<blockquote>
<p>It is difficult to provide a comprehensive list of combinations of consonants that have never appeared in a word in the English language. The English language is a dynamic and evolving language, and new words are being created all the time. Additionally, there are many regional and dialectal variations of the language, which can result in different sets of words being used in different parts of the world.</p>
</blockquote>
<blockquote>
<p>It is also worth noting that the frequency of use of a particular combination of consonants in the English language is difficult to quantify, as the existing literature on the subject is limited. The best way to determine the frequency of use of a particular combination of consonants would be to analyze a large corpus of written or spoken English.</p>
</blockquote>
<blockquote>
<p>In general, most combinations of consonants are used in some words in the English language, but some combinations of consonants may be relatively rare. Some examples of relatively rare combinations of consonants in English include &quot;xh&quot;, &quot;xw&quot;, &quot;ckq&quot;, and &quot;cqu&quot;. However, it is still possible that some words with these combinations of consonants exist.</p>
</blockquote>
",Multilingual Language Processing & Language Identification,regex identify string word got interesting one come solid idea thought maybe someone else may done something similar want able identify string letter longer sentence word remove essentially thing like everything annoyingly lowercase make difficult since care english essentially looking opposite consonant cluster group make phonetically pronounceable sound ha anyone heard done something like edit chatgpt tell difficult provide comprehensive list combination consonant never appeared word english language english language dynamic evolving language new word created time additionally many regional dialectal variation language result different set word used different part world also worth noting frequency use particular combination consonant english language difficult quantify existing literature subject limited best way determine frequency use particular combination consonant would analyze large corpus written spoken english general combination consonant used word english language combination consonant may relatively rare example relatively rare combination consonant english include xh xw ckq cqu however still possible word combination consonant exist
What&#39;s the best way to detect garbled text in an OCR-ed document,"<p>Are there any good NLP or statistical techniques for detecting garbled characters in OCR-ed text? Off the top of my head I was thinking that looking at the distribution of n-grams in text might be a good starting point but I'm pretty new to the whole NLP domain.</p>

<p>Here is what I've looked at so far:</p>

<ul>
<li><a href=""http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/33035.pdf"" rel=""nofollow"">N-gram Statistics in English and Chinese: Similarities and Differences</a></li>
<li><a href=""http://www.data-compression.com/english.html"" rel=""nofollow"">Statistical Distributions of English Text</a></li>
</ul>

<p>The text will mostly be in english but a general solution would be nice. The text is currently indexed in Lucene so any ideas on a term based approach would be useful too.</p>

<p><br/></p>

<p>Any suggestions would be great! Thanks!</p>
",Multilingual Language Processing & Language Identification,best way detect garbled text ocr ed document good nlp statistical technique detecting garbled character ocr ed text top head wa thinking looking distribution n gram text might good starting point pretty new whole nlp domain looked far n gram statistic english chinese similarity difference statistical distribution english text text mostly english general solution would nice text currently indexed lucene idea term based approach would useful suggestion would great thanks
How can I easily draw a parse tree from Stanford parsing data in python?,"<p>So I have this Stanford-style parsing of an english sentence:</p>

<pre><code>""There is a tree behind a car""
Parse: [S [NP There_EX NP] [VP is_VBZ [NP [NP a_DT tree_NN NP] [PP behind_IN [NP a_DT car_NN NP] PP] NP] VP] S]
</code></pre>

<p>I want to use some of the tree drawing methods in python to draw a parsing tree from the data.</p>

<p>Is there an easy way to use that parsing representation to draw a tree with python or should I change the representation somehow?</p>
",Multilingual Language Processing & Language Identification,easily draw parse tree stanford parsing data python stanford style parsing english sentence want use tree drawing method python draw parsing tree data easy way use parsing representation draw tree python change representation somehow
Lemmatization taking forever with Spacy,"<p>I'm trying to lemmatize chat registers in a dataframe using spacy. My code is:</p>
<pre><code>nlp = spacy.load(&quot;es_core_news_sm&quot;)
df[&quot;text_lemma&quot;] = df[&quot;text&quot;].apply(lambda row: &quot; &quot;.join([w.lemma_ for w in nlp(row)]))
</code></pre>
<p>I have aprox 600.000 rows and the apply takes more than two hours to execute. Is there a faster package/way to lemmatize? (I need a solution that works for spanish)</p>
<p>I have only tried using spacy package</p>
",Multilingual Language Processing & Language Identification,lemmatization taking forever spacy trying lemmatize chat register dataframe using spacy code aprox row apply take two hour execute faster package way lemmatize need solution work spanish tried using spacy package
Combining a Tokenizer into a Grammar and Parser with NLTK,"<p>I am making my way through the NLTK book and I can't seem to do something that would appear to be a natural first step for building a decent grammar.</p>

<p>My goal is to build a grammar for a particular text corpus. </p>

<p><em>(Initial question: Should I even try to start a grammar from scratch or should I start with a predefined grammar? If I should start with another grammar, which is a good one to start with for English?)</em></p>

<p>Suppose I have the following simple grammar:</p>

<pre><code>simple_grammar = nltk.parse_cfg(""""""
S -&gt; NP VP
PP -&gt; P NP
NP -&gt; Det N | Det N PP
VP -&gt; V NP | VP PP
Det -&gt; 'a' | 'A'
N -&gt; 'car' | 'door'
V -&gt; 'has'
P -&gt; 'in' | 'for'
 """""");
</code></pre>

<p>This grammar can parse a very simple sentence, such as:</p>

<pre><code>parser = nltk.ChartParser(simple_grammar)
trees = parser.nbest_parse(""A car has a door"")
</code></pre>

<p>Now I want to extend this grammar to handle sentences with other nouns and verbs. How do I add those nouns and verbs to my grammar without manually defining them in the grammar? </p>

<p>For example, suppose I want to be able to parse the sentence ""A car has wheels"". I know that the supplied tokenizers can <em>magically</em> figure out which words are verbs/nouns, etc. How can I use the output of the tokenizer to tell the grammar that ""wheels"" is a noun?</p>
",Multilingual Language Processing & Language Identification,combining tokenizer grammar parser nltk making way nltk book seem something would appear natural first step building decent grammar goal build grammar particular text corpus initial question even try start grammar scratch start predefined grammar start another grammar good one start english suppose following simple grammar grammar parse simple sentence want extend grammar handle sentence noun verb add noun verb grammar without manually defining grammar example suppose want able parse sentence car ha wheel know supplied tokenizers magically figure word verb noun etc use output tokenizer tell grammar wheel noun
Stemmers vs Lemmatizers,"<p>Natural Language Processing (NLP), especially for English, has evolved into the stage where stemming would become an archaic technology if ""perfect"" lemmatizers exist. It's because stemmers change the surface form of a word/token into some meaningless stems. </p>

<p>Then again the definition of the ""perfect"" lemmatizer is questionable because different NLP task would have required different level of lemmatization. E.g. <a href=""https://stackoverflow.com/questions/14489309/convert-words-between-verb-noun-adjective-forms"">Convert words between verb/noun/adjective forms</a>. </p>

<p><strong>Stemmers</strong> </p>

<pre><code>[in]: having
[out]: hav
</code></pre>

<p><strong>Lemmatizers</strong></p>

<pre><code>[in]: having
[out]: have
</code></pre>

<ul>
<li><p>So the question is, are English stemmers any useful at all today? Since we have a plethora of lemmatization tools for English</p></li>
<li><p>If not, then how should we move on to build robust lemmatizers that
can take on <code>nounify</code>, <code>verbify</code>, <code>adjectify</code> and <code>adverbify</code>
preprocesses?</p></li>
<li><p>How could the lemmatization task be easily scaled to other languages
that have similar morphological structures as English?</p></li>
</ul>
",Multilingual Language Processing & Language Identification,stemmer v lemmatizers natural language processing nlp especially english ha evolved stage stemming would become archaic technology perfect lemmatizers exist stemmer change surface form word token meaningless stem definition perfect lemmatizer different nlp task would required different level lemmatization e g href word verb noun adjective form stemmer lemmatizers question english stemmer useful today since plethora lemmatization tool english move build robust lemmatizers take preprocesses could lemmatization task easily scaled language similar morphological structure english
How to exclude PII data and some Org level words from translation,"<p>I am building a translation pipeline for my organization using Helsinki-NLP Out Of Box translation models (no fine-tuning involved). I have a requirement where I have to retain certain PII data (like email, mobile number, and geographic data) and a few organization-based glossary words as-is in the translated text.</p>
<p>I understand that the quality of translation might get affected but I am ok with it.</p>
<p>Can someone suggest a way to achieve this with pre-trained models?</p>
",Multilingual Language Processing & Language Identification,exclude pii data org level word translation building translation pipeline organization using helsinki nlp box translation model fine tuning involved requirement retain certain pii data like email mobile number geographic data organization based glossary word translated text understand quality translation might get affected ok someone suggest way achieve pre trained model
How to remove Rows that have English (or a specific languages) sentences in pandas,"<p>I have a pandas data frame which has 2 columns, first contains Arabic sentences and the second one contain labels (1,0)</p>
<p>I want to remove all rows that contain English sentences.</p>
<p>Any suggestions?</p>
<p>Here is an example, I want to remove the second row</p>
<blockquote>
<p>ÿ•Ÿäÿ∑ÿßŸÑŸäÿß ŸÑÿ™ÿ≥ÿ±Ÿäÿπ ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™ ÿßŸÑŸÑÿ¨Ÿàÿ° ÿßŸÑŸÖŸáÿßÿ¨ÿ±ŸäŸÜÿå ÿßŸÑÿ™ÿ±ÿ≠ŸäŸÑ [0]</p>
<p>Border Patrol Agents Recover 44 Migrants from Stash House [0]</p>
<p>ÿßŸÑÿØŸäŸÖŸÇÿ±ÿßÿ∑ŸäŸàŸÜ ŸÖŸàÿßÿ¨Ÿáÿ© ÿßŸÑÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™ ÿ±ŸÅÿ∂ ÿπŸÇÿØ ÿßÿ¨ÿ™ŸÖÿßÿπÿßÿ™ ÿ™ÿßŸàŸÜ ŸáŸàŸÑ  [0]</p>
<p>ÿ¥ÿßŸáÿØ ŸÑÿßŸäŸÅ: ÿßÿ≠ÿ™ŸÅÿßŸÑ ÿ™ÿ±ÿßŸÖÿ® &quot;ÿßÿ¨ÿπŸÑ ÿ£ŸÖÿ±ŸäŸÉÿß ÿπÿ∏ŸäŸÖÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ&quot; - ÿ®ÿ±Ÿäÿ™ÿ®ÿßÿ±ÿ™   [0]</p>
<p>ÿßŸÑŸÖÿ∫ŸÜŸä ÿßŸÑÿ®ÿ±Ÿäÿ∑ÿßŸÜŸä ÿ•ŸÖ ÿ¢Ÿä ÿ•ŸäŸá: ÿ•ÿØÿßÿ±ÿ© ÿ™ÿ±ÿßŸÖÿ® ŸÖŸÑŸäÿ¶ÿ© ÿ®ŸÄ &quot;ŸÉÿ∞ÿßÿ®ŸàŸÜ ÿ®ÿßÿ´ŸàŸÑŸàÿ¨Ÿä&quot;    [0]</p>
</blockquote>
",Multilingual Language Processing & Language Identification,remove row english specific language sentence panda panda data frame ha column first contains arabic sentence second one contain label want remove row contain english sentence suggestion example want remove second row border patrol agent recover migrant stash house
&#39;BertModel&#39; object has no attribute &#39;bert&#39; error german bert model,"<p>i want to replicate the work in this repo
<a href=""https://github.com/theartificialguy/NLP-with-Deep-Learning/blob/master/BERT/Multi-Class%20classification%20TF-BERT/multi_class.ipynb"" rel=""nofollow noreferrer"">https://github.com/theartificialguy/NLP-with-Deep-Learning/blob/master/BERT/Multi-Class%20classification%20TF-BERT/multi_class.ipynb</a>
but with german texts and using other labels
i prepared my data and went through the steps doing the necessary modifications
instead of
<code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')</code>
i used <code>tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/bert-base-german-cased&quot;)</code>
from this documentation <a href=""https://huggingface.co/dbmdz/bert-base-german-cased"" rel=""nofollow noreferrer"">https://huggingface.co/dbmdz/bert-base-german-cased</a>
and
instead of
<code>model = TFBertModel.from_pretrained('bert-base-cased')</code>
i used <code>model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-german-cased&quot;)</code>
then when i came to the point where i had to execute this part</p>
<pre><code># defining 2 input layers for input_ids and attn_masks
input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')

bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -&gt; activation layer (3D), 
1 -&gt; pooled output layer (2D)
intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer') 
(bert_embds)
output_layer = tf.keras.layers.Dense(5, activation='softmax', name='output_layer') 
(intermediate_layer) # softmax -&gt; calcs probs of classes

sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)
sentiment_model.summary()
</code></pre>
<p>i had this error</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-42-ed437bbb2d3e&gt; in &lt;module&gt;
  3 attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')
  4 
 ----&gt; 5 bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -&gt; activation 
 layer (3D), 1 -&gt; pooled output layer (2D)
  6 intermediate_layer = tf.keras.layers.Dense(512, activation='relu', 
 name='intermediate_layer')(bert_embds)
  7 output_layer = tf.keras.layers.Dense(5, activation='softmax', name='output_layer') 
  (intermediate_layer) # softmax -&gt; calcs probs of classes

   /usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py in __getattr__(self, name)
  1263             if name in modules:
  1264                 return modules[name]
  -&gt; 1265         raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
  1266             type(self).__name__, name))
  1267 

  AttributeError: 'BertModel' object has no attribute 'bert'
</code></pre>
",Multilingual Language Processing & Language Identification,bertmodel object ha attribute bert error german bert model want replicate work repo german text using label prepared data went step necessary modification instead used documentation instead used came point execute part error
Predicting Missing Words in a sentence - Natural Language Processing Model,"<p>I have the sentence below :</p>

<pre><code>I want to ____ the car because it is cheap.
</code></pre>

<p>I want to predict the missing word ,using an NLP model. What NLP model shall I use? Thanks.</p>
",Multilingual Language Processing & Language Identification,predicting missing word sentence natural language processing model sentence want predict missing word using nlp model nlp model shall use thanks
Using NLP to process leave absence requests,"<p>I'm developing an online leave management system. The plan is to integrate a Natural Language Processing (NLP) component such that the system can decide whether leave request should be accepted or not. For example:</p>
<p>If someone requests leave for a funeral, the system should give this request a high priority. If someone requests leave for a <em>family</em> funeral, it should get a higher priority relative to the previous funeral leave request. Conversely, if someone requests leave for a simple trip/holiday, the system should assign a lower priority relative to the other leave requests.</p>
<p>I'm using the <code>.net</code> framework and <code>C#</code> programming language. Could someone please suggest how I can:</p>
<ul>
<li>Integrate NLP to understand the leave requests and assign priority to each request</li>
<li>Accept high priority leave requests and reject lower priority leave requests</li>
</ul>
",Multilingual Language Processing & Language Identification,using nlp process leave absence request developing online leave management system plan integrate natural language processing nlp component system decide whether leave request accepted example someone request leave funeral system give request high priority someone request leave family funeral get higher priority relative previous funeral leave request someone request leave simple trip system assign lower priority relative leave request using framework programming language could someone please suggest integrate nlp understand leave request assign priority request accept high priority leave request reject lower priority leave request
Convert chinese characters to hanyu pinyin,"<p>How to convert <strong><em>from</em></strong> chinese characters <strong><em>to</em></strong> hanyu pinyin?</p>

<p>E.g.</p>

<p>‰Ω† --> N«ê</p>

<p>È©¨ --> M«é</p>

<hr>

<p>More Info:</p>

<p>Either accents or numerical forms of hanyu pinyin are acceptable, the numerical form being my preference.</p>

<p>A Java library is preferred, however, a library in another language that can be put in a wrapper is also OK.</p>

<p>I would like anyone who has <strong><em>personally used</em></strong> such a library before to recommend or comment on it, in terms of its quality/ reliabilitty.</p>
",Multilingual Language Processing & Language Identification,convert chinese character hanyu pinyin convert chinese character hanyu pinyin e g n info either accent numerical form hanyu pinyin acceptable numerical form preference java library preferred however library another language put wrapper also ok would like anyone ha personally used library recommend comment term quality reliabilitty
Split string into segments according to the alphabet,"<p>I want to split the given string into alphabet segments that the string contains. So for example, if the following string is given:</p>
<pre><code>Los eventos automovil√≠sticos comenzaron poco despu√©s de la construcci√≥n exitosa de los primeros autom√≥viles a gasolina. El veloz zorro marr√≥n salt√≥ sobre el perezoso perro.

Motoring events began soon after the construction of the first successful gasoline-fueled automobiles. The quick brown fox jumped over the lazy dog.

–ú–æ—Ç–æ—Ä–∏ —Å—É –ø–æ—á–µ–ª–∏ —É–±—Ä–∑–æ –Ω–∞–∫–æ–Ω –∏–∑–≥—Ä–∞–¥—ö–µ –ø—Ä–≤–∏—Ö —É—Å–ø–µ—à–Ω–∏—Ö –∞—É—Ç–æ–º–æ–±–∏–ª–∞ –Ω–∞ –±–µ–Ω–∑–∏–Ω.–ë—Ä–∑–∞ —Å–º–µ—í–∞ –ª–∏—Å–∏—Ü–∞ —ò–µ –ø—Ä–µ—Å–∫–æ—á–∏–ª–∞ –ª–µ—ö–æ–≥ –ø—Å–∞.

–ê–≤—Ç–æ–º–æ–±–∏–ª–Ω–∏—Ç–µ —Å—ä–±–∏—Ç–∏—è –∑–∞–ø–æ—á–Ω–∞—Ö–∞ —Å–∫–æ—Ä–æ —Å–ª–µ–¥ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–∞–Ω–µ—Ç–æ –Ω–∞ –ø—ä—Ä–≤–∏—Ç–µ —É—Å–ø–µ—à–Ω–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ —Å –±–µ–Ω–∑–∏–Ω–æ–≤–æ –≥–æ—Ä–∏–≤–æ. –ë—ä—Ä–∑–∞—Ç–∞ –∫–∞—Ñ—è–≤–∞ –ª–∏—Å–∏—Ü–∞ –ø—Ä–µ—Å–∫–æ—á–∏ –º—ä—Ä–∑–µ–ª–∏–≤–æ—Ç–æ –∫—É—á–µ.

Ëá™ÂãïËªä„Ç§„Éô„É≥„Éà„ÅØ„ÄÅÊúÄÂàù„ÅÆÊàêÂäü„Åó„Åü„Ç¨„ÇΩ„É™„É≥ÁáÉÊñôËá™ÂãïËªä„ÅÆË£ΩÈÄ†Áõ¥Âæå„Å´Âßã„Åæ„Çä„Åæ„Åó„Åü„ÄÇ Á¥†Êó©„ÅÑËå∂Ëâ≤„ÅÆ„Ç≠„ÉÑ„Éç„ÅØÊÄ†„ÅëËÄÖ„ÅÆÁä¨„ÇíÈ£õ„Å≥Ë∂ä„Åà„Åæ„Åó„Åü„ÄÇ

ÿ®ÿØÿ£ÿ™ ÿ£ÿ≠ÿØÿßÿ´ ÿßŸÑÿ≥Ÿäÿßÿ±ÿßÿ™ ÿ®ÿπÿØ ŸàŸÇÿ™ ŸÇÿµŸäÿ± ŸÖŸÜ ÿ®ŸÜÿßÿ° ÿ£ŸàŸÑ ÿ≥Ÿäÿßÿ±ÿ© ŸÜÿßÿ¨ÿ≠ÿ© ÿ™ÿπŸÖŸÑ ÿ®ÿßŸÑÿ®ŸÜÿ≤ŸäŸÜ. ŸÇŸÅÿ≤ ÿßŸÑÿ´ÿπŸÑÿ® ÿßŸÑÿ®ŸÜŸä ÿßŸÑÿ≥ÿ±Ÿäÿπ ŸÅŸàŸÇ ÿßŸÑŸÉŸÑÿ® ÿßŸÑŸÉÿ≥ŸàŸÑ.
</code></pre>
<p>The above text contains spanish, english, serbian, bulgarian, japanese, arabic paragraphs (the order of the languages follows the paragraphs order).</p>
<p>Then, after applying some magic function, I would like to get the following output:</p>
<pre><code>{
    &quot;langs&quot;: [
        {
            &quot;alphabet&quot;: &quot;latin&quot;,
            &quot;text&quot;: &quot;Los eventos automovil√≠sticos comenzaron poco despu√©s de la construcci√≥n exitosa de los primeros autom√≥viles a gasolina. El veloz zorro marr√≥n salt√≥ sobre el perezoso perro. Motoring events began soon after the construction of the first successful gasoline-fueled automobiles. The quick brown fox jumped over the lazy dog.&quot;
        },
        {
            &quot;alphabet&quot;: &quot;cyrillic&quot;,
            &quot;text&quot;: &quot;–ú–æ—Ç–æ—Ä–∏ —Å—É –ø–æ—á–µ–ª–∏ —É–±—Ä–∑–æ –Ω–∞–∫–æ–Ω –∏–∑–≥—Ä–∞–¥—ö–µ –ø—Ä–≤–∏—Ö —É—Å–ø–µ—à–Ω–∏—Ö –∞—É—Ç–æ–º–æ–±–∏–ª–∞ –Ω–∞ –±–µ–Ω–∑–∏–Ω.–ë—Ä–∑–∞ —Å–º–µ—í–∞ –ª–∏—Å–∏—Ü–∞ —ò–µ –ø—Ä–µ—Å–∫–æ—á–∏–ª–∞ –ª–µ—ö–æ–≥ –ø—Å–∞. –ê–≤—Ç–æ–º–æ–±–∏–ª–Ω–∏—Ç–µ —Å—ä–±–∏—Ç–∏—è –∑–∞–ø–æ—á–Ω–∞—Ö–∞ —Å–∫–æ—Ä–æ —Å–ª–µ–¥ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–∞–Ω–µ—Ç–æ –Ω–∞ –ø—ä—Ä–≤–∏—Ç–µ —É—Å–ø–µ—à–Ω–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ —Å –±–µ–Ω–∑–∏–Ω–æ–≤–æ –≥–æ—Ä–∏–≤–æ. –ë—ä—Ä–∑–∞—Ç–∞ –∫–∞—Ñ—è–≤–∞ –ª–∏—Å–∏—Ü–∞ –ø—Ä–µ—Å–∫–æ—á–∏ –º—ä—Ä–∑–µ–ª–∏–≤–æ—Ç–æ –∫—É—á–µ.&quot;
        },
        {
            &quot;alphabet&quot;: &quot;japanese&quot;,
            &quot;text&quot;: &quot;Ëá™ÂãïËªä„Ç§„Éô„É≥„Éà„ÅØ„ÄÅÊúÄÂàù„ÅÆÊàêÂäü„Åó„Åü„Ç¨„ÇΩ„É™„É≥ÁáÉÊñôËá™ÂãïËªä„ÅÆË£ΩÈÄ†Áõ¥Âæå„Å´Âßã„Åæ„Çä„Åæ„Åó„Åü„ÄÇ Á¥†Êó©„ÅÑËå∂Ëâ≤„ÅÆ„Ç≠„ÉÑ„Éç„ÅØÊÄ†„ÅëËÄÖ„ÅÆÁä¨„ÇíÈ£õ„Å≥Ë∂ä„Åà„Åæ„Åó„Åü„ÄÇ&quot;
        },
        {
            &quot;alphabet&quot;: &quot;arabic&quot;,
            &quot;text&quot;: &quot;ÿ®ÿØÿ£ÿ™ ÿ£ÿ≠ÿØÿßÿ´ ÿßŸÑÿ≥Ÿäÿßÿ±ÿßÿ™ ÿ®ÿπÿØ ŸàŸÇÿ™ ŸÇÿµŸäÿ± ŸÖŸÜ ÿ®ŸÜÿßÿ° ÿ£ŸàŸÑ ÿ≥Ÿäÿßÿ±ÿ© ŸÜÿßÿ¨ÿ≠ÿ© ÿ™ÿπŸÖŸÑ ÿ®ÿßŸÑÿ®ŸÜÿ≤ŸäŸÜ. ŸÇŸÅÿ≤ ÿßŸÑÿ´ÿπŸÑÿ® ÿßŸÑÿ®ŸÜŸä ÿßŸÑÿ≥ÿ±Ÿäÿπ ŸÅŸàŸÇ ÿßŸÑŸÉŸÑÿ® ÿßŸÑŸÉÿ≥ŸàŸÑ.&quot;
        }
    ]
}
</code></pre>
<p>As you see, some of the languages are grouped by their family alphabets. For example, spanish and english paragraphs were grouped as latin, or serbian and bulgarian paragraphs were grouped as cyrillic. This is because it is hard to find a specific language (since most of the letters are shared between languages).</p>
<p>Ideally, my final output should be like this:</p>
<pre><code>{
    &quot;langs&quot;: [
        {
            &quot;lang&quot;: &quot;spanish&quot;,
            &quot;text&quot;: &quot;Los eventos automovil√≠sticos comenzaron poco despu√©s de la construcci√≥n exitosa de los primeros autom√≥viles a gasolina. El veloz zorro marr√≥n salt√≥ sobre el perezoso perro.&quot;
        },
        {
            &quot;lang&quot;: &quot;english&quot;,
            &quot;text&quot;: &quot;Motoring events began soon after the construction of the first successful gasoline-fueled automobiles. The quick brown fox jumped over the lazy dog.&quot;
        },
        {
            &quot;lang&quot;: &quot;serbian&quot;,
            &quot;text&quot;: &quot;–ú–æ—Ç–æ—Ä–∏ —Å—É –ø–æ—á–µ–ª–∏ —É–±—Ä–∑–æ –Ω–∞–∫–æ–Ω –∏–∑–≥—Ä–∞–¥—ö–µ –ø—Ä–≤–∏—Ö —É—Å–ø–µ—à–Ω–∏—Ö –∞—É—Ç–æ–º–æ–±–∏–ª–∞ –Ω–∞ –±–µ–Ω–∑–∏–Ω.–ë—Ä–∑–∞ —Å–º–µ—í–∞ –ª–∏—Å–∏—Ü–∞ —ò–µ –ø—Ä–µ—Å–∫–æ—á–∏–ª–∞ –ª–µ—ö–æ–≥ –ø—Å–∞.&quot;
        },
        {
            &quot;lang&quot;: &quot;bulgarian&quot;,
            &quot;text&quot;:&quot;–ê–≤—Ç–æ–º–æ–±–∏–ª–Ω–∏—Ç–µ —Å—ä–±–∏—Ç–∏—è –∑–∞–ø–æ—á–Ω–∞—Ö–∞ —Å–∫–æ—Ä–æ —Å–ª–µ–¥ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–∞–Ω–µ—Ç–æ –Ω–∞ –ø—ä—Ä–≤–∏—Ç–µ —É—Å–ø–µ—à–Ω–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ —Å –±–µ–Ω–∑–∏–Ω–æ–≤–æ –≥–æ—Ä–∏–≤–æ. –ë—ä—Ä–∑–∞—Ç–∞ –∫–∞—Ñ—è–≤–∞ –ª–∏—Å–∏—Ü–∞ –ø—Ä–µ—Å–∫–æ—á–∏ –º—ä—Ä–∑–µ–ª–∏–≤–æ—Ç–æ –∫—É—á–µ.&quot;
        },
        {
            &quot;lang&quot;: &quot;japanese&quot;,
            &quot;text&quot;: &quot;Ëá™ÂãïËªä„Ç§„Éô„É≥„Éà„ÅØ„ÄÅÊúÄÂàù„ÅÆÊàêÂäü„Åó„Åü„Ç¨„ÇΩ„É™„É≥ÁáÉÊñôËá™ÂãïËªä„ÅÆË£ΩÈÄ†Áõ¥Âæå„Å´Âßã„Åæ„Çä„Åæ„Åó„Åü„ÄÇ Á¥†Êó©„ÅÑËå∂Ëâ≤„ÅÆ„Ç≠„ÉÑ„Éç„ÅØÊÄ†„ÅëËÄÖ„ÅÆÁä¨„ÇíÈ£õ„Å≥Ë∂ä„Åà„Åæ„Åó„Åü„ÄÇ&quot;
        },
        {
            &quot;lang&quot;: &quot;arabic&quot;,
            &quot;text&quot;: &quot;ÿ®ÿØÿ£ÿ™ ÿ£ÿ≠ÿØÿßÿ´ ÿßŸÑÿ≥Ÿäÿßÿ±ÿßÿ™ ÿ®ÿπÿØ ŸàŸÇÿ™ ŸÇÿµŸäÿ± ŸÖŸÜ ÿ®ŸÜÿßÿ° ÿ£ŸàŸÑ ÿ≥Ÿäÿßÿ±ÿ© ŸÜÿßÿ¨ÿ≠ÿ© ÿ™ÿπŸÖŸÑ ÿ®ÿßŸÑÿ®ŸÜÿ≤ŸäŸÜ. ŸÇŸÅÿ≤ ÿßŸÑÿ´ÿπŸÑÿ® ÿßŸÑÿ®ŸÜŸä ÿßŸÑÿ≥ÿ±Ÿäÿπ ŸÅŸàŸÇ ÿßŸÑŸÉŸÑÿ® ÿßŸÑŸÉÿ≥ŸàŸÑ.&quot;
        }
    ]
}
</code></pre>
<p>I need to split the text into sub-strings according to the language. For that I am planning to use <a href=""https://github.com/aboSamoor/pycld2"" rel=""nofollow noreferrer""><code>cld2</code></a> which can split text into sentences, but according to my experiments, it does not do well when the string contains text with mixed alphabets (i.e. cyrillic + japanese etc.). However, <code>cld2</code> does well on the text with mixed languages that share the family of alphabets (i.e. french + english etc.).</p>
<p>That's why, I am planning to split the text into sub-strings by the family of alphabets, then for each of the family, I will aplly <code>cld2</code> to predict the specific language.</p>
<p>Another important requirements:</p>
<ul>
<li>the mixed languages might not be separated clearly by lines like above example (I did that for the sake of simplicity and to make the problem clear)</li>
<li>I need to be able to do this 'offline' without connecting to 3rd party servers like google etc. (since there will be tons of data that need to be handled)</li>
</ul>
<p>I would appreciate any ideas that you might have on the above problems. Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,split string segment according alphabet want split given string alphabet segment string contains example following string given text contains spanish english serbian bulgarian japanese arabic paragraph order language follows paragraph order applying magic function would like get following output see language grouped family alphabet example spanish english paragraph grouped latin serbian bulgarian paragraph grouped cyrillic hard find specific language since letter shared language ideally final output like need split text sub string according language planning use split text sentence according experiment doe well string contains text mixed alphabet e cyrillic japanese etc however doe well text mixed language share family alphabet e french english etc planning split text sub string family alphabet family aplly predict specific language another important requirement mixed language might separated clearly line like example sake simplicity make problem clear need able offline without connecting rd party server like google etc since ton data need handled would appreciate idea might problem thanks advance
Stanford Stanza NLP to networkx: superimpose NER entities onto graph of words,"<p>Here is a sample program which will take a text (example is in italian but Stanza supports many languages) and builds and displays a graph of the words (only certain Parts of Speech) and their syntactic relationships:</p>
<pre><code>&quot;&quot;&quot;
Sample program to analyze a phrase with Stanfords STANZA and
build a networkx graph selecting certain token types
&quot;&quot;&quot;
import stanza
import networkx as nx
import matplotlib.pyplot as plt

IN_TXT = &quot;All'ippica disse nel 1999 Ignazio Larussa. Poi a Taranto svenne!&quot;
LANG = &quot;it&quot;
ALLOWED_POS = [&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;VERB&quot;]
NXDOPTS = {
    &quot;node_color&quot;: &quot;orange&quot;,
    &quot;edge_color&quot;: &quot;powderblue&quot;,
    &quot;node_size&quot;: 400,
    &quot;width&quot;: 2,
}


# download appropriate language model
stanza.download(LANG, verbose=False)
nlp = stanza.Pipeline(
    LANG,
    processors=&quot;tokenize, pos, lemma, mwt, ner, depparse&quot;,
    verbose=False,
    use_gpu=False,
)
# generate a STANZA document from the given text via the pipeline
doc = nlp(IN_TXT)


for sentence in doc.sentences:
    # initialize G as MultiDiGraph for every sentence
    G = nx.DiGraph()
    # fill the graph with the relevant data
    G.add_node(0, id=0, text=&quot;ROOT&quot;, lemma=&quot;ROOT&quot;, upos=&quot;ROOT&quot;)  # fictitious ROOT node
    for word in [w for w in sentence.words if w.pos in ALLOWED_POS]:
        wordict = word.to_dict()
        G.add_node(wordict[&quot;id&quot;], **wordict)
        G.add_edge(wordict[&quot;id&quot;], wordict[&quot;head&quot;], label=wordict[&quot;deprel&quot;])
    # compute graph drawing parameters
    pos = nx.spring_layout(G)
    nodelabels = nx.get_node_attributes(G, &quot;text&quot;)
    edgelabels = nx.get_edge_attributes(G, &quot;label&quot;)
    # and now display the graphs
    nx.draw(G, pos, with_labels=True, labels=nodelabels, **NXDOPTS)
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edgelabels)
    plt.margins(0.2)
    plt.suptitle(sentence.text)
    plt.show()
    # clear the graph to repeat for another sentence
    G.clear()
</code></pre>
<p>and here is the resulting graph:
<a href=""https://i.sstatic.net/cpLSj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cpLSj.png"" alt=""enter image description here"" /></a>
The NLP pipeline also performs a NER on the same text and here are the two recognized name entities (a person and a place:</p>
<pre><code>[{
  &quot;text&quot;: &quot;Ignazio Larussa&quot;,
  &quot;type&quot;: &quot;PER&quot;,
  &quot;start_char&quot;: 26,
  &quot;end_char&quot;: 41
}, {
  &quot;text&quot;: &quot;Taranto&quot;,
  &quot;type&quot;: &quot;LOC&quot;,
  &quot;start_char&quot;: 49,
  &quot;end_char&quot;: 56
}]
</code></pre>
<p>I would like to be able to map these two on the graph above.</p>
<p>In Taranto's case where the NER span equals the word span, I guess I just need to map the NER span onto the right word start and end chars.</p>
<p>In &quot;Ignazio Larussa&quot;'s case things are more tricky, since the NER span encompasses TWO words. So in this case I want to show only ONE node with the NER span &quot;Ignazio Larussa&quot; connected via the nsubj edge to the &quot;disse&quot; verb.</p>
<p>How would you handle networkx G graph to achieve this? Thanks a lot</p>
",Multilingual Language Processing & Language Identification,stanford stanza nlp networkx superimpose ner entity onto graph word sample program take text example italian stanza support many language build display graph word certain part speech syntactic relationship resulting graph nlp pipeline also performs ner text two recognized name entity person place would like able map two graph taranto case ner span equal word span guess need map ner span onto right word start end char ignazio larussa case thing tricky since ner span encompasses two word case want show one node ner span ignazio larussa connected via nsubj edge disse verb would handle networkx g graph achieve thanks lot
Which learning model should be chosen to predict text news tags?,"<p>I have a database of news texts (100000 samples). Half of the dataset is tagged, and half is not, what methodology can I use to analyze the remaining news and fill them with tags?</p>
<p>Data example:</p>
<blockquote>
<p>Text = A cap on the price of Russian oil will restrict Russia's revenues for its &quot;illegal war Ukraine&quot;, the US says. The cap, approved by Western allies on Friday, is aimed at stopping countries paying more than $60 (¬£48) for a barrel of seaborne Russian crude oil. The measure - due to come into force on Monday - intensifies Western pressure on Russia over the invasion‚Ä¶ [long test is cut]</p>
</blockquote>
<blockquote>
<p>Tags = ['russian', 'oil', 'war']</p>
</blockquote>
<p>I know how to use python, pandas. But I found only methods that predict whether the text is bad or good.</p>
",Multilingual Language Processing & Language Identification,learning model chosen predict text news tag database news text sample half dataset tagged half methodology use analyze remaining news fill tag data example text cap price russian oil restrict russia revenue illegal war ukraine u say cap approved western ally friday aimed stopping country paying barrel seaborne russian crude oil measure due come force monday intensifies western pressure russia invasion long test cut tag russian oil war know use python panda found method predict whether text bad good
Determine the difficulty of an english word,"<p>I am working a word based game. My word database contains around 10,000 english words (sorted alphabetically). I am planning to have 5 difficulty levels in the game. Level 1 shows the easiest words and Level 5 shows the most difficult words, relatively speaking.</p>

<p>I need to divide the 10,000 long words list into 5 levels, starting from the easiest words to difficult ones. I am looking for a program to do this for me.</p>

<p><strong>Can someone tell me if there is an algorithm or a method to quantitatively measure the difficulty of an english word?</strong> </p>

<p>I have some thoughts revolving around using the ""<em>word length</em>"" and ""<em>word frequency</em>"" as factors, and come up with a formula or something that accomplishes this.</p>
",Multilingual Language Processing & Language Identification,determine difficulty english word working word based game word database contains around english word sorted alphabetically planning difficulty level game level show easiest word level show difficult word relatively speaking need divide long word list level starting easiest word difficult one looking program someone tell algorithm method quantitatively measure difficulty english word thought revolving around using word length word frequency factor come formula something accomplishes
Java API for plural forms of English words,"<p>Are there any Java API(s) which will provide plural form of English words (e.g. <code>cacti</code> for <code>cactus</code>)?</p>
",Multilingual Language Processing & Language Identification,java api plural form english word java api provide plural form english word e g
how can we use translate fucntion of fairseq without bpe,"<p>i was using pretrained model of japanes translation , trained via fairseq <a href=""http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/"" rel=""nofollow noreferrer"">http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/</a></p>

<p>this contains following files</p>

<p>model.pt dict.en.txt dict.ja.txt spm.en.nopretok.model spm.en.nopretok.vocab spm.ja.nopretok.model spm.ja.nopretok.vocab</p>

<p>i wanted to use </p>

<pre><code>  from fairseq.models.transformer import TransformerModel
    zh2en = TransformerModel.from_pretrained(
      '/path/to/checkpoints',
      checkpoint_file='checkpoint_best.pt',
      data_name_or_path='data-bin/wmt17_zh_en_full',
      bpe='subword_nmt',
      bpe_codes='data-bin/wmt17_zh_en_full/zh.code'
    )
</code></pre>

<p>this function of fairseq but i do not have BPE codes file.</p>

<p>please suggest how can i use this pretrined model in python using fairseq</p>
",Multilingual Language Processing & Language Identification,use translate fucntion fairseq without bpe wa using pretrained model japanes translation trained via fairseq contains following file model pt dict en txt dict ja txt spm en nopretok model spm en nopretok vocab spm ja nopretok model spm ja nopretok vocab wanted use function fairseq bpe code file please suggest use pretrined model python using fairseq
Stanford-NLP Parser incorrectly splits my sentence,"<p>I'm using the Stanford Parser to parse my corpus (for Machine Translation) to constituency trees. I am able to get the parser to work just fine, both through the GUI and the command line, but one problem I'm having is how it basically 'defines a line'.</p>
<p>Usually, when working with a corpus, a sentence is an entire string of words until it reaches a new line. With the Stanford parser, it seems to take a sentence to be up until an 'end-of-sentence character' like a full stop or question mark. In some situations,this ends with incorrectly taking a portion of a sentence to be an entire sentence on its own, which inflates the number of sentences I have and causes misalignment with my target dataset.
Is there any way I can get the parser to take a sentence to be up until the <code>\n</code> newline, or is it just defined this way?</p>
",Multilingual Language Processing & Language Identification,stanford nlp parser incorrectly split sentence using stanford parser parse corpus machine translation constituency tree able get parser work fine gui command line one problem basically defines line usually working corpus sentence entire string word reach new line stanford parser seems take sentence end sentence character like full stop question mark situation end incorrectly taking portion sentence entire sentence inflates number sentence cause misalignment target dataset way get parser take sentence newline defined way
Mt5 language translation - Language in parameters,"<p>I'm trying to train a Arabic to English language translation model, and want to know if there is any option where I can mention the input and target language in the code.</p>
<p>I'm using the below code, where df1 is the dataframe with columns (prefix,input_text,target_text)</p>
<pre><code>from simpletransformers.t5 import T5Model, T5Args
model = T5Model(&quot;mt5&quot;, &quot;google/mt5-small&quot;, args=model_args, use_cuda=False)
model.train_model(df1,eval_data=df2)
results = model.eval_model(df2, verbose=True)
</code></pre>
",Multilingual Language Processing & Language Identification,mt language translation language parameter trying train arabic english language translation model want know option mention input target language code using code df dataframe column prefix input text target text
How can I determine if a word is a part of an english word or is a portmanteau (a word created by combining parts of valid English words)?,"<p>I am trying to create a validator that takes in words and tries to determine if the word is one of the following:</p>
<ol>
<li>It is a valid English word</li>
<li>It is a part of an English word</li>
<li>It is an abbreviation</li>
<li>It is a portmanteau -- a word created by concatenating parts of valid English words</li>
</ol>
<p>Are there Java or Python libraries/frameworks that can perform this task?</p>
<p><strong>Samples of words:</strong>  <em>meds, ppg, reauthorization, appmetadata, reconsent, rawlog</em></p>
<p>I've tried Python NLTK (cursory investigation so far) and a Python library called <em>enchant</em> (this fails to identify many valid words/parts of words and portmanteaus).</p>
",Multilingual Language Processing & Language Identification,determine word part english word portmanteau word created combining part valid english word trying create validator take word try determine word one following valid english word part english word abbreviation portmanteau word created concatenating part valid english word java python library framework perform task sample word med ppg reauthorization appmetadata reconsent rawlog tried python nltk cursory investigation far python library called enchant fails identify many valid word part word portmanteau
Regular expressions: contains at least two 0s but not consecutive 0s,"<p>Is the solution of this exercise the below regular expression? I found it in the internet but I don't believe that this is correct.</p>

<pre><code>(1*011*(0+011*))*
</code></pre>

<p>According to the theory of Chapter 1 in the book ""The handbook of computational linguistics and natural language processing"", how could I solve this exercise?</p>

<p>I would like a regular expression that will satisfy the below regular language</p>

<pre><code>    L = {010,0101,0110,0101010,01011110,.....}
</code></pre>
",Multilingual Language Processing & Language Identification,regular expression contains least two consecutive solution exercise regular expression found internet believe correct according theory chapter book handbook computational linguistics natural language processing could solve exercise would like regular expression satisfy regular language
Obtaining data from both token and word objects in a Stanza Document / Sentence,"<p>I am using a Stanford STANZA pipeline on some (italian) text.</p>
<p>Problem I'm grappling with is that I need data from BOTH the Token and Word objects.</p>
<p>While I'm able to access one or the other separately I'm not wrapping my head on how to get data from both in a single loop over the Document -&gt; Sentence</p>
<p>Specifically I need both some Word data (such as lemma, upos and head) but I also need to know the corresponding start and end position, which in my understanding I can find in the token.start_char and token.end_char.</p>
<p>Here's my code to test what I've achieved:</p>
<pre><code>import stanza
IN_TXT = '''Il paziente Rossi e' stato ricoverato presso il nostro reparto a seguito di accesso
  al pronto soccorso con diagnosi sospetta di aneurisma aorta
  addominale sottorenale. In data 12/11/2022 e' stato sottoposto ad asportazione dell'aneurisma
  con anastomosi aorto aortica con protesi in dacron da 20mm. Paziente dimesso in data odierna in 
  condizioni stabili.'''
stanza.download('it', verbose=False)
it_nlp = stanza.Pipeline('it', processors='tokenize,lemma,pos,depparse,ner',
                         verbose=False, use_gpu=False)
it_doc = it_nlp(IN_TXT)
# iterate through the Token objects
T = 0
for token in it_doc.iter_tokens():
    T += 1
    token_id = 'T' + str((T))
    token_start = token.start_char
    token_end = token.end_char
    token_text = token.text
    print(f&quot;{token_id}\t{token_start} {token_end} {token_text}&quot;)
# iterate through Word objects
print(*[f'word: {word.text}\t\t\tupos: {word.upos}\txpos: {word.xpos}\tfeats: {word.feats if word.feats else &quot;_&quot;}' for sent in it_doc.sentences for word in sent.words], sep='\n')
</code></pre>
<p>Here is the documentation of these objects: <a href=""https://stanfordnlp.github.io/stanza/data_objects.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/data_objects.html</a></p>
",Multilingual Language Processing & Language Identification,obtaining data token word object stanza document sentence using stanford stanza pipeline italian text problem grappling need data token word object able access one separately wrapping head get data single loop document sentence specifically need word data lemma upos head also need know corresponding start end position understanding find token start char token end char code test achieved documentation object
Spacy ValueError: [E002] Can&#39;t find factory for &#39;relation_extractor&#39; for language English (en),"<p>I want to train a &quot;relation extractor&quot; component as in <a href=""https://www.youtube.com/watch?v=8HL-Ap5_Axo"" rel=""nofollow noreferrer"">this tutorial</a>. I have 3 .spacy files (train.spacy, dev.spacy, test.spacy.</p>
<p>I run:</p>
<pre><code>python3 -m spacy init fill-config config.cfg config.cfg
</code></pre>
<p>followed by</p>
<pre><code>python3 -m spacy train --output ./model config.cfg --paths.train train.spacy --paths.dev dev.spacy
</code></pre>
<p>Output:</p>
<pre><code>ValueError: [E002] Can't find factory for 'relation_extractor' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).

Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer
</code></pre>
<p>I have tried the two config files <a href=""https://github.com/explosion/projects/tree/v3/tutorials/rel_component/configs"" rel=""nofollow noreferrer"">here</a> but the output is the same.</p>
<p>To enable Transformers I have installed spacy-transformers downloaded en_core_web_trf via</p>
<pre><code>python3 -m spacy download en_core_web_trf
</code></pre>
<p>A similar issue was mentioned <a href=""https://github.com/explosion/spaCy/discussions/7378"" rel=""nofollow noreferrer"">on GitHub</a> but that solution is for an other context. Similarly, <a href=""https://github.com/explosion/spaCy/issues/7345"" rel=""nofollow noreferrer"">on GitHub</a> somebody raised the same issue with no solution. <a href=""https://stackoverflow.com/questions/70809137/how-could-i-use-gpu-in-google-colab-to-train-spacy-relation-extraction-model-e"">Here too</a> was not solved.</p>
",Multilingual Language Processing & Language Identification,spacy valueerror e find factory relation extractor language english en want train relation extractor component tutorial spacy file train spacy dev spacy test spacy run followed output tried two config file output enable transformer installed spacy transformer downloaded en core web trf via similar issue wa mentioned github solution context similarly github somebody raised issue solution href wa solved
Change morphology of german words with Spacy,"<p>I am parsing Noun Chunks with the help of Spacy and I want to &quot;normalize&quot; those Noun Chunks, that means I want to change a given Noun from its grammatikal case to another grammatikal case (f.e. Genitiv to Nominativ). All I managed to achive so far ist to label the noun chunks using the Spacy morphologizer. Since this problem is quite unique to the german language i was not able to find much about this problem on the internet.</p>
<p>An Example would be turning this:</p>
<p>&quot;dortiger Verf√ºgung&quot;
dortiger Case=Dat|Degree=Pos|Gender=Fem|Number=Sing
Verf√ºgung Case=Dat|Gender=Fem|Number=Sing</p>
<p>into this:
&quot;dortige Verf√ºgung&quot;
dortige Case=Nom|Degree=Pos|Gender=Fem|Number=Sing
Verf√ºgung Case=Nom|Gender=Fem|Number=Sing</p>
",Multilingual Language Processing & Language Identification,change morphology german word spacy parsing noun chunk help spacy want normalize noun chunk mean want change given noun grammatikal case another grammatikal case f e genitiv nominativ managed achive far ist label noun chunk using spacy morphologizer since problem quite unique german language wa able find much problem internet example would turning dortiger verf gung dortiger case dat degree po gender fem number sing verf gung case dat gender fem number sing dortige verf gung dortige case nom degree po gender fem number sing verf gung case nom gender fem number sing
POS Tagger for declension of german words in Java,"<p>The <a href=""https://www.cis.uni-muenchen.de/%7Eschmid/tools/RFTagger/"" rel=""nofollow noreferrer"">RFTagger</a> is a Part-Of-Speech Tagger with very detailed tags for german words.</p>
<p>According to their website, output looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>word</th>
<th>part of speech</th>
</tr>
</thead>
<tbody>
<tr>
<td>Das</td>
<td><code>PRO.Dem.Subst.-3.Nom.Sg.Neut</code></td>
</tr>
<tr>
<td>ist</td>
<td><code>VFIN.Sein.3.Sg.Pres.Ind</code></td>
</tr>
<tr>
<td>ein</td>
<td><code>ART.Indef.Nom.Sg.Masc</code></td>
</tr>
<tr>
<td>Test</td>
<td><code>N.Reg.Nom.Sg.Masc</code></td>
</tr>
<tr>
<td>.</td>
<td><code>SYM.Pun.Sent</code></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The tags contain information about the respective word's declension. This information is what I need.</strong><br>However the <a href=""https://www.cis.uni-muenchen.de/%7Eschmid/tools/RFTagger/"" rel=""nofollow noreferrer"">RFTagger</a> is pretty outdated (available <a href=""http://sifnos.sfs.uni-tuebingen.de/resource/A4/rftj/"" rel=""nofollow noreferrer"">here</a> as Beta for Java 6).</p>
<p>Is there any alternative POSTagger out there that is able to give me the declension of a german word?</p>
",Multilingual Language Processing & Language Identification,po tagger declension german word java rftagger part speech tagger detailed tag german word according website output look like word part speech da ist ein test tag contain information respective word declension information need however rftagger pretty outdated available beta java alternative postagger able give declension german word
Text segmentation: dictionary-based word splitting,"<h2>Background</h2>

<p>Split database column names into equivalent English text to seed a data dictionary. The English dictionary is created from a corpus of corporate documents, wikis, and email. The dictionary (<code>lexicon.csv</code>) is a CSV file with words and probabilities. Thus, the more often someone writes the word ""therapist"" (in email or on a wiki page) the higher the chance of ""therapistname"" splits to ""therapist name"" as opposed to something else. (The lexicon probably won't even include the word rapist.)</p>

<h2>Source Code</h2>

<ul>
<li>TextSegmenter.java @ <a href=""http://pastebin.com/taXyE03L"" rel=""nofollow noreferrer"">http://pastebin.com/taXyE03L</a></li>
<li>SortableValueMap.java @ <a href=""http://pastebin.com/v3hRXYan"" rel=""nofollow noreferrer"">http://pastebin.com/v3hRXYan</a></li>
</ul>

<h2>Data Files</h2>

<ul>
<li>lexicon.csv - <a href=""http://pastebin.com/0crECtXY"" rel=""nofollow noreferrer"">http://pastebin.com/0crECtXY</a></li>
<li>columns.txt - <a href=""http://pastebin.com/EtN9Qesr"" rel=""nofollow noreferrer"">http://pastebin.com/EtN9Qesr</a></li>
</ul>

<h2>Problem (updated 2011-01-03)</h2>

<p>When the following problem is encountered:</p>

<pre><code>dependentrelationship::end depend ent dependent relationship
end=0.86
ent=0.001
dependent=0.8
relationship=0.9
</code></pre>

<p>These possible solutions exist:</p>

<pre><code>dependentrelationship::dependent relationship
dependentrelationship::dep end ent relationship
dependentrelationship::depend ent relationship
</code></pre>

<p>The lexicon contains words with their relative probabilities (based on word frequency): <code>dependent 0.8</code>, <code>end 0.86</code>, <code>relationship 0.9</code>, <code>depend 0.3</code>, and <code>ent 0.001</code>.</p>

<p>Eliminate the solution of <code>dep end ent relationship</code> because <code>dep</code> is not in the lexicon (i.e., 75% word usage), whereas the other two solutions cover 100% of words in the lexicon. Of the remaining solutions, the probability of <code>dependent relationship</code> is <em>0.72</em> whereas <code>depend ent relationship</code> is <em>0.00027</em>. We can therefore select <code>dependent relationship</code> as the correct solution.</p>

<h2>Related</h2>

<ul>
<li><a href=""https://stackoverflow.com/questions/3856630/how-to-separate-words-in-a-sentence-with-spaces"">How to separate words in a &quot;sentence&quot; with spaces?</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjMzMTEzYTA5NTk3NjFjOTc"" rel=""nofollow noreferrer"">Top Coder - Text Segmentation Presentation 1/2</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjQ1YmFiZTNhODVjMzY2MmY"" rel=""nofollow noreferrer"">Top Coder - Text Segmentation Presentation 2/2</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjYyN2I4NTA0OTIxZDNlMGE"" rel=""nofollow noreferrer"">Linear Text Segmentation using Dynamic Programming Algorithm</a></li>
<li><a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxkanBkZnN0b3JlfGd4OjUxZmFkNzc0MmMzOTc5ZmI"" rel=""nofollow noreferrer"">Dynamic Programming: Segmentation</a></li>
<li><a href=""https://sites.google.com/site/djpdfstore/text-segmentation-02.pdf?attredirects=0&amp;d=1"" rel=""nofollow noreferrer"">Dynamic Programming: A Computational Tool</a></li>
</ul>

<h2>Question</h2>

<p>Given:</p>

<pre><code>// The concatenated phrase or database column (e.g., dependentrelationship).
String concat;

// All words (String) in the lexicon within concat, in left-to-right order; and
// the ranked probability of those words (Double). (E.g., {end, 0.97}
// {dependent, 0.86}, {relationship, 0.95}.)
Map.Entry&lt;String, Double&gt; word;
</code></pre>

<p>How would you implement a routine that generates the most likely solution based on lexicon coverage and probabilities? For example:</p>

<pre><code>for( Map.Entry&lt;String, Double&gt; word : words ) {
  result.append( word.getKey() ).append( ' ' );

  // What goes here?

  System.out.printf( ""%s=%f\n"", word.getKey(), word.getValue() );
}
</code></pre>

<p>Thank you!</p>
",Multilingual Language Processing & Language Identification,text segmentation dictionary based word splitting background split database column name equivalent english text seed data dictionary english dictionary created corpus corporate document wikis email dictionary csv file word probability thus often someone writes word therapist email wiki page higher chance therapistname split therapist name opposed something else lexicon probably even include word rapist source code textsegmenter java sortablevaluemap java data file lexicon csv column txt problem updated following problem encountered possible solution exist lexicon contains word relative probability based word frequency eliminate solution lexicon e word usage whereas two solution cover word lexicon remaining solution probability whereas therefore select correct solution related top coder text segmentation presentation top coder text segmentation presentation linear text segmentation using dynamic programming algorithm dynamic programming segmentation dynamic programming computational tool question given would implement routine generates likely solution based lexicon coverage probability example thank
MT5 machine learning model for paraphrasing,"<p>I'm trying to create a machine learning model to paraphrase given Persian text. I was introduced to mt5 as a multilingual text-to-text model. However, I can't figure out how to implement this. I have gathered the data. Here's a sample of the data:
<a href=""https://i.sstatic.net/qz6MC.jpg"" rel=""nofollow noreferrer"">Data sample</a></p>
<p><strong>---UPDATE---</strong></p>
<p>I have tried to paraphrase using the T5 model, and it works well for English. However, I can't get logical results from the MT5 model. Here is the T5 version code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;Vamsi/T5_Paraphrase_Paws&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Vamsi/T5_Paraphrase_Paws&quot;)
</code></pre>
<pre><code>sentence = sentence_1

text =  &quot;paraphrase: &quot; + sentence + &quot; &lt;/s&gt;&quot;

encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=&quot;pt&quot;)
input_ids, attention_masks = encoding[&quot;input_ids&quot;], encoding[&quot;attention_mask&quot;]


outputs = model.generate(
    input_ids=input_ids, attention_mask=attention_masks,
    max_length=256,
    do_sample=True,
    top_k=120,
    top_p=0.95,
    early_stopping=False,
    num_return_sequences=5
)

print (&quot;\n&quot;)
print(&quot;Origianl sentence:&quot;)
print(sentence)
print (&quot;\n&quot;)
print(&quot;Paraphrasing:&quot;)

for output in outputs:
    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
    print(line)
</code></pre>
<p>When I give the following sentence to the model, it returns the following results:</p>
<p><em>Original sentence:</em></p>
<ul>
<li>Washing your hands Properly will keep you away from COVID-19.</li>
</ul>
<p><em>Paraphrasing:</em></p>
<ul>
<li>By properly washing your hands, you will keep away from COVID-19.</li>
<li>Washing your hands correctly will keep you away from COVID-19.</li>
<li>Washing your hands correctly will keep you away from COVID-19.</li>
<li>Washing your hands correctly will keep you from COVID-19.</li>
<li>Washing your hands properly will keep you away from COVID-19.</li>
</ul>
<p>But when I change the model to the MT5-base, the results are absurd. Here is an example:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;google/mt5-base&quot;)
</code></pre>
<p><em>Original sentence:</em></p>
<ul>
<li>Washing your hands Properly will keep you away from COVID-19.</li>
</ul>
<p><em>Paraphrasing:</em></p>
<ul>
<li>&lt;extra_id_0&gt;, left.</li>
<li>&lt;extra_id_0&gt;, also.</li>
<li>&lt;extra_id_0&gt;. Comment</li>
<li>&lt;extra_id_0&gt;.</li>
<li>&lt;extra_id_0&gt;o.</li>
</ul>
",Multilingual Language Processing & Language Identification,mt machine learning model paraphrasing trying create machine learning model paraphrase given persian text wa introduced mt multilingual text text model however figure implement gathered data sample data data sample update tried paraphrase using model work well english however get logical result mt model version code give following sentence model return following result original sentence washing hand properly keep away covid paraphrasing properly washing hand keep away covid washing hand correctly keep away covid washing hand correctly keep away covid washing hand correctly keep covid washing hand properly keep away covid change model mt base result absurd example original sentence washing hand properly keep away covid paraphrasing extra id left extra id also extra id comment extra id extra id
Use the google transliterate api in python,"<p>I am trying to use google transliterate<a href=""https://developers.google.com/transliterate/"" rel=""noreferrer""> [1] </a>  to convert hindi words written in english to hindi. e.g-<br>
Input text- Main sahi hun.<br>
Required text -‡§Æ‡•à‡§Ç ‡§∏‡§π‡•Ä ‡§π‡•Ç‡§Å  </p>

<p>I want to pass the input string to api and wants a required text in hindi language.
I am using google transliterate but as it was deprecated long time ago so can't find a suitable way to do it on python as currently the example they are providing is in javascript and not very beginner friendly. 
How to do it?</p>
",Multilingual Language Processing & Language Identification,use google transliterate api python trying use google transliterate convert hindi word written english hindi e g input text main sahi hun required text want pas input string api want required text hindi language using google transliterate wa deprecated long time ago find suitable way python currently example providing javascript beginner friendly
Run NLP-based search and replace on Latex files,"<p>I have a handful of Latex documents which mainly contain plain English text, with a little Latex commands, e.g. <code>\footnote</code>, <code>\cite</code>.</p>
<p>With <strong>spacy</strong> in Python, I have managed to run NLP on the files and analyze something.</p>
<p>Right now, I got a new requirement that rather than merely searching, I need to replace and save back the file.</p>
<p>I have 10+ keyword/replacement pairs. For example, if a word lemma is &quot;report&quot; and part of speech is verb, I have to surround it with <code>\it{}</code> (a Latex command). Don't do this with &quot;report&quot; whose part of speech is noun.</p>
<pre><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load(&quot;en_core_web_sm&quot;)

text = ...

text = removeLatexComments(text)
text = removeSectionHeadings(text)
# remove latex commands
text = re.sub(r'\\\w+\{(.+)\}', '', text)

doc = nlp(text)

matcher = Matcher(nlp.vocab)
pattern = [{'LEMMA': 'report'}, {'POS': 'verb'}]
matches = matcher(doc)
for m in matches:
    print(&quot;Found {}&quot;.format(doc[m[1]:m[2]]))
</code></pre>
<p>The code is what I have right now. I'm able to check if there are noun &quot;report&quot;. Then How do I run a replace and save it back?</p>
<p>Moreover, I have removed section headings and uninteresting Latex commands, but I have to keep them when I write my file back. Are there any tools to help me this?</p>
<p>I feel it's like Antlr4's rewriter... but introducing Antlr might be too laborious.</p>
",Multilingual Language Processing & Language Identification,run nlp based search replace latex file handful latex document mainly contain plain english text little latex command e g spacy python managed run nlp file analyze something right got new requirement rather merely searching need replace save back file keyword replacement pair example word lemma report part speech verb surround latex command report whose part speech noun code right able check noun report run replace save back moreover removed section heading uninteresting latex command keep write file back tool help feel like antlr rewriter introducing antlr might laborious
spacy matcher pattern IN + REGEX Tag,"<p>My goal is to match with spacy the sentences that contain one of the following words:
['studium','abschluss','ausbildung']</p>
<p>I can solve the problem with this line:</p>
<pre><code>pattern = [{&quot;LOWER&quot;: {'IN':['studium','abschluss', 'ausbildung']}}]
</code></pre>
<p>My problem is that in German there is a vast use of composed words like Hochschulstudium, Masterstudium, Studiengang etc.</p>
<p>How can use the regex inside the IN sentence to match all words containing the word Studium?</p>
",Multilingual Language Processing & Language Identification,spacy matcher pattern regex tag goal match spacy sentence contain one following word studium abschluss ausbildung solve problem line problem german vast use composed word like hochschulstudium masterstudium studiengang etc use regex inside sentence match word containing word studium
Word Tokenization When There is No Space,"<p>I am wondering for the term in Machine Learning, Deep Learning, or in Natural Language Processing that split the word in a paragraph when there is no space between them.</p>

<p>example:
""iwanttocook""</p>

<p>become:
""i want to cook""</p>

<p>It wouldn't be easy since you do not have the character to tokenize the word.</p>

<p>I appreciate any help</p>
",Multilingual Language Processing & Language Identification,word tokenization space wondering term machine learning deep learning natural language processing split word paragraph space example iwanttocook become want cook easy since character tokenize word appreciate help
Paraphrase recognition using sentence level similarity,"<p>I'm a new entrant to NLP (Natural Language Processing). As a start up project, I'm developing a paraphrase recognizer (a system which can recognize two similar sentences). For that recognizer I'm going to apply various measures at three levels, namely: lexical, syntactic and semantic. At the lexical level, there are multiple similarity measures like cosine similarity, matching coefficient, Jaccard coefficient, et cetera. For these measures I'm using the <code>simMetrics</code> package developed by the University of Sheffield which contains a lot of similarity measures. But for the Levenshtein distance and Jaro-Winkler distance measures, the code is only at <strong>character level</strong>, whereas I require code at the sentence level (i.e. considering a single word as a unit instead of character-wise). Additionally, there is no code for computing the Manhattan distance in <code>SimMetrics</code>. Are there any suggestions for how I could develop the required code (or someone provide me the code) at the sentence level for the above mentioned measures?</p>
<p>Thanks a lot in advance for your time and effort helping me.</p>
",Multilingual Language Processing & Language Identification,paraphrase recognition using sentence level similarity new entrant nlp natural language processing start project developing paraphrase recognizer system recognize two similar sentence recognizer going apply various measure three level namely lexical syntactic semantic lexical level multiple similarity measure like cosine similarity matching coefficient jaccard coefficient et cetera measure using package developed university sheffield contains lot similarity measure levenshtein distance jaro winkler distance measure code character level whereas require code sentence level e considering single word unit instead character wise additionally code computing manhattan distance suggestion could develop required code someone provide code sentence level mentioned measure thanks lot advance time effort helping
Make Spacy tokenizer not split on /,"<p>How do I modify the English tokenizer to prevent splitting tokens on the <code>'/'</code> character?</p>
<p>For example, the following string should be one token:</p>
<pre><code>
import spacy

nlp = spacy.load('en_core_web_md')
doc = nlp(&quot;12/AB/568793&quot;)

for t in doc:
    print(f&quot;[{t.pos_} {t.text}]&quot;)

# produces
#[NUM 12]
#[SYM /]
#[ADJ AB/568793]
</code></pre>
",Multilingual Language Processing & Language Identification,make spacy tokenizer split modify english tokenizer prevent splitting token character example following string one token
Open source options for non-English term extraction?,"<p>I am looking for a open source project that does term extraction with multiple languages.</p>
<p>I have already found <a href=""http://developer.yahoo.com/search/content/V1/termExtraction.html"" rel=""nofollow noreferrer"">Yahoo BOSS Term Extraction Web Service</a>, and it is good. However, it does not handle languages other than English.</p>
<p>Are there any open source term extraction projects that support more languages?</p>
<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,open source option non english term extraction looking open source project doe term extraction multiple language already found yahoo bos term extraction web service good however doe handle language english open source term extraction project support language thanks
How to handle LemmatizerTrainer &#39;UTFDataFormatException: encoded string too long&#39;?,"<p>I am using Opennlp to train a model for lemmatization of german words. Therefore I use the opennlp cli and the training set of <a href=""https://github.com/UniversalDependencies/UD_German-HDT/blob/master/README.md"" rel=""nofollow noreferrer"">UD_German-HDT</a> which can be downloaded <a href=""https://universaldependencies.org/#download"" rel=""nofollow noreferrer"">here</a></p>
<p>The training itself works fine (just need a little bit of ram) but the cli fails to write the model because of an <code>UTFDataFormatException: encoded string too long</code> exception.</p>
<p>The cli command I am using: <code>opennlp LemmatizerTrainerME.conllu -params params.txt -lang de -model de-lemmatizer.bin -data UD_German-HDT/de_hdt-ud-train.conllu -encoding UTF-8</code></p>
<p>Stacktrace:</p>
<pre><code>Writing lemmatizer model ... failed
Error during writing model file 'de-lemmatizer.bin'
encoded string too long: 383769 bytes
java.io.UTFDataFormatException: encoded string too long: 383769 bytes
        at java.base/java.io.DataOutputStream.writeUTF(DataOutputStream.java:364)
        at java.base/java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
        at opennlp.tools.ml.maxent.io.BinaryGISModelWriter.writeUTF(BinaryGISModelWriter.java:71)
        at opennlp.tools.ml.maxent.io.GISModelWriter.persist(GISModelWriter.java:97)
        at opennlp.tools.ml.model.GenericModelWriter.persist(GenericModelWriter.java:75)
        at opennlp.tools.util.model.ModelUtil.writeModel(ModelUtil.java:71)
        at opennlp.tools.util.model.GenericModelSerializer.serialize(GenericModelSerializer.java:36)
        at opennlp.tools.util.model.GenericModelSerializer.serialize(GenericModelSerializer.java:29)
        at opennlp.tools.util.model.BaseModel.serialize(BaseModel.java:597)
        at opennlp.tools.cmdline.CmdLineUtil.writeModel(CmdLineUtil.java:182)
        at opennlp.tools.cmdline.lemmatizer.LemmatizerTrainerTool.run(LemmatizerTrainerTool.java:77)
        at opennlp.tools.cmdline.CLI.main(CLI.java:256)
</code></pre>
<p>Has somebody encountered this problem and has a solution?</p>
",Multilingual Language Processing & Language Identification,handle lemmatizertrainer utfdataformatexception encoded string long using opennlp train model lemmatization german word therefore use opennlp cli training set ud german hdt downloaded training work fine need little bit ram cli fails write model exception cli command using stacktrace ha somebody encountered problem ha solution
Lemmatizer/PoS-tagger for italian in Python,"<p>I'm searching for a Lemmatizer/PoS-tagger for the Italian language, that works on Python. I tried with Spacy, it works but it's not very precise, expecially for verbs it often returns the wrong lemma. NLKT has only english as language. I'm searching for an optimized tool for the Italian language, does it exists?
If it doesn't exist, is it possible, given a corpus, to create it? Whats the work needed to create it?</p>
",Multilingual Language Processing & Language Identification,lemmatizer po tagger italian python searching lemmatizer po tagger italian language work python tried spacy work precise expecially verb often return wrong lemma nlkt ha english language searching optimized tool italian language doe exists exist possible given corpus create whats work needed create
NLTK Perceptron Tagger - What does it recognize as FW (foreign word)?,"<p>Relatively new to NLP and working on tagging sentences that contain foreign words using NLTK's PerceptronTagger (in Python) - but it continues to tag the tokenized foreign word by position in the syntax rather than as a 'FW'. </p>

<p>Does the whole sentence have to be in the language (with the appropriate language pickle file loaded) for the 'FW' tag to work ala the <a href=""http://www.nltk.org/_modules/nltk/tag.html"" rel=""nofollow noreferrer"">NLTK documentation</a>? Is there a way of sensing a foreign word within an English sentence?</p>

<p>On the flip side of that coin, do sentences containing foreign words that have been normalized into the English language tag as English? (ie: entrepreneur, siesta, zeitgeist, etc)</p>
",Multilingual Language Processing & Language Identification,nltk perceptron tagger doe recognize fw foreign word relatively new nlp working tagging sentence contain foreign word using nltk perceptrontagger python continues tag tokenized foreign word position syntax rather fw doe whole sentence language appropriate language pickle file loaded fw tag work ala nltk documentation way sensing foreign word within english sentence flip side coin sentence containing foreign word normalized english language tag english ie entrepreneur siesta zeitgeist etc
Neural Machine Translation,"<p>I am doing a project about Neural Machine Translation. In the data processing step, is it necessary to padding after the sentences so that they are of equal length?</p>
",Multilingual Language Processing & Language Identification,neural machine translation project neural machine translation data processing step necessary padding sentence equal length
I am trying to convert hinglish dataset to english using python,"<p>for i in range(10300):
sentence = df[&quot;tweet&quot;][i]
translations = translator.translate(sentence.encode('unicode-escape').decode('ASCII'), dest='en')</p>
<p><a href=""https://i.sstatic.net/OcJjm.png"" rel=""nofollow noreferrer"">The error i am getting while executing</a></p>
",Multilingual Language Processing & Language Identification,trying convert hinglish dataset english using python range sentence df tweet translation translator translate sentence encode unicode escape decode ascii dest en error getting executing
Natural language processing for hinglish tweets,"<p>I have used textblob to assign polarity score to english tweets.Can textblob be used to assign polarity score to Hinglish tweets?
If yes how?</p>
<p>Thankyou</p>
",Multilingual Language Processing & Language Identification,natural language processing hinglish tweet used textblob assign polarity score english tweet textblob used assign polarity score hinglish tweet yes thankyou
is there a method to extract noun-adjs pairs for french?,"<p>i want to extract from a sentence a noun-adj pairs :
i tried this code :</p>
<pre><code>import stanza

nlp = stanza.Pipeline(&quot;fr&quot;)

doc = nlp(&quot;La voiture est belle et jolie, et grand. Le tableau qui est juste en dessous est grand. La femme intelligente et belle est grande. Le service est rapide et les plats sont d√©licieux.&quot;)

def recursive_find_adjs(root, sent):
    children = [w for w in sent.words if w.head == root.id]

    if not children:
        return []

    filtered_c = [w for w in children if w.deprel == &quot;conj&quot; and w.upos == &quot;ADJ&quot;]
    # Do not include an adjective if it is the parent of a noun to prevent
    results = [w for w in filtered_c if not any(sub.head == w.id and sub.upos == &quot;NOUN&quot; for sub in sent.words)]
    for w in children:
        results += recursive_find_adjs(w, sent)

    return results

for sent in doc.sentences:
    nouns = [w for w in sent.words if w.upos == &quot;NOUN&quot;]
    noun_adj_pairs = {}
    for noun in nouns:
        # Find constructions in the form of &quot;La voiture est belle&quot;
        # In this scenario, the adjective is the parent of the noun
        cop_root = sent.words[noun.head-1]
        adjs = [cop_root] + recursive_find_adjs(cop_root, sent) if cop_root.upos == &quot;ADJ&quot; else []

        # Find constructions in the form of &quot;La femme intelligente et belle&quot;
        # Here, the adjectives are descendants of the noun
        mod_adjs = [w for w in sent.words if w.head == noun.id and w.upos == &quot;ADJ&quot;]
        # This should only be one element because conjunctions are hierarchical
        if mod_adjs:
            mod_adj = mod_adjs[0]
            adjs.extend([mod_adj] + recursive_find_adjs(mod_adj, sent))

        if adjs:
            unique_adjs = []
            unique_ids = set()
            for adj in adjs:
                if adj.id not in unique_ids:
                    unique_adjs.append(adj)
                    unique_ids.add(adj.id)

            noun_adj_pairs[noun.text] = &quot; &quot;.join([adj.text for adj in unique_adjs])

    print(noun_adj_pairs)
</code></pre>
<p>it works well but not take into account the case of : &quot;the restaurant n'est pas bien&quot; (&quot;n√©gation&quot;)  or when we have  NOUN+ &quot;au&quot; ('pr√©positions') + &quot;bien&quot;(adj)<br />
any solution please  ?</p>
",Multilingual Language Processing & Language Identification,method extract noun adjs pair french want extract sentence noun adj pair tried code work well take account case restaurant n est pa bien n gation noun au pr position bien adj solution please
How skip string already parsed in [spacy] doc,"<p>Am working with a code that split hashtags with multiple word list: 1) One full of city names, 2) another with specific terms, 3) two big and common wordlist (spanish &amp; english), in order to detect city names in hashtags and then geolocate them. The script works ok, but its's really slow, becouse will look for lots of substrings in the city list array after a similar word it's finded in wordlist and then is validated in Geonamecache library | <a href=""https://github.com/yaph/geonamescache"" rel=""nofollow noreferrer"">https://github.com/yaph/geonamescache</a></p>
<p>Am trying to skip hashtags that already pass trough the tokenizer or at least those that the LanguageComponent already tag.</p>
<p>I have this code:</p>
<pre><code>@Language.component(&quot;custom_set_extension&quot;)
def custom_set_extension(doc):
    #Diccionario General
    wordlist = initialize_words(&quot;wl&quot;)
    #Diccionario particular en el que se buscan palabras juntas como San francisco y Sanfrancisco (util para hashtags)
    citieslist_arr = initialize_words(&quot;cities&quot;).splitlines()
    #print(wordlist)
    Token.set_extension(&quot;is_geo&quot;, default=False, force=True)
    Token.set_extension(&quot;geo_countrycode&quot;, default=None, force=True)
    Token.set_extension(&quot;geo_hashtag&quot;, default=None, force=True)
    #print(type(wordlist))
    for token_index, token in enumerate(doc):
        if token._.is_hashtag:
            #print(&quot;token es hashtag en componente&quot;,token.text)
            if token._.is_geo:
                print(&quot;ya tiene data geo&quot;) #Should skip token but it always go to parse_tag() func
            else:
                parse_tag(token, token.text, wordlist, citieslist_arr)
            #if token.text in wordlist:
            #    print(&quot;lo encontr√≥&quot;)
            #    print(token)
    return doc
</code></pre>
<p>It detects <em>is_hashtag</em> attribute but doesn't work wih <em>is_geo</em> and always execute parse_tag() function</p>
<pre><code>@Language.component(&quot;mention_hashtags_set_extension&quot;)
def mention_hashtags_set_extension(doc):
    hashtag_getter = lambda token: token.text.startswith('#')
    Token.set_extension('is_hashtag', getter=hashtag_getter, force=True)
    mention_getter = lambda token: token.text.startswith('@')
    Token.set_extension('is_mention', getter=mention_getter, force=True)
    return doc
</code></pre>
<p>So is there a way that Spacy marks already tokenized strings/words in this case hashtags so i can use this mark as a conditional to skip the whole process of spliting the hashtag, looking for substrings in the arrays and then validating in Geonamescache, and making this process faster?</p>
",Multilingual Language Processing & Language Identification,skip string already parsed spacy doc working code split hashtags multiple word list one full city name another specific term two big common wordlist spanish english order detect city name hashtags geolocate script work ok really slow becouse look lot substring city list array similar word finded wordlist validated geonamecache library trying skip hashtags already pas trough tokenizer least languagecomponent already tag code detects hashtag attribute work wih geo always execute parse tag function way spacy mark already tokenized string word case hashtags use mark conditional skip whole process spliting hashtag looking substring array validating geonamescache making process faster
How to locate Spacy span from string range?,"<p>I study natural language processing with spaCy and decided to write a pipe that will identify addresses like Dr, Mr, Mrs followed by names as person recognised entity. In the process, I decided to use regular expressions to identify the prefix part, and I found out that whenever I search RE pattern, it returns me the indexes in the whole source text. My task is to extract the addressed names and add them as <code>Span</code> objects, however the constructor demands me to pass a <code>Doc</code> object range.</p>
<p>Is it possible to locate the span in doc for a string position? I could have enumerated through every span individually storing the index as a separate variable and search the regular expression in it, and I will be satisfied with this solution, however is it possible to either translate string position to a span it belongs to or create spans for newly recognised entities in a different way?</p>
<pre class=""lang-py prettyprint-override""><code>import re
import spacy as sp
from spacy.tokens import Doc, Span
from spacy.language import Language
english = sp.load('en_core_web_sm') #Load the English language model
with open('sample.txt', 'r') as f:
    source = f.read() #Text used from Wikipedia https://en.wikipedia.org/wiki/Chocolate

@Language.component(&quot;addresses&quot;)
def searchNames(doc: Doc) -&gt; Doc:
    &quot;&quot;&quot;Searches for names in the text marked with addressing and adds them to the named entities.&quot;&quot;&quot;
    #The pattern expression searches for names with addresses Mr., Mrs., Ms., Dr., Prof., etc.
    pattern = re.compile(r&quot;(P|M|D)(r|s)(s|.)?&quot;) #(f|.)?\s[A-Z][a-z]+
    matches = [(match.start(), match.end()) for match in pattern.finditer(doc.text)]
    #Resolve the index error.
    #Add the names to the named entities.
    names = [Span(doc, start, end, &quot;PERSON&quot;) for start, end in matches]
    doc.ents = tuple(list(doc.ents) + names)
    return doc

english.add_pipe(&quot;addresses&quot;)
chocolate = english(source)
for entity in chocolate.ents:
    if entity.label_ == &quot;PERSON&quot;:
        print(entity.text, entity.label_)

</code></pre>
",Multilingual Language Processing & Language Identification,locate spacy span string range study natural language processing spacy decided write pipe identify address like dr mr mr followed name person recognised entity process decided use regular expression identify prefix part found whenever search pattern return index whole source text task extract addressed name add object however constructor demand pas object range possible locate span doc string position could enumerated every span individually storing index separate variable search regular expression satisfied solution however possible either translate string position span belongs create span newly recognised entity different way
Is there R function to extract number amounts from string of Chinese characters?,"<p>I have a string like <code>d</code></p>
<pre><code>d &lt;- c(&quot;ÊÇ®Â∞æÂè∑1234Âç°11Êúà11Êó•00:03ËΩ¨ÂÖ•‰∫∫Ê∞ëÂ∏Å1,500.00ÂÖÉÔºå‰ΩôÈ¢ù‰∫∫Ê∞ëÂ∏Å1,501.12ÂÖÉ&quot;,
       &quot;ÊÇ®Â∞æÂè∑3256Âç°11Êúà11Êó•00:03ËΩ¨Âá∫‰∫∫Ê∞ëÂ∏Å678.12ÂÖÉÔºå‰ΩôÈ¢ù‰∫∫Ê∞ëÂ∏Å1,501.12ÂÖÉ&quot;,
       &quot;ÊÇ®Â∞æÂè∑7894Âç°11Êúà11Êó•00:03ÂèñÁé∞0.85ÂÖÉÔºå‰ΩôÈ¢ù‰∫∫Ê∞ëÂ∏Å1,501.12ÂÖÉ&quot;,
       &quot;ÊÇ®Â∞æÂè∑3285Âç°11Êúà11Êó•00:03ÊîØ‰ªò3.85ÂÖÉÔºå‰ΩôÈ¢ù‰∫∫Ê∞ëÂ∏Å1,501.12ÂÖÉ&quot;)
</code></pre>
<p>The string is an activity log of money payments through a bank credit.
I want to extract the payment amount money. For example:
I want to extract &quot;1,500.00&quot; &quot;678.12&quot; &quot;0.85&quot; &quot;3.85&quot;. Additionally, how can I convert the values to to numeric?</p>
",Multilingual Language Processing & Language Identification,r function extract number amount string chinese character string like string activity log money payment bank credit want extract payment amount money example want extract additionally convert value numeric
Is there some way to easily map the source to its reference translation for the Google MQM Dataset?,"<h1>TL;DR</h1>
<p>There is a dataset from <a href=""https://github.com/google/wmt-mqm-human-evaluation"" rel=""nofollow noreferrer"">https://github.com/google/wmt-mqm-human-evaluation</a> that contains the source text and various machine translations, but its reference/human translations are missing.</p>
<p>The reference translations can be located on:</p>
<ul>
<li><a href=""https://data.statmt.org/wmt20/translation-task/test.tgz"" rel=""nofollow noreferrer"">https://data.statmt.org/wmt20/translation-task/test.tgz</a></li>
<li><a href=""https://data.statmt.org/wmt21/translation-task/test.tgz"" rel=""nofollow noreferrer"">https://data.statmt.org/wmt21/translation-task/test.tgz</a></li>
<li><a href=""https://www.cs.jhu.edu/%7Ekevinduh/a/multitarget-tedtalks/"" rel=""nofollow noreferrer"">https://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/</a></li>
</ul>
<p><strong>Q: What is the simplest way to map the data from <code>wmt-mqm-human-evaluation</code> to their reference translations?</strong></p>
<hr />
<h1>In Long:</h1>
<p>There is this dataset that exists in multiple <code>.tsv</code> files on <a href=""https://github.com/google/wmt-mqm-human-evaluation"" rel=""nofollow noreferrer"">https://github.com/google/wmt-mqm-human-evaluation</a> and the purpose of this data is that contains annotated translation errors from the <strong>following test sets</strong> created by previous machine translation shared tasks:</p>
<ul>
<li><strong>newstest 2020</strong>: This is the dataset from Conference for Machine Translation (aka WMT) in year 2020 that are structured in this SGML format (basically an XML format for very specific use-cases, there's more explanation here in the 1997 paper but it's not that important)
<ul>
<li>Direct URL: <a href=""https://data.statmt.org/wmt20/translation-task/test.tgz"" rel=""nofollow noreferrer"">https://data.statmt.org/wmt20/translation-task/test.tgz</a></li>
<li>Site explaning what the data contains and the shared task: <a href=""https://www.statmt.org/wmt21/"" rel=""nofollow noreferrer"">https://www.statmt.org/wmt21/</a></li>
</ul>
</li>
<li><strong>newstest 2021</strong>: Similar to the 2020 variant of the data set, this is for the 2021 shared task but structured in a more formal XML format (though without any DTD definition file).
<ul>
<li>Direct URL: <a href=""https://data.statmt.org/wmt21/translation-task/test.tgz"" rel=""nofollow noreferrer"">https://data.statmt.org/wmt21/translation-task/test.tgz</a></li>
<li>Site explaning what the data contains and the shared task: <a href=""https://www.statmt.org/wmt21/"" rel=""nofollow noreferrer"">https://www.statmt.org/wmt21/</a></li>
</ul>
</li>
<li><strong>TED talks</strong>: This is a dataset created for the spoken machine translation shared task that originated from <a href=""https://wit3.fbk.eu/"" rel=""nofollow noreferrer"">https://wit3.fbk.eu/</a> but the cleanest and most structured form is generated on <a href=""https://www.cs.jhu.edu/%7Ekevinduh/a/multitarget-tedtalks/"" rel=""nofollow noreferrer"">https://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/</a></li>
</ul>
<p><strong>Q (part 1):</strong> How to map the annotations from <code>wmt-mqm-human-evaluation</code> to their reference translations?</p>
<hr />
<h2>I've tried</h2>
<p>To get the source to reference mapping from <code>newstest2020</code> and <code>newstest2021</code> I've parsed the XML as such:</p>
<p><strong>Downloading the data:</strong></p>
<pre><code>! wget https://data.statmt.org/wmt21/translation-task/test.tgz 
! tar zxvf test.tgz
</code></pre>
<p><strong>Parsing the data:</strong></p>
<pre><code>from itertools import chain
from bs4 import BeautifulSoup

with open('test/newstest2021.en-de.xml') as fin:
    bsoup = BeautifulSoup(fin)
    
en2de = {}
for doc in bsoup.find_all('doc'):
    if doc.find('ref'):
        en2de.update(
            dict(
                zip(
                    [seg.text for seg in doc.find('src').find_all('seg')],
                    [seg.text for seg in doc.find('ref').find_all('seg')])
            )
        )
        
with open('sgm/newstest2020-ende-src.en.sgm') as fin:
    en = [seg.text for seg in BeautifulSoup(fin).find_all('seg')]
with open('sgm/newstest2020-ende-ref.de.sgm') as fin:
    de = [seg.text for seg in BeautifulSoup(fin).find_all('seg')]
    
en2de.update(dict(zip(en,de)))
</code></pre>
<p>Then to download and parse the <code>tedtalk</code> dataset, I did something like:</p>
<pre><code>! wget https://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/multitarget-ted.tgz 
! tar zxvf multitarget-ted.tgz 
</code></pre>
<p>and then:</p>
<pre><code>en_files, de_files = {}, {}
for filename in find_files('multitarget-ted/', '*en-de*'):
    prefix = filename.rpartition('.')[0]
    if filename.endswith('en'):
        en_files[prefix] = filename
    if filename.endswith('de'):
        de_files[prefix] = filename

en2de_ted = {}
for k in en_files:
    with open(en_files[k]) as en_fin, open(de_files[k]) as de_fin:
        en2de_ted.update({en.strip(): de.strip() for en, de in zip(en_fin, de_fin)})
        
en2de.update(en2de_ted)
</code></pre>
<p>Finally to join them up, I've tried reading the <code>mqm-dataset</code>:</p>
<pre><code>from lazyme import find_files
import pandas as pd

lol = []

for filename in find_files('wmt-mqm-human-evaluation-main', '*.tsv'):
    if &quot;avg_seg_scores&quot; in filename:
        continue
    print(filename)
    df = pd.read_csv(filename, sep='\t', error_bad_lines=False)
    #print(len(df))
    df = df[~df['source'].str.contains('|'.join(['\t']),  na=False)]
    #print(len(df))
    
    ann, dataset, lang = filename.split('_') 
    
    df['filename'] = filename.split('/')[-1]
    df['src_lang'] = lang[:2]
    df['trg_lang'] = lang[2:4]
    df['annotation_scheme'] = ann
    
    if len(lang.split('.')) &gt; 2:
        df['special_category'] = lang.split('.')[1]
    else:
        df['special_category'] = &quot;general&quot;
        
    lol.append(df)

df_google_mqm = pd.concat(lol)
</code></pre>
<p>Then mapping it to <code>zh2en</code> and <code>en2de</code> dictionaries:</p>
<pre><code>targets = []
num_none = 0
for src in df_google_mqm['source']:
    if src in zh2en:
        targets.append(zh2en[src])
    elif src in en2de:
        targets.append(en2de[src])
    else:
        num_none += 1
        targets.append(None)

df_google_mqm['reference'] = targets

df_google_mqm = df_google_mqm[df_google_mqm['reference'].notna()]
</code></pre>
<p>With the above code, I'm still missing around 88,471 data points out of the full 555,990 from the <code>wmt-mqm-human-evaluation</code> dataset.</p>
<p>A working code for the above can be found on <a href=""https://www.kaggle.com/code/alvations/lightyear2"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/alvations/lightyear2</a></p>
<p><strong>Q (part 2):</strong> Have I missed something when trying to map the data with the above attempt?</p>
<p><strong>Q (part 3):</strong> Is there a simpler way to map the data to their reference/human translations?</p>
",Multilingual Language Processing & Language Identification,way easily map source reference translation google mqm dataset tl dr dataset contains source text various machine translation reference human translation missing reference translation located q simplest way map data reference translation long dataset exists multiple file purpose data contains annotated translation error following test set created previous machine translation shared task newstest dataset conference machine translation aka wmt year structured sgml format basically xml format specific use case explanation paper important direct url site explaning data contains shared task newstest similar variant data set shared task structured formal xml format though without dtd definition file direct url site explaning data contains shared task ted talk dataset created spoken machine translation shared task originated structured form generated q part map annotation reference translation tried get source reference mapping parsed xml downloading data parsing data download parse dataset something like finally join tried reading mapping dictionary code still missing around data point full dataset working code found q part missed something trying map data attempt q part simpler way map data reference human translation
Conversion of facebook/nllb-200-3.3B to AWS neuron,"<p>I am trying to convert the <a href=""https://huggingface.co/facebook/nllb-200-3.3B"" rel=""nofollow noreferrer"">new translation model developed by Facebook (Meta)</a>, No Language Left Behind, to AWS's neuron model that can be used with the AWS SageMaker Inference using the Inferentia chips. However, I cannot figure out how to trace the model without errors. This <a href=""https://github.com/aws/aws-neuron-sdk/issues/420#issuecomment-1220885577"" rel=""nofollow noreferrer"">post</a> shows exactly what I am trying to do and working the AWS developers. I will copy my code to here as well for clarity:</p>
<pre><code>import copy
import itertools
from typing import List, Optional, Tuple

import torch
import torch.nn.functional as F

from transformers import M2M100Config
from transformers.generation_utils import GenerationMixin


def _convert_past_list_to_tuple(past_key_values):
    &quot;&quot;&quot;
    In Bart model, the type of past_key_values is tuple(tuple(torch.FloatTensor)) which is not
    TorchScript-compatible. To support this, we have to convert it during the export process.
    This function will convert past values from a list to tuple(tuple(torch.FloatTensor)) for
    the inner decoder.

    According to the definition of past_key_values, each inner tuple(torch.FloatTensor) has 4 tensors,
    so we convert every 4 elements in the list as a tuple(torch.FloatTensor).
    &quot;&quot;&quot;
    count_of_each_inner_tuple = 4
    results = ()
    temp_result = ()
    count_n = len(past_key_values) // count_of_each_inner_tuple
    for idx in range(count_n):
        real_idx = idx * count_of_each_inner_tuple
        temp_result = tuple(past_key_values[real_idx : real_idx + count_of_each_inner_tuple])
        results += ((temp_result),)

    return results


class EncoderForONNX(torch.nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, input_ids, attention_mask):
        return self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=False,
        )


class DecoderForONNX(torch.nn.Module):
    def __init__(self, decoder):
        super().__init__()
        self.decoder = decoder

    def forward(self, input_ids, encoder_state, attention_mask, past=None):
        all_results = None
        if past is not None:
            all_results = _convert_past_list_to_tuple(past)
            input_ids = input_ids[:, -1:]

        last_hidden_state, past_key_values = self.decoder(
            input_ids=input_ids,
            encoder_hidden_states=encoder_state,
            encoder_attention_mask=attention_mask,
            past_key_values=all_results,
            return_dict=False,
        )

        past_values = []
        for past in past_key_values:
            past_values = past_values + list(past)
        return last_hidden_state, past_values


def _create_traced_encoder(encoder, input_ids, attention_mask):
    encoder_c = copy.deepcopy(encoder)
    print(&quot;shapes&quot;,input_ids.shape, attention_mask.shape)
    encoder_for_onnx = EncoderForONNX(encoder_c)
    compiler_args = ['--fp32-cast', 'matmult', '--fast-math', 'no-fast-relayout']
    inputs = (
        input_ids,
        attention_mask,
        )

    return torch_neuron.trace(encoder_for_onnx, inputs,compiler_args=compiler_args)


def _create_traced_decoder(decoder, input_ids, encoder_state, attention_mask, past=None):
    decoder_c = copy.deepcopy(decoder)
    print(input_ids.shape,encoder_state.shape,attention_mask.shape)
    decoder_for_onnx = DecoderForONNX(decoder_c)
    past_values = list(itertools.chain.from_iterable(past or ()))
    compiler_args = ['--fp32-cast', 'matmult', '--fast-math', 'no-fast-relayout']
    print(past_values)
    # Do this twice so we got 2 different decoders for further work.
    if past_values:
        inputs = (
            input_ids,
            encoder_state,
            attention_mask,
            past_values,
        )
        return torch_neuron.trace(decoder_for_onnx, inputs,compiler_args=compiler_args)
    else:
        inputs = (
            input_ids,
            encoder_state,
            attention_mask,
        )
        return torch_neuron.trace(decoder_for_onnx, inputs,compiler_args=compiler_args)


class M2M100ConfigTS(M2M100Config, torch.nn.Module):
    &quot;&quot;&quot;
    BartConfigTS is a TorchScript-compatible transformers.models.bart.configuration_bart.BartConfig.
    TorchScript only supports sub-classes of torch.nn.Module.
    &quot;&quot;&quot;

    def __init__(self, config):
        M2M100Config.__init__(self, config)
        torch.nn.Module.__init__(self)


class MinLengthLogitsProcessorTS(torch.nn.Module):
    r&quot;&quot;&quot;
    :class:`transformers.LogitsProcessor` enforcing a min-length by setting EOS probability to 0.

    Args:
        min_length (:obj:`int`):
            The minimum length below which the score of :obj:`eos_token_id` is set to :obj:`-float(&quot;Inf&quot;)`.
        eos_token_id (:obj:`int`):
            The id of the `end-of-sequence` token.
    &quot;&quot;&quot;

    def __init__(self, min_length: int, eos_token_id: int):
        super().__init__()

        if not isinstance(min_length, int) or min_length &lt; 0:
            raise ValueError(f&quot;`min_length` has to be a positive integer, but is {min_length}&quot;)

        if not isinstance(eos_token_id, int) or eos_token_id &lt; 0:
            raise ValueError(f&quot;`eos_token_id` has to be a positive integer, but is {eos_token_id}&quot;)

        self.min_length = min_length
        self.eos_token_id = eos_token_id

    def forward(self, input_ids, scores) -&gt; torch.Tensor:
        cur_len = input_ids.shape[-1]
        if cur_len &lt; self.min_length:
            scores[:, self.eos_token_id] = -float(&quot;inf&quot;)
        return scores


class NLLBGenerator(torch.nn.Module, GenerationMixin):
    def __init__(self, model):
        super().__init__()
        self.config = M2M100ConfigTS(model.config)
        self.config.force_bos_token_to_be_generated = False
        self._trace_modules(model)
        self.logits_processor = MinLengthLogitsProcessorTS(self.config.min_length, self.config.eos_token_id)
        self.final_logits_weight = model.model.shared.weight
        self.final_logits_bias = model.final_logits_bias
        self.decoder_layers = model.config.decoder_layers
        self.d_model = model.config.d_model

    def _trace_modules(self, model):
        # input_ids = torch.tensor(
        #     [
        #         [
        #             19,669,18,420,8,664,57,42,8,664,21,3028,195,4445,331,1293,34,21,10,6174,1100,6,69,104,42,32,2621,1638,144,4,6174,558,108,4419,1091,28,4,1668,9,1509,1621,279,35,867,2734,85,11,2216,2734,85,203,2244,7,6,15,8102,7,57,8629,5,
        #             model.config.eos_token_id,
        #         ]
        #     ],
        #     device=model.device,
        #     dtype=torch.long,
        # )
        # attention_mask = torch.tensor(
        #     [[True] * input_ids.shape[-1]],
        #     device=model.device,
        #     dtype=torch.bool,
        # )
        pegasus_text = &quot;PG&amp;E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires.&quot;
        model_name = &quot;sshleifer/distilbart-cnn-12-6&quot;

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        inputs = tokenizer(pegasus_text , return_tensors=&quot;pt&quot;, max_length=32, truncation=True, padding='max_length')
        input_ids = inputs[&quot;input_ids&quot;]
        attention_mask = inputs[&quot;attention_mask&quot;]

        self.encoder = _create_traced_encoder(model.get_encoder(), input_ids, attention_mask)
        encoder_outputs = model.get_encoder()(input_ids, attention_mask=attention_mask, return_dict=True)
        decoder = model.model.decoder
        decoder_outputs = decoder(input_ids, attention_mask, encoder_outputs[&quot;last_hidden_state&quot;], None, None, None)
        # print(decoder_outputs[1])
        # print(decoder_outputs[1].shape)
        self.decoder_no_past = _create_traced_decoder(
            model.model.decoder, input_ids, encoder_outputs[&quot;last_hidden_state&quot;], attention_mask
        )
        self.decoder_with_past = _create_traced_decoder(
            model.model.decoder, input_ids, encoder_outputs[&quot;last_hidden_state&quot;], attention_mask, decoder_outputs[1]
        )

    def _encoder_forward(self, input_ids, attention_mask):
        return self.encoder(input_ids, attention_mask)[0]

    @staticmethod
    def _init_sequence_length_for_generation(
        input_ids: torch.LongTensor, max_length: int
    ) -&gt; Tuple[torch.Tensor, torch.Tensor, int]:
        unfinished_sequences = torch.zeros(input_ids.shape[0], dtype=torch.long, device=input_ids.device) + 1
        sequence_lengths = torch.zeros(input_ids.shape[0], dtype=torch.long, device=input_ids.device) + max_length

        cur_len = input_ids.shape[-1]
        return sequence_lengths, unfinished_sequences, cur_len

    def _decoder_forward(self, input_ids, encoder_output, attention_mask, past: List[torch.Tensor]):
        # Update here to use different decoder for different values of past.
        if past is None or len(past) == 0:
            decoder_output, past = self.decoder_no_past(
                input_ids=input_ids, encoder_state=encoder_output, attention_mask=attention_mask
            )
        else:
            decoder_output, past = self.decoder_with_past(
                input_ids=input_ids, encoder_state=encoder_output, attention_mask=attention_mask, past=past
            )

        lm_logits = F.linear(decoder_output, self.final_logits_weight, bias=self.final_logits_bias)

        return lm_logits, past

    def greedy_search(
        self, input_ids, encoder_output, attention_mask, max_length, pad_token_id: int, eos_token_id: int
    ):
        # init sequence length tensors
        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(
            input_ids, max_length
        )

        past: List[torch.Tensor] = []
        while cur_len &lt; max_length:

            logits, past = self._decoder_forward(input_ids, encoder_output, attention_mask, past)
            next_token_logits = logits[:, -1, :]

            # pre-process distribution
            scores = self.logits_processor(input_ids, next_token_logits)

            # argmax
            next_tokens = torch.argmax(scores, dim=-1)

            # add code that transfomers next_tokens to tokens_to_add
            if eos_token_id is not None:
                assert pad_token_id is not None, &quot;If eos_token_id is defined, make sure that pad_token_id is defined.&quot;
                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)

            # add token and increase length by one
            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)

            # update sequence length
            if eos_token_id is not None:
                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(
                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id
                )

            # stop when there is a &lt;/s&gt; in each sentence, or if we exceed the maximul length
            if unfinished_sequences.max() == 0:
                break

            # increase cur_len
            cur_len = cur_len + 1

        return input_ids

    def _prepare_decoder_input_ids_for_generation(
        self,
        input_ids: torch.LongTensor,
        decoder_start_token_id,
        bos_token_id: Optional[int] = None,
    ) -&gt; torch.LongTensor:

        decoder_input_ids = (
            torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)
            * decoder_start_token_id
        )
        return decoder_input_ids

    def forward(self, input_ids, attention_mask, max_length, decoder_start_token_id):
        pad_token_id = self.config.pad_token_id
        bos_token_id = self.config.bos_token_id
        eos_token_id = self.config.eos_token_id

        # special case if pad_token_id is not defined
        if pad_token_id is None and eos_token_id is not None:
            # Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.
            pad_token_id = eos_token_id

        encoder_output = self._encoder_forward(input_ids, attention_mask)

        input_ids = self._prepare_decoder_input_ids_for_generation(
            input_ids,
            decoder_start_token_id=decoder_start_token_id,
            bos_token_id=bos_token_id,
        )

        return self.greedy_search(
            input_ids,
            encoder_output,
            attention_mask,
            max_length=max_length,
            pad_token_id=pad_token_id,
            eos_token_id=eos_token_id,
        )
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/nllb-200-3.3B&quot;)
import torch
import torch_neuron


neuron_model = NLLBGenerator(model)
</code></pre>
<p>And the current error I am receiving:</p>
<pre><code>/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:326: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
INFO:Neuron:There are 1 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)
INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 1479, fused = 1456, percent fused = 98.44%
INFO:Neuron:Number of neuron graph operations 3581 did not match traced graph 3283 - using heuristic matching of hierarchical information
WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1631; falling back to native python function call
ERROR:Neuron:Error parsing message with type 'tensorflow.GraphDef'
Traceback (most recent call last):
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/convert.py&quot;, line 382, in op_converter
    item, inputs, compiler_workdir=sg_workdir, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/decorators.py&quot;, line 82, in trace
    graph_def = graph.as_graph_def()
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py&quot;, line 3238, in as_graph_def
    result, _ = self._as_graph_def(from_version, add_shapes)
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py&quot;, line 3166, in _as_graph_def
    graph.ParseFromString(compat.as_bytes(data))
google.protobuf.message.DecodeError: Error parsing message with type 'tensorflow.GraphDef'
INFO:Neuron:Number of arithmetic operators (post-compilation) before = 1479, compiled = 0, percent compiled = 0.0%
INFO:Neuron:The neuron partitioner created 1 sub-graphs
INFO:Neuron:Neuron successfully compiled 0 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 0.0%
INFO:Neuron:Compiled these operators (and operator counts) to Neuron:
INFO:Neuron:Not compiled operators (and operator counts) to Neuron:
INFO:Neuron: =&gt; aten::Int: 414 [supported]
INFO:Neuron: =&gt; aten::add: 75 [supported]
INFO:Neuron: =&gt; aten::bmm: 48 [supported]
INFO:Neuron: =&gt; aten::contiguous: 72 [supported]
INFO:Neuron: =&gt; aten::cumsum: 1 [supported]
INFO:Neuron: =&gt; aten::detach: 1 [supported]
INFO:Neuron: =&gt; aten::dropout: 97 [supported]
INFO:Neuron: =&gt; aten::embedding: 1 [not supported]
INFO:Neuron: =&gt; aten::expand: 1 [supported]
INFO:Neuron: =&gt; aten::index_select: 1 [supported]
INFO:Neuron: =&gt; aten::layer_norm: 49 [supported]
INFO:Neuron: =&gt; aten::linear: 144 [supported]
INFO:Neuron: =&gt; aten::masked_fill: 1 [supported]
INFO:Neuron: =&gt; aten::mul: 74 [supported]
INFO:Neuron: =&gt; aten::ne: 1 [supported]
INFO:Neuron: =&gt; aten::relu: 24 [supported]
INFO:Neuron: =&gt; aten::reshape: 24 [supported]
INFO:Neuron: =&gt; aten::rsub: 1 [supported]
INFO:Neuron: =&gt; aten::size: 77 [supported]
INFO:Neuron: =&gt; aten::slice: 2 [supported]
INFO:Neuron: =&gt; aten::softmax: 24 [supported]
INFO:Neuron: =&gt; aten::to: 5 [supported]
INFO:Neuron: =&gt; aten::transpose: 120 [supported]
INFO:Neuron: =&gt; aten::type_as: 1 [supported]
INFO:Neuron: =&gt; aten::unsqueeze: 2 [supported]
INFO:Neuron: =&gt; aten::view: 219 [supported]
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_4519/3952284984.py in &lt;module&gt;
    314 
    315 
--&gt; 316 neuron_model = NLLBGenerator(model)

/tmp/ipykernel_4519/3952284984.py in __init__(self, model)
    154         self.config = M2M100ConfigTS(model.config)
    155         self.config.force_bos_token_to_be_generated = False
--&gt; 156         self._trace_modules(model)
    157         self.logits_processor = MinLengthLogitsProcessorTS(self.config.min_length, self.config.eos_token_id)
    158         self.final_logits_weight = model.model.shared.weight

/tmp/ipykernel_4519/3952284984.py in _trace_modules(self, model)
    185         attention_mask = inputs[&quot;attention_mask&quot;]
    186 
--&gt; 187         self.encoder = _create_traced_encoder(model.get_encoder(), input_ids, attention_mask)
    188         encoder_outputs = model.get_encoder()(input_ids, attention_mask=attention_mask, return_dict=True)
    189         decoder = model.model.decoder

/tmp/ipykernel_4519/3952284984.py in _create_traced_encoder(encoder, input_ids, attention_mask)
     80         )
     81 
---&gt; 82     return torch_neuron.trace(encoder_for_onnx, inputs,compiler_args=compiler_args)
     83 
     84 

~/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/convert.py in trace(func, example_inputs, fallback, op_whitelist, minimum_segment_size, subgraph_builder_function, subgraph_inputs_pruning, skip_compiler, debug_must_trace, allow_no_ops_on_neuron, compiler_workdir, dynamic_batch_size, compiler_timeout, _neuron_trace, compiler_args, optimizations, verbose, **kwargs)
    182         logger.debug(&quot;skip_inference_context - trace with fallback at {}&quot;.format(get_file_and_line()))
    183         neuron_graph = cu.compile_fused_operators(neuron_graph, **compile_kwargs)
--&gt; 184     cu.stats_post_compiler(neuron_graph)
    185 
    186     # Wrap the compiled version of the model in a script module. Note that this is

~/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/convert.py in stats_post_compiler(self, neuron_graph)
    491         if succesful_compilations == 0 and not self.allow_no_ops_on_neuron:
    492             raise RuntimeError(
--&gt; 493                 &quot;No operations were successfully partitioned and compiled to neuron for this model - aborting trace!&quot;)
    494 
    495         if percent_operations_compiled &lt; 50.0:

RuntimeError: No operations were successfully partitioned and compiled to neuron for this model - aborting trace!
</code></pre>
<p>Any help would be appreciated.</p>
",Multilingual Language Processing & Language Identification,conversion facebook nllb b aws neuron trying convert new translation model developed facebook meta language left behind aws neuron model used aws sagemaker inference using inferentia chip however figure trace model without error post show exactly trying working aws developer copy code well clarity current error receiving help would appreciated
How to predict &lt;unk&gt; token for neural machine translation,"<p>For example, if I have the words MKIK or &quot;ÁâõÈÄº&quot; (which is artificially created) how can we tell neural networks (transformer model) to keep the same output?</p>
<p>The problem is with using the transformer model on fairseq.</p>
<p>I found fairseq has <code>--replace-unk</code> parameters, but it doesn't seem to work on transformer model or it has a bug</p>
",Multilingual Language Processing & Language Identification,predict unk token neural machine translation example word mkik artificially created tell neural network transformer model keep output problem using transformer model fairseq found fairseq ha parameter seem work transformer model ha bug
Algorithm for keyword/phrase trend search similar to Twitter trends,"<p>Wanted some ideas about building a tool which can scan text sentences (written in english language) and build a keyword rank, based on the most occurrences of words or phrases within the texts. </p>

<p>This would be very similar to the twitter trends wherin twitter detects and reports the top 10 words within the tweets.</p>

<p>I have identified the high level steps in the algorithm as follows  </p>

<ol>
<li>Scan the text and remove all the common , frequent words ( such as, ""the"" , ""is"" , ""are"", ""what"" , ""at"" etc..)</li>
<li>Add the remaining words to a hashmap. If the word is already in the map then increment its count.</li>
<li>To get the top 10 words , iterate through the hashmap and find out the top 10 counts.</li>
</ol>

<p>Step 2 and 3 are straightforward but I do not know in step 1 how do I detect the important words within a text and segregate them from the common words (prepositions, conjunctions etc )</p>

<p>Also if I want to track phrases what could be the approach ?
For example if I have a text saying ""This honey is very good"" 
I might want to track ""honey"" and ""good"" but I may also want to track the phrases ""very good"" or ""honey is very good"" </p>

<p>Any suggestions would be greatly appreciated.</p>

<p>Thanks in advance</p>
",Multilingual Language Processing & Language Identification,algorithm keyword phrase trend search similar twitter trend wanted idea building tool scan text sentence written english language build keyword rank based occurrence word phrase within text would similar twitter trend wherin twitter detects report top word within tweet identified high level step algorithm follows scan text remove common frequent word etc add remaining word hashmap word already map increment count get top word iterate hashmap find top count step straightforward know step detect important word within text segregate common word preposition conjunction etc also want track phrase could approach example text saying honey good might want track honey good may also want track phrase good honey good suggestion would greatly appreciated thanks advance
Using BERT in order to detect language of a given word,"<p>I have words in the Hebrew language. Part of them are originally in English, and part of them are 'Hebrew English', meaning that those are words that are originally from English but are written with Hebrew words. 
For example: 'insulin' in Hebrew is ""◊ê◊ô◊†◊°◊ï◊ú◊ô◊ü"" (Same phonetic sound). </p>

<p>I have a simple binary dataset. 
X: words (Written with Hebrew characters)
y: label 1 if the word is originally in English and is written with Hebrew characters, else 0</p>

<p>I've tried using the classifier, but the input for it is full text, and my input is just words. </p>

<p>I don't want any MASKING to happen, I just want simple classification.</p>

<p>Is it possible to use BERT for this mission? Thanks</p>
",Multilingual Language Processing & Language Identification,using bert order detect language given word word hebrew language part originally english part hebrew english meaning word originally english written hebrew word example insulin hebrew phonetic sound simple binary dataset x word written hebrew character label word originally english written hebrew character else tried using classifier input full text input word want masking happen want simple classification possible use bert mission thanks
How to perform word-level alignment between a sentence and its translation?,"<p>I want to align source and target sentences in a multilingual translation setting.</p>
<p>Conceptually, I want to do something like the following for an exemplary English source sentence and a German target sentence:</p>
<pre><code>0   1   2   3    4       5   6      7
i   saw the man  walking on  the    street  
ich sah den mann auf     der stra·∫ûe gehen
</code></pre>
<p>Word-level alignment would be: 0-0 1-1 2-2 3-3 4-7 5-4 6-5 7-6</p>
<p>Or in the case of different lengths between source and target sentence:</p>
<pre><code>0  1   2    3         4   5  6        7   8    9
it is  a    different way of saying   the same thing
es ist eine andere    art ,  dasselbe zu  sagen
</code></pre>
<p>Word-level alignment should be something like: 0-0 1-1 2-2 3-3 4-4 5-5 6-[7,8] 7-6 8-6 9-6</p>
<p>What's the best way to achieve this?
Thanks for any suggestions!</p>
",Multilingual Language Processing & Language Identification,perform word level alignment sentence translation want align source target sentence multilingual translation setting conceptually want something like following exemplary english source sentence german target sentence word level alignment would case different length source target sentence word level alignment something like best way achieve thanks suggestion
Semantic triples- how to make a function that finds dependensies for a given entity in a list?,"<pre><code>def get_entity(sen, ind):
    entity_str = sen[ind][1]
    
    # TODO: Add following entity to entity-items
    fo_i = ind + 1
    while fo_i &lt; len(sen) and ____:
        entity_str += &quot; &quot; + ____
        fo_i += 1
    
    # TODO: Add acendent to entity-iems
    pr_i = ind - 1
    while pr_i &gt; -1 and ____:
        entity_str = ____ + &quot; &quot; + entity_str
        pr_i -= 1
    
    return entity_str


doc = nlp(sentences[13])
sent = [(token.i, token.text, token.pos_, token.dep_.lower(), \
        token.head.i, token.head.text, token.head.pos_, \
        token.ent_iob_) for token in doc]

print(&quot;Mening:&quot;)
for w in sent:
    print(w)

print(&quot;Word on index=24:&quot;, sent[24])

complete_entity = get_entity(sent, 24)
print(&quot;Complete entity:&quot;, complete_entity)

</code></pre>
<p>I'm just supposed to add code to the empty lines in the code above. This is the list I'm working from:</p>
<blockquote>
<p>(0, 'French', 'ADJ', 'amod', 1, 'musician', 'NOUN', 'B')
(1, 'musician', 'NOUN', 'compound', 5, 'Jarre', 'PROPN', 'O')
(2, 'Jean', 'PROPN', 'compound', 4, 'Michel', 'PROPN', 'B')
(3, '-', 'PUNCT', 'punct', 4, 'Michel', 'PROPN', 'I')
(4, 'Michel', 'PROPN', 'compound', 5, 'Jarre', 'PROPN', 'I')
(5, 'Jarre', 'PROPN', 'nsubj', 6, 'is', 'AUX', 'I')
(6, 'is', 'AUX', 'root', 6, 'is', 'AUX', 'O')
(7, 'to', 'PART', 'aux', 8, 'perform', 'VERB', 'O')
(8, 'perform', 'VERB', 'xcomp', 6, 'is', 'AUX', 'O')
(9, 'at', 'ADP', 'prep', 8, 'perform', 'VERB', 'O')
(10, 'a', 'DET', 'det', 11, 'concert', 'NOUN', 'O')
(11, 'concert', 'NOUN', 'pobj', 9, 'at', 'ADP', 'O')
(12, 'in', 'ADP', 'prep', 11, 'concert', 'NOUN', 'O')
(13, 'Copenhagen', 'PROPN', 'pobj', 12, 'in', 'ADP', 'B')
(14, 'to', 'PART', 'aux', 15, 'mark', 'VERB', 'O')
(15, 'mark', 'VERB', 'advcl', 8, 'perform', 'VERB', 'O')
(16, 'the', 'DET', 'det', 17, 'bicentennial', 'NOUN', 'O')
(17, 'bicentennial', 'NOUN', 'dobj', 15, 'mark', 'VERB', 'O')
(18, 'of', 'ADP', 'prep', 17, 'bicentennial', 'NOUN', 'O')
(19, 'the', 'DET', 'det', 20, 'birth', 'NOUN', 'O')
(20, 'birth', 'NOUN', 'pobj', 18, 'of', 'ADP', 'O')
(21, 'of', 'ADP', 'prep', 20, 'birth', 'NOUN', 'O')
(22, 'writer', 'NOUN', 'compound', 25, 'Andersen', 'PROPN', 'O')
(23, 'Hans', 'PROPN', 'compound', 25, 'Andersen', 'PROPN', 'B')
(24, 'Christian', 'PROPN', 'compound', 25, 'Andersen', 'PROPN', 'I')
(25, 'Andersen', 'PROPN', 'pobj', 21, 'of', 'ADP', 'I')
(26, '.', 'PUNCT', 'punct', 6, 'is', 'AUX', 'O')</p>
</blockquote>
<p>My problem is that I only get one ancestor when writing this:</p>
<pre><code>pr_i = ind - 1
    while pr_i &gt; -1 and sen[pr_i][7] == 'B':
        entity_str = sen[pr_i][1] + &quot; &quot; + entity_str
        pr_i -= 1
</code></pre>
",Multilingual Language Processing & Language Identification,semantic triple make function find dependensies given entity list supposed add code empty line code list working french adj amod musician noun b musician noun compound jarre propn jean propn compound michel propn b punct punct michel propn michel propn compound jarre propn jarre propn nsubj aux aux root aux part aux perform verb perform verb xcomp aux adp prep perform verb det det concert noun concert noun pobj adp adp prep concert noun copenhagen propn pobj adp b part aux mark verb mark verb advcl perform verb det det bicentennial noun bicentennial noun dobj mark verb adp prep bicentennial noun det det birth noun birth noun pobj adp adp prep birth noun writer noun compound andersen propn han propn compound andersen propn b christian propn compound andersen propn andersen propn pobj adp punct punct aux problem get one ancestor writing
How to classify words with similar patterns on one unique word using Python?,"<p>I would like to ask about a way to create a dictionary or classify (both solutions are good for me) similar words into the same word.</p>
<p>For giving you a example, let's say we have in a list the following values:</p>
<p><code> ['table','tabla', 'tablon','tablera','tablet']</code></p>
<p>The words tabla, tablon and tablera are just table but in other languages, so I would like to automatically create a dictionary that would classify this words to the english word 'table', of course tablet shoud be excluded (i write it down in the list since it was too similar to the words I was analyzing)</p>
<p>I was thinking about a NLP word embbeding approachment, but my knowledge in the field is too shallow, I don¬¥t know if there are better ways to do this.</p>
<p>Any solutions is welcomed!!</p>
<p>Thanks a lot in advance</p>
",Multilingual Language Processing & Language Identification,classify word similar pattern one unique word using python would like ask way create dictionary classify solution good similar word word giving example let say list following value word tabla tablon tablera table language would like automatically create dictionary would classify word english word table course tablet shoud excluded write list since wa similar word wa analyzing wa thinking nlp word embbeding approachment knowledge field shallow know better way solution welcomed thanks lot advance
Python: how to automatically spellcheck and correct joined words such as &quot;reportthatexplains&quot; and &quot;havebeen&quot;,"<p>I have some large text files which are in correct English because extracted from pdfs. However, many words in these text files are joined: &quot;informationotherwise&quot;, &quot;havebeen&quot;, &quot;reportthatexplains&quot;. Every spell checker will spot these errors, e.g. LanguageTool, Sublime, MS-Word. However, Python struggles.</p>
<p>I tried pyspellchecker and TextBlob to check and correct these words, but, alas, to no avail.</p>
<p>See for example this code, which returns None three times.</p>
<pre><code>misspelled = spell.unknown([&quot;informationotherwise&quot;, &quot;havebeen&quot;, &quot;reportthatexplains&quot;])

for word in misspelled:
    print(spell.correction(word))
    print(spell.candidates(word))
</code></pre>
<p>And this code:</p>
<pre><code>t =&quot;havebeen&quot;
TextBlob(t).correct().string

&gt;&gt;&gt; 'havebeen'
</code></pre>
<p>Any suggestions?</p>
",Multilingual Language Processing & Language Identification,python automatically spellcheck correct joined word reportthatexplains havebeen large text file correct english extracted pdfs however many word text file joined informationotherwise havebeen reportthatexplains every spell checker spot error e g languagetool sublime word however python struggle tried pyspellchecker textblob check correct word ala avail see example code return none three time code suggestion
Pretrained model implemetation for NLP tasks. How to speed up the process?,"<p>I have a huge reviews data base (around 1.5 million on different products). The database does not have predefined sentiment column, so I am doing it in an unsupervised way.</p>
<p>I want to get the numeric evaluation of the sentiment for each line, and therefore I am implementing this pretrained model.
<strong>bert-base-multilingual-uncased-sentiment</strong></p>
<p>It always takes more than 4 hours to run such model on the huge dataset.
What approach might speed up the process?</p>
<p>I cleaned the reviews, deleted all numeric data, and stop words.</p>
",Multilingual Language Processing & Language Identification,pretrained model implemetation nlp task speed process huge review data base around million different product database doe predefined sentiment column unsupervised way want get numeric evaluation sentiment line therefore implementing pretrained model bert base multilingual uncased sentiment always take hour run model huge dataset approach might speed process cleaned review deleted numeric data stop word
Does IBM Watson support using multiple language models at the same time?,"<p>I am dealing with conversations containing both English and Spanish, and Spanish is the primary language here. The situation is that English is interweaving with Spanish in some parts of the conversation, and I would like to tell them apart.
I know that we will pass in a model name as a parameter, but I feel like Watson does not support me to indicate a second language as another parameter.</p>
",Multilingual Language Processing & Language Identification,doe ibm watson support using multiple language model time dealing conversation containing english spanish spanish primary language situation english interweaving spanish part conversation would like tell apart know pas model name parameter feel like watson doe support indicate second language another parameter
Unrecognized configuration class &lt;class &#39;transformers.models.bert.configuration_bert.BertConfig&#39;&gt; for this kind of AutoModel: AutoModelForSeq2SeqLM,"<p>Model type should be one of BartConfig, PLBartConfig, BigBirdPegasusConfig, M2M100Config, LEDConfig, BlenderbotSmallConfig, MT5Config, T5Config, PegasusConfig, MarianConfig, MBartConfig, BartConfig, BlenderbotConfig, FSMTConfig, XLMProphetNetConfig, ProphetNetConfig, EncoderDecoderConfig.</p>
<p>I am trying to load a fine-tuned Bert model for machine translation using AutoModelForSeq2SeqLM but it can't recognize the configuration class.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained('/content/drive/MyDrive/Models/CSE498')
</code></pre>
<p><strong>Config File</strong></p>
<pre><code>
{
  &quot;_name_or_path&quot;: &quot;ckiplab/albert-tiny-chinese&quot;,
  &quot;architectures&quot;: [
    &quot;BertForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.0,
  &quot;bos_token_id&quot;: 101,
  &quot;classifier_dropout&quot;: null,
  &quot;classifier_dropout_prob&quot;: 0.1,
  &quot;down_scale_factor&quot;: 1,
  &quot;embedding_size&quot;: 128,
  &quot;eos_token_id&quot;: 102,
  &quot;gap_size&quot;: 0,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.0,
  &quot;hidden_size&quot;: 312,
  &quot;initializer_range&quot;: 0.02,
  &quot;inner_group_num&quot;: 1,
  &quot;intermediate_size&quot;: 1248,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;net_structure_type&quot;: 0,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_groups&quot;: 1,
  &quot;num_hidden_layers&quot;: 4,
  &quot;num_memory_blocks&quot;: 0,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;tokenizer_class&quot;: &quot;BertTokenizerFast&quot;,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.18.0&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
",Multilingual Language Processing & Language Identification,unrecognized configuration class class transformer model bert configuration bert bertconfig kind automodel automodelforseq seqlm model type one bartconfig plbartconfig bigbirdpegasusconfig config ledconfig blenderbotsmallconfig mt config config pegasusconfig marianconfig mbartconfig bartconfig blenderbotconfig fsmtconfig xlmprophetnetconfig prophetnetconfig encoderdecoderconfig trying load fine tuned bert model machine translation using automodelforseq seqlm recognize configuration class config file
Does keybert support French or Spanish text?,"<p><a href=""https://pypi.org/project/keybert/"" rel=""nofollow noreferrer"">https://pypi.org/project/keybert/</a></p>
<pre><code>from keybert import KeyBERT
kw_model = KeyBERT(model='all-mpnet-base-v2')

test_text = &quot;&quot;&quot;We should save the mother earth so that our future generations can live in a safe environment. We can save the earth by saving trees, natural vegetation, water, natural resources, electricity, etc. We should strictly follow all the possible measures to control the environmental pollution and global warming.&quot;&quot;&quot;

keywords = kw_model.extract_keywords(test_text,
                                     keyphrase_ngram_range=(3, 3),
                                     stop_words='english',
                                     use_mmr=True,
                                     diversity=0.7)
keywords_list = list(dict(keywords).keys())
print(keywords_list)
</code></pre>
<p>I have no idea how to get topics from French and Spanish sentences using keybert.</p>
",Multilingual Language Processing & Language Identification,doe keybert support french spanish text idea get topic french spanish sentence using keybert
Vader Sentiment Analysis: How are the individual words rated?,"<p>So I was using Vader Sentiment Analyser to analyse certain customer feedbacks. While assessing the output i saw that the sentiment analyser was giving me mixed results. </p>

<pre><code>For eg: ""Again, human interaction needs to have resolutions. Your reps 
        cannot BLAME the system and shrug off being able to help. Let 
        alone blame the system and not know WHY the system makes 
        indiscriminate decisions.""

Output: compound: 0.2212 neg: 0.111 neu: 0.756, pos: 0.133
</code></pre>

<p>The O/P in this case should have been negative but instead it gave a compound score of more towards neutral to positive score which makes no sense.</p>

<p>I saw this file in AppData\Roaming\nltk_data\sentiment\vader_lexicon.txt which contained sentiment scores of most of the english words.  </p>

<p>I Just wanted to know how these individual words are given sentiment scores in terms of pos neg neu and compound? Is there any algorithm/process to rate them? </p>

<p>Finally, I was thinking of building my own dictionary for sentiment analysis  to get some better results, but for that i need to know how are each words assigned sentiment scores?</p>
",Multilingual Language Processing & Language Identification,vader sentiment analysis individual word rated wa using vader sentiment analyser analyse certain customer feedback assessing output saw sentiment analyser wa giving mixed result p case negative instead gave compound score towards neutral positive score make sense saw file appdata roaming nltk data sentiment vader lexicon txt contained sentiment score english word wanted know individual word given sentiment score term po neg neu compound algorithm process rate finally wa thinking building dictionary sentiment analysis get better result need know word assigned sentiment score
Is there any Treebank for free?,"<p>Is any place I can download Treebank of English phrases for free or less than $100? I need training data containing bunch of syntactic parsed sentences (>1000) in English in any format. Basically all I need is just words in this sentences being recognized by part of speech.</p>
",Multilingual Language Processing & Language Identification,treebank free place download treebank english phrase free le need training data containing bunch syntactic parsed sentence english format basically need word sentence recognized part speech
How does NLP model know the output length during translation tasks?,"<p>Translating English to French, we may have this:</p>
<p>Input: &quot;Please help me translate this sentence&quot;            6 tokens
Output: &quot;Merci de m'aider √† traduire cette phrase&quot;         7 tokens</p>
<p>We have 7 tokens in the output. How does Bert model know this length during the network processing? Which hyperparameters are involved?</p>
",Multilingual Language Processing & Language Identification,doe nlp model know output length translation task translating english french may input please help translate sentence token output merci de aider traduire cette phrase token token output doe bert model know length network processing hyperparameters involved
Tensorflow Dataset.bucket_by_sequence_length throws TypeError,"<p>I'm trying to construct a dataset of variable length English/Japanese sentences for a machine translation problem, but I can't get the <code>Dataset.bucket_by_sequence_length</code> function to work. It throws the following:</p>
<pre><code>TypeError: Tensor.__init__() missing 3 required positional arguments: 'op', 
'value_index', and 'dtype'
</code></pre>
<p>Despite my best efforts I have not been able to diagnose the problem. I've tried using a named function for <code>element_length_func</code>, passing a single dataset entry, passing only the english sentences, and trying various manual and dynamically generated values for <code>bucket_boundaries</code> and <code>bucket_batch_sizes</code>. The code for constructing the dataset from a list of lists of varying length containing integer indexes is included below. Any suggestions or possible solutions?</p>
<pre><code># Create initial dataset
eng, jap = map(list, zip(*data))
assert len(eng) == len(jap)
eng = tf.ragged.constant(eng, dtype=tf.uint16)
jap = tf.ragged.constant(jap, dtype=tf.uint16)

dataset = tf.data.Dataset.from_tensor_slices((eng, jap))

# Bucket based on sequence length
vocab = tokenizer.get_vocab()
dataset = dataset.bucket_by_sequence_length(
    element_length_func=lambda x, _=None: tf.shape(x)[0],
    bucket_boundaries=[100],
    bucket_batch_sizes=[BATCH_SIZE, BATCH_SIZE],
    padding_values=vocab[&quot;[PAD]&quot;],
)
</code></pre>
",Multilingual Language Processing & Language Identification,tensorflow dataset bucket sequence length throw typeerror trying construct dataset variable length english japanese sentence machine translation problem get function work throw following despite best effort able diagnose problem tried using named function passing single dataset entry passing english sentence trying various manual dynamically generated value code constructing dataset list list varying length containing integer index included suggestion possible solution
Apertium package in Python returns module not installed error after installing module,"<p>I'm trying to get the english to german translator from the apertium package to work, but it seems that even if the languages are downloaded and installed the translator cannot find them, returning a mode not installed error. Am I downloading the wrong module?</p>
<h2>Code example</h2>
<pre><code>import apertium
apertium.installer.install_module(&quot;eng&quot;)
apertium.installer.install_module(&quot;deu&quot;)
apertium.installer.install_module(&quot;eng-deu&quot;)

#checking if other languages work:
t = apertium.Translator('eng', 'spa')
print(t.translate('cats'))

#checking if english to german works after installation
r = apertium.Translator('eng', 'deu')
print(r.translate('cats'))
</code></pre>
<h2>Output</h2>
<pre><code>Gatos
---------------------------------------------------------------------------
ModeNotInstalled                          Traceback (most recent call last)
&lt;ipython-input-44-821185672685&gt; in &lt;module&gt;()
      3 
      4 r = apertium.Translator('eng', 'deu')
----&gt; 5 r.translate('cats')

/usr/local/lib/python3.7/dist-packages/apertium/translation/__init__.py in translate(self, text, mark_unknown, formatting, deformat, reformat)
    159             pair = map(to_alpha3_code, [self.l1, self.l2])
    160         else:
--&gt; 161             raise apertium.ModeNotInstalled()
    162 
    163         if pair is not None:

ModeNotInstalled: 
</code></pre>
",Multilingual Language Processing & Language Identification,apertium package python return module installed error installing module trying get english german translator apertium package work seems even language downloaded installed translator find returning mode installed error downloading wrong module code example output
SpaCy lemmatization returns 0,"<p>today I've tried using SpaCy lemmatization for the first time. I used Polish and English pipelines. I wrote a very simple code:</p>
<pre><code>for token in doc:
    print(token, token.lemma)
</code></pre>
<p>I don't understand why, but what I got in return was only &quot;Token, 0&quot;. I think I've properly loaded the language pipeline...</p>
",Multilingual Language Processing & Language Identification,spacy lemmatization return today tried using spacy lemmatization first time used polish english pipeline wrote simple code understand got return wa token think properly loaded language pipeline
How to extract the keywords on which universal sentence encoder was trained on?,"<p>I am using Universal sentence encoder to encode some documents into a 512 dimensional embeddings. These are then used to find similar items to a search query which is also encoded using USE. USE works pretty well on general english words in search query and documents but performs really bad when the search query contains rare keywords such as people's name etc. I am thinking of enabling a reranker over the search results that takes into account the number of rare words present in the search query and the document retrieved. This should boost the scores of documents which contain known words while reduce the score of documents that contain unknown words.</p>
<p>My question is How do I get the grammar of Universal sentence encoder to implement such re-ranker?</p>
",Multilingual Language Processing & Language Identification,extract keywords universal sentence encoder wa trained using universal sentence encoder encode document dimensional embeddings used find similar item search query also encoded using use use work pretty well general english word search query document performs really bad search query contains rare keywords people name etc thinking enabling reranker search result take account number rare word present search query document retrieved boost score document contain known word reduce score document contain unknown word question get grammar universal sentence encoder implement ranker
Is there an abstract language that &quot;compiles&quot; to natural languages?,"<p>Is there an abstract language that &quot;compiles&quot; to natural languages?</p>
<p>For example</p>
<pre><code>(verb-love :subject person-1sg :object person-2sg)
</code></pre>
<p>would compile to &quot;I love you&quot; in English but to &quot;Je t'aime&quot; in French.
With the possibility to add mood, tense, etc.</p>
<pre><code>(verb-love :subject person-1sg :object person-2g :tense future)
</code></pre>
<p>I used S-expressions the example but the syntax doesn't matter.
It does not have to be Turing-complete since a scripting language could generate such code.</p>
",Multilingual Language Processing & Language Identification,abstract language compiles natural language abstract language compiles natural language example would compile love english je aime french possibility add mood tense etc used expression example syntax matter doe turing complete since scripting language could generate code
Is there information in a spacy token indicative of the meaning of the token?,"<p>Suppose I have a spacy system and can easily mark a verb or punctuation member as having semantic meaning.</p>
<p>However, wherever possible I'd like to instead rely on native <code>spacy</code> information generated from the natural language processing pipeline.</p>
<hr />
<p>For now, I have marked the following three items as semantic assignment operators <em>in my code</em> and rely on <code>spacy</code>'s branch head identification system (obtained via an entity's head.lefts or head.rights) to isolate the colon.  Then, I analyze the semantic meaning of the sentence with understanding that the lemma of the colon is in fact &quot;be&quot; or &quot;list&quot;:</p>
<pre><code>{ 'is', 'are', ':' }
</code></pre>
<p>However, I'd instead like to rely on some generic <code>spacy</code> linguistic information so that the system is less English-specific.</p>
<p>Is there any information, member, or property that will allow me to derive that the punctuation token is a semantic assignment operator?</p>
<p>For example, the verbs have the <code>.lemma_</code> property that indicates they are what I am characterizing as assignment operators (<code>.lemma_ = 'be'</code>) whereas the punctuation mark ':' <em>does register</em> as a token, but seems to have no indicative information as to its logical purpose.</p>
<p>Yet it is an explicit transitive operator, and it comes up almost 35% of the time a noun is given a state or membership in the technical prose I am analyzing.</p>
",Multilingual Language Processing & Language Identification,information spacy token indicative meaning token suppose spacy system easily mark verb punctuation member semantic meaning however wherever possible like instead rely native information generated natural language processing pipeline marked following three item semantic assignment operator code rely branch head identification system obtained via entity head left head right isolate colon analyze semantic meaning sentence understanding lemma colon fact list however instead like rely generic linguistic information system le english specific information member property allow derive punctuation token semantic assignment operator example verb property indicates characterizing assignment operator whereas punctuation mark doe register token seems indicative information logical purpose yet explicit transitive operator come almost time noun given state technical prose analyzing
Heroku deployment of NLP model showing error ( app runs fine locally ),"<p>I have deployed my Flask App ( NLP model ) on Heroku. I was basically a price prediction model where some columns were in Japanese where I applied NLP + Nagisa Library for tokenization and some columns were numerical data.
I pickled vectorizers and the model and Finally added them to my Flask API. But after deployment when I added the values in the frontend and clicked on Predict button, the result is not getting displayed.
This is the exact error I am facing.</p>
<p><a href=""https://i.sstatic.net/PxUzt.png"" rel=""nofollow noreferrer"">Code</a></p>
<p>The exact code of Tokenizer_jp is :</p>
<pre><code>def tokenize_jp(doc):
        doc = nagisa.tagging(doc)
        return doc.words
</code></pre>
<p>I am not able to figure out how to fix this?</p>
",Multilingual Language Processing & Language Identification,heroku deployment nlp model showing error app run fine locally deployed flask app nlp model heroku wa basically price prediction model column japanese applied nlp nagisa library tokenization column numerical data pickled vectorizers model finally added flask api deployment added value frontend clicked predict button result getting displayed exact error facing code exact code tokenizer jp able figure fix
How to do a coding ( Arabic ÿ•ÿπÿ±ÿßÿ® Parsing) Python?,"<p>I need to do a python program that includes ÿ•ÿπÿ±ÿßÿ® ÿßŸÑŸÇÿ±ÿ¢ŸÜ ÿßŸÑŸÉÿ±ŸäŸÖ </p>

<p>For example 
input (ŸäŸÑÿπÿ® ÿßŸÑÿ∑ŸÅŸÑ  ) 
output ( 
ŸäŸÑÿπÿ® ŸÅÿπŸÑ ŸÖÿ∂ÿßÿ±ÿπ ŸÖÿ±ŸÅŸàÿπ Ÿà ÿπŸÑÿßŸÖÿ© ÿ±ŸÅÿπŸá ÿßŸÑÿ∂ŸÖÿ© ÿßŸÑÿ∏ÿßŸáÿ±ÿ© ÿπŸÑŸâ ÿßÿÆÿ±Ÿá
ÿßŸÑÿ∑ŸÅŸÑ ŸÅÿßÿπŸÑ ŸÖÿ±ŸÅŸàÿπ ŸàÿπŸÑÿßŸÖÿ© ÿ±ŸÅÿπŸá ÿßŸÑÿ∂ŸÖÿ© ÿßŸÑÿ∏ÿßŸáÿ±ÿ© ÿπŸÑŸâ ÿßÿÆÿ±Ÿá
)</p>
",Multilingual Language Processing & Language Identification,coding arabic parsing python need python program includes example input output
Wit AI partial translation causing a problem,"<p>I am using old code from a unity project but the wit.ai bot I've created is new. They have changed the way the response is sent and they use partial translations as well which does not work in my app. How do I remove the partial translations and only use the last chunk? I am using C# since the project is in Unity.</p>
<p>In the code snippet attached, the wit.ai checks for every word now. I just want the final result of the entire text. How do I do that?</p>
<pre><code>{
  &quot;text&quot;: &quot;Open&quot;
}
{
  &quot;text&quot;: &quot;Open driver&quot;
}
{
  &quot;text&quot;: &quot;Open driver's&quot;
}
{
  &quot;text&quot;: &quot;Open driver's door&quot;
}
{
  &quot;text&quot;: &quot;Open driver's door.&quot;
}
{
  &quot;entities&quot;: {},
  &quot;intents&quot;: [],
  &quot;speech&quot;: {
    &quot;confidence&quot;: 0.9983,
    &quot;tokens&quot;: [
      {
        &quot;end&quot;: 1260,
        &quot;start&quot;: 260,
        &quot;token&quot;: &quot;Open&quot;
      }
    ]
  },
  &quot;text&quot;: &quot;Open&quot;,
  &quot;traits&quot;: {}
}
{
  &quot;entities&quot;: {},
  &quot;intents&quot;: [],
  &quot;speech&quot;: {
    &quot;confidence&quot;: 0.8021,
    &quot;tokens&quot;: [
      {
        &quot;end&quot;: 1260,
        &quot;start&quot;: 260,
        &quot;token&quot;: &quot;Open&quot;
      },
      {
        &quot;end&quot;: 1800,
        &quot;start&quot;: 1260,
        &quot;token&quot;: &quot;driver's&quot;
      }
    ]
  },
  &quot;text&quot;: &quot;Open driver's&quot;,
  &quot;traits&quot;: {}
}
{
  &quot;entities&quot;: {},
  &quot;intents&quot;: [],
  &quot;speech&quot;: {
    &quot;confidence&quot;: 0.8374,
    &quot;tokens&quot;: [
      {
        &quot;end&quot;: 1260,
        &quot;start&quot;: 260,
        &quot;token&quot;: &quot;Open&quot;
      },
      {
        &quot;end&quot;: 1800,
        &quot;start&quot;: 1260,
        &quot;token&quot;: &quot;driver's&quot;
      },
      {
        &quot;end&quot;: 2040,
        &quot;start&quot;: 1800,
        &quot;token&quot;: &quot;door.&quot;
      }
    ]
  },
  &quot;text&quot;: &quot;Open driver's door.&quot;,
  &quot;traits&quot;: {}
}
{
  &quot;entities&quot;: {},
  &quot;intents&quot;: [],
  &quot;is_final&quot;: true,
  &quot;speech&quot;: {
    &quot;confidence&quot;: 0.8374,
    &quot;tokens&quot;: [
      {
        &quot;end&quot;: 1260,
        &quot;start&quot;: 260,
        &quot;token&quot;: &quot;Open&quot;
      },
      {
        &quot;end&quot;: 1800,
        &quot;start&quot;: 1260,
        &quot;token&quot;: &quot;driver's&quot;
      },
      {
        &quot;end&quot;: 2040,
        &quot;start&quot;: 1800,
        &quot;token&quot;: &quot;door.&quot;
      }
    ]
  },
  &quot;text&quot;: &quot;Open driver's door.&quot;,
  &quot;traits&quot;: {}
}
</code></pre>
<p>I would only like to get the below as response:</p>
<pre><code>&quot;text&quot;: &quot;Open driver's door.&quot;,
  &quot;traits&quot;: {}
}
{
  &quot;entities&quot;: {},
  &quot;intents&quot;: [],
  &quot;is_final&quot;: true,
  &quot;speech&quot;: {
    &quot;confidence&quot;: 0.8374,
    &quot;tokens&quot;: [
      {
        &quot;end&quot;: 1260,
        &quot;start&quot;: 260,
        &quot;token&quot;: &quot;Open&quot;
      },
      {
        &quot;end&quot;: 1800,
        &quot;start&quot;: 1260,
        &quot;token&quot;: &quot;driver's&quot;
      },
      {
        &quot;end&quot;: 2040,
        &quot;start&quot;: 1800,
        &quot;token&quot;: &quot;door.&quot;
      }
    ]
  },
  &quot;text&quot;: &quot;Open driver's door.&quot;,
  &quot;traits&quot;: {}
}
</code></pre>
",Multilingual Language Processing & Language Identification,wit ai partial translation causing problem using old code unity project wit ai bot created new changed way response sent use partial translation well doe work app remove partial translation use last chunk using c since project unity code snippet attached wit ai check every word want final result entire text would like get response
How can I convert Direct speech to Indirect Speech in Python?,"<p>I am working on a project to create summaries from call transcripts utilizing Extractive Summarization. Since extractive summarization is picking key sentences from call transcripts they are in direct speech.</p>
<p>Example:</p>
<p>&quot;I am facing issue with my laptop and I would like to get it replaced&quot;
to
&quot;Customer was facing issue with laptop and he would like to get his laptop replaced&quot;</p>
<p>I have tried looking for various resources, code or Library and the only thing that comes closest is one python library &quot;<a href=""https://pypi.org/project/despeech/"" rel=""nofollow noreferrer"">Despeech</a>&quot; but it is for German Language. I am unable to find something similar for English language.</p>
",Multilingual Language Processing & Language Identification,convert direct speech indirect speech python working project create summary call transcript utilizing extractive summarization since extractive summarization picking key sentence call transcript direct speech example facing issue laptop would like get replaced customer wa facing issue laptop would like get laptop replaced tried looking various resource code library thing come closest one python library despeech german language unable find something similar english language
How to create an iterable DataPipe with PyTorch using txt files,"<p>I have two text files to train a transformer model. However, instead of using PyTorch's own datasets, I'm using something I downloaded from the internet.</p>
<pre><code>source = open('./train_de.de', encoding='utf-8').read().split('\n')
target = open('./train_en.en', encoding='utf-8').read().split('\n')
</code></pre>
<p>With the code above, I have some Danish sentences in a list named &quot;source&quot;, and their translation in English sentences in another list named &quot;target&quot;.</p>
<p>My question is, how can I make an iterable DataPipe with PyTorch such that when I write something like:</p>
<pre><code>source, target = next(iter(train_iter))
</code></pre>
<p>this will give me the Danish sentence with it's corresponding English translation in seperate strings?</p>
",Multilingual Language Processing & Language Identification,create iterable datapipe pytorch using txt file two text file train transformer model however instead using pytorch datasets using something downloaded internet code danish sentence list named source translation english sentence another list named target question make iterable datapipe pytorch write something like give danish sentence corresponding english translation seperate string
List of dependencies in Spacy,"<p>I'm a beginner in NLP and i've decided to start with Spacy. It's simple to handle and to comprehend.
Neverthless, i can't acess to the full documentation or parsing.
I mean , i don't know the meaning of &quot;IN&quot; , &quot;RB&quot; for example
And, displacy that is used to display the dependency parsing doesn't show up a real information about the dependencies.
Exemple : <a href=""https://i.sstatic.net/eiXsA.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I understand the concept of dependency parsing, this example is in French.
What means the dependencies &quot;Fixed&quot; , &quot;cop&quot;, &quot;advmod&quot; and finally where can i get a full documentation about it.
Thank you</p>
",Multilingual Language Processing & Language Identification,list dependency spacy beginner nlp decided start spacy simple handle comprehend neverthless ace full documentation parsing mean know meaning rb example displacy used display dependency parsing show real information dependency exemple enter image description understand concept dependency parsing example french mean dependency fixed cop advmod finally get full documentation thank
Hindi english code mix language translation to english or hindi,"<p>i want to transliterate hindi english code mix language (popularly known as hinglish) to english .I tried google transliterate api ,but its not showing correct results is there any other alternative to that .for eg :- kya haal hai? to how are you?</p>
",Multilingual Language Processing & Language Identification,hindi english code mix language translation english hindi want transliterate hindi english code mix language popularly known hinglish english tried google transliterate api showing correct result alternative eg kya haal hai
NLP and spaCy: How to find a similar phrase in a string,"<p>thanks in advance for reading.</p>
<p>I'm in Python and using spaCy for processing English text. I have a phrase that I want to search for</p>
<pre><code>search_phrase = &quot;payment date&quot;
</code></pre>
<p>in a larger phrase</p>
<pre><code>text_to_be_searched = &quot;Party A will pay Party B on the transaction date.&quot;
</code></pre>
<p>And I want the search to match &quot;payment date&quot; with &quot;transaction date&quot; based on similarity.</p>
<p>How can I do this? I don't see an obvious way, and the only thing I can think of is to manually split <code>text_to_be_searched</code> into chunks.  One extra difficulty here is that the matching phrases could have different numbers of tokens, so I'd have to break it into chunks of 1, 2, ... 5 tokens and search each chunk in each set of chunks. For clarity, this would be:</p>
<p>Set of 1-token chunks:</p>
<pre><code>['Party','A','will', ..., 'date']
</code></pre>
<p>Set of 2-token chunks:</p>
<pre><code>['Party A','A will','will pay', ..., 'transaction date']
</code></pre>
<p>etc</p>
",Multilingual Language Processing & Language Identification,nlp spacy find similar phrase string thanks advance reading python using spacy processing english text phrase want search larger phrase want search match payment date transaction date based similarity see obvious way thing think manually split chunk one extra difficulty matching phrase could different number token break chunk token search chunk set chunk clarity would set token chunk set token chunk etc
how to define sample in a natural language processing model,"<pre><code>for doc in sample['documents']: 
</code></pre>
<p>The error is 'sample' undefined (I was trying to reproduce a natural language processing model)</p>
",Multilingual Language Processing & Language Identification,define sample natural language processing model error sample undefined wa trying reproduce natural language processing model
What does generate() do when using NLTK in Python?,"<p>I've been working with NLTK for the past three days to get familiar and reading the ""Natural Language processing"" book to understand what's going on. I'm curious if someone could clarify for me the following:</p>

<blockquote>
  <p>Note that the first time you run this command, it is slow because it
  gathers statistics about word sequences. Each time you run it, you
  will get different output text. Now try generating random text in the
  style of an inaugural address or an Internet chat room. Although the
  text is random, it re-uses common words and phrases from the source
  text and gives us a sense of its style and content. (What is lacking
  in this randomly generated text?)</p>
</blockquote>

<p>This part of the text, <a href=""http://nltk.org/book/ch01.html"" rel=""nofollow"">chapter 1,</a> simply says that it ""gathers statistics"" and it will get ""different output text""</p>

<p>What <em>specifically</em> does generate do and how does it work? </p>

<p>This example of <code>generate()</code> uses text3, which is the Bible's Genesis:</p>

<blockquote>
  <p>In the beginning , between me and thee and in the garden thou mayest
  come in unto Noah into the ark , and Mibsam , And said , Is there yet
  any portion or inheritance for us , and make thee as Ephraim and as
  the sand of the dukes that came with her ; and they were come . Also
  he sent forth the dove out of thee , with tabret , and wept upon them
  greatly ; and she conceived , and called their names , by their names
  after the end of the womb ? And he</p>
</blockquote>

<p>Here, the <code>generate()</code> function seems to simply output phrases created by cutting off text at punctuation and randomly reassembling it but it has a bit of readability to it.</p>
",Multilingual Language Processing & Language Identification,doe generate using nltk python working nltk past three day get familiar reading natural language processing book understand going curious someone could clarify following note first time run command slow gather statistic word sequence time run get different output text try generating random text style inaugural address internet chat room although text random us common word phrase source text give u sense style content lacking randomly generated text part text chapter simply say gather statistic get different output text specifically doe generate doe work example us text bible genesis beginning thee garden thou mayest come unto noah ark mibsam said yet portion inheritance u make thee ephraim sand duke came come also sent forth dove thee tabret wept upon greatly conceived called name name end womb function seems simply output phrase created cutting text punctuation randomly reassembling ha bit readability
How do I extract specific words from a text into another column (by looking at an external DF),"<p>I have scraped vacancies from Indeed, and these are two of the columns in my DataFrame:</p>
<pre><code>        Location           Description
0       deventer  Hygi√®ne S√©curit√© ...
1        hengelo  Supervisor Finan ...
2         zwolle  Compensations &amp;  ...
</code></pre>
<p>28000 rows</p>
<p>I want to have another column that extracts the skills from the Description column, by looking at the dataframe that is shown below.</p>
<pre><code>           soft_skill
0       communication
1  abstract reasoning
2   abstract thinking
</code></pre>
<p>930 rows</p>
<p>So ideally my new DF will look like this:</p>
<pre><code>        Location           Description            Skills
0       deventer  Hygi√®ne S√©curit√© ...     collaboration
1        hengelo  Supervisor Finan ...     communication
2         zwolle  Compensations &amp;  ... critical thinking
</code></pre>
<p>28000 rows</p>
<p>Does anybody have an idea on how I could do this? I am also working on a ML/NER approach (using spacy), but I would first like to try this.</p>
<p>A complete and clean description looks like this, and the bold words should be extracted into a new column:</p>
<blockquote>
<p>hygi√®ne s√©curit√© environnement keolis netherlands has approximately 2100 employees with whom we jointly provide public transport in ijsselvecht utrecht twente and almere and on the train routes zutphenoldenzaal zwolleenschede and zwollekampen each year we transport 45 million passengers with 700 buses and 25 trains working from two intrinsic values clear and enterprising keolis nederland is a 100 subsidiary of the internationally operating mobility company keolis group which provides public transport in 16 countries with 68500 employees missions as a qhse manager at keolis netherlands you will focus on the creation of a safe and healthy workplace the safety and health of the employees as well as the safeguarding of the quality of service and its environmental friendliness in this role you will contribute to the various management systems keolis strives for environmental management system quality management system safety management system and the other standards keolis commits to thus contributing to the achievement of continuous improvement together with a team of 3 colleagues you will bring the awareness quality and compliance of all qhse aspects within the organization to a higher level there are challenges in the department to further professionalize including further development of the qhse policy design of the csr policy qhse awareness and prevention campaigns organization and management of qhse audits internal control and especially how do we secure this in the organization and keep all stakeholders connected and prepared a nice challenging position within an international mobility provider with nice colleagues why with about 2100 employees we are a large organization but we remain informal so you can easily walk in on colleagues for a chat but also for example to spar from our headquarters in deventer you will manage qhse profil a dynamic position in which no day is the same within an organization that is developing and to which you yourself can make an essential contribution we have excellent fringe benefits such as at least 27 days of vacation depending on age a yearend bonus and a wide range of online training courses we have an attractive lease scheme and we facilitate a phone and laptop there is plenty of room for personal development and contribution within an internationally operating mobility organization and last but not least the informal working atmosphere at our cosy office in deventer competencies which fit within keolis are <strong>collaborating working together responsibility openness goodwill commitment relevant knowledge and experience</strong> we see for the position a relevant completed education at hbo level towards qhse demonstrable experience in similar positions demonstrable experience in the substantive supervision and coaching of qhse employees knowledge and working in the english language is an advantage extensive and uptodate knowledge of relevant environmental and occupational health and safety laws and regulations knowledge of the applicable nen  vca and iso standards excellent advisory and communication skills work experience within public transport organizations is an advantage</p>
</blockquote>
",Multilingual Language Processing & Language Identification,extract specific word text another column looking external df scraped vacancy indeed two column dataframe row want another column extract skill description column looking dataframe shown row ideally new df look like row doe anybody idea could also working ml ner approach using spacy would first like try complete clean description look like bold word extracted new column hygi ne curit environnement keolis netherlands ha approximately employee jointly provide public transport ijsselvecht utrecht twente almere train route zutphenoldenzaal zwolleenschede zwollekampen year transport million passenger bus train working two intrinsic value clear enterprising keolis nederland subsidiary internationally operating mobility company keolis group provides public transport country employee mission qhse manager keolis netherlands focus creation safe healthy workplace safety health employee well safeguarding quality service environmental friendliness role contribute various management system keolis strives environmental management system quality management system safety management system standard keolis commits thus contributing achievement continuous improvement together team colleague bring awareness quality compliance qhse aspect within organization higher level challenge department professionalize including development qhse policy design csr policy qhse awareness prevention campaign organization management qhse audit internal control especially secure organization keep stakeholder connected prepared nice challenging position within international mobility provider nice colleague employee large organization remain informal easily walk colleague chat also example spar headquarters deventer manage qhse profil dynamic position day within organization developing make essential contribution excellent fringe benefit least day vacation depending age yearend bonus wide range online training course attractive lease scheme facilitate phone laptop plenty room personal development contribution within internationally operating mobility organization last least informal working atmosphere cosy office deventer competency fit within keolis collaborating working together responsibility openness goodwill commitment relevant knowledge experience see position relevant completed education hbo level towards qhse demonstrable experience similar position demonstrable experience substantive supervision coaching qhse employee knowledge working english language advantage extensive uptodate knowledge relevant environmental occupational health safety law regulation knowledge applicable nen vca iso standard excellent advisory communication skill work experience within public transport organization advantage
How to create a custom BERT language model for a different language?,"<p>I want to create a language translation model using transformers. However, Tensorflow seems to only have a BERT model for English <a href=""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"" rel=""nofollow noreferrer"">https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4</a> . If I want a BERT for another language, what is the best way to go about accomplishing this? Should I create a new BERT or can I train Tensorflow's own BertTokenizer on another language?</p>
",Multilingual Language Processing & Language Identification,create custom bert language model different language want create language translation model using transformer however tensorflow seems bert model english want bert another language best way go accomplishing create new bert train tensorflow berttokenizer another language
C++ - How to read Unicode characters( Hindi Script for e.g. ) using C++ or is there a better Way through some other programming language?,"<p>I have a hindi script file like this:</p>
<pre><code>3.  ‡§≠‡§æ‡§∞‡§§ ‡§ï‡§æ ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§ï‡§æ‡§´‡•Ä ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß ‡§è‡§µ‡§Ç ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§π‡•à‡•§
</code></pre>
<p>I have to write a program which adds a position to each and every word in each sentence.
Thus the numbering for every line for a particular word position should start off with 1 in parentheses. The output should be something like this.</p>
<pre><code>3.  ‡§≠‡§æ‡§∞‡§§(1) ‡§ï‡§æ(2) ‡§á‡§§‡§ø‡§π‡§æ‡§∏(3) ‡§ï‡§æ‡§´‡•Ä(4) ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß(5) ‡§è‡§µ‡§Ç(6) ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§(7) ‡§π‡•à(8) ‡•§(9)
</code></pre>
<p>The meaning of the above sentence is:</p>
<pre><code>3.  India has a long and rich history.
</code></pre>
<p>If you observe the '‡•§'( which is a full stop in hindi equivalent to a '.' in English ) also has a word position and similarly other special symbols would also have as I am trying to go about English-Hindi Word alignment( a part of Natural Language Processing ( NLP ) ) so the full stop in english '.' should map to '‡•§'  in Hindi. Serial nos remain as it is untouched.
I thought reading character by character could be a solution. How can I do this?</p>
<p>The thing is I am able to get word positions for my English text using C++ as I was able to read character by character using ASCII values in C++ but I don't have a clue to how to go about the same for the hindi text.</p>
<p>The final aim of all this is to see which word position of the English text maps to which postion in Hindi. This way I can achieve bidirectional alignment.</p>
<p>Thank you for your time...:)</p>
",Multilingual Language Processing & Language Identification,c read unicode character hindi script e g using c better way programming language hindi script file like write program add position every word sentence thus numbering every line particular word position start parenthesis output something like meaning sentence observe full stop hindi equivalent english also ha word position similarly special symbol would also trying go english hindi word alignment part natural language processing nlp full stop english map hindi serial remain untouched thought reading character character could solution thing able get word position english text using c wa able read character character using ascii value c clue go hindi text final aim see word position english text map postion hindi way achieve bidirectional alignment thank time
How to detect what dialect a written text comes from (given a parent language),"<p>Ex. Suppose I have a Spanish text, and it has a lot of Colombian slang. How would I be able to detect that it is Colombian Spanish?</p>
",Multilingual Language Processing & Language Identification,detect dialect written text come given parent language ex suppose spanish text ha lot colombian slang would able detect colombian spanish
Keyphrase extraction in Python - How to preprocess the text to get better performances,"<p>I'm trying to extract keyphrases from some English texts but I think that the quality of my results is affected by how the sentences are formulated. For example:</p>
<p>Sentence 1</p>
<pre><code>import pke

text = &quot;Manufacture of equipment for the production and use of hydrogen.&quot;

# define the valid Part-of-Speeches to occur in the graph
pos = {'NOUN', 'PROPN', 'ADJ'}
# define the grammar for selecting the keyphrase candidates
grammar = &quot;NP: {&lt;ADJ&gt;*&lt;NOUN|PROPN&gt;+}&quot;

extractor = pke.unsupervised.PositionRank()
extractor.load_document(input=text, language='en')
extractor.grammar_selection(grammar = grammar)
extractor.candidate_selection(maximum_word_number = 5) 
extractor.candidate_weighting(window = 5, pos = pos) 
keyphrases = extractor.get_n_best(n = 10, redundancy_removal = True)
#dict_keys(['bert', 'state-of-the-art model'])

keyphrases
</code></pre>
<p>returns this:</p>
<pre><code>[('equipment', 0.2712123844387682),
 ('production', 0.24805759926043025),
 ('manufacture', 0.20214941371717332),
 ('use', 0.14005307983173715),
 ('hydrogen', 0.1385275227518909)]
</code></pre>
<p>While:</p>
<p>Sentence 2</p>
<pre><code>text = &quot;Equipment manufacture for hydrogen production and hydrogen use&quot;
</code></pre>
<p>with the same piece of code returns this:</p>
<pre><code>[('hydrogen production', 0.5110246649313613),
 ('hydrogen use', 0.4067693357279659),
 ('equipment manufacture', 0.3619113634611547)]
</code></pre>
<p>which, in my opinion, is a better result since allows me to understand what we're talking about.</p>
<p>I wonder if there's a way to preprocess Sentence 1 making it more similar to Sentence 2. I've already tried with Neuralcoref but, in this particular case, doesn't help me.</p>
<p>Thank you in advance for any suggestion.
Francesca</p>
",Multilingual Language Processing & Language Identification,keyphrase extraction python preprocess text get better performance trying extract keyphrases english text think quality result affected sentence formulated example sentence return sentence piece code return opinion better result since allows understand talking wonder way preprocess sentence making similar sentence already tried neuralcoref particular case help thank advance suggestion francesca
PYTHON: Extract Non-English words and iterate it over a dataframe,"<p>I have a table of about 30,000 rows and need to extract non-English words from a column named <code>dummy_df </code> from a <code>dummy_df</code> dataframe. I need to put the non-english words in an adjacent column named <code>non_english</code>.</p>
<p>A dummy data is as thus:</p>
<pre><code>dummy_df = pandas.DataFrame({'outcome':    [&quot;I want to go to church&quot;,  &quot;I love Matauranga&quot;, &quot;Take me to  Oranga Tamariki&quot;]})
</code></pre>
<p>My idea is to extract non-English words from a sentence, and then iterate the process over a dataframe. I was able to accurately extract non-English words from a sentence with this code:</p>
<pre><code>import nltk
nltk.download('words')
from nltk.corpus import words

words = set(nltk.corpus.words.words())

sent = &quot;I love Matauranga&quot;
&quot; &quot;.join(w for w in nltk.wordpunct_tokenize(sent) \
         if not w.lower() in words or not w.isalpha())
</code></pre>
<p>The result of the above code is <code>'Matauranga'</code> which is perfectly correct.</p>
<p>But when I try to iterate the code over a dataframe using this code:</p>
<pre><code>import nltk
nltk.download('words')
from nltk.corpus import words

def no_english(text):
  words = set(nltk.corpus.words.words())
  &quot; &quot;.join(w for w in nltk.wordpunct_tokenize(text['outcome']) \
         if not w.lower() in words or not w.isalpha())

dummy_df['non_english'] = dummy_df.apply(no_english, axis = 1)
print(dummy_df)
</code></pre>
<p>I got an undesirable result in that the <code>non_english</code> column has <code>none</code> value instead of the desired non-english words (see below):</p>
<pre><code>                       outcome non_english
0       I want to go to church        None
1            I love Matauranga        None
2  Take me to  Oranga Tamariki        None
3                                     None
</code></pre>
<p>Instead, the desired result should be:</p>
<pre><code>                       outcome        non_english
0       I want to go to church        
1            I love Matauranga        Matauranga
2  Take me to  Oranga Tamariki        Oranga Tamariki
</code></pre>
",Multilingual Language Processing & Language Identification,python extract non english word iterate dataframe table row need extract non english word column named dataframe need put non english word adjacent column named dummy data thus idea extract non english word sentence iterate process dataframe wa able accurately extract non english word sentence code result code perfectly correct try iterate code dataframe using code got undesirable result column ha value instead desired non english word see instead desired result
Making a biagram table from Corpus,"<p>i am trying to make a biagram table from a corpus with a text file which contains 100 sentences but i am unable to do it. right now i only have code with showing how many sentences that text file has. I am new to machine translation and couldnt work it out from NLTK packages. Can anyone help? thanks</p>
<pre><code>from nltk.corpus.reader import WordListCorpusReader

reader = nltk.corpus.PlaintextCorpusReader('.', ['100sentence.txt'])

print (&quot;The number of sentences =&quot;, len(reader.sents()))
</code></pre>
<p>basically what i want to do is make a biagram table from my txt file and being able to count words that i want from the text file</p>
",Multilingual Language Processing & Language Identification,making biagram table corpus trying make biagram table corpus text file contains sentence unable right code showing many sentence text file ha new machine translation couldnt work nltk package anyone help thanks basically want make biagram table txt file able count word want text file
How to return all the forms of a word in the English language with C#?,"<p>Is there a way to find all the forms of a word in C#. I need to write a program where the user inputs a word such as <code>box</code> and the program returns the a string array of all the forms of the word like below.</p>
<p><code>box unboxed boxing </code>
and when a user enters a word like <code>battery</code> the program may output as below</p>
<p><code>battery batteries</code>.
Is there an existing implementation of this or how can I proceed to accomplish that?
Thank You for your help.</p>
",Multilingual Language Processing & Language Identification,return form word english language c way find form word c need write program user input word program return string array form word like user enters word like program may output existing implementation proceed accomplish thank help
Training transformer model for summarization task with data generated by translation transformer,"<p>I am working on a project and this question popped into my mind. Please enlighten me if that is a wrong way of thinking or if it actually might work.</p>
<p>I have a large dataset of text data in language X but a summarization model pretrained for such language does not exist. However, there are models that are trained to do machine translation from language X to language Y that I could use with my dataset. Then, it would be possible to use a summarization model for language Y and translate the output back to X.</p>
<p>Please tell me if there is something wrong with my idea. I am fairly new to NLP.
Thanks</p>
",Multilingual Language Processing & Language Identification,training transformer model summarization task data generated translation transformer working project question popped mind please enlighten wrong way thinking actually might work large dataset text data language x summarization model pretrained language doe exist however model trained machine translation language x language could use dataset would possible use summarization model language translate output back x please tell something wrong idea fairly new nlp thanks
How to create a feature that detect age in text in different languages?,"<p>I have a text classification task in several languages. What aproach should use if I would like to create a feature that extract age from text if this are the possible classes: <code>18-24</code>,<code>25-34</code>,<code>35-49</code> and <code>50-xx""</code> and I have only tweets as a corpus. I all ready tried using all the tweets but with very low performance(0.66) any idea of how to aproach this task?. Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,create feature detect age text different language text classification task several language aproach use would like create feature extract age text possible class tweet corpus ready tried using tweet low performance idea aproach task thanks advance
How to solve missing words in nltk.corpus.words.words()?,"<p>I have tried to remove non-English words from a text. Problem many other words are absent from the NLTK words corpus.</p>
<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
    
lst = ['I have equipped my house with a new [xxx] HP203X climatisation unit']
df = pd.DataFrame(lst, columns=['Sentences'])
    
import nltk 
nltk.download('words')
words = set(nltk.corpus.words.words())
    
df['Sentences'] = df['Sentences'].apply(lambda x: &quot; &quot;.join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in (words)))
df
</code></pre>
<p>Input: <code>I have equipped my house with a new [xxx] HP203X climatisation unit</code><br>
Result: <code>I have my house with a new unit</code></p>
<p>Should have been: <code>I have equipped my house with a new climatisation unit</code></p>
<p>I can't figure out how to complete <code>nltk.corpus.words.words()</code> to avoid words like <code>equipped</code>, <code>climatisation</code> to be remouved from the sentences.</p>
",Multilingual Language Processing & Language Identification,solve missing word nltk corpus word word tried remove non english word text problem many word absent nltk word corpus code input result figure complete avoid word like remouved sentence
How to recognise a particular user in a long multi-user internet chat log?,"<p>Here is an online programming contest we are planning to have. </p>

<p><strong>What could be possible approaches to solving the same?</strong></p>

<p>From a random IRC (Internet Relay Chat) log, a small percentage of the user nicknames will be randomly deleted. The participant‚Äôs code must be able to fill in the missing user nicks. In other words, this event requires you to come up with an intelligent program that can figure out ‚Äúwho could have said what‚Äù.</p>

<p>It may be assumed that all communication will be in modern English, with or without punctuation.</p>

<p>For example -</p>

<p>Original Chat:
<code>...
&lt;user1&gt;: Hey!
&lt;user2&gt;: Hello! Where are you from, user1?
&lt;user3&gt;: Can anybody help me out with Gnome installation?
&lt;user1&gt;: India. user3, do you have the X Windows System installed?
&lt;user2&gt;: Cool. What is Gnome, user3?
&lt;user3&gt;: I don‚Äôt know. How do I check?
&lt;user3&gt;: Its a desktop environment, user2.
&lt;user2&gt;: Oh yeah! Just googled. 
&lt;user1&gt;: Type ‚Äústartx‚Äù on the command line. Login as root and type ‚Äúapt-get install gnome‚Äù.
&lt;user3&gt;: Thanks!
&lt;user5&gt;: I‚Äôm root, obey me!
&lt;user2&gt;: Huh?!
&lt;user3&gt;: user2, you better start using Linux!
...</code></p>

<p>The following only will be given to the participant.</p>

<p>Chat log with some nicks deleted:</p>

<p>..</p>

<p><strong></strong>: Hey!
<strong></strong>: Hello! Where are you from, user1?
<strong></strong>: Can anybody help me out with Gnome installation?
<strong></strong>: India. user3, do you have the X Windows System installed?
<strong></strong>: Cool. What is Gnome, user3?
<strong>&lt;%%%></strong>: I don‚Äôt know. How do I check?
<strong>&lt;%%%>:</strong> Its a desktop environment, user2.
<strong></strong>: Oh yeah! Just googled. 
<strong></strong>: Type ‚Äústartx‚Äù on the command line. Login as root and type ‚Äúapt-get install gnome‚Äù.
<strong></strong>: Thanks!
<strong>&lt;%%%></strong>: I‚Äôm root, obey me!
<strong>&lt;%%%></strong>: Huh?!
<strong></strong>: user2, you better start using Linux!
... </p>

<p>The participant‚Äôs code will have the task of replacing ""&lt;%%%>s"" with the appropriate user nicks. In ambiguous cases, like the random comment by  in the above example (which could have been said by any other user too!), the code should indicate the same.</p>
",Multilingual Language Processing & Language Identification,recognise particular user long multi user internet chat log online programming contest planning could possible approach solving random irc internet relay chat log small percentage user nickname randomly deleted participant code must able fill missing user nick word event requires come intelligent program figure could said may assumed communication modern english without punctuation example original chat following given participant chat log nick deleted hey hello user anybody help gnome installation india user x window system installed cool gnome user know check desktop environment user oh yeah googled type startx command line login root type apt get install gnome thanks root obey huh user better start using linux participant code task replacing appropriate user nick ambiguous case like random comment example could said user code indicate
Extract all the data within parenthesis using spacy matcher,"<p>I am trying to extract data from within paranthesis using a spacy matcher.</p>
<p>Say the text is: ' I am on StackOverflow(for x years) and I ask (technical) questions here about Natural Language Processing (NLP) (information retrieval)'</p>
<p>The desired output of the matcher is : (for x years), (technical) (NLP) (information retrieval)</p>
<p>Below is the code which I tried to work with</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)
text = 'I am on StackOverflow(for x years) and I ask (technical) questions here about Natural Language Processing (NLP) (information retrieval)'
doc = nlp(text)
matcher = Matcher(nlp.vocab)
pattern = [{&quot;ORTH&quot;: '(', }, {&quot;TEXT&quot;: {&quot;REGEX&quot;: r&quot;.*?&quot;}}, {&quot;ORTH&quot;: ')'}]
matcher.add('paranthesis_data', None, pattern)
matches = matcher(doc)
for match_id, start, end in matches:
   print(nlp.vocab.strings[match_id], doc[start:end])
</code></pre>
<p>The output I am getting is as below:</p>
<p><a href=""https://i.sstatic.net/yUaVu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yUaVu.png"" alt=""enter image description here"" /></a></p>
<p>but I would like the output like :
data (for x years)
data (technical)
data (NLP)
data (information retrieval)</p>
<p>I know I could use regex, but that's not an option in my project. if I use 'OP' it is returning very long string matching, something like: (for x years) and I ask (technical)</p>
<p>Any help is very much appreciated and I would be very thankful.</p>
",Multilingual Language Processing & Language Identification,extract data within parenthesis using spacy matcher trying extract data within paranthesis using spacy matcher say text stackoverflow x year ask technical question natural language processing nlp information retrieval desired output matcher x year technical nlp information retrieval code tried work output getting would like output like data x year data technical data nlp data information retrieval know could use regex option project use op returning long string matching something like x year ask technical help much appreciated would thankful
Mixed kana and kanji romanization to romaji in R,"<p>I have a large character vector of japanese words (mixed kanji and kana) which needs to be romanized (to romaji).</p>
<p>However with the available functions, (<code>zipangu::str_conv_romanhira()</code> and <code>audubon::strj_romanize()</code>), I am not getting the desired results.</p>
<p>For example for ÂåóÊµ∑ÈÅì (Hokkaido), <code>zipangu::str_conv_romanhira()</code> convert it to chinese pinyin and <code>audubon::strj_romanize()</code> converts only kana characters.</p>
<p>How to convert such mixed kana and kanji text to romaji.</p>
<pre><code>library(zipangu)
library(stringi)
library(audubon)


str_conv_romanhira(&quot;ÂåóÊµ∑ÈÅì&quot;, &quot;roman&quot;)
#&gt; [1] &quot;bƒõi h«éi d√†o&quot;

stri_trans_general(&quot;ÂåóÊµ∑ÈÅì&quot;, &quot;Any-Latin&quot;)
#&gt; [1] &quot;bƒõi h«éi d√†o&quot;

strj_romanize(&quot;ÂåóÊµ∑ÈÅì&quot;)
#&gt; [1] &quot;&quot;

</code></pre>
",Multilingual Language Processing & Language Identification,mixed kana kanji romanization romaji r large character vector japanese word mixed kanji kana need romanized romaji however available function getting desired result example hokkaido convert chinese pinyin convert kana character convert mixed kana kanji text romaji
German keyword search - look for every possible combination,"<p>I'm working on a project where I define some nouns like <code>Haus, Boot, Kampf, ...</code> and what to detect every version (singular/plurar) and every combination of these words in sentences. For example, the algorithm should return true if a sentences does contain one of : <code>H√§user, Hausboot, H√§userkampf, Kampfboot, Hausbau, Bootsanleger, ...</code>.</p>
<p>Are you familiar with an algorithm that can do such a thing (preferable in R)? Of course I could implement this manually, but I'm pretty sure that something should already exist.</p>
<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,german keyword search look every possible combination working project define noun like detect every version singular plurar every combination word sentence example algorithm return true sentence doe contain one familiar algorithm thing preferable r course could implement manually pretty sure something already exist thanks
How to count the number of spoken syllables in an audio file?,"<p>I have many audio files with clean audio and only spoken voice in Mandarin Chinese. I need to estimate of how many syllables are spoken in each file. Is there a tool for OS X, Windows, or Linux that can estimate these?</p>

<pre><code>sample01.wav 15
sample02.wav 8
sample03.wav 5
sample04.wav 1
sample05.wav 18
</code></pre>

<p>As there are many files, command-line or batch-capable software is preferred, e.g.:</p>

<pre><code>$ application sample01.wav
15
</code></pre>

<ul>
<li>A solution that uses speech-to-text, then counts the number of characters present would be suitable to.</li>
</ul>
",Multilingual Language Processing & Language Identification,count number spoken syllable audio file many audio file clean audio spoken voice mandarin chinese need estimate many syllable spoken file tool x window linux estimate many file command line batch capable software preferred e g solution us speech text count number character present would suitable
which algorithm does google keyboard uses for automatic suggestions (personal vocab included)?,"<p>I am confused since google cannnot train their text generation models with each individuals personal vocabulary. </p>

<p>I was trying to develop something similar but i got stuck when the number of classes became dynamic during neural network training. </p>

<p>if i dont know the number of classes , how can 1 give the size of the layers and the dimension of input.</p>

<p>lets say google knows words from english vocab and i include some words from my slang into my personal dictionary, it is able to suggest those words to me in future.</p>

<p>assume that the google vocab consists of 10 words and i give it 5 words in a sequence , it one hot encodes them to a size of (5x10) vector of 1's and 0's.</p>

<p>and i then include 4 new words . now total vocab size = 14 </p>

<p>but the RNN (if used) was trained using a vocab size of just 10 words. it cannnot encode those new words since those didnt belong in the vocab while training. </p>

<p>does it retrain its text generation model using (5x14) sized vectors?</p>
",Multilingual Language Processing & Language Identification,algorithm doe google keyboard us automatic suggestion personal vocab included confused since google cannnot train text generation model individual personal vocabulary wa trying develop something similar got stuck number class became dynamic neural network training dont know number class give size layer dimension input let say google know word english vocab include word slang personal dictionary able suggest word future assume google vocab consists word give word sequence one hot encodes size x vector include new word total vocab size rnn used wa trained using vocab size word cannnot encode new word since didnt belong vocab training doe retrain text generation model using x sized vector
How to extract only English words from a from big text corpus using nltk?,"<p>I am want remove all  non dictionary english words from text corpus. I have removed stopwords, tokenized and countvectorized the data. I need extract only the English words and attach them back to the dataframe .</p>

<pre><code>data['Clean_addr'] = data['Adj_Addr'].apply(lambda x: ' '.join([item.lower() for item in x.split()]))
        data['Clean_addr']=data['Clean_addr'].apply(lambda x:"""".join([item.lower() for item in x if  not  item.isdigit()]))
        data['Clean_addr']=data['Clean_addr'].apply(lambda x:"""".join([item.lower() for item in x if item not in string.punctuation]))
        data['Clean_addr'] = data['Clean_addr'].apply(lambda x: ' '.join([item.lower() for item in x.split() if item not in (new_stop_words)]))
        cv = CountVectorizer( max_features = 200,analyzer='word')
        cv_addr = cv.fit_transform(data.pop('Clean_addr'))
</code></pre>

<p>Sample Dump of the File I am using</p>

<p><a href=""https://www.dropbox.com/s/allhfdxni0kfyn6/Test.csv?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/allhfdxni0kfyn6/Test.csv?dl=0</a></p>
",Multilingual Language Processing & Language Identification,extract english word big text corpus using nltk want remove non dictionary english word text corpus removed stopwords tokenized countvectorized data need extract english word attach back dataframe sample dump file using
While translating bulk text files to English using Googletrans it shows &#39;Could not find TKK token for this request&#39;. What could be the reason?,"<p>I am using <code>google trans 4.0.0rc1</code>. This is the code I am using:</p>
<pre><code>from googletrans import Translator
translator = Translator()

for file_number in range(0, 50):
    f = open(f'train\\train_{file_number}.txt', 'r')

    if f.mode == 'r':
        contents = f.read()
        print(contents)

    result = translator.translate(contents, dest='en')
    with open(f'train\\trans_{file_number}.txt', 'w') as f:
        f.write(result.text)
</code></pre>
<p>And here is the error. The link mentioned in the error has no info on the TKK token stuff. What should I do?</p>
<pre><code> File &quot;C:\Anaconda3\lib\site-packages\googletrans\gtoken.py&quot;, line 67, in _update
    raise Exception('Could not find TKK token for this request.\nSee https://github.com/ssut/py-googletrans/issues/234 for more details.')

Exception: Could not find TKK token for this request.
See https://github.com/ssut/py-googletrans/issues/234 for more details.
</code></pre>
",Multilingual Language Processing & Language Identification,translating bulk text file english using googletrans show could find tkk token request could reason using code using error link mentioned error ha info tkk token stuff
Create new dataset by Google Translator,"<p>I want to create dataset in Persian using google translation API. I want to use GoEmotions dataset. I do not know where should I start. Can somebody help me?</p>
",Multilingual Language Processing & Language Identification,create new dataset google translator want create dataset persian using google translation api want use goemotions dataset know start somebody help
How can I use &quot;NER&quot; for German Language with stanford-corenlp?,"<p>I am trying to use nlp for german language but it does not work!
I was making the pipeline and then NER to find the entity of each element in sentence which is working perfectly for English but not Geman language!
I also added German language to maven...
here is my pipeline:</p>
<pre><code>public class Pipeline {
private static Properties properties;
private static String propertiesName = &quot;tokenize, ssplit, pos, lemma, ner&quot;;
private static StanfordCoreNLP stanfordCoreNLP;

private Pipeline() {
}

static {
    properties = new Properties();
    properties.setProperty(&quot;annotators&quot;, propertiesName);
}

public static StanfordCoreNLP getPipeline(){
    if (stanfordCoreNLP == null){
        stanfordCoreNLP = new StanfordCoreNLP(properties);
    }
    return stanfordCoreNLP;
}
</code></pre>
<p>}</p>
<p>and here is my NER:</p>
<pre><code>public class NER {
public static void main(String[] args) {
    StanfordCoreNLP stanfordCoreNLP = Pipeline.getPipeline();
    String text = &quot;hello My name is xxx. I live in Austria.&quot;;

    CoreDocument coreDocument = new CoreDocument(text);
    stanfordCoreNLP.annotate(coreDocument);

    List&lt;CoreLabel&gt; coreLabelList = coreDocument.tokens();

    for (CoreLabel coreLabel: coreLabelList){
        String ner = coreLabel.get(CoreAnnotations.NamedEntityTagAnnotation.class);
        System.out.println(coreLabel.originalText() + &quot;-&gt;&quot;+ner);
    }
}
</code></pre>
<p>}</p>
<p>and here is my maven dependency:</p>
<pre><code>&lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.9.2&lt;/version&gt;
        &lt;classifier&gt;models&lt;/classifier&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;4.0.0&lt;/version&gt;
        &lt;classifier&gt;models-german&lt;/classifier&gt;
    &lt;/dependency&gt;
</code></pre>
<p>what should I change or add to use it also for German Language?</p>
",Multilingual Language Processing & Language Identification,use ner german language stanford corenlp trying use nlp german language doe work wa making pipeline ner find entity element sentence working perfectly english geman language also added german language maven pipeline ner maven dependency change add use also german language
R: Gibberish detection using Markov model (how to adjust for the text length),"<p>First, I create a table with frequencies of two-letter-combinations out of a German text file:</p>
<pre class=""lang-r prettyprint-override""><code># Load a German language text file
text &lt;- read_tsv(&quot;http://www.reduts.net/deutsch.txt&quot;)
colnames(text) &lt;- c(&quot;id&quot;, &quot;text&quot;)
text &lt;- paste(text$text, collapse = &quot; &quot;)

# Calculate all two-letters-combinations
tokens_char &lt;- function(str, window = 2) {
  str &lt;- stringi::stri_replace_all_regex(str, &quot;\\W&quot;, &quot;&quot;)
  str &lt;- tolower(str)
  win &lt;- window - 1
  len1 &lt;- seq_len(nchar(str) - win)
  stringi::stri_sub(str, from = len1, to = len1 + win)
}
</code></pre>
<p>This creates a lookup-table containing two columns: 1. the two-letter-combination and 2. the frequency of all combinations appearing in the text:</p>
<pre class=""lang-r prettyprint-override""><code>
lookuptable &lt;- tibble(
    token = tokens_char(text, window = 2) 
  ) %&gt;% count(token, sort = TRUE) %&gt;%
  mutate(token2 = token) %&gt;%
  separate(token, into = c(&quot;first&quot;, &quot;second&quot;), sep = 1) %&gt;%
  group_by(first) %&gt;%
  mutate(total = sum(n),
         freq = n / total) %&gt;%
  ungroup() %&gt;%
  mutate(token = token2,
         token2 = NULL,
         first = NULL,
         second = NULL,
         total = NULL) %&gt;%
  select(token, freq)

&gt; lookuptable
# A tibble: 1,522 x 2
   token  freq
   &lt;chr&gt; &lt;dbl&gt;
 1 en    0.233
 2 er    0.225
 3 ch    0.861
 4 de    0.446
 5 ei    0.127
 6 te    0.302
 7 nd    0.186
 8 in    0.228
 9 ie    0.209
10 ge    0.494
# ... with 1,512 more rows

</code></pre>
<p>Then, I have a function that calculates the product of the frequencies of all 2-letter-combinations for a given text. For example for the word &quot;test&quot; I lookup the probabilities of &quot;te&quot; &quot;es&quot; and &quot;st&quot;. These probabilities are then multiplied: P(&quot;te&quot;) * P(&quot;es&quot;) * P(&quot;st&quot;):</p>
<pre class=""lang-r prettyprint-override""><code>
lookup_text &lt;- function(text = &quot;&quot;){
  df &lt;- data.frame(token = tokens_char(text, window = 2)) %&gt;%
    left_join(lookuptable, by = &quot;token&quot;) 
  
  # Return product of all probabilities
  return(prod(df$freq))
}

</code></pre>
<p>Now, I can easily check how probable a given text is gibberish or real.</p>
<pre class=""lang-r prettyprint-override""><code>&gt; lookup_text(&quot;test&quot;)
[1] 0.004262462
</code></pre>
<p>There is just one major drawback: Obviously the value I get is heavily depending on the length of the string I want to check. So my question is: How can I fix this?</p>
<p>In this threat (<a href=""https://stackoverflow.com/questions/6297991/is-there-any-way-to-detect-strings-like-putjbtghguhjjjanika"">Is there any way to detect strings like putjbtghguhjjjanika?</a>) someone writes: &quot;Then normalize by the length of the query.&quot; But how can this be done? Thanks for your help!</p>
",Multilingual Language Processing & Language Identification,r gibberish detection using markov model adjust text length first create table frequency two letter combination german text file creates lookup table containing two column two letter combination frequency combination appearing text function calculates product frequency letter combination given text example word test lookup probability te e st probability multiplied p te p e p st easily check probable given text gibberish real one major drawback obviously value get heavily depending length string want check question fix threat href way detect string like putjbtghguhjjjanika someone writes normalize length query done thanks help
Best practice for dealing with NLP input in multiple languages for combined text analysis?,"<p>As part of a university research project, I scraped job posts for 4 professions in Germany. Because I could not get enough job posts in only 1 language in the time frame I have, I decided to scrape for both English and German posts.</p>
<p>I already went through the whole NLP workflow with both the English and the German text (tokenize, lemmatize, POS, stopwords,...) using different tools due to the language being different.</p>
<p>Now I would need to extract the most common skills required for each profession and differences between them.</p>
<p>I realize that this is a problem I should have predicted, but now I have two corpuses in two different languages which have to be analyzed together.</p>
<p><strong>What do you suggest is the best way to reach a scientifically sound end result with input data in two languages?</strong></p>
<p>So far, no good solution came to my mind:</p>
<ul>
<li>translate the German input to English and treat with the rest</li>
<li>translate the German input after processing word by word</li>
<li>manually map English and German words</li>
</ul>
",Multilingual Language Processing & Language Identification,best practice dealing nlp input multiple language combined text analysis part university research project scraped job post profession germany could get enough job post language time frame decided scrape english german post already went whole nlp workflow english german text tokenize lemmatize po stopwords using different tool due language different would need extract common skill required profession difference realize problem predicted two corpus two different language analyzed together suggest best way reach scientifically sound end result input data two language far good solution came mind translate german input english treat rest translate german input processing word word manually map english german word
Automatically Add Diacritic/Accent Marks to a Non-English Document,"<p>In my spare time, I am transcribing a very old, rare book written in Romanian (in fact, it is the only remaining copy, to my knowledge). It was written over a hundred years ago, well before any computers existed. As such, no digital copies exist, and I am manually transcribing and digitizing it.</p>
<p>The book is thousands of pages long, and it is surprisingly time consuming (for me, at least) to add diacritic and accented marks (<code>ƒÉ/√¢/√Æ/≈ü/≈£</code>) to every single word as I type. If I omit the marks and just type the bare letters (i.e <code>a</code> instead of <code>ƒÉ/√¢</code>), I am able to type more than twice as fast, which is a huge benefit. Currently I am typing everything directly into a <code>.tex</code> file to apply special formatting for the pages and illustrations.</p>
<p>However, I know that eventually I will have to add all these marks back into the text, and it seems tedious/unecessary to do all that manually, since I already have all the letters. I'm looking for some way to automatically/semi-automatically <strong>ADD</strong> diacritic/accent marks to a large body of text (<em>not</em> remove - I see plenty of questions asking how to remove the marks on SO).</p>
<p>I tried searching for large corpora of Romanian words (<a href=""https://raw.githubusercontent.com/ManiacDC/TypingAid/master/Wordlists/Wordlist%20Romanian.txt"" rel=""nofollow noreferrer"">this</a> and <a href=""https://github.com/dumitrescustefan/RoWordNet"" rel=""nofollow noreferrer"">this</a> were the most promising two), but everything I found fell short, missing at least a few words on any random sample of text I fed it (I used a short python script). It doesn't help that the book uses many archaic/uncommon words or uncommon spellings of words.</p>
<p>Does anyone have any ideas on how I might go about this? There are no dumb ideas here - any document format, machine learning technique, coding language, professional tool, etc that you can think of that might help is appreciated.</p>
<p>I should also note that I have substantial coding experience, and would not consider it a waste of time to build something myself. Tbh, I think it might be beneficial to the community, since I could not find such a tool in <strong>any</strong> western language (french, czech, serbian, etc). Just need some guidance on how to get started.</p>
",Multilingual Language Processing & Language Identification,automatically add diacritic accent mark non english document spare time transcribing old rare book written romanian fact remaining copy knowledge wa written hundred year ago well computer existed digital copy exist manually transcribing digitizing book thousand page long surprisingly time consuming least add diacritic accented mark every single word type omit mark type bare letter e instead able type twice fast huge benefit currently typing everything directly file apply special formatting page illustration however know eventually add mark back text seems tedious unecessary manually since already letter looking way automatically semi automatically add diacritic accent mark large body text remove see plenty question asking remove mark tried searching large corpus romanian word promising two everything found fell short missing least word random sample text fed used short python script help book us many archaic uncommon word uncommon spelling word doe anyone idea might go dumb idea document format machine learning technique coding language professional tool etc think might help appreciated also note substantial coding experience would consider waste time build something tbh think might beneficial community since could find tool western language french czech serbian etc need guidance get started
How do I use gensim to vectorize these words in my dataframe so I can perform clustering on them?,"<p>I am trying to do a clustering analysis (preferably k-means) of poetry words on a pandas dataframe. I am firstly trying to vectorize the words by using the word-to-vector feature in the gensim package. However, the vectors just come out with 0s, so my code is failing to translate the words into vectors. As a result, the clustering doesn't work. Here is my code:</p>
<pre><code># create a gensim model 
model = gensim.models.Word2Vec(vector_size=100) 
# copy original pandas dataframe with poems
data = poems.copy(deep=True)
# get data ready for kmeans clustering
final_data = [] # empty list 
for i, row in data.iterrows(): 
    poem_vectorized = [] 
    poem = row['Main_text']
    poem_all_words = poem.split(sep=&quot; &quot;)
    for poem_w in poem_all_words: #iterate through list of words 
        try:
            poem_vectorized.append(list(model.wv[poem_w]))
        except Exception as e:
            pass
    try:
        poem_vectorized = np.asarray(poem_vectorized)
        poem_vectorized_mean = list(np.mean(poem_vectorized, axis=0))
    except Exception as e:
        poem_vectorized_mean = list(np.zeros(100))
        pass
    try:
        len(poem_vectorized_mean)
    except:
        poem_vectorized_mean = list(np.zeros(100))
    temp_row = np.asarray(poem_vectorized_mean)
    final_data.append(temp_row)
X = np.asarray(final_data)
print(X)
</code></pre>
<p><a href=""https://i.sstatic.net/jUSKd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jUSKd.png"" alt=""Output"" /></a></p>
<p>At closer inspection of:</p>
<pre><code>poem_vectorized.append(list(model.wv[poem_w]))
</code></pre>
<p>the problem seems to be this:
<a href=""https://i.sstatic.net/0cgkP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0cgkP.png"" alt=""syntax error"" /></a></p>
",Multilingual Language Processing & Language Identification,use gensim vectorize word dataframe perform clustering trying clustering analysis preferably k mean poetry word panda dataframe firstly trying vectorize word using word vector feature gensim package however vector come code failing translate word vector result clustering work code closer inspection problem seems
Difference between Semantic Web and NLP?,"<p>What exactly is the difference between Semantic Web and Natural Language Processing? </p>

<p>Is Semantic Web a part of Natural Language Processing?</p>
",Multilingual Language Processing & Language Identification,difference semantic web nlp exactly difference semantic web natural language processing semantic web part natural language processing
Obtaining the index of a word between two columns in pandas,"<p>I am checking on which words the SpaCy Spanish lemmatizer works on using the .has_vector method. In the two columns of the datafame I have the output of the function that indicates which words can be lemmatized and in the other one the corresponding phrase.</p>
<p>I would like to know how I can extract all the words that have False output to correct them so that I can lemmatize.</p>
<p>So I created the function:</p>
<pre><code>def lemmatizer(text):
doc = nlp(text)
return ' '.join([str(word.has_vector) for word in doc])
</code></pre>
<p>And applied it to the column sentences in the DataFrame</p>
<pre><code>df[&quot;Vectors&quot;] = df.reviews.apply(lemmatizer)
</code></pre>
<p>And put in another data frame as:</p>
<pre><code>df2= pd.DataFrame(df[['Vectors', 'reviews']])
</code></pre>
<p>The output is</p>
<pre><code>index             Vectors              reviews
  1     True True True False        'La¬†pelicula¬†es¬†aburridora'
</code></pre>
",Multilingual Language Processing & Language Identification,obtaining index word two column panda checking word spacy spanish lemmatizer work using ha vector method two column datafame output function indicates word lemmatized one corresponding phrase would like know extract word false output correct lemmatize created function applied column sentence dataframe put another data frame output
Flair English sentiment model download,"<p>I'm trying to download the en-sentiment model using the following code:</p>
<pre><code>clf = TextClassifier.load('en-sentiment')
</code></pre>
<p>but I'm getting the following SSL error:</p>
<pre><code>SSLError: HTTPSConnectionPool(host='nlp.informatik.hu-berlin.de', port=443): Max retries exceeded with url: /resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt (Caused by SSLError(SSLError(1, '[SSL] unknown error (_ssl.c:1129)')))
</code></pre>
<p>I understand this is an issue with my firewall but I can't seem to get around it. I've tried installing openssl in my virtual env but this isn't working. I understand you can download the model manually but I can't find where to download them from.</p>
<p>Any help would be appreciated.</p>
",Multilingual Language Processing & Language Identification,flair english sentiment model download trying download en sentiment model using following code getting following ssl error understand issue firewall seem get around tried installing openssl virtual env working understand download model manually find download help would appreciated
How can I change the white space between spans when adding them programmatically?,"<p>Working on this <a href=""https://github.com/eelegiap/texthighlight"" rel=""nofollow noreferrer"">text alignment project</a> in D3.js, HTML/CSS, and Javascript. I am projecting all the words (actually tokens, including punctuation) in a text and its translation as spans. But the problem is that programmatically adding spans adds white space between them, and I need to be able to get rid of white space before punctuation. And yes, I have to add the spans programmatically because there are too many to write up in the HTML by hand. <strong>How can I add a bunch of spans and change the white space between them?</strong></p>
<p>There is a <code>span</code> for the sentence and then a <code>span</code> with a <code>mark</code> for every token in the sentence. I add them like this (no CSS to note):</p>
<pre><code>        data.tgtSentsInOrder.text.forEach(function (sent, i) {
            tgtdiv
                .append('span')
                .attr('id', 'tgtsent' + i)
                .attr('class', 'sentence chosen')

            var tokens = data.tgtSentsInOrder.tokens[i]

            tokens.forEach(function (t) {
                tgtcharcount += t.text.length
            })

            d3.select('#tgtsent' + i)
                .selectAll('span')
                .data(tokens)
                .enter()
                .append('span')
                .attr('id', function (d, j) { return ('tgtsent' + i + 'span' + j) })
                .attr('class', 'token')
                .append('mark')
                .attr('id', function (d, j) { return ('tgtsent' + i + 'token' + j) })
                .html(function (d, j) {
                    return d.text + ' '
                })
                .style('background-color', 'white')
        })
</code></pre>
<p>Any other design criticisms are welcome, thank you!</p>
",Multilingual Language Processing & Language Identification,change white space span adding programmatically working text alignment project j html cs javascript projecting word actually token including punctuation text translation span problem programmatically adding span add white space need able get rid white space punctuation yes add span programmatically many write html hand add bunch span change white space sentence every token sentence add like cs note design criticism welcome thank
Using textblob or spacy for correction spelling in french,"<p>I would like to correct the misspelled words of a text in french, it seems that spacy is the most accurate and faster package to do it, but it's to complex,
I tried with textblob, but I didn't manage to do it with french words.</p>
<p>It works perfectly in english, but when I try to do the same in french I get the same misspelled words:</p>
<pre class=""lang-py prettyprint-override""><code>#english words 
from textblob import TextBlob
misspelled=[&quot;hapenning&quot;, &quot;mornin&quot;, &quot;windoow&quot;, &quot;jaket&quot;]
[str(TextBlob(word).correct()) for word in misspelled]

#french words
misspelled2=[&quot;resaissir&quot;, &quot;matinn√©e&quot;, &quot;plonbier&quot;, &quot;tecnicien&quot;]
[str(TextBlob(word).correct()) for word in misspelled2]
</code></pre>
<p>I get this:</p>
<pre><code>#english:
['happening', 'morning', 'window', 'jacket']

#french:
['resaissir', 'matinn√©e', 'plonbier', 'tecnicien']
</code></pre>
",Multilingual Language Processing & Language Identification,using textblob spacy correction spelling french would like correct misspelled word text french seems spacy accurate faster package complex tried textblob manage french word work perfectly english try french get misspelled word get
Calculate F-score for GEC,"<p>I am working on Sequence to Sequence encoder-decoder model with bidirectional GRU for the task of grammar error detection and correction for Arabic language. I want to calculate the F0.5 score for my model.</p>
<p>This is how my data divided:</p>
<pre><code>train_data, valid_data, test_data = torchtext.legacy.data.TabularDataset.splits(
                            path = '',
                            train = 'train.csv',
                            test = 'test.csv',
                            validation = 'val.csv',
                            format = 'csv',
                            fields = fields)
</code></pre>
<p>and this is my Seq2Seq code:</p>
<pre><code>class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, src_pad_idx, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.src_pad_idx = src_pad_idx
        self.device = device
        
    def create_mask(self, src):
        mask = (src != self.src_pad_idx).permute(1, 0)
        return mask
        
    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):
        
        #src = [src len, batch size]
        #src_len = [batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time
                    
        batch_size = src.shape[1]
        #print(src.type())
        #print(trg.type())
        #print(src)
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        #encoder_outputs is all hidden states of the input sequence, back and forwards
        #hidden is the final forward and backward hidden states, passed through a linear layer
        encoder_outputs, hidden = self.encoder(src, src_len)
                
        #first input to the decoder is the &lt;sos&gt; tokens
        input = trg[0,:]
        
        mask = self.create_mask(src)

        #mask = [batch size, src len]
                
        for t in range(1, trg_len):
            
            #insert input token embedding, previous hidden state, all encoder hidden states 
            #  and mask
            #receive output tensor (predictions) and new hidden state
            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)
            
            #place predictions in a tensor holding predictions for each token
            outputs[t] = output
            
            #decide if we are going to use teacher forcing or not
            teacher_force = random.random() &lt; teacher_forcing_ratio
            
            #get the highest predicted token from our predictions
            top1 = output.argmax(1) 
            
            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            input = trg[t] if teacher_force else top1
            
        return outputs
</code></pre>
<p>I tried to use sklearn.metrics but I think my output is not suitable for this function</p>
",Multilingual Language Processing & Language Identification,calculate f score gec working sequence sequence encoder decoder model bidirectional gru task grammar error detection correction arabic language want calculate f score model data divided seq seq code tried use sklearn metric think output suitable function
Is there a way to make python print to file for every iteration of a for loop instead of storing all in the buffer?,"<p>I am looping over a very large document to try and lemmatise it.
Unfortunately python does not seem to print to file for every line but run through the whole document before printing, which given the size of my file exceeds the memory...
Before I chunk my document into more bite-sized chunks I wondered if there was a way to force python to print to file for every line.</p>
<p>So far my code reads:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('de_core_news_lg')
  
fin = &quot;input.txt&quot; 
fout = &quot;output.txt&quot;
    
    
#%%
    
with open(fin) as f:
   corpus = f.readlines()
    
corpus_lemma = []
    
for word in corpus:
   result = ' '.join([token.lemma_ for token in nlp(word)])
   corpus_lemma.append(result)
    
   with open(fout, 'w') as g:
      for item in corpus_lemma:
         g.write(f'{item}')
</code></pre>
<p>To give credits for the code, it was kindly suggested here: <a href=""https://stackoverflow.com/questions/57857240/ho-to-do-lemmatization-on-german-text"">Ho to do lemmatization on German text?</a></p>
",Multilingual Language Processing & Language Identification,way make python print file every iteration loop instead storing buffer looping large document try lemmatise unfortunately python doe seem print file every line run whole document printing given size file exceeds memory chunk document bite sized chunk wondered wa way force python print file every line far code read give credit code wa kindly suggested href lemmatization german text
How to use python3 remove all emojis? (include mobile emojis),"<p>Right now I have a client string consists an emoji &quot;√∞≈∏‚Äú¬≤&quot;, which means Mobile Phone With Arrow. I want to remove it in my text pre-process step so that I can pass it to my NLP model. I tried to use:</p>
<pre><code>    def remove_emojis(text: str) -&gt; str:
    emojis = re.compile(&quot;[&quot;
                        u&quot;\U0001F600-\U0001F64F&quot;  # emoticons
                        u&quot;\U0001F300-\U0001F5FF&quot;  # symbols &amp; pictographs
                        u&quot;\U0001F680-\U0001F6FF&quot;  # transport &amp; map symbols
                        u&quot;\U0001F1E0-\U0001F1FF&quot;  # flags (iOS)
                        u&quot;\U00002500-\U00002BEF&quot;  # chinese char
                        u&quot;\U00002702-\U000027B0&quot;
                        u&quot;\U00002702-\U000027B0&quot;
                        u&quot;\U000024C2-\U0001F251&quot;
                        u&quot;\U0001f926-\U0001f937&quot;
                        u&quot;\U00010000-\U0010ffff&quot;
                        u&quot;\u2640-\u2642&quot;
                        u&quot;\u2600-\u2B55&quot;
                        u&quot;\u200d&quot;
                        u&quot;\u23cf&quot;
                        u&quot;\u23e9&quot;
                        u&quot;\u231a&quot;
                        u&quot;\ufe0f&quot;  # dingbats
                        u&quot;\u3030&quot;  # flags (iOS)
                        &quot;]+&quot;, flags=re.UNICODE)
    return emojis.sub(r'', text)
</code></pre>
<p>But it is not working. I think it does not cover this emoji. Is there any way I can remove emojis like this one?</p>
<p>PS: Keeping only English characters does not work for my case cause the client string is not in English.</p>
",Multilingual Language Processing & Language Identification,use python remove emojis include mobile emojis right client string consists emoji mean mobile phone arrow want remove text pre process step pas nlp model tried use working think doe cover emoji way remove emojis like one p keeping english character doe work case cause client string english
BERT Vocabulary : Why every word has &#39;‚ñÅ&#39; before?,"<p>my question is related to camemBERT model (french version of BERT) and its Tokenizer :</p>
<p>Why every word of the vocabulary has a &quot;‚ñÅ&quot; character before ?
For example, it's not &quot;sirop&quot; but &quot;‚ñÅsirop&quot; (sirop =&gt; syrup).</p>
<pre><code>from transformers import CamembertTokenizer
tokenizer = Camembert.Tokenizer.from_pretrained(&quot;camembert-base&quot;)
voc = tokenizer.get_vocab() #Vocabulary of the model

print(&quot;sirop&quot; in voc) # Will display False
print(&quot;‚ñÅsirop&quot; in voc) # Will display True
</code></pre>
<p>Thank you for answering :)</p>
",Multilingual Language Processing & Language Identification,bert vocabulary every word ha question related camembert model french version bert tokenizer every word vocabulary ha character example sirop sirop sirop syrup thank answering
Automatic text translation,"<p>What tools or web services are available for machine text translation.</p>

<p>For example</p>

<pre><code>ENGLISH TEXT &gt; SERVER or LIB &gt; GERMAN TEXT
</code></pre>

<p>Libraries are also acceptable. </p>

<p>Is <a href=""http://code.google.com/apis/ajaxlanguage/"" rel=""nofollow noreferrer"">Google language API</a> the only one ? </p>
",Multilingual Language Processing & Language Identification,automatic text translation tool web service available machine text translation example library also acceptable google language api one
How to replace Spacy English model with spacy.load(&quot;en_core_web_lg&quot;) model?,"<p>For changing definition of IS_CURRENCY in the language model, used the below code.</p>
<pre><code>from spacy.lang.nl import EnglishDefaults, English

def is_currency(text):
    &quot;&quot;&quot;
    Custom function used for detecting currency symbols.
    
    :param text: The text that is to be checked.
    :return: A boolean.
    &quot;&quot;&quot;
    # Stripping punctuation
    table = str.maketrans({key: None for key in string.punctuation if key != &quot;$&quot;})
    text = text.translate(table)
    
    all_currencies = (&quot;‡∏ø&quot;, &quot;US$&quot;, &quot;C$&quot;, &quot;A$&quot;, &quot;‚Ç∫&quot;, &quot;‚Çπ&quot;, &quot;‡ß≥&quot;, &quot;‚Ç©&quot;, &quot;Mex$&quot;, &quot;‚Ç£&quot;, &quot;E¬£&quot;)
    if text in all_currencies:
        return True
    return False
    
    
class CustomEnglishDefaults(EnglishDefaults):
    lex_attr_getters = {**EnglishDefaults.lex_attr_getters,  IS_CURRENCY: is_currency}


class CustomEnglish(English):
    Defaults = CustomEnglishDefaults

nlp = CustomEnglish()
doc = nlp(text)
</code></pre>
<p>How to replace EnglishDefaults, English with <code>spacy.load(&quot;en_core_web_lg&quot;)</code> in the above code? After the change it should still use the custom <code>is_currency()</code>.</p>
<p>This is because of need to keep using <code>spacy.load(&quot;en_core_web_lg&quot;)</code> for consistency in the project to get same tokens, POS, lemma etc.</p>
",Multilingual Language Processing & Language Identification,replace spacy english model spacy load en core web lg model changing definition currency language model used code replace englishdefaults english code change still use custom need keep using consistency project get token po lemma etc
Getting the maximum key (argmax) of the nested dictionary inside a dictionary (Python),"<p>(Python) So I have a nested dictionary set up something like:</p>
<pre><code>{spanish word: {english word: .5, english word 2: .3, ...}}
</code></pre>
<p>where the keys are spanish words, and the values are nested dictionaries whose keys are english words and their probabilities.
And I want to get the spanish word such that it has the maximum value.
I've tried (for a current spanish word)</p>
<pre><code>maxenglish = max(d[word], key = d[word].get)
</code></pre>
<p>but this returns the english word. I'm not too familiar with lambda functions, but I saw that could be employed? Thanks for the help!</p>
<p>Edited: Switched English and Spanish</p>
<p>Example:</p>
<pre><code>nesteddict = {&quot;hola&quot;: {&quot;hello&quot;:.5,&quot;goodbye&quot;:.1}, &quot;ciao&quot;: {&quot;hello&quot;:.1,&quot;goodbye&quot;:.5}
word = &quot;hola&quot;
argmax = max(nesteddict[word], key = nesteddict[word].get)
</code></pre>
<p>returns &quot;hello,&quot; but I want it to return &quot;hola&quot;.</p>
",Multilingual Language Processing & Language Identification,getting maximum key argmax nested dictionary inside dictionary python python nested dictionary set something like key spanish word value nested dictionary whose key english word probability want get spanish word ha maximum value tried current spanish word return english word familiar lambda function saw could employed thanks help edited switched english spanish example return hello want return hola
Loading pre-trained CBOW/skip-gram embeddings from a file that has unknown encoding?,"<p>I'm trying to load pre-trained word embeddings for the Arabic language (Mazajak embeddings: <a href=""http://mazajak.inf.ed.ac.uk:8000/"" rel=""nofollow noreferrer"">http://mazajak.inf.ed.ac.uk:8000/</a>). The embeddings file does not have a particular extension and I'm struggling to get it to load. What's the usual process to load these embeddings?</p>
<p>I've tried doing <code>with open(&quot;get_sg250&quot;, encoding = encoding) as file: file.readlines() </code> for different encodings but it seems like none of them are the answer (utf-8 does not work at all), if I try windows-1256 I get gibberish:</p>
<p>e.g.</p>
<pre><code>['8917028 300\n',
 '&lt;/s&gt; HŸÑ¬Æ:0\x16ÿ°:ÿüX¬ßÿõR8⁄àÿõ\xa0ÿ≥√Æ9K\u200f∆í::m¬§9¬º¬ª‚Äú8¬§p\u200cÿõtÿπA:UU¬æÿõ‚Äú_ÿπ9‚ÄöN∆í¬π¬ÆG¬ß¬πŸÇŸÅ⁄Øÿõww$ÿõ\u200eba:\x14.‚Äû:R¬∏Ÿæ:0‚Äì\x0b:‚Äì√º\x06:√ó#¬¶ÿõYŸç¬≤ÿõm ÿ∏:{\x14¬¶:¬µ\x01‚Ä°:Ÿá\x17S¬πYr¬Ø:j\x03-¬πff‚Ç¨9√ó¬£P¬∏\n',
 'W‚ÄöÿõUUŸá9¬º¬ª√©¬π&quot;&quot;¬ßÿõ\u200c¬∂ÿØ:UUÿü:\u200ebÿü¬π{\x14\u200d¬∏,√π19√Ø√Æ\u200dÿõÿ¶\x12¬Øÿõ\x00\x00ÿß:\u200c6¬∞7A¬ßaÿõÿ∞√©‚Äûÿõÿ∞i‚Ä†ÿõ¬ÆG\x14:ÿ≠ÿ¨≈í8\x03\u200c√®9Ÿá\x17¬∏ÿõŸÇ]¬¶ÿõ⁄àÿ¢5¬∏ŸÇŸÅÿß9ÿ≠ÿ¨^:\x00‚Ç¨Ÿπÿõq=¬≤:\x00\x00¬¢9\x14¬Æÿ£9√ó¬£T¬πŸÑz‚Äö:\x1b√®Gÿõ¬ÆG7ÿõ⁄ë‚Ñ¢&lt;:m\xa0∆í¬π&quot;&quot;¬¥9\x14¬Æ\x1d:&quot;¬¢¬≤ÿõ¬ÆG-ÿõ⁄ë‚Ñ¢~:¬±ŸÜ¬∏:\x18ÿ´¬´:¬∏\x1e‚Ä¶ÿõ`,8ÿõHŸÑ\u200d¬π¬±ŸÜ.:\x1f‚Ä¶¬•ÿõŸÑŸí‚Äö:⁄ë‚Ñ¢s:R¬∏\x0bÿõÿ¶‚Äô\x07ÿõ0‚ÄìCÿõ⁄àÿ¢¬∏:ÿ∞√©⁄æ:ÿ©/ÿÆ¬πA\'¬∏:⁄ë‚Ñ¢ÿ≤:m\xa0\x1e:√®¬¥ÿ∏::Ÿä‚Ä°ÿõ\n',
 '√ó\x05ÿõ≈í%8ÿõÿ¥\x06~ÿõÿ£Ÿèu:\x00\x00\n',
 &quot;:‚Ä∞ÀÜ\x149\x14¬Æ?ÿõŸê(\x05:¬´⁄æ‚Ä¶:)\\‚Ä°833G:Haÿ∑ÿõ\x1f‚Ä¶¬º:¬º¬ª'9\x00\x00 ÿõ=\n&quot;,
 '6ÿõR¬∏‚Äö¬π¬º;‚Ç¨ÿõ\x1b√®¬æÿõ\x1b√®wÿõŸÇŸÅÿõ:A¬ß\x1aÿõ&quot;&quot;jÿõK~J:HŸÑ\x14ÿõŸâÿ±ÿØ:\u200c6\x0cÿõ‚Äì|ÿ®ÿõ‚ÄöNm:cÿ©ÿØ¬∑:m⁄©ÿõ‚Ä∞ÀÜ⁄æ9\x00\x00√º9DD(¬πÿ∞i\x1f:ÿ∞√©¬¨ÿõ,√π‚Ñ¢9¬º¬ª\x1e:ww∆íÿõ\x03\u200cF87ÿ∞¬©¬∑√ó¬£Qÿõ\x1f‚Ä¶wÿõÿ¶\x12ÿ≠ÿõ\x00\x00\x007Ÿç‚ÄπU8\x0etZ6‚ÄúŸÉ¬´ÿõcÿ©ÿ∑ÿõHaÿØÿõ‚Äì√º¬ºÿõ33?¬π≈í%Ÿé9ÿ£ŸèÿÆ9=\n',
 '‚ÄπÿõŸÇ]ÿπ:⁄àÿ¢/ÿõ0‚ÄìŸÇ¬π¬§pŸè¬πDÿ§ÿÆ:¬§p¬§ÿõ\x1b√®ÿ™9\u200eb√©¬π√πE‚Äπ:‚Äì√ºb7=Ÿπÿõ:ÿüXvÿõ√ó¬£cÿõŸê(¬∑ÿõ√®4\xa0ÿõcÿ©‚Äπÿõ0\x16ÀÜÿõÿ¶‚ÄôU:&quot;&quot;#ÿõÿ©/j:R8ÿå:ÿ£ŸèŸâ9ÿ∞√©‚Ç¨:ŸâQX:\x1f‚Ä¶L:&quot;&quot;‚Ä∫ÿõK\u200f‚Ä¢ÿõ⁄àÿ¢⁄∫ÿõ‚Ä∞ÀÜ8¬∏ww¬¥:&quot;&quot;oÿõ√®¬¥‚Ä¶ÿõ\n',
 'W¬∑ÿõ¬§p⁄Ø:{‚Äù¬∂ÿõ\x0etJ¬π\u200eb&gt;:√πÿ•ÿ©ÿõ`¬¨ÿ£ÿõŸê(√º9K\u200f‚Ñ¢:‚ÄöNÿõ:ŸÑz;:Ÿê(Ÿπ:≈í¬•ÀÜÿõ¬ß\n',
 '⁄∫ÿõŸê¬®\xad:⁄ë‚Ñ¢qÿõ\u200c6\x19:√ó¬£H9¬§p\x1c:\x03\u200cÿÆ¬π‚Äì√ºŸπ8UU\x13ÿõHŸÑÿ§¬π√®¬¥ÿ°ÿõ√Øn⁄òÿõ¬ÆG⁄©:√®¬¥¬Ø9\x0etNÿõO\x1b\x0bÿõ\x00\x00Z:\n',
 'W⁄ëÿõ&quot;&quot;Jÿõÿüÿ∑ÿÆ:\x03\u200c¬π:ŸÑŸí¬¨ÿõ\u200c6⁄©9⁄ë‚Ñ¢Dÿõ\x1b√®T8ŸÇ]∆í:¬º¬ªÿ≥:0‚Äì-:~¬±¬≥:,y‚Ä∞:√®¬¥ÿå¬∏j∆íÿ£:m\xa0]:A\'ÿØ:j\x03\x15ÿõHaÿØ:&quot;&quot;¬Ω:ww√π¬πŸá\x17ÿ°ÿõ√ó#ÿ≥:&amp;ÿü≈ì9√ó¬£5ÿõHŸÑz¬π\\⁄à‚Ç¨¬π)\\¬®ÿõO\x1bŸí¬πŸá\x17\x1b¬π⁄àB√óÿõ\x03\u200c‚Ñ¢ÿõŸâQÿ≤¬πŸÑz¬§¬πÿ∞i\x1c:\\⁄à⁄ò9√πÿ•V¬πR¬∏‚Ç¨:√πÿ•√º9ww?9‚Ä∞\x08\u200d:~¬±ÿ§¬π‚ÄöN√π¬π‚Ä∞ÀÜ\x10¬πUUnÿõ\x11\x11∆íÿõŸç‚Äπ⁄Ü8‚Ä∞ÀÜ¬Ω:\x1b√®√Æ¬πO\x1b√®¬∂`¬¨¬¥ÿõ=\n',
 '¬¢:\n',
</code></pre>
<p>I've also tried using pickle but that also doesn't work.</p>
<p>Any suggestions on what I could try out?</p>
",Multilingual Language Processing & Language Identification,loading pre trained cbow skip gram embeddings file ha unknown encoding trying load pre trained word embeddings arabic language mazajak embeddings embeddings file doe particular extension struggling get load usual process load embeddings tried different encoding seems like none answer utf doe work try window get gibberish e g also tried using pickle also work suggestion could try
extract all two-character-combinations from a string,"<p>In order to identify nonsense text (e.g. djsarejslslasdfhsl) from real (German) words, I would like to do an analysis of letter-frequencies.</p>
<p>My idea is to calculate the relative frequencies of two-letter-combinations (&quot;te&quot;, &quot;ex&quot;, &quot;xt&quot;, &quot;is&quot; etc.) using a long text. Based on that information I would like to calculate the probability that a given word (or sentence) is real German.</p>
<p>But my first problem is, how to extract all the two-letter-combinations and to count them? I fear that using <code>substring(string, start, stop)</code> and increasing the values of <code>start</code> and <code>stop</code> in a loop might not be a very efficient solution. Do you have any idea?</p>
<pre class=""lang-r prettyprint-override""><code># A short sample text
text &lt;- 'Es ist ein Freudentag ‚Äì ohne Zweifel. Gesundheitsminister Alain Berset und der Bundesrat gehen weiter, als man annehmen durfte. Die Zertifikatspflicht wird aufgehoben, die Maskenpflicht gilt nur noch im √ñV und in Gesundheitseinrichtungen.
Die beste Meldung des Tages aber ist: Die Covid-19-Task-Force, inzwischen als ¬´Task-Farce¬ª verballhornt, wird auf Ende M√§rz aufgehoben ‚Äì zwei Monaten fr√ºher als geplant. Die Dauerkritik war wohl mit ein Grund, dass dieses Gremium sich jetzt rasch aufl√∂sen will.
Keine Rosen ohne Dornen: Einzelne Punkte von Bersets Ausf√ºhrungen geben zu denken.
Die ¬´Isolationshaft¬ª f√ºr positiv Getestete bleibt zwingend. Das ist Unsinn und steht in einem scharfen Kontrast zu den √ºbrigen √ñffnungsschritten. Die Grundimmunit√§t der Bev√∂lkerung betr√§gt √ºber 90 Prozent, das Virus ist nicht mehr gef√§hrlich, warum will man weiter Leute zu Hause einsperren? Wer schwer krank ist, geht von sich aus nicht zur Arbeit. Die krankheitsbedingte Bettruhe muss man den Menschen nicht vorschreiben.
Gesundheitsminister Berset findet, das Modell Task-Force habe eine interessante M√∂glichkeit aufgezeigt f√ºr die Zusammenarbeit zwischen Regierung und Wissenschaft. Unter Umst√§nden eigne sich dieses Modell auch f√ºr andere Bereiche.
Nein danke, Herr Berset.
Die Task-Force war mit ihrem √∂ffentlichen Dauer-Alarmismus und ihren haarstr√§ubenden Falsch-Prognosen vor allem eine Manipulationsmaschine.
Und dann noch dies: Irgendwann w√§hrend der heutigen Pressekonferenz gab Alain Berset zu verstehen, man habe mit all diesen Massnahmen die Bev√∂lkerung sch√ºtzen wollen. Vielleicht hatte man diese hehre Absicht einmal im Hinterkopf. Alle Massnahmen ab der zweiten Welle erfolgten nicht zum Schutz der Bev√∂lkerung, sondern, um einen Zusammenbruch des Spital-Systems zu verhindern.
Doch jetzt stossen wir erst einmal auf das Ende der Apartheit an.'

# Some cleaning:

library(stringr)
text &lt;- str_replace_all(text, &quot;[^[:alnum:]]&quot;, &quot; &quot;)
text &lt;- tolower(text)
words &lt;- strsplit(text, &quot;\\s+&quot;)[[1]]
words

for(word in words){
  ??? 
}



</code></pre>
",Multilingual Language Processing & Language Identification,extract two character combination string order identify nonsense text e g djsarejslslasdfhsl real german word would like analysis letter frequency idea calculate relative frequency two letter combination te ex xt etc using long text based information would like calculate probability given word sentence real german first problem extract two letter combination count fear using increasing value loop might efficient solution idea
Can the monolingual Transformers models be used for another languages in NLP tasks?,"<p>I need to fine-tune the Sentence Transformer model for Tr language. I realized that some pre-trained models are giving similar results in different languages.</p>
<ul>
<li>So, Can we use it to fine-tune ST pre-trained model in English, for
other languages?</li>
<li>Can we say that the English model can be used in another
language in Sentence Embeddings(Sentence Transformer)?</li>
</ul>
<p>I will be glad if you can help.</p>
",Multilingual Language Processing & Language Identification,monolingual transformer model used another language nlp task need fine tune sentence transformer model tr language realized pre trained model giving similar result different language use fine tune st pre trained model english language say english model used another language sentence embeddings sentence transformer glad help
Natural Language Processing - Returning results as JSON objects,"<p>I am working on completing a Python programing challenge to do with Natural Language Processing and scraping information from BBC News articles. My previous submission of code (shown below) was marked and failed due to not passing the test criteria (shown below). However I am unable to think of any way to make my code pass the test.</p>
<p>From what I can tell the code is failing for a few reasons:</p>
<ul>
<li>Escape characters not matching the test code</li>
<li>Use of &quot;      &quot; instead of '   ' in some variables</li>
</ul>
<p>This is my code:</p>
<pre><code># Import modules used to complete challenge
import json
import requests
import spacy
from bs4 import BeautifulSoup

# Load the Spacy English model
nlp = spacy.load(&quot;en_core_web_sm&quot;)
# Url's stored in a list to test that the scraper function is iterable
url = [&quot;https://www.bbc.co.uk/news/uk-52255054&quot;]


# Function used to scrape a given BBC news article and return URL, Title, Date and Content as a json object
def bbc_scraper(url):
    # Error catching on invalid URL's and connectivity issues
    try:
        r = requests.session()
        page = r.get(url)
    except requests.exceptions.RequestException as e:
        print(&quot;Double check the URL and try again.&quot;)
        raise SystemExit(e)
    # Use beautifulSoup to parse the html content and store it as soup
    soup = BeautifulSoup(page.content, &quot;html.parser&quot;)
    # Using beautifulSoup to find specific HTML tags within HTML classes
    title = soup.find(&quot;h1&quot;, class_=&quot;story-body__h1&quot;).text
    date = soup.find(&quot;div&quot;, class_=&quot;date date--v2&quot;).text
    main_content = soup.findAll(&quot;div&quot;, class_=&quot;story-body__inner&quot;)
    # Finding all &quot;p&quot; HTML tags within the main content
    news_content_paragraphs = main_content[0].find_all(&quot;p&quot;)
    list_paragraphs = []
    # Loop through all paragraphs and joins them to the final_article string
    for paragraphs in news_content_paragraphs:
        list_paragraphs.append(paragraphs.text)
        final_article = ''.join(list_paragraphs)
        # Build the json object with content scraped from the website
    results_json = json.dumps({'URL':url, 'Title':title,'Date_published':date, 'Content':final_article})
    return results_json
</code></pre>
<p>And then this is the test criteria used to test the JSON that my code returns:</p>
<pre><code>def test_bbc_scrape():
    results = {'URL': 'https://www.bbc.co.uk/news/uk-52255054',
                'Title': 'Coronavirus: \'We need Easter as much as ever,\' says the Queen',
                'Date_published': '11 April 2020',
                'Content': '&quot;Coronavirus will not overcome us,&quot; the Queen has said, in an Easter message to the nation. While celebrations would be different for many this year, she said: &quot;We need Easter as much as ever.&quot; Referencing the tradition of lighting candles to mark the occasion, she said: &quot;As dark as death can be - particularly for those suffering with grief - light and life are greater.&quot; It comes as the number of coronavirus deaths in UK hospitals reached 9,875. Speaking from Windsor Castle, the Queen said many religions had festivals celebrating light overcoming darkness, which often featured the lighting of candles. She said: &quot;They seem to speak to every culture, and appeal to people of all faiths, and of none. &quot;They are lit on birthday cakes and to mark family anniversaries, when we gather happily around a source of light. It unites us.&quot; The monarch, who is head of the Church of England, said: &quot;As darkness falls on the Saturday before Easter Day, many Christians would normally light candles together.  &quot;In church, one light would pass to another, spreading slowly and then more rapidly as more candles are lit. It\'s a way of showing how the good news of Christ\'s resurrection has been passed on from the first Easter by every generation until now.&quot; As far as we know, this is the first time the Queen has released an Easter message. And coming as it does less than a week since the televised broadcast to the nation, it underlines the gravity of the situation as it is regarded by the monarch. It serves two purposes really; it is underlining the government\'s public safety message, acknowledging Easter will be difficult for us but by keeping apart we keep others safe, and the broader Christian message of hope and reassurance.  We know how important her Christian faith is, and coming on the eve of Easter Sunday, it is clearly a significant time for people of all faiths, but particularly Christian faith. She said the discovery of the risen Christ on the first Easter Day gave his followers new hope and fresh purpose, adding that we could all take heart from this.  Wishing everyone of all faiths and denominations a blessed Easter, she said: &quot;May the living flame of the Easter hope be a steady guide as we face the future.&quot; The Queen, 93, recorded the audio message in the White Drawing Room at Windsor Castle, with one sound engineer in the next room.  The Palace described it as &quot;Her Majesty\'s contribution to those who are celebrating Easter privately&quot;.  It follows a speech on Sunday, in which the monarch delivered a rallying message to the nation. In it, she said the UK &quot;will succeed&quot; in its fight against the coronavirus pandemic, thanked people for following government rules about staying at home and praised those &quot;coming together to help others&quot;. She also thanked key workers, saying &quot;every hour&quot; of work &quot;brings us closer to a return to more normal times&quot;.'}
    scraper_result = bbc_scraper('https://www.bbc.co.uk/news/uk-52255054')
    assert json.loads(scraper_result) == results
</code></pre>
<p>My code returns the following:</p>
<pre><code>{&quot;URL&quot;: &quot;https://www.bbc.co.uk/news/uk-52255054&quot;, &quot;Title&quot;: &quot;Coronavirus: 'We need Easter as much as ever,' says the Queen&quot;, &quot;Date_published&quot;: &quot;11 April 2020&quot;, &quot;Content&quot;: &quot;\&quot;Coronavirus will not overcome us,\&quot; the Queen has said, in an Easter message to the nation.While celebrations would be different for many this year, she said: \&quot;We need Easter as much as ever.\&quot;Referencing the tradition of lighting candles to mark the occasion, she said: \&quot;As dark as death can be - particularly for those suffering with grief - light and life are greater.\&quot;It comes as the number of coronavirus deaths in UK hospitals reached 9,875.Speaking from Windsor Castle, the Queen said many religions had festivals celebrating light overcoming darkness, which often featured the lighting of candles.She said: \&quot;They seem to speak to every culture, and appeal to people of all faiths, and of none.\&quot;They are lit on birthday cakes and to mark family anniversaries, when we gather happily around a source of light. It unites us.\&quot;The monarch, who is head of the Church of England, said: \&quot;As darkness falls on the Saturday before Easter Day, many Christians would normally light candles together. \&quot;In church, one light would pass to another, spreading slowly and then more rapidly as more candles are lit. It's a way of showing how the good news of Christ's resurrection has been passed on from the first Easter by every generation until now.\&quot;As far as we know, this is the first time the Queen has released an Easter message.And coming as it does less than a week since the televised broadcast to the nation, it underlines the gravity of the situation as it is regarded by the monarch.It serves two purposes really; it is underlining the government's public safety message, acknowledging Easter will be difficult for us but by keeping apart we keep others safe, and the broader Christian message of hope and reassurance. We know how important her Christian faith is, and coming on the eve of Easter Sunday, it is clearly a significant time for people of all faiths, but particularly Christian faith.She said the discovery of the risen Christ on the first Easter Day gave his followers new hope and fresh purpose, adding that we could all take heart from this. Wishing everyone of all faiths and denominations a blessed Easter, she said: \&quot;May the living flame of the Easter hope be a steady guide as we face the future.\&quot;The Queen, 93, recorded the audio message in the White Drawing Room at Windsor Castle, with one sound engineer in the next room. The Palace described it as \&quot;Her Majesty's contribution to those who are celebrating Easter privately\&quot;. It follows a speech on Sunday, in which the monarch delivered a rallying message to the nation.In it, she said the UK \&quot;will succeed\&quot; in its fight against the coronavirus pandemic, thanked people for following government rules about staying at home and praised those \&quot;coming together to help others\&quot;.She also thanked key workers, saying \&quot;every hour\&quot; of work \&quot;brings us closer to a return to more normal times\&quot;.&quot;}
</code></pre>
<p>Obviously I know that it doesn't match the criteria and thats why it is failing but I'm strugging to get the JSON to be in the correct format to pass the test.</p>
<p>Any help or pointers in the right direction would be greatly appreciated. Thanks</p>
",Multilingual Language Processing & Language Identification,natural language processing returning result json object working completing python programing challenge natural language processing scraping information bbc news article previous submission code shown wa marked failed due passing test criterion shown however unable think way make code pas test tell code failing reason escape character matching test code use instead variable code test criterion used test json code return code return following obviously know match criterion thats failing strugging get json correct format pas test help pointer right direction would greatly appreciated thanks
How to clean non Arabic letters from a text file in python?,"<p>UPDATE-
Very new to python,
How to clean the text from everything but Arabic letters. I used regex function but without success.</p>
<p>This is my code</p>
<pre><code># load text
filename = '/content/drive/MyDrive/Colab Notebooks/ArabicKidsStories.txt'
file = open(filename,'rt')
text = file.read()
file.close()
import re
text = re.sub('([@A-Za-z0-9_]+)|[^\w\s]|#|http\S+', '', text) # cleaning up
print (text)
</code></pre>
<p>This is a sample of the output</p>
<pre><code> ÿ™ŸÅŸÇÿØÿ™ ŸÜÿ∏ÿßÿ±ÿ™Ÿä  ÿ≠ŸäŸÜ ÿßÿ≥ÿ™ŸäŸÇÿ∏ÿ™ ÿµÿ®ÿßÿ≠ÿß  ŸÅŸÑŸÖ ÿ£ÿ¨ÿØŸáÿß ŸÅŸä ŸÖŸÉÿßŸÜŸáÿß  Ÿàÿ®ÿ≠ÿ´ÿ™ ÿπŸÜŸáÿß ŸÅŸä ŸÉŸÑ ŸÖŸÉÿßŸÜ  ÿØŸàŸÜ ÿ£ŸÜ ÿ£ÿπÿ´ÿ± ŸÑŸáÿß ÿπŸÑŸâ ÿ£ÿ´ÿ±  Ÿäÿß ÿ•ŸÑŸáŸä  ŸÉŸäŸÅ ÿ≥ÿ£ÿÆÿ±ÿ¨ ÿßŸÑŸäŸàŸÖ ŸÖŸÜ ÿßŸÑÿ®Ÿäÿ™  Ÿàÿ£Ÿàÿßÿ¨Ÿá ÿßŸÑŸÜŸáÿßÿ±  
 Ÿàÿ™ŸÜÿßŸáŸâ ÿ•ŸÑŸä ŸÖŸÜ ÿßŸÑÿÆÿßÿ±ÿ¨  ÿµŸàÿ™ ŸÜŸÇÿßÿ± ÿßŸÑÿÆÿ¥ÿ®  ŸÅŸàŸÇ ÿ¨ÿ∞ÿπ ÿ¥ÿ¨ÿ±ÿ© ŸÇÿ±Ÿäÿ®ÿ© ŸÅÿ£ÿ≥ÿ±ÿπÿ™ ÿ•ŸÑŸâ ÿßŸÑÿ®ÿßÿ®  ŸàŸÅÿ™ÿ≠ÿ™Ÿá  Ÿàÿ•ÿ∞ÿß ÿ∂Ÿàÿ° ÿßŸÑŸÜŸáÿßÿ± Ÿäÿ®Ÿáÿ± ÿ®ÿµÿ±Ÿä  ŸÅÿ£ÿ∫ŸÑŸÇÿ™ ÿπŸäŸÜŸä  ŸàŸáÿ™ŸÅÿ™  ÿ£ŸäŸáÿß ÿßŸÑŸÜŸÇÿßÿ±  ÿ£ŸäŸÜ ÿ£ŸÜÿ™  
 Ÿàÿ≠ÿßŸàŸÑÿ™ ÿπÿ®ÿ´ÿß ÿ£ŸÜ ÿ£ŸÅÿ™ÿ≠ ÿπŸäŸÜŸä  Ÿàÿ£ŸÜÿß ÿ£ŸÇŸàŸÑ  ÿπŸÅŸàÿß  ŸÑÿß ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿ£ŸÜ ÿ£ŸÅÿ™ÿ≠ ÿπŸäŸÜŸä  ÿ•ŸÜ ÿßŸÑÿ∂Ÿàÿ° ŸäÿπŸÖŸäŸÜŸä  
 ŸÅŸÇÿßŸÑ ŸÜŸÇÿßÿ± ÿßŸÑÿÆÿ¥ÿ®  Ÿáÿ∞ÿß ÿ∑ÿ®ŸäÿπŸä  Ÿäÿß ÿπÿ≤Ÿäÿ≤ÿ™Ÿä  ŸÅÿ£ŸÜÿ™ ŸÑŸÖ ÿ™ÿ∂ÿπŸä ŸÜÿ∏ÿßÿ±ÿ™ŸÉ ÿßŸÑÿ¥ŸÖÿ≥Ÿäÿ©  
 Ÿàÿ™ÿ±ÿßÿ¨ÿπÿ™ ŸÇŸÑŸäŸÑÿß  ŸàŸÇŸÑÿ™  ŸÑŸÇÿØ ÿßÿÆÿ™ŸÅÿ™ ŸÜÿ∏ÿßÿ±ÿ™Ÿä  
 ŸÅÿ™ÿ≥ÿßÿ°ŸÑ ŸÜŸÇÿßÿ± ÿßŸÑÿÆÿ¥ÿ®  ÿßÿÆÿ™ŸÅÿ™  ŸÖÿßÿ∞ÿß ÿ™ŸÇŸàŸÑŸäŸÜ  
 Ÿàÿ®ÿØŸÑ ÿ£ŸÜ ÿ£ÿ¨Ÿäÿ®Ÿá  ŸÇŸÑÿ™  ÿ£ÿ±ÿ¨ŸàŸÉ  ÿßÿ®ÿ≠ÿ´ ŸÑŸä ÿπŸÜ ŸÜÿ∏ÿßÿ±ÿ™Ÿä  ÿ•ŸÜŸÜŸä ŸÑÿß ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿÆÿ±Ÿàÿ¨ ŸÖŸÜ ÿØŸàŸÜŸáÿß  
 ŸàŸÑÿßÿ∞ ŸÜŸÇÿßÿ± ÿßŸÑÿÆÿ¥ÿ® ŸÑÿ≠ÿ∏ÿ©  ÿ´ŸÖ ŸÇÿßŸÑ  ÿ≠ÿ≥ŸÜ  ÿßÿ®ŸÇŸä ÿ£ŸÜÿ™ ŸÅŸä ÿßŸÑÿ®Ÿäÿ™  Ÿàÿ≥ÿ£ÿ®ÿ≠ÿ´ ŸÑŸÉ ÿ£ŸÜÿß ÿπŸÜŸáÿß  
 ŸàŸÖÿ∂Ÿâ ŸÜŸÇÿßÿ± ÿßŸÑÿÆÿ¥ÿ®  ŸÅÿ£ÿ∫ŸÑŸÇÿ™ ÿßŸÑÿ®ÿßÿ® ŸàÿßŸÑŸÜÿßŸÅÿ∞ÿ©  ŸàŸÇÿ®ÿπÿ™ ŸÅŸä ÿßŸÑÿ∏ŸÑÿßŸÖ  Ÿäÿß ŸÑŸÑÿ∫ÿ±ÿßÿ®ÿ©  ÿ•ŸÜŸÜŸä ÿ£ÿ±Ÿâ ŸÅŸä ÿßŸÑŸÑŸäŸÑ ÿ£Ÿäÿ∂ÿß  ÿ£ŸàŸá  ŸÉŸÑÿß  ÿ•ŸÜŸÜŸä ÿ£ÿ≠ÿ® ÿßŸÑŸÜŸáÿßÿ±  Ÿàÿ£ÿ≠ÿ®ÿ∞ ÿ£ŸÜ ÿ£ÿ∑Ÿäÿ± ÿØŸàŸÖÿß ŸÅŸä ÿßŸÑŸÜŸàÿ± ŸÖÿπ ÿ±ŸÅÿßŸÇŸä  ÿ•ŸÜŸÜŸä ŸÑÿß ÿ£ÿ≠ÿ® ÿßŸÑŸÑŸäŸÑ  ŸàŸÑÿß ÿ£ÿ±ŸäÿØ ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿ∏ŸÑÿßŸÖ ÿπÿßŸÑŸÖŸä  ÿ™ÿ±Ÿâ ÿ£ŸäŸÜ ÿßÿÆÿ™ŸÅÿ™ Ÿáÿ∞Ÿá ÿßŸÑŸÜÿ∏ÿßÿ±ÿ© ÿßŸÑŸÑÿπŸäŸÜÿ©  
    
 ŸÄŸÄŸÄŸÄŸÄŸÄŸÄŸÄŸÄŸÄŸÄŸÄŸÄ 
 ÿπÿßÿØ ŸÜŸÇÿßÿ± ÿßŸÑÿÆÿ¥ÿ® ŸÖÿ™ÿπÿ®ÿß  ŸÇÿ®ŸÑ ÿßŸÑŸÖÿ≥ÿßÿ°  ŸàŸÇÿßŸÑ ŸÑŸä  ÿ¢ÿ≥ŸÅ  Ÿäÿß ÿπÿ≤Ÿäÿ≤Ÿä  ÿ≥ÿ£ŸÑÿ™ ÿπŸÜ ŸÜÿ∏ÿßÿ±ÿ™ŸÉ ÿßŸÑÿ∑ŸäŸàÿ± ÿ¨ŸÖŸäÿπÿß  ŸÑŸÉŸÜ ÿ£ÿ≠ÿØÿß ŸÖŸÜŸáŸÖ ŸÑŸÖ Ÿäÿ±Ÿáÿß  
 ŸÅÿ£ÿ∑ÿ±ŸÇÿ™ ÿ®ÿ±ÿ£ÿ≥Ÿä ÿ®ÿ±Ÿáÿ©  ÿ´ŸÖ ŸÇŸÑÿ™  ÿ£ÿ¥ŸÉÿ±ŸÉ  Ÿäÿß ÿπÿ≤Ÿäÿ≤Ÿä  ÿ≥ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜŸáÿß ÿ®ŸÜŸÅÿ≥Ÿä ŸÑŸäŸÑÿß  
 Ÿàÿßÿ™ÿ≥ÿπÿ™ ÿπŸäŸÜÿß ŸÜŸÇÿßÿ± ÿßŸÑÿÆÿ¥ÿ® ÿØŸáÿ¥ÿ©  ŸàŸÇÿßŸÑ  ŸÑŸäŸÑÿß  
 ŸàŸÇÿ®ŸÑ ÿ£ŸÜ ÿ£ÿ¨Ÿäÿ®Ÿá  ŸÖÿ∂Ÿâ ÿπŸÑŸâ ÿπÿ¨ŸÑ  ŸàŸáŸà ŸäŸÇŸàŸÑ  ÿπŸÅŸàÿß  ÿµÿ∫ÿßÿ±Ÿä ŸäŸÜÿ™ÿ∏ÿ±ŸàŸÜŸÜŸä ÿßŸÑÿ¢ŸÜ  ÿ•ŸÑŸâ ÿßŸÑŸÑŸÇÿßÿ°  
</code></pre>
<p>Any help will be appreciated.
Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,clean non arabic letter text file python update new python clean text everything arabic letter used regex function without success code sample output help appreciated thanks advance
How to interpret Python NLTK bigram likelihood ratios?,"<p>I'm trying to figure out how to properly interpret <code>nltk</code>'s ""likelihood ratio"" given the below code (taken from this <a href=""https://stackoverflow.com/questions/8683588/understanding-nltk-collocation-scoring-for-bigrams-and-trigrams"">question</a>). </p>

<pre><code>import nltk.collocations
import nltk.corpus
import collections

bgm = nltk.collocations.BigramAssocMeasures()
finder = nltk.collocations.BigramCollocationFinder.from_words(nltk.corpus.brown.words())
scored = finder.score_ngrams(bgm.likelihood_ratio)

# Group bigrams by first word in bigram.                                        
prefix_keys = collections.defaultdict(list)
for key, scores in scored:
    prefix_keys[key[0]].append((key[1], scores))

for key in prefix_keys:
    prefix_keys[key].sort(key = lambda x: -x[1])

prefix_keys['baseball']
</code></pre>

<p><strong>With the following output:</strong></p>

<pre><code>[('game', 32.11075451975229),
 ('cap', 27.81891372457088),
 ('park', 23.509042621473505),
 ('games', 23.10503351305401),
 (""player's"", 16.22787286342467),
 ('rightfully', 16.22787286342467),
[...]
</code></pre>

<p>Looking at the <a href=""http://www.nltk.org/api/nltk.metrics.html#nltk.metrics.association.NgramAssocMeasures"" rel=""nofollow noreferrer"">docs</a>, it looks like the likelihood ratio printed next to each bigram is from</p>

<blockquote>
  <p>""Scores ngrams using likelihood ratios as in Manning and Schutze
  5.3.4.""</p>
</blockquote>

<p>Referring to <a href=""https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"" rel=""nofollow noreferrer"">this article</a>, which states on pg. 22:</p>

<blockquote>
  <p>One advantage of likelihood ratios is that they have a clear intuitive
  interpretation. For example, the bigram powerful computers is
  e^(.5*82.96) = 1.3*10^18 times more likely under the hypothesis that
  computers is more likely to follow powerful than its base rate of
  occurrence would suggest. This number is easier to interpret than the
  scores of the t test or the 2 test which we have to look up in a
  table.</p>
</blockquote>

<p>What I'm confused about is what would be the ""base rate of occurence"" in the event that I'm using the <code>nltk</code> code noted above with my own data. Would it be safe to say, for example, that ""game"" is 32 times more likely to appear next to ""baseball"" in the current dataset than in the average use of the standard English language? Or is it that ""game"" is more likely to appear next to ""baseball"" than other words appearing next to ""baseball"" within the <em>same</em> set of data?</p>

<p>Any help/guidance towards a clearer interpretation or example is much appreciated!</p>
",Multilingual Language Processing & Language Identification,interpret python nltk bigram likelihood ratio trying figure properly interpret likelihood ratio given code taken doc look like likelihood ratio printed next bigram score ngrams using likelihood ratio manning schutze referring article state pg one advantage likelihood ratio clear intuitive interpretation example bigram powerful computer e time likely hypothesis computer likely follow powerful base rate occurrence would suggest number easier interpret score test test look table confused would base rate occurence event using code noted data would safe say example game time likely appear next baseball current dataset average use standard english language game likely appear next baseball word appearing next baseball within set data help guidance towards clearer interpretation example much appreciated
Can I use Facebook translate service in my python NLP project?,"<p>I hope you are feeling good and safe.</p>
<p>I'm working on Natural language processing project for my master degree, and I do need to translate
my local dialect to ENG, and I noticed that Facebook translate machine did very well with my local dialect.</p>
<p>So my question is there any way to use Facebook translate service in my project, like is there any api or python module that use it.</p>
",Multilingual Language Processing & Language Identification,use facebook translate service python nlp project hope feeling good safe working natural language processing project master degree need translate local dialect eng noticed facebook translate machine well local dialect question way use facebook translate service project like api python module use
How to remove Non English words in Python?,"<p>I am doing a sentiment analysis project in Python (using Natural Language Processing). I already collected the data from twitter and saved it as a CSV file. The file contains tweets, which are mostly about cryptocurrency. I cleaned the data but there is one more thing before I apply sentiment analysis using classfication algorithms. Here's the out for importing libraries </p>

<pre><code># importing Libraries
from pandas import DataFrame, read_csv
import chardet
import matplotlib.pyplot as plt; plt.rcdefaults()
from matplotlib import rc
%matplotlib inline
import pandas as pd
plt.style.use('ggplot')
import numpy as np
import re
import warnings

#Visualisation
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from IPython.display import display
from mpl_toolkits.basemap import Basemap
from wordcloud import WordCloud, STOPWORDS

#nltk
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
from nltk import tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.snowball import SnowballStemmer


matplotlib.style.use('ggplot')
pd.options.mode.chained_assignment = None
warnings.filterwarnings(""ignore"")

%matplotlib inline

    ## Reading CSV File and naming the object called crime
ltweet=pd.read_csv(""C:\\Users\\name\\Documents\\python assignment\\litecoin1.csv"",index_col = None, skipinitialspace = True)
print(ltweet)
</code></pre>

<p>I already clean most of the data, so no need to put the codes for that part. In my column there are tweets that contains mostly non English language. I want to remove all of them(Non English text only). Here's the output for example</p>

<pre><code>ltweet['Tweets'][0:3]

output:
0      the has published a book on understanding ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé
1      accepts litecoin gives % discount on all iphon...
2      days until litepay launches accept store and s...
3           ltc to usd price litecoin ltc cryptocurrency
</code></pre>

<p>Is there a way to remove non English words in the data? Can anyone help me write the code for it? By the way, the code is based on Pandas.</p>
",Multilingual Language Processing & Language Identification,remove non english word python sentiment analysis project python using natural language processing already collected data twitter saved csv file file contains tweet mostly cryptocurrency cleaned data one thing apply sentiment analysis using classfication algorithm importing library already clean data need put code part column tweet contains mostly non english language want remove non english text output example way remove non english word data anyone help write code way code based panda
How to add temporal variables to first-order logic formulas?,"<p>I've written an English parser in Prolog that uses first-order logic .</p>
<p>I now want to extend  this parser so that it's
able to tell whether a sentence is in the past, present or future.</p>
<p>In other words, I want my English grammar parser to be able to parse a sentence into a first-order logic formula that contains tense/temporal variables</p>
<p>For example :</p>
<p>The sentence <code>Bill slept.</code> should  be parsed as <code>some(T,and(before(T,now),sleep(bill,T))).</code></p>
<p>And ambigue  example sentences like <code>Every mother loves a child.</code> should still have several possible parsings :</p>
<p><code>- some(T,and(eq(T,now), all(X,imp(mother(X),some(Y,and(child(Y),loves(X,Y))))))).</code>
<code>- some(T,and(eq(T,now), some(X,and(child(X),all(Y,imp(mother(Y),loves(Y,X))))))).</code></p>
<p>However, currently I don't really know how to approach the extension of the first-order logic-formulas with the needed temporal variables.</p>
<p>Therefore my question is if anyone here knows and could point me towards any resources  regarding the extension of first-order logic formulas with temportal variables, or kows about any parsers written in Prolog that can handle tense?</p>
<p>Here is my grammar and parts of the lexicon that are relevant for the example sentences:</p>
<p><strong>Grammar</strong></p>
<pre><code>/*========================================================================
   Texts
========================================================================*/

t([sem:T])--&gt; 
   s([coord:no,sem:S]),
   {combine(t:T,[s:S])}.

t([sem:T])--&gt; 
   s([coord:yes,sem:S]),
   {combine(t:T,[s:S])}.

t([sem:T])--&gt; 
   q([sem:Q]),
   {combine(t:T,[q:Q])}.


/*========================================================================
   Sentences
========================================================================*/

s([coord:no,sem:Sem])--&gt; 
   np([coord:_,num:Num,gap:[],sem:NP]), 
   vp([coord:_,inf:fin,num:Num,gap:[],sem:VP]), 
   {combine(s:Sem,[np:NP,vp:VP])}.

s([coord:yes,sem:Sem])--&gt; 
   s([coord:ant,sem:S1]), 
   s([coord:con,sem:S2]), 
   {combine(s:Sem,[s:S1,s:S2])}.

s([coord:yes,sem:Sem])--&gt; 
   s([coord:either,sem:S1]), 
   s([coord:or,sem:S2]), 
   {combine(s:Sem,[s:S1,s:S2])}.

s([coord:ant,sem:Sem])--&gt; 
   [if], 
   s([coord:no,sem:S]),
   {combine(s:Sem,[if:S])}.

s([coord:either,sem:Sem])--&gt; 
   [either], 
   s([coord:no,sem:S]),
   {combine(s:Sem,[either:S])}.

s([coord:con,sem:Sem])--&gt; 
   [then], 
   s([coord:no,sem:S]),
   {combine(s:Sem,[then:S])}.

s([coord:con,sem:Sem])--&gt;
   s([coord:no,sem:S]),
   {combine(s:Sem,[then:S])}.

s([coord:or,sem:Sem])--&gt;
   [or], 
   s([coord:no,sem:S]),
   {combine(s:Sem,[or:S])}.

sinv([gap:G,sem:S])--&gt;
   av([inf:fin,num:Num,sem:Sem]),
   np([coord:_,num:Num,gap:[],sem:NP]),
   vp([coord:_,inf:inf,num:Num,gap:G,sem:VP]), 
   {combine(sinv:S,[av:Sem,np:NP,vp:VP])}.


/*========================================================================
   Questions
========================================================================*/

q([sem:Sem])--&gt; 
   whnp([num:Num,sem:NP]), 
   vp([coord:_,inf:fin,num:Num,gap:[],sem:VP]), 
   {combine(q:Sem,[whnp:NP,vp:VP])}.

q([sem:Sem])--&gt; 
   whnp([num:_,sem:NP]), 
   sinv([gap:[np:NP],sem:S]),
   {combine(q:Sem,[sinv:S])}.


/*========================================================================
   Noun Phrases
========================================================================*/

np([coord:no,num:sg,gap:[np:NP],sem:NP])--&gt; [].

np([coord:yes,num:pl,gap:[],sem:NP])--&gt; 
   np([coord:no,num:sg,gap:[],sem:NP1]), 
   coord([type:conj,sem:C]), 
   np([coord:_,num:_,gap:[],sem:NP2]), 
   {combine(np:NP,[np:NP1,coord:C,np:NP2])}.

np([coord:yes,num:sg,gap:[],sem:NP])--&gt; 
   np([coord:no,num:sg,gap:[],sem:NP1]), 
   coord([type:disj,sem:C]), 
   np([coord:_,num:sg,gap:[],sem:NP2]), 
   {combine(np:NP,[np:NP1,coord:C,np:NP2])}.

np([coord:no,num:sg,gap:[],sem:NP])--&gt; 
   det([mood:decl,type:_,sem:Det]), 
   n([coord:_,sem:N]), 
   {combine(np:NP,[det:Det,n:N])}.

np([coord:no,num:sg,gap:[],sem:NP])--&gt; 
   pn([sem:PN]), 
   {combine(np:NP,[pn:PN])}.

np([coord:no,num:sg,gap:[],sem:NP])--&gt; 
   qnp([mood:decl,sem:QNP]), 
   {combine(np:NP,[qnp:QNP])}.


/*========================================================================
   WH Noun Phrases
========================================================================*/

whnp([num:sg,sem:NP])--&gt; 
   qnp([mood:int,sem:QNP]), 
   {combine(whnp:NP,[qnp:QNP])}.

whnp([num:sg,sem:NP])--&gt; 
   det([mood:int,type:_,sem:Det]), 
   n([coord:_,sem:N]), 
   {combine(whnp:NP,[det:Det,n:N])}.


/*========================================================================
   Nouns
========================================================================*/

n([coord:yes,sem:N])--&gt; 
   n([coord:no,sem:N1]), 
   coord([type:_,sem:C]),  
   n([coord:_,sem:N2]),
   {combine(n:N,[n:N1,coord:C,n:N2])}.

n([coord:C,sem:Sem])--&gt; 
   adj([sem:A]), 
   n([coord:C,sem:N]), 
   {combine(n:Sem,[adj:A,n:N])}.

n([coord:no,sem:N])--&gt; 
   noun([sem:Noun]),
   {combine(n:N,[noun:Noun])}.

n([coord:no,sem:Sem])--&gt; 
   noun([sem:N]), 
   nmod([sem:PP]),
   {combine(n:Sem,[noun:N,nmod:PP])}. 

nmod([sem:N])--&gt; 
   pp([sem:PP]),
   {combine(nmod:N,[pp:PP])}.

nmod([sem:N])--&gt; 
   rc([sem:RC]),
   {combine(nmod:N,[rc:RC])}.

nmod([sem:Sem])--&gt; 
   pp([sem:PP]), 
   nmod([sem:NMod]),
   {combine(nmod:Sem,[pp:PP,nmod:NMod])}.


/*========================================================================
   Verb Phrases
========================================================================*/

vp([coord:yes,inf:Inf,num:Num,gap:[],sem:VP])--&gt; 
   vp([coord:no,inf:Inf,num:Num,gap:[],sem:VP1]), 
   coord([type:_,sem:C]), 
   vp([coord:_,inf:Inf,num:Num,gap:[],sem:VP2]),
   {combine(vp:VP,[vp:VP1,coord:C,vp:VP2])}.

vp([coord:no,inf:Inf,num:Num,gap:[],sem:VP])--&gt; 
   av([inf:Inf,num:Num,sem:Mod]), 
   vp([coord:_,inf:inf,num:Num,gap:[],sem:V2]), 
   {combine(vp:VP,[av:Mod,vp:V2])}.

vp([coord:no,inf:Inf,num:Num,gap:[],sem:VP])--&gt; 
   cop([inf:Inf,num:Num,sem:Cop]), 
   np([coord:_,num:_,gap:[],sem:NP]), 
   {combine(vp:VP,[cop:Cop,np:NP])}.

vp([coord:no,inf:Inf,num:Num,gap:[],sem:VP])--&gt; 
   iv([inf:Inf,num:Num,sem:IV]), 
   {combine(vp:VP,[iv:IV])}.

vp([coord:no,inf:I,num:Num,gap:G,sem:VP])--&gt;   
   tv([inf:I,num:Num,sem:TV]), 
   np([coord:_,num:_,gap:G,sem:NP]), 
   {combine(vp:VP,[tv:TV,np:NP])}.


/*========================================================================
   Prepositional Phrases
========================================================================*/

pp([sem:PP])--&gt; 
   prep([sem:Prep]), 
   np([coord:_,num:_,gap:[],sem:NP]), 
   {combine(pp:PP,[prep:Prep,np:NP])}.


/*========================================================================
   Relative Clauses
========================================================================*/

rc([sem:RC])--&gt; 
   relpro([sem:RP]), 
   vp([coord:_,inf:fin,num:sg,gap:[],sem:VP]), 
   {combine(rc:RC,[relpro:RP,vp:VP])}.


/*========================================================================
   Lexical Rules
========================================================================*/

%intransitive_Verbs

iv([inf:Inf,num:Num,sem:Sem])--&gt; 
   {lexEntry(iv,[symbol:Sym,syntax:Word,inf:Inf,num:Num])},
   Word,
   {semLex(iv,[symbol:Sym,sem:Sem])}.
   
%transitive_Verbs

tv([inf:Inf,num:Num,sem:Sem])--&gt; 
   {lexEntry(tv,[symbol:Sym,syntax:Word,inf:Inf,num:Num])},
   Word,
   {semLex(tv,[symbol:Sym,sem:Sem])}.
   
%Copulas

cop([inf:Inf,num:Num,sem:Sem])--&gt; 
   {lexEntry(cop,[pol:Pol,syntax:Word,inf:Inf,num:Num])},
   Word,
   {semLex(cop,[pol:Pol,sem:Sem])}.
   
%Determiners

det([mood:M,type:Type,sem:Det])--&gt; 
   {lexEntry(det,[syntax:Word,mood:M,type:Type])},
   Word,
   {semLex(det,[type:Type,sem:Det])}. 
   
%Proper_nouns

pn([sem:Sem])--&gt; 
   {lexEntry(pn,[symbol:Sym,syntax:Word])},
   Word,  
   {semLex(pn,[symbol:Sym,sem:Sem])}.
   
%relative_Pronouns

relpro([sem:Sem])--&gt; 
   {lexEntry(relpro,[syntax:Word])},
   Word,
   {semLex(relpro,[sem:Sem])}.
   
%Prepositions

prep([sem:Sem])--&gt; 
   {lexEntry(prep,[symbol:Sym,syntax:Word])},
   Word,
   {semLex(prep,[symbol:Sym,sem:Sem])}.
   
%Adjectives

adj([sem:Sem])--&gt; 
   {lexEntry(adj,[symbol:Sym,syntax:Word])},
   Word,
   {semLex(adj,[symbol:Sym,sem:Sem])}.

%Adverbs

av([inf:Inf,num:Num,sem:Sem])--&gt; 
   {lexEntry(av,[syntax:Word,inf:Inf,num:Num,pol:Pol])},
   Word,
   {semLex(av,[pol:Pol,sem:Sem])}.
   
%Coordinators

coord([type:Type,sem:Sem])--&gt; 
   {lexEntry(coord,[syntax:Word,type:Type])},
   Word, 
   {semLex(coord,[type:Type,sem:Sem])}.
   
%Quantified_Noun_Phrases

qnp([mood:M,sem:NP])--&gt; 
   {lexEntry(qnp,[symbol:Symbol,syntax:Word,mood:M,type:Type])},
   Word,
   {semLex(qnp,[type:Type,symbol:Symbol,sem:NP])}.
   
%Nouns

noun([sem:Sem])--&gt; 
   {lexEntry(noun,[symbol:Sym,syntax:Word])},
   Word,
   {semLex(noun,[symbol:Sym,sem:Sem])}.
</code></pre>
<p><strong>Lexicon</strong></p>
<pre><code>%Determiners

lexEntry(det,[syntax:[every],mood:decl,type:uni]).
lexEntry(det,[syntax:[a],mood:decl,type:indef]).

%(Proper)Nouns

lexEntry(pn,[symbol:bill,syntax:[bill]]).

lexEntry(noun,[symbol:mother,syntax:[mother]]).
lexEntry(noun,[symbol:child,syntax:[child]]).

%Verbs

lexEntry(iv,[symbol:sleep,syntax:[sleep],inf:inf,num:sg]).
lexEntry(iv,[symbol:sleep,syntax:[sleeps],inf:fin,num:sg]).
lexEntry(iv,[symbol:sleep,syntax:[sleep],inf:fin,num:pl]).

lexEntry(tv,[symbol:love,syntax:[love],inf:inf,num:sg]).
lexEntry(tv,[symbol:love,syntax:[loves],inf:fin,num:sg]).
lexEntry(tv,[symbol:love,syntax:[love],inf:fin,num:pl]).
</code></pre>
",Multilingual Language Processing & Language Identification,add temporal variable first order logic formula written english parser prolog us first order logic want extend parser able tell whether sentence past present future word want english grammar parser able parse sentence first order logic formula contains tense temporal variable example sentence parsed ambigue example sentence like still several possible parsings however currently really know approach extension first order logic formula needed temporal variable therefore question anyone know could point towards resource regarding extension first order logic formula temportal variable kows parser written prolog handle tense grammar part lexicon relevant example sentence grammar lexicon
How to parse and encode Chinese Characters in Jupyter Notebook?,"<p>I want to train a really basic NLP model but using Chinese characters. Read_csv doesn't really work.</p>
<p>I was also wondering if there is any way to extract the different parts of the character, like for example, I would like to write an if function that works something like this:</p>
<pre><code>x = input()

if x contains Â•≥ [as part of the word]:
   female = 1
</code></pre>
<p>So if i typed in Â™õÔºå then female=1</p>
<p>The bottomline is to train a Naive Bayes model on Chinese characters. If anyone can direct me to resources that can help me to do this will be appreciated!</p>
<p>Thanks</p>
",Multilingual Language Processing & Language Identification,parse encode chinese character jupyter notebook want train really basic nlp model using chinese character read csv really work wa also wondering way extract different part character like example would like write function work something like typed female bottomline train naive bayes model chinese character anyone direct resource help appreciated thanks
What languages are supported for nltk.word_tokenize and nltk.pos_tag,"<p>I need to conduct name entity extraction for text in multiple languages: spanish, portuguese, greek, czech, chinese.</p>

<p>Is there somewhere a list of all supported languages for these two functions? And is there a method to use other corpora so that these languages can be included?</p>
",Multilingual Language Processing & Language Identification,language supported nltk word tokenize nltk po tag need conduct name entity extraction text multiple language spanish portuguese greek czech chinese somewhere list supported language two function method use corpus language included
MT: Calculating METEOR Score of two different files,"<p>For an Machine Translation evaluation I need to calculate the METEOR score between the translation output file and the reference file.</p>
<p>I already found this question <a href=""https://stackoverflow.com/questions/63778133/how-can-i-implement-meteor-score-when-evaluating-a-model-when-using-the-meteor-s"">How can I implement meteor score when evaluating a model when using the meteor_score module from nltk?</a>, but it was not helpful.</p>
<ul>
<li>Is there any official Python package or Github repository for the calculation of the METEOR score you would recommend?</li>
<li>How to calculate the METEOR between the reference file and the output translation file?</li>
</ul>
",Multilingual Language Processing & Language Identification,mt calculating meteor score two different file machine translation evaluation need calculate meteor score translation output file reference file already found question href implement meteor score evaluating model using meteor score module nltk wa helpful official python package github repository calculation meteor score would recommend calculate meteor reference file output translation file
Finding contradictory semantic sentences through natural language processing,"<p>I'm working on a project that aims to find conflicting <strong>Semantic Sentences</strong> (<strong>NLP - Semantic Search</strong> )
For example</p>
<p>Our text is: &quot;<strong>I ate today. The lunch was very tasty. I was an honest guest.</strong>&quot;</p>
<p>Query: &quot;<strong>I had lunch with my friend</strong>&quot;</p>
<p>Do we want to give the query model and find the meaning of the sentences with a certain point in terms of synonyms and antonyms?</p>
<p>The solution that came to my mind was to first find the <strong>synonymous sentences</strong> and extract the key words from the synonymous sentences and then get the <strong>semantic opposite</strong> words and then find the <strong>semantic synonymous</strong> sentences based on these opposite words.
Do you think this idea is possible? If you have a solution or experience in this area, please reply</p>
<p>Thanks</p>
",Multilingual Language Processing & Language Identification,finding contradictory semantic sentence natural language processing working project aim find conflicting semantic sentence nlp semantic search example text ate today lunch wa tasty wa honest guest query lunch friend want give query model find meaning sentence certain point term synonym antonym solution came mind wa first find synonymous sentence extract key word synonymous sentence get semantic opposite word find semantic synonymous sentence based opposite word think idea possible solution experience area please reply thanks
Creating a Neural Machine Translation basics,"<p>I'm currently working on a project design where I will create a program/model to translate my native dialect to English, I'm asking is there any books or anything that can you recommend to me in creating my project.</p>
",Multilingual Language Processing & Language Identification,creating neural machine translation basic currently working project design create program model translate native dialect english asking book anything recommend creating project
Sentiment analysis of non-English texts,"<p>I want to analyze sentiment of texts that are written in German. I found a lot of tutorials on how to do this with English, but I found none on how to apply it to different languages.</p>

<p>I have an idea to use the <code>TextBlob</code> Python library to first translate the sentences into English and then to do sentiment analysis, but I am not sure whether or not it is the best way to solve this task.</p>

<p>Or are there any other possible ways to solve this task?</p>
",Multilingual Language Processing & Language Identification,sentiment analysis non english text want analyze sentiment text written german found lot tutorial english found none apply different language idea use python library first translate sentence english sentiment analysis sure whether best way solve task possible way solve task
True definition of an English word?,"<p>What would be the best definition of an English word?</p>

<p>What are the other cases of an English word than just <code>\w+</code>?
Some may include <code>\w+-\w+</code> or <code>\w+'\w+</code>; some may exclude cases like <code>\b[0-9]+\b</code>. But I haven't seen 
any general consensus on those cases. 
Do we have a formal defintion of such?
Can any of you clarify?</p>

<p>(Edit: broaden the question so it doesn't depend on regexp only.)</p>
",Multilingual Language Processing & Language Identification,true definition english word would best definition english word case english word may include may exclude case like seen general consensus case formal defintion clarify edit broaden question depend regexp
How to identify/detect a vocabulary in a text (Node JS),"<p>I'm currently working on an app on which I have blocs of text and would like to know if they're related to cooking / recipe vocabulary. I've seen and tried a few things, but I'm starting to wonder if I'm not going to much overkill on that ( I don't want to recreate the wheel ).</p>
<p>The road on which I'm working now implies to get all words related to this vocabulary ( ingredients, actions, objects.. in many languages) and compare my database to each word on my texts blocs and then define a score for each bloc that would be used to decide (depending on my threshold) if should keep it or not.</p>
<p>The main problem with this method is that I need to create a very big database myself (which is a long ass process) and the bigger my database gets, the longer/less effective the comparing process might be. Any ideas of howto do that ? Thank you !</p>
",Multilingual Language Processing & Language Identification,identify detect vocabulary text node j currently working app bloc text would like know related cooking recipe vocabulary seen tried thing starting wonder going much overkill want recreate wheel road working implies get word related vocabulary ingredient action object many language compare database word text bloc define score bloc would used decide depending threshold keep main problem method need create big database long process bigger database get longer le effective comparing process might idea howto thank
Implementing a Top Down Parser in C#,"<p>I am student and I want to implement a top-down parser in my C# language translation project. For example, if I need to construct a parser tree for the sentence &quot;My name is Husni and I am a student&quot;, how can I do it with C#?</p>
",Multilingual Language Processing & Language Identification,implementing top parser c student want implement top parser c language translation project example need construct parser tree sentence name husni student c
Tokenizing Chinese text with keras.preprocessing.text.Tokenizer,"<p><code>keras.preprocessing.text.Tokenizer</code> doesn't work correctly with Chinese text. How can I modify it to work on Chinese text?</p>
<pre class=""lang-py prettyprint-override""><code>from keras.preprocessing.text import Tokenizer
def fit_get_tokenizer(data, max_words):
    tokenizer = Tokenizer(num_words=max_words, filters='!&quot;#%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n')
    tokenizer.fit_on_texts(data)
    return tokenizer
tokenizer = fit_get_tokenizer(df.sentence,max_words=150000)
print('Total number of words: ', len(tokenizer.word_index))
vocabulary_inv = {}
for word in tokenizer.word_index:
    vocabulary_inv[tokenizer.word_index[word]] = word
print(vocabulary_inv)
</code></pre>
",Multilingual Language Processing & Language Identification,tokenizing chinese text kera preprocessing text tokenizer work correctly chinese text modify work chinese text
Generating pronounceable digrams in Python,"<p>I am generating all two letter (digram) combinations of the standard alphabet</p>
<pre><code>import string
import itertools
UPPERCASE_ALPHABET = string.ascii_uppercase
TWO_LETTER_PERMS = itertools.permutations(UPPERCASE_ALPHABET, 2)
</code></pre>
<p>By using the 650 generated digrams in a search application of drug names,I am finding that some of them always retrieve no results probably being not pronounceable.</p>
<p>For example &quot;ZQ&quot; or &quot;KM&quot; (just as an example). While I do understand that each language might carry some differences, at least for English is there a way to judge if a generated digram is pronounceable?</p>
",Multilingual Language Processing & Language Identification,generating pronounceable digram python generating two letter digram combination standard alphabet using generated digram search application drug name finding always retrieve result probably pronounceable example zq km example understand language might carry difference least english way judge generated digram pronounceable
Is there a way to remove punctuation from Persian text?,"<p>I want to get rid of punctuations from my text file which is an English-Persian sentence pairs data.</p>

<p>I have tried the following code:</p>

<pre><code>import string
import re
from numpy import array, argmax, random, take
import pandas as pd

# function to read raw text file
def read_text(filename):
    # open the file
    file = open(filename, mode='rt', encoding='utf-8')

    # read all text
    text = file.read()
    file.close()
    return text

# split a text into sentences
def to_lines(text):
  sents = text.strip().split('\n')
  sents = [i.split('\t') for i in sents]
  return sents


data = read_text(""pes.txt"")
pes_eng = to_lines(data)
pes_eng = array(pes_eng)

# Remove punctuation
pes_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s         
in pes_eng[:,0]]
pes_eng[:,1] = [s.replace(""ÿü!.ÿå,?"" ,"""") for s in pes_eng]

print(pes_eng)
</code></pre>

<p>the code above works with English sentences but it is not doing anything with Persian sentences.</p>

<p>Here the output is: </p>

<pre><code>Traceback (most recent call last):
  File "".\persian_to_english.py"", line 29, in &lt;module&gt;
    pes_eng[:,1] = [s.replace(""ÿü!.ÿå,?"" ,"""") for s in pes_eng]
  File "".\persian_to_english.py"", line 29, in &lt;listcomp&gt;
    pes_eng[:,1] = [s.replace(""ÿü!.ÿå,?"" ,"""") for s in pes_eng]
AttributeError: 'numpy.ndarray' object has no attribute 'replace'
</code></pre>

<p>But what I want is something like this:</p>

<pre><code>['Who' '⁄ÜŸá ⁄©ÿ≥€å']
</code></pre>
",Multilingual Language Processing & Language Identification,way remove punctuation persian text want get rid punctuation text file english persian sentence pair data tried following code code work english sentence anything persian sentence output want something like
gender identification in natural language processing,"<p>I have written below code using stanford nlp packages.</p>

<pre><code>GenderAnnotator myGenderAnnotation = new GenderAnnotator();
myGenderAnnotation.annotate(annotation);
</code></pre>

<p>But for the sentence ""Annie goes to school"", it is not able to identify the gender of Annie. </p>

<p>The output of application is:</p>

<pre><code>     [Text=Annie CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP Lemma=Annie NamedEntityTag=PERSON] 
     [Text=goes CharacterOffsetBegin=6 CharacterOffsetEnd=10 PartOfSpeech=VBZ Lemma=go NamedEntityTag=O] 
     [Text=to CharacterOffsetBegin=11 CharacterOffsetEnd=13 PartOfSpeech=TO Lemma=to NamedEntityTag=O] 
     [Text=school CharacterOffsetBegin=14 CharacterOffsetEnd=20 PartOfSpeech=NN Lemma=school NamedEntityTag=O] 
     [Text=. CharacterOffsetBegin=20 CharacterOffsetEnd=21 PartOfSpeech=. Lemma=. NamedEntityTag=O]
</code></pre>

<p>What is the correct approach to get the gender?</p>
",Multilingual Language Processing & Language Identification,gender identification natural language processing written code using stanford nlp package sentence annie go school able identify gender annie output application correct approach get gender
Effective DataSet for NLP practice,"<p>Please share a link to a dataset that is effective for practicing NLP (Natural Language Processing). </p>

<p>I am beginner level and would like to improve my skills.</p>
",Multilingual Language Processing & Language Identification,effective dataset nlp practice please share link dataset effective practicing nlp natural language processing beginner level would like improve skill
Model could not be saved,"<pre><code>class Seq2Seq(keras.Model):
    def __init__(self, enc_v_dim, dec_v_dim, emb_dim, units, attention_layer_size, max_pred_len, start_token,
                 end_token):
        super().__init__()
        self.units = units

        ....

    def encode(self, x):
        o = self.enc_embeddings(x)
        init_s = [tf.zeros((x.shape[0], self.units)), tf.zeros((x.shape[0], self.units))]
       ...
        return s

    def inference(self, x, return_align=False):
         ... 
         return pred_id

    def train_logits(self, x, y, seq_len):
        ...
        return logits

    def step(self, x, y, seq_len):
        with tf.GradientTape() as tape:
            logits = self.train_logits(x, y, seq_len)
            dec_out = y[:, 1:]  # ignore &lt;GO&gt;
            loss = self.cross_entropy(dec_out, logits)
            grads = tape.gradient(loss, self.trainable_variables)
        self.opt.apply_gradients(zip(grads, self.trainable_variables))
        return loss
</code></pre>
<p>I found a model for natural language processing that works well and trained well. But I don't know how to save it, it's different from the typical model structure I've seen, and I can't call functions like build or compile. I would like to know how can I save such a model.</p>
",Multilingual Language Processing & Language Identification,model could saved found model natural language processing work well trained well know save different typical model structure seen call function like build compile would like know save model
Is it possible to check if a short sequence of text is random or not?,"<p>Is it possible to check if a short sequence of text, e.g. two or three words, is random or not?
My first thought was to calculate the entropy on the string.</p>

<pre><code>H(""hello world"") = 2.84535
H(""sdzfjksher"") = 3.12193
</code></pre>

<p>but any combination of the chars in <code>""hello world""</code> will result in the same entropy, but will create a random string like <code>""llloo ehrdw""</code>. Entropy based methods works great on long strings like text. Here you can also count single chars to determinate that its a language. You can also use Zipfs Law here to check for real languages...</p>

<p>the next method would be a lookup table of common words, like a normal english dictionary. The problem with this method is to create a list of words first.</p>

<p>For example:</p>

<pre><code>input string        result
------------------------------------------------------
""hello world""       matches 2 words
""helloworld""        random string
""lllooehrdw""        random string
""hello.world""       probably 2 words 
""a.be.was""          probably 3 words (but this is probably a strange edge case)
</code></pre>

<p>So its all about finding words here to compare them with your wordlist, which can be really hard. </p>

<p>Another problem with all these methods could be, that they only detect certain languages or need to be trained to a certain language. Consider that we only want to use english for now.</p>

<p>So is there any good method to do this, or do i need to accept False Positives and False Negatives?</p>
",Multilingual Language Processing & Language Identification,possible check short sequence text random possible check short sequence text e g two three word random first thought wa calculate entropy string combination char result entropy create random string like entropy based method work great long string like text also count single char determinate language also use zipfs law check real language next method would lookup table common word like normal english dictionary problem method create list word first example finding word compare wordlist really hard another problem method could detect certain language need trained certain language consider want use english good method need accept false positive false negative
What is the equivalent of Google&#39;s Dialogflow&#39;s `@sys.date-period` if you&#39;re using Actions Console?,"<p>I'm struggling to come to grips with the differences in Google's natural language processing tools. It seems <a href=""https://en.wikipedia.org/wiki/Dialogflow"" rel=""nofollow noreferrer"">Dialogflow</a> was the original purchase but now Google appears to be egging people on to the &quot;Actions Console&quot;. I'm not even sure if this is a correct statement because the internet is littered with so many different Google tools and one never quite knows which one is current, recommended, or perhaps deprecated.</p>
<p>Anyhow, my concern is how to get &quot;date intervals&quot;. For example, my application wants to say:</p>
<p><em>&quot;How much money did I spend on fuel in December?&quot;</em></p>
<p>by implication I need a start date and an end date.</p>
<p>In Google's Dialowflow's lingo it appears we are fishing for a <strong>System Entity</strong>, Date and Time, called <a href=""https://cloud.google.com/dialogflow/es/docs/reference/system-entities"" rel=""nofollow noreferrer""><code>@sys.date.date-period</code></a>. This matches a date interval. This is perfect for my use-case, yet from what I understand Dialogflow is outdated.</p>
<p>In contrast the modern &quot;modern&quot; tool called <strong>Actions on Google</strong> or something by that name has this concept a conversational type variable called <a href=""https://developers.google.com/assistant/conversational/types"" rel=""nofollow noreferrer""><code>actions.type.DateTime</code></a></p>
<p>In contrast to Dialogflow, Actions on Google seems highly limited. A screenshot of the system types are below:</p>
<p><a href=""https://i.sstatic.net/vuKBx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vuKBx.png"" alt=""enter image description here"" /></a></p>
<p>What am I missing? Are the tools just so different and I should rather be using Dialogflow? Is &quot;Actions on Google&quot; just so new that I started with the wrong tool? Any way of migrating from &quot;Actions on Google&quot; to Dialogflow, or is this a one way street?</p>
<p>I started coding this in PHP but I see I'm going down a serious rabbit hole and would rather use built-in types than re-invent the wheel. Code snippet:</p>
<pre class=""lang-php prettyprint-override""><code>list($start, $end, $message) = match ($interval) {            
            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'
            =&gt; $this-&gt;processMonth($date, $interval),
            'last-week', 'the-last-week'            
            =&gt; $this-&gt;processWeeks($date, $interval, 1),
            'two-weeks-ago'            
            =&gt; $this-&gt;processWeeks($date, $interval, 1),
            'the-last-2-weeks'            
            =&gt; $this-&gt;processWeeks($date, $interval, 2),
            'last-month', 'the-last-month'
            =&gt; Carbon::createFromFormat('Y-m-d', $date)-&gt;addMonths(1)-&gt;format('Y-m-d'),
            'last-two-months'
            =&gt; Carbon::createFromFormat('Y-m-d', $date)-&gt;addMonths(2)-&gt;format('Y-m-d'),
            default =&gt; now(),
        };
</code></pre>
",Multilingual Language Processing & Language Identification,equivalent google dialogflow using action console struggling come grip difference google natural language processing tool seems dialogflow wa original purchase google appears egging people action console even sure correct statement internet littered many different google tool one never quite know one current recommended perhaps deprecated anyhow concern get date interval example application want say much money spend fuel december implication need start date end date google dialowflow appears fishing system entity date time called match date interval perfect use case yet understand dialogflow outdated contrast modern modern tool called action google something name ha concept conversational type variable called contrast dialogflow action google seems highly limited screenshot system type missing tool different rather using dialogflow action google new started wrong tool way migrating action google dialogflow one way street started coding php see going serious rabbit hole would rather use built type invent wheel code snippet
Tokenizing words in German,"<p>I am trying to determine the subject in a German sentence. For English I used to do:</p>
<pre><code>import spacy
nlp = spacy.load('en')
sent = &quot;I shot an elephant&quot;
doc=nlp(sent)

sub_toks = [tok for tok in doc if (tok.dep_ == &quot;nsubj&quot;) ]

print(sub_toks) 
</code></pre>
<p>But it doesn't work for <code>nlp = spacy.load('de_core_news_sm')</code> and using a German sentence. It returns an empty list.</p>
<p>I tried looking <a href=""https://www.sketchengine.eu/german-stts-part-of-speech-tagset/"" rel=""nofollow noreferrer"">here</a>, even if they have parts of speech instead of subject, object etc. But it returns empty lists too. Is this even possible in German?</p>
",Multilingual Language Processing & Language Identification,tokenizing word german trying determine subject german sentence english used work using german sentence return empty list tried looking even part speech instead subject object etc return empty list even possible german
Finding the shortest citation path between two academic papers?,"<p>I wanted to understand the development of ideas in natural language processing, and I thought one way to understand it was to see how distant the citations are between the major advances e.g. between word2vec and transformers</p>
<p>If the citation distance was long relative to the publication dates, we could surmise that the ideas were independently reached. Otherwise, we could see how major ideas progressed.</p>
<p>Has someone done this already? Or if not, what would be a good approach?</p>
",Multilingual Language Processing & Language Identification,finding shortest citation path two academic paper wanted understand development idea natural language processing thought one way understand wa see distant citation major advance e g word vec transformer citation distance wa long relative publication date could surmise idea independently reached otherwise could see major idea progressed ha someone done already would good approach
Chinese Whispers for NLP How to implement for my corpus file,"<p>I'm new to Python, I want to make this code to implement my corpus from (.csv)files Can anybody help me how to implement it like in the picture below.</p>
<pre class=""lang-py prettyprint-override""><code>from chinese_whispers import __version__ as cw_version
from networkx import __version__ as nx_version
from matplotlib import __version__ as plt_version
print('Chinese Whispers {}'.format(cw_version))
print('NetworkX {}'.format(nx_version))
print('matplotlib {}'.format(plt_version))
import networkx as nx
from chinese_whispers import chinese_whispers, aggregate_clusters
G = nx.karate_club_graph()
# Perform clustering of G, parameters weighting and seed can be omitted
chinese_whispers(G, weighting='top', seed=10000) 

# Print the clusters in the descending order of size
print('ID\tCluster\n')

for label, cluster in sorted(aggregate_clusters(G).items(), key=lambda e: len(e[1]), reverse=True):
    print('{}\t{}\n'.format(label, cluster))
import matplotlib.pyplot as plt
colors = [1. / G.nodes[node]['label'] for node in G.nodes()]

nx.draw_networkx(G, cmap=plt.get_cmap('jet'), node_color=colors, font_color='white')
</code></pre>
<p><a href=""https://i.sstatic.net/F7CnL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F7CnL.jpg"" alt=""wanted output"" /></a></p>
",Multilingual Language Processing & Language Identification,chinese whisper nlp implement corpus file new python want make code implement corpus csv file anybody help implement like picture
"multiple pipelines on the same assistant, depending on language","<p>Is it possible to define two different pipelines and decide which one to use at inference time? (and different models, too)
To keep it simple think to an English-Italian MoodBot. Firstly you detect the language used by the user and based on the result you use the Italian pipeline or the English one. I'd need this because different languages could require different NLU pipelines. I hope it's clear what I mean. Thanks!</p>
",Multilingual Language Processing & Language Identification,multiple pipeline assistant depending language possible define two different pipeline decide one use inference time different model keep simple think english italian moodbot firstly detect language used user based result use italian pipeline english one need different language could require different nlu pipeline hope clear mean thanks
I was looking for how to unload the obtained data into a .txt or csv file,"<p>I was looking for how to unload the obtained data into a .txt or csv file, but I could not find a simple and understandable solution with my level of understanding of this process.</p>
<p>I need to sort words by frequency and highlight the top 100 words. I did it (I know not in the best way, I did everything in the Google collaboratori)</p>
<pre><code>from collections import Counter

Counter(&quot; &quot;.join(test_data['body']).split()).most_common(100)

DATA= Counter(&quot; &quot;.join(test_data['body']).split()).most_common(100)

DATA
</code></pre>
<p>Question:<br />
how to save the result from these top 100 words to a text file .csv or .txt, and possibly an Excel version.(or in three versions at once)</p>
<p>I'm just learning and don't know a lot, trying to figure it out and understand.</p>
<p>Here is a link to the collab, for me the problem is that the words are Russian, and all the practices are for English texts, and it's easier than working with Russian text.
<a href=""https://colab.research.google.com/drive/1LZ3RHPTjTib8lUjzKGcCJgzYnODSjewL?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1LZ3RHPTjTib8lUjzKGcCJgzYnODSjewL?usp=sharing</a></p>
",Multilingual Language Processing & Language Identification,wa looking unload obtained data txt csv file wa looking unload obtained data txt csv file could find simple understandable solution level understanding process need sort word frequency highlight top word know best way everything google collaboratori question save result top word text file csv txt possibly excel version three version learning know lot trying figure understand link collab problem word russian practice english text easier working russian text
Can I make a transformer based chatbot which is pretrained on some other dataset?,"<p>I'm currently making a medical transformer chatbot from the tutorial from this page:</p>
<p><a href=""https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html"" rel=""nofollow noreferrer"">https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html</a></p>
<p>I'm using a text corpus of medical questions/answers. When I train the model and test it, it just gives answers to questions relevant to medical domain. However, I want to create a conversational chatbot which can answer basic questions like 'How are you' and 'I need help'. Is there a way I can us some pretrained weights, then train the model on my medical dataset?
I'm pretty new to natural language processing domain so could really use some guidance. Thanks!</p>
",Multilingual Language Processing & Language Identification,make transformer based chatbot pretrained dataset currently making medical transformer chatbot tutorial page using text corpus medical question answer train model test give answer question relevant medical domain however want create conversational chatbot answer basic question like need help way u pretrained weight train model medical dataset pretty new natural language processing domain could really use guidance thanks
Imposing grammar rules manually on Sequence2Sequence keras model,"<p>I have a fairly standard Sequence to sequence translator in keras, which looks like this:</p>
<pre><code># create model 

encoder_inputs = Input(shape=(None,))
en_x=  Embedding(num_encoder_tokens, EMBEDDING_SIZE)(encoder_inputs)
encoder = LSTM(50, return_state=True)
encoder_outputs, state_h, state_c = encoder(en_x)
# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]


# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None,))
dex=  Embedding(num_decoder_tokens, EMBEDDING_SIZE)
final_dex= dex(decoder_inputs)

decoder_lstm = LSTM(50, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)


model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.05)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])
</code></pre>
<p>I know it isn't a great idea, but the data I am trying to translate is not spoken language and I want to impose further rules on the decoded sequence, which is that
&quot;any word should only occur once in the decoded sequence&quot; among others. The rule does not apply to the sequence being encoded.</p>
<p>The data I am using to train the model does already adhere to this rule, but the current output of the model does not. (I know this rule doesn't really make sense language-wise)</p>
<p>Is there a way to do this, and if so how?</p>
",Multilingual Language Processing & Language Identification,imposing grammar rule manually sequence sequence kera model fairly standard sequence sequence translator kera look like know great idea data trying translate spoken language want impose rule decoded sequence word occur decoded sequence among others rule doe apply sequence encoded data using train model doe already adhere rule current output model doe know rule really make sense language wise way
Not able to log training and validation loss to visualise in tensor-board as tfevents?,"<p>I am downloading the model <a href=""https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/tree/main</a> microsoft/Multilingual-MiniLM-L12-H384 and then using it.</p>
<p>Transformer Version: '4.11.3'</p>
<p>I have written the below code:</p>
<pre><code>def compute_metrics(eval_pred):
    logits, labels = eval_pred
   

    predictions = np.argmax(logits, axis=-1)
    
    acc = np.sum(predictions == labels) / predictions.shape[0]
    
    return {&quot;accuracy&quot; : acc}
</code></pre>
<pre><code>model = tr.BertForSequenceClassification.from_pretrained(&quot;/home/pc/minilm_model&quot;,num_labels=2)
model.to(device)

print(&quot;hello&quot;)

training_args = tr.TrainingArguments(
    output_dir='/home/pc/proj/results2',          # output directory
    num_train_epochs=10,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    learning_rate=2e-5,
    warmup_steps=1000,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=1000,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;no&quot;
)


trainer = tr.Trainer(
    model=model,                         # the instantiated ü§ó Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_data,         # training dataset
    eval_dataset=val_data,             # evaluation dataset
    compute_metrics=compute_metrics
)

</code></pre>
<p>Is there way to retrieve my ( <strong>I  want to use tensor-board</strong>) :</p>
<ol>
<li><strong>Training loss</strong> for every epoch</li>
<li><strong>Validation loss</strong> for every epoch</li>
</ol>
<p>I do not see anything in my log directory apart from model arguments which is empty?</p>
<p><strong>How can I save my Training and validation loss so that tensorboard events captures it.</strong></p>
",Multilingual Language Processing & Language Identification,able log training validation loss visualise tensor board tfevents downloading model microsoft multilingual minilm l h using transformer version written code way retrieve want use tensor board training loss every epoch validation loss every epoch see anything log directory apart model argument empty save training validation loss tensorboard event capture
Model stopped training once I introduced &lt;&lt; report_to = &#39;wandb&#39; &gt;&gt; in TrainingArguments,"<p>I am downloading the model <a href=""https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/tree/main</a> microsoft/Multilingual-MiniLM-L12-H384 and then using it.</p>
<p>Transformer Version: '4.11.3'</p>
<p>I have written the below code:</p>
<pre><code>import wandb
wandb.login()
%env WANDB_LOG_MODEL=true

model = tr.BertForSequenceClassification.from_pretrained(&quot;/home/pc/minilm_model&quot;,num_labels=2)
model.to(device)

print(&quot;hello&quot;)

training_args = tr.TrainingArguments(
report_to = 'wandb',
output_dir='/home/pc/proj/results2', # output directory
num_train_epochs=10, # total number of training epochs
per_device_train_batch_size=16, # batch size per device during training
per_device_eval_batch_size=32, # batch size for evaluation
learning_rate=2e-5,
warmup_steps=1000, # number of warmup steps for learning rate scheduler
weight_decay=0.01, # strength of weight decay
logging_dir='./logs', # directory for storing logs
logging_steps=1000,
evaluation_strategy=&quot;epoch&quot;,
save_strategy=&quot;no&quot;
)

print(&quot;hello&quot;)

trainer = tr.Trainer(
model=model, # the instantiated ü§ó Transformers model to be trained
args=training_args, # training arguments, defined above
train_dataset=train_data, # training dataset
eval_dataset=val_data, # evaluation dataset
compute_metrics=compute_metrics
)

</code></pre>
<p>After Executing this:</p>
<p>The model stuck at this point:</p>
<p>***** Running training *****</p>
<pre><code>Num examples = 12981
 Num Epochs = 20
 Instantaneous batch size per device = 16
 Total train batch size (w. parallel, distributed &amp; accumulation) = 32
 Gradient Accumulation steps = 1
 Total optimization steps = 8120
Automatic Weights &amp; Biases logging enabled, to disable set os.environ[&quot;WANDB_DISABLED&quot;] = &quot;true&quot;

</code></pre>
<p><strong>What could be the possible solution?</strong></p>
",Multilingual Language Processing & Language Identification,model stopped training introduced report wandb trainingarguments downloading model microsoft multilingual minilm l h using transformer version written code executing model stuck point running training could possible solution
How to split arabic documents into multiple sentences,"<p>Is there any method to split arabic documents into multiple sentences. Currently i'm using "".""
for sentence splitting.</p>

<p>I'm looking for better sentence splitting models such as the ones available for english in opennlp.</p>

<p>Please let me know, if you know any packages which does this.</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,split arabic document multiple sentence method split arabic document multiple sentence currently using sentence splitting looking better sentence splitting model one available english opennlp please let know know package doe thanks
How to get the best merger from symspellpy word segmentation of many languages in Python?,"<p>The following code uses SymSpell in Python, see the <a href=""https://symspellpy.readthedocs.io/en/latest/examples/word_segmentation.html"" rel=""nofollow noreferrer"">symspellpy guide on word_segmentation</a>.</p>
<p>It uses &quot;de-100k.txt&quot; and &quot;en-80k.txt&quot; frequency dictionaries from a <a href=""https://github.com/wolfgarbe/SymSpell/tree/master/SymSpell.FrequencyDictionary"" rel=""nofollow noreferrer"">github repo</a>, you need to save them in your working directory. As long as you do not want to use any SymSpell logic, you do not need to install and run this script to answer the question, take just the output of the two language's word segmentations and go on.</p>
<pre class=""lang-py prettyprint-override""><code>import pkg_resources
from symspellpy.symspellpy import SymSpell

input_term = &quot;sonnenempfindlichkeitsunoil farbpalettesuncreme&quot;

# German:
# Set max_dictionary_edit_distance to 0 to avoid spelling correction
sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)
dictionary_path = pkg_resources.resource_filename(
    &quot;symspellpy&quot;, &quot;de-100k.txt&quot;
)
# term_index is the column of the term and count_index is the
# column of the term frequency
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
result = sym_spell.word_segmentation(input_term)
print(f&quot;{result.corrected_string}, {result.distance_sum}, {result.log_prob_sum}&quot;)

# English:
# Reset the sym_spell object
sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)
dictionary_path = pkg_resources.resource_filename(
    &quot;symspellpy&quot;, &quot;en-80k.txt&quot;
)
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
result = sym_spell.word_segmentation(input_term)
print(f&quot;{result.corrected_string}, {result.distance_sum}, {result.log_prob_sum}&quot;)
</code></pre>
<p>Out:</p>
<pre class=""lang-none prettyprint-override""><code>sonnen empfindlichkeit s uno i l farb palette sun creme, 8, -61.741842760725255
sonnen empfindlichkeit sun oil farb palette sun creme, 6, -45.923471400632884
</code></pre>
<p>The aim is to find out the most relevant words by some logic: most frequent ngram neighours and/or word frequency, longest word, and the like. The logic is free of choice.</p>
<p>In this example with two languages, the two outputs need to be compared so that only the best segments are kept while dropping the rest, without interceptions of parts of words. In the outcome, each letter is used one time and uniquely.</p>
<p>If there are spaces between words in the input_term, these words should not be joined to become a new segment. For example, if you have 'cr eme' with a wrong space in it, that should still not be allowed to become 'creme'. It is just likely that the space is right more often than the errors that would appear from taking neighoured letters.</p>
<pre class=""lang-none prettyprint-override""><code>array('sonnen', 'empfindlichkeit', 'sun', 'oil', 'farb', 'palette', 'sun', 'creme')
array(['DE'], ['DE'], ['EN'], ['EN'], ['DE'], ['DE', 'EN'], ['EN'], ['DE', 'EN'])
</code></pre>
<p>The 'DE/EN' tag is just an optional idea to show that the word exists in German and English, you can also choose 'EN' over 'DE' in this example. The language tags are a  bonus, you can also answer without that.</p>
<p>There is probably a fast solution that uses <code>numpy</code> arrays and/or <code>dictionaries</code> instead of <code>lists</code> or <code>Dataframes</code>, but choose as you like.</p>
<p><em>How to use many languages in symspell word segmentation and combine them to one chosen merger? The aim is a sentence of words built from all letters, using each letter once, keeping all original spaces.</em></p>
",Multilingual Language Processing & Language Identification,get best merger symspellpy word segmentation many language python following code us symspell python see symspellpy guide word segmentation us de k txt en k txt frequency dictionary github repo need save working directory long want use symspell logic need install run script answer question take output two language word segmentation go aim find relevant word logic frequent ngram neighours word frequency longest word like logic free choice example two language two output need compared best segment kept dropping rest without interception part word outcome letter used one time space word input term word joined become new segment example cr eme wrong space still allowed become creme likely space right often error would appear taking neighoured letter de en tag optional idea show word exists german english also choose en de example language tag bonus also answer without probably fast solution us array instead choose like use many language symspell word segmentation combine one chosen merger aim sentence word built letter using letter keeping original space
Python Convert Column to Row Issue,"<p><a href=""https://i.sstatic.net/S7gi9.png"" rel=""nofollow noreferrer"">  Shape1 &amp;&amp; Shape2 </a></p>
<p>Hi, I'm Trying to processing my dataframe but I have some issues to solve. I have a dataframe like shape 1 and I want to turn it as shape 2. Basicly, I want to change tweets column as row and if there were two or more tweets which are in same days, insert them as a one row next to time section with seperated columns also I want to calculate tweet's polarity and subjectivity which are tweeted in same days.
I'm a beginner of python and I couldn't find solution about it I would be preciated if you guys help me about it.
If there were any mistakes of english sorry for it basicly I want to change My dataframe (shape1) to shape2</p>
",Multilingual Language Processing & Language Identification,python convert column row issue shape shape hi trying processing dataframe issue solve dataframe like shape want turn shape basicly want change tweet column row two tweet day insert one row next time section seperated column also want calculate tweet polarity subjectivity tweeted day beginner python find solution would preciated guy help mistake english sorry basicly want change dataframe shape shape
Google Translate Python package not working after X calls?,"<p>so I am translating almost 1755 English sentences (short ones each are less than 10 words). The code below works fine.</p>
<p><strong>Problem faced:</strong> however, after translating almost 500 rows (sentences) in my data frame, it stops translating (without getting an error), and 'newLanguage' is the same as the original sentence. (I tried Italian 'it' instead of Arabic also faced the same problem).</p>
<p>Do I have a limit to the # of API calls to translate? Any ideas how to fix this ?</p>
<pre><code>!pip install googletrans==3.1.0a0


from googletrans import Translator

translator = Translator()
backTrans_sentences=[]
backTrans_labels=[]

for sentence,label in zip(df_en_train['Sentence'],df_en_train['Labels']):
        newLanguage= translator.translate(text=sentence, dest='ar').text
        eng=translator.translate(text=newLanguage, dest='en').text
        backTrans_sentences.append(eng)
        backTrans_labels.append(label)
   
</code></pre>
",Multilingual Language Processing & Language Identification,google translate python package working x call translating almost english sentence short one le word code work fine problem faced however translating almost row sentence data frame stop translating without getting error newlanguage original sentence tried italian instead arabic also faced problem limit api call translate idea fix
&#39;TranslateR&#39; R package is not showing the results of the automated translation,"<p>When using the function <code>translate</code> from the <code>translateR</code> package, the function keeps the source language, i.e. no translation is occuring.</p>
<pre><code>library(translateR)
data(enron)
translate(dataset = enron, content.field = 'email', 
                            google.api.key = mygooglekey, source.lang = 'en', 
                            target.lang = 'de')
</code></pre>
<p>What seems to be the problem? are there any alternatives for doing automated translation in R?</p>
",Multilingual Language Processing & Language Identification,translater r package showing result automated translation using function package function keep source language e translation occuring seems problem alternative automated translation r
Removing all English and other punctuation form the text file in Jupyter,"<p>I have a text file I wanted to work on some NLP task. But I am processing for Local language. That file contains lots of English words and Punctuation marks. I wanted to get rid of all the Latin and other punctuation from that text file. How this is possible using Jupyter notebook
TIA</p>
",Multilingual Language Processing & Language Identification,removing english punctuation form text file jupyter text file wanted work nlp task processing local language file contains lot english word punctuation mark wanted get rid latin punctuation text file possible using jupyter notebook tia
"OSError: Java command failed for corenlp in python to get text POS tagging, segmentation, parsing","<p>I am trying to get POS tags for Arabic language using NLTK and stanford coreNLP using Python. the problem that I am facing is</p>
<pre><code>OSError: Java command failed : ['C:/Program Files/Java/jdk-17.0.1/bin/java.exe', '-mx1000m', '-cp', 'C:/Users/USER/Desktop/ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿßŸÑÿßŸÑŸÖÿßŸÜŸäÿ©/CoreNLP/stanford-tagger-4.2.0/stanford-postagger-full-2020-11-17/stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'C:/Users/USER/Desktop/ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿßŸÑÿßŸÑŸÖÿßŸÜŸäÿ©/CoreNLP/stanford-tagger-4.2.0/stanford-postagger-full-2020-11-17/models/arabic.tagger', '-textFile', 'C:\\Users\\USER\\AppData\\Local\\Temp\\tmpk9yh0phm', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf-8']
</code></pre>
<p>I am trying to understand the error where everything in my code is clear and the paths are correct.
here is the code</p>
<pre><code>import nltk
from nltk import *
from nltk.tag.stanford import StanfordPOSTagger
from nltk.tokenize import word_tokenize

os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk-17.0.1/bin/java.exe'

jar = &quot;C:/Users/USER/Desktop/ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿßŸÑÿßŸÑŸÖÿßŸÜŸäÿ©/CoreNLP/stanford-tagger-4.2.0/stanford-postagger-full-2020-11-17/stanford-postagger.jar&quot;
model = &quot;C:/Users/USER/Desktop/ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿßŸÑÿßŸÑŸÖÿßŸÜŸäÿ©/CoreNLP/stanford-tagger-4.2.0/stanford-postagger-full-2020-11-17/models/arabic.tagger&quot;

pos_tagger = StanfordPOSTagger(model, jar, encoding = &quot;utf-8&quot;)

text = open(&quot;C:/Users/USER/Desktop/test.txt&quot;, encoding=&quot;utf8&quot;).read()
 
words = nltk.word_tokenize(text)
tagged_words = pos_tagger.tag(words)
print(tagged_words)

</code></pre>
<p>can you help me in this please? I have spent a long time to solve it but it still gives me the same error weather for text POS tagging, text segmentation, and text Parsing.<br />
Thank you</p>
",Multilingual Language Processing & Language Identification,oserror java command failed corenlp python get text po tagging segmentation parsing trying get po tag arabic language using nltk stanford corenlp using python problem facing trying understand error everything code clear path correct code help please spent long time solve still give error weather text po tagging text segmentation text parsing thank
How to interpret logit score from Hugging face binary classification model and convert it to probability sore,"<p>I am downloading the model <a href=""https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/tree/main</a> microsoft/Multilingual-MiniLM-L12-H384 and then using it. I am loading model using <strong>BertForSequenceClassification</strong></p>
<p><a href=""https://huggingface.co/docs/transformers/model_doc/bert#:%7E:text=sentence%20was%20random-,BertForSequenceClassification,-class%20transformers.BertForSequenceClassification"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/bert#:~:text=sentence%20was%20random-,BertForSequenceClassification,-class%20transformers.BertForSequenceClassification</a></p>
<p>Transformer Version: '4.11.3'</p>
<p>I have written the below code:</p>
<pre><code>def compute_metrics(eval_pred):
    logits, labels = eval_pred
   

    predictions = np.argmax(logits, axis=-1)
    
    acc = np.sum(predictions == labels) / predictions.shape[0]
    return {&quot;accuracy&quot; : acc}

model = tr.BertForSequenceClassification.from_pretrained(&quot;/home/pc/minilm_model&quot;,num_labels=2)
model.to(device)

print(&quot;hello&quot;)

training_args = tr.TrainingArguments(
    output_dir='/home/pc/proj/results2',          # output directory
    num_train_epochs=10,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    learning_rate=2e-5,
    warmup_steps=1000,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=1000,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;no&quot;
)



trainer = tr.Trainer(
    model=model,                         # the instantiated ü§ó Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_data,         # training dataset
    eval_dataset=val_data,             # evaluation dataset
    compute_metrics=compute_metrics
)
</code></pre>
<p>The folder is empty after I train the model.</p>
<p><strong>Is it okay to pass classes=2 for binary classification?</strong></p>
<p><strong>The model last layer is simple linear connection which gives logits value. How to get its interpretation and probability score out of it? Does logit score is directly proportional to probability.?</strong></p>
<pre><code>model = tr.BertForSequenceClassification.from_pretrained(&quot;/home/pchhapolika/minilm_model&quot;,num_labels=2)
</code></pre>
",Multilingual Language Processing & Language Identification,interpret logit score hugging face binary classification model convert probability sore downloading model microsoft multilingual minilm l h using loading model using bertforsequenceclassification transformer version written code folder empty train model okay pas class binary classification model last layer simple linear connection give logits value get interpretation probability score doe logit score directly proportional probability
Hunspell python detects words as english only if first letter is capitalised,"<p>For example &quot;India&quot; is detected as true but &quot;india&quot; is false</p>
<pre><code>d = Hunspell()
d.spell('india')
Out[59]: False

d.spell('India')
Out[60]: True
</code></pre>
<p>How do i get hunspell to detect both as true</p>
",Multilingual Language Processing & Language Identification,hunspell python detects word english first letter capitalised example india detected true india false get hunspell detect true
How to Iterate the tokenized pytorch Multi30k dataset in BucketIterator?,"<p>I am using Pytorch(1.10 v), I am using Multi30k German to English dataset for machine translation. I am using spacy for tokenization (both for English and German words) and wanna pass the tokenized data to (<strong>torchtext.legacy.data.BucketIterator</strong>) for padding and for converting string to index. Some error is occurring related to sort_key, I am not getting it. Somebody, please help me out.</p>
<p>The code</p>
<pre><code>import spacy
from torchtext.datasets import Multi30k # this is a en and gr dataset for machine translation
from torchtext.legacy.data import Field, BucketIterator

spacy_eng = spacy.load(&quot;en_core_web_sm&quot;)
spacy_ger = spacy.load(&quot;de_core_news_sm&quot;)

def tokenize_eng(text):
    return [tok.text for tok in spacy_eng.tokenizer(text)]

def tokenize_ger(text):
    return [tok.text for tok in spacy_ger.tokenizer(text)]

english = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, lower=True, init_token='&lt;sos&gt;', eos_token='&lt;eos&gt;')
german = Field(sequential=True, use_vocab=True, tokenize=tokenize_ger, lower=True, init_token='&lt;sos&gt;', eos_token='&lt;eos&gt;')

train, valid, test = Multi30k(root=&quot;.data&quot;, split=('train', 'valid', 'test'), language_pair=('en', 'de'))

# will make vocabulary from train data
english.build_vocab(train, max_size=10000, min_freq=2)
german.build_vocab(train, max_size=10000, min_freq=2)


train_data, valid_data, test_data = BucketIterator.splits((train, valid, test),
                                                          batch_size=64,
                                                          device='cuda')
</code></pre>
<p>Error</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\Torch\Multi30K_inbuilt_dataset.py&quot;, line 28, in &lt;module&gt;
    train_data, valid_data, test_data = BucketIterator.splits((train, valid, test),
  File &quot;C:\Users\Devanshu\anaconda3\envs\deeplearning\lib\site-packages\torchtext\legacy\data\iterator.py&quot;, line 99, in splits
    ret.append(cls(
  File &quot;C:\Users\Devanshu\anaconda3\envs\deeplearning\lib\site-packages\torchtext\legacy\data\iterator.py&quot;, line 59, in __init__
    self.sort_key = dataset.sort_key
  File &quot;C:\Users\Devanshu\anaconda3\envs\deeplearning\lib\site-packages\torch\utils\data\dataset.py&quot;, line 226, in __getattr__
    raise AttributeError
AttributeError
</code></pre>
",Multilingual Language Processing & Language Identification,iterate tokenized pytorch multi k dataset bucketiterator using pytorch v using multi k german english dataset machine translation using spacy tokenization english german word wan na pas tokenized data torchtext legacy data bucketiterator padding converting string index error occurring related sort key getting somebody please help code error
Python to detect dipthongs in English,"<p>Is there a Python library that can detect/highlight all the dipthongs (in normal spelling not IPA) in a given English text?</p>
",Multilingual Language Processing & Language Identification,python detect dipthongs english python library detect highlight dipthongs normal spelling ipa given english text
NLP model - Arabic diacritized_text [Errno 22] Invalid argument,"<p>i am trying to read  diacritized_text  from pdf file by using this code:</p>
<pre><code>import PyPDF2 
import pdfplumber.utils
import pdfminer.pdftypes
import arabic_reshaper
from pdfplumber.pdf import PDF
from bidi.algorithm import get_display
from PyPDF2 import PdfFileReader, PdfFileWriter
#import pyPdf
import codecs
input_filepath = &quot;D:\Arabic research\input.pdf&quot; file path
output_filepath = &quot;D:\Arabic research\output.txt&quot;#output text file path
output_file = open(r&quot;D:\Arabic research\output.txt&quot;, &quot;wb&quot;)#open output file
pdf = PyPDF2.PdfFileReader(codecs.open(r&quot;D:\Arabic research\input.pdf&quot;, &quot;rb&quot;, encoding='utf-8'))#read PDF
for page in PyPDF2.pages:#loop through pages
    page_text = page.extractText()#get text from page
    page_text = page_text.decode(encoding='utf-8')#decode 
    print(page_text)
    output_file.write(page_text)#write to file
output_file.close()#close
</code></pre>
<p>but i have the following error:[Errno 22] Invalid argument</p>
",Multilingual Language Processing & Language Identification,nlp model arabic diacritized text errno invalid argument trying read diacritized text pdf file using code following error errno invalid argument
Apache Open NLP vs NLTK,"<p>We have a spring boot application integrated with Node.js and socket.io chat application , to which we want to integrate Natural language processing. Not getting any direction on which of these two <strong><em><code>Apache-OpenNlp or NLTK</code></em></strong> would be a better choice for us as both of the frameworks offer the kind of processing we need. </p>

<p>Wrt to the features provided by the frameworks , they both are good. Both have features that we are looking for. More than how to choose between features , what would suit our architecture better is a perspective I would like.. </p>

<p>Any suggestions ? </p>
",Multilingual Language Processing & Language Identification,apache open nlp v nltk spring boot application integrated node j socket io chat application want integrate natural language processing getting direction two would better choice u framework offer kind processing need wrt feature provided framework good feature looking choose feature would suit architecture better perspective would like suggestion
NLP translation giving me sentence translations instead of word translation,"<p>I trained a Transformers using a Portuguese-English dataset from <a href=""http://www.manythings.org/anki/"" rel=""nofollow noreferrer"">http://www.manythings.org/anki/</a>. This is a parallelized sentence dataset.</p>
<p>After training, I tried translating the word &quot;doente&quot; which should've translated to &quot;sick&quot; but it instead I got &quot;I feel sick&quot;.</p>
<p>Any ideas of how do I get just the word sick ?</p>
<p>Am I training my model with the wrong dataset ? sentence based instead of word based ?</p>
<p>tks in advance</p>
",Multilingual Language Processing & Language Identification,nlp translation giving sentence translation instead word translation trained transformer using portuguese english dataset parallelized sentence dataset training tried translating word doente translated sick instead got feel sick idea get word sick training model wrong dataset sentence based instead word based tks advance
Upload a pre-trained spanish language word vectors and then retrain it with custom sentences? (GENSIM -FASTTEXT),"<p>I am trying to upload a pre-trained spanish language word vectors and then retrain it with custom sentences:</p>
<pre><code>!pip install fasttext
import fasttext
import fasttext.util
#download pre-trained spanish language word vectors c
fasttext.util.download_model('es', if_exists='ignore')  # Spanish
ft = fasttext.load_model('cc.es.300.bin')
</code></pre>
<p>but once I try to update the vocabulary it gives me this AttributeError:</p>
<pre><code>ft.build_vocab(sentences, update=True)
AttributeError: '_FastText' object has no attribute 'build_vocab'
</code></pre>
<p>Any advices?</p>
",Multilingual Language Processing & Language Identification,upload pre trained spanish language word vector retrain custom sentence gensim fasttext trying upload pre trained spanish language word vector retrain custom sentence try update vocabulary give attributeerror advice
"list of english verbs and their tenses, various forms, etc","<p>Is there a huge CSV/XML or whatever file somewhere that contains a list of english verbs and their variations (e.g sell -> sold, sale, selling, seller, sellee)?</p>

<p>I imagine this will be useful for NLP systems, but there doesn't seem to be a listing anywhere, or it could be my terrible googling skills. Does anybody have a clue otherwise?</p>
",Multilingual Language Processing & Language Identification,list english verb tense various form etc huge csv xml whatever file somewhere contains list english verb variation e g sell sold sale selling seller sellee imagine useful nlp system seem listing anywhere could terrible googling skill doe anybody clue otherwise
Getting synsets of custom hungarian wordnet dictionary with nltk,"<p>I am very new to NLP and I might be doing something wrong.</p>
<p>I would like to work with a hungarian text where I can get the synset/hyponym/hypernym of some selected words. I am working in python.</p>
<p>As Open Multilingual Wordnet does not have hungarian wordnet dictionary I have downloaded one from this github site: <a href=""https://github.com/mmihaltz/huwn"" rel=""nofollow noreferrer"">https://github.com/mmihaltz/huwn</a></p>
<p>As it is an xml file I have converted it to .tab with a converter available in other language folders.</p>
<p>So at this stage I created the '\nltk_data\corpora\omw\hun' library and placed my new wn-data-hun.tab inside this directory.</p>
<p>But unfortunately it is not working</p>
<p>After importing nltk and wordnet the <code>wn.langs()</code> command shows the 'hun' also as available language.</p>
<p>However trying:  <code>wn.lemmas('cane', lang='hun')</code> command is showing an empty list. Trying with other languages (built in languages in open multilanguage wordnet), it works.</p>
<p>Could you pleaes help me or point me in the right direction in order to make it work?</p>
<p>Thank you in advance!</p>
<p>Attached hungarian .tab file: <a href=""https://drive.google.com/file/d/1LSntOxAnSDijBl_TdZGn8YTUgdAOuHfx/view?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
<p>Hungarian text:</p>
<blockquote>
<p>A sz√∂veg megfelel≈ëje gyakorlatilag az √∂sszes eur√≥pai nyelvben &quot;Text&quot;
(k√ºl√∂nb√∂z≈ë √≠r√°sk√©pekkel a nemzeti helyes√≠r√°s miatt), ami a latin
&quot;textum&quot; sz√≥b√≥l ered, amely sz√≥ eredeti jelent√©se: sz√∂vet, sz√∂veg. A
magyarban a nyelv√∫j√≠t√°s idej√©n a jelent√©st magyar sz√≥val jel√∂lt√ºk. A
sz√∂veg egy √∂sszef√ºgg≈ë √©s a k√∂rnyezet√©t≈ël j√≥l elhat√°rolt vagy
elhat√°rolhat√≥ megnyilv√°nul√°s, kijelent√©s √≠rott vagy t√°gabb √©rtelemben
nem √≠rott de (le)√≠rhat√≥ nyelven. A nem felt√©tlen√ºl √≠rott, de le√≠rhat√≥
sz√∂vegre p√©lda a dalsz√∂veg, egy film sz√∂vege vagy improviz√°lt sz√≠nh√°zi
sz√∂veg.</p>
</blockquote>
<p>The problem is that in case of hungarian language, it does not find anything but in case of french it finds. See below:</p>
<p><a href=""https://i.sstatic.net/imV7z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/imV7z.png"" alt=""enter image description here"" /></a></p>
",Multilingual Language Processing & Language Identification,getting synset custom hungarian wordnet dictionary nltk new nlp might something wrong would like work hungarian text get synset hyponym hypernym selected word working python open multilingual wordnet doe hungarian wordnet dictionary downloaded one github site xml file converted tab converter available language folder stage created nltk data corpus omw hun library placed new wn data hun tab inside directory unfortunately working importing nltk wordnet command show hun also available language however trying command showing empty list trying language built language open multilanguage wordnet work could pleaes help point right direction order make work thank advance attached hungarian tab file hungarian text sz veg megfelel je gyakorlatilag az sszes eur pai nyelvben text k l nb z r sk pekkel nemzeti helyes r miatt ami latin textum sz b l ered amely sz eredeti jelent se sz vet sz veg magyarban nyelv j idej n jelent st magyar sz val jel lt k sz veg egy sszef gg k rnyezet l j l elhat rolt vagy elhat rolhat megnyilv nul kijelent rott vagy gabb rtelemben nem rott de le rhat nyelven nem felt tlen l rott de le rhat sz vegre p lda dalsz veg egy film sz vege vagy improviz lt sz nh zi sz veg problem case hungarian language doe find anything case french find see
Python NLP British English vs American English,"<p>I'm currently working on NLP in python. However, in my corpus, there are both British and American English(realize/realise) I'm thinking to convert British to American. However, I did not find a good tool/package to do that. Any suggestions?</p>
",Multilingual Language Processing & Language Identification,python nlp british english v american english currently working nlp python however corpus british american english realize realise thinking convert british american however find good tool package suggestion
Python - How to loop through each index position in a list?,"<p>Given a list <code>[[[&quot;source1&quot;], [&quot;target1&quot;], [&quot;alignment1&quot;]], [&quot;source2&quot;], [&quot;target2&quot;], [&quot;alignment2&quot;]], ...]</code> , I want to extract the words in the source that align with the words in the target.
For example, in the English-German sentence pair <em>The hat is on the table . - Der Hut liegt auf dem Tisch .</em>, I want to print the following:</p>
<pre><code>The - Der
hat - Hut
is - liegt
on - auf
the - dem
table - Tisch
. - . 
</code></pre>
<p>So I have written the following:</p>
<pre><code>en_de = [
[['The', 'hat', 'is', 'on', 'the', 'table', '.'], ['Der', 'Hut', 'liegt', 'auf', 'dem', 'Tisch', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6'], 
[['The', 'picture', 'is', 'on', 'the', 'wall', '.'], ['Das', 'Bild', 'h√§ngt', 'an', 'der', 'Wand', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6'], 
[['The', 'bottle', 'is', 'under', 'the', 'sink', '.'], ['Die', 'Flasche', 'ist', 'under', 'dem', 'Waschbecken', '.'], '0-0 1-1 2-2 3-3 4-4 5-5 6-6']
]

for group in en_de:
    src_sent = group[0]
    tgt_sent = group[1]
    aligns = group[2]

    split_aligns = aligns.split()

    hyphen_split = [align.split(&quot;-&quot;) for align in split_aligns]

    align_index = hyphen_split[0]

    print(src_sent[int(align_index[0])],&quot;-&quot;, tgt_sent[int(align_index[1])])
</code></pre>
<p>This prints, as expected, the words in index position 0 of <code>src_sent</code> and <code>tgt_sent</code>:</p>
<pre><code>The - Der
The - Das
The - Die
</code></pre>
<p>Now, I don't know how I can print the words of all index positions of <code>src_sent</code> and <code>tgt_sent</code>. Obviously, I could manually update <code>align_index</code> to a new index position for each position in the sentence pair, but on the full dataset, some sentences will have up to 25 index positions.
Is there a way to possibly for-loop through each index position?
When I try:</p>
<pre><code>align_index = hyphen_split[0:]
print(src_sent[int(align_index[0])],&quot;-&quot;, tgt_sent[int(align_index[1])])
</code></pre>
<p>I get a <code>TypeError:  int() argument must be a string, a bytes-like object or a number, not 'list'</code>
It's clear that <code>align_index</code> can't be a list, but I'm not sure how to convert it into something that will do what I want it to do.
Any advice or help would be greatly appreciated. Thank you in advance.</p>
",Multilingual Language Processing & Language Identification,python loop index position list given list want extract word source align word target example english german sentence pair hat table der hut liegt auf dem tisch want print following written following print expected word index position know print word index position obviously could manually update new index position position sentence pair full dataset sentence index position way possibly loop index position try get clear list sure convert something want advice help would greatly appreciated thank advance
Why won&#39;t my program filter out stop words and punctuation as I programmed it to do? (Python &amp; NLTK),"<p>for a lab in my Data Science course I had to create a program in Python using NLTK for natural language processing. We have to use a for loop to iterate over each word of macbeth and filter out all English stop words and punctuation by adding non-stop word/punctuation words to another list. Then, we have to print out a list of the most common words and their frequencies from that filtered list. I had thought that I had done everything correct logically, but the results include punctuation and stop words (see below). What am I doing wrong here? (P.S. this is my first time using NLTK).</p>
<p><strong>Program:</strong></p>
<pre><code># import required libraries and modules
import nltk
from nltk.corpus import gutenberg, stopwords
from nltk.probability import FreqDist

macbeth_allwords = gutenberg.words('shakespeare-macbeth.txt') # read in words from macbeth
macbeth_noStop = [] # empty list to hold words from macbeth excluding stopwords
punctuations = [&quot;.&quot;, &quot;!&quot;, &quot;?&quot;, &quot;,&quot;, &quot;;&quot;, &quot;:&quot;, &quot;-&quot;, &quot;[&quot;, &quot;]&quot;, &quot;{&quot;, &quot;}&quot;, &quot;(&quot;, &quot;)&quot;, &quot;/&quot;, &quot;*&quot;, &quot;~&quot;,
&quot;&lt;&quot;, &quot;&gt;&quot;, &quot;`&quot;, &quot;^&quot;, &quot;_&quot;, &quot;|&quot;, &quot;#&quot;, &quot;$&quot;, &quot;%&quot;, &quot;+&quot;, &quot;=&quot;, &quot;&amp;&quot;, &quot;@&quot;, &quot; &quot;] # list of common punctuation characters

# iterate through each word in macbeth, making a new list excluding all the stopwords and punctuation characters
for word in macbeth_allwords:
    if (word not in stopwords.words('english')) or (word not in punctuations):
        macbeth_noStop.append(word)

macbeth_freq = FreqDist(macbeth_noStop) # get word frequencies from the filtered list of words from macbeth

# print the 50 most common words from the filtered list of words from macbeth
print(&quot;50 Most Common Words in Macbeth (no stopwords or punctuation):&quot;)
print(&quot;-----------------------------------------------&quot;)
print(macbeth_freq.most_common(50))
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>50 Most Common Words in Macbeth (no stopwords or punctuation):
-----------------------------------------------
[(',', 1962), ('.', 1235), (&quot;'&quot;, 637), ('the', 531), (':', 477), ('and', 376), ('I', 333), ('of', 315), ('to', 311), ('?', 241), ('d', 224), ('a', 214), ('you', 184), ('in', 173), ('my', 170), ('And', 170), ('is', 166), ('that', 158), ('not', 155), ('it', 138), ('Macb', 137), ('with', 134), ('s', 131), ('his', 129), ('be', 124), ('The', 118), ('haue', 117), ('me', 111), ('your', 110), ('our', 103), ('-', 100), ('him', 90), ('for', 82), ('Enter', 80), ('That', 80), ('this', 79), ('he', 76), ('What', 74), ('To', 73), ('so', 70), ('all', 67), ('thou', 63), ('are', 63), ('will', 62), ('Macbeth', 61), ('thee', 61), ('but', 60), ('But', 60), ('on', 59), ('they', 58)]
</code></pre>
",Multilingual Language Processing & Language Identification,program filter stop word punctuation programmed python nltk lab data science course create program python using nltk natural language processing use loop iterate word macbeth filter english stop word punctuation adding non stop word punctuation word another list print list common word frequency filtered list thought done everything correct logically result include punctuation stop word see wrong p first time using nltk program output
Tokenisation with Spacy - how to get left and right tokens,"<p>I am using Spacy for text tokenization and getting stuck with it:</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_sm"")
mytext = ""This is some sentence that spacy will not appreciate""
doc = nlp(mytext)

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
</code></pre>

<p>returns something that seems to me to say that tokenisation was succesful: </p>

<pre><code>This this DET DT nsubj Xxxx True False 
is be VERB VBZ ROOT xx True True 
some some DET DT det xxxx True True 
sentence sentence NOUN NN attr xxxx True False 
that that ADP IN mark xxxx True True 
spacy spacy NOUN NN nsubj xxxx True False 
will will VERB MD aux xxxx True True 
not not ADV RB neg xxx True True 
appreciate appreciate VERB VB ccomp xxxx True False
</code></pre>

<p>but on the other hand</p>

<pre><code>[token.text for token in doc[2].lefts]
</code></pre>

<p>returns an empty list. Is there a bug in <code>lefts/rights</code>?</p>

<p>Beginner at natural language processing, hope I am not falling into a conceptual trap. Using Spacy v'2.0.4'.</p>
",Multilingual Language Processing & Language Identification,tokenisation spacy get left right token using spacy text tokenization getting stuck return something seems say tokenisation wa succesful hand return empty list bug beginner natural language processing hope falling conceptual trap using spacy v
"cleaning a sentence from numbers, signs and other languages","<p>I have a txt file that contain Japanese sentences. I would like to remove all non Japanese words. Such as numbers, English alphabets or any other non Japanese language, signs, symbols. Is there a quick way to do it? Thanks</p>
<pre><code>Hi !„Åì„Çì„Å´„Å°„ÅØ„ÄÅÁßÅ„ÅÆÁµ¶Êñô„ÅØÊúàÈ°ç10000„Éâ„É´„Åß„Åô„ÄÇ XO XO
ÁßÅ„ÅØ„ÅÇ„Å™„Åü„ÅÆÊñôÁêÜ„ÅåÂ§ßÂ•Ω„Åç„Åß„Åô
ÁßÅ„ÅÆ„Éï„É©„Ç§„Éà„ÅØAPX1999„Åß„Åô„ÄÇ
ÁßÅ„ÅØ„Çµ„ÉÉ„Ç´„Éº„ÅÆË©¶Âêà„ÇíË¶ã„Çã„ÅÆ„ÅåÂ§ßÂ•Ω„Åç„Åß„Åô„ÄÇ
</code></pre>
<p>Words to remove :
Hi !
XO XO
10000
APX1999</p>
",Multilingual Language Processing & Language Identification,cleaning sentence number sign language txt file contain japanese sentence would like remove non japanese word number english alphabet non japanese language sign symbol quick way thanks word remove hi xo xo apx
Installing spacy returns &#39;set_default_tensor_type&#39; error,"<p>In a Jupyter Notebook, using Python 3.9.9</p>
<p>I went to <a href=""https://spacy.io/usage"" rel=""nofollow noreferrer"">https://spacy.io/usage</a> and followed the instructions for installing Spacy</p>
<p>‚Ä¢ MacOS/OSX</p>
<p>‚Ä¢ conda</p>
<p>‚Ä¢ virtual env</p>
<p>‚Ä¢ English</p>
<p>‚Ä¢ efficiency</p>
<pre class=""lang-py prettyprint-override""><code>!python -m venv .env
</code></pre>
<p>my virtual environment's name is firstEnv</p>
<pre class=""lang-py prettyprint-override""><code>!source .env/bin/activate
</code></pre>
<pre class=""lang-py prettyprint-override""><code>!conda install -c conda-forge spacy
</code></pre>
<pre class=""lang-py prettyprint-override""><code>python -m spacy download en_core_web_sm
</code></pre>
<p>I then import spacy</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
</code></pre>
<p>spacy version is 3.2.0</p>
<p>Instantiate the class</p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)
</code></pre>
<p>Returns error</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/dp/5k1wgpbj6d72lnbdgvwv16l40000gn/T/ipykernel_41554/3909579629.py in &lt;module&gt;
----&gt; 1 nlp = spacy.load(&quot;en_core_web_sm&quot;)

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/__init__.py in load(name, vocab, disable, exclude, config)
     49     RETURNS (Language): The loaded nlp object.
     50     &quot;&quot;&quot;
---&gt; 51     return util.load_model(
     52         name, vocab=vocab, disable=disable, exclude=exclude, config=config
     53     )

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/util.py in load_model(name, vocab, disable, exclude, config)
    418             return get_lang_class(name.replace(&quot;blank:&quot;, &quot;&quot;))()
    419         if is_package(name):  # installed as package
--&gt; 420             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]
    421         if Path(name).exists():  # path to model data directory
    422             return load_model_from_path(Path(name), **kwargs)  # type: ignore[arg-type]

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/util.py in load_model_from_package(name, vocab, disable, exclude, config)
    451     &quot;&quot;&quot;
    452     cls = importlib.import_module(name)
--&gt; 453     return cls.load(vocab=vocab, disable=disable, exclude=exclude, config=config)  # type: ignore[attr-defined]
    454 
    455 

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/en_core_web_sm/__init__.py in load(**overrides)
      8 
      9 def load(**overrides):
---&gt; 10     return load_model_from_init_py(__file__, **overrides)

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config)
    613     if not model_path.exists():
    614         raise IOError(Errors.E052.format(path=data_path))
--&gt; 615     return load_model_from_path(
    616         data_path,
    617         vocab=vocab,

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config)
    486     overrides = dict_to_dot(config)
    487     config = load_config(config_path, overrides=overrides)
--&gt; 488     nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)
    489     return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)
    490 

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/util.py in load_model_from_config(config, vocab, disable, exclude, auto_fill, validate)
    523     # registry, including custom subclasses provided via entry points
    524     lang_cls = get_lang_class(nlp_config[&quot;lang&quot;])
--&gt; 525     nlp = lang_cls.from_config(
    526         config,
    527         vocab=vocab,

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/language.py in from_config(cls, config, vocab, disable, exclude, meta, auto_fill, validate)
   1753         # then we would load them twice at runtime: once when we make from config,
   1754         # and then again when we load from disk.
-&gt; 1755         nlp = lang_cls(vocab=vocab, create_tokenizer=create_tokenizer, meta=meta)
   1756         if after_creation is not None:
   1757             nlp = after_creation(nlp)

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/language.py in __init__(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)
    174         if vocab is True:
    175             vectors_name = meta.get(&quot;vectors&quot;, {}).get(&quot;name&quot;)
--&gt; 176             vocab = create_vocab(self.lang, self.Defaults, vectors_name=vectors_name)
    177         else:
    178             if (self.lang and vocab.lang) and (self.lang != vocab.lang):

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/vocab.pyx in spacy.vocab.create_vocab()

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/vocab.pyx in spacy.vocab.Vocab.__init__()

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/spacy/vectors.pyx in spacy.vectors.Vectors.__init__()

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/thinc/backends/__init__.py in get_current_ops()
    124     &quot;&quot;&quot;Get the current backend object.&quot;&quot;&quot;
    125     if context_ops.get() is None:
--&gt; 126         require_cpu()
    127     return cast(Ops, context_ops.get())
    128 

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/thinc/util.py in require_cpu()
    164 
    165     ops = get_ops(&quot;cpu&quot;)
--&gt; 166     set_current_ops(ops)
    167     set_torch_tensor_type_for_ops(ops)
    168 

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/thinc/backends/__init__.py in set_current_ops(ops)
    132     context_ops.set(ops)
    133     _get_thread_state().ops = ops
--&gt; 134     set_torch_tensor_type_for_ops(ops)
    135 
    136 

/opt/anaconda3/envs/firstEnv/lib/python3.9/site-packages/thinc/util.py in set_torch_tensor_type_for_ops(ops)
    487             torch.set_default_tensor_type(&quot;torch.cuda.FloatTensor&quot;)
    488         else:
--&gt; 489             torch.set_default_tensor_type(&quot;torch.FloatTensor&quot;)
    490     except ImportError:
    491         pass

AttributeError: module 'torch' has no attribute 'set_default_tensor_type'

‚Äã```
</code></pre>
",Multilingual Language Processing & Language Identification,installing spacy return set default tensor type error jupyter notebook using python went followed instruction installing spacy macos osx conda virtual env english efficiency virtual environment name firstenv import spacy spacy version instantiate class return error
Finding similarity between Arabic text,"<p>I'm currently working on an Oracle 12c database production and on this production on of the tables contain about 2 millions record and the table contain a <strong>name</strong> column which contain both Arabic and English text. what I'm trying to do is to find a way to analyze the <strong>name</strong> column to get all the rows with similar to a given name. i tried using the <code>utl_match</code> package with contain implementation for <em>edit_distance</em> and <em>jaro_winkler</em> but this don't work perfectly for Arabic text since there are more similar letters in Arabic which the algorithm treats the as totally different letters like (ÿ£, ÿß, ÿ•) which results inefficient results. so what I'm looking for now is something to normalize the Arabic text, so i can use it with the <code>utl_match</code> package or any alternative which can help me do the job.
the task should be done in <strong>PL/SQl</strong> but if it's impossible I'm open to use any other tool or ideas</p>
",Multilingual Language Processing & Language Identification,finding similarity arabic text currently working oracle c database production production table contain million record table contain name column contain arabic english text trying find way analyze name column get row similar given name tried using package contain implementation edit distance jaro winkler work perfectly arabic text since similar letter arabic algorithm treat totally different letter like result inefficient result looking something normalize arabic text use package alternative help job task done pl sql impossible open use tool idea
Applying function to pandas dataframe: is there a more efficient way of doing this?,"<p>I have a dataframe that has a small number of columns but many rows (about 900K right now, and it's going to get bigger as I collect more data). It looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th style=""text-align: left;"">Author</th>
<th style=""text-align: left;"">Title</th>
<th style=""text-align: left;"">Date</th>
<th style=""text-align: left;"">Category</th>
<th style=""text-align: left;"">Text</th>
<th style=""text-align: left;"">url</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">Amira Charfeddine</td>
<td style=""text-align: left;"">Wild Fadhila 01</td>
<td style=""text-align: left;"">2019-01-01</td>
<td style=""text-align: left;"">novel</td>
<td style=""text-align: left;"">ÿßŸÑŸÉÿ™ÿßÿ® Ÿáÿ∞ÿß ŸÜŸáÿØŸäŸá ŸÑŸÉŸÑ ÿ™ŸàŸÜÿ≥Ÿä ÿ≠ÿ≥ ÿ•ŸÑŸä ÿßŸÑŸÉÿ™ÿßÿ® Ÿäÿ≠ŸÉŸä ...</td>
<td style=""text-align: left;"">NaN</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Amira Charfeddine</td>
<td style=""text-align: left;"">Wild Fadhila 02</td>
<td style=""text-align: left;"">2019-01-01</td>
<td style=""text-align: left;"">novel</td>
<td style=""text-align: left;"">ŸÅŸä ÿßŸÑÿ™ÿ≤ÿ∫ÿ±Ÿäÿ™ÿå ŸàÿßŸÑÿπŸäÿßÿ∑ Ÿà ÿßŸÑÿ≤ŸÖÿßŸÖÿ±ÿå ŸÑŸäŸàŸÖ ŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑÿ®...</td>
<td style=""text-align: left;"">NaN</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">253826</td>
<td style=""text-align: left;"">1515368_7636953</td>
<td style=""text-align: left;"">2010-12-28</td>
<td style=""text-align: left;"">/forums/forums/91/</td>
<td style=""text-align: left;"">Ÿáÿ∞ÿß ŸÖÿß ŸäŸÜÿµ ÿπŸÑŸäŸá ÿ•ÿØŸàÿ≥ÿ™Ÿàÿ± ÿßŸÑÿ™ŸàŸÜÿ≥Ÿä ŸÑÿß ÿ±ÿ¶ÿßÿ≥ÿ© ŸÖÿØŸâ ÿß...</td>
<td style=""text-align: left;""><a href=""https://www.tunisia-sat.com/forums/threads/151.."" rel=""nofollow noreferrer"">https://www.tunisia-sat.com/forums/threads/151..</a>.</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: left;"">250442</td>
<td style=""text-align: left;"">1504416_7580403</td>
<td style=""text-align: left;"">2010-12-21</td>
<td style=""text-align: left;"">/forums/sports/</td>
<td style=""text-align: left;"">\n\n\n\n\n\nÿßÿπŸÑŸÜÿ™ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ© ŸÑŸÉÿ±ÿ© ÿßŸÑŸäÿØ ÿß...</td>
<td style=""text-align: left;""><a href=""https://www.tunisia-sat.com/forums/threads/150.."" rel=""nofollow noreferrer"">https://www.tunisia-sat.com/forums/threads/150..</a>.</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">312628</td>
<td style=""text-align: left;"">1504416_7580433</td>
<td style=""text-align: left;"">2010-12-21</td>
<td style=""text-align: left;"">/forums/sports/</td>
<td style=""text-align: left;"">quel est le r√©sultat final\n,,,,????</td>
<td style=""text-align: left;""><a href=""https://www.tunisia-sat.com/forums/threads/150.."" rel=""nofollow noreferrer"">https://www.tunisia-sat.com/forums/threads/150..</a>.</td>
</tr>
</tbody>
</table>
</div>
<p>The &quot;Text&quot; Column has a string of text that may be just a few words (in the case of a forum post) or it may a portion of a novel and have tens of thousands of words (as in the two first rows above).</p>
<p>I have code that constructs the dataframe from various corpus files (.txt and .json), then cleans the text and saves the cleaned dataframe as a pickle file.</p>
<p>I'm trying to run the following code to analyze how variable the spelling of different words are in the corpus. The functions seem simple enough: One counts the occurrence of a particular spelling variable in each Text row; the other takes a list of such frequencies and computes a Gini Coefficient for each lemma (which is just a numerical measure of how heterogenous the spelling is). It references a spelling_var dictionary that has a lemma as its key and the various ways of spelling that lemma as values. (like {'color': ['color', 'colour']} except not in English.)</p>
<p>This code works, but it uses a lot of CPU time. I'm not sure how much, but I use PythonAnywhere for my coding and this code sends me into the tarpit (in other words, it makes me exceed my daily allowance of CPU seconds).</p>
<p>Is there a way to do this so that it's less CPU intensive? Preferably without me having to learn another package (I've spent the past several weeks learning Pandas and am liking it, and need to just get on with my analysis). Once I have the code and have finished collecting the corpus, I'll only run it a few times; I won't be running it everyday or anything (in case that matters).</p>
<p>Here's the code:</p>
<pre><code>import pickle
import pandas as pd
import re

with open('1_raw_df.pkl', 'rb') as pickle_file:
    df = pickle.load(pickle_file)

spelling_var = {
    'illi': [&quot;ÿßŸÑŸä&quot;, &quot;ÿßŸÑŸÑŸä&quot;],
    'besh': [&quot;ÿ®ÿßÿ¥&quot;, &quot;ÿ®ÿ¥&quot;],
    ...
    }

spelling_df = df.copy()

def count_word(df, word):
    pattern = r&quot;\b&quot; + re.escape(word) + r&quot;\b&quot;
    return df['Text'].str.count(pattern)

def compute_gini(freq_list):
    proportions = [f/sum(freq_list) for f in freq_list]
    squared = [p**2 for p in proportions]
    return 1-sum(squared)

for w, var in spelling_var.items():
    count_list = []
    for v in var:
        count_list.append(count_word(spelling_df, v))
        gini = compute_gini(count_list)
    spelling_df[w] = gini
</code></pre>
",Multilingual Language Processing & Language Identification,applying function panda dataframe efficient way dataframe ha small number column many row k right going get bigger collect data look like author title date category text url amira charfeddine wild fadhila novel nan amira charfeddine wild fadhila novel nan forum forum forum sport n n n n n n forum sport quel est le r sultat final n text column ha string text may word case forum post may portion novel ten thousand word two first row code construct dataframe various corpus file txt json clean text save cleaned dataframe pickle file trying run following code analyze variable spelling different word corpus function seem simple enough one count occurrence particular spelling variable text row take list frequency computes gini coefficient lemma numerical measure heterogenous spelling reference spelling var dictionary ha lemma key various way spelling lemma value like color color colour except english code work us lot cpu time sure much use pythonanywhere coding code sends tarpit word make exceed daily allowance cpu second way le cpu intensive preferably without learn another package spent past several week learning panda liking need get analysis code finished collecting corpus run time running everyday anything case matter code
using gpu with simple transformer mt5 training,"<p>mt5 fine-tuning does not use gpu(volatile gpu utill 0%)</p>
<p>Hi, im trying to fine tuning for ko-en translation with mt5-base model.
I think the Cuda setting was done correctly(cuda available is True)
But during training, the training set doesn't use GPU except getting dataset first(very short time).</p>
<p>I want to use GPU resource efficiently and get advice about translation model fine-tuning
here is my code and training env.</p>
<pre><code>import logging
import pandas as pd
from simpletransformers.t5 import T5Model, T5Args
import torch

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(&quot;transformers&quot;)
transformers_logger.setLevel(logging.WARNING)

train_df = pd.read_csv(&quot;data/enko_train.tsv&quot;, sep=&quot;\t&quot;).astype(str)
eval_df = pd.read_csv(&quot;data/enko_eval.tsv&quot;, sep=&quot;\t&quot;).astype(str)

train_df[&quot;prefix&quot;] = &quot;&quot;
eval_df[&quot;prefix&quot;] = &quot;&quot;

model_args = T5Args()
model_args.max_seq_length = 96
model_args.train_batch_size = 64
model_args.eval_batch_size = 32
model_args.num_train_epochs = 10
model_args.evaluate_during_training = True
model_args.evaluate_during_training_steps = 1000
model_args.use_multiprocessing = False
model_args.fp16 = True
model_args.save_steps = 1000
model_args.save_eval_checkpoints = True
model_args.no_cache = True
model_args.reprocess_input_data = True
model_args.overwrite_output_dir = True
model_args.preprocess_inputs = False
model_args.num_return_sequences = 1
model_args.wandb_project = &quot;MT5 Korean-English Translation&quot;
print(&quot;Is cuda available?&quot;, torch.cuda.is_available())

model = T5Model(&quot;mt5&quot;, &quot;google/mt5-base&quot;, cuda_device=0 , args=model_args)

# Train the model
model.train_model(train_df, eval_data=eval_df)

# Optional: Evaluate the model. We'll test it properly anyway.
results = model.eval_model(eval_df, verbose=True)
</code></pre>
<p>nvcc: NVIDIA (R) Cuda compiler driver<br />
Copyright (c) 2005-2021 NVIDIA Corporation<br />
Built on Mon_May__3_19:15:13_PDT_2021<br />
Cuda compilation tools, release 11.3, V11.3.109<br />
Build cuda_11.3.r11.3/compiler.29920130_0</p>
<p>gpu 0 = Quadro RTX 6000</p>
",Multilingual Language Processing & Language Identification,using gpu simple transformer mt training mt fine tuning doe use gpu volatile gpu utill hi im trying fine tuning ko en translation mt base model think cuda setting wa done correctly cuda available true training training set use gpu except getting dataset first short time want use gpu resource efficiently get advice translation model fine tuning code training env nvcc nvidia r cuda compiler driver copyright c nvidia corporation built mon may pdt cuda compilation tool release v build cuda r compiler gpu quadro rtx
Are there good ways to reduce the size of a vocabulary in natural language processing?,"<p>While working on tasks like text classification, QA, the original vocabulary generated from the corpus is usually too large, containing a lot of 'unimportant' words. The most popular ways I've seen to reduce the vocabulary size are discarding stop words and words with low frequencies.</p>

<p>For example, in <code>gensim</code></p>

<pre><code>gensim.utils.prune_vocab(vocab, min_reduce, trim_rule=None):
    Remove all entries from the vocab dictionary with count smaller than min_reduce.
    Modifies vocab in place, returns the sum of all counts that were pruned.
</code></pre>

<p>But in practice, setting the minimum count is empirical and does not seems quite exact. I notice that the term frequency of each word in the vocabulary often follows long-tail distribution, is it a good way if I only keep the top-K words that occupies X% (95%, 90%, 85%, ...) of the total term frequency? Or are there any sensible ways to reduce the vocabulary, without seriously influencing the NLP task? </p>
",Multilingual Language Processing & Language Identification,good way reduce size vocabulary natural language processing working task like text classification qa original vocabulary generated corpus usually large containing lot unimportant word popular way seen reduce vocabulary size discarding stop word word low frequency example practice setting minimum count empirical doe seems quite exact notice term frequency word vocabulary often follows long tail distribution good way keep top k word occupies x total term frequency sensible way reduce vocabulary without seriously influencing nlp task
What is Two-Level Morphology?,"<p>In Natural Language Processing what are the two levels of this two-level Morphology framework ?</p>
",Multilingual Language Processing & Language Identification,two level morphology natural language processing two level two level morphology framework
AI Based Deduplication using Textual Similarity Measure in Python,"<p>Given I have a dataframe that contains rows like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Title</th>
<th>Abstract</th>
<th>Keywords</th>
<th>Author</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr>
<td>5875</td>
<td>Textual Similarity: A Review</td>
<td>Textual Similarity has been used for measuring ...</td>
<td>X, Y, Z</td>
<td>James Thomas</td>
<td>2018</td>
</tr>
<tr>
<td>8596</td>
<td>Natural Language Processing: A Review</td>
<td>Natural Language Processing has been used for ...</td>
<td>NLP, AI, BERT</td>
<td>Rami John</td>
<td>2015</td>
</tr>
<tr>
<td>4586</td>
<td>Textual Similarity: Systematic Review</td>
<td>Text Similarity is being used for</td>
<td>Y, Z, AI</td>
<td>J Thomas</td>
<td>2018</td>
</tr>
</tbody>
</table>
</div>
<p>I would like to make a function <code>deduplicate</code> which can ingest the dataframe and outputs a matrix that allows me to compare the records with each other.</p>
<pre class=""lang-py prettyprint-override""><code>def deduplicate(df):
    matrix = take in each row and compute a similarity matrix
    return matrix
</code></pre>
<p>Whereas matrix can be</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>5875</th>
<th>8596</th>
<th>4586</th>
</tr>
</thead>
<tbody>
<tr>
<td>5875</td>
<td>1</td>
<td>0.4</td>
<td>0.9</td>
</tr>
<tr>
<td>8596</td>
<td>0.4</td>
<td>1</td>
<td>0.5</td>
</tr>
<tr>
<td>4586</td>
<td>0.9</td>
<td>0.5</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>This will allow me to find which records are similar to each other by comparing how similar the records are. I think I need to use some NLP Models here, as the rows contain textual as well as numerical data.</p>
<p>Is there a way in Python to do this? Some people suggest using dedupe, but due to privacy laws at place in my organization, we can only have in-house capacity for the same. Any suggestions would be welcome.</p>
",Multilingual Language Processing & Language Identification,ai based deduplication using textual similarity measure python given dataframe contains row like id title abstract keywords author year textual similarity review textual similarity ha used measuring x z james thomas natural language processing review natural language processing ha used nlp ai bert ramus john textual similarity systematic review text similarity used z ai j thomas would like make function ingest dataframe output matrix allows compare record whereas matrix id allow find record similar comparing similar record think need use nlp model row contain textual well numerical data way python people suggest using dedupe due privacy law place organization house capacity suggestion would welcome
Data PreProcessing for BERT (base-german),"<p>I am working on a sentiment analysis solution with BERT to analyze tweets in german. My training dataset of is a class of 1000 tweets, which have been manually annotated into the classes neutral, positive and negative.</p>
<p>The dataset with 10.000 tweets is quite unevenly distributed:</p>
<p>approx.
3000 positive
2000 negative
5000 neutral</p>
<p>the tweets contain formulations with @names, https links, numbers, punctuation marks, smileys like :3 :D :) etc..</p>
<p>The interesting thing is, if I remove them with the following code during Data Cleaning, the F1 score gets worse. Only the removal of https links (if I do it alone) leads to a small improvement.</p>
<pre><code># removing the punctuation and numbers
def remove_punct(text):
    text = re.sub(r'http\S+', '', text)                                         # removing links
    text = re.sub(r'@\S+', '', text)                                            # removing referencing on usernames with @
    text = re.sub(r':\S+', '', text)                                            # removing smileys with : (like :),:D,:( etc) 
    text  = &quot;&quot;.join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text

data['Tweet_clean'] = data['Tweet'].apply(lambda x: remove_punct(x))            # extending the dataset with the column tweet_clean
data.head(40)
</code></pre>
<p>also steps like stop words removal or lemmitazation lead more to a deterioration. Is this because I do something wrong or can the model BERT actually handle such values?</p>
<p>A second question is:</p>
<p>I found other records that were also manually annotated, but these are not tweets and the structure of the sentences and language use is different. Would you still recommend to add these records to my original?</p>
<p>There are about 3000 records in German.</p>
<p>My last question:</p>
<p>Should I reduce the class sizes to the size of the smallest unit and thus balance?</p>
",Multilingual Language Processing & Language Identification,data preprocessing bert base german working sentiment analysis solution bert analyze tweet german training dataset class tweet manually annotated class neutral positive negative dataset tweet quite unevenly distributed approx positive negative neutral tweet contain formulation name link number punctuation mark smiley like etc interesting thing remove following code data cleaning f score get worse removal link alone lead small improvement also step like stop word removal lemmitazation lead deterioration something wrong model bert actually handle value second question found record also manually annotated tweet structure sentence language use different would still recommend add record original record german last question reduce class size size smallest unit thus balance
Apache Tika fails to detect language on short sentence. Why?,"<p>I tried to detect language on short phrase and was surprised as detection result is wrong. </p>

<pre><code>    LanguageDetector detector = new OptimaizeLangDetector();
    try {
        detector.loadModels();
    } catch (IOException e) {
        LOG.error(e.getMessage(), e);
        throw new ExceptionInInitializerError(e);
    }
    LanguageResult languageResult = detector.detect(""Hello, my friend!"")
</code></pre>

<p>The languageResult contains Norwegian with ""medium"" probability. Why? I think it have to be English instead. Longer phrases seems to be detected properly. Does this means that Apache Tika should not be used on short text?</p>
",Multilingual Language Processing & Language Identification,apache tika fails detect language short sentence tried detect language short phrase wa surprised detection result wrong languageresult contains norwegian medium probability think english instead longer phrase seems detected properly doe mean apache tika used short text
How can I prevent words with hyphens from being tokenized when using scikit-learn`s term document matrix?,"<p>I am currently working with a large corpus of articles (around 205 thousand), which require the construction of a term document matrix.</p>
<p>I have looked around and it seems that sklearn offers an efficient way to construct it. However, when applying the proposed code to a small list of documents (as a test), I find out that words containing hyphens are divided, with the hyphens as delimiters. This is not desirable, as I am working with documents in Portuguese, in which hyphens are very common due to the large number of compound nouns. I would like to find out how I can generate a term document matrix that simply contains, as coluns, all tokens of my corpus, in which only empty spaces are used as delimiters between tokens (if a word contains a hyphen, it should be considered as a single token).</p>
<p>Here is the code:</p>
<pre><code>index=['doc 1','doc 2','doc 3','doc 4']
docs=['como voc√™ est√°', 'guarda-chuva!','covid-19 piorou','teto-de-gastos do tesouro']

df = pd.DataFrame(list(zip(index, docs)))
df.columns = ['index', 'docs']

from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()  
vects = vect.fit_transform(df.docs)
td = pd.DataFrame(vects.todense()).iloc[:len(df)]  
td.columns = vect.get_feature_names()
term_document_matrix = td.T
term_document_matrix.columns = ['Doc '+str(i) for i in range(1, len(df)+1)]
term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)
</code></pre>
<p>When printing the matrix, I find that ‚Äúteto-de-gastos‚Äù was transformed into ‚Äúteto‚Äù,‚Äùde‚Äù,‚Äùgastos‚Äù, which I do not want. Any suggestions on how to fix this hyphen issue?</p>
",Multilingual Language Processing & Language Identification,prevent word hyphen tokenized using scikit learn term document matrix currently working large corpus article around thousand require construction term document matrix looked around seems sklearn offer efficient way construct however applying proposed code small list document test find word containing hyphen divided hyphen delimiters desirable working document portuguese hyphen common due large number compound noun would like find generate term document matrix simply contains coluns token corpus empty space used delimiters token word contains hyphen considered single token code printing matrix find teto de gastos wa transformed teto de gastos want suggestion fix hyphen issue
Remove special characters but not accented letters,"<p>I do the following:</p>

<pre><code>re.sub(r'[^ \nA-Za-z0-9/]+', '', document)
</code></pre>

<p>to remove every character which is not alphanumeric, space, newline, or forward slash.</p>

<p>So I basically I want to remove all special characters except for the newline and the forward slash.</p>

<p>However, I do not want to remove the accented letters which various languages have such as in French, German etc.</p>

<p>But if I run the code above then for example the word </p>

<p><code>Mot√∂rhead</code> </p>

<p>becomes </p>

<p><code>Motrhead</code> </p>

<p>and I do not want to do this.</p>

<p>So how do I run the code above but without removing the accented letters?</p>

<p><strong>UPDATE:</strong></p>

<p>@MattM below has suggested a solution which does work for languages such as English, French, German etc but it certainly does not work for languages such as Polish where all the accented letters were still removed.</p>
",Multilingual Language Processing & Language Identification,remove special character accented letter following remove every character alphanumeric space newline forward slash basically want remove special character except newline forward slash however want remove accented letter various language french german etc run code example word becomes want run code without removing accented letter update mattm ha suggested solution doe work language english french german etc certainly doe work language polish accented letter still removed
INT8 quantization for matmul,"<p>Being inspired by &quot;Efficient 8-Bit Quantization of Transformer Neural Machine Language
Translation Model&quot;, I decided to follow through with the caveat of the paper. However, I get confused about setting offset variables during quantization.</p>
<pre><code>INPUT : A (tensor of FP32, [1, 4, 1024, 256])

# Quantization
offset = torch.empty(A.shape)
offset = torch.zeros_like(offset)
scale = 255 / (torch.max(A) - torch.min(A))
A_int8 = (A - offset) * scale

# Probability Distribution
P = norm.pdf(A, torch.mean(A, dim=[2, 3]), torch.std(A, dim = [2,3]))
Q = norm.pdf(A_int8, torch.mean(A_int8, dim=[2, 3]), torch.std(A_int8, dim = [2,3]))
P = torch.from_numpy(P)
Q = torch.from_numpy(Q)

# KLD
kld = (P * (P / Q).log()).sum()
print(kld)    

# After this, I'm going to apply self-attention operation.
# B_int8 = A_int8.clone()
# AB = A_int8.matmul(B_int8.transpose(-1, -2))
</code></pre>
<p>I get positive kld value for now, but I'm not sure that I went through the right way to do it. Any help or advice is appreciated.</p>
",Multilingual Language Processing & Language Identification,int quantization matmul inspired efficient bit quantization transformer neural machine language translation model decided follow caveat paper however get confused setting offset variable quantization get positive kld value sure went right way help advice appreciated
How to predict translations (decode) for IBM models in nltk?,"<p>I'm fairly new to the NLTK API and wanted to get started with the simplistic IBM Model 1, which does word-level translations. This is what I did so far, with the help of the <a href=""https://www.nltk.org/_modules/nltk/translate/ibm1.html"" rel=""nofollow noreferrer"">NLTK API documentation</a>.</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.translate.ibm1 import IBMModel1
from nltk.translate import AlignedSent

def get_text(filename):
    senteces = []
    with open(filename,'r') as f:
        for sentence in f:
            sentences.append(sentence.split())
    return sentences

src_sentences = get_text('source.txt')
trg_sentences = get_text('target.txt')

bitext = []
for i in range(len(src_sentences)):
    bitext.append(AlignedSent(src_sentences[i], trg_sentences[i]))

ibm1 = IBMModel1(bitext, 5)
</code></pre>
<p>Now that <code>IBMModel1</code> has been created, I'm unsure how to perform the <a href=""https://www.statmt.org/book/slides/06-decoding.pdf"" rel=""nofollow noreferrer"">decoding</a>, ie. predicting translations from a test set. I can't seem to find this on the documentation as well.</p>
<p>In simple words, I want to obtain a predicted translation, given a random source sentence using this model. How do I achieve this?</p>
",Multilingual Language Processing & Language Identification,predict translation decode ibm model nltk fairly new nltk api wanted get started simplistic ibm model doe word level translation far help nltk api documentation ha created unsure perform decoding ie predicting translation test set seem find documentation well simple word want obtain predicted translation given random source sentence using model achieve
Create unique name from array of names in NodeJS,"<p>I'm trying to create a unique name which includes all the data that could be found in an array of strings (names). It shouldn't repeat words.</p>
<p>Given this:</p>
<pre><code>[
  'Xiaomi Mi',
  'Xiaomi',
  'Mi',
  'TV',
  'Stick',
  'Xiaomi Mi TV Stick',
  'Xiaomi Mi TV',
  'Mi TV Stick',
  'Mi TV',
  'TV Stick',
  'Reproductor',
  'Reproductor Multimedia',
  'Multimedia'
]
</code></pre>
<p>I would like to get something like this:</p>
<pre><code>'Xiaomi Mi TV Stick, Reproductor Multimedia'
</code></pre>
<p>I would like to analyze the data (natural language processing, maybe) and come up with a unique name, so the solution shouldn't be about taking the longest elements in the array, for example.</p>
",Multilingual Language Processing & Language Identification,create unique name array name nodejs trying create unique name includes data could found array string name repeat word given would like get something like would like analyze data natural language processing maybe come unique name solution taking longest element array example
Type-Token Ratio in Google Sheets: How to manipulate long strings of text (millions of characters),"<p>Here‚Äôs the challenge. In a Google Sheets spreadsheet, I have a column in which can be found a range of cells containing lists of words separated by comas, one per row, up to a thousand row. Each list show the words taken from a text, in alpha-numeral order, from a few hundred to a few thousand words. I need to count both the total of words in all the rows, taken together, and the number of unique word forms too. In other words, from the glossary of natural language processing, I want to know the number of <em>tokens</em> and the number of <em>types</em> in my corpus, in order to calculate the <em>type-token ratio</em> or lexical density.</p>
<p>In particular, finding the number of unique word forms in the whole column have proven to be a challenge. In an ARRAY FORMULA, with corresponding functions, I‚Äôve JOINED the strings, SPLITED the words, TRANSPOSED them, then removed duplicates with UNIQUE function, then counted the remaining word forms. This worked on a sample corpus constituted of a little over ten lists of words, but failed when I reached fifteen or so lists of words taken together, a far cry from the thousand lists I need to join in my formula to obtain the results I am looking for.</p>
<p>From what I can gather, the problem would reside in that the resulting string I intend to manipulate is exceeding 50,000 characters. Here and there, for specific cases, I‚Äôve found similar questions, and propositions for workarounds, mostly through custom functions, but I could not replicate the result. Needless to say, writing custom fonctions on my own is beyond my reach. Someone suggests to use QUERY headers, but I did not figured either if this was of any help in my case.</p>
<p>The formulas I came up with are the following:</p>
<p>To obtain the total number of words (tokens) through all the lists:
<code>=COUNTA(ARRAYFORMULA(SPLIT(JOIN(&quot;,&quot;;1;B2:B);&quot;,&quot;)))</code></p>
<p>To obtain the number of unique word forms (types) through all the lists:
<code>=COUNTA(ARRAYFORMULA(UNIQUE(TRANSPOSE(SPLIT(JOIN(&quot;,&quot;;1;B2:B);&quot;,&quot;)))))</code></p>
<p><a href=""https://docs.google.com/spreadsheets/d/1GzgReYGiDtgkbS9bf0biudgJS0J4tAhraNqWTKH-O98/edit?usp=sharing"" rel=""nofollow noreferrer"">A sample in a spreadsheet can be found here</a>.</p>
<p>EDIT 1:</p>
<p>I‚Äôve included the column of texts stripped of ponctuation, from which the lists of words are generated, and the formula used to generate them.</p>
<p>EDIT 2:</p>
<p>Changed the title to better reflect the general intent.</p>
",Multilingual Language Processing & Language Identification,type token ratio google sheet manipulate long string text million character challenge google sheet spreadsheet column found range cell containing list word separated coma one per row thousand row list show word taken text alpha numeral order hundred thousand word need count total word row taken together number unique word form word glossary natural language processing want know number token number type corpus order calculate type token ratio lexical density particular finding number unique word form whole column proven challenge array formula corresponding function joined string splited word transposed removed duplicate unique function counted remaining word form worked sample corpus constituted little ten list word failed reached fifteen list word taken together far cry thousand list need join formula obtain result looking gather problem would resulting string intend manipulate exceeding character specific case found similar question proposition workarounds mostly custom function could replicate result needle say writing custom fonctions beyond reach someone suggests use query header figured either wa help case formula came following obtain total number word token list obtain number unique word form type list sample spreadsheet found edit included column text stripped ponctuation list word generated formula used generate edit changed title better reflect general intent
How can reach the list of characters using the bigram/n-gram algorithm in PySpark?,"<p>I'm a newbie in PySpark, and I want to translate the NLP-based feature code which is pythonic, into PySpark.</p>
<pre class=""lang-py prettyprint-override""><code>#Python
N = 2

n_grams = lambda input_text: 0 if pd.isna(input_text) else len(set([input_text[character_index:character_index+N] for character_index in range(len(input_text)-N+1)]))


#quick test 
n_grams_example = 'zhang1997'  #output = [‚Äòzh‚Äô, ‚Äòha‚Äô, ‚Äòan‚Äô, ‚Äòng‚Äô, ‚Äòg1‚Äô, ‚Äò19‚Äô, ‚Äò99‚Äô , ‚Äò97‚Äô]
n_grams(n_grams_example)       # 8
</code></pre>
<p>I checked the <a href=""https://spark.apache.org/docs/2.2.0/ml-features.html#n-gram"" rel=""nofollow noreferrer"">NGram Python docs</a> and I tried the following unseccessfully:</p>
<pre class=""lang-py prettyprint-override""><code>#PySpark
from pyspark.ml.feature import NGram

ndf = spark.createDataFrame([
    (0, [&quot;zhang1997&quot;])], [&quot;id&quot;, &quot;words&quot;])

ndf.show()

+---+-----------+
| id|      words|
+---+-----------+
|  0|[zhang1997]|
+---+-----------+

ngram = NGram(n=2, inputCol=&quot;words&quot;, outputCol=&quot;ngrams&quot;)

ngramDataFrame = ngram.transform(ndf)
ngramDataFrame.select(&quot;ngrams&quot;).show(truncate=False)
+------+
|ngrams|
+------+
|[]    |
+------+
</code></pre>
<p>Do I miss something here I get empty <code>[]</code> as a result instead of <code>[‚Äòzh‚Äô, ‚Äòha‚Äô, ‚Äòan‚Äô, ‚Äòng‚Äô, ‚Äòg1‚Äô, ‚Äò19‚Äô, ‚Äò99‚Äô , ‚Äò97‚Äô]</code>? I'm interested to get its length of n-gram sets which is <code>8</code> in this case.</p>
<p><strong>update</strong>: I found a way to do this without using <code>NGram</code> but I'm not happy with its performance.</p>
<pre><code>def n_grams(input_text):
    if input_text is None:
        return 0
    N = 2
    return len(set([input_text[character_index:character_index+N] for character_index in range(len(input_text)-N+1)]))
</code></pre>
",Multilingual Language Processing & Language Identification,reach list character using bigram n gram algorithm pyspark newbie pyspark want translate nlp based feature code pythonic pyspark checked ngram python doc tried following unseccessfully miss something get empty result instead interested get length n gram set case update found way without using happy performance
LIWC English Dictionary,"<p>How can I get the English dictionary of LIWC (Linguistic Inquiry and Word Count)?</p>
<p>I want to make an Arabic dictionary for LIWC based on the English Dictionary.</p>
",Multilingual Language Processing & Language Identification,liwc english dictionary get english dictionary liwc linguistic inquiry word count want make arabic dictionary liwc based english dictionary
Is there a way to split a string by multiple different strings?,"<p>I a trying to make a translator to a custom language, and at the moment I am going towards the point where I can type <code>the man sits with the woman</code> and receive the output <code>de mno di felio colten aili</code>, word for word being <code>the man the woman sits with</code>.</p>
<p>When the translator gets to a verb it adds it to <code>verbo</code> and a preposition to <code>prepo2</code> (it's <code>prepo2</code> not <code>prepo</code> for other reasons). After it finishes translating word for word, it splits it by all translations of <code>the</code> (<code>de</code> and <code>di</code>) and the verb, previously defined <code>verbo</code>. It then should put the verb on the end and go on to run for a preposition instead of a verb, and then put that on the end.</p>
<p>When I enter in <code>the man sits with the woman</code> I get <code>de mno di felio  aili</code> with no <code>colten</code>, when I enter <code>the man sits the woman</code> (yes I know not a correct sentence but it's the verb I'm testing with) I just get <code>de mno colten di felio</code> and when I enter <code>the man is with the woman</code> I get <code>de mno aili di felio</code>.</p>
<p>Trying to get this done quickly so an answer would be appreciated :)</p>
<pre><code>import string
import re

NounM = {
    'man': 'mno',
    'rock': 'lehr',
    'dog': 'krua'
}

NounF = {
    'woman': 'felio',
    'chair': 'poen',
    'cat': 'keile'
}

Verb = {
    'sit': 'colt',
    'sing': 'alet'
}

Preposition = {
    'on': 'mit',
    'with': 'ail',
    'at': 'zal'
}

Pronoun = {
    'he': 'tse',
    'she': 'se',
    'i': 'ile',
    'me': 'men',
    'they': 'er',
    'it': 'ze',
    'you': 'j√º'
}

Adjective = {
    'happy': 'kliony',
    'sad': 'probo',
    'good': 'klio',
    'bad': 'pro'
}

Article = {
    'that': 'arei',
    'those': 'sie'
}

Question = {
    'who': 'nej',
    'what': 'k√§r',
    'when': 'woin',
    'where': 'ten',
    'why': 'apr'
}

Skip = ('is', 'are', 'am')

Preposition_Replace = ('aile', 'aili', 'mite', 'miti')

Verb_Replace = ('colten', 'aleten')

def translate(j=None):
    sentence = input('Enter the sentence to turn into your custom language! ')
    split = sentence.split()
    translated_list = []
    translated_sentence = ''

    for index, word in enumerate(split):
        char = ''
        for a in string.punctuation:
            if str(a) in word:
                char = a
        if word in NounM:
            translated_sentence += NounM[word]
        elif word in NounF:
            translated_sentence += NounF[word]
        elif word in Verb:
            translated_sentence += Verb[word]
            verbo = Verb[word]
        elif word in Preposition:
            translated_sentence += Preposition[word]
            prepo = Preposition[word]
            try:
                c = split[index + 1]
                while c not in NounM and c not in NounF:
                    a = 2
                    c = split[index + a]
                    a += 1
                if c in NounM:
                    translated_sentence += 'e'
                    prepo2 = prepo + 'e'
                elif c in NounF:
                    translated_sentence += 'i'
                    prepo2 = prepo + 'i'
            except IndexError:
                pass
        elif word in Pronoun:
            translated_sentence += Pronoun[word]
        elif word in Adjective:
            translated_sentence += Adjective[word]
        elif word in Article:
            translated_sentence += Article[word]
        elif word in Question:
            translated_sentence += Question[word]
        elif word == 'the':
            c = split[index + 1]
            while c not in NounM and c not in NounF:
                a = 2
                c = split[index + a]
                a += 1
            if c in NounM:
                translated_sentence += 'de'
            elif c in NounF:
                translated_sentence += 'di'
            else:
                pass
        elif word == 'a':
            c = split[index + 1]
            while c not in NounM and c not in NounF:
                    a = 2
                    c = split[index + a]
                    a += 1
            if c in NounM:
                translated_sentence += 'es'
            elif c in NounF:
                translated_sentence += 'en'
            else:
                pass
        elif word in Skip:
            c = split[index + 1]
            if c == 'not':
                split.remove('not')
                translated_sentence += 'nen'
            else:
                pass
        elif word[len(word) - 1] == 's':
            word = word[:-1]
            if word in Verb:
                translated_sentence += Verb[word]
                translated_sentence += 'en'
                verbo = Verb[word] + 'en'
            else:
                pass
        else:
            translated_sentence += word
        word += str(char)
        for i in translated_sentence:
            translated_list += i
        translated_list += str(char)
        if word == 'is' or word == 'are' or word == 'am':
            if c == 'not':
                translated_list += ' '
            else:
                pass
        else:
            translated_list += ' '
        translated_sentence = ''

    leng = len(translated_list) - 2
    final = translated_list[leng]
    if final in string.punctuation:
        translated_list.remove(final)

    translated_sentence = ''
    for i in translated_list:
        translated_sentence += i

    if final in string.punctuation:
        translated_sentence += final

    try:
        new_sentence2 = ''
        new_sentence = re.split('(?=de |di )' and ' '+verbo, translated_sentence)
        print(new_sentence)
        for i in new_sentence:
            new_sentence2 += i
        print(new_sentence2)
        new_sentence += verbo
        new_sentence = re.split('(?=de |di )' and ' '+prepo2, new_sentence2)
        print(new_sentence)
        new_sentence.append(' '+prepo2)
        print(new_sentence)
        translated_sentence = ''
        for i in new_sentence:
            translated_sentence += i
    except:
        pass

    print(translated_sentence)
    other_translate = input('Would you like to translate another sentence? y/n ')
    if other_translate == 'y':
        translate()


translate()

</code></pre>
<p><strong>Main code for the splitting part:</strong></p>
<pre><code>try:
        new_sentence2 = ''
        new_sentence = re.split('(?=de |di )' and ' '+verbo, translated_sentence)
        print(new_sentence)
        for i in new_sentence:
            new_sentence2 += i
        print(new_sentence2)
        new_sentence += verbo
        new_sentence = re.split('(?=de |di )' and ' '+prepo2, new_sentence2)
        print(new_sentence)
        new_sentence.append(' '+prepo2)
        print(new_sentence)
        translated_sentence = ''
        for i in new_sentence:
            translated_sentence += i
    except:
        pass
</code></pre>
",Multilingual Language Processing & Language Identification,way split string multiple different string trying make translator custom language moment going towards point type receive output word word translator get verb add preposition reason finish translating word word split translation verb previously defined put verb end go run preposition instead verb put end enter get enter yes know correct sentence verb testing get enter get trying get done quickly answer would appreciated main code splitting part
How to adapt transfomer pretrained tokenizers to work with this translation tutorial?,"<p>The tutorial url:
<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p>The pt-en tokenizer model code:</p>
<pre><code>examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']

# 2. Get BertTokenizer
model_name = &quot;ted_hrlr_translate_pt_en_converter&quot;
tf.keras.utils.get_file(
    f&quot;{model_name}.zip&quot;,
    f&quot;https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip&quot;,
    cache_dir='.', cache_subdir='', extract=True
)

tokenizers = tf.saved_model.load(model_name)
en_tokenizer_items = [item for item in dir(tokenizers.en) if not item.startswith('_')]
print('En tokenizer methods: ', en_tokenizer_items)

# 3. Tokenizer examples
def tokenize_pairs(pt, en):
    pt = tokenizers.pt.tokenize(pt)
    
    # Convert from ragged to dense, padding with zeros.
    pt = pt.to_tensor()

    en = tokenizers.en.tokenize(en)
    # Convert from ragged to dense, padding with zeros.
    en = en.to_tensor()
    return pt, en


# 4. Make batches
BUFFER_SIZE = 20000
BATCH_SIZE = 64
def make_batches(ds):
  return (
      ds
      .cache()
      .shuffle(BUFFER_SIZE)
      .batch(BATCH_SIZE)
      .map(tokenize_pairs, num_parallel_calls=tf.data.experimental.AUTOTUNE)
      .prefetch(tf.data.experimental.AUTOTUNE))

train_batches = make_batches(train_examples)
val_batches = make_batches(val_examples)
</code></pre>
<p>In this line of code below, the tokenize takes a tensor of string as input:</p>
<pre><code>pt = tokenizers.pt.tokenize(pt)
</code></pre>
<p>A transorformer pretrained tokenizer usually takes a string as input rather than a tensor here. If I want to switch the tokenizers from portugues to Chinese, how can I adapt the transformers tokenizer to work with the 'make_batches' and 'tokenize_pairs' functions?</p>
<p>I simply import the transformer tokenizers but it didn't work:</p>
<pre><code>from transformers import BertTokenizer
tokenizer_en = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
tokenizer_zh = BertTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)

def tokenize_pairs(zh, en):
    zh = tokenizer_zh.tokenize(zh)
    # Convert from ragged to dense, padding with zeros.
    zh = zh.to_tensor()
    en = tokenizer_en.tokenize(en)
    # Convert from ragged to dense, padding with zeros.
    en = en.to_tensor()
    return zh, en
</code></pre>
<p>This line below reports an error:</p>
<pre><code>zh = tokenizer_zh.tokenize(zh)



/Users/cong/transformer/data_zh.py:44 tokenize_pairs  *
        zh = tokenizer_zh.tokenize(zh)
    /Users/cong/.venv/tf2/lib/python3.8/site-packages/transformers/tokenization_utils.py:336 split_on_tokens  *
        if not text.strip():
    /Users/cong/.venv/tf2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:401 __getattr__
        self.__getattribute__(name)

    AttributeError: 'Tensor' object has no attribute 'strip'
</code></pre>
",Multilingual Language Processing & Language Identification,adapt transfomer pretrained tokenizers work translation tutorial tutorial url pt en tokenizer model code line code tokenize take tensor string input transorformer pretrained tokenizer usually take string input rather tensor want switch tokenizers portugues chinese adapt transformer tokenizer work make batch tokenize pair function simply import transformer tokenizers work line report error
GPU goes out of memory during training large dataset,"<p>I am using a Transformer network for machine translation, during training of model the GPU runs out of memory during large dataset, it works fine with small data.</p>
<p>This is the self attention part, The error comes during the computation of matrices.</p>
<pre><code>import tensorflow as tf

class SelfAttention(tf.keras.layers.Layer):
    def __init__(self, embed_size, head):
        super(SelfAttention, self).__init__()
        self.head = head
        self.embed_size = embed_size
        self.head_dim = embed_size // head

        assert (self.head_dim * head == embed_size), 'size of head_dim is not matching'

        self.query = tf.keras.layers.Dense(self.head_dim, activation='linear', use_bias=False)
        self.value = tf.keras.layers.Dense(self.head_dim, activation='linear', use_bias=False)
        self.key = tf.keras.layers.Dense(self.head_dim, activation='linear', use_bias=False)
        self.fc_layer = tf.keras.layers.Dense(self.embed_size, activation='linear')

    def call(self, value, key, query, mask):
        # Number of training examples
        N = query.shape[0]
        query_len, value_len, key_len = query.shape[1], value.shape[1], key.shape[1]

        # Reshape according to the number of examples and words
        query = tf.reshape(query, (N, query_len, self.head, self.head_dim))
        value = tf.reshape(value, (N, value_len, self.head, self.head_dim))
        key = tf.reshape(key, (N, key_len, self.head, self.head_dim))

        query = self.query(query)
        value = self.value(value)
        key = self.key(key)

        # energy shape: (N, head, query_len, key_len) try to imagine the shape in mind
        energy = tf.einsum(&quot;nqhd, nkhd-&gt;nhqk&quot;, query, key)

        if mask is not None:
            energy = energy * mask
            energy = tf.where(tf.equal(energy, 0), -1e20, energy)

        attention = tf.keras.activations.softmax(energy, axis=3)

        # attention shape: (N, head, query_len, key_len)
        # value shape:(N, value_len, head, head_dim)
        # output: (N, query_len, head, head_dim)
        output = tf.reshape(tf.einsum(&quot;nhql, nlhd-&gt;nqhd&quot;, attention, value), (N, query_len, self.head*self.head_dim))

        output = tf.keras.activations.linear(output)

        return output
</code></pre>
<p>The error is</p>
<pre><code>2021-09-20 11:51:49.615495: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 35477760 totalling 33.83MiB
2021-09-20 11:51:49.615502: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 40866304 totalling 38.97MiB
2021-09-20 11:51:49.615509: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 47409664 totalling 45.21MiB
2021-09-20 11:51:49.615516: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 47547136 totalling 45.34MiB

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6860   message = e.message + (&quot; name: &quot; + name if name is not None else &quot;&quot;)
   6861   # pylint: disable=protected-access
-&gt; 6862   six.raise_from(core._status_to_exception(e.code, message), None)
   6863   # pylint: enable=protected-access
   6864 

/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

ResourceExhaustedError: OOM when allocating tensor with shape[32,334,25335] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd]
</code></pre>
<p>What should I do?</p>
",Multilingual Language Processing & Language Identification,gpu go memory training large dataset using transformer network machine translation training model gpu run memory large dataset work fine small data self attention part error come computation matrix error
Converting an English Statement into a Questi0n,"<p>(Apologies for the title. Stack overflow doesn't allow the word ""Question"" in titles.)</p>

<p>How would one go about writing an algorithm to convert an english statement into a question? Where would one even begin? For example:</p>

<p>""<em>The ingredients for an omelette are eggs, bacon, cheese, and onions</em>"" would become ""<em>What are the ingredients for an omelette?</em>"" or ""<em>The ingredients for an omelette are what?</em>""</p>

<p>I can imagine parsing a sentence into it's components, and then re-arranging these while adding and removing words to form a grammatically correct sentence, but I'd have no idea where to start. I know this is by no means a trivial task, and I think the most helpful thing right now would be pointers to literature or similar problems.</p>
",Multilingual Language Processing & Language Identification,converting english statement questi n apology title stack overflow allow word question title would one go writing algorithm convert english statement question would one even begin example ingredient omelette egg bacon cheese onion would become ingredient omelette ingredient omelette imagine parsing sentence component arranging adding removing word form grammatically correct sentence idea start know mean trivial task think helpful thing right would pointer literature similar problem
BLEU score implementation for sentence similarity detection,"<p>I need to calculate BLEU score for identifying whether two sentences are similar or not.I have read some articles which are mostly about BLEU score for Measuring Machine translation accuracy.But i'm in need of a BLEU score to find out similarity between sentences in a same language[English].(i.e)(Both the sentences are in English).Thanks in anticipation.</p>
",Multilingual Language Processing & Language Identification,bleu score implementation sentence similarity detection need calculate bleu score identifying whether two sentence similar read article mostly bleu score measuring machine translation accuracy need bleu score find similarity sentence language english e sentence english thanks anticipation
Grouping words by their similarity,"<p>I have a huge dictionary/dataframe of German words and how often they appeared in a huge text corpus. For example:</p>
<pre><code>der                                23245
die                                23599
das                                23959
eine                               22000
dass                               18095
Buch                               15988
B√ºchern                             1000
Arbeitsplatz-Management              949
Arbeitsplatz-Versicherung            800
</code></pre>
<p>Since words like &quot;Buch&quot; (book) and &quot;B√ºchern&quot; (books, but in a different declension form) have similar meanings, I want to add up their frequencies. Same thing with the articles &quot;der, die, das&quot;, but not with the last two words that have completely different meanings even if they stem from the same words.</p>
<p>I tried the Levenshtein distance, which is &quot;the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.&quot; But I get bigger Levenshtein distances between &quot;Buch&quot; and &quot;B√ºcher&quot; than between &quot;das&quot; and &quot;dass&quot; (completely different meanings)</p>
<pre><code>import enchant
string1 = &quot;das&quot;
string2 = &quot;dass&quot;
string3 = &quot;Buch&quot;
string4 = &quot;B√ºchern&quot;
print(enchant.utils.levenshtein(string1, string2))
print(enchant.utils.levenshtein(string3, string4))
&gt;&gt;&gt;&gt; 1
&gt;&gt;&gt;&gt; 4
</code></pre>
<p>Is there any other way to cluster such words efficiently?</p>
",Multilingual Language Processing & Language Identification,grouping word similarity huge dictionary dataframe german word often appeared huge text corpus example since word like buch book b chern book different declension form similar meaning want add frequency thing article der die da last two word completely different meaning even stem word tried levenshtein distance minimum number single character edits insertion deletion substitution required change one word get bigger levenshtein distance buch b cher da das completely different meaning way cluster word efficiently
How to detect the language of a sentence in python,"<p>I am trying to detect the language of a sentence in python. I tried 'langdetect' and 'nltk word corpus' but nothing is giving the expected results:
My example df is:</p>
<pre><code>df = pd.DataFrame({'text': ['Auxiliar Director/a de Hotel', 'Jefe de Tienda', 'Data Analyst']})
</code></pre>
<p>and expected result is:</p>
<pre><code>    text                            detected_language
0   Auxiliar Director/a de Hotel    spanish
1   Jefe de Tienda                  spanish
2   Data Analyst                    english 
</code></pre>
<p>TIA!</p>
",Multilingual Language Processing & Language Identification,detect language sentence python trying detect language sentence python tried langdetect nltk word corpus nothing giving expected result example df expected result tia
Spacy 3.06 NLP Named Entity lower case issue,"<p>I'm trying to determine the named entity from lower case and merge those entities based upon Entity Type i.e &quot;GPE&quot;<a href=""https://i.sstatic.net/YlDGf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YlDGf.png"" alt=""Output image below here"" /></a> but spacy is not identifying the entities n lower case e.g &quot;new york&quot;.</p>
<p>Example Sentence: &quot;You can hear the musicality in his voice. Some combination of leftover italian rhythms and a new york inflected North Jersey accent.&quot;</p>
",Multilingual Language Processing & Language Identification,spacy nlp named entity lower case issue trying determine named entity lower case merge entity based upon entity type e gpe spacy identifying entity n lower case e g new york example sentence hear musicality voice combination leftover italian rhythm new york inflected north jersey accent
Training and fine tuning MBart on a specefic (unsupported yet) language,"<p>I would like to train Hugging face MBart model to be able to translate between Hungarian and English or any other unsupported language (it currently supports 25 languages ).
I have read the original documentation and a <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">tutorial</a> on how to train a language model on their website but I am not sure how translate that to what I want to do or if that is of any benefit in the first place. I would appreciate any idea or reference.</p>
",Multilingual Language Processing & Language Identification,training fine tuning mbart specefic unsupported yet language would like train hugging face mbart model able translate hungarian english unsupported language currently support language read original documentation tutorial train language model website sure translate want benefit first place would appreciate idea reference
Extracting German words with regex,"<p>I have some large <strong>German text files</strong> (that I can split into smaller text files, so that's not a problem) from which I want to <strong>extract the words</strong> and later count how many times a each word appears (the latter is also not a problem).</p>
<p>The texts are pretty messy:</p>
<pre><code>'''Alan Smithee''' steht als [[Pseudonym]] f√ºr einen fiktiven Regisseur, der Filme verantwortet, bei denen der eigentliche [[Regisseur]] seinen Namen nicht mit dem Werk in Verbindung gebracht haben m√∂chte. Von 1968 bis 2000 wurde es von der [[Directors Guild of America]] (DGA) f√ºr solche Situationen empfohlen, seither ist es '''Thomas Lee'''.&lt;ref&gt;[[Los Angeles Times|latimes]].com: [http://articles.latimes.com/2000/jan/15/entertainment/ca-54271 ''Name of Director Smithee Isn't What It Used to Be''], zuletzt gepr√ºft am 2. April 2011.&lt;/ref&gt; ''Alan Smithee'' ist jedoch weiterhin in Gebrauch.

Alternative Schreibweisen sind unter anderem die Ursprungsvariante ''Al'''len''' Smithee'' sowie ''Alan Sm'''y'''thee'' und ''A'''dam''' Smithee''. Auch zwei teilweise asiatisch anmutende Schreibweisen ''Alan Smi Thee'' und ''Sumishii Aran'' geh√∂ren ‚Äì so die [[Internet Movie Database]] ‚Äì dazu.&lt;ref name=&quot;IMDb&quot;&gt;[http://www.imdb.com/name/nm0000647/ Eigener Eintrag f√ºr ''Alan Smithee'' in der IMDb]&lt;/ref&gt;

== Geschichte ==
=== Entstehung ===
Das Pseudonym entstand 1968 infolge der Arbeiten am Western-Film ''Death of a Gunfighter'' (deutscher Titel ''[[Frank Patch ‚Äì Deine Stunden sind gez√§hlt]]''). Regisseur [[Robert Totten]] und Hauptdarsteller [[Richard Widmark]] gerieten in einen Streit, woraufhin [[Don Siegel]] als neuer Regisseur eingesetzt wurde.

Der Film trug nach Abschluss der Arbeiten noch deutlich Tottens [[Manier (Stil)|Handschrift]], der auch mehr Drehtage als Siegel daran gearbeitet hatte, weshalb dieser die Nennung seines Namens als Regisseur ablehnte. Totten selbst lehnte aber ebenfalls ab. Als L√∂sung wurde  ''Allen Smithee'' als ein m√∂glichst einzigartiger Name gew√§hlt.&lt;ref&gt;[http://www.imdb.com/name/nm0000647/bio ''Biography for Alan Smithee''] in der Internet Movie Database&lt;/ref&gt;

In den zeitgen√∂ssischen Kritiken wurde der Regisseur u.&amp;nbsp;a. von [[Roger Ebert]] mit den Worten gelobt: 
{{Zitat
 |Text=Director Allen Smithee, a name I‚Äôm not familiar with, allows his story to unfold naturally. He never preaches, and he never lingers on the obvious. His characters do what they have to do.&lt;ref&gt;rogerebert.[[Chicago Sun-Times|suntimes]].com: [https://www.rogerebert.com/reviews/death-of-a-gunfighter-1969 ''Death of a Gunfighter''], zuletzt gepr√ºft am 2. April 2011&lt;/ref&gt;
 |Sprache=en
 |√úbersetzung=Regisseur Alan Smithee, ein Name, der mir nicht vertraut ist, erlaubt es seiner Handlung, sich nat√ºrlich zu entfalten. Er predigt niemals, und er verweilt nie beim Offensichtlichen. Seine Charaktere tun, was sie tun m√ºssen.}}

=== Aufdeckung und Abkehr ===
1997 kam die Parodie ''An Alan Smithee Film: Burn Hollywood Burn'' (deutscher Titel ''[[Fahr zur H√∂lle Hollywood]]'') in die Kinos, was das Pseudonym einem gr√∂√üeren Publikum bekannt machte, nicht zuletzt weil [[Arthur Hiller (Regisseur)|Arthur Hiller]], der eigentliche Regisseur des Films, selbst seinen Namen zur√ºckzog und analog zum Filmtitel das Pseudonym ''Alan Smithee'' benutzte. Der Film gilt als einer der schlechtesten Filme der 1990er Jahre und gewann f√ºnf [[Goldene Himbeere]]n.

Der Film ''[[Supernova (2000)|Supernova]]'' ist der erste Post-Smithee-Film, dort f√ºhrte ein gewisser ''Thomas Lee'' alias [[Walter Hill]] die Regie.
&lt;!-- fand nur einen f√ºr den von 1990, siehe ''[[Das Kinderm√§dchen]]'':
‚ÄûSmithee wurde allerdings auch nach ''Supernova'' gesichtet, in einem Film namens ''The Guardian''.‚Äú
--&gt;

== Verwendung ==
Die Verwendung dieses oder eines anderen Pseudonyms ist f√ºr Mitglieder der DGA streng reglementiert. Ein Regisseur, der f√ºr einen von ihm gedrehten Film seinen Namen nicht hergeben m√∂chte, hat nach Sichtung des fertigen Films drei Tage Zeit, anzuzeigen, dass er ein Pseudonym verwenden m√∂chte. Der Rat der DGA entscheidet binnen zwei Tagen √ºber das Anliegen. Erhebt die Produktionsfirma Einspruch, entscheidet ein Komitee aus Mitgliedern der DGA und der Vereinigung der Film- und Fernsehproduzenten, ob der Regisseur ein Pseudonym angeben darf. √úber die Beantragung muss der Regisseur Stillschweigen halten, ebenso darf er den fertigen Film nicht √∂ffentlich kritisieren, wenn die DGA ihm die Verwendung eines Pseudonyms zugesteht.&lt;ref&gt;Siehe zu diesen Regelungen [http://www.dga.org/~/media/Files/Contracts/Agreements/2008%20BA/008ba2008article8.pdf Artikel 8, Abschnitt 8-211 des ''Basic Agreement''] (PDF; 125&amp;nbsp;kB) der DGA von 2008, abgerufen am 25. April 2012.&lt;/ref&gt; Ein Antrag des Regisseurs auf Pseudonymisierung kann abgelehnt werden, so durfte [[Tony Kaye (Regisseur)|Tony Kaye]] den Namen Smithee bei dem Film ''[[American History X]]'' nicht einsetzen, obwohl er den Antrag stellte.

Auch bei nicht-US-amerikanischen Produktionen wird der Name verwendet, wie etwa beim [[Pilotfilm]] der Fernsehserie ''[[Schulm√§dchen (Fernsehserie)|Schulm√§dchen]]''. 2007 sendete die ARD am 8. und 9. August den zweiteiligen TV-Film ''Paparazzo''. Auch in diesem Werk erscheint anstatt des eigentlichen Regisseurs [[Stephan Wagner (Regisseur)|Stephan Wagner]] Alan Smithee im Abspann.

Regisseure, die das Pseudonym benutzt haben:
* [[Don Siegel]] und [[Robert Totten]] (f√ºr ''[[Frank Patch ‚Äì Deine Stunden sind gez√§hlt]]'')
* [[David Lynch]] (f√ºr die dreist√ºndige Fernsehfassung von ''[[Der W√ºstenplanet (Film)|Der W√ºstenplanet]]'')
* [[Chris Christensen]] (''The Omega Imperative'')
* [[Gianni Bozzacchi]] (f√ºr ''I Love N.Y.'')
* [[Stuart Rosenberg]] (f√ºr ''Let‚Äôs Get Harry'')
* [[Richard C. Sarafian]] (f√ºr ''[[Starfire]]'')
* [[Dennis Hopper]] (f√ºr ''[[Catchfire]]'')
* [[Arthur Hiller (Regisseur)|Arthur Hiller]] (f√ºr ''[[Fahr zur H√∂lle Hollywood]]'')
* [[Rick Rosenthal]] (''Die V√∂gel II ‚Äì Die R√ºckkehr'')
* [[Kevin Yagher]] (''[[Hellraiser IV ‚Äì Bloodline]]'')
* [[William Lustig]] (''[[Maniac Cop 3]]'')
</code></pre>
<p>I wrote the following code:</p>
<pre><code>import re
with open('C:\\path\\text.txt', encoding=&quot;unicode_escape&quot;) as f, open('word_list.txt', 'a', encoding=&quot;utf-8&quot;) as f1:
    f1.write('\n'.join(set(re.findall(&quot;[a-zA-Z_√§√∂√º√Ñ√ñ√ú√ü]+&quot;, f.read()))))
</code></pre>
<p>A text editor says I have 741 words, but the output only has approximately 460 words. The output looks like this (this is just a snippet) and it also contains single letters or groups of letters that don't make sense:</p>
<pre><code>Hill
Himbeere
An
never
gepr
Frank
X
Eigener
Thee
Auch
anstatt
com
mehr
fiktiven
</code></pre>
<p>Should I use something else? Should I alter anything in my code? Is there a better way to filter such a messy text?</p>
",Multilingual Language Processing & Language Identification,extracting german word regex large german text file split smaller text file problem want extract word later count many time word appears latter also problem text pretty messy wrote following code text editor say word output ha approximately word output look like snippet also contains single letter group letter make sense use something else alter anything code better way filter messy text
Detect Tense in German Sentence (with SpaCy),"<p>I would like to (programmatically) detect the tense (and mood) of German sentences, preferably with SpaCy. I am able to find the root in the sentence and to determine whether it is a finite verb or not. However, Searching SpaCy's documentation I didn't find a solution to determine the tense. Is this possible with SpaCy, or do I need to create my own solution for this?</p>
<p>If it is possible with SpaCy, how?</p>
<p>If not, what would be a good approach to do this? My first approach would be to discriminate between Perfekt and Plusquamperfekt tense based on the existence of a participle verb form, and to identify Futur by checking if the root is a form of werden and the existence of a dependent infinite verb form, with some extra logic to check for Futur II, analogue to checking for Plusquamperfekt. For discrimination of Pr√§teritum against Pr√§sens I would think of doing a look-up in a verb table. Is that a good idea, or is there a better approach, maybe a prebuilt tool?</p>
<p>I have found this paper: <a href=""https://www.cis.uni-muenchen.de/%7Efraser/pubs/ramm_acldemo2017.pdf"" rel=""nofollow noreferrer"">Annotating tense, mood and voice for English, French and German</a>, but they are not overly explicit how they do it; at least I am unable to reproduce their work.</p>
",Multilingual Language Processing & Language Identification,detect tense german sentence spacy would like programmatically detect tense mood german sentence preferably spacy able find root sentence determine whether finite verb however searching spacy documentation find solution determine tense possible spacy need create solution possible spacy would good approach first approach would discriminate perfekt plusquamperfekt tense based existence participle verb form identify futur checking root form werden existence dependent infinite verb form extra logic check futur ii analogue checking plusquamperfekt discrimination pr teritum pr sen would think look verb table good idea better approach maybe prebuilt tool found paper annotating tense mood voice english french german overly explicit least unable reproduce work
count the occurrences of POS tagging pattern,"<p>So I've applied POS tagging to one of the columns in my dataframe. For each sentence, I want to count the occurrences of this pattern: NNP, MD, VB.</p>
<p>For example, I have the following sentence:
communications between the Principal and the Contractor shall be in the English language</p>
<p>The POS tagging would be:
(communications, NNS), (between,IN), (the, DT), (Principal, NNP), (and, CC), (the, DT), <strong>(Contractor, NNP), (shall, MD), (be,VB)</strong>, (in, DT), (the, DT), (English, JJ), (language, NN).</p>
<p>Notice that in the POS tagging result, the pattern (NNP, MD, VB) exists and occurs 1 time. I'd like to create a new column in the df for this number of occurrences.</p>
<p>Any ideas how I can do this?</p>
<p>Thanks in advance</p>
",Multilingual Language Processing & Language Identification,count occurrence po tagging pattern applied po tagging one column dataframe sentence want count occurrence pattern nnp md vb example following sentence communication principal contractor shall english language po tagging would communication nns dt principal nnp cc dt contractor nnp shall md vb dt dt english jj language nn notice po tagging result pattern nnp md vb exists occurs time like create new column df number occurrence idea thanks advance
how to fine-tune &quot;distiluse-base-multilingual-cased&quot; model for text similarity customisation,"<p>I am trying to do semantic search but pre-trained model is not accurate on Italian grocery data.</p>
<p>eg.</p>
<pre><code>Query: latte al cioccolato  #chocolate milk

Top 3 most similar sentences in the corpus:
Milka  cioccolato al latte 100 g (Score: 0.7714)   #Milka milk chocolate 100 g
Alpro, Cioccolato bevanda a base di soia 1 ltr (Score: 0.5586)  #Alpro, Chocolate soy drink 1 ltr(soya milk)
Danone, HiPRO 25g Proteine gusto cioccolato 330 ml (Score: 0.4569) #Danone, HiPRO 25g Protein chocolate flavor 330 ml(protein chocolate milk) 
</code></pre>
<p>here in the above example, the problem is pre-trained BERT model is not returning context similarity. the result should be in the following order.</p>
<p><strong>Expected result:</strong></p>
<pre><code>Query: latte al cioccolato  #chocolate milk

Top 3 most similar sentences in the corpus:
Alpro, Cioccolato bevanda a base di soia 1 ltr (Score: 0.99)  #Alpro, Chocolate soy drink 1 ltr(soya milk)
Danone, HiPRO 25g Proteine gusto cioccolato 330 ml (Score: 0.95) #Danone, HiPRO 25g Protein chocolate flavor 330 ml(protein chocolate milk)
Milka  cioccolato al latte 100 g (Score: 0.40)   #Milka milk chocolate 100 g
</code></pre>
<p><strong>Fine-Tune Try:</strong></p>
<pre><code>!pip install sentence-transformers
import scipy
import numpy as np
from sentence_transformers import models, SentenceTransformer
model = SentenceTransformer('distiluse-base-multilingual-cased') # workes with Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish


#Fine-Tuning 
import pandas as pd
df = pd.DataFrame({
    &quot;message&quot;:[
          &quot;latte al cioccolato&quot;  ,
          &quot;Alpro, Cioccolato bevanda a base di soia 1 ltr &quot;, #Alpro, Chocolate soy drink 1 ltr
          &quot;Milka  cioccolato al latte 100 g&quot;, #Milka milk chocolate 100 g
          &quot;Danone, HiPRO 25g Proteine gusto cioccolato 330 ml&quot;, #Danone, HiPRO 25g Protein chocolate flavor 330 ml
         ],
    &quot;lbl&quot;:[&quot;liquid&quot;,&quot;liquid&quot;,&quot;chocolate&quot;,&quot;liquid&quot;]
})
df


X=list(df['message'])
y=list(df['lbl'])


y=list(pd.get_dummies(y,drop_first=True)['liquid'])


from transformers import AutoTokenizer, AutoModel
  
tokenizer = AutoTokenizer.from_pretrained(&quot;kiri-ai/distiluse-base-multilingual-cased-et&quot;)
encodings = tokenizer(X, truncation=True, padding=True)


import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(encodings),
    y
))



from transformers import AutoModel, TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=2,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)



with training_args.strategy.scope():
    model = AutoModel.from_pretrained(&quot;kiri-ai/distiluse-base-multilingual-cased-et&quot;)

trainer = TFTrainer(
    model=model,                         # the instantiated ü§ó Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset         # training dataset
)

trainer.train()
</code></pre>
",Multilingual Language Processing & Language Identification,fine tune distiluse base multilingual cased model text similarity customisation trying semantic search pre trained model accurate italian grocery data eg example problem pre trained bert model returning context similarity result following order expected result fine tune try
NLP on powerbi on prem,"<p>I am building a dashboard with power bi on prem (no cloud) and I want to add natural language processing component. What technologies should I be using as nlp is only available for power bi cloud and I cannot use it so I must think of another way, it's like the Q&amp;A cloud solution equivalent for on prem</p>
",Multilingual Language Processing & Language Identification,nlp powerbi prem building dashboard power bi prem cloud want add natural language processing component technology using nlp available power bi cloud use must think another way like q cloud solution equivalent prem
How to convert text into a dataframe in Python (with applying some rules),"<p>I am new to Python and have this complex function that I need to build but do not know how</p>
<p>I have a dataframe of text</p>
<pre><code>RepID     RepText
---------------------------
1         Math Math Math  English Physics Sport Sport English English English English 
2         Sport English English English Math Math Physics Physics Physics Computer Computer Computer Computer 
3         Chemistry Chemistry Math Math Math English English English Math Math Math Math Math Sport Sport
</code></pre>
<p>The function I need to create called fnClusters</p>
<p>It just finds the N repeated words in the RepText and return them in a dataframe</p>
<p>if N is 3 then same words that showed 3 times or more next to each other will be counted</p>
<p>so Math Math Math English Physics English English Math will count as</p>
<pre><code>Math  English  Physics
------------------------
4       0       0
</code></pre>
<p>English English English English English Math Math Math English Math Sports Sports will count as</p>
<pre><code>Math  English  Sports
------------------------
4       6       0
 
</code></pre>
<p>How can I build this function in Python?</p>
",Multilingual Language Processing & Language Identification,convert text dataframe python applying rule new python complex function need build know dataframe text function need create called fnclusters find n repeated word reptext return dataframe n word showed time next counted math math math english physic english english math count english english english english english math math math english math sport sport count build function python
wordninja does not work with other languages,"<p>I have a question that I cant solve alone. I am currently building an NLP preprocessing pipeline and though about using <a href=""https://github.com/keredson/wordninja"" rel=""nofollow noreferrer"" title=""github"">wordninja</a> with cyrilic languages (Russian and Ukrainian) I have set the dictionaries as described and everything seemed to look alright, but I can make it work.</p>
<pre><code>import wordninja
wordninja.DEFAULT_LANGUAGE_MODEL = wordninja.LanguageModel('setup/ru_ninja_dict.txt.gz')
wordninja.split(&quot;–ø—Ä–∏–≤–µ—Ç–ø–æ–∫–∞&quot;)
</code></pre>
<p>(the output is an empty list [], while [&quot;–ø—Ä–∏–≤–µ—Ç&quot;, &quot;–ø–æ–∫–∞&quot;] was expected)</p>
<p>My main assumption is that there is an issue with encodings. However, I do not know how to check it myself.</p>
<p>Please let me know if you have any ideas!</p>
",Multilingual Language Processing & Language Identification,wordninja doe work language question cant solve alone currently building nlp preprocessing pipeline though using wordninja cyrilic language russian ukrainian set dictionary described everything seemed look alright make work output empty list wa expected main assumption issue encoding however know check please let know idea
Measuring co-occurence patterns in media articles over time with Quanteda,"<p>I am trying to measure the number of times that different words co-occur with a particular term in collections of Chinese newspaper articles from each quarter of a year. To do this, I have been using Quanteda and written several R functions to run on each group of articles. My work steps are:</p>
<ol>
<li>Group the articles by quarter.</li>
<li>Produce a frequency co-occurence matrix (FCM) for the articles in each quarter (Function 1).</li>
<li>Take the column from this matrix for the 'term' I am interested in and convert this to a data.frame (Function 2)</li>
<li>Merge the data.frames for each quarter together, then produce a large csv file with a column for each quarter and a row for each co-occurring term.</li>
</ol>
<p>This seems to work okay. But I wondered if anybody more skilled in R might be able to check what I am doing is correct, or might suggest a more efficient way of doing it?</p>
<p>Thanks for any help!</p>
<pre class=""lang-r prettyprint-override""><code>#Function 1 to produce the FCM

get_fcm &lt;- function(data) {
  ch_stop &lt;- stopwords(&quot;zh&quot;, source = &quot;misc&quot;)
  corp = corpus(data)
  toks = tokens(corp, remove_punct = TRUE) %&gt;% tokens_remove(ch_stop)  
  fcm = fcm(toks, context = &quot;window&quot;, window = 1, tri = FALSE)
  return(fcm)
}

&gt;fcm_14q4 &lt;- get_fcm(data_14q4)
&gt;fcm_15q1 &lt;- get_fcm(data_15q1)

#Function 2 to select the column for the 'term' of interest (such as China ‰∏≠ÂõΩ) and make a data.frame

convert2df &lt;- function(matrix, term){
  mat_term = matrix[,term]
  df = convert(mat_term, to = &quot;data.frame&quot;)
  colnames(df)[1] = &quot;Term&quot;
  colnames(df)[2] = &quot;Freq&quot;
  x = df[order(-df$Freq),]
  return(x)
}

&gt;CH14 &lt;- convert2df(fcm_14q4, &quot;‰∏≠ÂõΩ&quot;)
&gt;CH15 &lt;- convert2df(fcm_15q1, &quot;‰∏≠ÂõΩ&quot;)

#Merging the data.frames

df &lt;- merge(x=CH14q4, y=CH15q1, by=&quot;Term&quot;, all.x=TRUE, all.y=TRUE)
df &lt;- merge(x=df, y=CH15q2, by=&quot;Term&quot;, all.x=TRUE, all.y=TRUE) #etc for all the dataframes... 

</code></pre>
<p>UPDATE: Following Ken's advice in the comments below, I have tried doing it a different way, using the window function of tokens_select() and then a document feature matrix. After labelling the corpus documents according to their quarter, the following R function should take the tokenized corpus <code>toks</code> and then produce a data.frame of the number of times words co-occur within a specified <code>window</code> of a <code>term</code>.</p>
<pre class=""lang-r prettyprint-override""><code>COOCdfm &lt;- function(toks, term, window){
  ch_stop = stopwords(&quot;zh&quot;, source = &quot;misc&quot;)
  cooc_toks = tokens_select(toks, term, window = window)
  cooc_toks2 = tokens(cooc_toks, remove_punct = TRUE)
  cooc_toks3 = tokens_remove(cooc_toks2, ch_stop)
  dfmat = dfm(cooc_toks3)
  dfmat_grouped = dfm_group(dfmat, groups = &quot;quarter&quot;)
  counts = convert(t(dfmat_grouped), to = &quot;data.frame&quot;)
  colnames(counts)[1] &lt;- &quot;Feature&quot;
  return(counts)
} 
</code></pre>
",Multilingual Language Processing & Language Identification,measuring co occurence pattern medium article time quanteda trying measure number time different word co occur particular term collection chinese newspaper article quarter year using quanteda written several r function run group article work step group article quarter produce frequency co occurence matrix fcm article quarter function take column matrix term interested convert data frame function merge data frame quarter together produce large csv file column quarter row co occurring term seems work okay wondered anybody skilled r might able check correct might suggest efficient way thanks help update following ken advice comment tried different way using window function token select document feature matrix labelling corpus document according quarter following r function take tokenized corpus produce data frame number time word co occur within specified
Do you have to clean your test data before feeding into an NLP model?,"<p>This is a natural language processing related question.</p>
<p>Suppose I have a labelled train and unlabelled test set. After I have cleaned my train data(stopword, stem, punctuations etc), I use this cleaned data to build my model.</p>
<p>When fitting it on my test data, will I also have to clean the test data text using the same manner as I did with my train set? or should I not touch the test data completly.</p>
<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,clean test data feeding nlp model natural language processing related question suppose labelled train unlabelled test set cleaned train data stopword stem punctuation etc use cleaned data build model fitting test data also clean test data text using manner train set touch test data completly thanks
Any specific python library to perform sentiment analysis for reviews written in german or french please?,"<p>We have reviews written in German and French which needs to be analysed and classified either as positive, neutral or negative based on the sentiment it reflects. We tried some tools which translate the reviews to English but the accuracy wasnt that great since the meaning is lost during translation. Any specific library that can be used in such a case? Any help is highly appreciated. Thanks in advance. </p>
",Multilingual Language Processing & Language Identification,specific python library perform sentiment analysis review written german french please review written german french need analysed classified either positive neutral negative based sentiment reflects tried tool translate review english accuracy wasnt great since meaning lost translation specific library used case help highly appreciated thanks advance
"Extract Acronyms and MƒÅori (non-english) words in a dataframe, and put them in adjacent columns within the dataframe","<p>Regular expression seems a steep learning curve for me. I have a dataframe that contains texts (up to 300,000 rows). The text as contained in <code>outcome</code> column of a dummy file named <code>foo_df.csv</code> has a mixture of English words, acronyms and MƒÅori words. <code>foo_df.csv</code> is as thus:</p>
<pre><code>    outcome
0   I want to go to DHB
1   Self Determination and Self-Management Rangatiratanga
2   mental health wellness and AOD counselling
3   Kai on my table
4   Fishing
5   Support with Oranga Tamariki Advocacy
6   Housing pathway with WINZ
7   Deal with personal matters
8   Referral to Owaraika Health services
</code></pre>
<p>The result I desire is in form of a table below such that has <code>Abreviation</code> and <code>MƒÅori_word</code> columns:</p>
<pre><code>    outcome                                                 Abbreviation     MƒÅori_word             
0   I want to go to DHB                                     DHB      
1   Self Determination and Self-Management Rangatiratanga                    Rangatiratanga
2   mental health wellness and AOD counselling              AOD              
3   Kai on my table                                                          Kai
4   Fishing                                                                  
5   Support with Oranga Tamariki Advocacy                                    Oranga Tamariki
6   Housing pathway with WINZ                               WINZ             
7   Deal with personal matters                                               
8   Referral to Owaraika Health services                                     Owaraika
</code></pre>
<p>The approach I am using is to extract the ACRONYMS using regular expression and extract the MƒÅori words using nltk module.</p>
<p>I have been able to extract the ACRONYMS using regular expression with this code:</p>
<pre><code>pattern = '(\\b[A-Z](?:[\\.&amp;]?[A-Z]){1,7}\\b)'
foo_df['Abbreviation'] = foo_df.outcome.str.extract(pattern)
</code></pre>
<p>I have been able to extract non-english words from a sentence using the code below:</p>
<pre><code>import nltk
nltk.download('words')
from nltk.corpus import words

words = set(nltk.corpus.words.words())

sent = &quot;Self Determination and Self-Management Rangatiratanga&quot;
&quot; &quot;.join(w for w in nltk.wordpunct_tokenize(sent) \
         if not w.lower() in words or not w.isalpha())
</code></pre>
<p>However, I got an error <code>TypeError: expected string or bytes-like object</code> when I tried to iterate the above code over a dataframe. The iteration I tried is below:</p>
<pre><code>def no_english(text):
  words = set(nltk.corpus.words.words())
  &quot; &quot;.join(w for w in nltk.wordpunct_tokenize(text['outcome']) \
         if not w.lower() in words or not w.isalpha())

foo_df['MƒÅori_word'] = foo_df.apply(no_english, axis = 1)
print(foo_df)
   
</code></pre>
<p>Any help in python3 will be appreciated. Thanks.</p>
",Multilingual Language Processing & Language Identification,extract acronym ori non english word dataframe put adjacent column within dataframe regular expression seems steep learning curve dataframe contains text row text contained column dummy file named ha mixture english word acronym ori word thus result desire form table ha column approach using extract acronym using regular expression extract ori word using nltk module able extract acronym using regular expression code able extract non english word sentence using code however got error tried iterate code dataframe iteration tried help python appreciated thanks
How to evaluate pre-trained NER model on my domain specific text with stanza (for coreNLP)?,"<p>I am trying to get F1 scores for the pre-trained English model on my specific text domain without doing any training.</p>
<p>The docs mention the following command:</p>
<pre><code>python -m stanza.utils.training.run_ete ${corpus} --score_${split}
</code></pre>
<p>However as I don't want to do any training, how can I evaluate the model as is?
Also, the format of <code>${corpus}</code> is not stated in the docs.</p>
<p>I've got an annotated dataset for my domain in BIO format.</p>
",Multilingual Language Processing & Language Identification,evaluate pre trained ner model domain specific text stanza corenlp trying get f score pre trained english model specific text domain without training doc mention following command however want training evaluate model also format stated doc got annotated dataset domain bio format
latin bases language segmentation gramatical rules,"<p>I am working on one feature i.e. to apply language segmentation rules (grammatical) for Latin based language (English currently).</p>
<p>Currently I am in phase of breaking sentences of user input.</p>
<pre><code>e.g.:

&quot;I am working in language translation&quot;. &quot;I have used Google MT API for this&quot;
</code></pre>
<p>In above example i will break above sentence by full stop <code>.</code> This is normal cases where I am breaking sentence on dot, but there are n number of characters for breaking sentence like (<code>.</code> <code>!</code> <code>?</code>, etc).</p>
<p>I have following SRX rules for segmentation.</p>
<p><strong>Is there any reference</strong> which I can use for resolving my language segmentation rules?</p>
",Multilingual Language Processing & Language Identification,latin base language segmentation gramatical rule working one feature e apply language segmentation rule grammatical latin based language english currently currently phase breaking sentence user input example break sentence full stop normal case breaking sentence dot n number character breaking sentence like etc following srx rule segmentation reference use resolving language segmentation rule
Extracting age-related info from text,"<p>I am trying to find mention of age in a large dataset of messages posted by users on the internet (stored in a .csv)</p>

<p>I am currently using regular expressions in python to extract age and save it in a list</p>

<p>For example,
    ""I am 20 years old"" would return 20 to the list
    ""He is 30 now"" would return 30
    ""She is in her fifties"" would return 50</p>

<p>But the problem is, using RE is very slow for a huge dataset and if text is in a pattern not satisfied by my RE, then I cannot get the age... So, my question is: Is there a better way of doing this? Perhaps some NLP packages/tools in python?
I tried researching if nltk has something for this, but it doesnt.</p>

<p>ps:Sorry if the question is unclear, english is not my first language..
   I have included some of the RE i used below..</p>

<pre><code>m = re.search(r'.*(I|He|She) (is|am) ([0-9]{2}).*',s,re.IGNORECASE)
n = re.search(r'.*(I|He|She) (is|am) in (my|his|her) (late|mid|early)? ?(tens|twenties|thirties|forties|fifties|sixties|seventies|eighties|nineties|hundreds).*',s,re.IGNORECASE)
o = re.search(r'.*(I|He|She) (is|am) (twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen) ?(one|two|three|four|five|six|seven|eight|nine)?.*',s,re.IGNORECASE)
p = re.search(r'.*(age|is|@|was) ([0-9]{2}).*',s,re.IGNORECASE)
q = re.search(r'.*(age|is|@|was) (twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen) ?(one|two|three|four|five|six|seven|eight|nine)?.*',s,re.IGNORECASE)
r = re.search(r'.*([0-9]{2}) (yrs|years).*',s,re.IGNORECASE)
s = re.search(r'.*(twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen) ?(one|two|three|four|five|six|seven|eight|nine)? (yrs|years).*',s,re.IGNORECASE)
</code></pre>
",Multilingual Language Processing & Language Identification,extracting age related info text trying find mention age large dataset message posted user internet stored csv currently using regular expression python extract age save list example year old would return list would return fifty would return problem using slow huge dataset text pattern satisfied get age question better way perhaps nlp package tool python tried researching nltk ha something doesnt p sorry question unclear english first language included used
Using spacy with archaich/old english words?,"<p>I am using <strong>en_core_web_lg</strong> to compare some texts for similarity and I am not getting the expected results.</p>
<p>The issue I guess is that my texts are mostly religious, for example:
&quot;Thus hath it been decreed by Him Who is the Source of Divine inspiration.&quot;
&quot;He, verily, is the Expounder, the Wise.&quot;
&quot;Whoso layeth claim to a Revelation direct from God, ere the expiration of a full thousand years, such a man is assuredly a lying impostor. &quot;</p>
<p>My question is, is there a way I can check spacy's &quot;dictionary&quot;? Does it include words like &quot;whoso&quot; &quot;layeth&quot; &quot;decreed&quot; or &quot;verily&quot;?</p>
",Multilingual Language Processing & Language Identification,using spacy archaich old english word using en core web lg compare text similarity getting expected result issue guess text mostly religious example thus hath decreed source divine inspiration verily expounder wise whoso layeth claim revelation direct god ere expiration full thousand year man assuredly lying impostor question way check spacy dictionary doe include word like whoso layeth decreed verily
similarity method for italian language in spacy not working,"<p>how can I get the similarity method of spacy for the Italian language?</p>
<p><strong>Italian Code:</strong></p>
<pre><code>import it_core_news_lg
nlp = it_core_news_lg.load()

doc1 = nlp(&quot;vuole AUX aux&quot;)
doc2 = nlp(&quot;Apple PROPN ROOT&quot;)

# Similarity of two documents
print(doc1, &quot;&lt;-&gt;&quot;, doc2, doc1.similarity(doc2))
</code></pre>
<p><strong>return ---&gt;</strong></p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-35-db17c0966749&gt; in &lt;module&gt;()
      1 import it_core_news_lg
----&gt; 2 nlp = it_core_news_lg.load()
      3 
      4 doc1 = nlp(&quot;vuole AUX aux&quot;)
      5 doc2 = nlp(&quot;Apple PROPN ROOT&quot;)

3 frames
/usr/local/lib/python3.7/dist-packages/spacy/language.py in create_pipe(self, name, config)
    298         return SimpleFrozenList(
    299             self._components, error=Errors.E926.format(attr=&quot;components&quot;)
--&gt; 300         )
    301 
    302     @property

KeyError: &quot;[E002] Can't find factory for 'tok2vec'. This usually happens when spaCy calls `nlp.create_pipe` with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to `Language.factories['tok2vec']` or remove it from the model meta and add it via `nlp.add_pipe` instead.&quot;
</code></pre>
<p><strong>English Code:</strong></p>
<pre><code>import spacy
import en_core_web_lg
nlp = en_core_web_lg.load()
doc1 = nlp(&quot;I like salty fries and hamburgers.&quot;)
doc2 = nlp(&quot;Fast food tastes very good.&quot;)

# Similarity of two documents
print(doc1, &quot;&lt;-&gt;&quot;, doc2, doc1.similarity(doc2))
</code></pre>
<p><strong>Returns---&gt;</strong></p>
<pre><code>I like salty fries and hamburgers. &lt;-&gt; Fast food tastes very good. 0.7687607012190486
</code></pre>
",Multilingual Language Processing & Language Identification,similarity method italian language spacy working get similarity method spacy italian language italian code return english code return
Greek alphabet being converted to unicode in dataframe? any packages to support use of different alphabets?,"<p>I created a column containing Greek words, e.g:</p>
<p><code>vocab&lt;-c(&quot;ŒΩŒ±&quot;, &quot;œÑŒø&quot;,&quot;Œ¥ŒµŒΩ&quot;, &quot;ŒµŒØŒΩŒ±Œπ&quot;)</code>
however, when shown in a data frame these words are shown as:</p>
<p>&quot;„Øö, tŒø, deŒΩ, eŒØ„ØöŒπ &quot; respectivley.</p>
<p>Are there any packages that help R cope with the use of foreign alphabets and treat them the same as they would the standard English one? I don't really understand why they don't automatically treat these symbols as normal English letters.</p>
<p>Thanks for your help!</p>
",Multilingual Language Processing & Language Identification,greek alphabet converted unicode dataframe package support use different alphabet created column containing greek word e g however shown data frame word shown de e respectivley package help r cope use foreign alphabet treat would standard english one really understand automatically treat symbol normal english letter thanks help
Predicting correct match of French to English food descriptions,"<p>I have a training and test set of food descriptions pairs (please, see example below)
First name in a pair is a name of food in French
and second word is this food description in English.
Training set has also a <code>trans</code> field that is True for correct descriptions
and False for wrong descriptions.
The task is to predict <code>trans</code> field in a test set, in other words to predict
which food description is correct and which is wrong.</p>
<pre><code>dishes = [{&quot;fr&quot;:&quot;Agneau de lait&quot;, &quot;eng&quot;:&quot;Baby milk-fed lamb&quot;, &quot;trans&quot;: True},
{&quot;fr&quot;:&quot;Agrume&quot;, &quot;eng&quot;:&quot;Blackcurrants&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Algue&quot;, &quot;eng&quot;:&quot;Buttermilk&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Aligot&quot;, &quot;eng&quot;:&quot;potatoes mashed with fresh mountain cheese&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Baba au rhum&quot;, &quot;eng&quot;:&quot;Star anise&quot;, &quot;trans&quot;: True},
{&quot;fr&quot;:&quot;Babeurre&quot;, &quot;eng&quot;:&quot;seaweed&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Badiane&quot;, &quot;eng&quot;:&quot;Sponge cake (often soaked in rum)&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Boeuf bourguignon&quot;, &quot;eng&quot;:&quot;Cr√©ole curry&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Carbonade flamande&quot;, &quot;eng&quot;:&quot;Beef Stew&quot;, &quot;trans&quot;: True},
{&quot;fr&quot;:&quot;Cari&quot;, &quot;eng&quot;:&quot;Beef stewed in red wine&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Cassis&quot;, &quot;eng&quot;:&quot;citrus&quot;, &quot;trans&quot;: False},
{&quot;fr&quot;:&quot;Cassoulet&quot;, &quot;eng&quot;:&quot;Stew from the South-West of France&quot;, &quot;trans&quot;: True},
{&quot;fr&quot;:&quot;C√©leri-rave&quot;, &quot;eng&quot;:&quot;Celery root&quot;, &quot;trans&quot;: True}]

df = pd.DataFrame(dishes)

    fr                  eng                                          trans
0   Agneau de lait      Baby milk-fed lamb                           True
1   Agrume              Blackcurrants                                False
2   Algue               Buttermilk                                   False
3   Aligot              potatoes mashed with fresh mountain cheese   False
4   Baba au rhum        Star anise                                   True
5   Babeurre            seaweed                                      False
6   Badiane             Sponge cake (often soaked in rum)            False
7   Boeuf bourguignon   Cr√©ole curry                                 False
8   Carbonade flamande  Beef Stew                                    True
9   Cari                Beef stewed in red wine                      False
10  Cassis              citrus                                       False
11  Cassoulet           Stew from the South-West of France           True
12  C√©leri-rave         Celery root                                  True
</code></pre>
<p>I think to solve this as text classification problem, where text is a concatenation of French name and English description embeddings.</p>
<p>Questions:</p>
<ul>
<li>Which embeddings to use and how concatenate them?</li>
<li>Any other ideas on approach to this problem? BERT?</li>
</ul>
<p><strong>Update:</strong></p>
<p>How about the following approach:</p>
<ul>
<li>Translate (with BERT?) French names to English</li>
<li>Use embeddings to create two vectors: v1 - translated English vector and v2 - English description vector (from data set)</li>
<li>Compute v1 - v2</li>
<li>Create new data set with two columns: <code>v1 - v2</code> and <code>trans</code></li>
<li>Train classifier on this new data set</li>
</ul>
<p><strong>Update 2:</strong></p>
<p>It looks like <em>cross-lingual classification</em> may be the right solution for my problem:</p>
<p><a href=""https://github.com/facebookresearch/XLM#iv-applications-cross-lingual-text-classification-xnli"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/XLM#iv-applications-cross-lingual-text-classification-xnli</a></p>
<p>It is not clear yet from the description given on the page with the link above, where to fit my own training data set and how to run classifier on my test set. Please help to figure this out. It would be ideal to find end-to-end example / tutorial on cross-lingual classification.</p>
",Multilingual Language Processing & Language Identification,predicting correct match french english food description training test set food description pair please see example first name pair name food french second word food description english training set ha also field true correct description false wrong description task predict field test set word predict food description correct wrong think solve text classification problem text concatenation french name english description embeddings question embeddings use concatenate idea approach problem bert update following approach translate bert french name english use embeddings create two vector v translated english vector v english description vector data set compute v v create new data set two column train classifier new data set update look like cross lingual classification may right solution problem clear yet description given page link fit training data set run classifier test set please help figure would ideal find end end example tutorial cross lingual classification
translator() from Googletrans not translating the texts to English,"<p>I am trying to translate the field Short description to English since some of the rows are not in English. But using the code below I am not able to translate. The translate column and the original columns look exactly the same. Please see the image attached for the output.</p>
<pre><code>from googletrans import Translator
translator = Translator()

mask = data['Short description'] !='en'

data['Short description_translated'] = data['Short description']
f = lambda x: translator.translate(x, dest='en').text
data.loc[mask, 'Short description_translated'] = data.loc[mask, 'Short description'].apply(f)
print (data)
</code></pre>
<p><a href=""https://i.sstatic.net/uE0Rd.png"" rel=""nofollow noreferrer"">Output</a></p>
",Multilingual Language Processing & Language Identification,translator googletrans translating text english trying translate field short description english since row english using code able translate translate column original column look exactly please see image attached output output
Get all possible lemmas for a single word in Spanish (and other non-English languages),"<p>Given a single word I would like to get all the possible lemmas of that word. I am using Spacy 3. The following code will output up to one lemma for a given word. It doesn't find alternatives.</p>
<pre><code>import spacy

# Load Spanish model
nlp = spacy.load(&quot;es_core_news_sm&quot;, disable=['parser', 'ner'])

text = &quot;habla&quot;
doc = nlp(text)

for token in doc:
    print(&quot;{:&lt;12}{:&lt;12}{:&lt;12}{:&lt;12}&quot;.format(token.text, token.pos_, token.tag_, token.lemma_))

# Outputs:
# text = &quot;habla&quot; (you/he/she speaks; speech)
# habla       VERB        VERB        hablar
# &quot;hablar&quot; = to speak (verb). However &quot;habla&quot; also means &quot;speech&quot; (noun)

# text = &quot;como&quot; (I eat; as)
# como        SCONJ       SCONJ       como
# not returning possible verb lemma - &quot;comer&quot; - to eat

# text = &quot;come&quot; (you/he/she eats)
# come        VERB        VERB        come
# lemma should be &quot;comer&quot; not &quot;come&quot;

# text = &quot;sabe&quot; (you/she/he knows)
# sabe        VERB        VERB        saber
# Correct lemmatization of &quot;sabe&quot; 
</code></pre>
<p>In all cases, only one of the possible lemmas is being returned even where there are more possibilities.</p>
<p>Solutions that I have tried:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/67789544/given-a-word-can-we-get-all-possible-lemmas-for-it-using-spacy"">lemminflect</a> - possibly only works for English</li>
<li><a href=""https://stackoverflow.com/questions/64731868/spacy-how-to-manually-set-pos-tag-for-vertical-bar"">This answer</a>, in which you'd set a part of speech tag (and could therefore loop through all possible tags), but that seems to be for an older version of spacy</li>
</ul>
<p>The best alternative solution I have found is to use the <a href=""https://developer.oxforddictionaries.com/documentation#!/Sentences/get_sentences_source_lang_word_id"" rel=""nofollow noreferrer"">Oxford Dictionaries API</a> - that only supports a few languages - it doesn't provide lemmas for French for example.</p>
",Multilingual Language Processing & Language Identification,get possible lemma single word spanish non english language given single word would like get possible lemma word using spacy following code output one lemma given word find alternative case one possible lemma returned even possibility solution tried set part speech tag could therefore loop possible tag seems older version spacy best alternative solution found use oxford dictionary api support language provide lemma french example
Analyze semantic generality of sentence with python,"<p>I'm looking to analyze how specific a statement is. I've checked out packages like NLTK but haven't found anything that seems to fit. I'm looking for something that can give an English sentence a score of how specific or general it is.</p>
<p>An example of a specific sentence:</p>
<p>&quot;The box is green and weighed one pound last week.&quot;</p>
<p>And example of a general sentence:</p>
<p>&quot;Red is a color.&quot;</p>
<p>Any suggestions or ideas?</p>
",Multilingual Language Processing & Language Identification,analyze semantic generality sentence python looking analyze specific statement checked package like nltk found anything seems fit looking something give english sentence score specific general example specific sentence box green weighed one pound last week example general sentence red color suggestion idea
English dictionary as txt or xml file with support of synonyms,"<p>Can someone point me to where I can download English dictionary as a txt or xml file. I am building a simple app for myself and looking for something what I could start using immediately without learning complex API.</p>

<p>Support for synonyms would be great, that is it should be easier to retrieve all the synonyms for a particular word.</p>

<p>It would be absolutely fantastic if the dictionary would be listing British and American spelling of the words where they differ.</p>

<p>Even if it would be small dictionary (a few thousand words) that's OK, I only need it for a small project.</p>

<p>I even would be willing to buy one if the price is reasonable, and the dictionary is easy to use - simple XML would be great.</p>

<p>Any directions please.</p>
",Multilingual Language Processing & Language Identification,english dictionary txt xml file support synonym someone point download english dictionary txt xml file building simple app looking something could start using immediately without learning complex api support synonym would great easier retrieve synonym particular word would absolutely fantastic dictionary would listing british american spelling word differ even would small dictionary thousand word ok need small project even would willing buy one price reasonable dictionary easy use simple xml would great direction please
How to parse a lisp-readable file of property lists in Python,"<p>I am Trying to parse a verbs english lexicon in order to built a NLP application using Python, so I have to merge it with my NLTK scripts, the lexicon is a lisp-readable file of property lists, but I need it in a easier formart like a Json file or a pandas dataframe.</p>
<p>An example from that Lexicon database is:</p>
<pre><code>;; Grid: 51.2#1#_th,src#abandon#abandon#abandon#abandon+ingly#(1.5,01269572,01188040,01269413,00345378)(1.6,01524319,01421290,01524047,00415625)###AD

(
 :DEF_WORD &quot;abandon&quot;
 :CLASS &quot;51.2&quot;
 :WN_SENSE ((&quot;1.5&quot; 01269572 01188040 01269413 00345378)
            (&quot;1.6&quot; 01524319 01421290 01524047 00415625))
 :PROPBANK (&quot;arg1 arg2&quot;)
 :THETA_ROLES ((1 &quot;_th,src&quot;))
 :LCS (go loc (* thing 2)
          (away_from loc (thing 2) (at loc (thing 2) (* thing 4)))
          (abandon+ingly 26))
 :VAR_SPEC ((4 :optional) (2 (animate +)))
)

;; Grid: 45.4.a#1#_ag_th,instr(with)#abase#abase#abase#abase+ed#(1.5,01024949)(1.6,01228249)###AD

(
 :DEF_WORD &quot;abase&quot;
 :CLASS &quot;45.4.a&quot;
 :WN_SENSE ((&quot;1.5&quot; 01024949)
            (&quot;1.6&quot; 01228249))
 :PROPBANK (&quot;arg0 arg1 arg2(with)&quot;)
 :THETA_ROLES ((1 &quot;_ag_th,instr(with)&quot;))
 :LCS (cause (* thing 1)
       (go ident (* thing 2)
           (toward ident (thing 2) (at ident (thing 2) (abase+ed 9))))
       ((* with 19) instr (*head*) (thing 20)))
 :VAR_SPEC ((1 (animate +)))
)
</code></pre>
<p>The complete data is avaible here <a href=""https://raw.githubusercontent.com/ihmc/LCS/master/verbs-English.lcs"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/ihmc/LCS/master/verbs-English.lcs</a></p>
<p>I have tried the idea published in this post <a href=""https://stackoverflow.com/questions/14058985/parsing-a-lisp-file-with-python"">Parsing a lisp file with Python</a> using something like this, but I have obtained a format not as similar as I am looking for it</p>
<pre><code>inputdata = '''
(
 :DEF_WORD &quot;abandon&quot;
 :CLASS &quot;51.2&quot;
 :WN_SENSE ((&quot;1.5&quot; 01269572 01188040 01269413 00345378)
            (&quot;1.6&quot; 01524319 01421290 01524047 00415625))
 :PROPBANK (&quot;arg1 arg2&quot;)
 :THETA_ROLES ((1 &quot;_th,src&quot;))
 :LCS (go loc (* thing 2)
          (away_from loc (thing 2) (at loc (thing 2) (* thing 4)))
          (abandon+ingly 26))
 :VAR_SPEC ((4 :optional) (2 (animate +)))
)


(
 :DEF_WORD &quot;abase&quot;
 :CLASS &quot;45.4.a&quot;
 :WN_SENSE ((&quot;1.5&quot; 01024949)
            (&quot;1.6&quot; 01228249))
 :PROPBANK (&quot;arg0 arg1 arg2(with)&quot;)
 :THETA_ROLES ((1 &quot;_ag_th,instr(with)&quot;))
 :LCS (cause (* thing 1)
       (go ident (* thing 2)
           (toward ident (thing 2) (at ident (thing 2) (abase+ed 9))))
       ((* with 19) instr (*head*) (thing 20)))
 :VAR_SPEC ((1 (animate +)))
)'''

from pyparsing import OneOrMore, nestedExpr

data = OneOrMore(nestedExpr()).parseString(inputdata)
print (data)
</code></pre>
<p>I got an output like this:</p>
<pre><code>[
  [ ':DEF_WORD', '&quot;abandon&quot;', 
    ':CLASS', '&quot;51.2&quot;', 
    ':WN_SENSE', [
                    ['&quot;1.5&quot;', '01269572', '01188040', '01269413', '00345378'], 
                    ['&quot;1.6&quot;', '01524319', '01421290', '01524047', '00415625']
                 ],
    ':PROPBANK', ['&quot;arg1 arg2&quot;'],
    ':THETA_ROLES', [['1', '&quot;_th,src&quot;']],
    ':LCS', ['go', 'loc', ['*', 'thing', '2'], 
          ['away_from', 'loc', ['thing', '2'], 
          ['at', 'loc', ['thing', '2'], ['*', 'thing', '4']]], ['abandon+ingly', '26']],
    ':VAR_SPEC', [['4', ':optional'], ['2', ['animate', '+']]]]
  ,     
  [':DEF_WORD', '&quot;abase&quot;', 
    ':CLASS', '&quot;45.4.a&quot;', 
    ':WN_SENSE', [
                    ['&quot;1.5&quot;', '01024949'],
                    ['&quot;1.6&quot;', '01228249']
                ], 
    ':PROPBANK', ['&quot;arg0 arg1 arg2(with)&quot;'], 
    ':THETA_ROLES', [['1', '&quot;_ag_th,instr(with)&quot;']],
    ':LCS', ['cause', ['*', 'thing', '1'], 
              ['go', 'ident', ['*', 'thing', '2'], 
              ['toward', 'ident', ['thing', '2'], 
              ['at', 'ident', ['thing', '2'],
              ['abase+ed', '9']]]],
              [['*', 'with', '19'], 'instr', ['*head*'], ['thing', '20']]], 
    ':VAR_SPEC', [['1', ['animate', '+']]]
  ]
]
</code></pre>
<p>I am not sure how to handle this output format in order to get e.g <em>THETA_ROLES</em> value or another verbs characteristics in this lexicon, I have all my sentences in an array using pandas and NLTK so the idea is to look for sentences that have a kind of verbs with and especific THETA_ROLES value or other characteristics present in this lexicon.</p>
",Multilingual Language Processing & Language Identification,parse lisp readable file property list python trying parse verb english lexicon order built nlp application using python merge nltk script lexicon lisp readable file property list need easier formart like json file panda dataframe example lexicon database complete data avaible tried idea published post href lisp file python using something like obtained format similar looking got output like sure handle output format order get e g theta role value another verb characteristic lexicon sentence array using panda nltk idea look sentence kind verb especific theta role value characteristic present lexicon
What are alternatives to WordNet for finding antonyms?,"<p>I am looking for a resource where I can find antonyms to certain English words. WordNet would've been perfect, but it simply doesn't seem to have many antonyms in its database.</p>

<p>I used the <code>nltk</code> python module's WordNet implementation to find antonyms, from each lemma for each synset of the original word. But it simply doesn't provide enough words.</p>

<pre class=""lang-py prettyprint-override""><code>for synset in wordnet.synsets(word):
   for lemma in synset.lemmas():
      if lemma.antonyms():
         antonyms.append(l.antonyms()[0].name())
</code></pre>

<p>For example, the only antonym to the word '<strong>delete</strong>' is '<strong>recover</strong>' according to WordNet. But in sites like <a href=""https://thesaurus.com"" rel=""nofollow noreferrer"">https://thesaurus.com</a> , it provides more words like '<strong>add</strong>',
'<strong>restore</strong>', etc.</p>
",Multilingual Language Processing & Language Identification,alternative wordnet finding antonym looking resource find antonym certain english word wordnet would perfect simply seem many antonym database used python module wordnet implementation find antonym lemma synset original word simply provide enough word example antonym word delete recover according wordnet site like provides word like add restore etc
How to perform Paragraph boundary detection in NLP frameworks?,"<p>I am working on extracting names of people from various ads appearing in English newspapers . </p>

<p>However , i have noticed that I need to identify the boundary of an Ad , before extracting the names present in it ,since I need only the first occurring name to be extracted .I started with Stanford NLP . I was successful in extracting names . But I got stuck in identifying the paragraph boundary.</p>

<p>Is there any way of  identifying the paragraph boundary . ?</p>
",Multilingual Language Processing & Language Identification,perform paragraph boundary detection nlp framework working extracting name people various ad appearing english newspaper however noticed need identify boundary ad extracting name present since need first occurring name extracted started stanford nlp wa successful extracting name got stuck identifying paragraph boundary way identifying paragraph boundary
Translating text from english to Italian using hugging face Helsinki models not fully translating,"<p>I'm a newbie going through the hugging face library trying out the Translation models for a data entry task and translating text from English to Italian.</p>
<p>The code I tried based on the documentation:</p>
<pre><code>from transformers import MarianTokenizer, MarianMTModel
from typing import List

#src = 'en'  # source language
#trg = 'it'  # target language
#saved the model locally.
#model_name = f'Helsinki-NLP/opus-mt-{src}-{trg}'
#model.save_pretrained(&quot;./model_en_to_it&quot;)
#tokenizer.save_pretrained(&quot;./tokenizer_en_to_it&quot;)


model = MarianMTModel.from_pretrained('./model_en_to_it')
tokenizer = MarianTokenizer.from_pretrained('./tokenizer_en_to_it')

#Next, trying to iterate over each column - 'english_text' of the dataset and 
#translate the text from English to Italian and append the translated text to the 
#list 'italian'.
 
italian = []
for i in range(len(data)):   
    batch = tokenizer(dataset['english_text'][i], 
                      return_tensors=&quot;pt&quot;,truncation=True, 
                      padding = True)
    gen = model.generate(**batch)
    italian.append(tokenizer.batch_decode(gen, skip_special_tokens=True))
</code></pre>
<p>Two concerns over here:</p>
<ol>
<li>Translates and appends only partial text i.e., it truncates the paragraph if it exceeds a certain length. How to translate the text given any length?</li>
<li>I have near about 10k data and it is taking a hell of a lot of time.</li>
</ol>
<p>Even if any one of the problem could be solved, that's helpful. Would love to learn</p>
",Multilingual Language Processing & Language Identification,translating text english italian using hugging face helsinki model fully translating newbie going hugging face library trying translation model data entry task translating text english italian code tried based documentation two concern translates appends partial text e truncates paragraph exceeds certain length translate text given length near k data taking hell lot time even one problem could solved helpful would love learn
How to handle auto direction of text with dart-flutter?,"<p>I need to make <strong>auto-detection for posts language</strong> to change the text direction according to language, I Used <em>'firebase_mlkit_language.dart'</em> to make the auto-detection of language.</p>
<p>this is a screenshot of screen:
<a href=""https://i.sstatic.net/CSxLg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CSxLg.jpg"" alt=""enter image description here"" /></a></p>
<p>I need the English post alignment to be from ltr.</p>
<p><strong>this Function I made to check lang</strong></p>
<pre><code>Future&lt;String&gt; checkLang(String text)async{
  final LanguageIdentifier languageIdentifier = FirebaseLanguage.instance.languageIdentifier();
  final List&lt;LanguageLabel&gt; labels = await languageIdentifier.processText(text);
  for (LanguageLabel label in labels) {
    final String text = label.languageCode;
    final double confidence = label.confidence;
  }
  return labels[0].languageCode;
}
</code></pre>
<p>while <em>getposts</em> function is processing and get the posts from firebase, <strong>I added this line  to get a list of posts language</strong></p>
<pre><code> postsLang.add(checkLang(PostModel.fromJson(element.data()).text).toString());
</code></pre>
<p>then while generating the posts UI, in the post widget i made this condition to check whether the language is Arabic <em>'ar'</em>
so, the direction should be from right to left.</p>
<pre><code>textDirection: AppCubit.get(context).postsLang[index].toString() == 'ar' ?TextDirection.rtl:TextDirection.ltr,
</code></pre>
",Multilingual Language Processing & Language Identification,handle auto direction text dart flutter need make auto detection post language change text direction according language used firebase mlkit language dart make auto detection language screenshot screen need english post alignment ltr function made check lang getposts function processing get post firebase added line get list post language generating post ui post widget made condition check whether language arabic ar direction right left
"When I use transformer model to train a translator, why it cannot convert y to a tensor","<pre><code>def encoder():
    input_layer = Input(batch_shape=(None, 13, 128))
    h= layer(input_layer)
    h= Masking(mask_value=0.0)(h)
    h, hidden_layer, cell_layer = LSTM(512, return_state=True)(h)
    model = Model(inputs = input_layer, outputs = [hidden_layer, cell_layer])
    return model
model=encoder()
model.summary()

class Decoder(Model):
    def __init__(self):
        super(Decoder, self).__init__()
        self.embedding_layer = Embedding(input_dim=max_tokens+1, output_dim=128, mask_zero=True)
        self.lstm_layer = LSTM(512,
        return_state=True, return_sequences=True)
        self.dense_layer = Dense(units=max_tokens+1)
    def call(self,inputer,hidden_layer=None,cell_layer=None):
        x=self.embedding_layer(inputer)
        if hidden_layer!=None and cell_layer!=None:
            x, h, c = self.lstm_layer(x, initial_state=[hidden_layer, cell_layer])
        else:
             x, h, c = self.lstm_layer(x)
        x=self.dense_layer(x)
        return x,h,c
        
decoder=Decoder()
for eng,germ in train.take(1):
    y,hidden,cell = decoder(germ)
@tf.function
def loss_fn(en_input, germ_input, germ_output, loss):
    with tf.GradientTape() as tape:
        enc_hidden_s, enc_cell_s = model(en_input)
        dec_output, dec_hidden_s, dec_cell_s = decoder(germ_input, enc_hidden_s,enc_cell_s)
        loss_value = loss(germ_output, dec_output)
        return loss_value, tape.gradient(loss_value, variables)
 
def fit_german_shape(german):
    input_data = german[:,:-1]
    output_data = german[:,1:]
    return input_data,output_data
def training(train_data, test_data,optimizer, loss,epochs=5):
    batch_num=0
    batch_num2=0
    epoch_loss=0
    epoch_loss2=0
    for english,germany in train:
        germany_in,germany_out=fit_german_shape(germany)
        loss2, grad= loss_fn(english, germany_in, germany_out, loss)
        optimizer.apply_gradients(zip(grad,model.trainable_variables + decoder.trainable_variables))
        epoch_loss=epoch_loss+loss2
        batch_num=batch_num+1
        avg_loss=epoch_loss/batch_num
        avg_loss3=String(avg_loss1)
        print(&quot;In this train epoch, the loss is&quot;+ave_loss3)
    for english2,germany2 in test:
        germany_in2,germany_out2=fit_german_shape(germany2)
        hidden_state,cell_state=model(en)
        pred,temp1,temp2=decoder(germany_in2,hidden_state,cell_state)
        loss, temp3 = loss_fn(english2, germany_in2, germany_out2)
        epoch_loss2=loss+epoch_loss2
        batch_num=batch_num+1
        avg_loss2=epoch_loss2/batch_num2
        avg_loss4=String(avg_loss2)
        print(&quot;In this test epoch, the loss is&quot;+ave_loss4)
    return avg_loss,avg_loss2
</code></pre>
<p>When I use this model to translate German to English, it report the error that &quot;Tried to convert 'y' to a tensor and failed. Error: None values not supported.&quot; Error may occur in the decoder to assign value to x,h,c, but I dont know why cannot convert y to a tensor.</p>
",Multilingual Language Processing & Language Identification,use transformer model train translator convert tensor use model translate german english report error tried convert tensor failed error none value supported error may occur decoder assign value x h c dont know convert tensor
Langdetect: No Features in Text,"<p>I have a dataframe with text in each row (about 40,000) and I need to identify the language for each and then filter for English. I am having issues though, when I come across some fields in there that have only numbers. I understand the reason for the error, but the issue is that as soon as it comes across one of these fields, then the whole thing fails.</p>
<p>Is there a way to handle these errors where it can just label it as &quot;error&quot; or something like it and then move past and continue to process the rest of the records?</p>
<p>here is the code i have now. It identifies which record has an error, but then every other record after will fail, and nothing gets processed</p>
<pre><code>language = []
for _, row in df.iterrows():
   try:
       language.append(detect(str(row['text'])))
   except:
       language = 'error'
       print('this row has error', row[0])
df.loc[:, 'language'] = language
 
</code></pre>
",Multilingual Language Processing & Language Identification,langdetect feature text dataframe text row need identify language filter english issue though come across field number understand reason error issue soon come across one field whole thing fails way handle error label error something like move past continue process rest record code identifies record ha error every record fail nothing get processed
How to get verb forms of a verb in Spanish language in Python?,"<p>I am working on a project where I have to plug in verb forms of different Spanish verbs into a text. Is there any library that will allow me to do this?
Or is there any CSV file or pdf which I can read into my code and get the verb forms from there? Any help would be appreciated, thanks.</p>
",Multilingual Language Processing & Language Identification,get verb form verb spanish language python working project plug verb form different spanish verb text library allow csv file pdf read code get verb form help would appreciated thanks
Removing from a text sequences of items from a list,"<p>I'm conducting research on code-switching. I have a collection of bilingual Polish text messages with English code-switches (say, corpus A) as well as an English dictionary (also a list, corpus B). I wanted to extract from corpus A all instances of words from corpus B - this way I could see which English words appeared in the bilingual corpus. This is the code I used to create a list of these common words (it's far from elegant, but I'm a novice, so don't be too harsh on me haha):</p>
<pre><code>
intersection=common.intersection(corpusB)

commonlist=list(intersection)

with open(&quot;commonlist.txt&quot;,&quot;w&quot;) as z:

    print(commonlist, file=z)
</code></pre>
<p>However, I noticed that a large portion of my data is skewed because it contains words that are irrelevant to my research. For example, some text messages had large blocks of English text (e.g. copy-pasted paragraphs of English articles - so not really code-switching but quotations). So I'd like to get rid of all of these large blocks of English text from corpus A.</p>
<p>What I thought I should do was locate and delete any text messages that include, say, five English words in a sequence (these would be my big chunks of English text). In other words, I want to scan corpus A for messages that include four adjacent words from corpus B. How can I go about doing that?</p>
<p>(I also have .csv files of both corpora, might be more useful perhaps?)</p>
",Multilingual Language Processing & Language Identification,removing text sequence item list conducting research code switching collection bilingual polish text message english code switch say corpus well english dictionary also list corpus b wanted extract corpus instance word corpus b way could see english word appeared bilingual corpus code used create list common word far elegant novice harsh haha however noticed large portion data skewed contains word irrelevant research example text message large block english text e g copy pasted paragraph english article really code switching quotation like get rid large block english text corpus thought wa locate delete text message include say five english word sequence would big chunk english text word want scan corpus message include four adjacent word corpus b go also csv file corpus might useful perhaps
"identify elements with specific language, f.e. chinese","<p>I have a dataset that looks simplified similar to this:</p>
<pre><code>call_id&lt;- c(&quot;001&quot;,&quot;002&quot;,&quot;003&quot;,&quot;004&quot;,&quot;005&quot;,&quot;012&quot;,&quot;024&quot;)
transcript &lt;- c(&quot;All the best and happy birthday&quot;,
                &quot;‰∏á‰∫ãÂ¶ÇÊÑèÔºåÁîüÊó•Âø´‰πê&quot;,
                &quot;See you tomorrow&quot;,
                &quot;Nice hearing from you&quot;,
                &quot;ÂÜçÁõ∏ËßÅ&quot;,
                &quot;Áé©&quot;,
                &quot;ÊÅ≠Âñú‰Ω† &quot;)
df &lt;- as.data.frame(cbind(call_id, transcript))
</code></pre>
<p>I need a code that gives me the call_id or row numbers for the observations where the transcript column includes chinese language. My final goal is to exclude the rows where the transcript column contains chinese language. As I have a data set with 250,000 observation, obviously it must be a code that does this automatically, not one that does this by hand for this small data set. I have already done some analysis with Quanteda. Is there any possibility in Quanteda for this ? Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,identify element specific language f e chinese dataset look simplified similar need code give call id row number observation transcript column includes chinese language final goal exclude row transcript column contains chinese language data set observation obviously must code doe automatically one doe hand small data set already done analysis quanteda possibility quanteda thanks advance
Using Alexa to parse English to SQL using user input from a web app,"<p>I am wondering if it is possible to convert English to SQL by using Alexa to process it. I want to let users type information such as 'cars within the zip code 12345 that are red' and have it query my sql-server database.</p>
<p>My ideal workflow is:</p>
<pre><code>1. User input on web app is sent to Alexa
2. Alexa processes the sentence
3. Alexa sends it to server backend for processing
4. Server output displays on screen
</code></pre>
<p>I've researched this for a while and it seems that it is possible using the Alexa Skills Kit but that would be for using an Alexa device directly. I am interested in only using the processing logic for Alexa.</p>
<p>Essentially, I want to switch the input to Alexa from voice to user input on a web app. If this isn't possible, then I am open to any other approaches to my problem.</p>
",Multilingual Language Processing & Language Identification,using alexa parse english sql using user input web app wondering possible convert english sql using alexa process want let user type information car within zip code red query sql server database ideal workflow researched seems possible using alexa skill kit would using alexa device directly interested using processing logic alexa essentially want switch input alexa voice user input web app possible open approach problem
ValueError: not a built-in stop list: indonesian,"<p>I have an error on this called <code>ValueError: not a built-in stop list: indonesian.</code></p>
<pre><code>#Function to ngram
def get_top_n_gram(corpus,ngram_range,n=None):
    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'indonesian').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

#n2_bigram
n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),20)

n2_bigrams
</code></pre>
<p>Already search in stack overflow but still don't really understand it. In stack overflow told that ‚ÄòEnglish‚Äô is currently the only supported string value. in reference that a person in stack overflow gave told that many language are available and Indonesian is one of them, but why the error says not a built-in ?? how to use stop words in Indonesian cause dataset that I use is in Indonesian.</p>
",Multilingual Language Processing & Language Identification,valueerror built stop list indonesian error called already search stack overflow still really understand stack overflow told english currently supported string value reference person stack overflow gave told many language available indonesian one error say built use stop word indonesian cause dataset use indonesian
Data transmission between two different notebooks in google colab,"<p>I have 2 colab notebooks. First one for real-time video processing via webcam (using Yolov4 Darknet). Second one for NLP, it's like a voice assistant. Now i want to send outputs of video procssing to NLP notebook. I have to present the objects detected by Yolo(live) as audio feedback when needed. Is is it possible? Can you give me any ideas about this. Thanks!</p>
<p>Note: Yolo live detection and NLP work properly independently of each other.</p>
<p>(Btw sorry for my poor english :))</p>
",Multilingual Language Processing & Language Identification,data transmission two different notebook google colab colab notebook first one real time video processing via webcam using yolov darknet second one nlp like voice assistant want send output video procssing nlp notebook present object detected yolo live audio feedback needed possible give idea thanks note yolo live detection nlp work properly independently btw sorry poor english
How to find singular in the plural when some letters change? What is the best approach?,"<p>How can I find the singular in the plural when some letters change?</p>
<p>Following situation:</p>
<ul>
<li>The German word <code>Schlie√üfach</code> is a lockbox.</li>
<li>The plural is <code>Schlie√üf√§cher.</code></li>
</ul>
<p>As you see, the letter <code>a</code> has changed in <code>√§</code>. For this reason, the first word is not a substring of the second one anymore, they are &quot;regex-technically&quot; different.</p>
<p>Maybe I'm not in the right corner with my chosen tags below. Maybe Regex is not the right tool for me. I've seen <code>naturaljs</code> (<code>natural.NounIflector()</code>) provides this functionality out of the box for English words. Maybe there are also solutions for the German language in the same way?</p>
<p>What is the best approach, how can I find singular in the plural in German?</p>
",Multilingual Language Processing & Language Identification,find singular plural letter change best approach find singular plural letter change following situation german word lockbox plural see letter ha changed reason first word substring second one anymore regex technically different maybe right corner chosen tag maybe regex right tool seen provides functionality box english word maybe also solution german language way best approach find singular plural german
How to extract all types of nouns in Java?,"<p>I want to get all types of nouns from a text how can I get?  </p>

<pre><code>import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import java.io.BufferedReader;
import java.io.FileReader;


public class Noun_Code {


    public static void main(String[] args) {
        try{

            FileReader file = new FileReader(""C:\\Users\\NaB33L NaQ33B!\\Desktop\\TaggerDemo.java"");
            @SuppressWarnings(""resource"")
            BufferedReader reader = new BufferedReader(file);

            String text = """";
            String line = reader.readLine();
            while(line!=null){
                text +=line;
                line = reader.readLine();
            }
            System.out.println(text);
            String tagged;

            MaxentTagger LibAddress =  new MaxentTagger(""F:\\stanford-postagger-2015-04-20\\stanford-postagger-2015-04-20\\models/english-left3words-distsim.tagger"");
            tagged = LibAddress.tagString(text);

            System.out.println(""Frequency : ""+tagged);

            String[] words = tagged.split("" "");

            String[] keyword1 = new String[words.length];
            int len=keyword1.length;
        for(int i = 0;i&lt;words.length;i++)
        {
            int length= words[i].length();
            char chr1 = (char) (words[i].charAt(length-3));
            char chr2 = (char) (words[i].charAt(length-2));
            char chr3 = (char) (words[i].charAt(length-1));
            if(chr1=='N' &amp;&amp; chr2=='N' &amp;&amp; chr3=='P')
            {
            keyword1[i] = words[i]; 
            System.out.println(keyword1[i]);
            }
            else
            {
            keyword1[i] = ""-1"";
            }
        }
            int var =0;
        for(int i = 0;i&lt;keyword1.length;i++)
        {
            if(keyword1[i].equalsIgnoreCase(""-1""))
            {
            var=var+1;
            }
        }
            len=len-var;
            String[] original = new String[len];
            String[] temp = new String[len];
            int e=0;
        for(int i = 0;i&lt;keyword1.length;i++)
        {
            if(keyword1[i].equalsIgnoreCase(""-1"")){}
            else
            {original[e] = keyword1[i];
            temp[e] = keyword1[i];
            e=e+1;
            }
        }
        }
        catch(Exception ex){System.out.println(""Exception :""+ex);}
}
}   
</code></pre>

<p>Please guide me, to get all types of nouns. above is all the code which I am using. I want all the below noun types:
<strong>Common noun</strong>
A common noun is a noun that refers to people or things in general, e.g. boy, country, bridge, city, birth, day, happiness.</p>

<p><strong>Proper noun</strong>
A proper noun is a name that identifies a particular person, place, or thing, e.g. Steven, Africa, London, Monday. In written English, proper nouns begin with capital letters.</p>

<p><strong>Concrete noun</strong>
A concrete noun is a noun which refers to people and to things that exist physically and can be seen, touched, smelled, heard, or tasted. Examples include dog, building, coffee, tree, rain, beach, tune.</p>

<p><strong>Abstract noun</strong>
An abstract noun is a noun which refers to ideas, qualities, and conditions - things that cannot be seen or touched and things which have no physical reality, e.g. truth, danger, happiness, time, friendship, humour.</p>

<p><strong>Collective nouns</strong>
Collective nouns refer to groups of people or things, e.g. audience, family, government, team, jury. In American English, most collective nouns are treated as singular, with a singular verb: The whole family was at the table.</p>
",Multilingual Language Processing & Language Identification,extract type noun java want get type noun text get please guide get type noun code using want noun type common noun common noun noun refers people thing general e g boy country bridge city birth day happiness proper noun proper noun name identifies particular person place thing e g africa london monday written english proper noun begin capital letter concrete noun concrete noun noun refers people thing exist physically seen touched smelled heard tasted example include dog building coffee tree rain beach tune abstract noun abstract noun noun refers idea quality condition thing seen touched thing physical reality e g truth danger happiness time friendship humour collective noun collective noun refer group people thing e g audience family government team jury american english collective noun treated singular singular verb whole family wa table
Python PDF to TXT local character,"<p>I am tring to convert PDF to text file. But the characters which are unique to the language of the PDF file, I mean the ones not in English, are lost in conversion.</p>
<p>How can I convert PDF to text without losing any data?</p>
<p>Thanks in advance.</p>
<pre><code>import PyPDF2 

filename = 'Book1.pdf' 
pdfFileObj = open(filename,'rb')
pdfReader = PyPDF2.PdfFileReader(pdfFileObj)

str = &quot;&quot;
for i in range(pdfReader.getNumPages()):
    str += pdfReader.getPage(i).extractText()

with open(&quot;text2.txt&quot;, 'w', encoding = 'utf-8') as f:
    f.write(str)
</code></pre>
",Multilingual Language Processing & Language Identification,python pdf txt local character tring convert pdf text file character unique language pdf file mean one english lost conversion convert pdf text without losing data thanks advance
python dual for loops does not provide the expected results,"<p>I am new to python . i am trying to run the below code but the results are not as expected:</p>
<pre><code>c = [0,1,2,3,4]
clus = [c0,c1,c2,c3,c4] #each element in the list is a dataframe
for i in c:
    Movie = data.Title[data.labels == i]
    for j in clus:
        vect = CountVectorizer(stop_words='english',max_features=5)
        cv_fit = vect.fit_transform(j).toarray()
        key_features = vect.get_feature_names()
    print(&quot;Cluster&quot;,i,&quot;details:&quot;)
    print('-'*80)
    print(&quot;Key Features:&quot;, key_features)
    print(&quot;Movies in the cluster:&quot;)
    print(Movie)
    print(&quot;Movies in the cluster:&quot;,i)
    print(' ')
    print(' ')  
</code></pre>
<p>Expected Output:</p>
<pre><code>
Cluster 0 details:
--------------------
Key features: ['water', 'on the', 'her', 'while', 'she']
Movies in this cluster:
One Flew Over the Cuckoo's Nest, The Sound of Music, Star Wars, Chinatown, The Bridge on the River Kwai, Apocalypse Now, Jaws, The Good, the Bad and the Ugly, Butch Cassidy and the Sundance Kid
========================================
Cluster 1 details:
--------------------
Key features: ['her', 'she', 'about', 'to her', 'that she']
Movies in this cluster:
Gone with the Wind, The Wizard of Oz, Titanic, Psycho, Sunset Blvd., Vertigo
========================================

and so on .... 

</code></pre>
<p>But my Current Output is:</p>
<pre><code>Cluster 0 details:
--------------------------------------------------------------------------------
Key Features: ['water', 'on the', 'her', 'while', 'she']
Movies in the cluster:
0     One Flew Over the Cuckoo's Nest
1     The Sound of Music
3     Star Wars
4     Chinatown
6    The Bridge on the River Kwai
93    Apocalypse Now
94    Jaws
95    The Good
97    the Bad and the Ugly
99    Butch Cassidy and the Sundance Kid
Name: Title, Length: 67, dtype: object
 

Cluster 1 details:
--------------------------------------------------------------------------------
Key Features: ['water', 'on the', 'her', 'while', 'she']
Movies in the cluster:
7     Gone with the Wind
56    The Wizard of Oz
85    Titanic
89    Psycho
92    Sunset Blvd
100   Vertigo
Name: Title, dtype: object
 
and so on ...
</code></pre>
<p>Key features remains the same for all the clusters. What should i adjust in my code so that my
key features also changes for different clusters.</p>
<pre><code>data.head(2) looks like the below:

       Title       |          Synopsis                |    Labels |
     --------------------------------------------------------------
0    |The Godfather|Guests are gathered last summer...|       0   |
1    |Raging Bull  |The film opens in 1964 ....       |       1
    
</code></pre>
<p>CountVectorizer is an algorithmn that we use in natural language processing (NLP)</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer

</code></pre>
<p>I need the cluster number like (0,1,2,3,4) then followed by
key features in each cluster. Each cluster is a dataframe which is a subset of the &quot;data&quot;. c0 was taken from the data whereever the labels are &quot;0&quot;
similarly it was done for all the c0,c1,c2,c3,c4.</p>
<p>Each cluster will have a unique key features since the input is different for each cluster. but my code prints the c0 key features for all the clusters which is incorrect .</p>
<p>11th line of the code has some problem because of which it prints the same key feature results which it got for cluster0 instead of printing the result of cluster1</p>
",Multilingual Language Processing & Language Identification,python dual loop doe provide expected result new python trying run code result expected expected output current output key feature remains cluster adjust code key feature also change different cluster countvectorizer algorithmn use natural language processing nlp need cluster number like followed key feature cluster cluster dataframe subset data c wa taken data whereever label similarly wa done c c c c c cluster unique key feature since input different cluster code print c key feature cluster incorrect th line code ha problem print key feature result got cluster instead printing result cluster
Detecting syllables in a word,"<p>I need to find a fairly efficient  way to detect syllables in a word. E.g.,</p>

<p>Invisible -> in-vi-sib-le</p>

<p>There are some syllabification rules that could be used:</p>

<p>V
CV
VC
CVC
CCV
CCCV
CVCC</p>

<p>*where V is a vowel and C is a consonant.
E.g., </p>

<p>Pronunciation (5 Pro-nun-ci-a-tion; CV-CVC-CV-V-CVC)</p>

<p>I've tried few methods, among which were using regex (which helps only if you want to count syllables) or hard coded rule definition (a brute force approach which proves to be very inefficient) and finally using a finite state automata (which did not result with anything useful).</p>

<p>The purpose of my application is to create a dictionary of all syllables in a given language. This dictionary will later be used for spell checking applications (using Bayesian classifiers) and text to speech synthesis. </p>

<p>I would appreciate if one could give me tips on an alternate way to solve this problem besides my previous approaches. </p>

<p>I work in Java, but any tip in C/C++, C#, Python, Perl... would work for me.</p>
",Multilingual Language Processing & Language Identification,detecting syllable word need find fairly efficient way detect syllable word e g invisible vi sib le syllabification rule could used v cv vc cvc ccv cccv cvcc v vowel c consonant e g pronunciation pro nun ci tion cv cvc cv v cvc tried method among using regex help want count syllable hard coded rule definition brute force approach prof inefficient finally using finite state automaton result anything useful purpose application create dictionary syllable given language dictionary later used spell checking application using bayesian classifier text speech synthesis would appreciate one could give tip alternate way solve problem besides previous approach work java tip c c c python perl would work
Identifying languages of individual words within list,"<p>Suppose I have a list of words in different languages and I need to sort the words by language. What would be the most efficient way to do this? Currently I am using Python's <a href=""https://pypi.org/project/langdetect/"" rel=""nofollow noreferrer"">langdetect</a> which is very good at identifying individual words of unique character sets. For example 'ËøôÊòØ‰ªÄ‰πà' has a &gt; 0.99 probability of being Mandarin and '◊™◊§◊ï◊ó◊ô◊ù' a &gt; 0.99 probability of being Hebrew, but struggles when given one word from different languages from the Latin alphabet. For example 'intro' is given a &gt; 0.99 probability of being Italian and 'j√°' a &gt; 0.99 probability of being Hungarian. I was thinking of somehow pooling the words together (as each language has multiple words) and 'sweaty intro' produces a &gt; 0.99 probability of being English while 'ele j√°' produces a &gt; 0.99 probability of being Portuguese.</p>
<pre><code>testWords = ['ËøôÊòØ‰ªÄ‰πà', 'ËøôÊòØ‰ªÄ‰πà', 'ÁéãÊòéÊòØÂ≠¶Áîü„ÄÇ', 'sweaty', 'intro', 'am', '◊™◊§◊ï◊ó' ,'◊™◊§◊ï◊ó◊ô◊ù', '◊ê◊†◊ô', 'crian√ßa', 'ele', 'j√°']
</code></pre>
",Multilingual Language Processing & Language Identification,identifying language individual word within list suppose list word different language need sort word language would efficient way currently using python langdetect good identifying individual word unique character set example ha probability mandarin probability hebrew struggle given one word different language latin alphabet example intro given probability italian j probability hungarian wa thinking somehow pooling word together language ha multiple word sweaty intro produce probability english ele j produce probability portuguese
How to organize articles on a 2D map based on their meaning?,"<p>I have saved locally on disk the first paragraph of every article in the English version of Wikipedia (6,000,000 total).</p>
<p>How can I make a 2D map where similar articles are close to each other while dissimilar articles are proportionally distant?</p>
<p>For example, think of the map as a 256x256 grid with 65,536 cells.</p>
<p>The article about &quot;<a href=""https://en.wikipedia.org/wiki/Canis"" rel=""nofollow noreferrer"">canis</a>&quot; which is a breed of dogs is located in the cell at (356,39) along with other articles like the one for &quot;<a href=""https://en.wikipedia.org/wiki/Chihuahua_(dog)"" rel=""nofollow noreferrer"">chihuahua</a>&quot;, articles for some breeds of cats might be at (350,31) which is close while articles for &quot;<a href=""https://en.wikipedia.org/wiki/Aerospace_engineering"" rel=""nofollow noreferrer"">aerospace engineering</a>&quot; and &quot;<a href=""https://en.wikipedia.org/wiki/Computer_programming"" rel=""nofollow noreferrer"">computer programming</a>&quot; are on the opposite site of the map.</p>
",Multilingual Language Processing & Language Identification,organize article map based meaning saved locally disk first paragraph every article english version wikipedia total make map similar article close dissimilar article proportionally distant example think map x grid cell article canis breed dog located cell along article like one chihuahua article breed cat might close article aerospace engineering computer programming opposite site map
Pandas dataframe filter out rows with non-english text,"<p>I have a pandas <code>df</code> which has 6 columns, the last one is <code>input_text</code>. I want to remove from <code>df</code> all rows that have non-english text in that column. I would like to use <code>langdetect</code>'s <code>detect</code> function.</p>
<p>Some template</p>
<pre><code>from langdetect import detect
import pandas as pd

def filter_nonenglish(df):
    new_df = None  # Do some magical operations here to create the filtered df
    return new_df

df = pd.read_csv('somecsv.csv')
df_new = filter_nonenglish(df)
print('New df is: ', df_new)
</code></pre>
<p>Note! It doesn't matter what the other 5 columns are.
Also note: using <code>detect</code> is as simple as:</p>
<pre><code>t = 'I am very cool!'
print(detect(t))
</code></pre>
<p>Output is:</p>
<pre><code>en
</code></pre>
",Multilingual Language Processing & Language Identification,panda dataframe filter row non english text panda ha column last one want remove row non english text column would like use function template note matter column also note using simple output
How to inverse lemmatization process given a lemma and a token?,"<p>Generally, in natural language processing, we want to get the lemma of a token. </p>

<p>For example, we can map 'eaten' to 'eat' using wordnet lemmatization.</p>

<p><strong>Is there any tools in python that can inverse lemma to a certain form?</strong></p>

<p>For example, we map 'go' to 'gone' given target form 'eaten'.</p>

<p>PS: Someone mentions we have to store such mappings.
<a href=""https://stackoverflow.com/questions/30266502/how-to-un-stem-a-word-in-python"">How to un-stem a word in Python?</a></p>
",Multilingual Language Processing & Language Identification,inverse lemmatization process given lemma token generally natural language processing want get lemma token example map eaten eat using wordnet lemmatization tool python inverse lemma certain form example map go gone given target form eaten p someone mention store mapping href un stem word python
Retrain the multi language NER model(ner_ontonotes_bert_mult) from DeepPavlov with a dataset in a different language,"<p>I have successfully installed the multi-language NER model from DeepPavlov(ner_ontonotes_bert_mult). I want to retrain this model with new data(in the same format as they suggest in the <a href=""http://docs.deeppavlov.ai/en/master/features/models/ner.html#training-data"" rel=""nofollow noreferrer"">documentation</a> page) that are in the Albanian language.Is this possible(to retrain the multi-language NER model from DeepPavlov with data in a different language), or the retrain works only if we have English data??</p>
",Multilingual Language Processing & Language Identification,retrain multi language ner model ner ontonotes bert mult deeppavlov dataset different language successfully installed multi language ner model deeppavlov ner ontonotes bert mult want retrain model new data format suggest documentation page albanian language possible retrain multi language ner model deeppavlov data different language retrain work english data
Removing text based on the Non-English language used in the text,"<p>This is my sample dataset:</p>
<pre><code>text=c(&quot;I went to Helsinki&quot;,&quot;I went to  H√©lsinki&quot;,&quot;I went all√© Helsinki&quot;,
       &quot;je vais a Helsinli&quot;,&quot;I met Mr Smith&quot;,&quot;I met Monsi√©ur Smith&quot;,&quot;J'ai rencontr√© Monsieur Smith&quot;
       )

rank=c(1,2,3,4,5,6,7)

df &lt;- data.frame(text,rank)
df %&gt;% top_n(10)

                           text rank
1            I went to Helsinki    1
2           I went to  H√©lsinki    2
3          I went all√© Helsinki    3
4            je vais a Helsinli    4
5                I met Mr Smith    5
6          I met Monsi√©ur Smith    6
7 J'ai rencontr√© Monsieur Smith    7

</code></pre>
<p>All texts are either in <code>English</code>  or <code>French</code> . I'd like to remove texts that are mainly in French and not only having one or a few characters in French.</p>
<p>I am using the solution offered <a href=""https://stackoverflow.com/questions/43049015/removing-text-containing-non-english-character"">here</a> as follows:</p>
<pre><code>df %&gt;%
    mutate(text_selected= iconv(text, from = &quot;latin1&quot;, to = &quot;ASCII&quot;)) %&gt;% 
    select(text,text_selected)

                           text        text_selected
1            I went to Helsinki   I went to Helsinki
2           I went to  H√©lsinki                 &lt;NA&gt;
3          I went all√© Helsinki                 &lt;NA&gt;
4            je vais a Helsinli   je vais a Helsinli
5                I met Mr Smith       I met Mr Smith
6          I met Monsi√©ur Smith                 &lt;NA&gt;
7 J'ai rencontr√© Monsieur Smith                 &lt;NA&gt;
</code></pre>
<p>Using this solution, I have rows <code>1,5, and 7</code>  classified <code>correct</code>  but other rows are classified wrong. It is because this solution works based on character extraction I believe. For instance, in <code>row 2</code> , there is character <code>√©</code>  in <code>H√©lsinki</code>  and that's why this text classified as <code>NA</code>  or <code>Non-English</code>  whereas the text is manly written in the <code>English</code>  language. Or, in <code>row 4</code> , the main language of the text is <code>French</code>  but because there is no French character due to punctuation and written issues in the text, it is classified as English text.</p>
<p>I wonder if there is solution for classifying text based on its main language of the text that is more sophisticated than only finding one character in the text. So, in this case my ideal output should be:</p>
<pre><code>                           text        text_selected
1            I went to Helsinki    I went to Helsinki
2           I went to  H√©lsinki    I went to  H√©lsinki
3          I went all√© Helsinki    I went all√© Helsinki
4            je vais a Helsinli            &lt;NA&gt;
5                I met Mr Smith    I met Mr Smith
6          I met Monsi√©ur Smith    I met Monsi√©ur Smith 
7 J'ai rencontr√© Monsieur Smith            &lt;NA&gt;
</code></pre>
",Multilingual Language Processing & Language Identification,removing text based non english language used text sample dataset text either like remove text mainly french one character french using solution offered href follows p using solution row classified row classified wrong solution work based character extraction believe instance character text classified whereas text manly written language main language text french character due punctuation written issue text classified english text wonder solution classifying text based main language text sophisticated finding one character text case ideal output
How to fix the few incorrect outputs in this Arabic transliterator?,"<p>Specifically I am stuck on the <code>al-</code> which you commonly see on Arabic words, like <a href=""https://en.wikipedia.org/wiki/Masjid_al-Qiblatayn"" rel=""nofollow noreferrer"">al-Qiblatayn</a>. Here is the transliterator which I have made, you can for the most part ignore the <a href=""https://github.com/mountbuild/ovo/blob/62cee1230e22cf42aacec59b5948054b7a7a717d/form.js#L435"" rel=""nofollow noreferrer"">exact output</a> syntax, just note that the meaning of each character in the output has a specific meaning, and doubling a letter means making a vowel long or geminating a consonant.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const m = {
  '\u0626': ""'y"",
  '\u0624': ""'w"",
  '\u064E\u0627': 'aa',
  '\u0650\u0649': 'ii',
  '\u064E\u0649': 'aa',
  '\u0650\u064A': 'ii',
  '\u064F\u0648': 'uu',
  '\u064A\u064E': 'ay',
  '\u0648\u064E': 'aw',

  '\u0649': 'aa',

  'ÿ°': ""'"",

  'ÿß': 'aa',
  'Ô∫ç': 'aa',
  'Ô∫é': 'aa',

  'ÿ®': 'b',
  'Ô∫è': 'b',
  'Ô∫ê': 'b',
  'Ô∫í': 'b',
  'Ô∫ë': 'b',

  'ÿ™': 't',
  'Ô∫ï': 't',
  'Ô∫ñ': 't',
  'Ô∫ò': 't',
  'Ô∫ó': 't',

  'ÿ´': 'c',
  'Ô∫ô': 'c',
  'Ô∫ö': 'c',
  'Ô∫ú': 'c',
  'Ô∫õ': 'c',

  'ÿ¨': 'dj',
  'Ô∫ù': 'dj',
  'Ô∫û': 'dj',
  'Ô∫†': 'dj',
  'Ô∫ü': 'dj',

  'ÿ≠': 'h~',
  'Ô∫°': 'h~',
  'Ô∫¢': 'h~',
  'Ô∫§': 'h~',
  'Ô∫£': 'h~',

  'ÿÆ': 'H',
  'Ô∫•': 'H',
  'Ô∫¶': 'H',
  'Ô∫®': 'H',
  'Ô∫ß': 'H',

  'ÿØ': 'd',
  'Ô∫©': 'd',
  'Ô∫™': 'd',

  'ÿ∞': 'C',
  'Ô∫´': 'C',
  'Ô∫¨': 'C',

  'ÿ±': 'r!',
  'Ô∫≠': 'r!',
  'Ô∫Æ': 'r!',

  'ÿ≤': 'z',
  'Ô∫Ø': 'z',
  'Ô∫∞': 'z',

  'ÿ≥': 's',
  'Ô∫±': 's',
  'Ô∫≤': 's',
  'Ô∫¥': 's',
  'Ô∫≥': 's',

  'ÿ¥': 'x',
  'Ô∫µ': 'x',
  'Ô∫∂': 'x',
  'Ô∫∏': 'x',
  'Ô∫∑': 'x',

  'ÿµ': ""s\"""",
  'Ô∫π': ""s\"""",
  'Ô∫∫': ""s\"""",
  'Ô∫º': ""s\"""",
  'Ô∫ª': ""s\"""",

  'ÿ∂': ""d\"""",
  'Ô∫Ω': ""d\"""",
  'Ô∫æ': ""d\"""",
  'ÔªÄ': ""d\"""",
  'Ô∫ø': ""d\"""",

  'ÿ∑': ""t\"""",
  'ÔªÅ': ""t\"""",
  'ÔªÇ': ""t\"""",
  'ÔªÑ': ""t\"""",
  'ÔªÉ': ""t\"""",

  'ÿ∏': 'C',
  'ÔªÖ': 'C',
  'ÔªÜ': 'C',
  'Ôªà': 'C',
  'Ôªá': 'C',

  'ÿπ': ""'"",
  'Ôªâ': ""'"",
  'Ôªä': ""'"",
  'Ôªå': ""'"",
  'Ôªã': ""'"",

  'ÿ∫': 'r~',
  'Ôªç': 'r~',
  'Ôªé': 'r~',
  'Ôªê': 'r~',
  'Ôªè': 'r~',

  'ŸÅ': 'f',
  'Ôªë': 'f',
  'Ôªí': 'f',
  'Ôªî': 'f',
  'Ôªì': 'f',

  'ŸÇ': 'K',
  'Ôªï': 'K',
  'Ôªñ': 'K',
  'Ôªò': 'K',
  'Ôªó': 'K',

  'ŸÉ': 'k',
  'Ôªô': 'k',
  'Ôªö': 'k',
  'Ôªú': 'k',
  'Ôªõ': 'k',

  'ŸÑ': 'l',
  'Ôªù': 'l',
  'Ôªû': 'l',
  'Ôª†': 'l',
  'Ôªü': 'l',

  'ŸÖ': 'm',
  'Ôª°': 'm',
  'Ôª¢': 'm',
  'Ôª§': 'm',
  'Ôª£': 'm',

  'ŸÜ': 'n',
  'Ôª•': 'n',
  'Ôª¶': 'n',
  'Ôª®': 'n',
  'Ôªß': 'n',

  'Ÿá': 'h',
  'Ôª©': 'h',
  'Ôª™': 'h',
  'Ôª¨': 'h',
  'Ôª´': 'h',

  'Ÿà': 'w',
  'Ôª≠': 'w',
  'ÔªÆ': 'w',

  'Ÿä': 'y',
  'Ôª±': 'y',
  'Ôª≤': 'y',
  'Ôª¥': 'y',
  'Ôª≥': 'y',

  'ÿ¢': ""'aa"",
  'ŸÄÿ¢': ""'aa"",
  'Ÿ±': '',
  'Ô∫Å': ""'aa"",
  'Ô∫Ç': ""'aa"",

  'ÿ©': 'at',
  'ŸÄÿ©': 'at',
  'Ô∫ì': 'at',
  'Ô∫î': 'at',

  'ÿßŸÑ': 'al',
  'ŸÄŸâ': 'aa',

  'Ôªª': 'laa',
  'Ôªº': 'laa',

  'Ô≤ì': 'iim',
  'Ô≥∞': 'iim',
  'Ô≥ù': 'iim',
  'Ô±ò': 'iim',

  'Ô≤Ö': 'lm',
  'Ô≥≠': 'lm',
  'Ô≥å': 'lm',
  'Ô±Ç': 'lm',

  'Ÿâ': 'a',
  'ÔªØ': 'a',
  'Ôª∞': 'a',

  'ÿå‚Äé': ',',
  'ÿü': '?',
  'ÿõ': ';',
  '€î': '.',
  'Ô¥æ': '{',
  'Ô¥ø': '}',

  '\u064e': 'a',
  '\u0650': 'i',
  '\u064f': 'u',
  '\u0627': 'aa',
  '\u064a': 'ii',
  '\u0648': 'uu',
  '\u064b': 'an',
  '\u064d': 'in',
  '\u064c': 'un',

  '⁄Ø': 'g',
  '⁄®': 'g',
  '⁄≠': 'g',
  '⁄Ü': 'j',
  '⁄•': 'v',
  '⁄§': 'v',
  'Ÿæ': 'p',

  '\u06e1': '',
  '\u0652': '',

  '\u0651': '',

  '\u0670': 'aa',
  'ÿ•': ""'i"", // Arabic Letter Alef With Hamza Below
  'ÿ£': ""'"",
  '\u200e': '', // ltr marker
  ' ': ' ',
}

const generate = (s) =&gt; {
  let out = []
  let i = 0
  let r = s
  let last
  while (r.length) {
    let found = false
    x:
    for (let k in m) {
      if (r.indexOf(k) === 0) {
        let v = k == '\u0651' ? last.split('').pop() : m[k]
        last = v
        i += k.length
        out.push(v)
        r = r.substr(k.length)
        found = true
        break x
      }
    }
    if (!found) {
      throw `oops ${out.join('')} from ${s}, ${r.codePointAt(0).toString(16)}`
    }
  }
  return out.join('')
    .replace(/ii+/g, 'ii')
    .replace(/ee+/g, 'ee')
    .replace(/aa+/g, 'aa')
    .replace(/oo+/g, 'oo')
    .replace(/uu+/g, 'uu')
}

console.log(generate('Ÿ±ŸÑŸíŸÉŸéÿπŸíÿ®Ÿéÿ©‚Äé'))
console.log(generate('ÿßŸéŸÑŸíŸÖŸéÿ≥Ÿíÿ¨ŸêÿØŸè Ÿ±ŸÑŸíÿ≠Ÿéÿ±ŸéÿßŸÖŸè‚Äé'))</code></pre>
</div>
</div>
</p>
<p>Here are the outputs of a few of them:</p>
<pre><code>Ÿ±ŸÑŸíŸÉŸéÿπŸíÿ®Ÿéÿ©‚Äé,lka'baat
ÿßŸéŸÑŸíŸÖŸéÿ≥Ÿíÿ¨ŸêÿØŸè Ÿ±ŸÑŸíÿ≠Ÿéÿ±ŸéÿßŸÖŸè‚Äé,aalmasdjidu lh~ar!aamu
Ÿ±ŸÑŸíŸÖŸéÿ≥Ÿíÿ¨ŸêÿØ Ÿ±ŸÑŸíÿ£ŸéŸÇŸíÿµŸéŸâŸ∞‚Äé,lmasdjid l'aKs&quot;aa
ÿßŸÑÿ≠ÿ±ŸÖ ÿßŸÑÿ¥ÿ±ŸäŸÅ,aalh~r!m aalxr!iif
ŸÖÿ≥ÿ¨ÿØ ÿßŸÑŸÇÿ®ŸÑÿ™ŸäŸÜ,msdjd aalKbltiin
</code></pre>
<p>Notice (1) and (3) one starts with <code>l-</code> instead of <code>al-</code>, and 2, 4, and 5 start with <code>aal-</code>. Also (tangent note), the first one ends in <code>-t</code> even though the word is normally written <a href=""https://en.wikipedia.org/wiki/Kaaba"" rel=""nofollow noreferrer"">Kabaa</a> with no <code>t</code>.</p>
<p>Why is the <code>al-</code> written in two different ways from what I am seeing here (copied these words from Wikipedia). And how do I fix the generator so it writes it properly (a short <code>a</code> sound it seems, rather than a long <code>aa</code>)?</p>
<p><em>If you notice anything else wrong I would love to know, this is just what I have found in getting started so far. If you know some general things I am missing from this implementation please let me know. I have included the &quot;Arabic Shadda U+0651&quot; to double a consonant, and everything else I got from the Wikipedia <a href=""https://en.wikipedia.org/wiki/Arabic_alphabet"" rel=""nofollow noreferrer"">Arabic Alphabet</a> page.</em></p>
<p>I am not that familiar with reading Arabic, but I notice some of the vowel diacritics are different in the two sets, just not sure why or how to resolve it.</p>
",Multilingual Language Processing & Language Identification,fix incorrect output arabic transliterator specifically stuck commonly see arabic word like al qiblatayn transliterator made part ignore exact output syntax note meaning character output ha specific meaning doubling letter mean making vowel long geminating consonant output notice one start instead start also tangent note first one end even though word normally written kabaa written two different way seeing copied word wikipedia fix generator writes properly short sound seems rather long notice anything else wrong would love know found getting started far know general thing missing implementation please let know included arabic shadda u double consonant everything else got wikipedia arabic alphabet page familiar reading arabic notice vowel diacritic different two set sure resolve
Replicating a Google ngram plot in R,"<p>I'm trying to replicate a plot from the paper Michel et al., 'Quantitative Analysis of Culture Using Millions of Digitized Books' (2011). Specifically I'm trying to make the one on the top right here:
<a href=""https://pubmed.ncbi.nlm.nih.gov/21163965/#&amp;gid=article-figures&amp;pid=fig-3-uid-2"" rel=""nofollow noreferrer"">https://pubmed.ncbi.nlm.nih.gov/21163965/#&amp;gid=article-figures&amp;pid=fig-3-uid-2</a></p>
<p>I know the paper used v1 of the corpus but I'm doing it with v2 as it's easier to work with. When I use the Google Ngram viewer (specifying the English 2012 corpus which corresponds to v2, a year range of 1875 to 1975, and no smoothing) I get <a href=""https://books.google.com/ngrams/graph?content=1883%2C1910%2C1950&amp;year_start=1875&amp;year_end=1975&amp;corpus=15&amp;smoothing=0"" rel=""nofollow noreferrer"">this</a>, which looks pretty close.</p>
<p>When I tried to replicate this in <code>R</code>/<code>ggplot</code> I get this:
<a href=""https://i.sstatic.net/x5zjw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x5zjw.png"" alt=""enter image description here"" /></a></p>
<p>1950 and 1883 look pretty consistent with what is happening in the viewer plot, but I can't figure out what is happening with 1910. There appears to be very few occurrences of the year '1910' in the data set in comparison to some of the other years. Would anyone with a better understanding of the Google ngrams data set be able to point me in the right direction? Should I be supplementing this with something other than just the 1-gram dataset? Does the Google ngram viewer pick out occurrences of 1-grams in a different way?</p>
<p>The code I've used is below. A couple of other points: 1910 and 1950 do not seem to exist as 1-grams in the v2 data set, but 1883 does. To get this to even remotely work, I had to <code>grepl</code> for 1950 and 1910 to get any hits (i.e. they all seem to appear as parts of date ranges like 1890-1910, or with some other characters tacked on), rather than just doing a fixed search for those years in the <code>ngram</code> field. I also used <code>purrr::map_dfr</code> to do this rather than just a <code>dplyr::case_when</code> in case years appeared in the same ngram picked up by a <code>grepl</code> (e.g. the range 1883-1910 should be a hit for both of those years, not just one).</p>
<pre><code>library(ggplot2)
library(dplyr)
library(purrr)

#---- Load data ----
counts_file &lt;- file.path(&quot;data&quot;, &quot;total_counts.txt&quot;)
ngrams_file &lt;- file.path(&quot;data&quot;, &quot;google_books_1gram_eng_v2.gz&quot;)

if (!dir.exists(&quot;data&quot;)) {
  dir.create(&quot;data&quot;)
}

if (!file.exists(counts_file)) {
  download.file(
    &quot;http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-totalcounts-20120701.txt&quot;,
    counts_file
  )
}
if (!file.exists(ngrams_file)) {
  download.file(
    &quot;http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-1.gz&quot;,
    ngrams_file
  )
}

one_grams &lt;- read.delim(
  gzfile(ngrams_file),
  header = FALSE
)
names(one_grams) &lt;- c(&quot;ngram&quot;, &quot;year&quot;, &quot;match_count&quot;, &quot;volume_count&quot;)
one_grams_subset &lt;- one_grams %&gt;%
  filter(year &gt;= 1875 &amp; year &lt;= 1975)

total_counts_temp &lt;- t(
  read.table(
    counts_file, 
    header = FALSE
  )
)
total_counts_char &lt;- do.call(
  rbind, 
  strsplit(total_counts_temp, &quot;,&quot;)
)
total_counts &lt;- apply(total_counts_char, 2, as.numeric)
colnames(total_counts) &lt;- c(&quot;year&quot;, &quot;match_count&quot;, &quot;page_count&quot;, &quot;volume_count&quot;)

#---- Recreate plot 3A from Michel et al. (2011) ----
year_subset &lt;- function(year_char, one_grams_data) {
  one_grams_data %&gt;% 
    filter(grepl(year_char, .[[&quot;ngram&quot;]], fixed = TRUE)) %&gt;% 
    group_by(year) %&gt;% 
    summarise(year_count = sum(match_count, na.rm = TRUE)) %&gt;% 
    mutate(year_gram = year_char)
}

plot_data &lt;- map_dfr(c(&quot;1883&quot;, &quot;1910&quot;, &quot;1950&quot;),
                     year_subset,
                     one_grams_subset) %&gt;% 
  left_join(as_tibble(total_counts), by = &quot;year&quot;) %&gt;% 
  mutate(frequency = 10000 * year_count/match_count) %&gt;% 
  select(year_gram, year, frequency, year_count)

ggplot(plot_data) +
  geom_line(aes(x = year, y = frequency, colour = year_gram)) +
  theme_minimal() +
  labs(col = &quot;ngram&quot;, x = &quot;Year&quot;, y = &quot;Frequency&quot;)
</code></pre>
",Multilingual Language Processing & Language Identification,replicating google ngram plot r trying replicate plot paper michel et al quantitative analysis culture using million digitized book specifically trying make one top right know paper used v corpus v easier work use google ngram viewer specifying english corpus corresponds v year range smoothing get look pretty close tried replicate get look pretty consistent happening viewer plot figure happening appears occurrence year data set comparison year would anyone better understanding google ngrams data set able point right direction supplementing something gram dataset doe google ngram viewer pick occurrence gram different way code used couple point seem exist gram v data set doe get even remotely work get hit e seem appear part date range like character tacked rather fixed search year field also used rather case year appeared ngram picked e g range hit year one
How to increase batch size in GPT2 training for translation task?,"<p>I am developing a code to use the pre-trained <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">GPT2</a> model for a machine translation task. The length of my data's word-to-id is 91, and I developed the following code for my model:</p>
<pre><code>import torch
from torch.utils.data import DataLoader
from transformers.models.gpt2.modeling_gpt2 import GPT2Model

# data preparation code

def batch_sequences(x, y, env):
    &quot;&quot;&quot;
    Take as input a list of n sequences (torch.LongTensor vectors) and return
    a tensor of size (slen, n) where slen is the length of the longest
    sentence, and a vector lengths containing the length of each sentence.
    &quot;&quot;&quot;
    lengths_x = torch.LongTensor([len(s) + 2 for s in x])
    lengths_y = torch.LongTensor([len(s) + 2 for s in y])
    max_length = max(lengths_x.max().item(), lengths_y.max().item())
    sent_x = torch.LongTensor(
        max_length, lengths_x.size(0)).fill_(env.pad_index)
    sent_y = torch.LongTensor(
        max_length, lengths_y.size(0)).fill_(env.pad_index)
    assert lengths_x.min().item() &gt; 2
    assert lengths_y.min().item() &gt; 2

    sent_x[0] = env.eos_index
    for i, s in enumerate(x):
        sent_x[1:lengths_x[i] - 1, i].copy_(s)
        sent_x[lengths_x[i] - 1, i] = env.eos_index

    sent_y[0] = env.eos_index
    for i, s in enumerate(y):
        sent_y[1:lengths_y[i] - 1, i].copy_(s)
        sent_y[lengths_y[i] - 1, i] = env.eos_index

    return sent_x, sent_y, max_length

def collate_fn(elements):
    &quot;&quot;&quot;
    Collate samples into a batch.
    &quot;&quot;&quot;
    x, y = zip(*elements)
    x = [torch.LongTensor([env.word2id[w]
                          for w in seq if w in env.word2id]) for seq in x]
    y = [torch.LongTensor([env.word2id[w]
                          for w in seq if w in env.word2id]) for seq in y]
    x, y, length = batch_sequences(x, y, env)
    return (x, length), (y, length), torch.LongTensor(nb_ops)

loader = DataLoader(data, batch_size=1, shuffle=False, collate_fn=collate_fn)
gpt2 = GPT2Model.from_pretrained('gpt2')
in_layer = nn.Embedding(len(env.word2id), 768)
out_layer = nn.Linear(768, len(env.word2id))

parameters = list(gpt2.parameters()) + list(in_layer.parameters()) + list(out_layer.parameters())
optimizer = torch.optim.Adam(parameters)
loss_fn = nn.CrossEntropyLoss()
for layer in (gpt2, in_layer, out_layer):
    layer.train()

accuracies = list()
n_epochs = 5
for i in range(n_epochs):
    for (x, x_len), (y, y_len) in loader:

        x = x.to(device=device)
        y = y.to(device=device)

        embeddings = in_layer(x.reshape(1, -1))
        hidden_state = gpt2(inputs_embeds=embeddings).last_hidden_state[:, :]
        logits = out_layer(hidden_state)[0]
        loss = loss_fn(logits, y.reshape(-1))
        accuracies.append(
            (logits.argmax(dim=-1) == y.reshape(-1)).float().mean().item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if len(accuracies) % 500 == 0:
            accuracy = sum(accuracies[-50:]) / len(accuracies[-50:])
            print(f'Samples: {len(accuracies)}, Accuracy: {accuracy}')
</code></pre>
<p>This code works pretty well when the batch size is 1. But it is so slow. I wanted to increase the batch size from 1 to 32, but I get some dimension compatibility problems. How can I increase the batch size without errors?</p>
<p>My data consists of pair of sentences, the first one is a sentence in the first language and the second one is its translation in the second language.</p>
<p>For example, assume that x.shape is (batch_size, 12) (meaning we have 'batch_size' sentences of length 12 as input and y.shape is also (batch_size, 12) (the translations). And also we have a word-to-id dictionary of length 90 that matches each word in a sentence with its index)</p>
",Multilingual Language Processing & Language Identification,increase batch size gpt training translation task developing code use pre trained gpt model machine translation task length data word id developed following code model code work pretty well batch size slow wanted increase batch size get dimension compatibility problem increase batch size without error data consists pair sentence first one sentence first language second one translation second language example assume x shape batch size meaning batch size sentence length input shape also batch size translation also word id dictionary length match word sentence index
"UFuncTypeError: ufunc ‚Äòclip‚Äô did not contain a loop with signature matching types (dtype(‚Äò&lt;U32‚Äô), dtype(‚Äò&lt;U32‚Äô), dtype(‚Äò&lt;U32‚Äô)) -&gt; dtype(‚Äò&lt;U32‚Äô)","<p>I am using the Deep Pavlov framework to work with Bert Classifier simply because the language I need to predict staff is Russian. Basically, I am trying to solve a multi-class classification problem. According to the Deep Pavlov, we can easily change some configs on config file. I took this config file <a href=""https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/configs/classifiers/rusentiment_convers_bert.json"" rel=""nofollow noreferrer"">https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/configs/classifiers/rusentiment_convers_bert.json</a> and trained it, and it took me around 13 hours to finish and it turned out to be that my model is overfitting.</p>
<p>I made some changes, particularly these:</p>
<pre><code>&quot;weight_decay_rate&quot;: 0.001,
&quot;learning_rate_drop_patience&quot;: 1,
&quot;learning_rate_drop_div&quot;: 2.0,
&quot;load_before_drop&quot;: True, 
&quot;min_learning_rate&quot;: 1e-03,
&quot;attention_probs_keep_prob&quot;: 0.5,
&quot;hidden_keep_prob&quot;: 0.5,
</code></pre>
<p>also, I increased the batch size, it was 16 before now:</p>
<pre><code>&quot;batch_size&quot;: 32
</code></pre>
<p>and added some metrics:</p>
<pre><code>&quot;log_loss&quot;,
&quot;matthews_correlation&quot;,
</code></pre>
<p>Also changed validation_patience to 1 and added tensorboard func</p>
<pre><code>&quot;validation_patience&quot;: 1,
&quot;tensorboard_log_dir&quot;: &quot;logs/&quot;,
</code></pre>
<p>and that is it. these are all the changes I made to my model, and when i tried to train my model, it is giving me following error:</p>
<pre><code>UFuncTypeError                            Traceback (most recent call last)
        /usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)
             60     try:
        ---&gt; 61         return bound(*args, **kwds)
             62     except TypeError:

15 frames
UFuncTypeError: ufunc 'clip' did not contain a loop with signature matching types (dtype('&lt;U32'), dtype('&lt;U32'), dtype('&lt;U32')) -&gt; dtype('&lt;U32')

During handling of the above exception, another exception occurred:

UFuncTypeError                            Traceback (most recent call last)
&lt;__array_function__ internals&gt; in clip(*args, **kwargs)

/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py in _clip_dep_invoke_with_casting(ufunc, out, casting, *args, **kwargs)
     83     # try to deal with broken casting rules
     84     try:
---&gt; 85         return ufunc(*args, out=out, **kwargs)
     86     except _exceptions._UFuncOutputCastingError as e:
     87         # Numpy 1.17.0, 2019-02-24

UFuncTypeError: ufunc 'clip' did not contain a loop with signature matching types (dtype('&lt;U32'), dtype('&lt;U32'), dtype('&lt;U32')) -&gt; dtype('&lt;U32')
</code></pre>
<p>At first, I thought it has something to do with a dataset, however, I did not change my dataset and it has run the first time I trained this model.</p>
",Multilingual Language Processing & Language Identification,ufunctypeerror ufunc clip contain loop signature matching type dtype u dtype u dtype u dtype u using deep pavlov framework work bert classifier simply language need predict staff russian basically trying solve multi class classification problem according deep pavlov easily change configs config file took config file trained took around hour finish turned model overfitting made change particularly also increased batch size wa added metric also changed validation patience added tensorboard func change made model tried train model giving following error first thought ha something dataset however change dataset ha run first time trained model
cleaning scraped text in python,"<p>I am new to python and just started learning web-scraping using beautiful soup (in Jupyter notebook). I scraped a book off Project Gutenberg, and want to do translation. However, had trouble cleaning the text, followed by doing the translation.</p>
<p>I want to get rid of the stuff at the beginning of the scraped text (e.g.BODY { color: Black; background: White;....) and after that translate the entire text using google API.</p>
<p>Would be grateful for help/advice on both. my code so far is below.The translation code did not work, and returned the following error &quot;WriteError: [Errno 32] Broken pipe&quot;</p>
<pre><code>#Store url
url = 'https://www.gutenberg.org/files/514/514-h/514-h.htm'
html = r.text
print(html)
#Create a BeautifulSoup object from the HTML
soup = BeautifulSoup(html, &quot;html5lib&quot;)
type(soup)
#Scrape entire text using 'get' and print it
text = soup.get_text()
print(text)
#translate text using google API translator
init the Google API translator
translator = Translator()
translation = translator.translate(text,dest=&quot;ar&quot;)
print(translation)
</code></pre>
",Multilingual Language Processing & Language Identification,cleaning scraped text python new python started learning web scraping using beautiful soup jupyter notebook scraped book project gutenberg want translation however trouble cleaning text followed translation want get rid stuff beginning scraped text e g body color black background white translate entire text using google api would grateful help advice code far translation code work returned following error writeerror errno broken pipe
Segmentation of a single Japanese handwritten word into characters,"<p>I want to segment a handwritten Japanese word into characters. Although I did the segmentation using the projection method, I did not reach an accuracy of more than 70%. I need suggestions and also the path to do the same using other advanced methods.</p>
",Multilingual Language Processing & Language Identification,segmentation single japanese handwritten word character want segment handwritten japanese word character although segmentation using projection method reach accuracy need suggestion also path using advanced method
&#39;float&#39; object is not iterable in NLP,"<p>I am working on a natural language processing project. The purpose of the code is to remove all punctuation in the data set. But an error was reported when calling the function. What is the reason?</p>
<pre><code>def remove_punct(text):
    text_nopunct = ''.join([char for char in text if char not in string.punctuation])
    return text_nopunct

data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))
</code></pre>
",Multilingual Language Processing & Language Identification,float object iterable nlp working natural language processing project purpose code remove punctuation data set error wa reported calling function reason
AllenNLP - Support for different languages,"<p>Is there any current support for other languages? For example, Spanish.
If not, is it planned?
Or else, what would be the way to add it?</p>
",Multilingual Language Processing & Language Identification,allennlp support different language current support language example spanish planned else would way add
How to generate all derived terms out of a root or lemma word in English using SpaCy or NLTK?,"<p>Is there a function in NLTK or SpaCy that provides all possible terms that can be derived out of a given lemma word? For example: if the lemma is &quot;breath&quot;, I need all the derived terms for &quot;breath&quot; such as &quot;breathe&quot;, &quot;breathing&quot; etc. If the root word is &quot;eat&quot;, I need the terms &quot;eating&quot;, &quot;ate&quot;, &quot;eaten&quot; and so on.</p>
<p>The .lemma_ attribute in SpaCy and the WordNetLemmatizer() function in NLTK can be used for determining the lemma of a word, but how do I do the reverse task, that is determining all the derived terms out of a given lemma word?</p>
",Multilingual Language Processing & Language Identification,generate derived term root lemma word english using spacy nltk function nltk spacy provides possible term derived given lemma word example lemma breath need derived term breath breathe breathing etc root word eat need term eating ate eaten lemma attribute spacy wordnetlemmatizer function nltk used determining lemma word reverse task determining derived term given lemma word
NLP: Understanding Token IDs,"<p>I am getting into NLP and neural machine translation. I understand how sentencepiece etc can translate a work into subwords, and subwords into token IDs. But these token IDs are just integers representing the subword token. How do these IDs actually get used with NLP models?</p>
",Multilingual Language Processing & Language Identification,nlp understanding token id getting nlp neural machine translation understand sentencepiece etc translate work subwords subwords token id token id integer representing subword token id actually get used nlp model
Correct way of loading torchtext model + vocab for inference,"<p>I trained a Transformer model for translation. The saved model achieves ~27 BLEU on my test set. When I reload the model in a new session the BLEU score dips to ~0.5. I managed to find out that the issue is probably the fact that I build my vocab anew everytime I start a session instead of loading it from memory, and the itos mapping is changing everytime (I randomly split loaded data into train,dev,test and build vocab from train).</p>
<p>That got me thinking: is there a correct way of loading a vocab from memory so that my inference results are consistent? Minding that I also need to continue training the model so the vocab needs to be part of the TabularDataset.</p>
",Multilingual Language Processing & Language Identification,correct way loading torchtext model vocab inference trained transformer model translation saved model achieves bleu test set reload model new session bleu score dip managed find issue probably fact build vocab anew everytime start session instead loading memory itos mapping changing everytime randomly split loaded data train dev test build vocab train got thinking correct way loading vocab memory inference result consistent minding also need continue training model vocab need part tabulardataset
How can I conjugate English words to the progressive form in Python?,"<p>I have looked at <code>pattern.en</code>'s <code>conjugate</code>, but it only conjugates into a few forms, and I would rather not have to sit down and program all of the exceptions to those rules that would allow me to make conjugations such as</p>

<ul>
<li>free - freeing</li>
<li>eat - eating</li>
<li>bathe - bathing </li>
<li>be - being</li>
<li>ban - banning</li>
</ul>

<p><code>nltk</code> has stemming, but it doesn't seem to have the reverse operation, at least from searching StackOverflow. This seems like a very elementary NLP task, but I cannot find anything modern that does this in Python. Any general conjugation tool would be nice, although the progressive form in English doesn't have irregularities I know of.</p>

<p>I am also trying to see if there are exceptions to this rule, which might work as an alternate function:</p>

<pre><code>def present_to_progressive(x):
    vowels = set(['a','e','i','o','u'])
    size = len(x)
    if size == 2:
        return x + 'ing'
    elif x[size - 2:] == 'ie':
        return x[:(size-2)] + 'ying'
    elif x[size - 1] not in vowels and x[size - 2] not in vowels:
        return x + 'ing'
    elif x[size - 1] == 'e' and x[size-2] not in vowels:
        return x[0:(size-1)] + 'ing'
    elif x[size - 1] not in vowels and x[size-2] in vowels:
        if x[size - 3] not in vowels:
             return x + x[size-1] + 'ing'
        else:
             return x + 'ing'
    else:
        return x + 'ing'
</code></pre>

<p><em>Edit: Added case for ""ie"" verbs</em></p>
",Multilingual Language Processing & Language Identification,conjugate english word progressive form python looked conjugate form would rather sit program exception rule would allow make conjugation free freeing eat eating bathe bathing ban banning ha stemming seem reverse operation least searching stackoverflow seems like elementary nlp task find anything modern doe python general conjugation tool would nice although progressive form english irregularity know also trying see exception rule might work alternate function edit added case ie verb
Problems translating a pandas dataframe from english to french using googletrans,"<p>I am using googlentrans to translate a column from a dataframe.</p>
<pre class=""lang-py prettyprint-override""><code>translator = Translator()  # initalize the Translator object

data['phrase']=data['sentence'].apply(lambda x: translator.translate(x, src='en', dest='fr').text )
</code></pre>
<p>It is supposed to work, but after I applying these 2 lines of code I am creating a column called &quot;phrase&quot; but is not in french. The text remains in english...</p>
<p>I have also used this line of code</p>
<pre class=""lang-py prettyprint-override""><code>data['phrase']=data['sentence'].apply(translator.translate, src='en',dest='fr').apply(getattr,args=('text',))
</code></pre>
<p>And the problem remains the same, it seems like google trans is not applying the translation.</p>
<p>Best,</p>
",Multilingual Language Processing & Language Identification,problem translating panda dataframe english french using googletrans using googlentrans translate column dataframe supposed work applying line code creating column called phrase french text remains english also used line code problem remains seems like google trans applying translation best
Hindi to English Transliteration,"<p>Is there a python library for transliterating Hindi to English?</p>
<p>e.g. &quot;‡§ñ‡§æ‡§Ø‡§æ&quot; should be converted to &quot;khaya&quot;</p>
",Multilingual Language Processing & Language Identification,hindi english transliteration python library transliterating hindi english e g converted khaya
How do I translate using HuggingFace from Chinese to English?,"<p>I want to translate from Chinese to English using HuggingFace's transformers using a pretrained <code>&quot;xlm-mlm-xnli15-1024&quot;</code> model. <a href=""https://huggingface.co/transformers/usage.html#translation"" rel=""noreferrer"">This tutorial</a> shows how to do it from English to German.</p>
<p>I tried following the tutorial but it doesn't detail how to manually change the language or to decode the result. I am lost on where to start. Sorry that this question could not be more specific.</p>
<p>Here is what I tried:</p>
<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer
base_model = &quot;xlm-mlm-xnli15-1024&quot;
model = AutoModelWithLMHead.from_pretrained(base_model)
tokenizer = AutoTokenizer.from_pretrained(base_model)

inputs = tokenizer.encode(&quot;translate English to Chinese: Hugging Face is a technology company based in New York and Paris&quot;, return_tensors=&quot;pt&quot;)
outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)

print(tokenizer.decode(outputs.tolist()[0]))
</code></pre>
<pre><code>'&lt;s&gt;translate english to chinese : hugging face is a technology company based in new york and paris &lt;/s&gt;china hug ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢ ‚Ñ¢'
</code></pre>
",Multilingual Language Processing & Language Identification,translate using huggingface chinese english want translate chinese english using huggingface transformer using pretrained model tutorial show english german tried following tutorial detail manually change language decode result lost start sorry question could specific tried
Count number of English words in string in R,"<p>I would like to count the number of English words in a string of text.</p>
<pre><code>df.words &lt;- data.frame(ID = 1:2,
              text = c(c(&quot;frog friend fresh frink foot&quot;),
                       c(&quot;get give gint gobble&quot;)))

df.words

  ID                         text
1  1 frog friend fresh frink foot
2  2         get give gint gobble
</code></pre>
<p>I'd like the final product to look like this:</p>
<pre><code>  ID                         text count
1  1 frog friend fresh frink foot     4
2  2         get give gint gobble     3
</code></pre>
<p>I'm guessing I'll have to first separate based on spaces and then reference the words against a dictionary?</p>
",Multilingual Language Processing & Language Identification,count number english word string r would like count number english word string text like final product look like guessing first separate based space reference word dictionary
Training a BERT and Running out of memory - Google Colab,"<p>I keep running out of memory even after i bought google colab pro which has 25gb RAM usage. I have no idea why is this happening. I tried every kernel possible (Google colab, Google colab pro, Kaggle kernel, Amazon Sagemaker, Google Cloud Platform). I reduced my batch size to 8, no success whatsoever.</p>
<p>My goal is to train Bert in Deep Pavlov (with Russian text classification extension) to predict emotion of the tweet. It is a multiclass classification with 5 classes</p>
<p>Here is my whole code:</p>
<pre><code>!pip3 install deeppavlov
import pandas as pd
train_df = pd.read_csv('train_pikabu.csv')
test_df = pd.read_csv('test_pikabu.csv')
val_df = pd.read_csv('validation_pikabu.csv')

from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader
# read data from particular columns of `.csv` file
data = BasicClassificationDatasetReader().read(
   data_path='./',
   train='train_pikabu.csv',
   valid=&quot;validation_pikabu_a.csv&quot;, 
   test=&quot;test_pikabu.csv&quot;,
   x = 'content',
   y = 'emotions'
   )

from deeppavlov.dataset_iterators.basic_classification_iterator import 
BasicClassificationDatasetIterator
# initializing an iterator
iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)

!python -m deeppavlov install squad_bert
from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor
bert_preprocessor = BertPreprocessor(vocab_file=&quot;./bert/vocab.txt&quot;,
                                 do_lower_case=False,
                                 max_seq_length=256)

from deeppavlov.core.data.simple_vocab import SimpleVocabulary
vocab = SimpleVocabulary(save_path=&quot;./binary_classes.dict&quot;)
iterator.get_instances(data_type=&quot;train&quot;)
vocab.fit(iterator.get_instances(data_type=&quot;train&quot;)[1])

from deeppavlov.models.preprocessors.one_hotter import OneHotter
one_hotter = OneHotter(depth=vocab.len, 
                   single_vector=True  # means we want to have one vector per sample
                  )

from deeppavlov.models.classifiers.proba2labels import Proba2Labels
prob2labels = Proba2Labels(max_proba=True)

from deeppavlov.models.bert.bert_classifier import BertClassifierModel
from deeppavlov.metrics.accuracy import sets_accuracy

bert_classifier = BertClassifierModel(
 n_classes=vocab.len,
 return_probas=True,
 one_hot_labels=True,
 bert_config_file=&quot;./bert/bert_config.json&quot;,
 pretrained_bert=&quot;./bert/bert_model.ckpt&quot;,
 save_path=&quot;sst_bert_model/model&quot;,
 load_path=&quot;sst_bert_model/model&quot;,
 keep_prob=0.5,
 learning_rate=1e-05,
 learning_rate_drop_patience=5,
 learning_rate_drop_div=2.0
 )


 # Method `get_instances` returns all the samples of particular data field
 x_valid, y_valid = iterator.get_instances(data_type=&quot;valid&quot;)
 # You need to save model only when validation score is higher than previous one.
 # This variable will contain the highest accuracy score
 best_score = 0.
 patience = 2
 impatience = 0

 # let's train for 3 epochs
 for ep in range(3):

     nbatches = 0
     for x, y in iterator.gen_batches(batch_size=8, 
                                 data_type=&quot;train&quot;, shuffle=True):
        x_feat = bert_preprocessor(x)
        y_onehot = one_hotter(vocab(y))
        bert_classifier.train_on_batch(x_feat, y_onehot)
        print(&quot;Batch done\n&quot;)
        nbatches += 1
    
        if nbatches % 1 == 0:
            # validating every 100 batches
            y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
            score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
            print(&quot;Batches done: {}. Valid Accuracy: {}&quot;.format(nbatches, score))
        
     y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
     score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
     print(&quot;Epochs done: {}. Valid Accuracy: {}&quot;.format(ep + 1, score))
     if score &gt; best_score:
          bert_classifier.save()
          print(&quot;New best score. Saving model.&quot;)
          best_score = score    
          impatience = 0
     else:
        impatience += 1
        if impatience == patience:
             print(&quot;Out of patience. Stop training.&quot;)
             break
</code></pre>
<p>It runs up to 1 batch and then crushes.</p>
",Multilingual Language Processing & Language Identification,training bert running memory google colab keep running memory even bought google colab pro ha gb ram usage idea happening tried every kernel possible google colab google colab pro kaggle kernel amazon sagemaker google cloud platform reduced batch size success whatsoever goal train bert deep pavlov russian text classification extension predict emotion tweet multiclass classification class whole code run batch crush
Calculate frequency of word pattern in documents,"<p>I am trying to calculate frequency of word pattern in documents.
e.g. How many times word pattern &quot;Natural Language Processing&quot; is appearing in the documents.
I tried it using TF-IDF and Bag of words. however, it is giving me frequency of each word separately.</p>
<p>Is there any way to solve this problem using NLP, Text mining?</p>
<p>Thanks in advance</p>
",Multilingual Language Processing & Language Identification,calculate frequency word pattern document trying calculate frequency word pattern document e g many time word pattern natural language processing appearing document tried using tf idf bag word however giving frequency word separately way solve problem using nlp text mining thanks advance
extract nodes list between two nodes on Xquery,"<p>I work on an NLP project and i need to extract some informations form an XML document. Here is a piece of it. Each node item is a token with parts of speech, tag, lemma...</p>
<pre><code>&lt;basetalismane&gt;
&lt;file type=&quot;titre&quot; name=&quot;2017/01/01/19-00-00/0,2-3208,1-0,0.xml&quot;&gt;
&lt;p type=&quot;description&quot;&gt;
&lt;item&gt;&lt;a&gt;1&lt;/a&gt;&lt;a&gt;Le&lt;/a&gt;&lt;a&gt;le&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=s|g=m&lt;/a&gt;&lt;a&gt;2&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;2&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;2&lt;/a&gt;&lt;a&gt;bateau&lt;/a&gt;&lt;a&gt;bateau&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=s|g=m&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;se&lt;/a&gt;&lt;a&gt;se&lt;/a&gt;&lt;a&gt;CLR&lt;/a&gt;&lt;a&gt;CLR&lt;/a&gt;&lt;a&gt;n=p,s|p=3&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;aff&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;aff&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;rendait&lt;/a&gt;&lt;a&gt;rendre&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;n=s|t=I|p=3&lt;/a&gt;&lt;a&gt;0&lt;/a&gt;&lt;a&gt;root&lt;/a&gt;&lt;a&gt;0&lt;/a&gt;&lt;a&gt;root&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;5&lt;/a&gt;&lt;a&gt;sur&lt;/a&gt;&lt;a&gt;sur&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;6&lt;/a&gt;&lt;a&gt;l'&lt;/a&gt;&lt;a&gt;le&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=s&lt;/a&gt;&lt;a&gt;7&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;7&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;7&lt;/a&gt;&lt;a&gt;√Æle&lt;/a&gt;&lt;a&gt;√Æle&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;5&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;a&gt;5&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;8&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;9&lt;/a&gt;&lt;a&gt;Tidung&lt;/a&gt;&lt;a&gt;_&lt;/a&gt;&lt;a&gt;NPP&lt;/a&gt;&lt;a&gt;NPP&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;8&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;a&gt;8&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;,&lt;/a&gt;&lt;a&gt;,&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;9&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;9&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;11&lt;/a&gt;&lt;a&gt;destination&lt;/a&gt;&lt;a&gt;destination&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;12&lt;/a&gt;&lt;a&gt;touristique&lt;/a&gt;&lt;a&gt;touristique&lt;/a&gt;&lt;a&gt;ADJ&lt;/a&gt;&lt;a&gt;ADJ&lt;/a&gt;&lt;a&gt;n=s&lt;/a&gt;&lt;a&gt;11&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;11&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;13&lt;/a&gt;&lt;a&gt;√†&lt;/a&gt;&lt;a&gt;√†&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;11&lt;/a&gt;&lt;a&gt;dep&lt;/a&gt;&lt;a&gt;11&lt;/a&gt;&lt;a&gt;dep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;14&lt;/a&gt;&lt;a&gt;50&lt;/a&gt;&lt;a&gt;50&lt;/a&gt;&lt;a&gt;ADJ&lt;/a&gt;&lt;a&gt;ADJ&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;15&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;15&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;15&lt;/a&gt;&lt;a&gt;km&lt;/a&gt;&lt;a&gt;kilom√®tre&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;g=m&lt;/a&gt;&lt;a&gt;13&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;a&gt;13&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;17&lt;/a&gt;&lt;a&gt;Jakarta&lt;/a&gt;&lt;a&gt;_&lt;/a&gt;&lt;a&gt;NPP&lt;/a&gt;&lt;a&gt;NPP&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;18&lt;/a&gt;&lt;a&gt;,&lt;/a&gt;&lt;a&gt;,&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;17&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;17&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;quand&lt;/a&gt;&lt;a&gt;quand&lt;/a&gt;&lt;a&gt;CS&lt;/a&gt;&lt;a&gt;CS&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;20&lt;/a&gt;&lt;a&gt;le&lt;/a&gt;&lt;a&gt;le&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=s|g=m&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;moteur&lt;/a&gt;&lt;a&gt;moteur&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=s|g=m&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;22&lt;/a&gt;&lt;a&gt;a&lt;/a&gt;&lt;a&gt;avoir&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;n=s|t=P|p=3&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;aux_tps&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;aux_tps&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;eu&lt;/a&gt;&lt;a&gt;avoir&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;n=s|g=m|t=K&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;sub&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;sub&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;24&lt;/a&gt;&lt;a&gt;des&lt;/a&gt;&lt;a&gt;des&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=p&lt;/a&gt;&lt;a&gt;25&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;25&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;25&lt;/a&gt;&lt;a&gt;probl√®mes&lt;/a&gt;&lt;a&gt;probl√®me&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=p|g=m&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;obj&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;obj&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;26&lt;/a&gt;&lt;a&gt;,&lt;/a&gt;&lt;a&gt;,&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;25&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;25&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;27&lt;/a&gt;&lt;a&gt;puis&lt;/a&gt;&lt;a&gt;puis&lt;/a&gt;&lt;a&gt;CC&lt;/a&gt;&lt;a&gt;CC&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;coord&lt;/a&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;coord&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;28&lt;/a&gt;&lt;a&gt;a&lt;/a&gt;&lt;a&gt;avoir&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;n=s|t=P|p=3&lt;/a&gt;&lt;a&gt;29&lt;/a&gt;&lt;a&gt;aux_tps&lt;/a&gt;&lt;a&gt;29&lt;/a&gt;&lt;a&gt;aux_tps&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;29&lt;/a&gt;&lt;a&gt;explos√©&lt;/a&gt;&lt;a&gt;exploser&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;n=s|g=m|t=K&lt;/a&gt;&lt;a&gt;27&lt;/a&gt;&lt;a&gt;dep_coord&lt;/a&gt;&lt;a&gt;27&lt;/a&gt;&lt;a&gt;dep_coord&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;30&lt;/a&gt;&lt;a&gt;.&lt;/a&gt;&lt;a&gt;.&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;29&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;29&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;/p&gt;
&lt;p type=&quot;description&quot;&gt;
&lt;item&gt;&lt;a&gt;1&lt;/a&gt;&lt;a&gt;Il&lt;/a&gt;&lt;a&gt;il&lt;/a&gt;&lt;a&gt;CLS&lt;/a&gt;&lt;a&gt;CLS&lt;/a&gt;&lt;a&gt;n=s|g=m|p=3&lt;/a&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;2&lt;/a&gt;&lt;a&gt;a&lt;/a&gt;&lt;a&gt;avoir&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;n=s|t=P|p=3&lt;/a&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;aux_tps&lt;/a&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;aux_tps&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;annonc√©&lt;/a&gt;&lt;a&gt;annoncer&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;n=s|g=m|t=K&lt;/a&gt;&lt;a&gt;0&lt;/a&gt;&lt;a&gt;root&lt;/a&gt;&lt;a&gt;0&lt;/a&gt;&lt;a&gt;root&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;que&lt;/a&gt;&lt;a&gt;que&lt;/a&gt;&lt;a&gt;CS&lt;/a&gt;&lt;a&gt;CS&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;obj&lt;/a&gt;&lt;a&gt;3&lt;/a&gt;&lt;a&gt;obj&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;5&lt;/a&gt;&lt;a&gt;la&lt;/a&gt;&lt;a&gt;la&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;6&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;6&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;6&lt;/a&gt;&lt;a&gt;reconqu√™te&lt;/a&gt;&lt;a&gt;reconqu√™te&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;suj&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;7&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;6&lt;/a&gt;&lt;a&gt;dep&lt;/a&gt;&lt;a&gt;6&lt;/a&gt;&lt;a&gt;dep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;8&lt;/a&gt;&lt;a&gt;la&lt;/a&gt;&lt;a&gt;la&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;9&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;8&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;8&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;capitale&lt;/a&gt;&lt;a&gt;capitale&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;7&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;a&gt;7&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;11&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;12&lt;/a&gt;&lt;a&gt;autoproclam√©e&lt;/a&gt;&lt;a&gt;autoproclamer&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;n=s|g=f|t=K&lt;/a&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;10&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;13&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;12&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;12&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;14&lt;/a&gt;&lt;a&gt;l'&lt;/a&gt;&lt;a&gt;le&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=s&lt;/a&gt;&lt;a&gt;15&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;15&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;15&lt;/a&gt;&lt;a&gt;EI&lt;/a&gt;&lt;a&gt;_&lt;/a&gt;&lt;a&gt;NPP&lt;/a&gt;&lt;a&gt;NPP&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;13&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;a&gt;13&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;√©tait&lt;/a&gt;&lt;a&gt;√™tre&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;V&lt;/a&gt;&lt;a&gt;n=s|t=I|p=3&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;sub&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;sub&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;17&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;18&lt;/a&gt;&lt;a&gt;une&lt;/a&gt;&lt;a&gt;une&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;DET&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;det&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;question&lt;/a&gt;&lt;a&gt;question&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=s|g=f&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;obj&lt;/a&gt;&lt;a&gt;16&lt;/a&gt;&lt;a&gt;obj&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;20&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;de&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;P&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;dep&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;dep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;semaines&lt;/a&gt;&lt;a&gt;semaine&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;NC&lt;/a&gt;&lt;a&gt;n=p|g=f&lt;/a&gt;&lt;a&gt;20&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;a&gt;20&lt;/a&gt;&lt;a&gt;prep&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;22&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;&quot;&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;.&lt;/a&gt;&lt;a&gt;.&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;item&gt;&lt;a&gt;24&lt;/a&gt;&lt;a&gt;¬ß&lt;/a&gt;&lt;a&gt;¬ß&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;PONCT&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;a&gt;21&lt;/a&gt;&lt;a&gt;ponct&lt;/a&gt;&lt;/item&gt;
&lt;/p&gt;
&lt;/description&gt;
</code></pre>
<p>I work on syntactic dependencies. Here you can see that nodes item are tokens (with parts of speech tag etc... My task is to target item with a[8]='sub'. After that, i need to extract the words in relation between. It's a[9]. It's the index of the beginning of the syntactic dependence. In the first sentence (description node), the sub item is</p>
<pre><code>&lt;item&gt;&lt;a&gt;23&lt;/a&gt;&lt;a&gt;eu&lt;/a&gt;&lt;a&gt;avoir&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;VPP&lt;/a&gt;&lt;a&gt;n=s|g=m|t=K&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;sub&lt;/a&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;sub&lt;/a&gt;&lt;/item&gt;
</code></pre>
<p>I need to extract his a[9] (here is 19). In fact, it's the index of the first word of my syntactic dependecie. This is this item (basing on index a[1])</p>
<pre><code>&lt;item&gt;&lt;a&gt;19&lt;/a&gt;&lt;a&gt;quand&lt;/a&gt;&lt;a&gt;quand&lt;/a&gt;&lt;a&gt;CS&lt;/a&gt;&lt;a&gt;CS&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;a&gt;4&lt;/a&gt;&lt;a&gt;mod&lt;/a&gt;&lt;/item&gt;
</code></pre>
<p>What i have to do ? get all items (in fact a[2] value between the index of this word and my item
with 'sub'. In the first sentence, the following output would be</p>
<pre><code>quand le moteur a eu
</code></pre>
<p>it's an extraction of nodes between two nodes with index. But here is my following code. I can't grab the items nodes between each other item. Be careful, it may have more than one sub item by sentence so i needed to add a for loop</p>
<pre><code>for $p in /basetalismane/file/*//p
let $items := /$p//item[a[8]='sub']
for $p in /basetalismane/file/*//p
let $items := /$p//item[a[8]='sub']
for $item in $items
let $target := /$item/a[9]
let $source := /$item/a[1]
return (
for $i in ($target to $source)
return string-join( $p/item[$i]/a[2]  , ' '))
</code></pre>
<p>I get only each word but not the sequence. I can't concatenate strings one word by one.
i've done a return $nodes to see what i grab. It's only sub items. I want the item between. I would like a list of item or a string with their a[2] to have the words. In the second sentence, the following output would be</p>
<pre><code>que la reconquete autoproclam√©e de la &quot;capitale&quot; de l'EI √©tait
</code></pre>
<p>Thx for your help. I hope it's clear for you guys but it's hard to explains (i'm a french guy)</p>
",Multilingual Language Processing & Language Identification,extract node list two node xquery work nlp project need extract information form xml document piece node item token part speech tag lemma work syntactic dependency see node item token part speech tag etc task target item sub need extract word relation index beginning syntactic dependence first sentence description node sub item need extract fact index first word syntactic dependecie item basing index get item fact value index word item sub first sentence following output would extraction node two node index following code grab item node item careful may one sub item sentence needed add loop get word sequence concatenate string one word one done return node see grab sub item want item would like list item string word second sentence following output would thx help hope clear guy hard explains french guy
scikit-learn LogisticRegression Classify another value,"<p>i'm new to python and have to make a natural language processing task.
Using a kaggle dataset a sentiment classify should be implemented using python.
For this i'm using a dataframe and the LogisticRegression, as described in <a href=""https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6"" rel=""nofollow noreferrer"">this article</a> and everythin works fine.</p>
<p>Now i want to know if it is possible to classify another string which is not in the dataset, so that i can experiment with the classifier interactively.</p>
<p>Is this possible?
Thank you!</p>
",Multilingual Language Processing & Language Identification,scikit learn logisticregression classify another value new python make natural language processing task using kaggle dataset sentiment classify implemented using python using dataframe logisticregression described article everythin work fine want know possible classify another string dataset experiment classifier interactively possible thank
Regex to remove excess backslashes in python,"<p>I've the following text in a list:</p>
<p>text='More Fulham open to offers for Stefan Johansen, Jean Michael Seri and Maxime Le Marchand QPR are keen on Fulham\'s Stefan Johansen who was not included in Scott Parker\'s Premier League squad this season; Galatasaray want Jean Michael Seri back on loan while Maxime Le Marchand is attracting interest from former club Nice Fill 2 Copy 11 Wednesday 20 January 2021 16:51, UK Image: Queens Park Rangers are interested in signing Stefan Johansen\\nFulham are listening to offers for Stefan Johansen, Maxime Le Marchand and Jean Michael Seri during the January transfer window.\\nQueens Park Rangers are interested in signing Fulham midfielder Johansen. The Norwegian international was not included in Scott Parker\'s Premier League squad for the first half of this season.\\nJohansen, 30, has made 142 appearances for the club since arriving from Celtic in 2016, he is under contract at Craven Cottage until 2022. Parker: I don\'t owe Mourinho an apology\\nThe midfielder spent the second half of the 2018/19 campaign on loan at West Brom but returned ahead of the following season and played 33 times in the Championship as Fulham won promotion back to the Premier League via the play-offs. Image: Jean Michael Seri spent last season on loan at Galatasaray\\nTalks continue between Fulham and Galatasaray over the possibility of a loan deal for midfielder Jean Michael Seri. Trending Latest News\\nA move would see the 29-year-old central midfielder join the Turkish club on loan for a second time, with Seri having played 37 times for the Super Lig side while he was there last season.\\nSeri\'s contract expires in the summer of 2022, he has the option to extend the deal by a further year. Also See: Image: Maxime Le Marchand played 12 times for Fulham last season but has made just two appearances under Scott Parker so far this term\\nMeanwhile, Ligue 1 club Nice want defender Maxime Le Marchand to return to the Allianz Riviera.\\nFulham signed Le Marchand from the French side in July 2018 and the centre-back has featured just twice in the top flight so far this season.\\nLe Marchand also has a year and a half remaining on his current contract at Fulham.\\nFulham, who are 18th and four points from safety in the Premier League, play Manchester United (January 20), Burnley in the FA Cup (January 24), Brighton (January 27), and West Brom (January 30) before the end of the month.\\nFollow the January transfer window with Sky Sports\\nThe winter transfer window is open until Monday, February 1 at 11pm.\\nFollow all the news and analysis on Sky Sports News and across Sky Sports \' digital platforms, including our dedicated Transfer Centre blog'</p>
<p>and using the following regex with expecting to remove \:</p>
<pre><code>re.sub(r'\\\\\'',&quot;\\'&quot;,text))
</code></pre>
<p>however, output is cleaning the extra \</p>
<p>for example, the text has {don\\'t} and with following regex I intended to convert it to don't</p>
",Multilingual Language Processing & Language Identification,regex remove excess backslashes python following text list text fulham open offer stefan johansen jean michael seri maxime le marchand qpr keen fulham stefan johansen wa included scott parker premier league squad season galatasaray want jean michael seri back loan maxime le marchand attracting interest former club nice fill copy wednesday january uk image queen park ranger interested signing stefan johansen nfulham listening offer stefan johansen maxime le marchand jean michael seri january transfer window nqueens park ranger interested signing fulham midfielder johansen norwegian international wa included scott parker premier league squad first half season njohansen ha made appearance club since arriving celtic contract craven cottage parker owe mourinho apology midfielder spent second half campaign loan west brom returned ahead following season played time championship fulham back premier league via play offs image jean michael seri spent last season loan galatasaray ntalks continue fulham galatasaray possibility loan deal midfielder jean michael seri trending latest news na move would see year old central midfielder join turkish club loan second time seri played time super lig side wa last season nseri contract expires summer ha option extend deal year also see image maxime le marchand played time fulham last season ha made two appearance scott parker far term nmeanwhile ligue club nice want defender maxime le marchand return allianz riviera nfulham signed le marchand french side july centre back ha twice top flight far season nle marchand also ha year half remaining current contract fulham nfulham th four point safety premier league play manchester united january burnley fa cup january brighton january west brom january end month nfollow january transfer window sky sport winter transfer window open monday february pm nfollow news analysis sky sport news across sky sport digital platform including dedicated transfer centre blog using following regex expecting remove however output cleaning extra example text ha following regex intended convert
Spell Check/DidYouMean for Japanese language,"<p>looking for ideas for implementing Spellcheck/DidYouMean for the Japanese language (mostly).
The target for spellcheck is search queries, search engine build on solr, but the solution is not bound to it.</p>
<p>So far found two main approached:</p>
<ul>
<li>edit distance for dictionary (libraries like SymSpell)</li>
<li>statistic, based on user rewritten queries</li>
</ul>
<p>The first approach seems not very feasible for Kanji/Kana.
Also, its results as-is are quite noisy and it's complicated to build a lot of clean N-grams for contextual spellcheck (so 'hollow world' would be fixed as 'hello world').
Any suggestions on how it could be done?</p>
<p>The second approach is complicated because it's difficult to detect rewritten queries and since users rarely do so or do it correctly - it's hard to gather such statistics.</p>
<p>The main articles/videos that I found so far are quite a high level and too simple (for edit distance they don't provide applicable for real-world approaches to reduce noise to a reasonable level - 95% or higher) or focused on English only.</p>
<p>Any pointers for some published papers are welcome :)
Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,spell check didyoumean japanese language looking idea implementing spellcheck didyoumean japanese language mostly target spellcheck search query search engine build solr solution bound far found two main approached edit distance dictionary library like symspell statistic based user rewritten query first approach seems feasible kanji kana also result quite noisy complicated build lot clean n gram contextual spellcheck hollow world would fixed hello world suggestion could done second approach complicated difficult detect rewritten query since user rarely correctly hard gather statistic main article video found far quite high level simple edit distance provide applicable real world approach reduce noise reasonable level higher focused english pointer published paper welcome thanks advance
Are there any Python libraries to convert a sentence in hiragana to kanji?,"<p>If I have a phrase or sentence written in Hiragana such as „Äå„Åä„Åµ„Çç„ÅØ„ÅÑ„Çã„Äç I would like to translate/guess at the appropriate Kanji for the string.</p>
<p>I have found libraries for going from Kanji to Hiragana, Katakana, Romaji, or English, but I haven't found ones that go the other way. Dictionaries work fine for single words but not sentences.</p>
",Multilingual Language Processing & Language Identification,python library convert sentence hiragana kanji phrase sentence written hiragana would like translate guess appropriate kanji string found library going kanji hiragana katakana romaji english found one go way dictionary work fine single word sentence
Python prints Arabic text that includes punctuation incorrectly although it seems to be correctly stored,"<p>There is strange behavior on printing Arabic text that includes punctuation in Python.</p>
<p>Example:</p>
<pre><code>word_1 = &quot;...Ÿàÿßÿ≠ÿØ&quot;
word_2 = &quot;ÿßÿ´ŸÜÿßŸÜ&quot;
word_list = [word_1, word_2]
print(word_list)
</code></pre>
<p>The output of the previous code is <code>['...Ÿàÿßÿ≠ÿØ', 'ÿßÿ´ŸÜÿßŸÜ']</code> but the correct output should be <code>['Ÿàÿßÿ≠ÿØ...', 'ÿßÿ´ŸÜÿßŸÜ']</code>. This problem seems to exist only when I print the output. However, when I retrieve the first index I see the correct form. So the output of <code>word_list[0]</code> is <code>...Ÿàÿßÿ≠ÿØ</code> which is correct.</p>
<p>The problem is that I want to print something to the user at the end so I want to print the output correctly. Therefore, I want the output to be <code>['Ÿàÿßÿ≠ÿØ...', 'ÿßÿ´ŸÜÿßŸÜ']</code>.</p>
<p>I would appreciate any help. Thank you in advance.</p>
",Multilingual Language Processing & Language Identification,python print arabic text includes punctuation incorrectly although seems correctly stored strange behavior printing arabic text includes punctuation python example output previous code correct output problem seems exist print output however retrieve first index see correct form output correct problem want print something user end want print output correctly therefore want output would appreciate help thank advance
How to break a corpus into paragraphs using custom delimiters,"<p>I am scraping the New york Times webpages to do some natural language processing on it, I want to split the webpage into paragraphs when using corpus in order to do frequency counts on words that appear in paragraphs which also contain key words or phrases .</p>
<p>The below works with sentences but the paragraphs are donated by a ‚Ä¢ in NYT, so I need to replace this into how corpus reads paragraphs - anyone got any idea's? I have tried gsub(&quot;‚Ä¢&quot;,&quot;/n&quot;,...) and gsub(&quot;‚Ä¢&quot;,&quot;/r/n&quot;) but this didn't work.</p>
<p>If anyone knows how to do this all in tm corpus's rather than having to switch between quanteda and TM that would save some code.</p>
<pre><code> website&lt;-read_html(&quot;https://www.nytimes.com/2017/01/03/briefing/asia-australia-briefing.html&quot;) #Read URL
     


  #Obtain any text with the paragraph Html deliminator 
  text&lt;-website%&gt;%
    html_nodes(&quot;p&quot;) %&gt;%
    html_text() %&gt;% as.character()
  
  #Collapse the string as it is currently text[1]=para1 and text[2]= para 2
  text&lt;- str_c(text,collapse=&quot; &quot;)


data_corpus_para &lt;- 
  corpus_reshape(corpus((text),to=&quot;paragraphs&quot;))


data_corpus_para &lt;-tolower(data_corpus_para )


containstarget &lt;- 
  stringr::str_detect(texts(data_corpus_para ), &quot;pull out of peace talks&quot;) #Random string in only one of the paragraphs to proof concept

#Filter for the para's that only contain the sentence above
data_corpus_para &lt;- 
  corpus_subset(data_corpus_para , containstarget)                 

data_corpus_para &lt;-corpus_reshape(data_corpus_para , to = &quot;documents&quot;)


#There are quanteda corpus and TM Corpuses. And so I have to convert to a dataframe and then make back into a vcorupus.. this is very messy

data_corpus_para &lt;-quanteda::convert(data_corpus_para )
data_corpus_para_VCorpus&lt;-tm::VCorpus(tm::VectorSource(data_corpus_para$text))

dt.dtm = tm::DocumentTermMatrix(data_corpus_para_VCorpus)
tm::findFreqTerms(dt.dtm, 1)
</code></pre>
",Multilingual Language Processing & Language Identification,break corpus paragraph using custom delimiters scraping new york time webpage natural language processing want split webpage paragraph using corpus order frequency count word appear paragraph also contain key word phrase work sentence paragraph donated nyt need replace corpus read paragraph anyone got idea tried gsub n gsub r n work anyone know tm corpus rather switch quanteda tm would save code
GATE extract NE from documents in German,"<p>I need to extract people names from documents in German (not my native). After a bit of search, I've found GATE framework  which seems to support English, German and many other languages. The accuracy for English is quite decent, but it's unacceptable for German (see screenshots).</p>
<p>Here are the PRs:
<a href=""https://i.sstatic.net/l5ybm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l5ybm.png"" alt=""enter image description here"" /></a></p>
<p>And a chunk of highlighted people names:
<a href=""https://i.sstatic.net/4SiqN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4SiqN.jpg"" alt=""enter image description here"" /></a></p>
<p>A friend of mine says that none of these is a person name, so I wonder if I misconfigured something. Do I need to specify the language somehow?</p>
",Multilingual Language Processing & Language Identification,gate extract ne document german need extract people name document german native bit search found gate framework seems support english german many language accuracy english quite decent unacceptable german see screenshots pr chunk highlighted people name friend mine say none person name wonder misconfigured something need specify language somehow
How to extract only the english words from the list?,"<p>I tried to extract only the English words from the following list:</p>
<pre><code>l = ['0', 'b', 'x14', 'x00', 'x1fP', 'xe0O', 'xd0', 'xea', 'i', 'x10', 'xa2', 'xd8', 'x08', 'x00', '00', 'x9d', 'x14', 'x00', 'x80', 'xcc', 'xbf', 'xb4', 'xdbLB', 'xb0', 'x7f', 'xe9', 'x9a', 'x87', 'xc6AZ', 'x005', 'x00', 'x00', 'x00', 'x00', 'x00yR', 'G', 'x10', 'x00', 'xdc', 'x05', 'xde', 'x05', 'xe2', 'x05', 'xe8', 'x05', 'xdb', 'x05', 'xea', 'x05', 'x00', 'x00', 'x00', 't', 'x00', 'x04', 'x00', 'xef', 'xbeyRnDyR', 'G', 'x00', 'x00', 'x00', 'xe5E', 'x00', 'x00', 'x00', 'x00', 'xfb', 'x05', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'xe2', 'x0e', 'x00', 'xdc', 'x05', 'xde', 'x05', 'xe2', 'x05', 'xe8', 'x05', 'xdb', 'x05', 'xea', 'x05', 'x00', 'x00', 'x1c', 'x00', 'x80', 'x001', 'x00', 'x00', 'x00', 'x00', 'x00yR', 'G', 'x10', 'x00VBS', '', '', 'x00', 'x00', 't', 'x00', 'x04', 'x00', 'xef', 'xbeyR', 'GyR', 'G', 'x00', 'x00', 'x00', 'x9e', 'xa5', 'x00', 'x00', 'x00', 'x00K', 'x02', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'xe2', 'x0e', 'x00V', 'x00B', 'x00S', 'x00', 'x00R', 'x00a', 'x00n', 'x00s', 'x00o', 'x00m', 'x00w', 'x00a', 'x00r', 'x00e', 'x00', 'x00', 'x00', 'x00d', 'x00o', 'x00n', 'x00e', 'x00', 'x00', 'x00', 'x00', 'x80', 'x001', 'x00', 'x00', 'x00', 'x00', 'x00yRmG', 'x10', 'x00VBS', '', '', 'x00', 'x00', 't', 'x00', 'x04', 'x00', 'xef', 'xbeyR', 'GyRmG', 'x00', 'x00', 'x00', 'xb6', 'xba', 'x00', 'x00', 'x00', 'x00', 'xa4', 'x01', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x98w', 'x00V', 'x00B', 'x00S', 'x00', 'x00R', 'x00a', 'x00n', 'x00s', 'x00o', 'x00m', 'x00w', 'x00a', 'x00r', 'x00e', 'x00', 'x00', 'x00', 'x00d', 'x00o', 'x00n', 'x00e', 'x00', 'x00', 'x00', 'x00', 'xa4', 'x002', 'x00c', 'xf1', 'x02', 'x00oRjX', 'Test', 'For', 'SO', 'PDF', 'pdf', 'x00t', 'x00', 't', 'x00', 'x04', 'x00', 'xef', 'xbeyR', 'GyR', 'G', 'x00', 'x00', 'x00', 'xcf', 'xbc', 'x00', 'x00', 'x00', 'x00z', 'x04', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'x00', 'xd23', 'x98', 'x00D', 'x00e', 'x00f', 'x00e', 'x00n', 'x00s', 'x00e', 'x00', 'x00R', 'x00u', 'x00l', 'x00e', 'x00', 'x00', 'x00', 'x00V', 'x00B', 'x00S', 'x00', 'x00R', 'x00a', 'x00n', 'x00s', 'x00o', 'x00m', 'x00w', 'x00a', 'x00r', 'x00e', 'x00', 'x00p', 'x00d', 'x00f', 'x00', 'x00', 'x000', 'x00', 'x00', 'x00', '3']
</code></pre>
<p>From this list, the words I need are <code>[&quot;Test&quot;, &quot;For&quot;, &quot;SO&quot;, &quot;PDF&quot;]</code>.</p>
<p>I tried the following:</p>
<pre><code>for i in range(num_of_values):
    values = EnumValue(key, i)
    res = re.findall(r'\w+', str(values))
    print(res)
</code></pre>
<p>Did anyone manage to extract the words?</p>
",Multilingual Language Processing & Language Identification,extract english word list tried extract english word following list list word need tried following anyone manage extract word
Why is both identical sentence&#39;s METEOR score used in Machine Translation not equals to one?,"<p>I have both identical sentences. But METEOR Score is not 1 it's 0.99.</p>
<p>Here's the code:  <code>score1 = nltk.translate.meteor_score.meteor_score(sentence1,sentence2)</code></p>
",Multilingual Language Processing & Language Identification,identical sentence meteor score used machine translation equal one identical sentence meteor score code
Python: Importing a large function only when needed,"<p>I want to use <code>spacy</code> in my module for some natural language processing. I can load the function <code>nlp</code> as</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_lg&quot;) 
</code></pre>
<p>which takes a long time because the database is nearly 1gb. Suppose the above code is in the file <code>nlp_file.py</code>. If I want to use the function <code>nlp</code> in my module importing using <code>from nlp_file import nlp</code>, I can do one of a few things:</p>
<ol>
<li>Load it when my module is imported in the <code>__init__.py</code> file.</li>
<li>Load it at the beginning of each file that uses it.</li>
<li>Load only when I need to call <code>nlp</code>.</li>
</ol>
<p>I think that 3 is the best option here, but goes against all my python instincts to put import statements at the top of files. If I put it in  <code>__init__.py</code> then everytime I want to use part of my module that doesn't require nlp I will take a massive performance hit. If I load it at the beginning of each file that uses it, I will have the same problem. Therefore it makes most sense to import at the beginning of functions and classes that require nlp.</p>
<p>Am I correct in believing this? Is there a better way to use <code>nlp</code> in different parts of my module? Thank you in advance!</p>
<p>Similar question: <a href=""https://stackoverflow.com/questions/53359891/is-it-good-practice-to-load-large-objects-in-one-file-and-import-them-in-other-p"">Is it good practice to load large objects in one file and import them in other Python files?</a></p>
",Multilingual Language Processing & Language Identification,python importing large function needed want use module natural language processing load function take long time database nearly gb suppose code file want use function module importing using one thing load module imported file load beginning file us load need call think best option go python instinct put import statement top file put everytime want use part module require nlp take massive performance hit load beginning file us problem therefore make sense import beginning function class require nlp correct believing better way use different part module thank advance similar question href good practice load large object one file import python file
Natural Language date and time parser for java,"<p>I am working on a Natural Language parser which examines a sentence in english and extracts some information like name, date etc.</p>

<p>for example: ""<em>Lets meet next tuesday at 5 PM at the beach.</em>""</p>

<p>So the output will be something like : ""<em>Lets meet 15/09/2009 at 1700 hr at the beach</em>""</p>

<p>So basically, what i want to know is that <strong>is there any framework or library available for JAVA to do these kind of operations like parsing dates from a sentence and give a output with some specified format.</strong> </p>

<p>Regards,
Pranav</p>

<hr>

<p>Thanks for the replies. I have looked on few NLPs like <a href=""http://alias-i.com/lingpipe/index.html"" rel=""noreferrer"">LingPipe</a>, OpenPL, <a href=""http://nlp.stanford.edu/index.shtml"" rel=""noreferrer"">Stanford NLP</a>. I wanted to ask do they hav anything for date parsing for java.</p>
",Multilingual Language Processing & Language Identification,natural language date time parser java working natural language parser examines sentence english extract information like name date etc example let meet next tuesday pm beach output something like let meet hr beach basically want know framework library available java kind operation like parsing date sentence give output specified format regard pranav thanks reply looked nlp like lingpipe openpl stanford nlp wanted ask hav anything date parsing java
Plural to singular of french words in python,"<p>I have a list of words and I'm trying to turn plural words in singular in python, then I remove the duplicates. This is how I do it :</p>
<pre><code>import spacy
nlp = spacy.load('fr_core_news_md')

words = ['animaux', 'poule', 'adresse', 'animal', 'janvier', 'poules']
clean_words = []

for word in words:
    doc = nlp(word)
    
for token in doc:
    clean_words.append(token.lemma_)
    
clean_words = list(set(clean_words))
</code></pre>
<p>This is the output :</p>
<pre><code>['animal', 'janvier', 'poule', 'adresse']
</code></pre>
<p>It works well, but my problem is that 'fr_core_news_md' takes a little too long to load so I was wondering if there was another way to do this ?</p>
",Multilingual Language Processing & Language Identification,plural singular french word python list word trying turn plural word singular python remove duplicate output work well problem fr core news md take little long load wa wondering wa another way
Transliterate sentence written in 2 different scripts to a single script,"<p>I am able to convert an Hindi script written in English back to Hindi</p>
<pre><code>import codecs,string
from indic_transliteration import sanscript
from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate


def is_hindi(character):
    maxchar = max(character)
    if u'\u0900' &lt;= maxchar &lt;= u'\u097f':
    return character
    else:
        print(transliterate(character, sanscript.ITRANS, sanscript.DEVANAGARI)

character = 'bakrya'
is_hindi(character)

Output:
‡§¨‡§ï‡•ç‡§∞‡•ç‡§Ø
</code></pre>
<p>But If I try to do something like this, I don't get any conversions</p>
<pre><code>character = 'Bakrya ‡§µ‡§ø‡§ï‡§£‡•á ‡§Ü‡§π‡•á'
is_hindi(character)

Output:
Bakrya ‡§µ‡§ø‡§ï‡§£‡•á ‡§Ü‡§π‡•á

Expected Output:
‡§¨‡§ï‡•ç‡§∞‡•ç‡§Ø ‡§µ‡§ø‡§ï‡§£‡•á ‡§Ü‡§π‡•á
</code></pre>
<p>I also tried the library Polyglot but I am getting similar results with it.</p>
",Multilingual Language Processing & Language Identification,transliterate sentence written different script single script able convert hindi script written english back hindi try something like get conversion also tried library polyglot getting similar result
POS tagging and NER for Chinese Text with Spacy,"<ul>
<li>I am trying to print the entities and pos present in Chinese text. </li>
<li>I have installed # !pip3 install jieba and used Google colab for the below script.</li>
</ul>

<p>But I am getting empty tuples for the entities and no results for pos_.</p>

<pre class=""lang-py prettyprint-override""><code>from spacy.lang.zh import Chinese

nlp = Chinese()
doc = nlp(u""ËòãÊûúÂÖ¨Âè∏Ê≠£ËÄÉÈáèÁî®‰∏ÄÂÑÑÂÖÉË≤∑‰∏ãËã±ÂúãÁöÑÊñ∞ÂâµÂÖ¨Âè∏"")

doc.ents
# returns (), i.e. empty tuple


for word in doc:
    print(word.text, word.pos_)

''' returns
ËòãÊûú 
ÂÖ¨Âè∏ 
Ê≠£ 
ËÄÉÈáè 
Áî® 
‰∏Ä 
ÂÑÑÂÖÉ 
Ë≤∑ 
‰∏ã 
Ëã±Âúã 
ÁöÑ 
Êñ∞Ââµ 
ÂÖ¨Âè∏ 
'''

</code></pre>

<p>I am new to NLP. I want to know what is the correct way to do ?</p>
",Multilingual Language Processing & Language Identification,po tagging ner chinese text spacy trying print entity po present chinese text installed pip install jieba used google colab script getting empty tuples entity result po new nlp want know correct way
POS tagging and NER for Chinese Text with Spacy,"<ul>
<li>I am trying to print the entities and pos present in Chinese text. </li>
<li>I have installed # !pip3 install jieba and used Google colab for the below script.</li>
</ul>

<p>But I am getting empty tuples for the entities and no results for pos_.</p>

<pre class=""lang-py prettyprint-override""><code>from spacy.lang.zh import Chinese

nlp = Chinese()
doc = nlp(u""ËòãÊûúÂÖ¨Âè∏Ê≠£ËÄÉÈáèÁî®‰∏ÄÂÑÑÂÖÉË≤∑‰∏ãËã±ÂúãÁöÑÊñ∞ÂâµÂÖ¨Âè∏"")

doc.ents
# returns (), i.e. empty tuple


for word in doc:
    print(word.text, word.pos_)

''' returns
ËòãÊûú 
ÂÖ¨Âè∏ 
Ê≠£ 
ËÄÉÈáè 
Áî® 
‰∏Ä 
ÂÑÑÂÖÉ 
Ë≤∑ 
‰∏ã 
Ëã±Âúã 
ÁöÑ 
Êñ∞Ââµ 
ÂÖ¨Âè∏ 
'''

</code></pre>

<p>I am new to NLP. I want to know what is the correct way to do ?</p>
",Multilingual Language Processing & Language Identification,po tagging ner chinese text spacy trying print entity po present chinese text installed pip install jieba used google colab script getting empty tuples entity result po new nlp want know correct way
how does a natural language date parser work?,"<p>can someone explain quickly what are the steps that a natural language parser executes to achieve its work?
are the steps the same regardless of language to translate or the language determines the way of translating?</p>
<p>Does exist some multilanguage parser in javascript? I need one in Italian or one I can expand with Italian sentences.</p>
<p>Anyway, my requirements are very simple, the sentences to translate are more or less syntactically similar, so at first, I thought to use a long &quot;if else/switch&quot;  block plus some regular expressions</p>
<ul>
<li>one week ago</li>
<li>three weeks ago</li>
<li>about 5 months ago</li>
<li>friday</li>
</ul>
",Multilingual Language Processing & Language Identification,doe natural language date parser work someone explain quickly step natural language parser executes achieve work step regardless language translate language determines way translating doe exist multilanguage parser javascript need one italian one expand italian sentence anyway requirement simple sentence translate le syntactically similar first thought use long else switch block plus regular expression one week ago three week ago month ago friday
Would it be very inefficient to have a repository of words to check a single language against?,"<p>I am doing some NLP with Python on YouTube comments I have downloaded, and I only want to process English ones. So far I have experimented with different libraries (many of the ones discussed in <a href=""https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language"">this</a> thread) and it works fine for longer strings, but many of the libraries often run into problems with the shorter, one or two worders. My question is whether it would be hopelessly inefficient to download a dictionary of English words and check each of these short, problematic comments against it, obviously discarding the ones that don't match.</p>
<p>I can forsee problems with things such as misspellings or words that appear in English and a foreign language, but at present I am more concerned about speed as I have about 68 million comments to process.</p>
",Multilingual Language Processing & Language Identification,would inefficient repository word check single language nlp python youtube comment downloaded want process english one far experimented different library many one discussed href thread work fine longer string many library often run problem shorter one two worders question whether would hopelessly inefficient download dictionary english word check short problematic comment obviously discarding one match p forsee problem thing misspelling word appear english foreign language present concerned speed million comment process
How to add the Count Vectorizer to Simple RNN model?,"<p>For my <code>NLP</code> project I used <code>CountVectorizer</code> to Extract Features from a dataset using vectorizer = CountVectorizer(stop_words='english') and all_features = vectorizer.fit_transform(data.Text)  and i also wrote a Simple RNN model using keras but I am not sure how to do the padding and the tokeniser step and get the data be trained on the model.</p>
<p>my code for RNN is:</p>
<pre><code>model.add(keras.layers.recurrent.SimpleRNN(units = 1000, activation='relu',
use_bias=True))
model.add(keras.layers.Dense(units=1000, input_dim = 2000, activation='sigmoid'))
model.add(keras.layers.Dense(units=500, input_dim=1000, activation='relu'))
model.add(keras.layers.Dense(units=2, input_dim=500,activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

return model
</code></pre>
<p>can someone please give me some advice on this?</p>
<p>Thank you</p>
",Multilingual Language Processing & Language Identification,add count vectorizer simple rnn model project used extract feature dataset using vectorizer countvectorizer stop word english feature vectorizer fit transform data text also wrote simple rnn model using kera sure padding tokeniser step get data trained model code rnn someone please give advice thank
spacy-udpipe with pytextrank to extract keywords from non-English text,"<p>I've been using pytextrank (<a href=""https://github.com/DerwenAI/pytextrank/"" rel=""nofollow noreferrer"">https://github.com/DerwenAI/pytextrank/</a>) with spacy and English models for keywords extraction - it works great!</p>

<p>Now I need to process non-English texts and I found udpipe (<a href=""https://github.com/TakeLab/spacy-udpipe"" rel=""nofollow noreferrer"">https://github.com/TakeLab/spacy-udpipe</a>) but it doesn't work out of the box ... after </p>

<pre><code>nlp = spacy_udpipe.load(""sk"")
tr = pytextrank.TextRank()
nlp.add_pipe(tr.PipelineComponent, name=""textrank"", last=True)
doc = nlp(text)
</code></pre>

<p>I get tokens with POS and DEP tags, but there is nothing in <code>doc._.phrases</code> (<code>doc.noun_chunks</code> is also empty) and in <code>nlp.pipe_names</code> is just ['textrank']</p>

<p>What should I add to the spacy's pipeline to get it working? I assume pytextrank needs noun_chunks... </p>

<p>Any tip or suggestion where to look will help me - thanks!</p>
",Multilingual Language Processing & Language Identification,spacy udpipe pytextrank extract keywords non english text using pytextrank spacy english model keywords extraction work great need process non english text found udpipe work box get token po dep tag nothing also empty textrank add spacy pipeline get working assume pytextrank need noun chunk tip suggestion look help thanks
Is there an embeddings technique to represent multilingual paragraphs?,"<p>I have a dataset that includes English, Spanish and German documents. I want to represent them using document embeddings techniques to compute their similarities. However, as the documents are in different languages and the length of each one is paragraph-sized, it is difficult to find a pre-trained model (I do not have enough data for training)  .</p>
<p>I found some interesting models like <a href=""https://arxiv.org/pdf/1907.04307.pdf"" rel=""nofollow noreferrer"">Sent2Vec</a> and <a href=""https://medium.com/the-artificial-impostor/multilingual-similarity-search-using-pretrained-bidirectional-lstm-encoder-e34fac5958b0"" rel=""nofollow noreferrer"">LASER</a> that also work on multilingual context. However, both of them have been implemented for sentence representation. The question is two folds:</p>
<ul>
<li>Is there any model that could be used to represent multilingual paragraphs?</li>
<li>Is it possible to employ sent2vec (or LASER) to represent paragraphs (I mean to represent each paragraph using an embeddings vector)?</li>
</ul>
<p>Any help would be appreciated.</p>
",Multilingual Language Processing & Language Identification,embeddings technique represent multilingual paragraph dataset includes english spanish german document want represent using document embeddings technique compute similarity however document different language length one paragraph sized difficult find pre trained model enough data training found interesting model like sent vec laser also work multilingual context however implemented sentence representation question two fold model could used represent multilingual paragraph possible employ sent vec laser represent paragraph mean represent paragraph using embeddings vector help would appreciated
Restore original text from Keras‚Äôs imdb dataset,"<p>Restore original text from Keras‚Äôs imdb dataset</p>

<p>I want to restore imdb‚Äôs original text from Keras‚Äôs imdb dataset.</p>

<p>First, when I load Keras‚Äôs imdb dataset, it returned sequence of word index.</p>

<p>

<pre><code>&gt;&gt;&gt; (X_train, y_train), (X_test, y_test) = imdb.load_data()
&gt;&gt;&gt; X_train[0]
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
</code></pre>

<p>I found imdb.get_word_index method(), it returns word index dictionary like {‚Äòcreate‚Äô: 984, ‚Äòmake‚Äô: 94,‚Ä¶}. For converting, I create index word dictionary.


<pre><code>&gt;&gt;&gt; word_index = imdb.get_word_index()
&gt;&gt;&gt; index_word = {v:k for k,v in word_index.items()}
</code></pre>

<p>Then, I tried to restore original text like following.</p>

<p>

<pre><code>&gt;&gt;&gt; ' '.join(index_word.get(w) for w in X_train[5])
""the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i""
</code></pre>

<p>I‚Äôm not good at English, but I know this sentence is something strange.</p>

<p>Why is this happened? How can I restore original text?</p>
",Multilingual Language Processing & Language Identification,restore original text kera imdb dataset restore original text kera imdb dataset want restore imdb original text kera imdb dataset first load kera imdb dataset returned sequence word index found imdb get word index method return word index dictionary like create make converting create index word dictionary tried restore original text like following good english know sentence something strange happened restore original text
Part of sentence alignement,"<p>I work on parallel texts in two languages.
Some tools exist to align sentences (ex: Hunalign).
But sentences are often too long for my use case. So I would like to align part of sentences.</p>
<p>Here is an example:</p>
<p><strong>Input</strong>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>French</th>
<th>English</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bonjour ! L'ann√©e derni√®re, je suis all√© √† Paris qui est la capitale de la France.</td>
<td>Hello! Last year, I went to Paris which is the capital of France.</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Existing result</strong> : Sentence level alignement:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>French</th>
<th>English</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bonjour !</td>
<td>Hello!</td>
</tr>
<tr>
<td>L'ann√©e derni√®re, je suis all√© √† Paris qui est la capitale de la France.</td>
<td>Last year, I went to Paris which is the capital of France.</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Expected result</strong>: Part of sentence alignment:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>French</th>
<th>English</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bonjour !</td>
<td>Hello!</td>
</tr>
<tr>
<td>L'ann√©e derni√®re,</td>
<td>Last year,</td>
</tr>
<tr>
<td>je suis all√© √† Paris</td>
<td>I went to Paris</td>
</tr>
<tr>
<td>qui est la capitale de la France.</td>
<td>which is the capital of France.</td>
</tr>
</tbody>
</table>
</div>
<p>Google Translate offers a hover function, which seems to do part of the job internally.
<a href=""https://i.sstatic.net/fWrsG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fWrsG.png"" alt=""Example with Google Translate"" /></a></p>
<p>Any idea about how to get this result from Google Translate API?
Any idea about how to do it with any other tool? I'm trying to cut sentences in chunks and evaluate similarity with Facebook LASER and Google Universal Sentence Encoder, without much success so far.</p>
<p>Ideally, I'm looking for a solution which is language agnostic, much like LASER is. Or which supports ~40 most common languages.</p>
<p>Thanks in advance for your kind advice</p>
",Multilingual Language Processing & Language Identification,part sentence alignement work parallel text two language tool exist align sentence ex hunalign sentence often long use case would like align part sentence example input french english bonjour l ann e derni je suis paris qui est la capitale de la france hello last year went paris capital france existing result sentence level alignement french english bonjour hello l ann e derni je suis paris qui est la capitale de la france last year went paris capital france expected result part sentence alignment french english bonjour hello l ann e derni last year je suis paris went paris qui est la capitale de la france capital france google translate offer hover function seems part job internally idea get result google translate api idea tool trying cut sentence chunk evaluate similarity facebook laser google universal sentence encoder without much success far ideally looking solution language agnostic much like laser support common language thanks advance kind advice
How to loop over a text file (to remove tags and normalize) using python,"<p>I am trying to loop over a text file to remove tags, punctuation, stopwords. I have already used Python 3.8.3 Beautiful Soup to scrape the text (newspaper articles) from the site. It returns a list that I saved as a file. However, I am not sure how to use a loop in order to process all the items in the text file.</p>
<p>In the code below  I have used  listfilereduced.text(containing data from one news article,  <a href=""https://drive.google.com/file/d/1ojwN4u8cmh_nUoMJpdZ5ObaGW5URYYj3/view?usp=sharing"" rel=""nofollow noreferrer"">link to listfilereduced.txt here</a>), however I would like to run this code on listfile.text(containing data from multiple articles, <a href=""https://drive.google.com/file/d/1V3s8w8a3NQvex91EdOhdC9rQtCAOElpm/view?usp=sharing"" rel=""nofollow noreferrer"">link to listfile.text</a>). Any help would be greatly appreciated!</p>
<p>P.S. The text is in a Non-English script, but the tags are all in English.</p>
<pre><code>#This text file contains just one news item

with open('listfilereduced.txt', 'r', encoding='utf8') as my_file:
    rawData = my_file.read()
    print(rawData)

#Separating body text from other data

articleStart = rawData.find(&quot;&lt;div class=\&quot;story-element story-element-text\&quot;&gt;&quot;)
articleData = rawData[:articleStart]
articleBody = rawData[articleStart:]
print(articleData)
print(&quot;*******&quot;)
print(articleBody)
print(&quot;*******&quot;)


#First, I define a function to strip tags from the body text

def stripTags(pageContents):
    insideTag = 0
    text = ''

    for char in pageContents:
        if char == '&lt;':
            insideTag = 1
        elif (insideTag == 1 and char == '&gt;'):
            insideTag = 0
        elif insideTag == 1:
            continue
        else:
            text += char
    return text

#Calling the function

articleBodyText = stripTags(articleBody)
print(articleBodyText)

 
#Isolating article title and publication date

TitleEndLoc = articleData.find(&quot;&lt;/h1&gt;&quot;)
dateStartLoc = articleData.find(&quot;&lt;div class=\&quot;storyPageMetaData-m__publish-time__19bdV\&quot;&gt;&quot;)
dateEndLoc=articleData.find(&quot;&lt;div class=\&quot;meta-data-icons storyPageMetaDataIcons-m__icons__3E4Xg\&quot;&gt;&quot;)
titleString = articleData[:TitleEndLoc]
dateString = articleData[dateStartLoc:dateEndLoc]


#Call stripTags function to clean
articleTitle= stripTags(titleString)
articleDate = stripTags(dateString)

print(articleTitle)
print(articleDate)

#Cleaning the date a bit more

startLocDate = articleDate.find(&quot;:&quot;)
endLocDate = articleDate.find(&quot;,&quot;)
articleDateClean = articleDate[startLocDate+2:endLocDate]
print(articleDateClean)


#save all this data to a dictionary that saves the title, data and the body text 
PAloTextDict = {&quot;Title&quot;: articleTitle, &quot;Date&quot;: articleDateClean, &quot;Text&quot;: articleBodyText}
print(PAloTextDict)

#Normalize text by: 

#1. Splitting paragraphs of text into lists of words
articleBodyWordList = articleBodyText.split()
print(articleBodyWordList)


#2.Removing punctuation and stopwords

#https://bnlp.readthedocs.io/en/latest/

from bnlp.corpus import stopwords, punctuations

#A. Remove punctuation first

listNoPunct = []

for word in articleBodyWordList:
    for mark in punctuations:
        word=word.replace(mark, '')
    listNoPunct.append(word)
print(listNoPunct)



#B. removing stopwords
banglastopwords = stopwords()
print(banglastopwords)
 
cleanList=[]
for word in listNoPunct:
    if word in banglastopwords:
        continue
    else:
        cleanList.append(word)
print(cleanList)
</code></pre>
",Multilingual Language Processing & Language Identification,loop text file remove tag normalize using python trying loop text file remove tag punctuation stopwords already used python beautiful soup scrape text newspaper article site return list saved file however sure use loop order process item text file code used listfilereduced text containing data one news article link listfilereduced txt however would like run code listfile text containing data multiple article link listfile text help would greatly appreciated p text non english script tag english
How to word_tokenize pandas dataframe,"<p>My <strong>pandas dataframe</strong> (<code>df.tweet</code>) consits of one column with <strong>german tweets</strong>, I already did the data cleaning and dropped the columns I don¬¥t need. Now I want to <strong>word_tokenize</strong> the tweets in the pandas dataframe.
With TextBlob it only works for strings and I¬¥m only able to tokenize the dataframe string by string (see code below). I used <strong>textblob-de</strong> because it tokenizes german text.</p>
<p>Is there an opportunity to getting the tokenization done for the whole dataframe with a for loop? I¬¥m new to Python and NLP and really stack at that point. Some help would be great!</p>
<p>This is what I have:</p>
<pre><code>pip install -U textblob-de
from textblob_de import TextBlobDE as TextBlob
TextBlob(df.tweet [1]).words
</code></pre>
",Multilingual Language Processing & Language Identification,word tokenize panda dataframe panda dataframe consits one column german tweet already data cleaning dropped column need want word tokenize tweet panda dataframe textblob work string able tokenize dataframe string string see code used textblob de tokenizes german text opportunity getting tokenization done whole dataframe loop new python nlp really stack point help would great
Tokenizing dutch words,"<p>Reading <a href=""https://ilmoirfan.com/multi-lingual-support-in-nltk-for-sentence-tokenization/"" rel=""nofollow noreferrer"">this</a> article I found out that I can use a Dutch sentence tokenizer as follows:</p>
<pre><code>nltk.download('punkt')
tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')
tokernizer.tokenize('Ik liep naar huis. Dat deed ik gisteren')
</code></pre>
<p>However is there a way of using a dutch word tokenizer? The English one (the default 'punkt') seems to work but I am guessing that it could go wrong at some point.</p>
",Multilingual Language Processing & Language Identification,tokenizing dutch word reading article found use dutch sentence tokenizer follows however way using dutch word tokenizer english one default punkt seems work guessing could go wrong point
Why polyglot does not return the un-transliterated text back?,"<pre><code>eg = 'bajra bechna ‡§¨‡§æ‡§ú‡§∞‡§æ ‡§¨‡•á‡§ö‡§®‡§æ'
fin=''
text = Text(eg)

for x in text.transliterate(&quot;en&quot;):
    fin = fin + ' ' + str(x)


print(fin)  

output:
‡§¨‡§æ‡§ú‡§∞‡§æ ‡§¨‡•á‡§ö‡§®‡§æ
</code></pre>
<p>I am loosing the initial English text in the output. How can I get all of it in the output?</p>
",Multilingual Language Processing & Language Identification,polyglot doe return un transliterated text back loosing initial english text output get output
Can anyone suggest the best transliteration library to Marathi from English or Marathi written in English?,"<p>I tried using indic-transliteration library but it doesnt work with marathi and the only other possible option I found was google translate API which has a lot of time lag.</p>
<p>I tried using this but my output is different with a &quot;B&quot;</p>
<pre><code>import codecs,string
from indic_transliteration import sanscript
from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate

def is_hindi(character):
    maxchar = max(character)
    if u'\u0900' &lt;= maxchar &lt;= u'\u097f':
        return character
    else:
        print(transliterate(character, sanscript.ITRANS, sanscript.DEVANAGARI)) 

character = 'Bakrya vikne ahe'
is_hindi(character)

Output:

B‡§Ö‡§ï‡•ç‡§∞‡•ç‡§Ø ‡§µ‡§ø‡§ï‡•ç‡§®‡•á ‡§Ö‡§π‡•á
</code></pre>
",Multilingual Language Processing & Language Identification,anyone suggest best transliteration library marathi english marathi written english tried using indic transliteration library doesnt work marathi possible option found wa google translate api ha lot time lag tried using output different b
NLP detect English conditional statements,"<p>I'm currently working on a project in which I need to manipulate English conditional statements. I downloaded the NLTK library and it seems a very good NLP module. The question is: Is there a built-in mechanism in NLTK (or other libraries) to identify conditional statements? or maybe a paper explaining how to detect conditional statements?</p>

<p>I've read that there are different types of conditional statements, most of them rely on the word IF. But also there are other situations in which a condition can be present without IF (like using unless for example).</p>

<p>Can anyone guide me?</p>
",Multilingual Language Processing & Language Identification,nlp detect english conditional statement currently working project need manipulate english conditional statement downloaded nltk library seems good nlp module question built mechanism nltk library identify conditional statement maybe paper explaining detect conditional statement read different type conditional statement rely word also situation condition present without like using unless example anyone guide
Is there a limit to the size of target word vocabulary that should be used in seq2seq models?,"<p>In a machine translation seq2seq model (using RNN/GRU/LSTM) we provide sentence in a source language and train the model to map it to a sequence of words in another language (e.g., <em>English to German</em>).</p>
<p>The idea is, that the decoder part generates a classification vector (which has the size of target word vocabulary) and a softmax is applied on this vector followed by an argmax to get the index of the most probable word.</p>
<p><strong>My question is</strong>: is there an upper limit to how large the target word vocabulary should be, considering:</p>
<ol>
<li>The performance remains reasonable (<em>softmax will take more time for larger vectors</em>)</li>
<li>The accuracy/correctness of prediction is acceptable</li>
</ol>
",Multilingual Language Processing & Language Identification,limit size target word vocabulary used seq seq model machine translation seq seq model using rnn gru lstm provide sentence source language train model map sequence word another language e g english german idea decoder part generates classification vector ha size target word vocabulary softmax applied vector followed argmax get index probable word question upper limit large target word vocabulary considering performance remains reasonable softmax take time larger vector accuracy correctness prediction acceptable
Trouble with spacy in Jupyter Notebooks,"<p>I have been trying to get spacy running in Jupyter Notebooks.  I have the book &quot;Natural Language Processing in Python and Spacy.&quot;  I reinstalled the en module (got a message saying, &quot;linking successful&quot;) and tried this code from the book for a test</p>
<pre><code>import spacy
nlp = spacy.load('en')
doc = nlp(u'I am flying to Frisco')
print([w.text for w in doc])
</code></pre>
<p>but got this error:</p>
<pre><code>OSError                                   Traceback (most recent call last)
&lt;ipython-input-1-16fb1067de06&gt; in &lt;module&gt;
  1 import spacy
----&gt; 2 nlp = spacy.load('en')
  3 doc = nlp(u'I am flying to Frisco')
  4 print([w.text for w in doc])

c:...\python\python38-32\lib\site-packages\spacy\__init__.py in 
load(name, **overrides)
 19     if depr_path not in (True, False, None):
 20         deprecation_warning(Warnings.W001.format(path=depr_path))
---&gt; 21     return util.load_model(name, **overrides)
 22 
 23 

c:...\python\python38-32\lib\site-packages\spacy\util.py in 
load_model(name, **overrides)
117     elif hasattr(name, 'exists'):  # Path or Path-like to model data
118         return load_model_from_path(name, **overrides)
--&gt; 119     raise IOError(Errors.E050.format(name=name))
120 
121 

OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a 
valid path to a data directory.
</code></pre>
<p>Frankly any help would be so much appreciated</p>
",Multilingual Language Processing & Language Identification,trouble spacy jupyter notebook trying get spacy running jupyter notebook book natural language processing python spacy reinstalled en module got message saying linking successful tried code book test got error frankly help would much appreciated
Translating a json dataset to different natural language,"<p>I have a very big JSON dataset of conversations in the English language, and I would like to ask if there is any tool or way that can translate them from English to the Arabic language?</p>
",Multilingual Language Processing & Language Identification,translating json dataset different natural language big json dataset conversation english language would like ask tool way translate english arabic language
How remove special characters in csv-file and keeping umlaut with python,"<p>Hej, I¬¥m new in the python-world and try to clean data of a twitter-corpus for an linguistic-assignment. The tweets are stored in a csv-file and I want to remove the special characters. The german Umlaut is coded in special characters in the csv-file. So I lose all the umlauts when I remove the special characters.
How can I remove the special charakters and at the same time keep the umlaut?
It would be great if somebody could help me.</p>
<p>That is, what I have so far:</p>
<pre><code>import pandas as pd
data = pd.read_csv(&quot;test_newtest90.csv&quot;) 
data = pd.read_csv(&quot;train_newtest90.csv&quot;) 
import re
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import string
import nltk
import warnings 
warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning)

%matplotlib inline

train  = pd.read_csv(&quot;train_newtest90.csv&quot;)
test = pd.read_csv(&quot;test_newtest90.csv&quot;)

combi = train.append(test, ignore_index=True)

def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
        
    return input_txt    

#Removing Twitter Handles (@user)
combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], &quot;@[\w]*&quot;)

#Removing Short Words: bis len 3
combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)&gt;3]))

#Removing Punctuations, Numbers, and Special Characters
combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(&quot;[^a-zA-Z#]&quot;, &quot; &quot;)
</code></pre>
",Multilingual Language Processing & Language Identification,remove special character csv file keeping umlaut python hej new python world try clean data twitter corpus linguistic assignment tweet stored csv file want remove special character german umlaut coded special character csv file lose umlaut remove special character remove special charakters time keep umlaut would great somebody could help far
Natural categorization of common nouns,"<p>In the software I'm developing, I have to <strong>divide arbitrary nouns</strong> of common things (belonging to a single natural language) <strong>into some predefined natural categories</strong>, obtaining a tree structure.</p>
<p>Example: &quot;<em>wine</em>&quot; must fall under &quot;<em>drink</em>&quot;, &quot;<em>drink</em>&quot; under &quot;<em>food</em>&quot;.</p>
<p><strong>Are there any archives</strong> that can be used for this type of categorization, or component/services that perform it automatically (even for a fee)? If for Javascript or Java or C++, all the better, but the important for me is the archive, not the software.
Or could you tell me if there is a precise label with which to call it, so that I can search for material in google?</p>
<p>Any suggestions, or references to useful focused reading, will be greatly appreciated.</p>
<h2>Details:</h2>
<p>In the non-digital world, the categorization I need is operated in dictionaries called -at least in Italy- &quot;<strong>Analog Dictionaries</strong>&quot; (but in the English language, it doesn't seem to be a fixed term to define these dictionaries), but I need a <em>digital</em> archive/tool, to use in my program.</p>
<p>My research has made me find tools related to Machine Learning or Natural Language Processing (that I don't know), but none seem to meet my need (despite they being able to perform much more complex categorizations). For example, Google's &quot;<strong>Cloud Natural Language</strong>&quot; API offers the &quot;Classifying Content&quot; tool: passing a text to the <code>classifyText()</code> method, a category is returned. However, the possible categories form an unstructured and very limited list, and there is also a constraint on the input that must be at least 20 words.
Some tools (related to &quot;tagging&quot;) instead seem able to do what I need, but first providing them with a database, which is the main thing I miss.</p>
<p>Since in my case the set of input words is probably limited to commonly used terms, I also thought of writing such an archive myself, perhaps using the various monothematic lists I found on the web; however it would not be a short job and probably already done by others.</p>
<p>Furthermore, in the future I may need to support in input not only nouns but noun phrases (I refer to the Generative Grammar terminlogy: article + adjective + noun), and therefore having to remove the article and any adjectives before categorizing the substantive: it would therefore be useful if the syntactic category of each word was also available in the archive (even if, for my needs, the syntactic order would often be enough for the parsing: for example &quot;a good man&quot; -&gt; &quot;man&quot;).</p>
",Multilingual Language Processing & Language Identification,natural categorization common noun software developing divide arbitrary noun common thing belonging single natural language predefined natural category obtaining tree structure example wine must fall drink drink food archive used type categorization component service perform automatically even fee javascript java c better important archive software could tell precise label call search material google suggestion reference useful focused reading greatly appreciated detail non digital world categorization need operated dictionary called least italy analog dictionary english language seem fixed term define dictionary need digital archive tool use program research ha made find tool related machine learning natural language processing know none seem meet need despite able perform much complex categorization example google cloud natural language api offer classifying content tool passing text method category returned however possible category form unstructured limited list also constraint input must least word tool related tagging instead seem able need first providing database main thing miss since case set input word probably limited commonly used term also thought writing archive perhaps using various monothematic list found web however would short job probably already done others furthermore future may need support input noun noun phrase refer generative grammar terminlogy article adjective noun therefore remove article adjective categorizing substantive would therefore useful syntactic category word wa also available archive even need syntactic order would often enough parsing example good man man
Web scraping python for Arabic text,"<p>I am trying to web Scrape the website: &quot;http://norumors.net/?post_type=rumors?post_type=rumors&quot; to get only the heading news and put them in a CSV file using Beautifulsoup and python, This is the code I am using after i look into the HTML source code &quot;view-source:<a href=""http://norumors.net/?post_type=rumors?post_type=rumors%22"" rel=""nofollow noreferrer"">http://norumors.net/?post_type=rumors?post_type=rumors&quot;</a></p>
<pre><code>import urllib.request,sys,time
from bs4 import BeautifulSoup
import requests
import pandas as pd

pagesToGet= 1

upperframe=[]  
for page in range(1,pagesToGet+1):
    print('processing page :', page)
    url = 'http://norumors.net/?post_type=rumors/?page='+str(page)
    print(url)
    
    #an exception might be thrown, so the code should be in a try-except block
    try:
        #use the browser to get the url. This is suspicious command that might blow up.
        page=requests.get(url)                             # this might throw an exception if something goes wrong.
    
    except Exception as e:                                   # this describes what to do if an exception is thrown
        error_type, error_obj, error_info = sys.exc_info()      # get the exception information
        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem
        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception
        continue                                              #ignore this page. Abandon this and go back.
    time.sleep(2)   
    soup=BeautifulSoup(page.text,'html.parser')
    frame=[]
    links=soup.find_all('li',attrs={'class':'o-listicle__item'})
    print(len(links))
    filename=&quot;NEWS.csv&quot;
    f=open(filename,&quot;w&quot;, encoding = 'utf-8')
    headers=&quot;Statement,Link\n&quot;
    f.write(headers)
    
    for j in links:
        Statement = j.find(&quot;div&quot;,attrs={'class':'row d-flex'}).text.strip()
       # Link = &quot;http://norumors.net/&quot;
        Link += j.find(&quot;div&quot;,attrs={'class':'col-lg-4 col-md-4 col-sm-6 col-xs-6'}).find('a')['href'].strip()
        frame.append((Statement,Link))
        f.write(Statement.replace(&quot;,&quot;,&quot;^&quot;)+&quot;,&quot;+Link+&quot;,&quot;+Date.replace(&quot;,&quot;,&quot;^&quot;)+&quot;,&quot;+Source.replace(&quot;,&quot;,&quot;^&quot;)+&quot;,&quot;+Label.replace(&quot;,&quot;,&quot;^&quot;)+&quot;\n&quot;)
    upperframe.extend(frame)
f.close()
data=pd.DataFrame(upperframe, columns=['Statement','Link'])
data.head()
</code></pre>
<p>but After I run the code I am getting the pandas data frame and CSV file empty, any suggestion why is that? knowing that i want to get the text between  tags.</p>
",Multilingual Language Processing & Language Identification,web scraping python arabic text trying web scrape website get heading news put csv file using beautifulsoup python code using look html source code view source run code getting panda data frame csv file empty suggestion knowing want get text tag
Python library to scrape formatted text from arbitrary webpage,"<p>I'm doing a project in Python which involves:</p>

<ol>
<li>Extracting source code from webpage in url given as input (using Selenium with Chrome WebDriver)</li>
<li>Parse the source code and extract the visible text (using BeautifulSoup)</li>
<li>Do Natural Language Processing (NLP) on the extracted text</li>
</ol>

<p>Although, for the NLP to be successful, the text needs to be extracted as it is visually presented in the browser - which doesn't always coincide with the formatting on the source code, i.e. sometimes the text from a paragraph is broken down into several different elements -> which breaks the scraping all together and makes it non-viable for NLP.</p>

<p>I know it's a really challenging task to develop such ""visual based"" scraper, as opposed to a ""element based"" one, but is there something already developed on that direction?</p>
",Multilingual Language Processing & Language Identification,python library scrape formatted text arbitrary webpage project python involves extracting source code webpage url given input using selenium chrome webdriver parse source code extract visible text using beautifulsoup natural language processing nlp extracted text although nlp successful text need extracted visually presented browser always coincide formatting source code e sometimes text paragraph broken several different element break scraping together make non viable nlp know really challenging task develop visual based scraper opposed element based one something already developed direction
Learning embeddingment function for word embeddings,"<p>Given a sentence:</p>
<p>&quot;I went to the mall&quot; I translate this to french, &quot;Je suis all√© au centre commercial&quot; and translate this sentence back to English again from french giving me the following: &quot;I went to the shopping center&quot;</p>
<p>So S_1 = &quot;I went to the mall&quot;, S_2 = &quot;I went to the shopping center&quot;. My task is to minimize the distance between these two sentences, yet I am unsure which method would prove to be the best. The overall task is to cluster n sentences into semantically equal clusters. Is there an NLP model architecture which takes a sentence as input and a sentence as target and minimizes the distance between these using the weights? Maybe representation learning? Any hints are greatly appreciated.</p>
",Multilingual Language Processing & Language Identification,learning embeddingment function word embeddings given sentence went mall translate french je suis au centre commercial translate sentence back english french giving following went shopping center went mall went shopping center task minimize distance two sentence yet unsure method would prove best overall task cluster n sentence semantically equal cluster nlp model architecture take sentence input sentence target minimizes distance using weight maybe representation learning hint greatly appreciated
stop words cleaning with list comprehension,"<p>My data is a nested Chinese characters list.</p>
<pre><code>text1:

[['Ê≤í‰∫∫',
  'ÈóúÂøÉ',
  'Â±èÊù±',
  'ÊòØÂê¶',
  'Ê∑πÊ∞¥',
  '‰∫Ü',
  'Â§©Èæç',
  'Êñ∞ËÅûÂè∞',
  'Âè™',
  'ÈóúÂøÉ',
  'ÈÇÑÊ≤í‰æÜ',
  'ÁöÑ',
  'È¢±',
  'È¢®'],
 ['‰∏çÂ†±', 'Ê≤í‰∫∫', 'Áü•ÈÅì', '‰∏ÄÂ†±', '‰∏ÄÂ†Ü', '‰∫∫Âéª', 'ÁúãÁÑ∂', 'Âæå', 'Â∞±']]
</code></pre>
<p>I want to delete empty strings and stopwords with this list comprehension:</p>
<pre><code>stopwords('zh')

{'Âç≥Êàñ', 'ÈÇ£‰∫õ', 'Âì™', 'Â¶ÇÊ≠§', 'Âà´Â§Ñ', 'Â•π', 'ËøôÂ∞±ÊòØËØ¥', 'Ëá™Êâì', 'Âè™', 'Ëµ∂', 'ÂÖ∂‰∫å', 'ÂØπÊØî', 'ÂÆÉ', 'Ôºõ', '‰πå‰πé', 'ÂÖ∂', 'ÂÆÅ', '‰∏çÊòØ', '‰∏™', 'Êù•Ëá™', 'Âï•', '‰πà', 'Â∞±Ë¶Å', 'Á∫µÁÑ∂', '‰ø∫', '‰∫å', 'Â∞ΩÁÆ°Â¶ÇÊ≠§', 'ËÆ©', 'Âêó', '‰∏çÊàê', 'Ôºà', '‰æùÁÖß', 'ÁöÑËØù', 'Ëøá', '‰Ωú‰∏∫', '‰∫õ', '‰∏É', 'Ë¶ÅÊòØ', 'ÂêÑËá™', 'Ëøô‰πà‰∫õ', '‰ª¨', 'ÊÄªÁöÑÊù•Áúã', 'Áäπ‰∏î', 'Êàñ', 'Âá†', 'Êú¨ÁùÄ', 'Âõ†Ê≠§', 'ÔºÜ', 'ÊïÖ', 'Â¶ÇÊòØ', 'Ôºú', 'ÂÄòÊàñ', 'ÔΩû', '‰ª•ÂÖç', 'È°∫ÁùÄ', 'Áü£Âìâ', '‰ªªÂá≠', 'Êüê‰∏™', 'ÊàñËÄÖ', '‰ª•‰∏∫', 'Âìü', 'ÊÅ∞ÊÅ∞Áõ∏Âèç', '‰ªä', 'Ôºê', 'ÂæÖ', 'ÊàñÊõ∞', 'Ëá≥', 'Ëã•', 'Âõ∫ÁÑ∂', 'Âà´ËØ¥', 'Ë¶Å‰∏ç', 'Èô§Èùû', 'ÂÜµ‰∏î', 'Âòò', 'Âó°', '‰ªã‰∫é', 'Áîö‰∏î', 'Â¶ÇÊûú', 'Êúâ', 'ÈöèÁùÄ', 'ÂÖ∂Ê¨°', 'Â∞îÂ∞î', 'ÈÇ£‰∏™', '‰ªñ‰ª¨', 'Êõæ', 'Âè™ÊÄï', '‰∏™Âà´', 'Êõ¥', 'ÂèØ', 'Â¶Ç‰∏ã', 'Âèä', '‰∏¥', 'ÊïÖËÄå', '‰∏ÄÊñπÈù¢', 'ÈíàÂØπ', 'Â∞ΩÁÆ°', 'ÂñîÂî∑', 'Âìá', 'Âä†‰πã', 'Ê≠§Â§Ñ', 'ËøôÈáå', '‰ª•Ëá≥', 'Âõ†‰∫Ü', 'Âì™‰∫õ', 'ÈÇ£Êó∂', '‰æù', 'Ë∑ü', 'Âá≠', 'Âó≥', 'Âê´', '„Äã', 'ÈÄöËøá', 'ËøòÊúâ', 'ÂÄò‰Ωø', 'Êà™Ëá≥', 'Áõ¥Âà∞', 'ÈÇ£‰πà‰∫õ', 'Èô§Â§ñ', 'Ôº†', '‰∏ÄÁßç', '‰πÉËá≥‰∫é', 'Á∫µ‰ª§', 'Â∞±ÊòØËØ¥', 'Âëú', 'Âê¶Âàô', 'Âï¶', 'ÂæÄ', 'Áî±Ê≠§ÂèØËßÅ', 'ÁªßÂêé', 'Âó°Âó°', '‰ª•Êúü', 'ÂêÑ‰Ωç', 'ÂêëÁùÄ', 'Âà´ÊòØ', 'Á≠âÁ≠â', 'ÁÑ∂Âàô', 'ÂèçËøáÊù•ËØ¥', 'ÂëÄ', 'Êüê‰∫õ', '„Äâ', 'ÁªèËøá', 'ÂÖ±', 'Âëï', '‰Ωï‰ª•', 'Èùû‰ΩÜ', 'ËØ•', 'ÂàÜÂà´', 'Âè´', 'Âì™Âπ¥', 'Èô§ÂºÄ', 'Ëøô‰∏™', 'ËøôÂÑø', 'Â§ÑÂú®', 'Á´üËÄå', 'Ë∂Å', 'Áõ∏ÂØπËÄåË®Ä', 'ÊØîÂèä', 'Âêé', 'ÊçÆÊ≠§', 'ËÄå', 'ÈÇ£‰πàÊ†∑', '‰∏çÈóÆ', '‰∏éÂÖ∂ËØ¥', 'ÂÅá‰Ωø', 'Âëµ', 'ÊØîÂ¶Ç', 'Êâì', 'Ëá™‰ªé', 'ËÆæÊàñ', 'ÂΩìÂú∞', 'Áî±‰∫é', 'ÂÖ´', 'ÂÜçÂÖ∂Ê¨°', '‰∏ç', 'ÂèäÂÖ∂', '‰∏ÄÂàô', 'ËÆæ‰Ωø', 'ÊòØ‰ª•', 'Â§ö‰πà', 'ËØ∏', 'ÂΩìÁÑ∂', 'ËøôÊ†∑', 'Âì™ÂÑø', 'Ôºö', '‰∫éÊòØ‰πé', 'ÊÄª‰πã', 'Âá°ÊòØ', '‰∫∫‰ª¨', 'ÈÇ£ÂÑø', 'Âí¶', 'Âêë', '‰∏çËá≥‰∫é', 'Â§ß', 'ÂÜçÊúâ', 'Êõø', 'ÂÖ∂‰Ωô', 'Âñè', 'Èô§‰∫Ü', '‰∏é', 'Âç¥', 'Êàë', 'ÊúâÁöÑ', 'ÂΩì', 'Èâ¥‰∫é', 'ÂÖ®ÈÉ®', '‰∏Ä‰∏™', 'Áü£‰πé', '‰ªª‰Ωï', 'Âòª', 'ÁÖß', 'ËÄåÂ§ñ', 'ÈÇ£‰πà', '‰∏Ä‰Ωï', '‰∏ç‰ªÖ', 'ËØ¥', 'Ëá™Âêé', '‰∏çÂæó', 'Ôºì', 'Êúõ', 'ÁùÄ', 'ÂºÄÂ§ñ', 'Âæó‰∫Ü', 'Êó¢ÊòØ', 'Á¶ª', '‰ΩôÂ§ñ', 'Ëá™‰∏™ÂÑø', 'ÊàñÊòØ', 'Ëá™', 'Âì¶', '‰∫ë‰∫ë', '‰∏ÄÊ†∑', '‰ª•Ëá¥', 'ÊïÖÊ≠§', 'ÔΩú', '‰∏çÂè™', 'ËÄåÊòØ', 'ÂâçÂêé', 'Ôºª', '‰∏çËøá', '„ÄÇ', '‰∫Ü', 'ÈÇ£‰ºöÂÑø', 'Êù•', 'ÁîöËá≥', 'ËÄÖ', 'ËØ∏‰Ωç', 'ÂÄüÂÇ•ÁÑ∂', 'ÈùûÁâπ', 'ËøòË¶Å', 'Ê≤øÁùÄ', 'ÈùûÁã¨', 'Áß∞', 'Áªè', 'ÂòøÂòø', 'Ëµñ‰ª•', 'ÁÆ°', '‰πü', 'ËøûÂêå', 'ÁÑ∂Âêé', 'Âíß', '‰∏ÄÂàá', 'Ëá™ÂÆ∂', 'ÂêåÊó∂', 'ÔΩù', 'ÂÜÖ', 'Âè™Êúâ', 'ÊûÅ‰∫Ü', 'Ëµ∑ËßÅ', '‰∏çÂ∞Ω', 'Ëé´‰∏çÁÑ∂', 'Ëá≥Ëã•', '‰ª•ÊïÖ', 'Ê¨§', 'ËÉΩ', 'Âì©', 'ÊóÅ‰∫∫', '‰∏éÂê¶', 'ËôΩÂàô', 'È¶ñÂÖà', 'Êó†ÂÆÅ', 'Âè¶Â§ñ', 'Âõ†ËÄå', 'Âõ†ÁùÄ', '‰∏ÄÊù•', '‰∏Ä‰∫õ', 'Ëã•Â§´', 'Ôºã', 'ÂÖ≠', 'ËØ¥Êù•', 'ÊÄªÁöÑËØ¥Êù•', '‰∏éÂÖ∂', 'ËÄåÂÜµ', '‰∏î‰∏çËØ¥', 'ÂèçËøáÊù•', 'Â∞îÂêé', 'ÈöèÊó∂', 'ËØöÂ¶Ç', 'ÂÜçËØ¥', '‰∏çÂÖâ', 'ÁªìÊûú', 'ËøôÊó∂', 'Âà´‰∫∫', '‰πãÁ±ª', '‰ªéÊ≠§', '‰∫∫ÂÆ∂', 'ÈöæÈÅìËØ¥', '‰∏§ËÄÖ', 'Â∫∂Âá†', 'ÂëºÂìß', 'ÂëúÂëº', 'ÂèÆÂíö', 'ËÄåÂ∑≤', 'ÂÄò', 'Âè™ÊòØ', '‰∏çËã•', 'ÂÅö', 'Âó¨', 'ÂΩº', 'Ôºï', 'Âîâ', '‰∏çÂ§ñ‰πé', 'Ëøô', 'ÁúÅÂæó', 'Ôºí', 'Âπ∂', 'Ë∂äÊòØ', '‰∏î', 'Âïê', 'ÊÄé‰πàÂäû', 'È°∫', 'ÂèØÊòØ', 'ÊâÄÂú®', 'ÂêÑ‰∏™', 'Âì™Ê†∑', 'Áü£', 'Â§ö', 'ÂìéÂìü', 'Âìà', 'Ëøò', 'ÂèàÂèä', 'Èù†', '‰ªÄ‰πàÊ†∑', 'ÁÖßÁùÄ', 'Ëà¨ÁöÑ', 'Âè¶ÊÇâ', 'Â∑≤', 'Ë≠¨Â¶Ç', 'Êó†', '‰æãÂ¶Ç', '‰∏Ä', '‰∏ã', 'ÊúâÂèä', 'ÂÖ∂‰ªñ', 'ÊúâÊó∂', 'ÁÑâ', 'ÂÆÉ‰ª¨', '‰ººÁöÑ', 'Ôºå', 'ÂÜçËÄÖËØ¥', 'Ê≠§Âú∞', '‰πüÂ•Ω', 'ÂâçËÄÖ', '‰Ω†', 'Âõ†', 'ËæÉ', 'ÂÖÆ', 'Âòõ', 'ËôΩÁÑ∂', 'ÂëµÂëµ', 'ÂÜçËÄÖ', '‰ΩÜÂá°', '‰ªéËÄå', 'ÂìºÂî∑', 'Ëã•Èùû', '‰∏ÄËà¨', 'Áî®Êù•', 'Ëøô‰πàÊ†∑', 'ÊäëÊàñ', 'ÊâÄÊúâ', '‰Ωï', 'Âë¢', 'ÊâÄ', 'ËøòÊòØ', '‰∏çÁâπ', 'Ë∞ÅÊñô', 'Ê≠£ÊòØ', 'Á¨¨', 'Ôø•', 'Âà´', '‰∏ç‰ΩÜ', 'ÊÖ¢ËØ¥', 'Á¥ßÊé•ÁùÄ', 'Ë¶Å‰πà', 'Ë∞Å', 'ÊâÄ‰ª•', 'Êº´ËØ¥', '‰ª•‰æø', 'Âì™Ëæπ', 'Âì™Èáå', 'Áî±', 'ÂØπ‰∫é', 'Á±ªÂ¶Ç', 'ÂÅáËã•', 'ÂèØËßÅ', 'ÈÇ£Ê†∑', 'ÊÄé‰πà', '‰∏çÂçï', 'Âê±', '‰πù', 'Â§ßÂÆ∂', 'Èô§', 'Ëé´Ëã•', '„ÄÅ', 'ËÖæ', 'Êâì‰ªé', '‰πÉ', 'Âíå', 'ÂÖ∑‰ΩìÂú∞ËØ¥', 'Âí±‰ª¨', 'Êú¨Ë∫´', 'ÂóØ', 'Êé•ÁùÄ', 'ÁîöËÄå', 'ÊúâÂÖ≥', '‰ªñ‰ª¨‰ª¨', 'ËøõËÄå', 'ÂêéËÄÖ', 'Êú¨', '‰∏çÊÄï', 'Âí≥', 'Ëá™ÂêÑÂÑø', 'Ëµ∑', 'Êàë‰ª¨', 'Ëøô‰πàÁÇπÂÑø', 'Âè™Èôê', '‰∫é', 'Êâç', 'Ë∞Å‰∫∫', '‰ªç', '‰∏çÊØî', '‰∏∫‰Ωï', 'Âç≥‰æø', 'ËÆæËã•', '‰πãÊâÄ‰ª•', '‰∏îËØ¥', 'Â¶Ç‰∏ä', '‰∏çÁÆ°', 'ËæÉ‰πã', 'Âá≠ÂÄü', 'Âèç‰πã', 'Â∞è', 'ÊûúÁúü', '‰∫∫', 'ÂÜçÂàô', 'ÂÜ≤', 'ÂΩºÊ≠§', 'Â∞±ÁÆó', '‰∫éÊòØ', '‰∏ä', 'ÊÄªÁöÑÊù•ËØ¥', 'Êó¢', 'Â∞±ÊòØ‰∫Ü', 'Ôºë', 'Ëé´Â¶Ç', '‰∏∫‰ªÄ‰πà', '‰∫ëÂ∞î', 'ÊØèÂΩì', 'ÂÖº‰πã', 'Ôºî', 'ÂÖà‰∏çÂÖà', 'Ë¶Å‰∏çÊòØ', '‰ª•Âèä', '‰ΩïÂ§Ñ', 'Âæó', '‰∏∫ÁùÄ', 'ÊàñÂàô', '‰∏çÊÉü', 'ÊúÄ', '‰ª•Ëá≥‰∫é', 'ÈÇ£', 'Âç≥‰Ωø', 'ÂΩºÊó∂', 'Ëá≥‰∫é', 'Áú®Áúº', '‰ΩÜÊòØ', 'Âú®‰∫é', 'Ë¥ºÊ≠ª', 'ÈÇ£Ëæπ', 'ËÉΩÂê¶', 'Âíã', '‰∏çÊãò', 'Áî±Ê≠§', '‰πÉËá≥', 'Âòø', 'Áîö‰πà', 'Ê≤°Â•à‰Ωï', 'Á∫µ', 'Êää', '‰ªñ‰∫∫', 'Âìâ', 'Âêß', 'Ëá™Ë∫´', '‰∫î', 'ÂÖ®‰Ωì', 'Èùû', 'ÈÅµÁÖß', 'ÂñΩ', 'ÂÖâÊòØ', 'Âç≥‰ª§', 'Âêë‰Ωø', 'ÂÆÅËÇØ', 'ÁÇπ', 'Áªß‰πã', 'Â≤Ç‰ΩÜ', 'Á∫µ‰Ωø', '‰∏ä‰∏ã', 'ËøôËæπ', 'ÂÆÅÊÑø', 'Â•Ω', 'ÔºÅ', 'ÂÄòÁÑ∂', 'Ô∏ø', '‰∏çÁã¨', 'Â•π‰ª¨', 'ÂñÇ', 'ÂºÄÂßã', 'ÁîöÊàñ', 'Ëøô‰πà', 'Â∞î', 'ÁªßËÄå', 'Áúã', '‰ΩïÂÜµ', '‰∏≠', 'ÂÜí', 'ÊâÄÂπ∏', '„Äà', 'Âï™Ëææ', 'ÊûúÁÑ∂', '‰Ω†‰ª¨', 'Ôºñ', 'Áªô', 'Êú¨‰∫∫', 'Êúà', 'Ëøô‰∫õ', '‰πé', 'Ë∑ù', 'ÊÄé‰πàÊ†∑', 'Â∑¥Â∑¥', 'Â¶Ç', 'Êõø‰ª£', 'Áî®', 'ÈÄêÊ≠•', 'ÂèØ‰ª•', 'Â∞Ω', 'Â¶ÇÂêå', 'ÔºÑ', '‰Ωø', '‰πüÁΩ¢', '‰∏éÊ≠§ÂêåÊó∂', 'Âá†Êó∂', 'ÂßãËÄå', '‰∏çÊñô', 'Âè™Ê∂à', 'ÁîöËá≥‰∫é', 'Ê≠£Â∑ß', 'Âïä', '‰∏∫', 'Ê≠§Èó¥', '‰∏çÂ∞ΩÁÑ∂', 'Âéª', 'ÂèçËÄå', '‰∏çÂ¶Ç', '‰ªé', 'ÂÖ≥‰∫éÂÖ∑‰ΩìÂú∞ËØ¥', 'Êó•', 'ËÄå‰∏î', 'ÈÉΩ', 'ÂÑø', 'Âà∞', 'ÊÄéÊ†∑', '‰∏∫‰∫Ü', 'Âì™‰∏™', 'Áªº‰∏äÊâÄËø∞', 'ÁΩ¢‰∫Ü', 'ÂìàÂìà', 'Ë∞ÅÁü•', '‰ªÄ', 'ÊâçËÉΩ', 'Âä†‰ª•', 'ÂÄü', 'Âìº', '‰ªçÊóß', 'ËÄåÂêé', 'Âà´ÁöÑ', 'Á≠â', 'ÂØπ', 'Êåâ', 'ÂÖ∂‰∏Ä', 'ÔºÉ', 'ËÆ∫', 'ËØ∏Â¶Ç', 'Êó¢ÂæÄ', 'ÂêßÂìí', 'Âè¶‰∏ÄÊñπÈù¢', 'ÂÜç', 'Âíö', 'Êç¢Ë®Ä‰πã', 'Âπ¥', 'Â¶ÇÂÖ∂', 'ÈÇ£Èáå', 'Ëã•Êûú', 'Â§öÂ∞ë', 'Ôºâ', '‰ΩÜ', 'Ôºô', 'Â¶ÇËã•', 'Âπ∂‰∏î', 'ÂÖ∂ÂÆÉ', 'ÂêÑ', 'ÊòØÁöÑ', 'Ê≠£Â¶Ç', 'Ë¶Å‰∏çÁÑ∂', 'ËøôÊ¨°', 'ÊØîÊñπ', 'Ôºä', 'ÊØãÂÆÅ', '„Ää', '‰ΩøÂæó', 'ËÆ∏Â§ö', 'ÂèäËá≥', 'Êúù', 'ÈÑô‰∫∫', 'ÊçÆ', '‰ΩïÊó∂', 'Âá∫Êù•', 'ÂÖ∂‰∏≠', 'Ôºó', 'Âè™ÂΩì', '‰∏∫Ê≠§', 'Ë≠¨Âñª', 'Âõõ', 'Êù•ÁùÄ', 'Âí±', 'ÁÆÄË®Ä‰πã', 'Âà´ÁÆ°', 'ÂîØÊúâ', 'Âì™Â§©', 'ËÄåË®Ä', 'ÂêÑÁßç', '‰πã‰∏Ä', '‰æùÊçÆ', 'Èöè', '‰πò', 'Âú®', 'Â¶Ç‰∏äÊâÄËø∞', '‰πã', 'ÊåâÁÖß', 'ÊúùÁùÄ', '‰∏çËÆ∫', 'ÊØè', 'Âú∞', 'Ëøû', 'Ê≠§Êó∂', 'Â∑≤Áü£', 'Ëá¥', 'Âæà', 'Â∞Ü', '‰∏ÄËΩ¨Áúº', 'Âü∫‰∫é', 'ÂÖ≥‰∫é', 'Âêì', 'Â∑¥', 'Êç¢Âè•ËØùËØ¥', 'ÂìéÂëÄ', 'ÂÄòËã•', '‰∫¶', 'ÁöÑÁ°Æ', '‰∏ÄÊó¶', 'Ôºò', 'ÊÉüÂÖ∂', 'Âç≥Ëã•', 'ÂÖ∑‰ΩìËØ¥Êù•', 'Ë¢´', 'ÊüêÊüê', 'Ê†πÊçÆ', 'Âè¶', 'ÂèóÂà∞', 'Â∞±ÊòØ', '‰∏á‰∏Ä', 'Â∞ö‰∏î', 'ÂΩìÁùÄ', 'ÊÄéÂ•à', 'ÂÉè', 'Ë¶Å', '‰ª•‰∏ä', 'ËôΩËØ¥', 'ÁùÄÂë¢', 'Âì™ÊÄï', 'ÂÆÅÂèØ', '‰∏çÁÑ∂', 'Â≠∞Êñô', 'ÂòéÁôª', 'Èô§Ê≠§‰πãÂ§ñ', 'Â¶Ç‰Ωï', 'ÊòØ', 'Âàô', 'Ê≤ø', 'Êù•ËØ¥', 'Á≠âÂà∞', 'ÂØπÂæÖ', 'Ëøô‰∏ÄÊù•', '‰∏∫Ê≠¢', 'Âç≥', 'Êú¨Âú∞', 'Êãø', 'Ê≠§', 'ÁÑ∂ËÄå', 'ËØ∑', 'Âá°', 'ÔºΩ', 'Âìé', 'ÊÇ®', '‰ºö', 'Ëøô‰ºöÂÑø', 'ËøôËà¨', '‰∏â', 'Êüê', 'Ôºû', 'ÊÄªËÄåË®Ä‰πã', 'Áßí', 'ÈöèÂêé', '‰∏çÂ¶®', 'ËôΩ', 'Â∫∂‰πé', 'Êó∂ÂÄô', 'ÂàôÁîö', 'Âèà', 'Ëá™Â∑±', 'ÈÅµÂæ™', '‰ªÄ‰πà', 'Êó∂', 'Âë∏', 'Âè™Ë¶Å', 'ÊØî', '‰ª•Êù•', 'ÁäπËá™', 'Êó†ËÆ∫', 'Êó¢ÁÑ∂', 'Âå∫', 'Ëá≥‰ªä', 'Ôºü', 'ÂØπÊñπ', 'ÂëÉ', 'Âêå', 'Ê≠§Â§ñ', 'Âìó', 'Â≠∞Áü•', '‰ªñ', 'Áî±ÊòØ', 'Âá∫‰∫é', 'ËØöÁÑ∂', 'Ëã•ÊòØ', 'Âõ†‰∏∫', 'ÈÇ£Ëà¨', 'Â∑±', 'Â∞±', '‰ø∫‰ª¨', 'Âòé', 'ÔΩõ', 'Ë∂ÅÁùÄ', '‰æø‰∫é', 'ÈùûÂæí', 'ÂÖâ', 'Âú®‰∏ã', 'ÂÅáÂ¶Ç', 'Ëæπ', 'ÂàÜ', 'ÂΩí', 'Âç≥Â¶Ç', 'ÔºÖ', 'Èõ∂', '‰∫åÊù•', 'Èòø', 'Âï∑ÂΩì', 'ÂΩíÈΩê', 'Êúâ‰∫õ', 'Âëó', 'Ê≠£ÂÄº', 'Ê≠§Ê¨°', 'ÁöÑ', '‰ª•', 'ÊÄé', '‰ªª', 'ÂâçÊ≠§'}



text2 = [w for x in text1 for w in x if not w in stopwords('zh') and w != '']
</code></pre>
<p>but for the result, each of the characters is separated and the inner list seems to be gone.</p>
<pre><code>text2:

['Ê≤í',
 'Èóú',
 'ÂøÉ',
 'Â±è',
 'Êù±',
 'Âê¶',
 'Ê∑π',
 'Ê∞¥',
 'Â§©',
 'Èæç',
 'Êñ∞',
 'ËÅû',
 'Âè∞',
 'Èóú',
 'ÂøÉ',
 'ÈÇÑ',
 'Ê≤í',
 '‰æÜ',
 'È¢±',
 'È¢®',
 'Â§Æ',
 'Â±±',
 'ËÑà',
 'Ê∏õ',
 'Ëºï',
 'È¢®',
 'Âã¢',
 'ÈÅá',
 'Âàù',
 'ÂçÅ',
 'ÊΩÆ',
 'Âè∞',
 'ÁÅ£',
 'Ë•ø',
 'ÈÉ®',
 'Êµ∑',
 'ÂçÄ',
 'Ê®£',
 'Ê∑π',
 'Ê∞¥',
 'Áµ¶',
 'Â†±',
 'Ê≤í',
 'Áü•',
 'ÈÅì',
 'Â†±',
 'Â†Ü',
 'ÁÑ∂',
 'Âæå',
 'Á®Ä',
 'ÈÇÑ',
 'Ë®ò',
 'Ââç',
 'Ê∑π',
 'Ê∞¥',
 'Ê∑π',
 'Á°¨',
 'Á¢ü',
 'Êº≤',
 'ÂÖ©',
 'ÂÄç',
 'ÂÉπ',
 'Ê†º',
 'Áµê',
 'Êûú',
 'ËÆä',
 'Êó±',
 'ÁÅΩ',
 'Ê•ä',
 'Áí®',
 'Êæ§',
 'Âè∞',
 'Âçó',
 'Êñ∞',
 'Â∏Ç',
 'ÊØõ',
 'ÊØõ',
 'Èõ®',
 '‰πñ',
 '‰πñ',
 'Áè≠',
 'Êîπ',
 'Ëôü',
 'Âæå',
 'ÂÖ¨',
 'Ëªä',
 'Â∫ß',
 'Ê≤í',
 'ÊãÜ',
 'Êãú',
 'Ë®ó',
 'ÂÄë',
 'Âõû',
 'ÂÆ∂',
 'ËÆÄ',
 'Êõ∏',
 'È∫º',
 'Áàõ',
 'Ëªä',
 'ÈÄÅ',
 'Áµ¶',
 'Â§©',
 'Èæç',
 'Âúã',
 'Êñ∞',
 'Âåó',
 'Èõ®',
 'Â§©',
 'Ë£°',
 'Èù¢',
 'ÊúÉ',
 'Ê∑π',
 'Ê∞¥',
 'Â§™',
 'Èõ¢',
 'Ë≠ú',
 'Âè∞',
 'Â∏Ç',
 'ÂÄã',
 'ÁàΩ',
 'ÁàÜ',
 'ÂÄë',
 'ÂÖç',
 'Ë≤ª',
 'ÁàΩ',
 'Èå¢',
 'ÂÄë',
 'Âè∞',
 'Â∏Ç',
 '‰ªò',
 'Èå¢',
 'È¢±',
 'È¢®',
 '‰∏ç',
 'Â†±',
 'Ê≤í',
 '‰∫∫',
 'Áü•',
 'ÈÅì',
 '‰∏Ä',
 'Â†±',
 '‰∏Ä',
 'Â†Ü',
 '‰∫∫',
 'Âéª',
 'Áúã',
 'ÁÑ∂',
 'Âæå']
</code></pre>
<p>is there a way to clean stopwords and still both maintain it as a word (not characters) and as a nested list?</p>
",Multilingual Language Processing & Language Identification,stop word cleaning list comprehension data nested chinese character list want delete empty string stopwords list comprehension result character separated inner list seems gone way clean stopwords still maintain word character nested list
HttpError 400 relating to a json response size being too large when running the python script from the Google Cloud ML BERT tutorial,"<p>I am new to Google Cloud and am learning BERT by following this great tutorial: ‚ÄúIncorporating natural language processing using AI Platform and BERT‚Äù. I get stuck when running embed.py as described in this section:
<a href=""https://cloud.google.com/solutions/incorporating-natural-language-processing-using-ai-platform-and-bert?authuser=1#embedding_tokenized_text_using_bert"" rel=""nofollow noreferrer"">https://cloud.google.com/solutions/incorporating-natural-language-processing-using-ai-platform-and-bert?authuser=1#embedding_tokenized_text_using_bert</a> (5.)</p>
<p>This is the relevant snipit from the embed.py output:</p>
<p>googleapiclient.errors.HttpError: &lt;HttpError 400 when requesting <a href=""https://us-central1-ml.googleapis.com/v1/projects/this-project-id/models/BERT/versions/bert_en_cased_L12_H768_A12:predict?alt=json"" rel=""nofollow noreferrer"">https://us-central1-ml.googleapis.com/v1/projects/this-project-id/models/BERT/versions/bert_en_cased_L12_H768_A12:predict?alt=json</a> returned &quot;Response size too large. Received at least 3116404 bytes; max is 2000000.&quot;</p>
<p>And this is from the log:</p>
<p>jsonPayload:¬†{
@type:¬†&quot;type.googleapis.com/google.cloud.ml.v1.PredictionLogEntry&quot;
numInstances:¬†&quot;1&quot;
message:¬†&quot;Response size too large. Received at least 3441268 bytes; max is 2000000.&quot;
}
httpRequest:¬†{
requestMethod:¬†&quot;POST&quot;
requestUrl:¬†‚Äúus-central1-ml.googleapis.com/v1/projects/this-project-id/models/BERT/versions/bert_en_cased_L12_H768_A12:predict?alt=json&quot;
requestSize:¬†&quot;1299&quot;
status:¬†400
responseSize:¬†&quot;75&quot;
userAgent:¬†&quot;(gzip),gzip(gfe)&quot;
remoteIp:¬†&quot;34.22.66.111&quot;
}</p>
<p>Hope this is understandable. Any pointers to a fix or workaround would be greatly appreciated.</p>
",Multilingual Language Processing & Language Identification,httperror relating json response size large running python script google cloud ml bert tutorial new google cloud learning bert following great tutorial incorporating natural language processing using ai platform bert get stuck running embed py described section relevant snipit embed py output googleapiclient error httperror httperror requesting returned response size large received least byte max log jsonpayload type type googleapis com google cloud ml v predictionlogentry numinstances message response size large received least byte max requestmethod post requesturl u central ml googleapis com v project project id model bert version bert en cased l h predict alt json requestsize status responsesize useragent gzip gzip gfe remoteip hope understandable pointer fix workaround would greatly appreciated
Using regular expression to delete words in between specific words,"<p>My data contains something like these:</p>
<pre><code>‚Üí muching :ÈÖãÈï∑Âú®ÈÄôÂÄãË∑ùÈõ¢ÁöÑÊôÇÂÄôÔºåÂåóÈÉ®Â±±ÂçÄÈÉΩË∂ÖÈÅéÂ§ßË±™Èõ®Ê®ôÊ∫ñ‰∫Ü

Êé®aitt :Âè∞Êù±ÊÅÜÊò•ÈñìÁôªÈô∏‰∏ç‰ª£Ë°®ÂåóÂè∞‰∏çÊúÉÊúâÂº∑È¢®.

‚Üí teras: 7Ôºè7
</code></pre>
<p>I want my data to look like these:</p>
<pre><code>ÈÖãÈï∑Âú®ÈÄôÂÄãË∑ùÈõ¢ÁöÑÊôÇÂÄôÔºåÂåóÈÉ®Â±±ÂçÄÈÉΩË∂ÖÈÅéÂ§ßË±™Èõ®Ê®ôÊ∫ñ‰∫Ü

Âè∞Êù±ÊÅÜÊò•ÈñìÁôªÈô∏‰∏ç‰ª£Ë°®ÂåóÂè∞‰∏çÊúÉÊúâÂº∑È¢®.

 7Ôºè7
</code></pre>
<p>I already tried some regex <code>re.sub(r'^Êé®:$', '', x)</code> but I'm pretty sure this way is wrong.</p>
<p>does regex work with Chinese characters or ‚Üí symbols?</p>
",Multilingual Language Processing & Language Identification,using regular expression delete word specific word data contains something like want data look like already tried regex pretty sure way wrong doe regex work chinese character symbol
Aspect-level entity extraction and sentiment analysis,"<p>I am working on Natural Language Processing and having a hard time on analyzing sentiment values on certain entities. For example if i say: &quot;I love the fact that I am finally leaving Chicago&quot;, it says that the sentence is a positive sentence. But as you can see it is pretty negative for the entity &quot;Chicago&quot;. Anyone knows if there is any related work on this? Thanks in advance</p>
",Multilingual Language Processing & Language Identification,aspect level entity extraction sentiment analysis working natural language processing hard time analyzing sentiment value certain entity example say love fact finally leaving chicago say sentence positive sentence see pretty negative entity chicago anyone know related work thanks advance
How can I put every sentence of a text in a row of a table next to its translation in a Google Document?,"<p>I want to put a text to be translated in a Google Doc so that every sentence is in its own row of a table. In each row of the next column is the corresponding, machine-translated sentence. Finally, there is a third blank column where I will write my own translation for each sentence.</p>
<p><a href=""https://i.sstatic.net/f07Dj.png"" rel=""nofollow noreferrer"">Like so</a></p>
<p>What would some command line code look like for accessing the specific Google Doc, inserting a table, splitting the text on each sentence, and then writing each sentence to a row of the table?</p>
<p>I am aware of Google API, but I have struggled with authentication problems so far. If someone can sketch out a general outline of what my script should look like, hopefully I can fill in the details.</p>
<p>I am trying my best to meet Stack Overflow's post guidelines, so if the question is not a good one, I'm happy to reformulate it. My question is multi-part, so I prefer to ask the broader version first, before breaking it into smaller aspects.</p>
",Multilingual Language Processing & Language Identification,put every sentence text row table next translation google document want put text translated google doc every sentence row table row next column corresponding machine translated sentence finally third blank column write translation sentence like would command line code look like accessing specific google doc inserting table splitting text sentence writing sentence row table aware google api struggled authentication problem far someone sketch general outline script look like hopefully fill detail trying best meet stack overflow post guideline question good one happy reformulate question multi part prefer ask broader version first breaking smaller aspect
Get predictions from Keras/Tensorflow once model is trained,"<p>I am working on a project involving neural machine translation (translating English to French).</p>
<p>I have worked through some examples online, and have now finished the model. Once a model is trained using Keras, how do I then get a prediction of a translation without training the entire model again, because with the large dataset I am using, each epoch takes some time and of course, I can't train the model every time I want a translation.</p>
<p>So what is the correct way of then generating predictions on new inputs without training the whole model again?</p>
<p>Thanks</p>
<p><a href=""https://i.sstatic.net/RfEg2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RfEg2.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/V2Vkk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/V2Vkk.png"" alt=""enter image description here"" /></a></p>
",Multilingual Language Processing & Language Identification,get prediction kera tensorflow model trained working project involving neural machine translation translating english french worked example online finished model model trained using kera get prediction translation without training entire model large dataset using epoch take time course train model every time want translation correct way generating prediction new input without training whole model thanks
Performance metric for DNN chatbot?,"<p>I want to evaluate the performance of <a href=""https://github.com/Conchylicultor/DeepQA#pretrained-model"" rel=""nofollow noreferrer"">pretrained chatbot models</a>, which are based on the <a href=""https://arxiv.org/abs/1506.05869"" rel=""nofollow noreferrer"">Neural Conversational Model</a> by Vinyals and Le. I have researched error metrics such as BLEU or perplexity; however, I feel like these do not make sense here, as there is no ""ground truth"" to compare to. The chatbot is intended to do small talk, not translate text, do question answering or any of the like.</p>

<p>I found <a href=""https://stackoverflow.com/questions/46518041/what-evaluation-metric-should-i-use-to-compare-knowledge-based-approach-with-gen"">this question</a>, but sadly, it was not answered.</p>
",Multilingual Language Processing & Language Identification,performance metric dnn chatbot want evaluate performance pretrained chatbot model based neural conversational model vinyals le researched error metric bleu perplexity however feel like make sense ground truth compare chatbot intended small talk translate text question answering like found href question sadly wa answered
Determining Grammatical Validity of Text Input,"<p>I am looking for some way to determine if textual input takes the form of a valid sentence; I would like to provide a warning to the user if not. Examples of input I would like to warn the user about:</p>
<blockquote>
<p>&quot;dog hat can ah!&quot;</p>
</blockquote>
<blockquote>
<p>&quot;slkj ds dsak&quot;</p>
</blockquote>
<p>It seems like this is a difficult problem, since grammars are usually derived from textbanks, and the words in the provided sentence input might not appear in the grammar. It also seems like parsers maybe make assumptions that the textual input is comprised of valid English words to begin with. (just my brief takeaway from playing around with Stanford NLP's GUI tool). My questions are as follows:</p>
<ol>
<li>Is there some tool available to scan through text input and determine if it is made up of valid English words, or at least offer a probability on that? If not, I can write this, just wondering if it already exists. I figure this would be step 1 before determining grammatical correctness.</li>
<li>My understanding is that determining whether a sentence is grammatically correct is done simply by attempting to parse the sentence and see if it is possible. Is that accurate? Are there probabilistic parsers that offer a degree of confidence when ambiguity is encountered? (e.g., a proper noun not recognized)</li>
<li>I hesitate to ask this last question, since I saw it was <a href=""https://stackoverflow.com/q/6115677/290136"">asked on SO over a decade ago</a>, but any updates as to whether there is a basic, readily available grammar for NLTK? I know English isn't simple, but I am truly just looking to parse relatively simple, single sentence input.</li>
</ol>
<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,determining grammatical validity text input looking way determine textual input take form valid sentence would like provide warning user example input would like warn user dog hat ah slkj dsak seems like difficult problem since grammar usually derived textbanks word provided sentence input might appear grammar also seems like parser maybe make assumption textual input comprised valid english word begin brief takeaway playing around stanford nlp gui tool question follows tool available scan text input determine made valid english word least offer probability write wondering already exists figure would step determining grammatical correctness understanding determining whether sentence grammatically correct done simply attempting parse sentence see possible accurate probabilistic parser offer degree confidence ambiguity encountered e g proper noun recognized hesitate ask last question since saw wa href decade ago update whether basic readily available grammar nltk know english simple truly looking parse relatively simple single sentence input thanks
How an alphabet file is processed in NLP?,"<p>I trained a model using the commands found here...</p>
<p><a href=""https://github.com/bakwc/JamSpell#train"" rel=""nofollow noreferrer"">https://github.com/bakwc/JamSpell#train</a></p>
<p>There is no problem with the English text. But I need to train a similar model based on Hindi Corpus.
I have a file that can be replaced with <code>sherlockholmes.txt</code> but I am not sure what should I refer to instead of <code>alphabet_en.txt</code>.</p>
<p>Should I just collect all Unicode characters used in Hindi in a text file?</p>
",Multilingual Language Processing & Language Identification,alphabet file processed nlp trained model using command found problem english text need train similar model based hindi corpus file replaced sure refer instead collect unicode character used hindi text file
How to classify natural languages written in other forms of characters?,"<h1>Background</h1>
<p>I would like to classify all the three phrases as Chinese, <code>'zh'</code> using fastText.</p>
<pre><code>[&quot;Ni hao!&quot;, '‰Ω†Â•Ω!', 'ni hao!']
</code></pre>
<p>However the trained model looks not applicable for the semantic classification.</p>
<p>Is there any idea to do the same task with different ways?</p>
<h1>Output</h1>
<pre><code>[('zh', 0.9305274486541748)]
[('eo', 0.9765485525131226)]
[('hr', 0.6364055275917053)]
</code></pre>
<h1>Code</h1>
<p>sample.py</p>
<pre class=""lang-py prettyprint-override""><code>from fasttext import load_model
model = load_model(&quot;lid.176.bin&quot;)

speech_texts = [&quot;Ni hao!&quot;, '‰Ω†Â•Ω!', 'ni hao!']

def categolize_func(texts, model, k):
    for i in range(len(texts)):
        text = texts[0]
        label, prob = model.predict(text, k)
        return list(zip([l.replace(&quot;__label__&quot;, &quot;&quot;) for l in label], prob))

print(categolize_func(speech_texts, model, 1))
</code></pre>
",Multilingual Language Processing & Language Identification,classify natural language written form character background would like classify three phrase chinese using fasttext however trained model look applicable semantic classification idea task different way output code sample py
How to process similar notations with Python?,"<p>I have list with keywords and their corresponding searchvolumes as CSV file. Keywords are in german language. Some keywords are unique, other keywords have slightly different notations, like in this example:</p>
<pre><code>+--------------------------------------+-----+
| verkehrsrechtsschutz r√ºckwirkend     | 50  |
+--------------------------------------+-----+
| verkehrs-rechtsschutz r√ºckwirkend    | 50  |
+--------------------------------------+-----+
| familien rechtsschutzversicherung    | 100 |
+--------------------------------------+-----+
| familienrechtsschutzversicherung     | 100 |
+--------------------------------------+-----+
| privat rechtsschutz ohne wartezeit   | 20  |
+--------------------------------------+-----+
| privater rechtsschutz ohne wartezeit | 20  |
+--------------------------------------+-----+
| rechtsschutzversicherung strafrecht  | 80  |
+--------------------------------------+-----+
| strafrechtsschutz                    | 80  |
+--------------------------------------+-----+
| rechtsschutzversicherung gewerbe     | 200 |
+--------------------------------------+-----+
| rechtsschutzversicherung gewerblich  | 200 |
+--------------------------------------+-----+
| fahrer rechtsschutz                  | 160 |
+--------------------------------------+-----+
| fahrerrechtsschutz                   | 160 |
+--------------------------------------+-----+
| fahrer-rechtsschutz                  | 160 |
+--------------------------------------+-----+
</code></pre>
<p>Similar noted keywords often have same suchvolumes - but not always.</p>
<p>I'm looking a way to move all keywords with similar notation into another file.</p>
<p>I guess, it could be done with Python, but don't know, what module, package or library has such special language processing capability to recognize similar notations and to decide about relation of keywords between each other.</p>
<p>Please point me into the right direction.</p>
<p><strong>Update</strong>: Solutions, which calculate similarity ratio will deliver very high amount of false positives and negatives - because of german language structure. I think rather about a tool, which &quot;knows&quot; german linguistics and works with a language and not with string differences. Maybe something like <a href=""https://pypi.org/project/textblob-de/"" rel=""nofollow noreferrer"">https://pypi.org/project/textblob-de/</a>, <a href=""https://spacy.io/models/de"" rel=""nofollow noreferrer"">https://spacy.io/models/de</a> or something from <a href=""https://github.com/adbar/German-NLP"" rel=""nofollow noreferrer"">https://github.com/adbar/German-NLP</a></p>
<p>I was already trying some tools, which calculate string differences - some VBA and Google App scripts, they fail miserably.</p>
",Multilingual Language Processing & Language Identification,process similar notation python list keywords corresponding searchvolumes csv file keywords german language keywords unique keywords slightly different notation like example similar noted keywords often suchvolumes always looking way move keywords similar notation another file guess could done python know module package library ha special language processing capability recognize similar notation decide relation keywords please point right direction update solution calculate similarity ratio deliver high amount false positive negative german language structure think rather tool know german linguistics work language string difference maybe something like something wa already trying tool calculate string difference vba google app script fail miserably
Natural Language Processing LSTM Neural Network Accuracy too Low,"<p>I have been learning Neural networks for a while now, predominantly with respect to natural language processing. I have been using Kaggle notebooks since I am a beginner. So, recently I was working on a Tamil News Classification dataset I found on Kaggle. The model uses LSTM RNN neural network to classify the news into appropriate news groups. The code in the notebook has an accuracy of around 90+. (Notebook for Reference: <a href=""https://www.kaggle.com/sagorsemantics/tamil-nlp-lstm"" rel=""nofollow noreferrer"">https://www.kaggle.com/sagorsemantics/tamil-nlp-lstm</a>) When I tried to create an LSTM Model, my accuracy was around 34%, despite using the same layers, activation function, optimizer, hyperparameters etc. Which I thought was strange. After asking around, I was advised to use hyperparameter tuning to achieve a higher accuracy. I did so. (My code here: <a href=""https://github.com/Vijeeguna/Tamil-News-Article-Classification/blob/main/tamil_news_classification_LSTM_RNN_CNN.py"" rel=""nofollow noreferrer"">https://github.com/Vijeeguna/Tamil-News-Article-Classification/blob/main/tamil_news_classification_LSTM_RNN_CNN.py</a>) But my accuracy continues to be low at 34%. I have played around with layers, dropout, etc. But the accuracy wont budge.</p>
<p>I am at a loss. I don't understand how/why this is. Any nudge in the right direction would be most welcome.</p>
<p>Code on Collab with accuracy I got: <a href=""https://colab.research.google.com/drive/1P7H6J98GGizrGpMXl8QtTAzWsdgIvGAw?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1P7H6J98GGizrGpMXl8QtTAzWsdgIvGAw?usp=sharing</a></p>
<p>[Also, I am a true novice. I  have been learning thro Kaggle notebooks almost exclusively. Please be patient and dumb things down for me.]</p>
",Multilingual Language Processing & Language Identification,natural language processing lstm neural network accuracy low learning neural network predominantly respect natural language processing using kaggle notebook since beginner recently wa working tamil news classification dataset found kaggle model us lstm rnn neural network classify news appropriate news group code notebook ha accuracy around notebook reference tried create lstm model accuracy wa around despite using layer activation function optimizer hyperparameters etc thought wa strange asking around wa advised use hyperparameter tuning achieve higher accuracy code accuracy continues low played around layer dropout etc accuracy wont budge loss understand nudge right direction would welcome code collab accuracy got also true novice learning thro kaggle notebook almost exclusively please patient dumb thing
Natural Language Processing technique for rephrasing question-answer pairs as full sentence?,"<p>Is there an NLP technique for rephrasing question-answer pairs as a full and grammatically correct sentence? For example:</p>
<p>Question: Where does Joe live?</p>
<p>Answer: Joe lives in Los Angeles.</p>
<p>I‚Äôve looked into Answer Ellipsis, which is essentially the opposite of this issue and what most Machine Reading Comprehension or Question Answering systems already generate. For example, the answer to the above with answer ellipsis would be ‚ÄúLos Angeles‚Äù.</p>
<p>For the sake of reader comprehension, I want to turn question-answer pairs into something understandable. What techniques exist to do this?</p>
",Multilingual Language Processing & Language Identification,natural language processing technique rephrasing question answer pair full sentence nlp technique rephrasing question answer pair full grammatically correct sentence example question doe joe live answer joe life los angeles looked answer ellipsis essentially opposite issue machine reading comprehension question answering system already generate example answer answer ellipsis would los angeles sake reader comprehension want turn question answer pair something understandable technique exist
How to programmatically download many large files from dropbox,"<p>The National Speech Corpus is a Natural Language Processing corpus of Singaporean's speaking English, which can be found here: <a href=""https://www.imda.gov.sg/programme-listing/digital-services-lab/national-speech-corpus"" rel=""nofollow noreferrer"">https://www.imda.gov.sg/programme-listing/digital-services-lab/national-speech-corpus</a>.</p>
<p>When you sign up for the free corpus, you are directed to a dropbox folder. The corpus is 1 TB and (as of this writing) has four parts. I only wanted to download PART 1 but even this has 1446 zip files that are each quite larger. My question is: how do I programmatically download many large files from dropbox onto a Linux (Ubunut 16.04) VM using only the command line.</p>
<p>The directory tree for the relevant part looks like:</p>
<pre><code>root
|-LEXICON
|-PART1
  |-DATA
    |-CHANNEL0
      |-WAVE
        |-SPEAKER0001.zip
        |-SPEAKER0002.zip
        ...
        |-SPEAKER1446.zip
</code></pre>
<p>I looked into a few different approaches:</p>
<ol>
<li><p>Downloading the <code>WAVE</code> parent directory using a shared link via the <code>wget</code> command as described in <a href=""https://superuser.com/questions/470664/how-to-download-dropbox-files-using-wget-command"">this question</a>. However, this didn't work as I received this error:</p>
<p>Reusing existing connection to <a href=""http://www.dropbox.com:443"" rel=""nofollow noreferrer"">www.dropbox.com:443</a>
HTTP request sent, awaiting response... 400 Bad Request
2021-01-06 23:09:06 ERROR 400: Bad Request.</p>
</li>
</ol>
<p>I assumed this was because the <code>WAVE</code> directory was too large for Dropbox to zip.</p>
<ol start=""2"">
<li><p>Based on <a href=""https://www.dropboxforum.com/t5/Dropbox-files-folders/Getting-downloading-link-of-files-in-Dropbox-automatically/td-p/263073"" rel=""nofollow noreferrer"">this post</a>, it was suggested that I could download the HTML of the <code>WAVE</code> parent directory and find all of the direct links to the individual zip files but the direct links to the individual files were not in the HTML file.</p>
</li>
<li><p>Based on the same post as in (2), I could also try to create shared links for each zip file using the dropbox API, though this seemed too cumbersome.</p>
</li>
<li><p>Download the Linux dropbox client and sync the relevant files as outlined in <a href=""https://www.dropbox.com/install-linux"" rel=""nofollow noreferrer"">this installation</a>.</p>
</li>
</ol>
<p>In the end, the 4th option did work for me, but I wanted to post this investigation for anyone who needs to download this dataset in the future. Also, I wanted to see if anyone else had better approaches.</p>
",Multilingual Language Processing & Language Identification,programmatically download many large file dropbox national speech corpus natural language processing corpus singaporean speaking english found sign free corpus directed dropbox folder corpus tb writing ha four part wanted download part even ha zip file quite larger question programmatically download many large file dropbox onto linux ubunut vm using command line directory tree relevant part look like looked different approach downloading parent directory using shared link via command described http request sent awaiting response bad request error bad request assumed wa directory wa large dropbox zip based post wa suggested could download html parent directory find direct link individual zip file direct link individual file html file based post could also try create shared link zip file using dropbox api though seemed cumbersome download linux dropbox client sync relevant file outlined installation end th option work wanted post investigation anyone need download dataset future also wanted see anyone else better approach
Abbreviations in natural language processing,"<p>I work on sentiment analysis . Abbreviations are one of the most widely used in natural languages. I used Spellcheker to correct spelling mistakes, and one of the problems with using this method is that it translates Abbreviations into the closest word to English. This affects the sentiment detection. Is there any code or a method that these Abbreviations can be extended according to their neighbor words?</p>
",Multilingual Language Processing & Language Identification,abbreviation natural language processing work sentiment analysis abbreviation one widely used natural language used spellcheker correct spelling mistake one problem using method translates abbreviation closest word english affect sentiment detection code method abbreviation extended according neighbor word
"What is the more natural parsing, the one that leads to the preferred reading of the sentence","<p>I have those rules:
<a href=""https://i.sstatic.net/jKfG2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jKfG2.png"" alt=""enter image description here"" /></a></p>
<p>and those two possible parse trees:</p>
<p><a href=""https://i.sstatic.net/gxHgF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gxHgF.png"" alt=""enter image description here"" /></a></p>
<p>I am asked for the next question:</p>
<p>What is the more natural parsing, the one that leads to the preferred reading of the sentence?</p>
<p>Can anyone explain to me, what is more natural in English and why ?</p>
<p>according to <a href=""https://sites.google.com/site/partofspeechhelp/#TOC-DT-"" rel=""nofollow noreferrer"">this</a></p>
<pre><code>A determiner is a noun-modifier that expresses the reference of a noun or noun phrase in the context.
</code></pre>
<p>I don't see any possible more natural distinguish.</p>
",Multilingual Language Processing & Language Identification,natural parsing one lead preferred reading sentence rule two possible parse tree asked next question natural parsing one lead preferred reading sentence anyone explain natural english according see possible natural distinguish
Algorithm to Detect and Compare Phrases,"<p>I have a couple of non-English texts. I would like to perform stylistic comparisons on them.</p>

<p>One method of comparing style is to look for similar phrases. If I find in one book ""fishing, skiing and hiking"" a couple of times and in another book ""fishing, hiking and skiing"" the similarity in style points to one author. I need to also be able to find ""fishing and even skiing or hiking"" though. Ideally I would also find ""angling, hiking and skiing"" but because they are non-English texts (Koine Greek), synonyms are harder to allow for and this aspect is not vital.</p>

<p>What is the best way to (1) go about detecting these sorts of phrases and then (2) searching for them in a way that is not overly rigid in other texts (so as to find ""fishing and even skiing or hiking"")?</p>
",Multilingual Language Processing & Language Identification,algorithm detect compare phrase couple non english text would like perform stylistic comparison one method comparing style look similar phrase find one book fishing skiing hiking couple time another book fishing hiking skiing similarity style point one author need also able find fishing even skiing hiking though ideally would also find angling hiking skiing non english text koine greek synonym harder allow aspect vital best way go detecting sort phrase searching way overly rigid text find fishing even skiing hiking
Which parameters within the sklearn.decomposition.LatentDirichletAllocation will improve the performance of the Topic Modelling?,"<p>I am currently trying to optimise a LDA model I am doing on some text data. Beyond changing the number of topics, what other parameters will significantly improve the model?</p>
<p>This is my current code which I copied from another <a href=""https://stackoverflow.com/questions/61373994/how-to-specify-random-state-in-lda-model-for-topic-modelling"">stackoverflow Q&amp;A</a>:</p>
<pre><code>LatentDirichletAllocation(n_components=10,        
                                  max_iter=10,               
                                  learning_method='online',   
                                  random_state=100,          
                                  batch_size=128,            
                                  evaluate_every = -1,       
                                  n_jobs = -1 )
</code></pre>
<p>I am having trouble understanding what these different parameters actually do. The <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"" rel=""nofollow noreferrer"">sklearn explanation from the official site lacks detail</a> and every <a href=""https://medium.com/@yanlinc/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6"" rel=""nofollow noreferrer"">medium article</a> skims over what these parameters do. Is there a source that breaks down these in plain english?:</p>
<ul>
<li>learning_method</li>
<li>evaluate _every</li>
<li>n_jobs = -1</li>
<li>learning_offset=10.0,</li>
<li>ax_doc_update_iter=100</li>
<li>max_iter=10,</li>
<li>n_jobs=None,</li>
<li>random_state=42,</li>
<li>topic_word_prior=None</li>
</ul>
<p>I'm aware I don't have a statistics background but I want to understand the code I'm implementing a bit more. Thanks</p>
",Multilingual Language Processing & Language Identification,parameter within sklearn decomposition latentdirichletallocation improve performance topic modelling currently trying optimise lda model text data beyond changing number topic parameter significantly improve model current code copied another sklearn explanation official site lack detail every medium article skim parameter source break plain english learning method evaluate every n job learning offset ax doc update iter max iter n job none random state topic word prior none aware statistic background want understand code implementing bit thanks
How to normalize Persian texts with Hazm,"<p>I have a folder containing some other <em>folders</em> and each contains a lot of text files. I have to extract <strong>5 words</strong> before and after a specific word and following code works fine.</p>

<p>The problem is that because I did not normalize the texts, it just returns a few sentences while there is more. 
In Persian there is a module called <strong>hazm</strong> for normalizing the texts. How can I use that in this code? </p>

<p>For example of normalizing: <strong>""ŸÉ""</strong>  should change to <strong>""⁄©""</strong> or <strong>""ÿ§""</strong> should change to ""Ÿà"". Because the first two ones are actually <em>Arabic</em> alphabets which were used in <em>Persian</em>. Without normalizing the code just returns the words that are written with the second form and it does not recognize the words which are in the first forms <em>Arabic</em>).</p>

<pre><code>import os
from hazm import Normalizer


def getRollingWindow(seq, w):
    win = [next(seq) for _ in range(11)]
    yield win
    for e in seq:
        win[:-1] = win[1:]
        win[-1] = e
        yield win


def extractSentences(rootDir, searchWord):
    with open(""Ÿæÿß⁄©ÿ™"", ""w"", encoding=""utf-8"") as outfile:
        for root, _dirs, fnames in os.walk(rootDir):
            for fname in fnames:
                print(""Looking in"", os.path.join(root, fname))
                with open(os.path.join(root, fname), encoding = ""utf-8"") as infile:
                    #normalizer = Normalizer()
                    #fname = normalizer.normalize(fname)
                    for window in getRollingWindow((word for line in infile for word in line(normalizer.normalize(line)).split()), 11):
                        if window[5] != searchWord: continue
                        outfile.write(' '.join(window)+ ""\n"")
</code></pre>
",Multilingual Language Processing & Language Identification,normalize persian text hazm folder containing folder contains lot text file extract word specific word following code work fine problem normalize text return sentence persian module called hazm normalizing text use code example normalizing change change first two one actually arabic alphabet used persian without normalizing code return word written second form doe recognize word first form arabic
Translating using pre-trained hugging face transformers not working,"<p>I have a situation where I am trying to using the pre-trained hugging-face models to translate a pandas column of text from Dutch to English. My input is simple:</p>
<pre><code>Dutch_text             
Hallo, het gaat goed
Hallo, ik ben niet in orde
Stackoverflow is nuttig
</code></pre>
<p>I am using the below code to translate the above column and I want to store my result into a new column ENG_Text. So the output will look like this:</p>
<pre><code>ENG_Text             
Hello, I am good
Hi, I'm not okay
Stackoverflow is helpful
</code></pre>
<p>The code that I am using is as follows:</p>
<pre><code>#https://huggingface.co/Helsinki-NLP for other pretrained models 
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
input_1 = df['Dutch_text']
input_ids = tokenizer(&quot;translate English to Dutch: &quot;+input_1, return_tensors=&quot;pt&quot;).input_ids # Batch size 1
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)
</code></pre>
<p>Any help would be appreciated!</p>
",Multilingual Language Processing & Language Identification,translating using pre trained hugging face transformer working situation trying using pre trained hugging face model translate panda column text dutch english input simple using code translate column want store result new column eng text output look like code using follows help would appreciated
Is there any solution if spacy can&#39;t be located on my system?,"<p>As the picture shows, spacy is well installed:</p>
<p><img src=""https://i.sstatic.net/YyXQQ.png"" alt=""verification"" /></p>
<p>But I can't still &quot;import&quot; it:</p>
<p><img src=""https://i.sstatic.net/gynfK.png"" alt=""error on Python"" /></p>
<p>By consulting the official site, it seems that spacy can't be located on my system(win32):</p>
<p><img src=""https://i.sstatic.net/tZ1aW.png"" alt=""explanation from the official site"" /></p>
<p>I want to know if there are some solution to it. In fact, I want to use it to treat french corpus, so if it's impossible to profit it, is there any other similar tool I can use to lemmatize french and so on?</p>
",Multilingual Language Processing & Language Identification,solution spacy located system picture show spacy well installed still import consulting official site seems spacy located system win want know solution fact want use treat french corpus impossible profit similar tool use lemmatize french
Analyzing Token Data from a Pandas Dataframe,"<p>I'm a relative python noob and also new to natural language processing (NLP).</p>
<p>I have dataframe containing names and sales. I want to: 1) break out all the tokens and 2) aggregate sales by each token.</p>
<p>Here's an example of the dataframe:</p>
<pre><code>name    sales
Mike Smith  5
Mike Jones  3
Mary Jane   4
</code></pre>
<p>Here's the desired output:</p>
<pre><code>token   sales
mike    8
mary    4
Smith   5
Jones   3
Jane    4
</code></pre>
<p>Thoughts on what to do? I'm using Python.</p>
",Multilingual Language Processing & Language Identification,analyzing token data panda dataframe relative python noob also new natural language processing nlp dataframe containing name sale want break token aggregate sale token example dataframe desired output thought using python
How to view the activity logs/schedule of my natural language processing program on gcp,"<p>As the title, I would like to view the schedule/logs of my natural language processing on GCP, so that I can see which step/instance that the program processes on.</p>
<p>In other words, is there any possibility that I can view every process, such as that which instance the program is working on and which instances are done. An example is shown below:</p>
<p><img src=""https://i.sstatic.net/u9Yab.jpg"" alt=""this image"" /></p>
",Multilingual Language Processing & Language Identification,view activity log schedule natural language processing program gcp title would like view schedule log natural language processing gcp see step instance program process word possibility view every process instance program working instance done example shown
NLP using NLTK take join between Tokenized list and csv file,"<pre><code>mast_dict = pd.read_csv('C:/Users/ZAM/Desktop/LoughranMcDonald_MasterDictionary_2018.csv')
stop_words = set(stopwords.words('english')) 
#print(stopwords.words('english'))

filtered_text = []
with open('C:/Users/ZAM/Downloads/HEURO/file0.txt') as fin:
    tokens = word_tokenize(fin.read()) 
for r in words:  
    if not r in stop_words:  
        filtered_text.append(r)
filtered_text = [''.join(c for c in s if c not in string.punctuation) for s in filtered_text]
filtered_text = [x for x in filtered_text if not (x.isdigit())]
print(filtered_text)
</code></pre>
<pre><code>Output
['BEGIN', 'PRIVACYENHANCED', 'MESSAGE', 'ProcType', '2001MICCLEAR', 'OriginatorName', 'webmasterwwwsecgov', 'OriginatorKeyAsymmetric', 'MFgwCgYEVQgBAQICAf8DSgAwRwJAW2sNKK9AVtBzYZmr6aGjlWyK3XmZv3dTINen', 'TWSM7vrzLADbmYQaionwg5sDW3P6oaM5D3tdezXMm7z1TBtwIDAQAB', 'MICInfo', 'RSAMD5RSA', 'RHiO0hkbOslk2iB2eQM8lSgcXyjFKRha0FtFBz3xsf7Z6kKaWhrhStjKFZUvWZf', 'eDg67b8ZljTBWxC8ybQfg', ...]
</code></pre>
<p>The <code>mast_dict</code> variable contains around 86K English words with positive scores, negative scores, polarity scores, etc.
I want to make a joint between the <code>filtered_text</code>list which contains tokenized words as shown in the <code>mast_dict</code> CSV file so I can get the scores of the tokenized words in the <code>filtered_text</code> list.</p>
<p>Let me know. Thanks!</p>
",Multilingual Language Processing & Language Identification,nlp using nltk take join tokenized list csv file variable contains around k english word positive score negative score polarity score etc want make joint list contains tokenized word shown csv file get score tokenized word list let know thanks
What do the logits and probabilities from RobertaForSequenceClassification represent?,"<p>Being new to the &quot;Natural Language Processing&quot; scene, I am experimentally learning and have implemented the following segment of code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
    
path = &quot;D:/LM/rb/&quot;
tokenizer = RobertaTokenizer.from_pretrained(path)
model = RobertaForSequenceClassification.from_pretrained(path)
    
inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
pred_logits = outputs.logits
print(pred_logits)
probs = pred_logits.softmax(dim=-1).detach().cpu().flatten().numpy().tolist()
print(probs)
</code></pre>
<p>I <em>understand</em> that applying the model returns a <em>&quot;<code>torch.FloatTensor</code> comprising various elements depending on the configuration (RobertaConfig) and inputs&quot;</em>, and that the logits are accessible using <code>.logits</code>. As demonstrated I have applied the <em>.softmax</em> function to the tensor to return normalised probabilities and have converted the result into a list. I am outputted with the following:</p>
<pre><code>[0.5022980570793152, 0.49770188331604004]
</code></pre>
<p>Do these probabilities represent some kind of overall &quot;masked&quot; probability?</p>
<p><strong>What do the first and second index represent in context of the input?</strong></p>
<hr />
<p>EDIT:</p>
<pre><code>model.num_labels
</code></pre>
<p>Output:</p>
<pre><code>2
</code></pre>
<p><a href=""https://stackoverflow.com/users/6664872/cronoik"">@cronoik</a> explains that the model &quot;tries to classify if a sequence belongs to one class or another&quot;</p>
<p>Am I to assume that because there are no trained output layers these classes don't mean anything yet?</p>
<p>For example, I <em>can</em> assume that the probability that the sentence, post analysis, belongs to class 1 is 0.5. However, what is class 1?</p>
<p>Additionally, model cards with pre-trained output layers such as the <a href=""https://huggingface.co/roberta-large-openai-detector"" rel=""nofollow noreferrer"">open-ai detector</a> help differentiate between what is <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;real&quot;</a> and <a href=""https://github.com/openai/gpt-2-output-dataset/blob/master/detector/server.py#L46"" rel=""nofollow noreferrer"">&quot;fake&quot;</a>, and so I can assume the class that a sentence belongs to. However, how can I confirm these &quot;labels&quot; without some type of &quot;mapping.txt&quot; file?</p>
",Multilingual Language Processing & Language Identification,logits probability robertaforsequenceclassification represent new natural language processing scene experimentally learning implemented following segment code understand applying model return comprising various element depending configuration robertaconfig input logits accessible using demonstrated applied softmax function tensor return normalised probability converted result list outputted following probability represent kind overall masked probability first second index represent context input edit output open ai detector help differentiate real fake assume class sentence belongs however confirm label without type mapping txt file
quanteda - stopwords not working in French,"<p>For some reason, stop words is not working for my corpus, entirely in French. I've been trying repeatedly over the past few days, but many words that should have been filtered simply are not. I am not sure if anyone else has a similar issue? I read somewhere that it could be because of the accents. I tried <code>stringi::stri_trans_general(x, &quot;Latin-ASCII&quot;)</code> but I am not certain I did this correctly. Also, I notice that French stop words are sometimes referred to as &quot;french&quot; or &quot;fr&quot;.</p>
<p>This is one example of code I tried, I would be extremely grateful for any advice.
I also manually installed quanteda, because I had difficulties downloading it, so it could be linked to that.</p>
<pre><code>text_corp &lt;- quanteda::corpus(data,
   text_field=&quot;text&quot;)

head(stopwords(&quot;french&quot;))

summary(text_corp)

my_dfm &lt;- dfm(text_corp)
myStemMat &lt;- dfm(text_corp, remove = stopwords(&quot;french&quot;), stem = TRUE, remove_punct = TRUE, remove_numbers = TRUE, remove_separators = TRUE)

myStemMat[, 1:5]

topfeatures(myStemMat 20)
</code></pre>
<p>In this last step, there are still words like &quot;etre&quot; (to be), &quot;plus&quot; (more), comme (&quot;like&quot;), avant (&quot;before&quot;), avoir (&quot;to have&quot;)</p>
<p>I also tried to filter stop words in a different way, through token creation:</p>
<pre><code>tokens &lt;-
tokens(
text_corp,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
split_hyphens = TRUE,
include_docvars = TRUE,
)

mydfm &lt;- dfm(tokens,
    tolower = TRUE,
   stem = TRUE,
   remove = stopwords(&quot;french&quot;)
   )

topfeatures(mydfm, 20)
</code></pre>
",Multilingual Language Processing & Language Identification,quanteda stopwords working french reason stop word working corpus entirely french trying repeatedly past day many word filtered simply sure anyone else ha similar issue read somewhere could accent tried certain correctly also notice french stop word sometimes referred french fr one example code tried would extremely grateful advice also manually installed quanteda difficulty downloading could linked last step still word like etre plus comme like avant avoir also tried filter stop word different way token creation
How to improve my multiclass text-classification on German text?,"<p>I am new in NLP and it is a bit confusing me.
I am trying to do a text classification with SVC on my dataset.
I have an imbalanced dataset of 6 classes.
The text is news for classes of health, sport, culture, economy, science and web.
I am using TF-IDF for vectorization.</p>
<p>the preprocessing steps: <code>lower-case</code> all the texts and to remove the <code>stop-words</code>. since my text is in German I did not use <code>lemmatization</code></p>
<p>my first try:</p>
<pre><code>from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.2, random_state=42)
X_train = train['text']
y_train = train['category']
X_test = test['text']
y_test = test['category']

# Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC()),])
predictions = text_clf_lsvc.predict(X_test)
</code></pre>
<p>my metrci acuuracy score was: 93%</p>
<p>then I decided to reduce the dimensionality: so on my 2nd try I added TruncatedSVD</p>
<pre><code># Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),('svd', TruncatedSVD()),
                     ('clf', LinearSVC()),])
predictions = text_clf_lsvc.predict(X_test)
</code></pre>
<p>my metrci acuuracy score dropped to 34%.</p>
<p><em><strong>my questions:</strong></em></p>
<p>1- How can I improve my model if I want to stick to TF-IDF and SVC for classification<br />
2- What can I do other than that if I want to have a good classification</p>
",Multilingual Language Processing & Language Identification,improve multiclass text classification german text new nlp bit confusing trying text classification svc dataset imbalanced dataset class text news class health sport culture economy science web using tf idf vectorization preprocessing step text remove since text german use first try metrci acuuracy score wa decided reduce dimensionality nd try added truncatedsvd metrci acuuracy score dropped question improve model want stick tf idf svc classification want good classification
Map city names to countries - python?,"<p>I have a dataframe that represents the location of some people.</p>
<p>This dataframe is not cleaned and the names are a mess. some rows have only the country name, others have name and city, and others have only the city. I also have sentences that are not in English.</p>
<p>How can I use python with NLP to tidy this dataset and get a homogenous dataset?</p>
<p>Here is a screenshot of the dataset:
<a href=""https://i.sstatic.net/pc50m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pc50m.png"" alt=""enter image description here"" /></a></p>
<p>Thanks in advance</p>
",Multilingual Language Processing & Language Identification,map city name country python dataframe represents location people dataframe cleaned name mess row country name others name city others city also sentence english use python nlp tidy dataset get homogenous dataset screenshot dataset thanks advance
Decide rather a text is about &quot;Topic A&quot; or not - NLP with Python,"<p>I'm trying to write a python program that will decide if a given post is about the topic of volunteering. My data-sets are small (only the posts, which are examined 1 by 1) so approaches like LDA do not yield results.</p>
<p>My end goal is a simple True/False, a post is about the topic or not.</p>
<p>I'm trying this approach:</p>
<ol>
<li>Using Google's word2vec model, I'm creating a &quot;cluster&quot; of words that are similar to the word: &quot;volunteer&quot;.</li>
</ol>
<pre><code>    CLUSTER = [x[0] for x in MODEL.most_similar_cosmul(&quot;volunteer&quot;, topn=120)]
</code></pre>
<ol start=""2"">
<li>Getting the posts and translating them to English, using Google translate.</li>
<li>Cleaning the translated posts using NLTK (removing stopwords, punctuation, and lemmatize the post)</li>
<li>Making a BOW out of the translated, clean post.</li>
<li>This stage is difficult for me. I want to calculate a &quot;distance&quot; / &quot;similarity&quot; / something that will help me get the True/False answer that I'm looking for, but I can't think of a good way to do that.</li>
</ol>
<p>Thank you for your suggestions and help in advance.</p>
",Multilingual Language Processing & Language Identification,decide rather text topic nlp python trying write python program decide given post topic volunteering data set small post examined approach like lda yield result end goal simple true false post topic trying approach using google word vec model creating cluster word similar word volunteer getting post translating english using google translate cleaning translated post using nltk removing stopwords punctuation lemmatize post making bow translated clean post stage difficult want calculate distance similarity something help get true false answer looking think good way thank suggestion help advance
How to use language independent canonical form in list entities in LUIS?,"<p>When using a closed list entity in LUIS, there always is a canonical form and optional multiple synonyms.</p>
<p>In the example below there is the canonical form &quot;green&quot; and one possible synonym &quot;olive&quot;.</p>
<p>When the user says &quot;olive&quot;, the frontend software doesn't have to care about &quot;olive&quot;, it get's the resolution &quot;green&quot; from LUIS instead.</p>
<p>But since language may change and the frontend software should be language independent, what I actually want to be delivered from the API is not the english term &quot;green&quot;, but a language independent identifier string for green, like &quot;my_chatbot_green_id&quot;.</p>
<p>Is it recommendable, to use the identifier as canocical form, and add &quot;green&quot; as a synonym instead? I suppose it isn't. Is there then any other way to achieve language independent identifiers?</p>
<p><strong>Original Example</strong></p>
<pre><code>  &quot;closedLists&quot;: [
    {
      &quot;name&quot;: &quot;ColorListEntity&quot;,
      &quot;subLists&quot;: [
        {
          &quot;canonicalForm&quot;: &quot;green&quot;,
          &quot;list&quot;: [
            &quot;olive&quot;
          ]
        },
        {
          &quot;canonicalForm&quot;: &quot;blue&quot;,
          &quot;list&quot;: [
            &quot;azure&quot;
          ]
        }
      ],
</code></pre>
<p><strong>Does this make sense?</strong></p>
<pre><code>  &quot;closedLists&quot;: [
    {
      &quot;name&quot;: &quot;ColorListEntity&quot;,
      &quot;subLists&quot;: [
        {
          &quot;canonicalForm&quot;: &quot;my_chatbot_green_id&quot;,
          &quot;list&quot;: [
            &quot;olive&quot;,
            &quot;green&quot;
          ]
        },
        {
          &quot;canonicalForm&quot;: &quot;my_chatbot_blue_id&quot;,
          &quot;list&quot;: [
            &quot;blue&quot;,
            &quot;azure&quot;
          ]
        }
      ],
</code></pre>
",Multilingual Language Processing & Language Identification,use language independent canonical form list entity luis using closed list entity luis always canonical form optional multiple synonym example canonical form green one possible synonym olive user say olive frontend software care olive get resolution green luis instead since language may change frontend software language independent actually want api english term green language independent identifier string green like chatbot green id recommendable use identifier canocical form add green synonym instead suppose way achieve language independent identifier original example doe make sense
packaging public data with R,"<p>I'm a college student who is interested in participating a hackathon for packaging public data with R.</p>
<p>This is my first time with R, packaging and public data. So I have few concerns that I'd like to be advised.
In order for me to participate the activity, I have to prepare a packaging idea with a simple business logic referring to how the package can do good to other businesses. My friend and I are currently working on developing a plan for a package that organizes csv files concerning bicycle roads and relevant gadget stations the government is providing access to individuals.</p>
<ol>
<li><p>How many functions should we devise for the plan? As I am new to packaging, other than few (about 4) simple functions like returning the number and the location of outdated gadgets if received the name of the region, I can't really imagine what to do with it.</p>
</li>
<li><p>Has anybody made a package for natural language processing with R using public data? I would like if this hackathon experience relate to my study of nlp but considering the nature of the language R, I'm thinking maybe this isn't the right project to plan for this time.</p>
</li>
</ol>
<p>Thank you in advance for your comment :)</p>
",Multilingual Language Processing & Language Identification,packaging public data r college student interested participating hackathon packaging public data r first time r packaging public data concern like advised order activity prepare packaging idea simple business logic referring package good business friend currently working developing plan package organizes csv file concerning bicycle road relevant gadget station government providing access individual many function devise plan new packaging simple function like returning number location outdated gadget received name region really imagine ha anybody made package natural language processing r using public data would like hackathon experience relate study nlp considering nature language r thinking maybe right project plan time thank advance comment
NLTK Brill demo does not produce errors.out,"<p>I'm reading the book <em>Natural Language Processing with Python</em> recently. At Section 5.6 <em>Transformation-Based Tagging</em>, the code <code>print(open(&quot;errors.out&quot;).read())</code>doesn't work and gives the following error information:</p>
<pre><code>Traceback (most recent call last):
  File &quot;E:/Python Practice/NLP/Chapter5.py&quot;, line 337, in &lt;module&gt;
    print(open(&quot;errors.out&quot;).read())
FileNotFoundError: [Errno 2] No such file or directory: 'errors.out'
</code></pre>
<p>The whole code from this demo is</p>
<pre><code>from nltk.tbl import demo as brill_demo
brill_demo.demo()
print(open(&quot;errors.out&quot;).read())
</code></pre>
<p>When I delete the last code line <code>print(open(&quot;errors.out&quot;).read())</code>, it works well and can output results.</p>
<p>Maybe the code line is suitable for python 2.x, but I'm using python 3.7 and haven't found an effective solution yet. Could anyone help me?</p>
",Multilingual Language Processing & Language Identification,nltk brill demo doe produce error reading book natural language processing python recently section transformation based tagging code work give following error information whole code demo delete last code line work well output result maybe code line suitable python x using python found effective solution yet could anyone help
How to remove a keyword from a given sentence,"<p>How can I remove a <strong>keyword</strong> from a phrase?</p>
<p>For example,</p>
<blockquote>
<p>Lionel Andr√©s Messi is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team.</p>
</blockquote>
<p>How can I remove a keyword (except for the name of the person) from this sentence, such as &quot;American&quot;, &quot;footballer&quot;, &quot;Barcelona&quot; to name a few.</p>
<p>I have realised that the keyword must be a <strong>noun</strong>, and I came across a library called NLTK, and maybe that can help me what I want to achieve.</p>
<p>Function example:</p>
<pre><code>remove(sentence, word_to_not_remove)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; sentence = 'Lionel Andr√©s Messi is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team.'
&gt;&gt;&gt; remove(sentence, 'Lionel Andr√©s Messi')
footballer
</code></pre>
",Multilingual Language Processing & Language Identification,remove keyword given sentence remove keyword phrase example lionel andr messi argentine professional footballer play forward captain spanish club barcelona argentina national team remove keyword except name person sentence american footballer barcelona name realised keyword must noun came across library called nltk maybe help want achieve function example
How to extract rules from conditional statements using NLP?,"<p>Consider that I am given an English sentence such as:</p>
<p>&quot;If x1 is greater than x2, set y to 2&quot;</p>
<p>Is there a method to extract the conditions and actions from such a statement in a &quot;action&quot;-parse tree or code format such as below?</p>
<pre><code>if x1 &gt; x2:
    y = 2
</code></pre>
",Multilingual Language Processing & Language Identification,extract rule conditional statement using nlp consider given english sentence x greater x set method extract condition action statement action parse tree code format
Autocorrect a document corpus,"<p>I have an approximately 6GB sized document corpus of mostly user generated content on mobile platforms. Due to the nature of origin of this corpus, it is rife with misspelled, abbreviated and truncated words. Is there a way i could autocorrect these words to the nearest English language word?</p>
",Multilingual Language Processing & Language Identification,autocorrect document corpus approximately gb sized document corpus mostly user generated content mobile platform due nature origin corpus rife misspelled abbreviated truncated word way could autocorrect word nearest english language word
How to suppress logging in spacy?,"<p>I'm using <code>spacy</code> right now to process some non-English text data. An example of the code snippet is below</p>
<pre><code>nlp = spacy.load('fr')
pos = POSTagger()
french_lemmatizer = LefffLemmatizer(after_melt=True, default=True)
nlp.add_pipe(pos, name='pos', after='parser')
nlp.add_pipe(french_lemmatizer, name='lefff', after='pos')
doc = nlp(u&quot;Apple cherche a acheter une startup anglaise pour 1 milliard de dollard&quot;)
for d in doc:
    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)
</code></pre>
<p>Whenever I run that code, it outputs these logs</p>
<pre><code>2020-11-05 17:35:00,497 - spacy_lefff.melt_tagger - INFO -   TAGGER: Loading lexicon...
2020-11-05 17:35:01,115 - spacy_lefff.melt_tagger - INFO -   TAGGER: Loading tags...
2020-11-05 17:35:01,152 - spacy_lefff.melt_tagger - INFO -   TAGGER: Loading model from /usr/local/Caskroom/miniconda/base/envs/computer_vision_playground/lib/python3.6/site-packages/spacy_lefff/data/tagger/models/fr...
2020-11-05 17:35:01,853 - spacy_lefff.melt_tagger - INFO -   TAGGER: Loading model from /usr/local/Caskroom/miniconda/base/envs/computer_vision_playground/lib/python3.6/site-packages/spacy_lefff/data/tagger/models/fr: done
2020-11-05 17:35:01,857 - spacy_lefff.lefff - INFO - New LefffLemmatizer instantiated.
2020-11-05 17:35:01,859 - spacy_lefff.lefff - INFO - Reading lefff data...
2020-11-05 17:35:02,472 - spacy_lefff.lefff - INFO - Successfully loaded lefff lemmatizer
2020-11-05 17:35:02,549 - spacy_lefff.melt_tagger - INFO -   TAGGER: POS Tagging...
</code></pre>
<p>Now, I need to use this POS tagger for thousands of text and I need to iterate through them one by one. The printing of these logs might be too much for a Jupyter Notebook, hence I want to suppress them and use progress monitor package <code>tqdm</code> which is easy to use on for loops. Is there any way to suppress the logs?</p>
",Multilingual Language Processing & Language Identification,suppress logging spacy using right process non english text data example code snippet whenever run code output log need use po tagger thousand text need iterate one one printing log might much jupyter notebook hence want suppress use progress monitor package easy use loop way suppress log
How to get general categories for text using NLP like fasttext?,"<p>I am working on an application and I would like to infer general categories from the text using natural language processing.  I am new to Natural Language Processing (NLP).</p>
<p>The Google Natural Language API does this using a reasonable high-level set of content categories such as &quot;/Arts &amp; Entertainment&quot;, &quot;/Hobbies &amp; Leisure&quot;, etc:</p>
<p><a href=""https://cloud.google.com/natural-language/docs/categories"" rel=""nofollow noreferrer"">https://cloud.google.com/natural-language/docs/categories</a></p>
<p>I am hoping to do this using open source and would like to use some general categories such as the Wikipedia high level classifications:</p>
<p><a href=""https://en.wikipedia.org/wiki/Category:Main_topic_classifications"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Category:Main_topic_classifications</a></p>
<p>fasttext seems like a good option but I'm struggling to find a corpus to use for training.  I do see the wikipedia word vector files and can get the full wikipedia download but I don't see an easy way to get the articles tagged with the categories for fasttext.</p>
<p>Is there some open source tool that can identify high-level general categories given some text -- or is there a training dataset I could use?</p>
",Multilingual Language Processing & Language Identification,get general category text using nlp like fasttext working application would like infer general category text using natural language processing new natural language processing nlp google natural language api doe using reasonable high level set content category art entertainment hobby leisure etc hoping using open source would like use general category wikipedia high level classification fasttext seems like good option struggling find corpus use training see wikipedia word vector file get full wikipedia download see easy way get article tagged category fasttext open source tool identify high level general category given text training dataset could use
Spacy pos tagging PPER,"<p>I'm trying to make spacy tag a german word with <code>PPER</code>. See <a href=""https://spacy.io/api/annotation"" rel=""nofollow noreferrer"">https://spacy.io/api/annotation</a>. Here is my code:</p>
<pre><code>import de_core_news_sm

nlp = de_core_news_sm.load()

tokenized = nlp(&quot;Der Mann liebt Kuchen.&quot;)
for token in tokenized:
    print(token, token.pos_, token.ent_type_)
</code></pre>
<p>Which string do I need to enter to get a <code>PPER</code> tag?</p>
",Multilingual Language Processing & Language Identification,spacy po tagging pper trying make spacy tag german word see code string need enter get tag
Where to start: Natural language processing and AI using Python,"<p>My goal is to write a program capable of extracting tone, personality, and intent from human language inquiries (e.g. I type: How are you doing today? And the AI system responds with something like: Fine. How are you?)</p>

<p>I'm aware this is a non-trivial problem, so what deep-learning topics should I start becoming familiar with and what Python modules are most useful? I've already started looking at NLTK. Thanks.</p>
",Multilingual Language Processing & Language Identification,start natural language processing ai using python goal write program capable extracting tone personality intent human language inquiry e g type today ai system responds something like fine aware non trivial problem deep learning topic start becoming familiar python module useful already started looking nltk thanks
&#39;bert-base-multilingual-uncased&#39; dataloader RuntimeError : stack expects each tensor to be equal size,"<p>I am a begineer in nlp, as I was giving this competition <a href=""https://www.kaggle.com/c/contradictory-my-dear-watson"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/contradictory-my-dear-watson</a> I am using the model 'bert-base-multilingual-uncased' and using BERT tokenizer from the same. I am also using kaggle tpu. This is the custom dataloader I created.</p>
<pre><code>class SherlockDataset(torch.utils.data.Dataset):

def __init__(self,premise,hypothesis,tokenizer,max_len,target = None):
    super(SherlockDataset,self).__init__()
    self.premise = premise
    self.hypothesis = hypothesis
    self.tokenizer = tokenizer
    self.max_len = max_len
    self.target = target

def __len__(self):
    return len(self.premise)

def __getitem__(self,item):
    sen1 = str(self.premise[item])
    sen2 = str(self.hypothesis[item])
    
    encode_dict = self.tokenizer.encode_plus(sen1,
                                        sen2,
                                        add_special_tokens = True,
                                        max_len = self.max_len,
                                        pad_to_max_len = True,
                                        return_attention_mask = True,
                                        return_tensors = 'pt'
                                       )
    input_ids = encode_dict[&quot;input_ids&quot;][0]
    token_type_ids = encode_dict[&quot;token_type_ids&quot;][0]
    att_mask = encode_dict[&quot;attention_mask&quot;][0]
    
    if self.target is not None:
        sample = {
        &quot;input_ids&quot;:input_ids,
        &quot;token_type_ids&quot;:token_type_ids,
        &quot;att_mask&quot;:att_mask,
        &quot;targets&quot;: self.target[item]
        }
    else:
        sample = {
        &quot;input_ids&quot;:input_ids,
        &quot;token_type_ids&quot;:token_type_ids,
        &quot;att_mask&quot;:att_mask
        }
    
    return sample
</code></pre>
<p>and during the time of loading data in dataloader</p>
<pre><code>def train_fn(model,dataloader,optimizer,criterion,scheduler = None):
model.train()
print(&quot;train&quot;)
for idx, sample in enumerate(dataloader):
    '''
    input_ids = sample[&quot;input_ids&quot;].to(config.DEVICE)
    token_type_ids = sample[&quot;token_type_ids&quot;].to(config.DEVICE)
    att_mask = sample[&quot;att_mask&quot;].to(config.DEVICE)
    targets = sample[&quot;targets&quot;].to(config.DEVICE)
    '''
    print(&quot;train_out&quot;)
    input_ids = sample[0].to(config.DEVICE)
    token_type_ids = sample[1].to(config.DEVICE)
    att_mask = sample[2].to(config.DEVICE)
    targets = sample[3].to(config.DEVICE)
    
    optimizer.zero_grad()
    output = model(input_ids,token_type_ids,att_mask)
    output = np.argmax(output,axis = 1)
    loss = criterion(outputs,targets)
    accuracy = accuracy_score(output,targets)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)
    xm.optimizer_step(optimizer, barrier=True)
    if scheduler is not None:
        scheduler.step()
    if idx%50==0:
        print(f&quot;idx : {idx}, TRAIN LOSS : {loss}&quot;)
</code></pre>
<p>I am facing this error again and again</p>
<pre><code>RuntimeError: Caught RuntimeError in DataLoader worker process 0. Original Traceback (most recent 
call last): File &quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&quot;, line 
178, 
in _worker_loop data = fetcher.fetch(index) File &quot;/opt/conda/lib/python3.7/site- 
packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch return self.collate_fn(data) File 
&quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 79, in 
 default_collate return [default_collate(samples) for samples in transposed] File 
&quot;/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 79, in return 
 [default_collate(samples) for samples in transposed] File &quot;/opt/conda/lib/python3.7/site- 
 packages/torch/utils/data/_utils/collate.py&quot;, line 55, in default_collate return torch.stack(batch, 
 0, out=out) RuntimeError: stack expects each tensor to be equal size, but got [47] at entry 0 and 
 [36] at entry 1
</code></pre>
<p>I have tried changing num_workers values,changing batch sizes. I have checked the data and none of the text in it is null, 0 or corrupt in any sense. I have also tried changing max_len in  tokenizer but I am not able to find out solution to this problem. Please check and let me know how can I fix it.</p>
",Multilingual Language Processing & Language Identification,bert base multilingual uncased dataloader runtimeerror stack expects tensor equal size begineer nlp wa giving competition using model bert base multilingual uncased using bert tokenizer also using kaggle tpu custom dataloader created time loading data dataloader facing error tried changing num worker value changing batch size checked data none text null corrupt sense also tried changing max len tokenizer able find solution problem please check let know fix
How to get translations of one batch of sentences after batch_encode_plus?,"<p>I want to get translations of one batch of sentences using pretrained model.</p>
<pre><code>model = AutoModelWithLMHead.from_pretrained(&quot;Helsinki-NLP/opus-mt-es-en&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-es-en&quot;)
batch_input_str = ((&quot;Mary spends $20 on pizza&quot;), (&quot;She likes eating it&quot;), (&quot;The pizza was great&quot;))
encoded = (tokenizer.batch_encode_plus(batch_input_str, pad_to_max_length=True))
</code></pre>
<p>The <code>encoded</code>is like:</p>
<pre><code>{'input_ids': [[4963, 10154, 5021, 9, 25, 1326, 2255, 35, 17462, 0], [552, 3996, 2274, 9, 129, 75, 2223, 25, 1370, 0], [42, 17462, 12378, 9, 25, 5807, 1949, 0, 65000, 65000]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}
</code></pre>
<p>Then, should I just pass the <code>encoded</code> to</p>
<pre><code>output = model.generate(a)
</code></pre>
<p>And then use</p>
<pre><code>res = tokenizer.decode(output)
</code></pre>
<p>?</p>
<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,get translation one batch sentence batch encode plus want get translation one batch sentence using pretrained model like pas use thanks
Accessing word level predictions with Google Translate,"<p>There's a research project involving translation I'm thinking about, and for it, I'd like to use google's translation system. However, I need access not just to the translation of an input sentence, but the output of a beam search with probabilities at each timestep, e.g.:</p>

<p>input: ""The cat is sitting""
output (e.g. in Spanish):</p>

<p>t=1:
el: 0.8, un: 0.1, ...: 0.1</p>

<p>t=2:
el gato: 0.9, el felino: 0.05, un gato: 0.001
...</p>

<p>t=3...</p>

<p>Is there any way to get ahold of this? I've tried training a model with Tensor2Tensor, but it's far worse than google's online translation and still very hard to access incremental results. Fairseq is better, but still far far worse than Google Translate.</p>

<p>Thanks!</p>

<p>Reuben</p>
",Multilingual Language Processing & Language Identification,accessing word level prediction google translate research project involving translation thinking like use google translation system however need access translation input sentence output beam search probability timestep e g input cat sitting output e g spanish el un el gato el felino un gato way get ahold tried training model tensor tensor far worse google online translation still hard access incremental result fairseq better still far far worse google translate thanks reuben
obtain confidence/distance on country extracted by geograpy3,"<p>I tried a simple demo to check if geograpy can do what i'm looking for: <strong>trying to find the country name and iso code in denormalized addresses</strong> (which is basically what geograpy is written for!).
<br/>The problem is that, in the test i made, geograpy is able to found several country for each address used, including the right in most of cases, but <strong>i can't find any type of parameters to decide which country is the most &quot;correct&quot;.</strong>
<br/>The list of fake addresses that i used, which may reflect reality that could be analyzed, is this:</p>
<ul><li>John Doe 115 Huntington Terrace Newark, New York 07112 Stati Uniti</li>
<li>John Doe 160 Huntington Terrace Newark, New York 07112 United States of America</li>
<li>John Doe 30 Huntington Terrace Newark, New York 07112 USA</li>
<li>John Doe 22 Huntington Terrace Newark, New York 07112 US</li>
<li>Mario Bianchi, Via Nazionale 256, 00148 Roma (RM) Italia</li>
<li>Mario Bianchi, Via Nazionale 256, 00148 Roma (RM) Italy</li>
</ul>
<p>This is the simple code written:</p>
<pre><code>import geograpy

ind = [&quot;John Doe 115 Huntington Terrace Newark, New York 07112 Stati Uniti&quot;,
&quot;John Doe 160 Huntington Terrace Newark, New York 07112 United States of America&quot;,
&quot;John Doe 30 Huntington Terrace Newark, New York 07112 USA&quot;,
&quot;John Doe 22 Huntington Terrace Newark, New York 07112 US&quot;,
&quot;Mario Bianchi, Via Nazionale 256, 00148 Roma (RM) Italia&quot;,
&quot;Mario Bianchi, Via Nazionale 256, 00148 Roma (RM) Italy&quot;]

locator = geograpy.locator.Locator()
for address in ind:
    places = geograpy.get_place_context(text=address)
    print(address)
    #print(places)
    for country in places.countries:
      print(&quot;Country:&quot;+country+&quot;, IsoCode:&quot;+locator.getCountry(name=country).iso)
    print()
</code></pre>
<p>and this is the output:</p>
<pre><code>John Doe 115 Huntington Terrace Newark, New York 07112 Stati Uniti
Country:United Kingdom, IsoCode:GB
Country:Jamaica, IsoCode:JM
Country:United States, IsoCode:US

John Doe 160 Huntington Terrace Newark, New York 07112 United States of America
Country:United States, IsoCode:US
Country:United Kingdom, IsoCode:GB
Country:Netherlands, IsoCode:NL
Country:Jamaica, IsoCode:JM
Country:Argentina, IsoCode:AR

John Doe 30 Huntington Terrace Newark, New York 07112 USA
Country:United Kingdom, IsoCode:GB
Country:Jamaica, IsoCode:JM
Country:United States, IsoCode:US

John Doe 22 Huntington Terrace Newark, New York 07112 US
Country:United Kingdom, IsoCode:GB
Country:Jamaica, IsoCode:JM
Country:United States, IsoCode:US

Mario Bianchi, Via Nazionale 256, 00148 Roma (RM) Italia
Country:Australia, IsoCode:AU
Country:Sweden, IsoCode:SE
Country:United States, IsoCode:US

Mario Bianchi, Via Nazionale 256, 00148 Roma (RM) Italy
Country:Italy, IsoCode:IT
Country:Australia, IsoCode:AU
Country:Sweden, IsoCode:SE
Country:United States, IsoCode:US
</code></pre>
<p>First of all, <strong>the biggest problem is that in italian address (number 4) is unable to find at all the right country</strong> (Italia/Italy), and i don't know from where the three country found comes from.
<br/>The seconds it that in most cases it find wrong country, in addiction to the right, and <strong>i don't have any type of indicator about confidence percentage, distance, or something that could me understand if a country</strong> located could be considered acceptable as answer and, in multiple results, what <strong>could be the &quot;best&quot;</strong>.
<br/><br/>I want  to apologize in advance, but I didn't have time to study geograpy3 in depth and i don't know if this is a stupid question, but i haven't found anything about confidence/probability/distance in documentation.</p>
",Multilingual Language Processing & Language Identification,obtain confidence distance country extracted geograpy tried simple demo check geograpy looking trying find country name iso code denormalized address basically geograpy written problem test made geograpy able found several country address used including right case find type parameter decide country correct list fake address used may reflect reality could analyzed john doe huntington terrace newark new york stati uniti john doe huntington terrace newark new york united state america john doe huntington terrace newark new york usa john doe huntington terrace newark new york u mario bianchi via nazionale rom rm italia mario bianchi via nazionale rom rm italy simple code written output first biggest problem italian address number unable find right country italia italy know three country found come second case find wrong country addiction right type indicator confidence percentage distance something could understand country located could considered acceptable answer multiple result could best want apologize advance time study geograpy depth know stupid question found anything confidence probability distance documentation
Extracting verb from german sentenceces,"<p>I'm trying to extract verbs from German sentences. The problem is, for example in this sentence</p>
<p><code>Ich rufe noch einmal an.</code></p>
<p>Im getting rufe as the verb but its anrufe. I'm using textBlob and dont really know anything about linguistic. and using textblob I came accross POS tags. It tagged <code>an</code> as &quot;RP&quot;(doesnt know what that means) and <code>rufe</code> as &quot;VB&quot;. I could just glue all &quot;RP&quot; and &quot;VB&quot; together but then again there could more than one verb in a sentence.</p>
<p>What is the right way of doing this?</p>
",Multilingual Language Processing & Language Identification,extracting verb german sentenceces trying extract verb german sentence problem example sentence im getting rufe verb anrufe using textblob dont really know anything linguistic using textblob came accross po tag tagged rp doesnt know mean vb could glue rp vb together could one verb sentence right way
how we can detect seperate text for a text multiligue?,"<p>I have a text ( in english and frensh) in input : i want to seperate the text into two sub-text for each language : so , we will detect the languages in text (&gt; 2 languages) , then , cut each text in his own language:</p>
<p>INPUT :</p>
<pre><code>You will then discover galleries in which 25-million bottles rest in the cellars, waiting for the 
perfect moment to be tasted. From the bottle to the salmanazar, from the youngest wines to the oldest   
vintages. - Vous trouverez alors des galeries parmi lesquelles 25 000 000 bouteilles reposent dans les   
caves, attendant le parfait moment pour √™tre d√©gust√©es.
</code></pre>
<p>DESIRED OUTPUT:</p>
<pre><code>This text contains two languages : &quot;fr&quot; and &quot;en&quot;

text_in_english= &quot;You will then discover galleries in which 25-million bottles rest in the cellars,  
waiting for the perfect moment to be tasted. From the bottle to the salmanazar, from the youngest  
wines to the oldest vintages.&quot;

text_in_frensh= &quot;- Vous trouverez alors des galeries parmi lesquelles 25 000 000 bouteilles reposent 
dans les caves, attendant le parfait moment pour √™tre d√©gust√©es.&quot;
</code></pre>
<p>How we can do this please ?</p>
",Multilingual Language Processing & Language Identification,detect seperate text text multiligue text english frensh input want seperate text two sub text language detect language text language cut text language input desired output please
NLTK Brown Corpus: Tabulate Function does not work,"<p>The code is excerpted from Natural Language Processing with Python, page 119. Frequency of modals in different sections of the Brown Corpus. My issue turns out to be that it fails to tabulate like the book describes. Basically I don't know why this would happen. My Python version is 3.7.9 64-bit. All extensions goes well.</p>
<p><strong>Frequency of modals in different sections of Brown corpus</strong></p>
<pre><code>def tabulate(cfdist, words, categories):
    print('%-16s' % 'Category')
    for word in words:                  # column headings
        print('%6s' % word,)
    print()
    for category in categories:
        print('%-16s' % category,)      # row headings
        for word in words:              # for each word
            print('%6d' % cfdist[category][word])   # print table cell
        print()                         # end the row

cfd = nltk.ConditionalFreqDist(
        (genre, word)
        for genre in brown.categories()
        for word in brown.words(categories=genre))
genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
modals = ['can', 'could', 'may', 'might', 'must', 'will']
tabulate(cfd, modals, genres)
</code></pre>
",Multilingual Language Processing & Language Identification,nltk brown corpus tabulate function doe work code excerpted natural language processing python page frequency modal different section brown corpus issue turn fails tabulate like book describes basically know would happen python version bit extension go well frequency modal different section brown corpus
Understanding ROUGE vs BLEU,"<p>I am looking into metrics for measuring the quality of text-summarization.
For this, I have found this SO <a href=""https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge"">answer</a> which states:</p>
<blockquote>
<p>Bleu measures precision: how much the words (and/or n-grams) in the
machine generated summaries appeared in the human reference summaries.</p>
<p>Rouge measures recall: how much the words (and/or n-grams) in the
human reference summaries appeared in the machine generated summaries.</p>
</blockquote>
<p>Although in this <a href=""https://stats.stackexchange.com/questions/301626/interpreting-rouge-scores/312354#312354"">answer</a> of SE I find this:</p>
<blockquote>
<p>ROUGE-n recall=40% means that 40% of the n-grams in the reference
summary are also present in the generated summary.</p>
<p>ROUGE-n precision=40% means that 40% of the n-grams in the generated summary
are also present in the reference summary.</p>
<p>ROUGE-n F1-score=40% is
more difficult to interpret, like any F1-score.</p>
</blockquote>
<p>This is contradictory. Its sounds like <strong>Rouge-Precision</strong> is <strong>equal</strong> to <strong>BLEU</strong> and <strong>Rouge-Recall</strong> is <strong>equal</strong> to the statement made in the <strong>SO answer</strong>. Is Rouge-Precision the same as BLEU as it implemented BLEU?</p>
<p>It is also stated in the <a href=""https://www.aclweb.org/anthology/W04-1013.pdf"" rel=""nofollow noreferrer"">paper</a>:</p>
<blockquote>
<p>It is clear that ROUGE-N is a recall-related measure because the
denominator of the equation is the total sum of the number of n-grams
occurring at the reference summary side. A closely related measure,
BLEU, used in automatic evaluation of machine translation, is a
precision-based measure.</p>
</blockquote>
<p>I dont understand this, as (atleast) <strong>rouge</strong> returns a <strong>precision</strong> and a <strong>recall</strong> value. Can anybody bring some clearness into this?
Thank you!</p>
",Multilingual Language Processing & Language Identification,understanding rouge v bleu looking metric measuring quality text summarization found rouge n recall mean n gram reference summary also present generated summary rouge n precision mean n gram generated summary also present reference summary rouge n f score difficult interpret like f score contradictory sound like rouge precision equal bleu rouge recall equal statement made answer rouge precision bleu implemented bleu also stated paper clear rouge n recall related measure denominator equation total sum number n gram occurring reference summary side closely related measure bleu used automatic evaluation machine translation precision based measure dont understand atleast rouge return precision recall value anybody bring clearness thank
Graph to connect sentences,"<p>I have a list of sentences of a few topics (two) like the below:</p>
<pre><code>Sentences
Trump says that it is useful to win the next presidential election. 
The Prime Minister suggests the name of the winner of the next presidential election.
In yesterday's conference, the Prime Minister said that it is very important to win the next presidential election. 
The Chinese Minister is in London to discuss about climate change.
The president Donald Trump states that he wants to win the presidential election. This will require a strong media engagement.
The president Donald Trump states that he wants to win the presidential election. The UK has proposed collaboration. 
The president Donald Trump states that he wants to win the presidential election. He has the support of his electors. 
</code></pre>
<p>As you can see there is similarity in sentences.</p>
<p><a href=""https://i.sstatic.net/TVJ8f.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/TVJ8f.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to relate multiple sentences and visualise the characteristics of them by using a graph (directed). The graph is built from a similarity matrix, by applying row ordering of sentences as shown above.
I created a new column, Time, to show the order of sentences, so first row (Trump says that....) is at time 1; second row (The Prime Minister suggests...) is at time 2, and so on.
Something like this</p>
<pre><code>Time    Sentences
1           Trump said that it is useful to win the next presidential election. 
2           The Prime Minister suggests the name of the winner of the next presidential election.

3           In today's conference, the Prime Minister said that it is very important to win the next presidential election. 

...
</code></pre>
<p>I would like then to find the relationships in order to have a clear overview of the topic.
Multiple paths for a sentence would show that there are multiple information associated with it.
To determine similarity between two sentences, I tried to extract nouns and verbs as follows:</p>
<pre><code>noun=[]
verb=[]
for  index, row in df.iterrows():
      nouns.append([word for word,pos in pos_tag(row[0]) if pos == 'NN'])
      verb.append([word for word,pos in pos_tag(row[0]) if pos == 'VB'])
</code></pre>
<p>as they are keywords in whatever sentence.
So when a keyword (noun or verb) appears in sentence x but not in the other sentences, it represents a difference between these two sentences.
I think a better approach, however, could be using word2vec or gensim (WMD).</p>
<p>This similarity has to be calculated for each sentence.
I would like to build a graph which shows the content of the sentence in my example above.
Since there are two topics (Trump and Chinese Minister), for each of them I need to look for sub-topics. Trump has sub-topic presidential election, for example. A node in my graph should represent a sentence. Words in each node represent differences for the sentences, showing new info in the sentence. For example, the word <code>states</code> in sentence at time 5 is in adjacent sentences at time 6 and 7.
I would like just to find a way to have similar results as shown in picture below. I have tried using mainly nouns and verbs extraction, but probably it is not the right way to proceed.
What I tried to do has been to consider sentence at time 1 and compare it with other sentences, assigning a similarity score (with noun and verbs extraction but also with word2vec), and repeat it for all the other sentences.
But my problem is now on how to extract difference to create a graph that can make sense.</p>
<p>For the part of the graph, I would consider to use networkx (DiGraph):</p>
<pre><code>G = nx.DiGraph()
N = Network(directed=True) 
</code></pre>
<p>to show direction of relationships.</p>
<p>I provided a different example to make it be clearer (but if you worked with the previous example, it would be fine as well. Apologies for the inconvenience, but since my first question was not so clear, I had to provide also a better, probably easier, example).</p>
",Multilingual Language Processing & Language Identification,graph connect sentence list sentence topic two like see similarity sentence trying relate multiple sentence visualise characteristic using graph directed graph built similarity matrix applying row ordering sentence shown created new column time show order sentence first row trump say time second row prime minister suggests time something like would like find relationship order clear overview topic multiple path sentence would show multiple information associated determine similarity two sentence tried extract noun verb follows keywords whatever sentence keyword noun verb appears sentence x sentence represents difference two sentence think better approach however could using word vec gensim wmd similarity ha calculated sentence would like build graph show content sentence example since two topic trump chinese minister need look sub topic trump ha sub topic election example node graph represent sentence word node represent difference sentence showing new info sentence example word sentence time adjacent sentence time would like find way similar result shown picture tried using mainly noun verb extraction probably right way proceed tried ha consider sentence time compare sentence assigning similarity score noun verb extraction also word vec repeat sentence problem extract difference create graph make sense part graph would consider use networkx digraph show direction relationship provided different example make clearer worked previous example would fine well apology inconvenience since first question wa clear provide also better probably easier example
Dutch sentiment analysis using R,"<p>In RStudio, I have column containing <strong>Dutch</strong> sentences which I would like to add a polarity score between -1.0 and +1.0 to via sentiment analysis. I've already tried to use the <code>pattern.nlp</code> package from jwfijffels, but this didn't work for me. I found an instruction on <a href=""https://github.com/bnosac/pattern.nlp"" rel=""nofollow noreferrer"">https://github.com/bnosac/pattern.nlp</a> in which it is explained that - in order for the nlp package to work, you should download a specific version of Python and perform some additional steps. However, these steps are a bit vague to me.</p>
<p>Is there someone who can explain this installation process to me in more detail? Actually, the whole section under &quot;Installation&quot; is a bit of a mystery to me. What should I download specifically?
Where to run the code <code>pip install pattern</code>? How do I properly set the PATH? It would be much appreciated if someone would guide me trough it step by step.</p>
<p>Or: if someone knows another way to perform sentiment analysis on text, I would of course be open to it, e.g. translating the Dutch sentences to English and then perform the sentiment analysis. Or would such a translation be a bad idea?</p>
<p>Here a set of 6 Dutch sentences.</p>
<pre><code>text = c(&quot;Slechte bediening, van begin tot eind&quot;,
         &quot;Het eten was heerlijk en de bediening was fantastisch&quot;,
         &quot;Geweldige service en beleefde bediening&quot;,
         &quot;Verschrikkelijk. Ik had een vlieg in mijn soep&quot;, 
         &quot;Het was ok√©. De bediening kon wat beter, maar het eten was wel lekker. Leuk sfeertje wel!&quot;,
         &quot;Ondanks dat het druk was toch op tijd ons eten gekregen. Complimenten aan de kok voor het op smaak brengen van mijn biefstuk&quot;)
identifier &lt;- c(&quot;3&quot;, &quot;4&quot;, &quot;6&quot;, &quot;7&quot;, &quot;1&quot;, &quot;5&quot;)
df &lt;- data.frame(identifier, text)
</code></pre>
",Multilingual Language Processing & Language Identification,dutch sentiment analysis using r rstudio column containing dutch sentence would like add polarity score via sentiment analysis already tried use package jwfijffels work found instruction explained order nlp package work download specific version python perform additional step however step bit vague someone explain installation process detail actually whole section installation bit mystery download specifically run code properly set path would much appreciated someone would guide trough step step someone know another way perform sentiment analysis text would course open e g translating dutch sentence english perform sentiment analysis would translation bad idea set dutch sentence
Where did I go wrong with retrieving POS proportions through analysis with spaCy?,"<p>So, recently, I've been quite intent on getting some statistics regarding the vocabulary of some of the books I like. Nothing advanced---just things like, what is the average length of a sentence? How many adjectives are used in proportion to the total number of words?</p>
<p>The problem is---spaCy, which is the tool I chose for sheer simplicity of use, seems to indicate that: EVERY TEXT I analyze seems to have EXACTLY 5.88235% of adjectives compared to total number of words. EXACTLY 2.94118% of auxiliaries. EXACTLY 14.7059% of nouns. EXACTLY 34 words per sentence (!).</p>
<p>Now, I am pretty sure that I have made some mistake in the code I am using---it's just, I can't see it!</p>
<p>Here is a sample code. I've divided the book by chapter (34 total) and printed a table of some statistics. The numbers seem legit---but these proportions, there is NO WAY every chapter in every text I am analyzing has exactly the same structure.</p>
<pre><code>headers = ['chapter', 'num_sents', 'num_words', 'num_verbs', 'prop_verbs', 'num_adjs', \
           'prop_adjs', 'num_adv', 'prop_adv', 'num_aux', 'prop_aux', 'num_intj', \
           'prop_intj', 'num_non', 'prop_non', 'num_pron', 'prop_pron', 'num_propn', \
           'prop_propn', 'words/sentences']
stats = list()
count_chpt = 1
for chapter in kf_spc:
  num_sents = 0
  num_words = 0
  num_verbs = 0
  num_adjs = 0
  num_adv = 0
  num_aux = 0
  num_intj = 0
  num_non = 0
  num_pron = 0
  num_propn = 0
  for sentence in chapter.sents:
    num_sents += 1
    for token in sent:
      num_words += 1
      if token.pos_ == 'VERB':
        num_verbs += 1
      elif token.pos_ == 'ADJ':
        num_adjs += 1
      elif token.pos_ == 'ADV':
        num_adv += 1
      elif token.pos_ == 'AUX':
        num_aux += 1
      elif token.pos_ == 'INTJ':
        num_intj += 1
      elif token.pos_ == 'NOUN':
        num_non += 1
      elif token.pos_ == 'PRON':
        num_pron += 1
      elif token.pos_ == 'PROPN':
        num_propn += 1
  stats.append([count_chpt, num_sents, num_words, num_verbs, num_verbs/num_words, \
                num_adjs, num_adjs/num_words, num_adv, num_adv/num_words, \
                num_aux, num_aux/num_words, num_intj, num_intj/num_words, num_non, \
                num_non/num_words, num_pron, num_pron/num_words, num_propn, \
                num_propn/num_words, num_words/num_sents])
  count_chpt += 1

print(tabulate(stats, headers=headers))
</code></pre>
<p>OUTPUT:</p>
<pre><code>  chapter    num_sents    num_words    num_verbs    prop_verbs    num_adjs    prop_adjs    num_adv    prop_adv    num_aux    prop_aux    num_intj    prop_intj    num_non    prop_non    num_pron    prop_pron    num_propn    prop_propn    words/sentences
---------  -----------  -----------  -----------  ------------  ----------  -----------  ---------  ----------  ---------  ----------  ----------  -----------  ---------  ----------  ----------  -----------  -----------  ------------  -----------------
        1          251         8534         1255      0.147059         251    0.0294118        502   0.0588235        251   0.0294118           0            0       1255    0.147059         502    0.0588235          753     0.0882353                 34
        2          155         5270          775      0.147059         155    0.0294118        310   0.0588235        155   0.0294118           0            0        775    0.147059         310    0.0588235          465     0.0882353                 34
        3          235         7990         1175      0.147059         235    0.0294118        470   0.0588235        235   0.0294118           0            0       1175    0.147059         470    0.0588235          705     0.0882353                 34
        4          226         7684         1130      0.147059         226    0.0294118        452   0.0588235        226   0.0294118           0            0       1130    0.147059         452    0.0588235          678     0.0882353                 34
        5           80         2720          400      0.147059          80    0.0294118        160   0.0588235         80   0.0294118           0            0        400    0.147059         160    0.0588235          240     0.0882353                 34
        6          276         9384         1380      0.147059         276    0.0294118        552   0.0588235        276   0.0294118           0            0       1380    0.147059         552    0.0588235          828     0.0882353                 34
        7          276         9384         1380      0.147059         276    0.0294118        552   0.0588235        276   0.0294118           0            0       1380    0.147059         552    0.0588235          828     0.0882353                 34
        8          412        14008         2060      0.147059         412    0.0294118        824   0.0588235        412   0.0294118           0            0       2060    0.147059         824    0.0588235         1236     0.0882353                 34
        9          123         4182          615      0.147059         123    0.0294118        246   0.0588235        123   0.0294118           0            0        615    0.147059         246    0.0588235          369     0.0882353                 34
       10          475        16150         2375      0.147059         475    0.0294118        950   0.0588235        475   0.0294118           0            0       2375    0.147059         950    0.0588235         1425     0.0882353                 34
       11           80         2720          400      0.147059          80    0.0294118        160   0.0588235         80   0.0294118           0            0        400    0.147059         160    0.0588235          240     0.0882353                 34
       12          169         5746          845      0.147059         169    0.0294118        338   0.0588235        169   0.0294118           0            0        845    0.147059         338    0.0588235          507     0.0882353                 34
       13          358        12172         1790      0.147059         358    0.0294118        716   0.0588235        358   0.0294118           0            0       1790    0.147059         716    0.0588235         1074     0.0882353                 34
       14          415        14110         2075      0.147059         415    0.0294118        830   0.0588235        415   0.0294118           0            0       2075    0.147059         830    0.0588235         1245     0.0882353                 34
       15          146         4964          730      0.147059         146    0.0294118        292   0.0588235        146   0.0294118           0            0        730    0.147059         292    0.0588235          438     0.0882353                 34
       16          275         9350         1375      0.147059         275    0.0294118        550   0.0588235        275   0.0294118           0            0       1375    0.147059         550    0.0588235          825     0.0882353                 34
       17          332        11288         1660      0.147059         332    0.0294118        664   0.0588235        332   0.0294118           0            0       1660    0.147059         664    0.0588235          996     0.0882353                 34
       18          172         5848          860      0.147059         172    0.0294118        344   0.0588235        172   0.0294118           0            0        860    0.147059         344    0.0588235          516     0.0882353                 34
       19          268         9112         1340      0.147059         268    0.0294118        536   0.0588235        268   0.0294118           0            0       1340    0.147059         536    0.0588235          804     0.0882353                 34
       20          740        25160         3700      0.147059         740    0.0294118       1480   0.0588235        740   0.0294118           0            0       3700    0.147059        1480    0.0588235         2220     0.0882353                 34
       21          301        10234         1505      0.147059         301    0.0294118        602   0.0588235        301   0.0294118           0            0       1505    0.147059         602    0.0588235          903     0.0882353                 34
       22          172         5848          860      0.147059         172    0.0294118        344   0.0588235        172   0.0294118           0            0        860    0.147059         344    0.0588235          516     0.0882353                 34
       23          223         7582         1115      0.147059         223    0.0294118        446   0.0588235        223   0.0294118           0            0       1115    0.147059         446    0.0588235          669     0.0882353                 34
       24          791        26894         3955      0.147059         791    0.0294118       1582   0.0588235        791   0.0294118           0            0       3955    0.147059        1582    0.0588235         2373     0.0882353                 34
       25          181         6154          905      0.147059         181    0.0294118        362   0.0588235        181   0.0294118           0            0        905    0.147059         362    0.0588235          543     0.0882353                 34
       26          357        12138         1785      0.147059         357    0.0294118        714   0.0588235        357   0.0294118           0            0       1785    0.147059         714    0.0588235         1071     0.0882353                 34
       27          253         8602         1265      0.147059         253    0.0294118        506   0.0588235        253   0.0294118           0            0       1265    0.147059         506    0.0588235          759     0.0882353                 34
       28           82         2788          410      0.147059          82    0.0294118        164   0.0588235         82   0.0294118           0            0        410    0.147059         164    0.0588235          246     0.0882353                 34
       29          213         7242         1065      0.147059         213    0.0294118        426   0.0588235        213   0.0294118           0            0       1065    0.147059         426    0.0588235          639     0.0882353                 34
       30          416        14144         2080      0.147059         416    0.0294118        832   0.0588235        416   0.0294118           0            0       2080    0.147059         832    0.0588235         1248     0.0882353                 34
       31          280         9520         1400      0.147059         280    0.0294118        560   0.0588235        280   0.0294118           0            0       1400    0.147059         560    0.0588235          840     0.0882353                 34
       32          306        10404         1530      0.147059         306    0.0294118        612   0.0588235        306   0.0294118           0            0       1530    0.147059         612    0.0588235          918     0.0882353                 34
       33          322        10948         1610      0.147059         322    0.0294118        644   0.0588235        322   0.0294118           0            0       1610    0.147059         644    0.0588235          966     0.0882353                 34
       34          312        10608         1560      0.147059         312    0.0294118        624   0.0588235        312   0.0294118           0            0       1560    0.147059         624    0.0588235          936     0.0882353                 34
</code></pre>
<p>I am using large English corpus. I can't see where I am going wrong...</p>
",Multilingual Language Processing & Language Identification,go wrong retrieving po proportion analysis spacy recently quite intent getting statistic regarding vocabulary book like nothing advanced thing like average length sentence many adjective used proportion total number word problem spacy tool chose sheer simplicity use seems indicate every text analyze seems exactly adjective compared total number word exactly auxiliary exactly noun exactly word per sentence pretty sure made mistake code using see sample code divided book chapter total printed table statistic number seem legit proportion way every chapter every text analyzing ha exactly structure output using large english corpus see going wrong
wordcloud for non-english corpus,"<p><a href=""https://i.sstatic.net/YdUCc.png"" rel=""noreferrer"">wordcloud for non English text</a></p>

<p>Dear friends
I am facing problems in generating proper wordcloud for non english text. The cloud is generated but it gives un-satisfactroy results. It shows wordcloud with characters only while I require wordcloud with proper words.
I processed following code to generate wordcloud.</p>

<pre><code>from os import path
from scipy.misc import imread
import matplotlib.pyplot as plt
import random
import unicodedata
from wordcloud import WordCloud, STOPWORDS
text = scorpus
wordcloud = WordCloud(font_path='MBKhursheed.ttf',
                      relative_scaling = 1.0,
                      stopwords = sw
                      ).generate(text)
plt.imshow(wordcloud)
plt.axis(""off"")
plt.show()
</code></pre>
",Multilingual Language Processing & Language Identification,wordcloud non english corpus wordcloud non english text dear friend facing problem generating proper wordcloud non english text cloud generated give un satisfactroy result show wordcloud character require wordcloud proper word processed following code generate wordcloud
Why special characters like () &quot;&quot; : [] are often removed from data before training translation machine?,"<p>I see that people often remove special characters like () &quot;&quot; : [] from data before training translation machine. Could you explain for me the benefits of doing so?</p>
",Multilingual Language Processing & Language Identification,special character like often removed data training translation machine see people often remove special character like data training translation machine could explain benefit
How to automatically detect code snippet from a text sample?,"<p>I'm doing some analysis on GitHub comments. But for that, I need to exclude the code samples and error messages from the comments automatically from a large set.</p>

<p>The other easier way to say this would be, I can keep only the English part of the comments. Although there are few libraries to detect the language of a sentence, there are few challenges in my case too.  1) the comment part does not always follow proper English grammar, 2) the code sample and error message mainly consist of English words too.</p>

<p>So what should be my best approach. The results don't need to be 100% accurate, I just want to know the best approach that can give me a satisfactory result at least. Any idea?</p>
",Multilingual Language Processing & Language Identification,automatically detect code snippet text sample analysis github comment need exclude code sample error message comment automatically large set easier way say would keep english part comment although library detect language sentence challenge case comment part doe always follow proper english grammar code sample error message mainly consist english word best approach result need accurate want know best approach give satisfactory result least idea
How to remove row completely when removing non-ascii characters?,"<p>I am using code below to remove all non english characters below:</p>
<pre><code>DF.text.replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
</code></pre>
<p>where df has a column called text with text in it like below:</p>
<pre><code>        text
hi what are you saying?
okay let me know
sounds great, mikey
ok.
right
„ÅîÊâøÁü•„ÅÆ„Å®„Åä„Çä„ÄÅÊÆãÂøµ„Å™„Åå„ÇâÊÇ™Ë≥™„Å™Ë©êÊ¨∫„ÅåÂ¢óÂä†„Åó„Å¶„ÅÑ„Çã„Çà„ÅÜ„Åß„Åô„ÅÆ„Åß„ÅäÊ∞ó„Çí‰ªò„Åë„Åè„Å†„Åï„ÅÑ„ÄÇ\n
¬°Hola miguel! Lamento mucho la confusi√≥n cau
</code></pre>
<p>expected output:</p>
<pre><code>          text
    hi what are you saying?
    okay let me know
    sounds great, mikey
    ok.
    right
</code></pre>
<p>For my rows where my code removes characters -</p>
<p>I want to delete those rows from the df completely, meaning if it does replace any non-english characters, I want to delete that row from the df completely to avoid having that row with either 0 characters or a few characters that are meaningless after they have been altered by the code above.</p>
",Multilingual Language Processing & Language Identification,remove row completely removing non ascii character using code remove non english character df ha column called text text like expected output row code remove character want delete row df completely meaning doe replace non english character want delete row df completely avoid row either character character meaningless altered code
How to use HolaVPN to resolve googletrans&#39;s JSON Decode error: line 1 column 1 (char 0),"<p>I have a vietnamese dataset of 18k rows that I'm trying to translate to English using googletrans module.</p>
<pre><code>from googletrans import Translator
translator = Translator()

def  trans_text(df, text_field):
    df[text_field] = df[text_field].apply(translator.translate, src='vi', dest='en').apply(getattr, args=('text',))  
    return df
trans_text(df_train.sample(1), &quot;question&quot;)


</code></pre>
<p>I end up with the following JSONDecode error:</p>
<pre><code>
JSONDecodeError                           Traceback (most recent call last)
&lt;ipython-input-21-d6791d78575e&gt; in &lt;module&gt;()
     24     df[text_field] = df[text_field].apply(translator.translate, src='vi', dest='en').apply(getattr, args=('text',))
     25     return df
---&gt; 26 trans_text(df_train.sample(1), &quot;question&quot;)
     27 
     28 

&lt;ipython-input-21-d6791d78575e&gt; in trans_text(df, text_field)
     22 
     23 def  trans_text(df, text_field):
---&gt; 24     df[text_field] = df[text_field].apply(translator.translate, src='vi', dest='en').apply(getattr, args=('text',))
     25     return df
     26 trans_text(df_train.sample(1), &quot;question&quot;)

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/site-packages/pandas/core/series.py in apply(self, func, convert_dtype, args, **kwds)
   4198             else:
   4199                 values = self.astype(object)._values
-&gt; 4200                 mapped = lib.map_infer(values, f, convert=convert_dtype)
   4201 
   4202         if len(mapped) and isinstance(mapped[0], Series):

pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/site-packages/pandas/core/series.py in f(x)
   4183 
   4184             def f(x):
-&gt; 4185                 return func(x, *args, **kwds)
   4186 
   4187         else:

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/site-packages/googletrans/client.py in translate(self, text, dest, src)
    170 
    171         origin = text
--&gt; 172         data = self._translate(text, dest, src)
    173 
    174         # this code will be updated when the format is changed.

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/site-packages/googletrans/client.py in _translate(self, text, dest, src)
     79         r = self.session.get(url, params=params)
     80 
---&gt; 81         data = utils.format_json(r.text)
     82         return data
     83 

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/site-packages/googletrans/utils.py in format_json(original)
     60         converted = json.loads(original)
     61     except ValueError:
---&gt; 62         converted = legacy_format_json(original)
     63 
     64     return converted

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/site-packages/googletrans/utils.py in legacy_format_json(original)
     52             text = text[:p] + states[j][1] + text[nxt:]
     53 
---&gt; 54     converted = json.loads(text)
     55     return converted
     56 

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/json/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    352             parse_int is None and parse_float is None and
    353             parse_constant is None and object_pairs_hook is None and not kw):
--&gt; 354         return _default_decoder.decode(s)
    355     if cls is None:
    356         cls = JSONDecoder

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/json/decoder.py in decode(self, s, _w)
    337 
    338         &quot;&quot;&quot;
--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    340         end = _w(s, end).end()
    341         if end != len(s):

/opt/anaconda3/envs/sam-pycaret/lib/python3.6/json/decoder.py in raw_decode(self, s, idx)
    355             obj, end = self.scan_once(s, idx)
    356         except StopIteration as err:
--&gt; 357             raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None
    358         return obj, end

JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></pre>
<p>I understand this arising due to a ban on my IP. I looked up ways to circumvent this and found that using VPN is worth the shot. I have HolaVPN installed already. However, being a newbie to all this, I'm not sure of how I could reproduce same steps as in the <a href=""https://stackoverflow.com/a/57025015/14332337"">solution</a> for hola instead. Any tips on a clear procedure to follow could greatly help. Thank you.</p>
",Multilingual Language Processing & Language Identification,use holavpn resolve googletrans json decode error line column char vietnamese dataset k row trying translate english using googletrans module end following jsondecode error understand arising due ban ip looked way circumvent found using vpn worth shot holavpn installed already however newbie sure could reproduce step href hola instead tip clear procedure follow could greatly help thank p
Setting NLTK with Stanford NLP (both StanfordNERTagger and StanfordPOSTagger) for Spanish,"<p>The <code>NLTK</code> documentation is rather poor in this integration. The steps I <a href=""https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#stanford-tagger-ner-tokenizer-and-parser"" rel=""noreferrer"">followed</a> were:</p>

<ul>
<li><p>Download <a href=""http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip"" rel=""noreferrer"">http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip</a> to <code>/home/me/stanford</code></p></li>
<li><p>Download <a href=""http://nlp.stanford.edu/software/stanford-spanish-corenlp-2015-01-08-models.jar"" rel=""noreferrer"">http://nlp.stanford.edu/software/stanford-spanish-corenlp-2015-01-08-models.jar</a> to <code>/home/me/stanford</code></p></li>
</ul>

<p>Then in a <code>ipython</code> console:</p>

<p>In [11]: import nltk                                                                                                                                                                 </p>

<pre><code>In [12]: nltk.__version__
Out[12]: '3.1'

In [13]: from nltk.tag import StanfordNERTagger
</code></pre>

<p>Then</p>

<pre><code>st = StanfordNERTagger('/home/me/stanford/stanford-postagger-full-2015-04-20.zip', '/home/me/stanford/stanford-spanish-corenlp-2015-01-08-models.jar')
</code></pre>

<p>But when I tried to run it:</p>

<pre><code>st.tag('Adolfo se la pasa corriendo'.split())
Error: no se ha encontrado o cargado la clase principal edu.stanford.nlp.ie.crf.CRFClassifier

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-14-0c1a96b480a6&gt; in &lt;module&gt;()
----&gt; 1 st.tag('Adolfo se la pasa corriendo'.split())

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/nltk/tag/stanford.py in tag(self, tokens)
     64     def tag(self, tokens):
     65         # This function should return list of tuple rather than list of list
---&gt; 66         return sum(self.tag_sents([tokens]), [])
     67 
     68     def tag_sents(self, sentences):

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/nltk/tag/stanford.py in tag_sents(self, sentences)
     87         # Run the tagger and get the output
     88         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,
---&gt; 89                                                        stdout=PIPE, stderr=PIPE)
     90         stanpos_output = stanpos_output.decode(encoding)
     91 

/home/nanounanue/.pyenv/versions/3.4.3/lib/python3.4/site-packages/nltk/__init__.py in java(cmd, classpath, stdin, stdout, stderr, blocking)
    132     if p.returncode != 0:
    133         print(_decode_stdoutdata(stderr))
--&gt; 134         raise OSError('Java command failed : ' + str(cmd))
    135 
    136     return (stdout, stderr)

OSError: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/home/nanounanue/Descargas/stanford-spanish-corenlp-2015-01-08-models.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/home/nanounanue/Descargas/stanford-postagger-full-2015-04-20.zip', '-textFile', '/tmp/tmp6y169div', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
</code></pre>

<p>The same occur with the <code>StandfordPOSTagger</code></p>

<p><strong>NOTE</strong>: I need that this will be the spanish version.
<strong>NOTE</strong>: I  am running this in <code>python 3.4.3</code></p>
",Multilingual Language Processing & Language Identification,setting nltk stanford nlp stanfordnertagger stanfordpostagger spanish documentation rather poor integration step followed download download console import nltk tried run occur note need spanish version note running
Dataset Language identification,"<p>I am working on a text classification problem with a multilingual dataset. I would like to know how the languages are distributed in my dataset and what languages are these. The number of languages might be approximately 8-12. I am considering this language detection as a part of the preprocessing. I would like to figure out the languages in order to be able to use the appropriate stop words and see how less data in some of the given languages could affect the occuracy of the classificatin. </p>

<p>Is langid.py or simple langdetect suitable? or any other suggestions?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,dataset language identification working text classification problem multilingual dataset would like know language distributed dataset language number language might approximately considering language detection part preprocessing would like figure language order able use appropriate stop word see le data given language could affect occuracy classificatin langid py simple langdetect suitable suggestion thanks
Multilingual free-text-items Text Classification for improving a recommender system,"<p>To improve the recomender system for Buyer Material Groups, our company is willing to train a model using customer historial spend data. The model should be trained on historical ""Short text descriptions"" to predict the appropriate BMG. The dataset has more that 500.000 rows and the text descriptions are multilingual (up to 40 characters).</p>

<p>1.Question: can i use supervised learning if i consider the fact that the descriptions are in multiple languages? If Yes, are classic approaches like multinomial naive bayes or SVM suitable?</p>

<p>2.Question: if i want to improve the first model in case it is not performing well, and use unsupervised multilingual emdedding to build a classifier. how can i train this classifier on the numerical labels later?</p>

<p>if you have other ideas or approaches please feel free :). (It is a matter of a simple text classification problem)</p>
",Multilingual Language Processing & Language Identification,multilingual free text item text classification improving recommender system improve recomender system buyer material group company willing train model using customer historial spend data model trained historical short text description predict appropriate bmg dataset ha row text description multilingual character question use supervised learning consider fact description multiple language yes classic approach like multinomial naive bayes svm suitable question want improve first model case performing well use unsupervised multilingual emdedding build classifier train classifier numerical label later idea approach please feel free matter simple text classification problem
How can I read english.pickle file from nltk module?,"<p>I am trying to figure out why I cant read the contents of the english.pickle file downloaded from nltk module.</p>
<p>I first downloaded the nltk file using this code:</p>
<pre><code>import nltk
nltk.download('punkt')
</code></pre>
<p>I then looked for inside the punkt file that I have on my home directory and found english.pickle file. I used the following code to read the file in python:</p>
<pre><code>import pickle
with open('english.pickle', 'rb') as file:
    x = pickle.load(file)
</code></pre>
<p>It all seemed fine, however, when I am running the variable x (which should be storing the pickled data) i am unable to retrieve the data from as I would from any other pickled file.</p>
<p>Instead I am only getting the object name and the id:</p>
<pre><code>&lt;nltk.tokenize.punkt.PunktParameters at 0x7f86cf6c0cd0&gt;
</code></pre>
<p>The problem is I need to access the content of the file and I cant iterate through as it is not iterable.</p>
<p>Has anyone encountered the same problem?</p>
",Multilingual Language Processing & Language Identification,read english pickle file nltk module trying figure cant read content english pickle file downloaded nltk module first downloaded nltk file using code looked inside punkt file home directory found english pickle file used following code read file python seemed fine however running variable x storing pickled data unable retrieve data would pickled file instead getting object name id problem need access content file cant iterate iterable ha anyone encountered problem
How to separate Japanese texts and English texts from a Pandas Dataframe?,"<p>I have a dataframe column which contains both English and Japanese texts. Like in the following manner:</p>
<pre><code>----IDs-------Texts ---------
    132   |  Âæ©ÊóßÂÆå‰∫Ü„ÄÇ„Çà„Çç„Åó„ÅèÈ†º„ÇÄÔºÅ 
    623   |  This is an English text 
    2364  |  &quot;&lt;@UD3JFBREV&gt; ÂèéÈõÜ„Åó„ÅüÊó•Êú¨Ë™û„ÅÆ„ÉÑ„Ç§„Éº„Éà„Éá„Éº„Çø„ÅØ„Å©„Åì„Å´„ÅÇ„Çä„Åæ„Åô„Åß„Åó„Çá„ÅÜ„Åã&quot; 
    ...   |  .....
</code></pre>
<p>Now, I want to separate English texts from Japanese texts from <code>Texts</code> column. My new dataframe should return only English texts ignoring the Japanese texts. How can I do it?</p>
",Multilingual Language Processing & Language Identification,separate japanese text english text panda dataframe dataframe column contains english japanese text like following manner want separate english text japanese text column new dataframe return english text ignoring japanese text
How to identify Spanish vs. English Text from a csv of tweets?,"<p>I'm trying to create a column that identifies English and Spanish tweets from a dataframe with several rows of tweets. Ideally tweets in English would be categorized as a 1 and those in Spanish would be marked as 0.</p>
<p>The end goal is to be able to filter out Spanish tweets from my dataframe to save English tweets in a new CSV. I looked at using Textblob, langdetect, and also fastText, but everything I've found gives instructions for just running the code on 1 string of text at a time.</p>
<p>Is there an easy way to categorize by language (English/Spanish) for a whole dataframe using Python?</p>
",Multilingual Language Processing & Language Identification,identify spanish v english text csv tweet trying create column identifies english spanish tweet dataframe several row tweet ideally tweet english would categorized spanish would marked end goal able filter spanish tweet dataframe save english tweet new csv looked using textblob langdetect also fasttext everything found give instruction running code string text time easy way categorize language english spanish whole dataframe using python
Encoding two different languages using pd.read_csv problem,"<p>I am building a Neural Machine Automatic Translator from German to Arabic. I am reading a CSV file containing German sentences and their corresponding Arabic translations. I want to read both languages at the same time using <code>pd.read_csv</code>. I have tried all the codes for all languages in this <a href=""https://docs.python.org/3/library/codecs.html#standard-encodings"" rel=""nofollow noreferrer"">Python documentation</a> but none of them worked.</p>
<p><a href=""https://i.sstatic.net/2JBdP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2JBdP.png"" alt=""Aliases encoding for all languages"" /></a></p>
<p>The only thing that worked best for me is this:</p>
<pre><code>df = pd.read_csv(&quot;DLATS.csv&quot;, encoding ='windows-1256')
</code></pre>
<p>'windows-1256' is the encoding Alias for the Arabic language. But the problem is that it doesn't catch the German special characters like (√§) but it converts them into question marks (?). So the word dr√§ngte became dr?ngte.</p>
<p>So, can anyone please help me to solve this problem or how to work around it? I have thought of separating the German and Arabic sentences in separate CSV files so that each CSV file contains one row only, and then maybe I will try to mix them in the Python code. But it seems that <code>pd.read_csv</code> requires at least two columns in the CSV file to work.</p>
<p>Update: I have noticed that the original csv file contains these problems as well for the German language. So, I have finally managed to solve my problem by reading excel directly instead of csv since the original file is in Excel, so I used pd.read_excel without any encoding attribute and it worked well. I didn't know before that pandas has pd.read_excel.</p>
",Multilingual Language Processing & Language Identification,encoding two different language using pd read csv problem building neural machine automatic translator german arabic reading csv file containing german sentence corresponding arabic translation want read language time using tried code language python documentation none worked thing worked best window encoding alias arabic language problem catch german special character like convert question mark word dr ngte became dr ngte anyone please help solve problem work around thought separating german arabic sentence separate csv file csv file contains one row maybe try mix python code seems requires least two column csv file work update noticed original csv file contains problem well german language finally managed solve problem reading excel directly instead csv since original file excel used pd read excel without encoding attribute worked well know panda ha pd read excel
Is it possible to get a list of words given the Lemma in Spacy?,"<p>I am trying to fix grammatical gender in French text and wanted to know if there is a way to get a list of all words from a certain lemma and if it possible to do a lookup in such list?</p>
",Multilingual Language Processing & Language Identification,possible get list word given lemma spacy trying fix grammatical gender french text wanted know way get list word certain lemma possible lookup list
How to retrieve value from a table in flask,"<p>This is a HTML page that generates an Arabic poem.</p>
<pre class=""lang-html prettyprint-override""><code>&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;&lt;/th&gt;
        &lt;td&gt;&lt;pre&gt;{{poemm}}&lt;/pre&gt;&lt;/td&gt;
        &lt;td&gt;&lt;pre&gt;{{poem}}&lt;/pre&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
</code></pre>
<p>I want to retrieve the generated poem from the table and use it in the prediction process but I didn't know-how. I searched and found out that I need to use the request form but I didn't know-how in a table.</p>
<p>This is my code for the prediction process:</p>
<pre><code>    elif request.form['action'] == 'Classify':
        train = pd.read_excel('/home/sirina/Desktop/Project/Corpus2/Two-versed Poems/corpusTwoColumnsTrain.xlsx')
        Quasida = [' '.join([x,y]) for x,y in zip(train.Sader, train.Ajez)]
        count_vect = CountVectorizer()
        X_train_counts = count_vect.fit_transform(Quasida)
        tfidf_transformer = TfidfTransformer()
        X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
        clf = LinearSVC().fit(X_train_tfidf, train.Era)
        #here i need the poem that i generated to predicted
        predicted=clf.predict(count_vect.transform(poem))
        return render_template('testt.html',poem=poem,prediction=predicted[0])
 return render_template('testt.html')
</code></pre>
<p>PS: I need to retrieve both poem and poemm from the table.</p>
",Multilingual Language Processing & Language Identification,retrieve value table flask html page generates arabic poem want retrieve generated poem table use prediction process know searched found need use request form know table code prediction process p need retrieve poem poemm table
Comparing sets in python nlp,"<p>I'm trying to teach myself python and natural language processing using an online tutorial</p>

<p><a href=""http://www.nltk.org/book/ch01.html#sec-automatic-natural-language-understanding"" rel=""nofollow"">http://www.nltk.org/book/ch01.html#sec-automatic-natural-language-understanding</a></p>

<p>At the end of each section they give practice questions, and for the first section I've gotten through all but one. This one really stumped me. </p>

<p>In nltk there is a function called set() that gives the set of all vocab in a list with all duplicate words removed. </p>

<p>We have been using sets to store vocabularies. Try the following Python expression: set(sent3) &lt; set(text1). Experiment with this using different arguments to set(). What does it do? Can you think of a practical application for this?</p>

<p>I've been running code with a few different arguments for set, but I just can't see a pattern in the output. Does anybody know what classifies one set as greater than another? And why this might be important?</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,comparing set python nlp trying teach python natural language processing using online tutorial end section give practice question first section gotten one one really stumped nltk function called set give set vocab list duplicate word removed using set store vocabulary try following python expression set sent set text experiment using different argument set doe think practical application running code different argument set see pattern output doe anybody know classifies one set greater another might important thanks
Longest Common Substring without cutting a word- python,"<p>Given the following, i can find the longest common substring:</p>

<pre><code>s1 = ""this is a foo bar sentence .""
s2 = ""what the foo bar blah blah black sheep is doing ?""

def longest_common_substring(s1, s2):
  m = [[0] * (1 + len(s2)) for i in xrange(1 + len(s1))]
  longest, x_longest = 0, 0
  for x in xrange(1, 1 + len(s1)):
    for y in xrange(1, 1 + len(s2)):
      if s1[x - 1] == s2[y - 1]:
        m[x][y] = m[x - 1][y - 1] + 1
        if m[x][y] &gt; longest:
          longest = m[x][y]
          x_longest = x
      else:
        m[x][y] = 0
  return s1[x_longest - longest: x_longest]

print longest_common_substring(s1, s2)
</code></pre>

<p>[out]:</p>

<pre><code>foo bar
</code></pre>

<p>But how do i ensure that the longest common substring respect English word boundary and don't cut up a word? For example, the following sentences:</p>

<pre><code>s1 = ""this is a foo bar sentence .""
s2 = ""what a kappa foo bar black sheep ?""
print longest_common_substring(s1, s2)
</code></pre>

<p>outputs the follow which is <strong>NOT</strong> desired since it breaks up the word <code>kappa</code> from s2:</p>

<pre><code>a foo bar
</code></pre>

<p>The desired output is still:</p>

<pre><code>foo bar
</code></pre>

<p>I've tried also an ngram way of getting the longest common substring respecting word boundary but <strong>is there other way that deals with strings without calculating ngrams</strong>? (see answer)</p>
",Multilingual Language Processing & Language Identification,longest common substring without cutting word python given following find longest common substring ensure longest common substring respect english word boundary cut word example following sentence output follow desired since break word desired output still tried also ngram way getting longest common substring respecting word boundary way deal string without calculating ngrams see answer
"Optical recognition of text and analysis of its structure (title, subtitle, text body)","<p>We wish to analyze scans of documents with text (non-handwritten) and images with very broad range of arrangements/structures in different languages. The first problem we try to solve, is extracting text and identifying and separating titles, subtitles and text bodies.</p>
<p>At the moment we are doing a literature research. There is plenty of literature about deep learning, computer vision, optical character recognition or natural language processing but none of these are actually focused on optical recognition of the structure of text.</p>
<p>We wonder, what is the name of the discipline/field that deals with optical recognition of structure of text?</p>
<p>What are the state-of-the-art approaches and tools for solving these problems?</p>
",Multilingual Language Processing & Language Identification,optical recognition text analysis structure title subtitle text body wish analyze scan document text non handwritten image broad range arrangement structure different language first problem try solve extracting text identifying separating title subtitle text body moment literature research plenty literature deep learning computer vision optical character recognition natural language processing none actually focused optical recognition structure text wonder name discipline field deal optical recognition structure text state art approach tool solving problem
How to make a spacy matcher pattern using verb tense/mood?,"<p>I've been trying to make a specific pattern for a spacy matcher using Verbs tenses and moods.<br />
I found out how to access morphological features of words parsed with spacy using model.vocab.morphology.tag_map[token.tag_], which prints out something like this when the verb is in subjunctive mode (the mode I am interested in):</p>
<p>{'Mood_sub': True, 'Number_sing': True, 'Person_three': True, 'Tense_pres': True, 'VerbForm_fin': True, 74: 100}</p>
<p>however, I would like to have a pattern like this one to retokenize specific verb phrases:
pattern = [{'TAG':'Mood_sub'}, {'TAG':'VerbForm_ger'}]</p>
<p>In the case of a spanish phrase like: 'Que siga aprendiendo', 'siga' has 'Mood_sub' = True in its tag, and 'aprendiendo' has 'VerbForm_ger' = True in its tag. However, the matcher is not detecting this match.</p>
<p>Can anyone tell me why this is and how I could fix it? This is the code I am using:</p>
<pre><code>model = spacy.load('es_core_news_md')
text = 'Que siga aprendiendo de sus alumnos'
doc = model(text)
pattern = [{'TAG':'Mood_sub'}, {'TAG':'VerbForm_ger'}] 
matcher.add(1, None, pattern)
matches = matcher(doc)
for i, start, end in matches:
    span = doc[start:end]
    if len(span) &gt; 0:
       with doc.retokenize() as retokenizer:
            retokenizer.merge(span)
</code></pre>
",Multilingual Language Processing & Language Identification,make spacy matcher pattern using verb tense mood trying make specific pattern spacy matcher using verb tense mood found access morphological feature word parsed spacy using model vocab morphology tag map token tag print something like verb subjunctive mode mode interested mood sub true number sing true person three true tense pres true verbform fin true however would like pattern like one retokenize specific verb phrase pattern tag mood sub tag verbform ger case spanish phrase like que siga aprendiendo siga ha mood sub true tag aprendiendo ha verbform ger true tag however matcher detecting match anyone tell could fix code using
String matching algorithms in python,"<p>I am looking for some suggestions on the algorithms which could be used for string matching which also supports non-english languages too.</p>
<p>Previously tried algorithm:</p>
<p>I have tried Levenshtein distance (Fuzzy matching) with token_sort_ratio algorithm. This algorithm works pretty well for most of my uses case and even for non-english languages. I considered two strings to be a match if the ratio is above 90%. The problem I am currently facing with this algorithm is, in the below example, 19th century and 18th century are not the same and I do not want them to be considered as a match.</p>
<pre><code>Str1 = &quot;19th Century&quot;
Str2 = &quot;18th Century&quot;
fuzz.token_sort_ratio(Str1,Str2)
&gt;&gt; 92%
</code></pre>
<p>If I change the ratio to be greater than 95% then, I would miss below example as a match. But these two strings are a match</p>
<pre><code>Str1 = &quot;Robert Jones&quot;
Str2 = &quot;Robert F. Jones&quot;
fuzz.token_sort_ratio(Str1,Str2)
&gt;&gt; 92%
</code></pre>
",Multilingual Language Processing & Language Identification,string matching algorithm python looking suggestion algorithm could used string matching also support non english language previously tried algorithm tried levenshtein distance fuzzy matching token sort ratio algorithm algorithm work pretty well us case even non english language considered two string match ratio problem currently facing algorithm example th century th century want considered match change ratio greater would miss example match two string match
What is the state of GPT-3 for text classification in spanish?,"<p>I would like to know if it is possible to reuse gpt-3 in a different language, Spanish in this case.</p>
<p>Do I need a gpt-3 model specifically trained with a Spanish corpus, or can I use transfer learning to produce Spanish text?</p>
",Multilingual Language Processing & Language Identification,state gpt text classification spanish would like know possible reuse gpt different language spanish case need gpt model specifically trained spanish corpus use transfer learning produce spanish text
Langid detect language showing wrong result in #Python,"<p>I need to detect the language of the strings stored below.</p>
<pre><code>0                                                          nice
1                                                       Insane3
2                                                           NaN
3                                                @bertelsen1986
4                       20 or 30 mm rise on the Renthal Fatbar?
Name: Comments, dtype: object
</code></pre>
<p>Using <code>langid</code> module to detect the language of the comments stored in <code>df['Comments']</code>:</p>
<pre><code>import langid
for row in df['Comments']:
  lang, log_prob = langid.classify(row)
  TM['Detected_Language']=lang
</code></pre>
<p>Below is the result, which is wrong:</p>
<pre><code>    Comments                                          Detected_Language
0   nice                                                      zh
1   Insane3                                                   zh
2   üòªüòª‚ù§Ô∏è                                                   zh
3   @bertelsen1986                                            zh
4   20 or 30 mm rise on the Renthal Fatbar?                   zh
</code></pre>
<p>The comments should return an 'en' instead.
(In the dataset, there are comments with other languages)</p>
",Multilingual Language Processing & Language Identification,langid detect language showing wrong result python need detect language string stored using module detect language comment stored result wrong comment return en instead dataset comment language
Detect language and translate string to English in Python,"<p>I would like to detect the language of strings within a pandas series and translate the strings from their detected languages to English.</p>
<pre><code>df['Comments'][:7]
0                                                        @bertelsen1986
1                               20 or 30 mm rise on the Renthal Fatbar?
2                                            Luckily I have one to üî•üí™üèª
3                                   @hatsofftojj isn‚Äôt this a dope bike
4                      My bike came with a old GX eagle I have big scam
5                                                                   NaN
6   Also ist ein Spulenschock jetzt ein Daumen hoch auf einem Spektral?
Name: Comments, dtype: object
</code></pre>
<p>Below is the code that is supposed to detect the language of the strings.
For only 22,000 rows, the code has run for almost an entire day (still running).</p>
<pre><code>from langdetect import detect
lang = []    
for row in df.Comments:
    lang = detect(r)
    df['Comments'] = str(df['Comments'])
    df['Language'] = df['Comments'].apply(detect)
</code></pre>
",Multilingual Language Processing & Language Identification,detect language translate string english python would like detect language string within panda series translate string detected language english code supposed detect language string row code ha run almost entire day still running
NLP: retrieve vocabulary from text,"<p>I have some texts in different languages and, potentially, with some typo or other mistake, and I want to retrieve their own vocabulary. I'm not experienced with NLP in general, so maybe I use some word improperly.</p>

<p>With <strong>vocabulary</strong> I mean a collection of words of a single language in which every word is unique and the inflections for gender, number, or tense are not considered (e.g. <em>think</em>, <em>thinks</em> and <em>thought</em> are are all consider <em>think</em>).</p>

<p>This is the master problem, so let's reduce it to the vocabulary retrieving of one language, English for example, and without mistakes.</p>

<p>I think there are (at least) three different approaches and maybe the solution consists of a combination of them:</p>

<ul>
<li><strong>search in a database</strong> of words stored in relation with each others. So, I could search for <em>thought</em> (considering the verb) and read the associated information that <em>thought</em> is an inflection of <em>think</em></li>
<li><strong>compute the ""base form""</strong> (a word without inflections) of a word by processing the inflected form. Maybe it can be done with stemming?</li>
<li><strong>use a service by any API</strong>. Yes, I accept also this approach, but I'd prefer to do it locally</li>
</ul>

<p>For a first approximation, it's not necessary that the algorithm distinguishes between nouns and verbs. For instance, if in the text there were the word <em>thought</em> like both noun and verb, it could be considered already present in the vocabulary at the second match.</p>

<p>We have reduced the problem to retrieve a vocabulary of an English text without mistakes, and without consider the tag of the words.</p>

<p>Any ideas about how to do that? Or just some tips?</p>

<p>Of course, if you have suggestions about this problem also with the others constraints (mistakes and multi-language, not only Indo-European languages), they would be much appreciated.</p>
",Multilingual Language Processing & Language Identification,nlp retrieve vocabulary text text different language potentially typo mistake want retrieve vocabulary experienced nlp general maybe use word improperly vocabulary mean collection word single language every word unique inflection gender number tense considered e g think think thought consider think master problem let reduce vocabulary retrieving one language english example without mistake think least three different approach maybe solution consists combination search database word stored relation others could search thought considering verb read associated information thought inflection think compute base form word without inflection word processing inflected form maybe done stemming use service api yes accept also approach prefer locally first approximation necessary algorithm distinguishes noun verb instance text word thought like noun verb could considered already present vocabulary second match reduced problem retrieve vocabulary english text without mistake without consider tag word idea tip course suggestion problem also others constraint mistake multi language indo european language would much appreciated
What are the major differences and benefits of Porter and Lancaster Stemming algorithms?,"<p>I'm Working on document classification tasks in java.</p>

<p>Both algorithms came highly recommended, what are the benefits and disadvantages of each and which is more commonly used in the literature for Natural Language Processing tasks? </p>
",Multilingual Language Processing & Language Identification,major difference benefit porter lancaster stemming algorithm working document classification task java algorithm came highly recommended benefit disadvantage commonly used literature natural language processing task
(TF-IDF)How to return the five related article after calculating cosine similarity,"<p>I get a dataframe <em>sample_df</em>(4 columns: <em>paper_id</em>,<em>title</em>,<em>abstract</em>,<em>body_text</em>). I extracted the abstract column(~1000 words per abstract) and apply the text cleaning process. Here's my question:</p>
<p>After finished calculating the cosine similarity between question and abstract, how can it return the top5 articles score with corresponding information(e.g. <em>paper_id</em>,<em>title</em>,<em>body_text</em>) since my goal is to do tf -idf question answering.</p>
<p>I'm really sorry that my english is poor and I am new to nlp. I would appreciated if someone can help.</p>
<pre><code>from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity  

txt_cleaned = get_cleaned_text(sample_df,sample_df['abstract'])
question = ['Can covid19 transmit through air']

tfidf_vector = TfidfVectorizer()

tfidf = tfidf_vector.fit_transform(txt_cleaned)

tfidf_question = tfidf_vector.transform(question)
cosine_similarities = linear_kernel(tfidf_question,tfidf).flatten()

related_docs_indices = cosine_similarities.argsort()[:-5:-1]
cosine_similarities[related_docs_indices]

#output([0.18986527, 0.18339485, 0.14951123, 0.13441914]) 
</code></pre>
",Multilingual Language Processing & Language Identification,tf idf return five related article calculating cosine similarity get dataframe sample df column paper id title abstract body text extracted abstract column word per abstract apply text cleaning process question finished calculating cosine similarity question abstract return top article score corresponding information e g paper id title body text since goal tf idf question answering really sorry english poor new nlp would appreciated someone help
"algorithm to detect time, date and place from invitation text","<p>I am researching some Natural Language Processing algorithms to read a piece of text, and if the text seems to be trying to suggest a meeting request, it sets up that meeting for you automatically.</p>

<p>For example, if an email text reads: </p>

<blockquote>
  <p>Let's <strong>meet tomorrow</strong> someplace in <strong>Downtown at 7pm</strong>"". </p>
</blockquote>

<p>The algorithm should be able to detect the Time, date and place of the event.</p>

<p>Does someone know of some already existing NLP algorithms that I could use for this purpose? I have been researching some NLP resources (like <a href=""http://nltk.org"" rel=""noreferrer"">NLTK</a> and <a href=""http://cran.r-project.org/web/views/NaturalLanguageProcessing.html"" rel=""noreferrer"">some tools in R</a>), but did not have much success.</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,algorithm detect time date place invitation text researching natural language processing algorithm read piece text text seems trying suggest meeting request set meeting automatically example email text read let meet tomorrow someplace downtown pm algorithm able detect time date place event doe someone know already existing nlp algorithm could use purpose researching nlp resource like nltk tool r much success thanks
Tokenizer in moses-SMT system stuck even with 10 sentences,"<p>I was trying to make a baseline MT system. Just for checking How it works I made Source (S) and Target (T) language corpus of just 2000 sentences. The very first step is to prepare the data for Machine Translation (MT) system. In this step first we have to perform tokenization as mentioned here <a href=""http://www.statmt.org/moses/?n=moses.baseline"" rel=""nofollow"">Baseline SMT</a>. I've used this code:</p>

<pre><code>~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en \
&lt; ~/corpus/training/news-commentary-v8.fr-en.en    \
&gt; ~/corpus/news-commentary-v8.fr-en.tok.en
~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr \
&lt; ~/corpus/training/news-commentary-v8.fr-en.fr    \
&gt; ~/corpus/news-commentary-v8.fr-en.tok.fr
</code></pre>

<p>( say S = French &amp; T = English)</p>

<p>I checked after 2 hours it was still running. I got curious since it was not expected. Then I tried with just ten sentences. To my surprise, it's been 30 minutes and it is still running.</p>

<p>Did I do anything wrong?</p>

<p>PS: OS = Ubuntu 14.04.5 LTS
    <a href=""http://www.sony.co.in/support/product/SVT13125CN"" rel=""nofollow"">Sony ultrabook</a>
    No dual boot.</p>
",Multilingual Language Processing & Language Identification,tokenizer moses smt system stuck even sentence wa trying make baseline mt system checking work made source target language corpus sentence first step prepare data machine translation mt system step first perform tokenization mentioned baseline smt used code say french english checked hour wa still running got curious since wa expected tried ten sentence surprise minute still running anything wrong p ubuntu lts sony ultrabook dual boot
Use http.client to login to the online Farasa lemmatizer and lemmatize arabic text file content,"<p>I am trying to use the code for Farasa lemmatizer into my code to lemmatize text files or csv files  that contain Arabic language. here is the link to the code <a href=""http://alt.qcri.org/farasa/"" rel=""nofollow noreferrer"">http://alt.qcri.org/farasa/</a></p>

<p>I tried to just pass a text for the <strong>payload</strong> variable and it works.
My question  can I pass file  to the code on the website of farasa to start the process of lemmatization. I am trying to understand what is (payload) but I couldnot</p>

<p>Here is what I attempted</p>

<pre><code>import http.client
from IPython.core import payload

conn = http.client.HTTPSConnection(""farasa-api.qcri.org"") 
payload = ""{\""text\"": \""Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿ±ÿ≠ŸÑŸàÿß Ÿà ŸäÿπŸàÿØŸàÿß ÿ•ŸÑŸâ ÿßŸÑŸàÿ∑ŸÜ Ÿáÿ∞ÿß ŸÖÿ´ÿßŸÑ ÿ®ÿ≥Ÿäÿ∑\""}"".encode(""utf-8"")

#the below line is not working 
#payload = ""{\""file\"":\""F:/AIenv/textAnalysis/testin2.txt\""}"".encode(""utf-8"")

headers = { ""content-type"": ""application/json"", ""cache-control"": ""no-cache"", }

conn.request(""POST"", ""/msa/webapi/lemma"", payload, headers)

# conn.request(""POST"", ""/msa/webapi/lemma"", files = files, headers=headers)
res = conn.getresponse()

data = res.read()

print(data.decode(""utf-8""))
</code></pre>

<p>does anyone have any idea about this problem and how can be fixed ?</p>
",Multilingual Language Processing & Language Identification,use login online farasa lemmatizer lemmatize arabic text file content trying use code farasa lemmatizer code lemmatize text file csv file contain arabic language link code tried pas text payload variable work question pas file code website farasa start process lemmatization trying understand payload couldnot attempted doe anyone idea problem fixed
Flutter with python backend,"<p>i have a website python-based that consists of a form that i want to fill from flutter.
this form takes one input text field (get request )
when submit is pressed, it will generate an output based on the python script ( it is an natural language processing)
is it possible to connect to this website using flutter and fill the form ?
i have tried using 'HTTP' package in flutter but i was not able to connect to the form
any help will be appreciated
i tried the following:</p>
<pre><code>    Future&lt;String&gt; sendQuery(query) async{
  final http.Response response = await http.get(
  'http://127.0.0.1:5000/getQuery/',
    headers: &lt;String, String&gt;{
      'Content-Type': 'application/json; charset=UTF-8',
    },
  );
}
</code></pre>
",Multilingual Language Processing & Language Identification,flutter python backend website python based consists form want fill flutter form take one input text field get request submit pressed generate output based python script natural language processing possible connect website using flutter fill form tried using http package flutter wa able connect form help appreciated tried following
How to extract all noun phrases in French Sentences with Spacy(Python),"<p>I am trying to extract all the noun phrases from French sentences using Spacy. My code appears not to be working well in all the cases I tried. For example, </p>

<pre><code>    nlp = spacy.load(""fr_core_news_sm"")
    doc = nlp(""Il y a plusieurs petits restaurants dans cette ville."")
    for chunk in doc.noun_chunks:
      print(chunk)
</code></pre>

<p>returns </p>

<p><code>[Il y a plusieurs petits restaurants dans cette ville.]</code> as the noun phrase, this appears to be incorrect as the noun phrase here is <code>petits restaurants dans cette ville</code>.</p>

<p>When I tried other sets of phrases, such as <code>J'ai trouv√© une jolie petite chambre.</code>, it returned 3 phrases, <code>[J' , une jolie, petite chambre]</code> which seems not to be correct also</p>

<p>Lastly, with <code>Les deux derni√®res semaines, il √©tait √† Paris..</code> it returned <code>[Les deux derni√®res semaines, il]</code> which appears to be correct. </p>

<p>I would appreciate any help or guidance on how to ensure the code works correctly for the first two examples also. </p>
",Multilingual Language Processing & Language Identification,extract noun phrase french sentence spacy python trying extract noun phrase french sentence using spacy code appears working well case tried example return noun phrase appears incorrect noun phrase tried set phrase returned phrase seems correct also lastly returned appears correct would appreciate help guidance ensure code work correctly first two example also
uploading arabic files in django not working returning codes,"<p>i have a view in my django project that should be able to read the content of an uploaded .txt file from the input type=&quot;file&quot;, but the thing is that with arabic content it doesn't print the actual text, but a series of codes &quot;\xd9\x88\xd9\x82\xd8\xa7\xd9\x84&quot; and i couldn't find any solution for this since the file is perfectly viewable on my pc and my website it the one exporting that file in &quot;utf-8&quot;. any help here ?</p>
<pre><code>Uploaded_File = request.FILES[&quot;Doc&quot;]
for chunk in Uploaded_File.chunks(chunk_size=None):
    print(chunk)
</code></pre>
",Multilingual Language Processing & Language Identification,uploading arabic file django working returning code view django project able read content uploaded txt file input type file thing arabic content print actual text series code xd x xd x xd xa xd x find solution since file perfectly viewable pc website one exporting file utf help
How to find the vocabulary size of a spaCy model?,"<p>I am trying to find the vocabulary size of the large English model, i.e. <code>en_core_web_lg</code>, and I find three different sources of information:</p>
<ul>
<li><p>spaCy's docs: 685k keys, 685k unique vectors</p>
</li>
<li><p><code>nlp.vocab.__len__()</code>: 1340242 # (number of lexemes)</p>
</li>
<li><p><code>len(vocab.strings)</code>: 1476045</p>
</li>
</ul>
<p>What is the difference between the three? I have not been able to find the answer in the docs.</p>
",Multilingual Language Processing & Language Identification,find vocabulary size spacy model trying find vocabulary size large english model e find three different source information spacy doc k key k unique vector number lexeme difference three able find answer doc
Elasticsearch language Analyzers - return the retrieved field after text analysis,"<ol>
<li><p>i am working on full-text search engine in Elasticsearch and using multilingual data in index time. i used elasticsearch for text analysis and i would like to be able to return the tokens (retrieved index) after the preprocessing. I know about Analyze API but doing this for +200.000 documents is very time consuming. I found &quot;terms aggregation&quot; but i am not sure how it works. Any ideas?</p>
</li>
<li><p>i used in the mapping language analyzers. Is there any out-of-the-box language detection when using language analyzers or every document is passing by every language analyzer? If so, does it make sense to work with language detection and create multifields for each language? What is the different between using language analyzers in settings or in mappings?</p>
</li>
</ol>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>PUT /index_sample
{
  ""settings"": {
    ""analysis"" : {
      ""analyzer"" : {
        ""my_analyzer"" : {
          ""type"" : ""custom"",
          ""tokenizer"" : ""standard"",
          ""filter"" : [
            ""my_asciifolding"",
            ""my_apostrophe"",
            ""cjk_bigram""]
        }
      },
      ""filter"" : {
        ""my_asciifolding"" : {
          ""type"" : ""asciifolding"",
          ""preserve_original"" : true
        },
        ""my_apostrophe"" :{
        ""type"" : ""apostrophe""
        }
      }
    }
  },
  ""mappings"" : {
    ""properties"": {
      ""category_number"" : {
        ""type"" : ""integer"",
        ""fields"" : {
          ""raw"" : {
          ""type"" : ""keyword""
          }
        }
      },
      ""product"": {
        ""type"" : ""text"",
        ""index"" : ""true"",
        ""store"" : ""true"",
        ""analyzer"" : ""my_analyzer"",
        ""fields"" : {
          ""german_field"": {
            ""type"" : ""text"",
            ""analyzer"": ""german""
          },
          ""english_field"" : {
            ""type"" : ""text"",
            ""analyzer"" : ""english""
          },
          ""chinese_field"" : {
            ""type"" : ""text"",
            ""analyzer"" : ""smartcn""
          },
          ""spanish_field"": {
            ""type"" : ""text"",
            ""analyzer"" : ""spanish""
          },
          ""czech_analyer"" : {
            ""type"" : ""text"",
            ""analyzer"" : ""czech""
          },
          ""french_field"": {
            ""type"" : ""text"",
            ""analyzer"" : ""french""
          },
          ""italian_field"" : {
            ""type"" : ""text"",
            ""analyzer"" : ""italian""
          },
          ""dutch_field"": {
            ""type"" : ""text"",
            ""analyzer"" : ""dutch""
          },
          ""portuguese_field"": {
            ""type"" : ""text"",
            ""analyzer"" : ""portuguese""
          }
        }  
      }
    }
  }
}</code></pre>
</div>
</div>
</p>
",Multilingual Language Processing & Language Identification,elasticsearch language analyzer return retrieved field text analysis working full text search engine elasticsearch using multilingual data index time used elasticsearch text analysis would like able return token retrieved index preprocessing know analyze api document time consuming found term aggregation sure work idea used mapping language analyzer box language detection using language analyzer every document passing every language analyzer doe make sense work language detection create multifields language different using language analyzer setting mapping
Pandas csv not reading Arabic characters,"<p>I have a csv file with mixed columns of numbers and Arabic text. I am trying to open it using pandas csv reader. I tried using the following.</p>
<pre><code>Text= pd.read_csv('text.csv', encoding ='utf-8-sig')
</code></pre>
<p>However, after reading the file, when I am looking at the columns, I am getting instead of Arabic text a bunch of exclamation marks for each row.
what could be the problem?</p>
<p>thanks</p>
",Multilingual Language Processing & Language Identification,panda csv reading arabic character csv file mixed column number arabic text trying open using panda csv reader tried using following however reading file looking column getting instead arabic text bunch exclamation mark row could problem thanks
Best way to detect person name / entity name from French text,"<p>I am trying to detect person name or company/institute name from French texts. I have tried the following and the results are not good at all.</p>
<pre><code>import spacy

# or any of the other two models fr_core_news_sm, fr_core_news_lg
nlp = spacy.load(&quot;fr_core_news_md&quot;)  

text =&quot;&quot;&quot;Tous les vents,
Balayent les mots de coeur
Moi, j'suis comme le vent:
L'esprit √† mille √† l'heure,
Je juge sans doute trop vite
C'est ok, tant pis
C'est juste l√†, je m'agite
Je grandis, l'amour aussi
C'est au gr√© du vent
Que j'aime vagabonder,
Moi, je suis comme le vent
J'embrasse toute une arm√©e
De r√™ves et de bleuets,
Me plonger dedans
Je sais ce que je sais 
Rapport de: Andre STE-GERMAINE,
&quot;&quot;&quot;

doc_fr = nlp_fr(text_fr)

</code></pre>
<p>Well, it marked</p>
<ol>
<li>&quot;Balayent as PERSON which should not,</li>
<li>Rapport de: Andre STE-GERMAINE', 'MISC' which should be PERSON</li>
</ol>
<p>I am not sure if NLTK can help on this.</p>
<p>Is there another better tool to help with this task?</p>
<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,best way detect person name entity name french text trying detect person name company institute name french text tried following result good well marked balayent person rapport de andre ste germaine misc person sure nltk help another better tool help task thanks
Where is English training/dev data for spaCy&#39;s pretrained dependency parser?,"<p>Does spaCy publicly release the dev/test data for their pretrained dependency parser (en_core_web_sm/md/lg), and if so where can it be accessed?</p>
<p>For context: I'm trying to measure the pretrained parser's accuracy <em>on a particular kind of sentence</em> (not just an entire treebank like Penn Treebank), and so I need gold labeled sentences from the dev set or test set so that I can identify the type of sentence and evaluate. I would use another dependency treebank like a Universal Dependencies treebank, but unfortunately the annotation schemes are slightly different.</p>
",Multilingual Language Processing & Language Identification,english training dev data spacy pretrained dependency parser doe spacy publicly release dev test data pretrained dependency parser en core web sm md lg accessed context trying measure pretrained parser accuracy particular kind sentence entire treebank like penn treebank need gold labeled sentence dev set test set identify type sentence evaluate would use another dependency treebank like universal dependency treebank unfortunately annotation scheme slightly different
Detecting whether or not text is English (in bulk),"<p>I'm looking for a simple way to detect whether a short excerpt of text, a few sentences, is English or not. Seems to me that this problem is much easier than trying to detect an arbitrary language. Is there any software out there that can do this? I'm writing in python, and would prefer a python library, but something else would be fine too. I've tried google, but then realized the TOS didn't allow automated queries.</p>
",Multilingual Language Processing & Language Identification,detecting whether text english bulk looking simple way detect whether short excerpt text sentence english seems problem much easier trying detect arbitrary language software writing python would prefer python library something else would fine tried google realized tos allow automated query
Using natural language processing to extract an address from a tweet,"<p>I'm building a twitter bot that will listen for tweets like the following:</p>

<pre><code>Hey @twitterbot, I'm looking for restaurants around 123 Main Street, New York
</code></pre>

<p>or, another example:</p>

<pre><code>@twitterbot, what's near Yonge &amp; Dundas, Toronto? I'm hungry!
</code></pre>

<p>It'll then reply with the kind of data you'd expect these questions to return. I've got most of the problem solved, but I'm stuck on something that shouldn't be so hard; extracting the address from the tweet.</p>

<p>I'll be forwarding the address to a geocoding service to get lat/lng, so I don't need to format or prepare the address in any way; I just need to isolate it from unrelated text like ""I'm looking for restaurants around"" or ""I'm hungry!"".</p>

<p>Are there any NLP tools that will perform this address-identification within a block of text? Any suggestions for another way to go about it? Because Google's geocoder handles such a wide array of address formats (even a point of interest like 'The eaton centre, Toronto' counts as an address), I can't use regex to pluck the address out.</p>

<p>Phrased another way, I just want to remove any text that is not part of an address.</p>

<p>The addresses I'm looking for need to work for US/Canada.</p>

<p>There are some similar questions on StackOverflow but none that tackle this exact problem that I could find. Because Google's geocoder is so forgiving, the solution doesn't have to be perfect, it just needs to get rid of enough of the fuzz so that Google knows what I'm trying to say.</p>

<p>I'm very new to NLP so I'd appreciate any guidance on the subject.</p>
",Multilingual Language Processing & Language Identification,using natural language processing extract address tweet building twitter bot listen tweet like following another example reply kind data expect question return got problem solved stuck something hard extracting address tweet forwarding address geocoding service get lat lng need format prepare address way need isolate unrelated text like looking restaurant around hungry nlp tool perform address identification within block text suggestion another way go google geocoder handle wide array address format even point interest like eaton centre toronto count address use regex pluck address phrased another way want remove text part address address looking need work u canada similar question stackoverflow none tackle exact problem could find google geocoder forgiving solution perfect need get rid enough fuzz google know trying say new nlp appreciate guidance subject
Dimension error with RNN and word classification,"<p>I'm pretty new with NLP and I want to classify different words depending on their language (basically my model should tell me if a word is french, or english, or spanish and so on).</p>
<p>When I fit the following model I get a dimension error. The &quot;dataset&quot; contains the words, it's a padded tensor of size (1550, 19) and the &quot;y&quot; contains the different languages, it's also a padded tensor of size (1550, 10).</p>
<pre><code>np.random.seed(42)
tf.random.set_seed(42)

from tensorflow.keras.layers import LSTM, GRU, Input, Embedding, Dense

input = Input(shape=[None])
z = Embedding(max_id + 1, 128, input_shape=[None], mask_zero=True)(input)
z = GRU(128)(z)
output = Dense(18, activation='softmax')(z)

model = keras.models.Model(input, output)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

h = model.fit(dataset, y, epochs=5)
</code></pre>
<p><strong>ValueError: Shapes (None, 10) and (None, 18) are incompatible</strong></p>
<p>Do you see where the problem is?</p>
<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,dimension error rnn word classification pretty new nlp want classify different word depending language basically model tell word french english spanish fit following model get dimension error dataset contains word padded tensor size contains different language also padded tensor size valueerror shape none none incompatible see problem thanks
R: Select option by introducing a number,"<p>I would like to build a very simple, sketchy, function where you can introduce a word (in Spanish) and get the most feasible next word (kind of a text predictor). So when the user introduces <em>me</em>, suggestions are <em>ha</em>, <em>he</em>, <em>lo</em>, <em>da</em> and <em>gusta</em>. Now, I would like to enable the user to select one of those five suggestions by typing 1, 2, 3, 4, or 5. Is there any way to do this in R?</p>
<pre><code>predictor&lt;-function(){
  word&lt;-readLines(stdin(),n=1)
  words&lt;-sort(table[first==word],decreasing=TRUE)[2:6]
  suggestions&lt;-gsub(&quot;.* &quot;,&quot;&quot;,names(words))
  return(suggestions)
}

#table = table with unigram and bigram frequencies of a corpus
#first = vector with the first word in each bigram

&gt; predictor()
me
[1] &quot;ha&quot;    &quot;he&quot;    &quot;lo&quot;    &quot;da&quot;    &quot;gusta&quot;
</code></pre>
",Multilingual Language Processing & Language Identification,r select option introducing number would like build simple sketchy function introduce word spanish get feasible next word kind text predictor user introduces suggestion ha lo da gusta would like enable user select one five suggestion typing way r
Calculate the percentage of accuracy with which user made the assigned sound,"<p>I want to design a web-app for my cousin who is 2 years of age in which i have implemented a functionality in which when an image is clicked some sound gets played and the user has to make the same sound which gets recorded.<br><br>
For eg-If i click on image of &quot;Apple&quot; the sound made is &quot;A for Apple&quot;.Now the user has to say those words which get recorded.<br>
Now I want to <strong>calculate the percentage of accuracy with which the user spoke</strong>.I want to know how can i know the accuracy percentage.I have not used machine learning or Natural Language Processing earlier so i want some guidance on what should i learn about or ways of implementing this functionality.I need some help on that.<br><br>
Also use nodejs frameworks quite frequently so is there any module in nodejs with the help of which the above requirement can be fulfilled.</p>
",Multilingual Language Processing & Language Identification,calculate percentage accuracy user made assigned sound want design web app cousin year age implemented functionality image clicked sound get played user ha make sound get recorded eg click image apple sound made apple user ha say word get recorded want calculate percentage accuracy user spoke want know know accuracy percentage used machine learning natural language processing earlier want guidance learn way implementing functionality need help also use nodejs framework quite frequently module nodejs help requirement fulfilled
Chatbot training platform that supports Arabic,"<p>I am looking for a <strong>chatbot training platform</strong> (like IBM Watson Conversation Service or api.ai) that has <strong>Arabic support</strong>.</p>

<p>I know wit.ai has a support for Arabic but reading their blogpost it seems like it's in Beta. Moreover, it has been suggested that wit.ai is suited towards to hobbyists and is not advisable to build complex conversational bots with it.</p>

<p>So I want to know whether there exists any service for chatbot training (<strong>except wit.ai</strong>) that has Arabic support ?</p>
",Multilingual Language Processing & Language Identification,chatbot training platform support arabic looking chatbot training platform like ibm watson conversation service api ai ha arabic support know wit ai ha support arabic reading blogpost seems like beta moreover ha suggested wit ai suited towards hobbyist advisable build complex conversational bot want know whether exists service chatbot training except wit ai ha arabic support
Splitting Text information in a dataframe into single words and detect if they are part of a dictionary R,"<p>I am trying write a script to detect if one word of an undefined amount of words is part of a dictionary.</p>
<p>To make this problem a bit more understandable, I have the following data:</p>
<pre><code>Items | Descriptions    | 
-------------------------
Item1 | poster           
Item2 | used cd music etc
Item3 | hckd herbal ingds.
Item4 | 823942 blc
</code></pre>
<p>So what I want to do know, is to check the column descriptions if any of those single words is part of a dictionary or self created vector of strings.
So the result should look something like:</p>
<pre><code>Items | Descriptions       | inDictionary
--------------------------------------------------
Item1 | poster             | TRUE
Item2 | used cd music etc  | TRUE
Item3 | hckd herbal ingds. | TRUE
Item4 | 823942 blc         | FALSE
</code></pre>
<p>For this example I just assume a english dictionary. In this specific case its sufficient if only one word is part of a dictionary.</p>
<p>I already tried this with the qdapDictionaries library and tokenizers to tokenize the contents of the dataframe cells but I fail to get the check right for cells where I have more than one word.</p>
<p>Help is much appreciated,</p>
<p>Thank you!</p>
",Multilingual Language Processing & Language Identification,splitting text information dataframe single word detect part dictionary r trying write script detect one word undefined amount word part dictionary make problem bit understandable following data want know check column description single word part dictionary self created vector string result look something like example assume english dictionary specific case sufficient one word part dictionary already tried qdapdictionaries library tokenizers tokenize content dataframe cell fail get check right cell one word help much appreciated thank
Meaning of text annotation in NLP context,"<p>In Natural Language Processing, what does it mean to annotate a corpus?<br>
Does it simply mean to add labels to text (i.e. &quot;positive, negative &amp; neutral&quot; classes in a sentiment analysis task)? Or there is more to its meaning/definition?</p>
",Multilingual Language Processing & Language Identification,meaning text annotation nlp context natural language processing doe mean annotate corpus doe simply mean add label text e positive negative neutral class sentiment analysis task meaning definition
Is there a way to use natural language processing to determine whether the phrase shows agreement or not?,"<p>I am doing some research related to fake news on twitter. I have isolated timelines of specific fake news stories in python, and I wanted to find out if there was a way to determine if each the text in each tweet was agreeing with the story or refuting it, thus giving me two seperate categories that I could compare over time. I know I could look for key words like 'fake' or 'false' to help classify, however I was hoping there was a more thorough way of doing it.</p>
<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,way use natural language processing determine whether phrase show agreement research related fake news twitter isolated timeline specific fake news story python wanted find wa way determine text tweet wa agreeing story refuting thus giving two seperate category could compare time know could look key word like fake false help classify however wa hoping wa thorough way thanks
is it worth it to release a single language model of Google&#39;s BERT for Italian?,"<p>I am currently working on my thesis which is related to automated question answering using a translation of Stanford's SQUAD data set in Italian. I am going to use Google's BERT <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a> as it is giving the best results in the SQUAD challenge by now. Google provided a multilingual pre-trained model for many languages including Italian. </p>

<p>is it worth it to release a single language model of Google's BERT only for Italian? my assumption is a single language model means a smaller network means less training time and smaller in size. </p>
",Multilingual Language Processing & Language Identification,worth release single language model google bert italian currently working thesis related automated question answering using translation stanford squad data set italian going use google bert giving best result squad challenge google provided multilingual pre trained model many language including italian worth release single language model google bert italian assumption single language model mean smaller network mean le training time smaller size
How to split Chinese words and English words in a string using python?,"<p>For example, I have some strings look like:</p>
<p><code>'‰∫∫ÂùáÊî∂ÂÖ• Per capital Income'</code>,</p>
<p><code>'ÊÄª‰∫ßÂÄº Gross Output Value'</code>.</p>
<p>I want to split them into</p>
<p><code>'‰∫∫ÂùáÊî∂ÂÖ•' 'Per capital Income'</code></p>
<p><code>'ÊÄª‰∫ßÂÄº' 'Gross Output Value'</code></p>
<p>Chinese characters are always before English words.</p>
",Multilingual Language Processing & Language Identification,split chinese word english word string using python example string look like want split chinese character always english word
Where to find resource of Japanese - Chinese dictionary,"<p>Hey I am trying to provide japanese - chinese translation functionality for my project. I have found <a href=""https://github.com/Kalamandea/Rikaichan"" rel=""nofollow noreferrer"">Rikaichan</a> which is a chrome plugin that achieves a popup japanese - english translation. Rikaichan uses some online dictionaries for translation. In the <a href=""https://drive.google.com/drive/folders/0BwVl0WUbZr5QQkpEdV9YS1RYNDg"" rel=""nofollow noreferrer"">link</a> this project provides there are a lot of dictionaries corresponding to different languges. I am wondering is there a Chinese version of this kind of dictionary? If not is there any online resource for japanese - chinese dictionary data I could retrieve?</p>
",Multilingual Language Processing & Language Identification,find resource japanese chinese dictionary hey trying provide japanese chinese translation functionality project found rikaichan chrome plugin achieves popup japanese english translation rikaichan us online dictionary translation link project provides lot dictionary corresponding different languges wondering chinese version kind dictionary online resource japanese chinese dictionary data could retrieve
"Tweets analysis: Get unique positive, unique negative and unique neutral words : Optimised solution:Natural Language processing:","<p>I have a dataframe <code>train</code>, with a column <code>tweet_content</code>. There is a column <code>sentiment</code> which tells the overall sentiment of tweet. Now there are lot of words which are common in tweets of neutral, positive and negative sentiments. I want to find the words which are unique to each specific sentiment</p>
<p>train</p>
<pre><code>tweet_content                                sentiment 
[PM, you, rock, man]                         Positive
[PM, you, are, a, total, idiot, man]         Negative
[PM, I, have, no, opinion, about, you, dear] Neutral and so on..There are 30,000 rows
</code></pre>
<p><strong>P.S. Note that each tweet or row is a list of words for the column tweet_content.</strong></p>
<p><strong>Expected output for the above tweets:</strong> (unique_positive, unique_negative etc is for all the tweets in the df. There are 30,000 rows. So unique positive will be the list of words which are unique for positive sentiment for all 30,000 rows combined. Here I have just taken 3 tweets as random eg</p>
<pre><code>unique_positive = [rock] #you and PM occur in Negative and Neutral tweets, man occurs in negative tweet
unique_negative = [are , an, idiot] #you and PM occur in Positive and Neutral tweets, man occurs in positive tweet 
unique_positive = [I, have, no, opinion, about, dear] #you and PM occur in Negative and Neutral tweets
</code></pre>
<p>where</p>
<pre><code>raw_text = [word for word_list in train['content'] for word in word_list] #list of all words
unique_Positive= words_unique('positive', 20, raw_text) #find 20 unique words which are only in positive sentiment from list of all words 
</code></pre>
<p><strong>Problem:</strong>
The <strong>below function runs perfectly</strong> and finds unique words for positive, neutral and negative sentiments. <strong>But</strong> the problem is it is <strong>taking 30 minutes</strong> to run. <strong>Is there a way to optimise this function and run it faster?</strong>.</p>
<p><strong>Function to find out the unique words for each sentiment:</strong></p>
<pre><code>def words_unique(sentiment,numwords,raw_words):
    '''
    Input:
        segment - Segment category (ex. 'Neutral');
        numwords - how many specific words do you want to see in the final result; 
        raw_words - list  for item in train_data[train_data.segments == segments]['tweet_content']:
    Output: 
        dataframe giving information about the numwords number of unique words in a particular sentiment (in descending order based on their counts)..

    '''
    allother = []
    for item in train[train.sentiment != sentiment]['tweet_content']:
        for word in item:
            allother .append(word)
    allother  = list(set(allother ))
    
    specificnonly = [x for x in raw_text if x not in allother]
    
    mycounter = Counter()
    
    for item in train[train.sentiment == sentiment]['tweet_content']:
        for word in item:
            mycounter[word] += 1
    keep = list(specificnonly)
    
    for word in list(mycounter):
        if word not in keep:
            del mycounter[word]
    
    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])
    
    return Unique_words
</code></pre>
",Multilingual Language Processing & Language Identification,tweet analysis get unique positive unique negative unique neutral word optimised solution natural language processing dataframe column column tell overall sentiment tweet lot word common tweet neutral positive negative sentiment want find word unique specific sentiment train p note tweet row list word column tweet content expected output tweet unique positive unique negative etc tweet df row unique positive list word unique positive sentiment row combined taken tweet random eg problem function run perfectly find unique word positive neutral negative sentiment problem taking minute run way optimise function run faster function find unique word sentiment
Working Azure HTTP function app breaks upon publishing,"<p>I am developing a natural language processing function with visual studio code and Microsoft Azure. The function triggers on an http request, and works perfectly on my local host during testing. However, as soon as I publish the function to Azure, the function becomes unresponsive, citing a &quot;500 internal error.&quot; I believe the problem might have something to do with a specific dependency not being installed upon publishing.</p>
<p>My &quot;requirements.txt&quot; file looks like this</p>
<pre><code>azure-functions
nltk
numpy
pyodbc
pandas
sklearn
spacy
</code></pre>
<p>And lastly, in order to run the function on my local machine, I download the &quot;en_core_web_sm&quot; package. I am able to quickly do this by running &quot;python -m spacy download en_core_web_sm&quot; in my virtual environment. If I do not use the <code>nlp = spacy.load('en_core_web_sm')</code> line in my code, the function will publish successfully.</p>
<p>My worry is that this dependency is not included in the function once published. I have tried to solve this problem by adding the github download string - <a href=""https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz"" rel=""nofollow noreferrer"">https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz</a> - in my requirements.txt folder, as I am able to download this using <code>pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.3.0.tar.gz</code>. After the addition, the requirements.txt folder looks like this:</p>
<pre><code>azure-functions
nltk
numpy
pyodbc
pandas
sklearn
spacy
https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.3.0.tar.gz

</code></pre>
<p>I have also attempted to configure the necessary command in my &quot;tasks.json&quot; folder, changing it to include the command as such:</p>
<pre><code>{
  &quot;version&quot;: &quot;2.0.0&quot;,
  &quot;tasks&quot;: [
    {
      &quot;type&quot;: &quot;func&quot;,
      &quot;command&quot;: &quot;host start&quot;,
      &quot;problemMatcher&quot;: &quot;$func-watch&quot;,
      &quot;isBackground&quot;: true,
      &quot;dependsOn&quot;: &quot;pipInstall&quot;
    },
    {
      &quot;label&quot;: &quot;pipInstall&quot;,
      &quot;type&quot;: &quot;shell&quot;,
      &quot;osx&quot;: {
        &quot;command&quot;: &quot;${config:azureFunctions.pythonVenv}/bin/python -m spacy download en_core_web_sm &amp;&amp; python -m pip install -r requirements.txt&quot;
      },
      &quot;windows&quot;: {
        &quot;command&quot;: &quot;${config:azureFunctions.pythonVenv}\\Scripts\\python -m spacy download en_core_web_sm &amp;&amp; python -m pip install -r requirements.txt&quot;
      },
      &quot;linux&quot;: {
        &quot;command&quot;: &quot;${config:azureFunctions.pythonVenv}/bin/python -m spacy download en_core_web_sm &amp;&amp; python -m pip install -r requirements.txt&quot;
      },
      &quot;problemMatcher&quot;: []
    }
  ]
}
</code></pre>
<p>I would appreciate any help as this is a time sensitive issue. Thank you so much!</p>
",Multilingual Language Processing & Language Identification,working azure http function app break upon publishing developing natural language processing function visual studio code microsoft azure function trigger http request work perfectly local host testing however soon publish function azure function becomes unresponsive citing internal error believe problem might something specific dependency installed upon publishing requirement txt file look like lastly order run function local machine download en core web sm package able quickly running python spacy download en core web sm virtual environment use line code function publish successfully worry dependency included function published tried solve problem adding github download string requirement txt folder able download using addition requirement txt folder look like also attempted configure necessary command task json folder changing include command would appreciate help time sensitive issue thank much
How can I use NLP to group multiple senteces by semantic similarity,"<p>I'm trying to increase the efficiency of a non-conformity management program. Basically, I have a database containing about a few hundred rows, each row describes a non-conformity using a text field.
Text is provided in Italian and I have no control over what the user writes.
I'm trying to write a python program using NTLK to detect how many of these rows report the same problem, written differently but with similar content.
For example, the following sentences need to be related, with a high rate of confidence</p>

<ul>
<li>I received 10 pieces less than what was ordered</li>
<li>10 pieces have not been shipped</li>
</ul>

<p>I already found the following article describing how to preprocess text for analysis:
<a href=""https://paraphrase.projecttopics.org/how-to-develop-a-paraphrasing-tool-using-nlp-natural-language-processing-model-in-python.html"" rel=""nofollow noreferrer"">How to Develop a Paraphrasing Tool Using NLP (Natural Language Processing) Model in Python</a></p>

<p>I also found other questions on SO but they all refer to word similarity, two sentences comparison, or comparison using a reference meaning.</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/24407333/how-to-perform-semantic-similarity-in-document"">This one uses a reference meaning</a></li>
<li><a href=""https://stackoverflow.com/questions/44234909/wordnet-based-semantic-similarity-measurements"">This one refers to two sentences comparison</a></li>
</ul>

<p>In my case, I have no reference and I have multiple sentences that needs to be grouped if they refer to similar problems, so I wonder if this job it's even possible to do with a script.</p>

<p><a href=""https://stackoverflow.com/a/2057825/10511191"">This answer</a> says that it cannot be done but it's quite old and maybe someone knows something new.</p>

<p>Thanks to everyone who can help me.</p>
",Multilingual Language Processing & Language Identification,use nlp group multiple senteces semantic similarity trying increase efficiency non conformity management program basically database containing hundred row row describes non conformity using text field text provided italian control user writes trying write python program using ntlk detect many row report problem written differently similar content example following sentence need related high rate confidence received piece le wa ordered piece shipped already found following article describing preprocess text analysis develop paraphrasing tool using nlp natural language processing model python also found question refer word similarity two sentence comparison comparison using reference meaning case reference multiple sentence need grouped refer similar problem wonder job even possible script href answer say done quite old maybe someone know something new thanks everyone help
Natural Language Processing books or resource for entry level person?,"<p>Can anyone gives some suggestions for good natural language processing book. Following are the factors I have in mind:</p>

<ul>
<li>It gives a good overview of these huge topics without too much depth.</li>
<li>Concepts need to explain in picture form.    </li>
<li>Sample code in JAVA/Python/R.</li>
</ul>
",Multilingual Language Processing & Language Identification,natural language processing book resource entry level person anyone give suggestion good natural language processing book following factor mind give good overview huge topic without much depth concept need explain picture form sample code java python r
Multilingual Search using language Elasticsearch,"<p>i am working on a project to perform multilingual full-text search using Elasticsearch. The historical training dataset i am using is also multilingual and i am trying now to configure text analysis with language analyzer and language detection.</p>

<p>1) i am using the following link as a guide and as it is written in the first paragraph i need to install <strong>an Inference Ingest Processor</strong>. How can i install it? (i am not familiar with Java and new in elasticsearch)
<a href=""https://www.elastic.co/de/blog/multilingual-search-using-language-identification-in-elasticsearch"" rel=""nofollow noreferrer"">https://www.elastic.co/de/blog/multilingual-search-using-language-identification-in-elasticsearch</a></p>

<p>2) Elasticsearch offers language Analyzer for many languages i will need to configure Analyzers in 8 languages if i follow this link <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html</a>  i will have to create 8 different custom analyzers which is quite long. Is there any shorter way to write one setting for 8 languages?</p>
",Multilingual Language Processing & Language Identification,multilingual search using language elasticsearch working project perform multilingual full text search using elasticsearch historical training dataset using also multilingual trying configure text analysis language analyzer language detection using following link guide written first paragraph need install inference ingest processor install familiar java new elasticsearch elasticsearch offer language analyzer many language need configure analyzer language follow link create different custom analyzer quite long shorter way write one setting language
Glove Word Embeddings supported languages,"<p>I started experimenting with word embeddings, and I found some results which I don't know how to interpret. </p>

<p>I first used an English corpus for both training and testing and afterwards, I used the English corpus for training and a small French corpus for testing (all corpora have been annotated for the same binary classification task). In both cases, I used the pre-trained on tweets Glove embeddings. </p>

<p>As the results in the case where I also used the French corpus improved (by almost 5%, reaching ~accuracy = 0.8), I was wondering if Glove was trained on multilingual data.</p>

<p>I haven't seen anyone making this statement, in contrast to FastText, for example, where you have embeddings for different languages.</p>
",Multilingual Language Processing & Language Identification,glove word embeddings supported language started experimenting word embeddings found result know interpret first used english corpus training testing afterwards used english corpus training small french corpus testing corpus annotated binary classification task case used pre trained tweet glove embeddings result case also used french corpus improved almost reaching accuracy wa wondering glove wa trained multilingual data seen anyone making statement contrast fasttext example embeddings different language
NLP: Generate Text from keywords (NLG),"<p>I would like to create an abstract in the end using Natural Language Processing.</p>

<p>As Input I would provide certain keywords belonging to its category.
EG</p>

<pre><code>Name -&gt; Sven
Owner -&gt; Felix
Species -&gt; Dog
YearOwned -&gt; 2019
</code></pre>

<p>Should result into
<code>‚ÄûFelix has a dog named Sven. He has him since 2019.‚Äú</code></p>

<p>But I don‚Äòt know what would be the best way to approach this. I thought about NNs bc I have enough data like this to Train it. But the results I found online were not pretty leading me into hardcoding a Template but this wouldnt be very flexible if one data value is missing or has two entries. </p>

<p>Maybe someone has an idea how I could best approach this.</p>

<p>Tyvm Taka</p>

<p>Ideally the resulting text should consist out of multiple sentences. Bc more data would follow.</p>
",Multilingual Language Processing & Language Identification,nlp generate text keywords nlg would like create abstract end using natural language processing input would provide certain keywords belonging category eg result know would best way approach thought nns bc enough data like train result found online pretty leading hardcoding template wouldnt flexible one data value missing ha two entry maybe someone ha idea could best approach tyvm taka ideally resulting text consist multiple sentence bc data would follow
Good Examples: English Parsing / Natural Language Processing,"<p>I would like to make a calendar application that accepts plain english input better than those that exist. I have found Stanford's NLP which seems cool, but I was wondering if it's helpful to this kind of task. I can't find examples of people using it for anything. Should an app actually understand the language?  It seems like the natural english calendars that exist are looking for keywords / patterns and trying to parse that way, but I think an app could do better than that.</p>

<p>My real question: Could someone tell me how to find examples of people using the NLP or a different (publicly available) english parser to make a really useful app?</p>
",Multilingual Language Processing & Language Identification,good example english parsing natural language processing would like make calendar application accepts plain english input better exist found stanford nlp seems cool wa wondering helpful kind task find example people using anything app actually understand language seems like natural english calendar exist looking keywords pattern trying parse way think app could better real question could someone tell find example people using nlp different publicly available english parser make really useful app
NLP - Fetch words which is not part of English grammar,"<p>I have a requirement where I have to read a text and extract only the SQL Table Names and Column names.</p>

<p>Eg statement: ""Direct move by joining the Table_A with Table_B on Column_1</p>

<p>In the above example I have to read only the table name and column name. Similarly I have even complex and large statements, from which I have to extract table and columns names. </p>

<p>I have tried using the POS tagging from NLTK package. But I am not sure if the POS tagging is a valid approach in this requirement, because the words I am trying to extract is not an exact English grammar word.</p>

<p>Any idea on how I can approach this? Please throw some light.</p>
",Multilingual Language Processing & Language Identification,nlp fetch word part english grammar requirement read text extract sql table name column name eg statement direct move joining table table b column example read table name column name similarly even complex large statement extract table column name tried using po tagging nltk package sure po tagging valid approach requirement word trying extract exact english grammar word idea approach please throw light
How to find the characteristics of a bunch of word Clusters?,"<p><strong>My Motivations</strong>  I'm trying to learn German and realized there's a confounding fact with the structure of German: every noun has a gender which seems unrelated to the noun itself in many cases.</p>

<p>Unlike languages such as English, each noun has a different definite article, depending on gender: <em>der</em> (masculine), <em>die</em> (feminine), and <em>das</em> (neuter). For example:
<em>das M√§dchen</em> (""the girl""), <em>der Rock</em> (""the skirt), <em>die Hose</em> (""the trousers/pants""). So, there seems to be no correlation between gender assignment of nouns and their meanings.</p>

<p><strong>The Data</strong>
I gathered up to 5000 German words with <strong>3 columns (das, der, die)</strong> for each word with 1's and 0's. So, my data is already clustered with one hot encoding and I'm not trying to predict anything. </p>

<p><strong>Why I'm here</strong> I am clueless on where to start, how to approach this problem as the concept of distance in clustering doesn't make sense to me in this setting. I can't think of a way to generate an understandable description of these clusters. The mixed data makes it impossible for me to think of some hard-coded metrics for evaluation.</p>

<p><strong>So, my question is:</strong>
I want to find some patterns, some characteristics of these words that made them fall in a specific cluster. I don't know if I'm making any sense but some people managed to find some patterns already (for example word endings, elongated long objects tend to be masculine etc., etc) and I believe ML/AI could do a way better job at this. Would it be possible for me to do something like this?</p>

<p><strong>Some personal thoughts</strong>
While I was doing some research (perhaps, naive), I realized the potential options are <a href=""https://en.wikipedia.org/wiki/Decision_tree"" rel=""nofollow noreferrer"">decision trees</a> and <a href=""https://en.wikipedia.org/wiki/Cobweb_(clustering)"" rel=""nofollow noreferrer"">cobweb algorithms</a>. Also, I was thinking if I could just scrape a few images (say 5) for every word and try to run some image classification and see the intermediate NN's to see if any specific shapes support a specific object gender. In addition to that, I was wondering whether scraping the data of google n-gram viewers of these words could help in anyway. I couldn't think of a way to use NLP or its sub domains.</p>

<p><strong>Alternatives</strong> If everything I just wrote sounds nonsensical, please suggest me a way to make visual representations of my dataframe (more like nodes and paths with images at nodes, one for each cluster) in Python so that I could make pictorial mind maps and try to by heart them.</p>

<p><strong>The ultimate purpose is to make learning German simpler</strong> for myself and possibly for others</p>
",Multilingual Language Processing & Language Identification,find characteristic bunch word cluster motivation trying learn german realized confounding fact structure german every noun ha gender seems unrelated noun many case unlike language english noun ha different definite article depending gender der masculine die feminine da neuter example da dchen girl der rock skirt die hose trouser pant seems correlation gender assignment noun meaning data gathered german word column da der die word data already clustered one hot encoding trying predict anything clueless start approach problem concept distance clustering make sense setting think way generate understandable description cluster mixed data make impossible think hard coded metric evaluation question want find pattern characteristic word made fall specific cluster know making sense people managed find pattern already example word ending elongated long object tend masculine etc etc believe ml ai could way better job would possible something like personal thought wa research perhaps naive realized potential option decision tree cobweb algorithm also wa thinking could scrape image say every word try run image classification see intermediate nn see specific shape support specific object gender addition wa wondering whether scraping data google n gram viewer word could help anyway think way use nlp sub domain alternative everything wrote sound nonsensical please suggest way make visual representation dataframe like node path image node one cluster python could make pictorial mind map try heart ultimate purpose make learning german simpler possibly others
Could not find stanford-parser\.jar jar file at .\stanford-corenlp-4.0.0,"<p>I'm new to nltk and seem to be following an outdated tutorial to get started with StanfordDependencyParser in nltk.</p>

<p>I've installed Stanford Core NLP and their English models from <a href=""https://stanfordnlp.github.io/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/</a></p>

<p>However, running the script below returns the following error:</p>

<pre><code>could not find stanford-parser\.jar jar file at .\stanford-corenlp-4.0.0
</code></pre>

<p>Would appreciate any pointers!</p>

<p>Using nltk 3.5 and stanfordcorenlp 4.0.0 with Python 3.7</p>

<p>Script attached below.</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser

path_to_jar = '.\stanford-corenlp-4.0.0'
path_to_models_jar = '.\stanford-corenlp-4.0.0\stanford-corenlp-4.0.0-models-english.jar'

dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)

result = dependency_parser.raw_parse('I shot an elephant in my pajamas')
dep = list(result)

for item in dep:
    print(list(item.triples()))
</code></pre>
",Multilingual Language Processing & Language Identification,could find stanford parser jar jar file stanford corenlp new nltk seem following outdated tutorial get started stanforddependencyparser nltk installed stanford core nlp english model however running script return following error would appreciate pointer using nltk stanfordcorenlp python script attached
Error when using google translate API to translate a dataframe,"<p>I'm trying to translate part of SQuAD 1.1 dataset to Sinhalese. I don't know whether i can use the json file straight into translation
What i tried so far is making a little dataframe of SQuAD dataset and try to translate that as a demo to myself. But i got different errors. Below is the error i'm getting now. Can you help me to fix that error or tell me a better way to complete my task using python.</p>

<pre><code>```import googletrans
from googletrans import Translator

import os
from google.cloud import translate_v2 as translate

os.environ['GOOGLE_APPLICATION_CREDENTIALS']=r""C:\Users\Sathsara\Documents\Python Learning\Translation test\translationAPI\flash-medley-278816-b2012b874797.json""

# create a translator object
translator = Translator()

# use translate method to translate a string - by default, the destination language is english
translated = translator.translate('I am Sathsara Rasantha',dest='si')

# the translate method returns an object
print(translated)


# obtain translated string by using attribute .text
translated.text

import pandas as pd


translate_example = pd.read_json(""example2.json"")
translate_example

contexts = []
questions = []
answers_text = []
answers_start = []
for i in range(translate_example.shape[0]):
    topic = translate_example.iloc[i,0]['paragraphs']
    for sub_para in topic:
        for q_a in sub_para['qas']:
            questions.append(q_a['question'])
            answers_start.append(q_a['answers'][0]['answer_start'])
            answers_text.append(q_a['answers'][0]['text'])
            contexts.append(sub_para['context'])   
df = pd.DataFrame({""context"":contexts, ""question"": questions, ""answer_start"": answers_start, ""text"": answers_text})
df
df=df.loc[0:2,:]
df


# make a deep copy of the data frame
df_si = df.copy()

# translate columns' name using rename function
df_si.rename(columns=lambda x: translator.translate(x).text, inplace=True)


df_si.columns


translations = {}
for column in df_si.columns:
    # unique elements of the column
    unique_elements = df_si[column].unique()
    for element in unique_elements:
        # add translation to the dictionary
        translations[element] = translator.translate(element,dest='si').text

print(translations)

# modify all the terms of the data frame by using the previously created dictionary
df_si.replace(translations, inplace = True)

# check translation
df_si.head()```
</code></pre>

<p>This is the error i get</p>

<pre><code>&gt; --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call
&gt; last) &lt;ipython-input-24-f55a5ca59c36&gt; in &lt;module&gt;
&gt;       5     for element in unique_elements:
&gt;       6         # add translation to the dictionary
&gt; ----&gt; 7         translations[element] = translator.translate(element,dest='si').text
&gt;       8 
&gt;       9 print(translations)
&gt; 
&gt; ~\Anaconda3\lib\site-packages\googletrans\client.py in translate(self,
&gt; text, dest, src)
&gt;     170 
&gt;     171         origin = text
&gt; --&gt; 172         data = self._translate(text, dest, src)
&gt;     173 
&gt;     174         # this code will be updated when the format is changed.
&gt; 
&gt; ~\Anaconda3\lib\site-packages\googletrans\client.py in
&gt; _translate(self, text, dest, src)
&gt;      73             text = text.decode('utf-8')
&gt;      74 
&gt; ---&gt; 75         token = self.token_acquirer.do(text)
&gt;      76         params = utils.build_params(query=text, src=src, dest=dest,
&gt;      77                                     token=token)
&gt; 
&gt; ~\Anaconda3\lib\site-packages\googletrans\gtoken.py in do(self, text)
&gt;     199     def do(self, text):
&gt;     200         self._update()
&gt; --&gt; 201         tk = self.acquire(text)
&gt;     202         return tk
&gt; 
&gt; ~\Anaconda3\lib\site-packages\googletrans\gtoken.py in acquire(self,
&gt; text)
&gt;     144         a = []
&gt;     145         # Convert text to ints
&gt; --&gt; 146         for i in text:
&gt;     147             val = ord(i)
&gt;     148             if val &lt; 0x10000:
&gt; 
&gt; TypeError: 'numpy.int64' object is not iterable
</code></pre>
",Multilingual Language Processing & Language Identification,error using google translate api translate dataframe trying translate part squad dataset sinhalese know whether use json file straight translation tried far making little dataframe squad dataset try translate demo got different error error getting help fix error tell better way complete task using python error get
How do I host CoreNLP server with caseless models?,"<p>I'm trying to host a CoreNLP server but with the caseless models but I don't think I was successful and the official site doesn't have example hosting such model.</p>

<p>I'm currently hosting with:</p>

<pre><code>java -mx4g \
           -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
           -port 9000 \
           -timeout 15000
</code></pre>

<p>but this is the default way of hosting which doesn't use the caseless models. I checked the app log and it was loading the standard models instead of caseless models:</p>

<pre><code>[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
</code></pre>

<p>According to <a href=""https://stanfordnlp.github.io/CoreNLP/caseless.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/caseless.html</a>, I have downloaded the english models jar file and put it under the corenlp module folder, but I don't know exactly how to specify and use those for server hosting.</p>

<p>In the client side, I'm doing the following:</p>

<pre><code>import requests

r = requests.post('http://[::]:9000/?properties={""annotators"":""tokenize,ssplit,truecase,pos,ner"",""outputFormat"":""json""}', 
                  data=""show me hotels in toronto for next weekend"")
print(r.text)
</code></pre>

<p>The truecase is working, but I don't see the caseless models being used.</p>

<p>Any help would be appreciated.</p>
",Multilingual Language Processing & Language Identification,host corenlp server caseless model trying host corenlp server caseless model think wa successful official site example hosting model currently hosting default way hosting use caseless model checked app log wa loading standard model instead caseless model according downloaded english model jar file put corenlp module folder know exactly specify use server hosting client side following truecase working see caseless model used help would appreciated
How to &quot;expand&quot; an English sentence,"<p>I would like to get some steps and resources about, how to build program to expand a sentence in English.</p>

<p>For example, if an input sentence is </p>

<blockquote>
  <p><code>""My father is coaching and refereeing soccer and basketball.""</code></p>
</blockquote>

<p>Then the program would output four sentences:</p>

<ul>
<li><p><code>""My father is coaching soccer.""</code></p></li>
<li><p><code>""My father is coaching basketball.""</code></p></li>
<li><p><code>""My father is refereeing soccer.""</code></p></li>
<li><p><code>""My father is refereeing basketball.""</code></p></li>
</ul>

<p>The generated sentences must be valid English sentences and their meaning must follow from the meaning of the input sentence.</p>
",Multilingual Language Processing & Language Identification,expand english sentence would like get step resource build program expand sentence english example input sentence program would output four sentence generated sentence must valid english sentence meaning must follow meaning input sentence
Bottleneck Feature extraction for phonemes in language translation model using machine learning,"<p>I have built a basic encoder decoder model architecture for language translation directly from audio speeches but it desperately needs some increase in performance/accuracy. The only thing I have not been able to achieve is extracting the bottleneck features from the input audio files and taking it forward to the decoder via the attention mechanism.</p>

<p>I'm very new to this. I came across the Bottleneck Feature Extractor (<a href=""https://github.com/iondel/multilingual-bottleneck-features"" rel=""nofollow noreferrer"">https://github.com/iondel/multilingual-bottleneck-features</a>) and Kaldi's ASR toolkit ( <a href=""https://github.com/kaldi-asr/kaldi"" rel=""nofollow noreferrer"">https://github.com/kaldi-asr/kaldi</a> ). The first one uses .scp files made in Kaldi style from what I could understand. I was wondering if someone could tell me in as simple language as possible, as to how do I use these amazing toolkits (Or any other out there, or any other tips) to extract phoneme features from my input audio spectrogram and send it forward in the pipeline to the decoder?</p>

<p>My pipeline:-</p>

<pre><code>#VoiceConversionSeq2Seq - https://arxiv.org/abs/1810.06865
inputs = Input(shape=(timesteps, features))

#Encoder
#masked_encoder_inputs = layers.Masking(mask_value=0)(inputs)
encoder_out1,f_h1, f_c1, b_h1, b_c1 = Bidirectional(LSTM(latent_dimE, return_sequences = True, return_state=True))(inputs)
encoder_out2,f_h2, f_c2, b_h2, b_c2 = Bidirectional(LSTM(latent_dimE, return_sequences = True, return_state=True))(encoder_out1)
encoder_out2 = LayerNormalization()(encoder_out2)
state_h2 = Concatenate()([f_h2, b_h2])
state_c2 = Concatenate()([f_c2, b_c2])
encoder_states = [state_h2, state_c2]

#PreNet
NN_1 = Dense(256, activation = 'relu')(encoder_out2)
drop1 = Dropout(rate = 0.5)(NN_1)
NN_2 = Dense(256, activation = 'relu')(NN_1)
drop2 = Dropout(rate = 0.5)(NN_2)


#Attention
att_out = MultiHeadAttention(head_num=4)(drop2)

#Decoder
#decoder_inputs = att_out#RepeatVector(timesteps)(encoder_outputs)
decoder_inputs = att_out
decoder_lstm1 = LSTM(latent_dimD*2,return_state=True,return_sequences=True)
decoder_outputs1, _, _ = decoder_lstm1(decoder_inputs, initial_state = encoder_states)

decoder_lstm2 = LSTM(latent_dimD*2,return_state=True,return_sequences=True)
decoder_outputs2, _, _ = decoder_lstm2(decoder_outputs1, initial_state = encoder_states)

decoder_lstm3 = LSTM(latent_dimD*2,return_state=True,return_sequences=True)
decoder_outputs3, _, _ = decoder_lstm3(decoder_outputs2, initial_state = encoder_states)

#PostNet
conv_1 = Conv1D(filters = 64, kernel_size = 3, kernel_initializer = gammaT(decoder_outputs3), padding = 'same', activation='relu', input_shape = (timesteps, features))(decoder_outputs3)
drop3 = Dropout(rate = 0.2)(conv_1)
conv_2 = Conv1D(filters = 64, kernel_size = 3, padding = 'same', activation='relu')(drop3)
drop4 = Dropout(rate = 0.2)(conv_2)
conv_3 = Conv1D(filters = 64, kernel_size = 3, padding = 'same', activation='relu')(drop4)
drop5 = Dropout(rate = 0.2)(conv_3)

NN_3 = Dense(timesteps, activation = 'softmax')(drop5)

decoder_outputs = Dense(features, activation = 'relu')(NN_3)

model = Model(inputs, decoder_outputs)
#encoder = Model(inputs, encoder_outputs)
</code></pre>

<p>Things I need to add :-</p>

<p>a. Gammatone filter in the first conv1d postnet layer (I'm still struggling with this) - I have posted it here - <a href=""https://stackoverflow.com/questions/61796298/how-to-initialize-cnn-layer-with-gammatone-filters-or-any-filter-for-sound-reg/61796590#61796590"">How to initialize CNN Layer with Gammatone Filters (or any filter) for sound regression (Or Classification)?</a></p>

<p>b. Phoneme bottleneck feature extraction after Encoder to send it forward down the pipeline along with encoder states and output.</p>

<p>Any help/tip is highly appreciated. Ask me any sort of questions regarding the code</p>
",Multilingual Language Processing & Language Identification,bottleneck feature extraction phoneme language translation model using machine learning built basic encoder decoder model architecture language translation directly audio speech desperately need increase performance accuracy thing able achieve extracting bottleneck feature input audio file taking forward decoder via attention mechanism new came across bottleneck feature extractor kaldi asr toolkit first one us scp file made kaldi style could understand wa wondering someone could tell simple language possible use amazing toolkits tip extract phoneme feature input audio spectrogram send forward pipeline decoder pipeline thing need add gammatone filter first conv postnet layer still struggling posted href initialize cnn layer gammatone filter filter sound regression classification b phoneme bottleneck feature extraction encoder send forward pipeline along encoder state output help tip highly appreciated ask sort question regarding code
removing non English words from text in df.columns words contain letters and numbers,"<p>How to removing non English words from text in  df.columns words contain letters and numbers</p>

<p>Ex</p>

<p>df['text']</p>

<p>'the interiors nrd studio | happy mothers day   ‚Äùthere is no influence so powerful as that of the mother.‚Äù ‚Äîsara josepha hale... happy mother‚Äôs day mom &amp; to all the mothers around the world! lots of light natasha<br>
0wet3bxtfl'</p>

<p>'but still missing you every day happy mothers day francis mcclafferty (mccool) 9wlhju7cxf'</p>

<p>from the above 2 rows I need to remove the  word  '0wet3bxtfl' &amp; '9wlhju7cxf'</p>
",Multilingual Language Processing & Language Identification,removing non english word text df column word contain letter number removing non english word text df column word contain letter number ex df text interior nrd studio happy mother day influence powerful mother sara josepha hale happy mother day mom mother around world lot light natasha wet bxtfl still missing every day happy mother day francis mcclafferty mccool wlhju cxf row need remove word wet bxtfl wlhju cxf
Identify duplicated paragraphs (boilerplate) within several email documents,"<p>I have started to learn text mining and natural language processing using R and Python. Recently, I was trying to perform some basics tasks such as: (1) the most frequent terms used within a set of documents (email documents) and (2) clustering. The ""problem"" comes with some repetitive paragraphs, such as disclaimers, signatures on emails, etc; because they are adding some noise to my results.... Is there a way to identify boilerplate or repetitive paragraphs within the set of documents? In order to remove them during the preprocessing tasks. </p>
",Multilingual Language Processing & Language Identification,identify duplicated paragraph boilerplate within several email document started learn text mining natural language processing using r python recently wa trying perform basic task frequent term used within set document email document clustering problem come repetitive paragraph disclaimer signature email etc adding noise result way identify boilerplate repetitive paragraph within set document order remove preprocessing task
How to identify character encoding from website?,"<p><strong>What I'm trying to do:</strong>
I'm getting from a database a list of uris and download them,
removing the stopwords and counting the frequency that the words appears in the webpage,
then trying to save in the mongodb.  </p>

<p><strong>The Problem:</strong>
When I try to save the result in the database I get the error
bson.errors.invalidDocument: the document must be a valid utf-8</p>

<p>it appears to be related to the codes '\xc3someotherstrangewords', '\xe2something'
when I'm processing the webpages I try remove the punctuation, but I can't remove accents because I'll get a wrong word.</p>

<p><strong>What I already tried</strong>
I've tried identify the char encode through the header from the webpage
I've tried utilize the chardet</p>

<p>utilize the re.compile(r""[^a-zA-Z]"") and/or unicode(variable,'ascii', 'ignore');<br>
that isn't good for non-English languages because they remove the accents. </p>

<p><strong>What I want know is:</strong><br>
anyone know how identify the chars and translate to the right word/encode?<br>
e.g. get this from webpage '\xe2' and translate to '√¢'</p>

<p>(English isn't my first language so forgive me)
EDIT: if anyone want see the <a href=""https://github.com/raphaeljlps/ProjetoIC/"" rel=""nofollow"">source code</a></p>
",Multilingual Language Processing & Language Identification,identify character encoding website trying getting database list uris download removing stopwords counting frequency word appears webpage trying save mongodb problem try save result database get error bson error invaliddocument document must valid utf appears related code xc someotherstrangewords xe something processing webpage try remove punctuation remove accent get wrong word already tried tried identify char encode header webpage tried utilize chardet utilize compile r za z unicode variable ascii ignore good non english language remove accent want know anyone know identify char translate right word encode e g get webpage xe translate english first language forgive edit anyone want see source code
"ValueError: Tensor(&quot;ExponentialDecay_4:0&quot;, shape=(), dtype=float32)","<p>A Classifier for finding type of languge, Hindi or English.</p>

<pre class=""lang-none prettyprint-override""><code>ValueError: Tensor(""ExponentialDecay_4:0"", shape=(), dtype=float32) must be from the same graph as Tensor((""dnn/hiddenlayer_0/kernel/part_0:0"", shape=(), dtype=resource)).
</code></pre>

<pre class=""lang-py prettyprint-override""><code>from __future__ import absolute_import, division, print_function, unicode_literals
from absl import logging

import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns


df = pd.read_csv(""intern_test/data/english_text.csv"",  encoding=""latin-1"")
df2 = pd.read_csv(""intern_test/data/hinglish_text.csv"",  encoding=""latin-1"")

df['label'] = 0 # English
df2['label'] = 1 # Hindi

df3 = pd.concat([df,df2])

df3.head()
# Training input on the whole training set with no limit on training epochs.
train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(
    df3, df3[""label""], num_epochs=None, shuffle=True)

# Prediction on the whole training set.
predict_train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(
    df3, df3[""label""], shuffle=False)
# Prediction on the test set.
#predict_test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(
#    X_test, y_test, shuffle=False)

embedded_text_feature_column = hub.text_embedding_column(
    key=""text"", 
    module_spec=""https://tfhub.dev/google/nnlm-en-dim128/1"")

optimizer = tf.compat.v1.train.AdamOptimizer(
    learning_rate=tf.compat.v1.train.exponential_decay(
        global_step=0,
        learning_rate=0.1,
        decay_steps=10000,
        decay_rate=0.96, staircase=True))

estimator = tf.estimator.DNNClassifier(
    hidden_units=[500, 100],
    feature_columns=[embedded_text_feature_column],
    n_classes=2,
    optimizer=optimizer
)

estimator.train(input_fn=train_input_fn, steps=5000);
</code></pre>

<p>Full traceback:</p>

<pre class=""lang-none prettyprint-override""><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-47-1c4563a14246&gt; in &lt;module&gt;
      2 # batch size. This is roughly equivalent to 25 epochs since the training dataset
      3 # contains 25,000 examples.
----&gt; 4 estimator.train(input_fn=train_input_fn, steps=5000);

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    356 
    357       saving_listeners = _check_listeners_type(saving_listeners)
--&gt; 358       loss = self._train_model(input_fn, hooks, saving_listeners)
    359       logging.info('Loss for final step: %s.', loss)
    360       return self

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1122       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1123     else:
-&gt; 1124       return self._train_model_default(input_fn, hooks, saving_listeners)
   1125 
   1126   def _train_model_default(self, input_fn, hooks, saving_listeners):

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1152       worker_hooks.extend(input_hooks)
   1153       estimator_spec = self._call_model_fn(
-&gt; 1154           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
   1155       global_step_tensor = training_util.get_global_step(g)
   1156       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1110 
   1111     logging.info('Calling model_fn.')
-&gt; 1112     model_fn_results = self._model_fn(features=features, **kwargs)
   1113     logging.info('Done calling model_fn.')
   1114 

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py in _model_fn(features, labels, mode, config)
    520           input_layer_partitioner=input_layer_partitioner,
    521           config=config,
--&gt; 522           batch_norm=batch_norm)
    523 
    524     super(DNNClassifier, self).__init__(

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config, use_tpu, batch_norm)
    300           labels=labels,
    301           optimizer=optimizer,
--&gt; 302           logits=logits)
    303 
    304 

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/head.py in create_estimator_spec(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)
    238           self._create_tpu_estimator_spec(
    239               features, mode, logits, labels, optimizer, train_op_fn,
--&gt; 240               regularization_losses))
    241       return tpu_estimator_spec.as_estimator_spec()
    242     except NotImplementedError:

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/head.py in _create_tpu_estimator_spec(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)
   1244         train_op = optimizer.minimize(
   1245             regularized_training_loss,
-&gt; 1246             global_step=training_util.get_global_step())
   1247       elif train_op_fn is not None:
   1248         train_op = train_op_fn(regularized_training_loss)

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    411 
    412     return self.apply_gradients(grads_and_vars, global_step=global_step,
--&gt; 413                                 name=name)
    414 
    415   def compute_gradients(self, loss, var_list=None,

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
    610           scope_name = var.op.name
    611         with ops.name_scope(""update_"" + scope_name), ops.colocate_with(var):
--&gt; 612           update_ops.append(processor.update_op(self, grad))
    613       if global_step is None:
    614         apply_updates = self._finish(update_ops, name)

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py in update_op(self, optimizer, g)
    169       return optimizer._resource_apply_sparse_duplicate_indices(
    170           g.values, self._v, g.indices)
--&gt; 171     update_op = optimizer._resource_apply_dense(g, self._v)
    172     if self._v.constraint is not None:
    173       with ops.control_dependencies([update_op]):

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/adam.py in _resource_apply_dense(self, grad, var)
    173         math_ops.cast(self._beta2_t, grad.dtype.base_dtype),
    174         math_ops.cast(self._epsilon_t, grad.dtype.base_dtype),
--&gt; 175         grad, use_locking=self._use_locking)
    176 
    177   def _apply_sparse_shared(self, grad, var, indices, scatter_add):

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/gen_training_ops.py in resource_apply_adam(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)
   1300                              beta2=beta2, epsilon=epsilon, grad=grad,
   1301                              use_locking=use_locking,
-&gt; 1302                              use_nesterov=use_nesterov, name=name)
   1303   return _op
   1304   _result = None

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    348       # Need to flatten all the arguments into a list.
    349       # pylint: disable=protected-access
--&gt; 350       g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
    351       # pylint: enable=protected-access
    352     except AssertionError as e:

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _get_graph_from_inputs(op_input_list, graph)
   5711         graph = graph_element.graph
   5712       elif original_graph_element is not None:
-&gt; 5713         _assert_same_graph(original_graph_element, graph_element)
   5714       elif graph_element.graph is not graph:
   5715         raise ValueError(""%s is not from the passed-in graph."" % graph_element)

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _assert_same_graph(original_item, item)
   5647   if original_item.graph is not item.graph:
   5648     raise ValueError(""%s must be from the same graph as %s."" % (item,
-&gt; 5649                                                                 original_item))
   5650 
   5651 
</code></pre>

<pre class=""lang-none prettyprint-override""><code>ValueError: Tensor(""ExponentialDecay_4:0"", shape=(), dtype=float32) must be from the same graph as Tensor(""dnn/hiddenlayer_0/kernel/part_0:0"", shape=(), dtype=resource).
</code></pre>
",Multilingual Language Processing & Language Identification,valueerror tensor exponentialdecay shape dtype float classifier finding type languge hindi english full traceback
Chomsky hierarchy - examples with real languages,"<p>I'm trying to understand the four levels of the Chomsky hierarchy by using some real languages as models. He thought that all the natural languages can be generated through a <strong>Context-free Grammar</strong>, but Schieber contradicted this theory proving that languages such as Swiss German can only be generated through <strong>Context-sensitive grammar</strong>. Since Chomsky is from US, I guess that the American language is an example of Context-free grammar. My questions are:</p>

<ol>
<li>Are there languages which can be generated by regular grammars (type 3)?</li>
<li>Since the Recursively enumerable grammars can generate all languages, why not using that? Are they too complicated and less linear?</li>
<li>What the characteristic of Swiss German which make it impossible to be generated through Context-free grammars?</li>
</ol>
",Multilingual Language Processing & Language Identification,chomsky hierarchy example real language trying understand four level chomsky hierarchy using real language model thought natural language generated context free grammar schieber contradicted theory proving language swiss german generated context sensitive grammar since chomsky u guess american language example context free grammar question language generated regular grammar type since recursively enumerable grammar generate language using complicated le linear characteristic swiss german make impossible generated context free grammar
Is there a pre-trained German sentiment analyzer in flairNLP?,"<p>I'm currently looking at a sentiment analysis task on text messages in German. The Flair library for Python seems to have a pretty powerful pre-trained model for English, but I can't find any comprehensive answer to whether it also contains a similar thing in German. I've tried googling the issue but got nowhere, and also attempted exchanging</p>

<pre><code>flair.models.TextClassifier.load('en-sentiment')
</code></pre>

<p>for </p>

<pre><code>flair.models.TextClassifier.load('de-sentiment')
</code></pre>

<p>and the same with 'ger-sentiment' and 'ge-sentiment' just in case that would work by some lucky fluke. However, I have so far gotten nowhere. </p>

<p>I am aware of textblob-de and will try that, but wanted to ask whether I missed something in Flair.</p>
",Multilingual Language Processing & Language Identification,pre trained german sentiment analyzer flairnlp currently looking sentiment analysis task text message german flair library python seems pretty powerful pre trained model english find comprehensive answer whether also contains similar thing german tried googling issue got nowhere also attempted exchanging ger sentiment ge sentiment case would work lucky fluke however far gotten nowhere aware textblob de try wanted ask whether missed something flair
Can someone explain how the probability of a word at the beginning of a sentence is calculated?,"<p><a href=""https://i.sstatic.net/5LIVw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5LIVw.png"" alt=""enter image description here""></a></p>

<p>Hi everyone, I'm trying to calculate the probability of the sentence ""I want Chinese food"", which I succeeded in, but only because P(I|) has been noted own under the table. I can't seem to understand how the 0.25 is calculated. Can someone push me in the right direction? Thanks!</p>
",Multilingual Language Processing & Language Identification,someone explain probability word beginning sentence calculated hi everyone trying calculate probability sentence want chinese food succeeded p ha noted table seem understand calculated someone push right direction thanks
How to change initial culture on Luis app?,"<p>I created an en-us application in LUIS, but I want to work with this same application, changing the culture to Portuguese. Is there any option to change and be able to import a .json with culture in pt-bR?</p>
",Multilingual Language Processing & Language Identification,change initial culture luis app created en u application luis want work application changing culture portuguese option change able import json culture pt br
How to search a text for compound phrases that may be separated in the text; in python?,"<p>Assume I have a text and want to check if it contains some compound phrase, where I also want to include the cases where the respective words may not be directly followed by each other.</p>

<p>For example, assume you want to check if a text is about firefighters, then a text like this</p>

<pre><code>text = ""currently there are over 4000 people involved in fighting the rapidly growing fires in Australia""
</code></pre>

<p>should also yield a positive result. (I actually want to apply this to german, where examples may be less artificial)</p>

<p>I have no expertise in NLP, so maybe there is some clever way to do this, and I just do not know the correct term to search for. 
Of course, if the text is not too large, one could do the following exhaustive search on all 2-word-combinations:</p>

<pre><code>import itertools
import spacy

nlp = spacy.load({model})
doc = nlp(text)
wordlist =[t.lemma_ for t in doc if (not t.is_punct and not t.is_stop and not t.is_digit)]

combs = itertools.combinations(wlist,2)
comb_set = [set(c) for c in combs]

{'fire','fight'} in comb_set

</code></pre>

<p>But I was thinking that there might be a more efficient way to do this.</p>
",Multilingual Language Processing & Language Identification,search text compound phrase may separated text python assume text want check contains compound phrase also want include case respective word may directly followed example assume want check text firefighter text like also yield positive result actually want apply german example may le artificial expertise nlp maybe clever way know correct term search course text large one could following exhaustive search word combination wa thinking might efficient way
"for nlp, Is there any text corpora with pronunciation alphabet?","<p>I'm student who started to study nlp with python and nltk.</p>

<p>Since I'm trying to find heteronyms in the text, it looks essential to look for pronunciation aspect of text.</p>

<p>Therefore, I am looking for the text corpus which is provided with correct phonetic alphabet of sentences and words.</p>

<p><a href=""https://linguistics.stanford.edu/resources/resources-corpora"" rel=""nofollow noreferrer"">https://linguistics.stanford.edu/resources/resources-corpora</a></p>

<p>I thought some of above link's corpora might be help, but it lseems only standford student can get access to those corpora.</p>

<p>In conclusion, I want to ask where can I find some text sources with, or tagged by correct phonetic alphabet.</p>

<p>I'm not sure I asked politely in the right format because my English is not good and this is my first question on the stack oveflow. Sorry for it :(</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,nlp text corpus pronunciation alphabet student started study nlp python nltk since trying find heteronym text look essential look pronunciation aspect text therefore looking text corpus provided correct phonetic alphabet sentence word thought link corpus might help lseems standford student get access corpus conclusion want ask find text source tagged correct phonetic alphabet sure asked politely right format english good first question stack oveflow sorry thanks
How to get a sort of inverse lemmatizations for every language?,"<p>I found the spacy lib that allows me to apply lemmatization to words (blacks -> black, EN) (bianchi -> bianco, IT). My work is to analyze entities, not verbs or adjectives.</p>

<p>I'm looking for something that allows me to have all the possible words starting from the caninical form. </p>

<p>Like from ""black"" to ""blacks"", for english, or from ""bianco"" (in italian) and get ""bianca"", ""bianchi"", ""bianche"", etc. Is there any library that do this?</p>
",Multilingual Language Processing & Language Identification,get sort inverse lemmatizations every language found spacy lib allows apply lemmatization word black black en bianchi bianco work analyze entity verb adjective looking something allows possible word starting caninical form like black black english bianco italian get bianca bianchi bianche etc library
How to get String back from hash value in Spacy Library?,"<p>I am working in Natural Language Processing with the spaCy library. How can I get back a string value from a hash?</p>

<p>Here is my code with details in python with the spaCy library</p>

<p>Note- this question is mainly related with spaCy library and not python.</p>

<pre><code># import spacy package for english language
from spacy.lang.en import English

# initialize nlp with English
nlp = English()

# get hash from string
cat_hash = nlp.vocab.strings[""cat""]
print(cat_hash)

# Look up the cat_hash to get the string
cat_string = nlp.vocab.strings[cat_hash]
print(cat_string)
</code></pre>
",Multilingual Language Processing & Language Identification,get string back hash value spacy library working natural language processing spacy library get back string value hash code detail python spacy library note question mainly related spacy library python
word2vec gensim multiple languages,"<p>This problem is going completely over my head. I am training a Word2Vec model using gensim. I have provided data in multiple languages i.e. English and Hindi. When I am trying to find the words closest to 'man', this is what I am getting:</p>

<pre><code>model.wv.most_similar(positive = ['man'])
Out[14]: 
[('woman', 0.7380284070968628),
 ('lady', 0.6933152675628662),
 ('monk', 0.6662989258766174),
 ('guy', 0.6513140201568604),
 ('soldier', 0.6491742134094238),
 ('priest', 0.6440571546554565),
 ('farmer', 0.6366012692451477),
 ('sailor', 0.6297377943992615),
 ('knight', 0.6290514469146729),
 ('person', 0.6288090944290161)]
--------------------------------------------
</code></pre>

<p>Problem is, these are all English words. Then I tried to find similarity between same meaning Hindi and English words, </p>

<pre><code>model.similarity('man', '‡§Ü‡§¶‡§Æ‡•Ä')
__main__:1: DeprecationWarning: Call to deprecated `similarity` (Method will 
be removed in 4.0.0, use self.wv.similarity() instead).
Out[13]: 0.078265618974427215
</code></pre>

<p>This accuracy should have been better than all the other accuracies. The Hindi corpus I have has been made by translating the English one. Hence the words appear in similar contexts. Hence they should be close.</p>

<p>This is what I am doing here:</p>

<pre><code>#Combining all the words together.
all_reviews=HindiWordsList + EnglishWordsList

#Training FastText model
cpu_count=multiprocessing.cpu_count()
model=Word2Vec(size=300,window=5,min_count=1,alpha=0.025,workers=cpu_count,max_vocab_size=None,negative=10)
model.build_vocab(all_reviews)
model.train(all_reviews,total_examples=model.corpus_count,epochs=model.iter)
model.save(""word2vec_combined_50.bin"")
</code></pre>
",Multilingual Language Processing & Language Identification,word vec gensim multiple language problem going completely head training word vec model using gensim provided data multiple language e english hindi trying find word closest man getting problem english word tried find similarity meaning hindi english word accuracy better accuracy hindi corpus ha made translating english one hence word appear similar context hence close
I am looking to create a OCR for Arabic language. How do i get started?,"<p>Pretty much the title. Using NLP, i want something that can extract text from images. I am new to this and don't know where to start and how to go about it. Help will be appreciated.</p>
",Multilingual Language Processing & Language Identification,looking create ocr arabic language get started pretty much title using nlp want something extract text image new know start go help appreciated
How to use natural language processing to map text to a preset list of topics,"<p>I'm interested in being able to use a service such as Google's Natural Language API to classify random user questions into a preset list of topics.  I have an advanced level of programming experience, and want to use Google's service as a base and if necessary build a codebase around it to accomplish our goal.  An example use case would be:</p>

<p>Hardcoded preset list of topics:<br>
Baseball<br>
Football<br>
Soccer</p>

<p>Sample user questions and expected results:<br>
How do I cook pasta?  RESULT: No results<br>
What is a referee?  RESULT: Baseball/Football/Soccer<br>
What is a home run?  RESULT: Baseball</p>

<p>1) Does anything like this already exist to classify random user text into preset list of topics?<br>
2) If not, is there a programming concept that already exists that shows ways to implement this, or allow me to learn the concepts around this? (I searched on Google and couldn't find anything -- I may simply not know what to look for)<br>
3) If not, any guidance on this could be implemented?</p>
",Multilingual Language Processing & Language Identification,use natural language processing map text preset list topic interested able use service google natural language api classify random user question preset list topic advanced level programming experience want use google service base necessary build codebase around accomplish goal example use case would hardcoded preset list topic baseball football soccer sample user question expected result cook pasta result result referee result baseball football soccer home run result baseball doe anything like already exist classify random user text preset list topic programming concept already exists show way implement allow learn concept around searched google find anything may simply know look guidance could implemented
Keras LSTM go_backwards usage,"<p>I have a question regarding the usage of the go_backwards argument in the Keras LSTM model layer.  The documentation for this layer can be found here:  <a href=""https://keras.io/layers/recurrent/#lstm"" rel=""nofollow noreferrer"">https://keras.io/layers/recurrent/#lstm</a>.  </p>

<p>Question1:  If I set the ""go_backwards"" flag to True, do I still feed the input data ""forwards"" during the training process.  For example, if an input sentence in English normally reads ""I fell"", and it's German translation reads ""Ich fiel"", would I feed it forwards (""I fell"", ""Ich fiel""), or backwards (""fell I"", ""fiel Ich"") during the training process.</p>

<p>Question 2: Same question for making model predictions, is the data fed forward (""I fell""), or reverse (""fell I"")?</p>

<p>Question 3: If I was trying to use the model below and wanted to reverse the corpus data, would I set the go_backwards flag to true in both LSTM layers or just one?</p>

<pre><code>model = Sequential()
model.add(Embedding(src_vocab, embedding_dim, input_length=source_steps, mask_zero=True))
model.add(LSTM(embedding_dim,go_backwards=True))
model.add(RepeatVector(target_steps))
model.add(LSTM(embedding_dim, return_sequences=True))
model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))
# compile model
model.compile(optimizer='adam', loss='categorical_crossentropy')
# summarize defined model
model.summary()
</code></pre>

<p>I think I should feed all data forward but I can't find any documentation that convinces me that this is correct.</p>
",Multilingual Language Processing & Language Identification,kera lstm go backwards usage question regarding usage go backwards argument kera lstm model layer documentation layer found question set go backwards flag true still feed input data forward training process example input sentence english normally read fell german translation read ich fiel would feed forward fell ich fiel backwards fell fiel ich training process question question making model prediction data fed forward fell reverse fell question wa trying use model wanted reverse corpus data would set go backwards flag true lstm layer one think feed data forward find documentation convinces correct
How to extract relevant information from article using Python? NLP+RegEX?,"<p>I am writing a program in Python to scan articles for topics that I am interested in. I want to filter out sentences related to the topic from many articles, then create a summary.</p>

<p>I have been using regex but often missing the sentence if it is not using the keywords. Worse than that, I often only get the questions regarding my keywords, leaving out the answers that matter the most.</p>

<p>For example, if I am interested in ""Trump"" in the article below:</p>

<p>""..... What's your view on Trump's foreign policy? I don't think anyone can give a clear answer but I think he is likely to do whatever it takes to contain the public health issue first.
.....""</p>

<p>By using ""Trump"" as the keywords via Regex, I only get the question highlighted, not the answer in the following sentence.</p>

<p>What techniques that I can use to select all relevant info? Open for natural language processing techniques, ideally available from open source packages can help me to do this? </p>

<p>Need not to be a perfect solution. Thank you!</p>
",Multilingual Language Processing & Language Identification,extract relevant information article using python nlp regex writing program python scan article topic interested want filter sentence related topic many article create summary using regex often missing sentence using keywords worse often get question regarding keywords leaving answer matter example interested trump article view trump foreign policy think anyone give clear answer think likely whatever take contain public health issue first using trump keywords via regex get question highlighted answer following sentence technique use select relevant info open natural language processing technique ideally available open source package help need perfect solution thank
Praat Script Segmenter not Outputting Files to Designated Directory,"<p>I'm trying to run a Praat script provided by:
<a href=""http://phonetics.linguistics.ucla.edu/facilities/acoustic/Segmenter.txt"" rel=""nofollow noreferrer"">http://phonetics.linguistics.ucla.edu/facilities/acoustic/Segmenter.txt</a>
However, when I run the script with the correct directories, setting my label$ = ‰ªñ, the tier = 2, and the extension = .wav. (What the Penn Phonetics Lab Chinese Aligner provided)</p>

<p>I get the end statement saying ""All files have been segmented!  Have a nice day!"" but there's nothing in my designated output folder.</p>

<p>What's going wrong?</p>

<pre><code>## This script sections a file into individual vowels, vowels having been marked  in some tier using some uniform label such as ""v"".
##  Each vowel is saved as an individual Praat Sound file with the name of the original file plus a number indicating which vowel is being saved.
## 25 milliseconds is included to either side of the vowel, to make sure that a formant object includes the entire vowel (25 ms. frames).

##  Specify the label you've used to mark your vowels here:
label$ = ""‰ªñ""

##  Specify the tier where the vowels are labeled here:
tier_number = 2

##  Specify the directory where your sound files and accompanying textgrids are located:
directory$ = ""[user pathing]""

##  Specify the directory where you want the segmented vowel sound files to be stored:
outdir$ = ""[user pathing]""

##  Specify what file extension your sound files end in (.wav, .aiff...)
extension$ = "".wav""

clearinfo
Create Strings as file list... list 'directory$'*'extension$'
number_of_files = Get number of strings

for a from 1 to number_of_files
     select Strings list
     current_sound$ = Get string... 'a'
     Read from file... 'directory$''current_sound$'
     current_sound$ = selected$(""Sound"")
     Read from file... 'directory$''current_sound$'.TextGrid
     number_vowels = Count labels... 1 'label$'
     Extract tier... 'tier_number'
     current_tier = selected (""IntervalTier"")
     Get starting points... 'label$'
     starting_points = selected (""PointProcess"")
     select 'current_tier'
     Get end points... 'label$'
     end_points = selected (""PointProcess"")
     for i from 1 to number_vowels
          select 'starting_points'
          start'i' = Get time from index... 'i'
           select 'end_points'
           end'i' = Get time from index... 'i'
     endfor
     select Sound 'current_sound$'
      Edit
      for j from 1 to number_vowels
      editor Sound 'current_sound$'
               start = start'j'
               end = end'j'
               Select... 'start' 'end'
               Extract selection
        endeditor
        Write to binary file... 'outdir$''current_sound$'-'j'.Sound
     endfor
     select all
     minus Strings list
     Remove
endfor
select all
Remove
print All files have been segmented!  Have a nice day!
</code></pre>
",Multilingual Language Processing & Language Identification,praat script segmenter outputting file designated directory trying run praat script provided however run script correct directory setting label tier extension wav penn phonetics lab chinese aligner provided get end statement saying file segmented nice day nothing designated output folder going wrong
"When I use TF-IDF in Natural language processing, it said list is not callable.Can you help me with it?","<p>I got error like this :</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-38-b9ac626e6121&gt; in &lt;module&gt;
      5 
      6 # Fitting TF-IDF to both training and test sets (semi-supervised learning)
----&gt; 7 tfv.fit(list(xtrain) + list(xvalid))
      8 xtrain_tfv =  tfv.transform(xtrain)
      9 xvalid_tfv = tfv.transform(xvalid)

TypeError: 'list' object is not callable
</code></pre>

<p>When I run these codes in python:</p>

<pre><code>tfv = TfidfVectorizer(min_df=3,  max_features=None, 
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,
            stop_words = 'english')

# Fitting TF-IDF to both training and test sets (semi-supervised learning)
tfv.fit(list(xtrain) + list(xvalid))
xtrain_tfv =  tfv.transform(xtrain) 
xvalid_tfv = tfv.transform(xvalid)
</code></pre>

<p>P.S. I also tried to convert the xtrain to list with <code>xtrain.tolist()</code>, but it doesn't work for me either.</p>
",Multilingual Language Processing & Language Identification,use tf idf natural language processing said list callable help got error like run code python p also tried convert xtrain list work either
can i use VaderSentiment to calculate polarity and subjectivity on language other than English?,"<p>i am trying to create a nlp project that calculate the polarity and subjectivity for texts that are not English so i can use 2 tools: <strong>Vader</strong> - <strong>Textblob</strong>.</p>

<p>After i done a lot of researches i found that Vader is more efficient and accurate for social media.</p>

<p>My question is : can i add language to vader in order to calculate socres?
 or is their a package for vader like multilanguage?</p>

<p>For the project i read from csv file and import it to dataframe pandas than pre-process and clean the text  than analyse it to extract the sentiments. </p>

<p>i will appreciate any help.</p>
",Multilingual Language Processing & Language Identification,use vadersentiment calculate polarity subjectivity language english trying create nlp project calculate polarity subjectivity text english use tool vader textblob done lot research found vader efficient accurate social medium question add language vader order calculate socres package vader like multilanguage project read csv file import dataframe panda pre process clean text analyse extract sentiment appreciate help
What are the list of all the algorithms for Natural Language processing (NLP)?,"<p>I know it is a little bit wide topic but all I am looking for is if someone can help me with the list of all the NLP algorithms and when to use them, or maybe a resource which I can refer. for example - **RNN **might be a good use case for a question and answer NLP use case and a simple dense network might just work quite good for binary segregation of documents or identify sarcastic comments from useful news.</p>

<p>I was hoping we can add to the list below from whatever anyone has an idea about will be great. Of course, the below list is not a hard and fast rule and more often than not on various use case we might have to try different things approaches but this is an effort just to have an exhaustive list for NLP algorithms.</p>

<p>Dense layer - useful for document segregation or (sarcastic comments from useful news)
RNN(LSTM) - Good for Question and Answer API</p>
",Multilingual Language Processing & Language Identification,list algorithm natural language processing nlp know little bit wide topic looking someone help list nlp algorithm use maybe resource refer example rnn might good use case question answer nlp use case simple dense network might work quite good binary segregation document identify sarcastic comment useful news wa hoping add list whatever anyone ha idea great course list hard fast rule often various use case might try different thing approach effort exhaustive list nlp algorithm dense layer useful document segregation sarcastic comment useful news rnn lstm good question answer api
Natural Language commands,"<p>I would like to enrich my product with a sort of a ""Power Search"" bar. The idea comes from the Word feature ""Tell me what you want to do"", where Word suggests both actions, searching and help. I assume the actions come from some sort of natural language parsing. But I am not sure exactly what the term to look for is. My product is an online product hosted on AWS, so I would prefer to use a product from AWS if possible and not have to write this myself.</p>

<p>One example of what I would like to do is to map a command like <code>BookResource(string resource, date when)</code> to what is written by the user, who might type something like ""Book room in two weeks"" or maybe just ""two weeks"" or ""next week"". Another use case is also recognizing that the user typed a name and then booking on behalf of that user. The user does not exist in our system, so it is just a case of recognizing that it is a name and not something else.</p>

<p>But my problem is that I don't know what this technology is called. I have tried searching for natural language processing but haven't been able to find what I am looking for. </p>

<p>Can anyone tell me what this is called and also if there one or more services I can use from AWS that offers some of that functionality?</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,natural language command would like enrich product sort power search bar idea come word feature tell want word suggests action searching help assume action come sort natural language parsing sure exactly term look product online product hosted aws would prefer use product aws possible write one example would like map command like written user might type something like book room two week maybe two week next week another use case also recognizing user typed name booking behalf user user doe exist system case recognizing name something else problem know technology called tried searching natural language processing able find looking anyone tell called also one service use aws offer functionality thanks advance
List of common first names for text analysis in R?,"<p>In analysing text, it can be useful to identify names of people in text data. </p>

<p>Objects prepackaged in <a href=""https://www.tidytextmining.com/tidytext.html"" rel=""nofollow noreferrer""><code>tidytext</code></a> include:</p>

<ul>
<li>English negators, modals, and adverbs (<code>nma_words</code>)</li>
<li>Parts of Speech (<code>parts_of_speech</code>)</li>
<li>Sentiments (<code>sentiments</code>), and</li>
<li>Stop Words (see: <code>?stop_words</code>)</li>
</ul>

<p>Is there a similar object in R (or in accessible format elsewhere) containing a canonical list of names? </p>

<p>For reference, here are the existing <code>data.frame</code>s that are supplied with <code>tidytext</code></p>

<pre><code>nma_words
# # A tibble: 44 x 2
# word      modifier
# &lt;chr&gt;     &lt;chr&gt;   
#   1 cannot    negator 
# 2 could not negator 
# 3 did not   negator 
# 4 does not  negator 
# 5 had no    negator 
# 6 have no   negator 
# 7 may not   negator 
# 8 never     negator 
# 9 no        negator 
# 10 not       negator 
# # ‚Ä¶ with 34 more rows


parts_of_speech
# # A tibble: 208,259 x 2
#    word    pos      
#    &lt;chr&gt;   &lt;chr&gt;    
#  1 3-d     Adjective
#  2 3-d     Noun     
#  3 4-f     Noun     
#  4 4-h'er  Noun     
#  5 4-h     Adjective
#  6 a'      Adjective
#  7 a-1     Noun     
#  8 a-axis  Noun     
#  9 a-bomb  Noun     
# 10 a-frame Noun     
# # ‚Ä¶ with 208,249 more rows


sentiments
# # A tibble: 6,786 x 2
#    word        sentiment
#    &lt;chr&gt;       &lt;chr&gt;    
#  1 2-faces     negative 
#  2 abnormal    negative 
#  3 abolish     negative 
#  4 abominable  negative 
#  5 abominably  negative 
#  6 abominate   negative 
#  7 abomination negative 
#  8 abort       negative 
#  9 aborted     negative 
# 10 aborts      negative 
# # ‚Ä¶ with 6,776 more rows


stop_words
# # A tibble: 1,149 x 2
#    word        lexicon
#    &lt;chr&gt;       &lt;chr&gt;  
#  1 a           SMART  
#  2 a's         SMART  
#  3 able        SMART  
#  4 about       SMART  
#  5 above       SMART  
#  6 according   SMART  
#  7 accordingly SMART  
#  8 across      SMART  
#  9 actually    SMART  
# 10 after       SMART  
# # ‚Ä¶ with 1,139 more rows

</code></pre>
",Multilingual Language Processing & Language Identification,list common first name text analysis r analysing text useful identify name people text data object prepackaged include english negators modal adverb part speech sentiment stop word see similar object r accessible format elsewhere containing canonical list name reference existing supplied
Is countvectorizer in sklearn only meant for English?,"<p>I am trying to apply count vectorizer for Telugu and Hindi which are Indic language.But the vectorizer is stemming the words automatically.</p>

<pre><code>count_vect = CountVectorizer()
xv=count_vect.fit_transform(['she is a good girl','‡§µ‡•ã ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§®‡•ç‡§¶‡§∞ ‡§π‡•à','‡∞á‡∞¶‡∞ø ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞≤‡∞æ‡∞°‡∞ø‡∞∑‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¨‡∞æ‡∞≤‡±ç‡∞Ø ‡∞ü‡±Ä‡∞®‡±á‡∞ú‡±ç ‡∞ï‡±Å‡∞∞‡±ç‡∞∞‡∞æ‡∞≥‡±ç‡∞≥‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞¶‡±Ä‡∞®‡∞ø‡∞®‡∞ø ‡∞´‡∞®‡±ç‡∞®‡±Ä‡∞ó‡∞æ ‡∞ö‡±Ç‡∞°‡∞µ‡∞ö‡±ç‡∞ö‡±Å', '‡∞¶‡±ã‡∞™‡∞ø‡∞°‡±Ä ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ‡∞ó‡∞æ ‡∞≤‡±ã‡∞§‡±Å ‡∞≤‡±á‡∞¶‡∞æ ‡∞Ö‡∞ß‡±Å‡∞®‡∞æ‡∞§‡∞®‡∞§ ‡∞≤‡±á‡∞®‡∞ø ‡∞®‡±á‡∞∞‡∞æ‡∞≤‡∞ï‡±Å ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞ó‡±ç‡∞∞‡∞æ‡∞´‡∞ø‡∞ï‡±ç ‡∞ö‡∞ø‡∞ï‡∞ø‡∞§‡±ç‡∞∏‡∞®‡±Å ‡∞ö‡±Ç‡∞°‡∞ü‡∞Ç ‡∞≠‡∞∞‡∞ø‡∞Ç‡∞ö‡∞¶‡∞ó‡∞ø‡∞®‡∞¶‡∞ø'])
count_vect.get_feature_names()
</code></pre>

<p>the output is as follows:</p>

<pre><code>['girl',
 'good',
 'is',
 'she',
 '‡§¶‡§∞',
 '‡§¨‡§π',
 '‡∞Ö‡∞ß',
 '‡∞á‡∞¶',
 '‡∞é‡∞ï',
 '‡∞ö‡∞¶‡∞ó',
 '‡∞°‡∞ü',
 '‡∞°‡∞µ‡∞ö',
 '‡∞§‡∞®‡∞§',
 '‡∞®‡∞¶',
 '‡∞´‡∞®',
 '‡∞≠‡∞∞',
 '‡∞Æ‡∞∞',
 '‡∞∞‡∞Æ',
 '‡∞≤‡∞ï',
 '‡∞µ‡∞ó',
 '‡∞∏‡∞®']
</code></pre>

<p>It is clearly evident that it is stemming the telugu and hindi words automatically, is there any way to avoid that?</p>
",Multilingual Language Processing & Language Identification,countvectorizer sklearn meant english trying apply count vectorizer telugu hindi indic language vectorizer stemming word automatically output follows clearly evident stemming telugu hindi word automatically way avoid
Why Python NLTK does not tag correctly in spanish language?,"<p>I have the following code:</p>

<pre><code>import nltk

sent='El gato est√° bajo la mesa de cristal.'
nltk.pos_tag(word_tokenize(sent), lang='spa')
</code></pre>

<p>But the output is not accurate at all:</p>

<pre><code>[('El', 'NNP'),
 ('gato', 'NN'),
 ('est√°', 'NN'),
 ('bajo', 'NN'),
 ('la', 'FW'),
 ('mesa', 'FW'),
 ('de', 'FW'),
 ('cristal', 'NN'),
 ('.', '.')]
</code></pre>

<p>For instance, <code>es</code> should be categorized as a verb.</p>

<p>If I try the same using English phrase:</p>

<pre><code>import nltk

sent='The cat is under the cristal table.'
nltk.pos_tag(word_tokenize(sent), lang='spa')
</code></pre>

<p>It works ok:</p>

<pre><code>[('The', 'DT'),
 ('cat', 'NN'),
 ('is', 'VBZ'),
 ('under', 'IN'),
 ('the', 'DT'),
 ('cristal', 'NN'),
 ('table', 'NN'),
 ('.', '.')]
</code></pre>

<p>Note that I've downloaded all the nltk resources. Could you tell me what I am missing here so the word tagging is not working in the Spanish language?</p>
",Multilingual Language Processing & Language Identification,python nltk doe tag correctly spanish language following code output accurate instance categorized verb try using english phrase work ok note downloaded nltk resource could tell missing word tagging working spanish language
Convert Scala code to Pyspark :Word2Vec Scala Tranform Routine,"<p>I want to translate following routine from  class [Word2VecModel]<a href=""https://github.com/apache/spark/blob/branch-2.3/mllib/src/main/scala/org/apache/spark/ml/feature/Word2Vec.scala"" rel=""nofollow noreferrer"">https://github.com/apache/spark/blob/branch-2.3/mllib/src/main/scala/org/apache/spark/ml/feature/Word2Vec.scala</a> 
into pyspark.</p>

<pre><code>  override def transform(dataset: Dataset[_]): DataFrame = {
        transformSchema(dataset.schema, logging = true)
        val vectors = wordVectors.getVectors
          .mapValues(vv =&gt; Vectors.dense(vv.map(_.toDouble)))
          .map(identity) // mapValues doesn't return a serializable map (SI-7005)
        val bVectors = dataset.sparkSession.sparkContext.broadcast(vectors)
        val d = $(vectorSize)
        val word2Vec = udf { sentence: Seq[String] =&gt;
          if (sentence.isEmpty) {
            Vectors.sparse(d, Array.empty[Int], Array.empty[Double])
          } else {
            val sum = Vectors.zeros(d)
            sentence.foreach { word =&gt;
              bVectors.value.get(word).foreach { v =&gt;
                BLAS.axpy(1.0, v, sum)
              }
            }
            BLAS.scal(1.0 / sentence.size, sum)
            sum
          }
        }
        dataset.withColumn($(outputCol), word2Vec(col($(inputCol))))
      }
</code></pre>

<p>Can some one help me how to convert this into pyspark equivalent code? I tried to do some of the portion in bits and pieces but not able to put whole.</p>

<p>Like I found BLAS.axpy() inner implementation which I can leverage for pyspark is </p>

<pre><code>axpy(double a, Vector x, Vector y)
    y += a * x
</code></pre>

<p>Same way for BLAS.scal() , the inner logic is</p>

<pre><code>scal(double a, Vector x)
    x = a * x
</code></pre>

<p>For scala idenity function I created same function in pyspark as pyspark dosen't have one.</p>

<pre><code>   def identity(x):
        return x
</code></pre>

<p>I tried to convert following line</p>

<pre><code> val vectors = wordVectors.getVectors
      .mapValues(vv =&gt; Vectors.dense(vv.map(_.toDouble)))
      .map(identity)
</code></pre>

<p>And I came up with this, not sure how to do vv.map(_.toDouble) in pyspark ? Is that right</p>

<pre><code>  vectors_final = model.getVectors().rdd.mapValues(lambda vv: Vectors.dense(vv)).map(lambda x: identity(x))
</code></pre>

<p>Thank you.    </p>
",Multilingual Language Processing & Language Identification,convert scala code pyspark word vec scala tranform routine want translate following routine class word vecmodel pyspark one help convert pyspark equivalent code tried portion bit piece able put whole like found blas axpy inner implementation leverage pyspark way blas scal inner logic scala idenity function created function pyspark pyspark dosen one tried convert following line came sure vv map todouble pyspark right thank
Vectorize document based on vocabulary AND regex,"<p>I am trying to train a text classifier using sklearn's CountVectorizer. The problem is that my training documents have many tokens that are document-specific. So for example there are regular english words that the CountVectorizer.fit_transform method works perfectly well on, but then there are some tokens that are formatted that would fit the regex: '\w\d\d\w\w\d', such as 'd84ke2'. As it is now, the fit_transform method would just take 'd84ke2' at face value and use that as a feature. </p>

<p>I want to be able to use those specific tokens that match that specific regex as their own feature, and leave the regular english words as their own features, since creating a feature such as 'd84ke2' would be useless as this will not come up again in any other document. </p>

<p>I've yet to find a way to do this, much less the ""best"" way. Below is an example of code I have, where you can see that the tokens 'j64ke2', 'r32kl4', 'w35kf9', and 'e93mf9' are all turned into their own features. I repeat for clarity: I want to basically condense these features into one and keep the others. </p>

<pre><code>docs = ['the quick brown j64ke2 jumped over the lazy dogs r32kl4.', 
        'an apple a day keeps the w35kf9 away', 
        'you got the lions share of the e93mf9']

import numpy as np
# define target and target_names  
target_names = ['zero', 'one', 'two']
target = np.array([0, 1, 2])

# Create message bunch. 
from sklearn.utils import Bunch
doc_info = Bunch(data=docs, target=target, target_names=target_names)


# Vectorize training data
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(doc_info.data)

vocab = count_vect.vocabulary_
vocab_keys = list(vocab.keys())
#vocab_vals = list(vocab.values())

X_train_counts = count_vect.transform(doc_info.data)
X = X_train_counts.toarray()        
import pandas as pd
df = pd.DataFrame(X, columns=vocab_keys)
</code></pre>
",Multilingual Language Processing & Language Identification,vectorize document based vocabulary regex trying train text classifier using sklearn countvectorizer problem training document many token document specific example regular english word countvectorizer fit transform method work perfectly well token formatted would fit regex w w w ke fit transform method would take ke face value use feature want able use specific token match specific regex feature leave regular english word feature since creating feature ke would useless come document yet find way much le best way example code see token j ke r kl w kf e mf turned feature repeat clarity want basically condense feature one keep others
How to separate Parts of Speech tags from Sentences and make them into two separate columns one with the raw sentence and one with only the POS tags,"<p>so I have a Bangla Parts of Speech Data-Set which looks like this:</p>

<pre><code>‡¶∞‡¶™‡ßç‡¶§‡¶æ‡¶®‡¶ø\JJ.n.n ‡¶¶‡ßç‡¶∞‡¶¨‡ßç‡¶Ø\NC.0.0.n.n -\PU ‡¶§‡¶æ‡¶ú‡¶æ\JJ.n.n ‡¶ì\CCD.n ‡¶∂‡ßÅ‡¶ï‡¶®‡¶æ\JJ.n.n ‡¶´‡¶≤\NC.0.0.n.n ,\PU ‡¶Ü‡¶´‡¶ø‡¶Æ\NC.0.0.n.n ,\PU ‡¶™‡¶∂‡ßÅ‡¶ö‡¶∞‡ßç‡¶Æ\NC.0.0.n.n ‡¶ì\CCD.n ‡¶™‡¶∂‡¶Æ\NC.0.0.n.n ‡¶è‡¶¨‡¶Ç\CCD.n ‡¶ï‡¶æ‡¶∞‡ßç‡¶™‡ßá‡¶ü\NC.0.0.n.n ‡ß∑\PU
‡¶∞‡¶æ‡¶ú‡¶æ\NP.0.0.n.n ‡¶Æ‡¶π‡¶æ‡¶®‡¶®‡ßç‡¶¶\NP.0.0.n.n ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ‡¶§‡ßá\NC.0.loc.n.n ‡¶§‡ßà‡¶∞‡¶ø\NC.0.0.n.n ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤\VM.3.pst.pft.dcl.fin.n.n.n ‡¶∂‡¶ø‡¶¨\NP.0.0.n.n ‡¶Æ‡¶®‡ßç‡¶¶‡¶ø‡¶∞\NC.0.0.n.n ‡¶ì\CCD.n ‡¶¨‡ßà‡¶∑‡ßç‡¶£‡¶¨‡¶¶‡ßá‡¶∞\NC.0.gen.n.n ‡¶Æ‡¶®‡ßç‡¶¶‡¶ø‡¶∞\NC.0.0.n.n ‡ß∑\PU
‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø\JQ.y.n.nnm ‡¶¨‡ßå‡¶¶‡ßç‡¶ß\JJ.n.n -\PU ‡¶∏‡¶®‡ßç‡¶®‡ßç‡¶Ø‡¶æ‡¶∏‡ßÄ\NC.0.0.n.n ,\PU ‡¶∏‡¶®‡ßç‡¶®‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶®‡ßÄ\NC.0.0.n.n ‡¶¨‡¶æ\CCD.n ‡¶ó‡ßÉ‡¶π‡¶∏‡ßç‡¶•\NC.0.0.n.n -\PU ‡¶Ø‡ßá‡¶á\PRL.sg.0.n.n.y.n ‡¶π‡ßã‡¶ï\VM.3.prs.sim.sbj.fin.n.n.n ‡¶®‡¶æ\CX.y ‡¶ï‡ßá‡¶®\CX.n ,\PU ‡¶™‡ßç‡¶∞‡¶æ‡¶§‡ßá\NC.0.loc.n.n ,\PU ‡¶¶‡ßç‡¶¨‡¶ø‡¶™‡ßç‡¶∞‡¶π‡¶∞‡ßá\NC.0.loc.n.n ,\PU ‡¶Ö‡¶™‡¶∞‡¶æ‡¶π‡ßç‡¶®‡ßá\NC.0.loc.n.n ,\PU ‡¶ì\CCD.n ‡¶∏‡¶®‡ßç‡¶ß‡ßç‡¶Ø‡¶æ‡ßü\NC.0.loc.n.n ‡¶è‡¶á\DAB.0.n ‡¶™‡¶¨‡¶ø‡¶§‡ßç‡¶∞\JJ.n.n ‡¶§‡ßç‡¶∞‡ßü‡ßÄ‡¶ï‡ßá\NC.0.acc.n.n ‡¶™‡ßç‡¶∞‡¶£‡¶æ‡¶Æ\NC.0.0.n.n ‡¶ì\CCD.n ‡¶ß‡ßç‡¶Ø‡¶æ‡¶®\NC.0.0.n.n ‡¶ï‡¶∞‡ßá\VM.0.0.0.0.nfn.n.n.n ,\PU ‡¶§‡¶æ‡¶ï‡ßá\PPR.sg.3.acc.n.n.n.n ‡¶ú‡¶™\NC.0.0.n.n ‡¶ï‡¶∞‡ßá\VM.0.0.0.0.nfn.n.n.n ‡¶è‡¶á\PPR.sg.3.0.n.n.n.n ‡¶¨'‡¶≤‡ßá\VM.0.0.0.0.nfn.n.n.n -\PU ""\PU ‡¶Ü‡¶Æ‡¶ø\PPR.sg.1.0.n.n.n.n ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡ßá‡¶∞\NP.0.gen.n.n ‡¶∂‡¶∞‡¶£‡¶æ‡¶ó‡¶§\JJ.n.n ‡¶π‡¶≤‡¶æ‡¶Æ\VM.3.pst.sim.dcl.fin.n.n.n ‡ß∑\PU
‡¶¨‡¶¶‡¶æ‡¶ì‡¶®‡ßÄ\NP.0.0.n.n ‡¶Ø‡ßá\CX.n ‡¶ñ‡ßÅ‡¶¨\JQ.n.n.nnm ‡¶ñ‡ßÅ‡¶∂‡¶ø\JJ.n.n ‡¶Æ‡¶®‡ßá\NC.0.loc.n.n ‡¶Ö‡¶®‡ßÅ‡¶¨‡¶æ‡¶¶‡ßá‡¶∞\NC.0.gen.n.n ‡¶ï‡¶æ‡¶ú‡ßá\NC.0.loc.n.n ‡¶Ü‡¶§‡ßç‡¶Æ‡¶®‡¶ø‡ßü‡ßã‡¶ó\NC.0.0.n.n ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®\VM.3.pst.pft.dcl.fin.n.n.y ‡¶§‡¶æ\PPR.sg.3.0.n.n.n.n ‡¶®‡ßü\VM.3.prs.sim.dcl.fin.n.y.n ,\PU ‡¶ï‡¶æ‡¶∞‡¶£\CSB.n ‡¶Æ‡¶π‡¶æ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞\NP.0.gen.n.n ‡¶ì‡¶á\DAB.sg.y ‡¶Ö‡¶Ç‡¶∂‡ßá‡¶∞\NC.0.gen.n.n ‡¶¨‡¶ø‡¶∑‡ßü‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞\NC.0.gen.n.n ‡¶∏‡¶ô‡ßç‡¶ó‡ßá\PP.0.n ‡¶§‡¶æ‡¶Å‡¶∞\PPR.sg.3.gen.n.n.n.y ‡¶ó‡ßã‡¶Å‡ßú‡¶æ\JJ.n.n ‡¶ß‡¶∞‡ßç‡¶Æ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏‡ßá‡¶∞\NC.0.gen.n.n ‡¶Ü‡¶¶‡¶™‡ßá\CX.n ‡¶ï‡ßã‡¶®\JQ.n.n.nnm ‡¶Æ‡¶ø‡¶≤\NC.0.0.n.n ‡¶®‡¶æ\CX.y ‡¶•‡¶æ‡¶ï‡¶æ‡ßü\NV.loc.n.n ‡¶§‡¶æ‡¶Å‡¶∞\PPR.sg.3.0.n.n.n.y ‡¶ï‡ßã‡¶®‡¶∞‡¶ï‡¶Æ\JQ.n.n.nnm ‡¶Æ‡¶æ‡¶®‡¶∏‡¶ø‡¶ï\JJ.n.n ‡¶§‡ßÉ‡¶™‡ßç‡¶§‡¶ø\NC.0.0.n.n ‡¶π‡¶§\VM.3.pst.sim.hab.fin.n.n.n ‡¶®‡¶æ\CX.y ,\PU ‡¶∏‡¶Æ‡¶∏‡ßç‡¶§\JQ.n.n.nnm ‡¶™‡¶∞‡¶ø‡¶∂‡ßç‡¶∞‡¶Æ\NC.0.0.n.n ‡¶Ö‡¶∞‡ßç‡¶•‡¶π‡ßÄ‡¶®\JJ.n.n ‡¶Æ‡¶®‡ßá\NC.0.loc.n.n ‡¶π‡¶§\VM.3.pst.sim.hab.fin.n.n.n ‡ß∑\PU
</code></pre>

<p>I have read the data-frame using Pandas:</p>

<pre><code>import pandas as pd

df = pd.read_csv('base_dataset.txt', sep='delimiter', encoding ='utf-8', header=None)

df

OUTPUT: 

0   ‡¶∞‡¶™‡ßç‡¶§‡¶æ‡¶®‡¶ø\JJ.n.n ‡¶¶‡ßç‡¶∞‡¶¨‡ßç‡¶Ø\NC.0.0.n.n -\PU ‡¶§‡¶æ‡¶ú‡¶æ\JJ....
1   ‡¶∞‡¶æ‡¶ú‡¶æ\NP.0.0.n.n ‡¶Æ‡¶π‡¶æ‡¶®‡¶®‡ßç‡¶¶\NP.0.0.n.n ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ‡¶§‡ßá\N...
2   ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø\JQ.y.n.nnm ‡¶¨‡ßå‡¶¶‡ßç‡¶ß\JJ.n.n -\PU ‡¶∏‡¶®‡ßç‡¶®‡ßç‡¶Ø‡¶æ‡¶∏‡ßÄ...
3   ‡¶¨‡¶¶‡¶æ‡¶ì‡¶®‡ßÄ\NP.0.0.n.n ‡¶Ø‡ßá\CX.n ‡¶ñ‡ßÅ‡¶¨\JQ.n.n.nnm ‡¶ñ‡ßÅ‡¶∂‡¶ø\...
4   ‡¶ï‡ßü‡ßá‡¶ï\JQ.n.n.nnm ‡¶¨‡¶ø‡¶ò‡¶æ\CCL.n ‡¶ß‡¶æ‡¶®‡ßÄ\JJ.n.n ‡¶ú‡¶Æ‡¶ø‡¶ì\NC...
5   ‡¶Æ‡¶æ‡¶ü‡¶ø\NC.0.0.n.n ‡¶•‡ßá‡¶ï‡ßá\PP.0.n ‡¶¨‡ßú‡¶ú‡ßã‡¶∞\JQ.n.n.nnm ‡¶ö...
6   ‡¶§‡¶æ‡¶¶‡ßá‡¶∞\PPR.pl.3.gen.n.n.n.n ‡¶ö‡¶æ\NC.0.0.n.n -\PU ...
7   ‡¶®‡¶ï‡¶≤\JJ.n.n ‡¶ì‡¶∑‡ßÅ‡¶ß‡ßá‡¶∞\NC.0.gen.n.n ‡¶ï‡ßá‡¶∞‡¶æ‡¶Æ‡¶§‡¶ø\NC.0.0....
</code></pre>

<p>My Query from you guys: 
I want to separate the Parts of Speech Tags from the Sentences and make two different columns.
Column 1  would be the Bangla Sentences and Column 2 would be the corresponding POS Tags so that I could use it to feed it to a Bi-directional LSTM and train</p>

<pre><code>Here is how the output should look like if I printed the First rows of both Columns:
Column 1 Row 1:
‡¶∞‡¶™‡ßç‡¶§‡¶æ‡¶®‡¶ø ‡¶¶‡ßç‡¶∞‡¶¨‡ßç‡¶Ø - ‡¶§‡¶æ‡¶ú‡¶æ ‡¶ì ‡¶∂‡ßÅ‡¶ï‡¶®‡¶æ ‡¶´‡¶≤, ‡¶Ü‡¶´‡¶ø‡¶Æ, ‡¶™‡¶∂‡ßÅ‡¶ö‡¶∞‡ßç‡¶Æ ‡¶ì ‡¶™‡¶∂‡¶Æ ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶æ‡¶∞‡ßç‡¶™‡ßá‡¶ü ‡ß∑

Column 2 Row 1:
JJ.n.n NC.0.0.n.n PU JJ.n.n CCD.n JJ.n.n NC.0.0.n.n PU NC.0.0.n.n PU NC.0.0.n.n CCD.n NC.0.0.n.n CCD.n NC.0.0.n.n PU

</code></pre>

<p>Update: 
If Bangla is not understandable for you can you show me the procedure for doing it in the English Language? 
For example consider a file containing 1000's of english sentences as such:</p>

<pre><code>People/NNS continue/VBP to/TO inquire/VB the/DT reason/NN for/IN the/DT race/NN for/IN outer/JJ space/NN 

Secretariat/NNP is/VBZ expected/VBN to/TO race/VB tomorrow/NN

</code></pre>

<p>What I basically want is to convert the Raw dataset into a data-set containing two columns; Column 1 containing just the plain sentences without the POS tags and Column 2 containing the labels as in the corresponding POS tags of the sentences in column 1.</p>

<p>I would like to do it for all the sentences in the data-set and I have attached the data-set here: 
<a href=""https://drive.google.com/drive/folders/1ipLMjW4LHLK0uSMvo-FKSZQ9r_ppiMob?usp=sharing"" rel=""nofollow noreferrer"">POS Bangla Data-set</a></p>

<p>Please note I want to keep punctuation such as a comma which is denoted with the tag PU since it plays a role in determining the structure of the sentence.</p>

<p>Any help would be highly appreciated.</p>
",Multilingual Language Processing & Language Identification,separate part speech tag sentence make two separate column one raw sentence one po tag bangla part speech data set look like read data frame using panda query guy want separate part speech tag sentence make two different column column would bangla sentence column would corresponding po tag could use feed bi directional lstm train update bangla understandable show procedure english language example consider file containing english sentence basically want convert raw dataset data set containing two column column containing plain sentence without po tag column containing label corresponding po tag sentence column would like sentence data set attached data set po bangla data set please note want keep punctuation comma denoted tag pu since play role determining structure sentence help would highly appreciated
nltk latent semantic analysis copies the first topics over and over,"<p>This is my first attempt with Natural Language Processing so I started with Latent Semantic Analysis and used <a href=""https://youtu.be/BJ0MnawUpaU"" rel=""nofollow noreferrer"">this tutorial</a> to build the algorithm. After testing it I see that it only classifies the first semantic words and repeats the same terms over and over on top of the other documents.</p>

<p>I tried feeding it the documents found in <a href=""https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"" rel=""nofollow noreferrer"">HERE</a> too and it does exactly the same. Repeating the values of the same topic several times in the other ones.</p>

<p>Could anyone help explain what is happening? I've been searching all over and everything seems exactly like in the tutorials.</p>

<pre><code>testDocs = [
""The Neatest Little Guide to Stock Market Investing"",
""Investing For Dummies, 4th Edition"",
""The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns"",
""The Little Book of Value Investing"",
""Value Investing: From Graham to Buffett and Beyond"",
""Rich Dad's Guide to Investing: What the Rich Invest in, That the Poor and the Middle Class Do Not!"",
""Investing in Real Estate, 5th Edition"",
""Stock Investing For Dummies"",
""Rich Dad's Advisors: The ABC's of Real Estate Investing: The Secrets of Finding Hidden Profits Most Investors Miss"",
                ]
    stopwords = ['and','edition','for','in','little','of','the','to']
    ignorechars = ''',:'!'''

    #First we apply the standard SKLearn algorithm to compare with.
    for element in testDocs:
        #tokens.append(tokenizer.tokenize(element.lower()))
        element = element.lower()

        print(testDocs)

    #Vectorize the features.
    vectorizer = tfdv(max_df=0.5, min_df=2, max_features=8, stop_words='english', use_idf=True)#, ngram_range=(1,3))
    #Store the values in matrix X.
    X = vectorizer.fit_transform(testDocs)
#Apply LSA.
    lsa = TruncatedSVD(n_components=3, n_iter=100)
    lsa.fit(X)

    #Get a list of the terms in the order it was decomposed.
    terms = vectorizer.get_feature_names()
    print(""Terms decomposed from the document: "" + str(terms))
    print()

    #Prints the matrix of concepts. Each number represents how important the term is to the concept and the position relates to the position of the term.
    print(""Number of components in element 0 of matrix of components:"")
    print(lsa.components_[0])
    print(""Shape: "" + str(lsa.components_.shape))
    print()
    for i, comp in enumerate(lsa.components_):
        #Stick each of the terms to the respective components. Zip command creates a tuple from 2 components.
        termsInComp = zip(terms, comp)
        #Sort the terms according to...
        sortedTerms = sorted(termsInComp, key=lambda x: x[1], reverse=True)
        print(""Concept %d"", i)
        for term in sortedTerms:
            print(term[0], end=""\t"")
        print()
</code></pre>
",Multilingual Language Processing & Language Identification,nltk latent semantic analysis copy first topic first attempt natural language processing started latent semantic analysis used tutorial build algorithm testing see classifies first semantic word repeat term top document tried feeding document found doe exactly repeating value topic several time one could anyone help explain happening searching everything seems exactly like tutorial
Distinguishing well formed English sentences from &quot;word salad&quot;,"<p>I'm looking for a library easily usable from C++, Python or F#, which can distinguish well formed English sentences from ""word salad"". I tried <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">The Stanford Parser</a> and unfortunately, it parsed this:</p>

<pre><code>Some plants have with done stems animals with exercise that to predict?
</code></pre>

<p>without a complaint. I'm not looking for something very sophisticated, able to handle all possible corner cases. I only need to filter out an obvious nonsense.</p>
",Multilingual Language Processing & Language Identification,distinguishing well formed english sentence word salad looking library easily usable c python f distinguish well formed english sentence word salad tried stanford parser unfortunately parsed without complaint looking something sophisticated able handle possible corner case need filter obvious nonsense
How to remove these footnotes from text,"<p>Alright so I have minimal experience with RStudio, I've been googling this for hours now and I'm fed up-- I don't care about the pride of figuring it out on my own anymore, I just want it done. I want to do some stuff with Canterbury Tales-- <a href=""https://www.gutenberg.org/files/22120/22120-h/22120-h.htm"" rel=""nofollow noreferrer"">the Middle English version on Gutenberg.</a> </p>

<p>Downloaded the plaintext, trimmed out the meta data, etc but it's chock-full of ""helpful"" footnotes and I can't figure out how to cut them out.  EX:</p>

<p>""And shortly, whan the sonne was to reste, </p>

<p>So hadde I spoken with hem everichon,</p>

<p>That I was of hir felawshipe anon, </p>

<p>And made forward erly for to ryse,</p>

<p>To take our wey, ther as I yow devyse.</p>

<pre><code>19. Hn. Bifel; E. Bifil.   23. E. were; _rest_ was.   24. E. Hn.
compaignye.   26, 32. E. felaweshipe.   Hl. pilgryms; E. pilgrimes.
34. E. oure

But natheles, whyl I have tyme and space,...""
</code></pre>

<p>I at least have the vague notion that this is a grep/regex puzzle. Looking at the text in TextEdit, each bundle of footnotes is indented by 4 spaces, and the next verse starts with a capitalized word indented by (edit: 4 spaces as well). </p>

<p>So I tried downloading the package qdap and using the rm_between function to specify removal of text between four spaces and a number; and two spaces and a capital letter (""    [0-9]"",""  ""[A-Z]"") to no avail. </p>

<p>I mean, this isn't nearly as simple as ""make the text lowercase and remove all the numbers dur-hur"" which all the tutorials are so helpfully offering. But I'm assuming this is a rather common thing that people have to do when dealing with big texts. Can anyone help me? Or do I have to go into textedit and just manually delete all the footnotes?</p>

<p>EDIT: I restarted the workspace today and all I have is a scan of the file, each line stored in a character vector, with the Gutenburg metadata trimmed out:</p>

<pre><code>text&lt;- scan(""thefilepath.txt, what = ""character"", sep = ""\n"")  
start &lt;-which(text==""GROUP A. THE PROLOGUE."")  
end &lt;-which(text==""""God bringe us to the Ioye . that ever schal be!"")  
cant.lines.v &lt;- text[start:end]

</code></pre>

<p>And that's it so far. Eventually I will</p>

<pre><code>cant.v&lt;- paste(cant.lines.v, collapse="" "")
</code></pre>

<p>And then <strong>strsplit</strong> and <strong>unlist</strong> into a vector of individual words-- but I'm assuming, to get rid of the footnotes, I need to <strong>gsub</strong> and replace with blank space, and that will be easier with each separate line? I just don't know how to encode the pattern I need to cut. I believe it is 4 spaces followed by a number, then continuing on until you get to 4 spaces followed by a capitalized word and a second word w/o numbers and special characters and punctuation. </p>

<p>I hope that I'm providing enough information, I'm not well-versed in this but I am looking to become so...thanks in advance.</p>
",Multilingual Language Processing & Language Identification,remove footnote text alright minimal experience rstudio googling hour fed care pride figuring anymore want done want stuff canterbury tale middle english version gutenberg downloaded plaintext trimmed meta data etc chock full helpful footnote figure cut ex shortly whan sonne wa reste hadde spoken hem everichon wa hir felawshipe anon made forward erly ryse take wey ther yow devyse least vague notion grep regex puzzle looking text textedit bundle footnote indented space next verse start capitalized word indented edit space well tried downloading package qdap using rm function specify removal text four space number two space capital letter z avail mean nearly simple make text lowercase remove number dur hur tutorial helpfully offering assuming rather common thing people dealing big text anyone help go textedit manually delete footnote edit restarted workspace today scan file line stored character vector gutenburg metadata trimmed far eventually strsplit unlist vector individual word assuming get rid footnote need gsub replace blank space easier separate line know encode pattern need cut believe space followed number continuing get space followed capitalized word second word w number special character punctuation hope providing enough information well versed looking become thanks advance
Match numbers before particular Chinese words,"<p>How can I use stringr to match number before particular Chinese words? For example 2020Âπ¥1Êúà4Êó• (4 Jan 2020)? I want to get something like this: </p>

<pre><code>[1] 2020 1 4
</code></pre>
",Multilingual Language Processing & Language Identification,match number particular chinese word use stringr match number particular chinese word example jan want get something like
"Regex pattern to find all matches for suffixes, end quotes and words in English POS tagged corpus","<p>I'm working on an NLP project where I have been given a POS tagged dataset of sentences to work with. The format of the dataset (shall be providing example sentences too) is</p>
<p>('word', 'pos_tag')</p>
<p>unless if the word has a single quote (affixes like 're,'s,n't and also '' for end quotes) in which case the format is</p>
<p>(&quot;word&quot;, &quot;pos_tag&quot;)</p>
<p>The code segment I am using to process this data set is as follows</p>
<pre><code>def corpus_reader(filepath):
 pattern = '\(\'(\w+)\', |(?&lt;=\&quot;).*?\&quot;, ' 
 sentences = []
 with open( filepath ) as f:
     corpus = f.readlines()

 for line in corpus:
    temp = re.findall( pattern, line )
    sentences.append( temp )

return sentences
</code></pre>
<p>The pattern consists of two patterns cond1|cond2 to detect.</p>
<p>cond1 matches and extracts all the words in the corpus.</p>
<p>cond2 is meant to match '', n't, 's and 're which are enclosed within double quotes like i have mentioned before, but the second condition doesn't work to do that.</p>
<p>Desired result is for a list of all the pos tagged tokens</p>
<p><strong>Could someone please provide the correct regex pattern to use to detect the cases I have mentioned?</strong></p>
<p>Here are example sentences that are to be parsed that contain 're, n't, 's and ''</p>
<blockquote>
<p>[('We', 'PRP'), (&quot;'re&quot;, 'VBP'), ('talking', 'VBG'), ('about', 'IN'), ('years', 'NNS'), ('ago', 'IN'), ('before', 'IN'), ('anyone', 'NN'), ('heard', 'VBD'), ('of', 'IN'), ('asbestos', 'NN'), ('having', 'VBG'), ('any', 'DT'), ('questionable', 'JJ'), ('properties', 'NNS'), ('.', '.')]</p>
<p>[('<code>', '</code>'), ('We', 'PRP'), ('have', 'VBP'), ('no', 'DT'), ('useful', 'JJ'), ('information', 'NN'), ('on', 'IN'), ('whether', 'IN'), ('users', 'NNS'), ('are', 'VBP'), ('at', 'IN'), ('risk', 'NN'), (',', ','), (&quot;''&quot;, &quot;''&quot;), ('said', 'VBD'), ('<em>T</em>-1', '-NONE-'), ('James', 'NNP'), ('A.', 'NNP'), ('Talcott', 'NNP'), ('of', 'IN'), ('Boston', 'NNP'), (&quot;'s&quot;, 'POS'), ('Dana-Farber', 'NNP'), ('Cancer', 'NNP'), ('Institute', 'NNP'), ('.', '.')]</p>
<p>[('The', 'DT'), ('U.S.', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('few', 'JJ'), ('industrialized', 'VBN'), ('nations', 'NNS'), ('that', 'WDT'), ('<em>T</em>-7', '-NONE-'), ('does', 'VBZ'), (&quot;n't&quot;, 'RB'), ('have', 'VB'), ('a', 'DT'), ('higher', 'JJR'), ('standard', 'NN'), ('of', 'IN'), ('regulation', 'NN'), ('for', 'IN'), ('the', 'DT'), ('smooth', 'JJ'), (',', ','), ('needle-like', 'JJ'), ('fibers', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('crocidolite', 'NN'), ('that', 'WDT'), ('<em>T</em>-1', '-NONE-'), ('are', 'VBP'), ('classified', 'VBN'), ('*-5', '-NONE-'), ('as', 'IN'), ('amphobiles', 'NNS'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Brooke', 'NNP'), ('T.', 'NNP'), ('Mossman', 'NNP'), (',', ','), ('a', 'DT'), ('professor', 'NN'), ('of', 'IN'), ('pathlogy', 'NN'), ('at', 'IN'), ('the', 'DT'), ('University', 'NNP'), ('of', 'IN'), ('Vermont', 'NNP'), ('College', 'NNP'), ('of', 'IN'), ('Medicine', 'NNP'), ('.', '.')]</p>
<p>[('<code>', '</code>'), ('What', 'WP'), ('<em>T</em>-14', '-NONE-'), ('matters', 'VBZ'), ('is', 'VBZ'), ('what', 'WP'), ('advertisers', 'NNS'), ('are', 'VBP'), ('paying', 'VBG'), ('<em>T</em>-15', '-NONE-'), ('per', 'IN'), ('page', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('that', 'DT'), ('department', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('doing', 'VBG'), ('fine', 'RB'), ('this', 'DT'), ('fall', 'NN'), (',', ','), (&quot;''&quot;, &quot;''&quot;), ('said', 'VBD'), ('<em>T</em>-1', '-NONE-'), ('Mr.', 'NNP'), ('Spoon', 'NNP'), ('.', '.')]</p>
</blockquote>
<p>Thanks and gratitude to all attempts of answering and helping</p>
",Multilingual Language Processing & Language Identification,regex pattern find match suffix end quote word english po tagged corpus working nlp project given po tagged dataset sentence work format dataset shall providing example sentence word po tag unless word ha single quote affix like n also end quote case format word po tag code segment using process data set follows pattern consists two pattern cond cond detect cond match extract word corpus cond meant match n enclosed within double quote like mentioned second condition work desired result list po tagged token could someone please provide correct regex pattern use detect case mentioned example sentence parsed contain n prp vbp talking vbg year nns ago anyone nn heard vbd asbestos nn vbg dt jj property nns prp vbp dt useful jj information nn whether user nns vbp risk nn said vbd none james nnp nnp talcott nnp boston nnp po dana farber nnp cancer nnp institute nnp dt u nnp vbz one cd dt jj industrialized vbn nation nns wdt none doe vbz n rb vb dt higher jjr standard nn regulation nn dt smooth jj needle like jj fiber nns jj crocidolite nn wdt none vbp classified vbn none amphobiles nns according vbg brooke nnp nnp mossman nnp dt professor nn pathlogy nn dt university nnp vermont nnp college nnp medicine nnp wp none matter vbz vbz wp advertiser nns vbp paying vbg none per page nn cc dt department nn prp vbp vbg fine rb dt fall nn said vbd none mr nnp spoon nnp thanks gratitude attempt answering helping
Pretrained Entity Linking model for Wikipedia/WikiData,"<p>I have some 34000 restaurant reviews in English to perform entity linking on and I'm looking for some scalable solution.</p>

<p>Is there some pretrained model for entity linking to Wikipedi/WikiData KB?</p>

<ul>
<li>Spacy recently started to support Entity Linking <a href=""https://spacy.io/usage/linguistic-features#entity-linking"" rel=""nofollow noreferrer"">1</a>,<a href=""https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking"" rel=""nofollow noreferrer"">2</a>,<a href=""https://github.com/explosion/spaCy/issues/4511"" rel=""nofollow noreferrer"">3</a> but they do not provide pretrained model for that and some people report <a href=""https://stackoverflow.com/questions/58541373/training-times-for-spacy-entity-linking-model"">training takes at least a week</a>.</li>
<li><a href=""http://wikifier.org/info.html#wikification"" rel=""nofollow noreferrer"">Wikifier</a> has a pretrained model that can be used for entity linking via REST service for that but I don't know what's their Rate Limit for my number of texts and how it scales. Hence, I would prefer offline solution.</li>
</ul>
",Multilingual Language Processing & Language Identification,pretrained entity linking model wikipedia wikidata restaurant review english perform entity linking looking scalable solution pretrained model entity linking wikipedi wikidata kb spacy recently started support entity linking provide pretrained model people report wikifier ha pretrained model used entity linking via rest service know rate limit number text scale hence would prefer offline solution
Word classification algorithm pro cons,"<p>As for college project I am required to build a software that, given some comments concerning a virtual construction site, detects its actual state (<code>just started</code>, <code>in construction</code>, <code>terminated</code>).</p>

<p>For example, given the comments:</p>

<ul>
<li>""Happy to hear we can walk through the English Channel bridge""</li>
<li>""Yesterday I went to the newly built bridge to have a trip to France with my friends""</li>
<li>""They just finished the site and there are already cracks in the 5th miles. What a letdown!""</li>
</ul>

<p>The system should detect that the ""English Channel bridge"" construction site has ended.</p>

<p>At the moment I'm trying to choose what word classification algorithm to use for this project. I searched online looking for the best classification algorithm to use. I've read about <code>SVC</code> but, since I'm not really an expert in this field, I am unsure about the compliance/goodness of SVC with my scenario.</p>

<p>What I'm trying to obtain is not the solution to my problem, but a list of available algorithms with their pros and cons.</p>
",Multilingual Language Processing & Language Identification,word classification algorithm pro con college project required build software given comment concerning virtual construction site detects actual state example given comment happy hear walk english channel bridge yesterday went newly built bridge trip france friend finished site already crack th mile letdown system detect english channel bridge construction site ha ended moment trying choose word classification algorithm use project searched online looking best classification algorithm use read since really expert field unsure compliance goodness svc scenario trying obtain solution problem list available algorithm pro con
NLP - linguistic consistency analysis,"<p>I hope you can help me :).</p>

<p>I am working for a translation company. </p>

<p>As you know, every translation consists in splitting the original text into small segments and then re-joining them into the final product.</p>

<p>In other words, the segments are considered as ""<strong>translation units</strong>"".</p>

<p>Often, especially for large documents, the translators make some linguistic consistency errors, I try to explain it with an example.</p>

<p>In Spanish, you can use ""tu"" or ""usted"", depending on the context, and this determines the formality-informality tone of the sentence.</p>

<p>So, if you consider these two sentences of a document:</p>

<pre><code>Lara, te has lavado las manos? (TU)

Lara usted se lav√≤ las manos? (USTED)
</code></pre>

<p>They are BOTH correct, but if you consider the whole document, there is a linguistic inconsistency. </p>

<p>I am studying NLP basic in my spare time, and I am figuring out how to create a tool to perform a linguistic consistency analysis on a set of sentences.  </p>

<p>I am looking in particular at <strong>Standford CoreNLP</strong> (I prefer Java to Python). 
I guess that I need some linguistic tools to perform verb analysis first of all. And naturally, the tool would be able to work with different languages (EN, IT, ES, FR, PT).</p>

<p>Anyone can help me to figure out how to start this? </p>

<p>Any help would be appreciated,</p>

<p>thanks in advance!</p>
",Multilingual Language Processing & Language Identification,nlp linguistic consistency analysis hope help working translation company know every translation consists splitting original text small segment joining final product word segment considered translation unit often especially large document translator make linguistic consistency error try explain example spanish use tu usted depending context determines formality informality tone sentence consider two sentence document correct consider whole document linguistic inconsistency studying nlp basic spare time figuring create tool perform linguistic consistency analysis set sentence looking particular standford corenlp prefer java python guess need linguistic tool perform verb analysis first naturally tool would able work different language en e fr pt anyone help figure start help would appreciated thanks advance
How to build lemmatizer using Fasttext,"<p>I have a huge amount of words (4M) in Arabic dialect with their correspending lemmas and i want to build a lemmatizer for new words not in that data by leveraging it. The question is how to use FastText to build this lemmatizer?</p>
",Multilingual Language Processing & Language Identification,build lemmatizer using fasttext huge amount word arabic dialect correspending lemma want build lemmatizer new word data leveraging question use fasttext build lemmatizer
How to identify Nouns in string and capitalize them?,"<p>I have simple plain text in lower case and no punctuation. Is there any library which can help changing the upper case like where Nouns are or where required? Like names after Mr. and so.? Any solution or guiding hint can be very helpful.
for example:
in english language in English .. in plain text, at several places are names. and several names needed to be capitalized. like </p>

<pre><code>mr. john is living in canada
</code></pre>

<p>to</p>

<pre><code>Mr. John is living in Canada
</code></pre>
",Multilingual Language Processing & Language Identification,identify noun string capitalize simple plain text lower case punctuation library help changing upper case like noun required like name mr solution guiding hint helpful example english language english plain text several place name several name needed capitalized like
Fasttext aligned word vectors for translating homographs,"<p><a href=""https://en.wikipedia.org/wiki/Homograph"" rel=""nofollow noreferrer"">Homograph</a> is a word that shares the same written form as another word but has a different meaning, like <strong>right</strong> in the sentences below:</p>

<ul>
<li>success is about making the <strong>right</strong> decisions.</li>
<li>Turn <strong>right</strong> after the traffic light</li>
</ul>

<p>The English word ""right"", in the first case is translated to Swedish as ""r√§tt"" and to ""h√∂ger"" in the second case. The correct translation is possible by looking at the context (surrounding words).</p>

<p><strong>Question 1.</strong> I wonder if fasttext aligned word embedding can come to help for translating these homograph words or words with several possible translations into another language?</p>

<p><strong>[EDIT]</strong> The goal is <strong>not</strong> to query the model for the right translation. The goal is to <strong>pick</strong> the right translation when the following information is given: </p>

<ul>
<li>the two (or several) possible translations options in the target language like ""r√§tt"" and ""h√∂ger""</li>
<li>the surrounding words in the source language </li>
</ul>

<p><strong>Question 2.</strong> I loaded the <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">english pre-trained vectors model</a> and the <a href=""https://fasttext.cc/docs/en/aligned-vectors.html"" rel=""nofollow noreferrer"">English <strong>aligned</strong> vector model</a>. While both were trained on Wikipedia articles, I noticed that the distances between two words were sort of preserved but the size of the dataset files (wiki.en.vec vs wiki.en.align.vec) are noticeably different (1GB). Wouldn't it make sense if we only use the aligned version? What information is not captured by the aligned dataset?</p>
",Multilingual Language Processing & Language Identification,fasttext aligned word vector translating homograph homograph word share written form another word ha different meaning like right sentence success making right decision turn right traffic light english word right first case translated swedish r tt h ger second case correct translation possible looking context surrounding word question wonder fasttext aligned word embedding come help translating homograph word word several possible translation another language edit goal query model right translation goal pick right translation following information given two several possible translation option target language like r tt h ger surrounding word source language question loaded english pre trained vector model english aligned vector model trained wikipedia article noticed distance two word sort preserved size dataset file wiki en vec v wiki en align vec noticeably different gb make sense use aligned version information captured aligned dataset
Unigram Gives Better Results than Ngram for Language Identification,"<p>I have a school project which consists of identifying each language of a tweet from a dataset of tweets. The dataset contains tweets in Spanish, Portuguese, English, Basque, Galician and Catalan. The task is to implement a language identification model using unigrams, bigrams and trigrams and to analyze the efficiency of each model.</p>

<p>I understand the concepts of ngrams and I understand that the languages are somewhat similar (hence it's not that trivial of a task), but what I don't understand is that I'm getting better results for unigrams than bigrams and I'm getting better results for bigrams than trigrams.</p>

<p>I can't comprehend how is that possible since I expected a better efficiency for bigrams and trigrams.</p>

<p>Could you help me shed some light on why is this happening?</p>

<p>Thank you for your time.</p>
",Multilingual Language Processing & Language Identification,unigram give better result ngram language identification school project consists identifying language tweet dataset tweet dataset contains tweet spanish portuguese english basque galician catalan task implement language identification model using unigrams bigram trigram analyze efficiency model understand concept ngrams understand language somewhat similar hence trivial task understand getting better result unigrams bigram getting better result bigram trigram comprehend possible since expected better efficiency bigram trigram could help shed light happening thank time
I am searching for the English thesaurus for all words we used id English to add to elastic search synonym list. Where can I get it?,"<p>I want list of synonyms words for all English words..where can I get this? I need to add then in Elastic search synonym list to get more efficient results.</p>
",Multilingual Language Processing & Language Identification,searching english thesaurus word used id english add elastic search synonym list get want list synonym word english word get need add elastic search synonym list get efficient result
Computer AI algorithm to write sentences?,"<p>I am searching for information on algorithms to process text sentences or to follow a structure when creating sentences that are valid in a normal human language such as English. I would like to know if there are projects working in this field that I can go learn from or start using.</p>

<p>For example, if I gave a program a noun, provided it with a thesaurus (for related words) and part-of-speech (so it understood where each word belonged in a sentence) - could it create a random, valid sentence?</p>

<p>I'm sure there are many sub-sections of this kind of research so any leads into this would be great.</p>
",Multilingual Language Processing & Language Identification,computer ai algorithm write sentence searching information algorithm process text sentence follow structure creating sentence valid normal human language english would like know project working field go learn start using example gave program noun provided thesaurus related word part speech understood word belonged sentence could create random valid sentence sure many sub section kind research lead would great
NLP Sentiment analysis - basic guidelines,"<p>I am doing my first project in the NLP domain which is sentiment analysis of a dataset with ~250 tagged english data points/sentences. The dataset is reviews of a pharmaceutical product having positive, negative or neutral tags. I have worked with numeric data in supervised learning for 3 years but NLP is unchartered territory for me. So I want to know the best pre-processing techniques and the steps that I need to do that are best suited to my problem. A guideline from an NLP expert would be much appreciated!</p>
",Multilingual Language Processing & Language Identification,nlp sentiment analysis basic guideline first project nlp domain sentiment analysis dataset tagged english data point sentence dataset review pharmaceutical product positive negative neutral tag worked numeric data supervised learning year nlp unchartered territory want know best pre processing technique step need best suited problem guideline nlp expert would much appreciated
How to use NLP to find out if two words have the same definition?,"<p>If you see another question with the same wording as this, please ignore, it has unnecessary code</p>

<p>I made a very basic chatbot/program in Python that simulates ordering from a restaurant. I was wondering if there is any way to use Natural Language Processing (NLP) to find out if two words are the same? For example, how can NLP find out that ""I'm feeling bad"" means the same thing as ""I'm feeling horrible"" ? </p>
",Multilingual Language Processing & Language Identification,use nlp find two word definition see another question wording please ignore ha unnecessary code made basic chatbot program python simulates ordering restaurant wa wondering way use natural language processing nlp find two word example nlp find feeling bad mean thing feeling horrible
Translating big amount of csv file (Flickr8k_text dataset) to &#39;Nepali&#39; Language in python,"<p>I've been working in a Image Captioning Project in 'Nepali Language'. For Dataset part I tried to translate all the English captions text to Nepali of the Flickr8k dataset. For this I'm using python <code>translate</code> tool as</p>

<pre><code>dataset = pd.read_csv('/content/gdrive/My Drive/out.csv',delimiter = '\t')
dataset.drop('Unnamed: 0',axis = 1)
def trans(x):
    translator= Translator(to_lang=""ne"")
    return translator.translate(x)
dataset['caption'] = dataset['caption'].apply(trans)
print('done')
</code></pre>

<p>But it only translated 130 rows of captions to Nepali language and then all other texts are translated as</p>

<p>MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  23 HOURS 24 MINUTES 38 SECONDSVISIT  TO TRANSLATE MORE</p>

<p><strong>Is there's any way of translating all the texts at once??</strong></p>

<p>I've tried googletrans too but it also fails due to frequent request on API</p>

<p>Note: the dataset contains 40458 rows with English sentences in caption column.</p>

<p>It will be great help if there's any way to translate all the text and Thanks in Advance :)</p>
",Multilingual Language Processing & Language Identification,translating big amount csv file flickr k text dataset nepali language python working image captioning project nepali language dataset part tried translate english caption text nepali flickr k dataset using python tool translated row caption nepali language text translated mymemory warning used available free translation today next available hour minute secondsvisit translate way translating text tried googletrans also fails due frequent request api note dataset contains row english sentence caption column great help way translate text thanks advance
Creating a dictionary which contains English words,"<p>I want a text file which contains commonly occuring English words but abbreviations or acronyms should not be present there. Is it possible to get that dictionary?
Any help is appreciated!!</p>
",Multilingual Language Processing & Language Identification,creating dictionary contains english word want text file contains commonly occuring english word abbreviation acronym present possible get dictionary help appreciated
Natural Language Processing for fast detection of nouns,"<p>I have long texts from which I need to extract nouns. I use <code>spaCy</code> as</p>

<pre><code>nlp = spacy.load(""en_core_web_lg"") # for better name entity detection
doc = nlp(text)
for token in doc:
    if token.tag_=='NN' or token.tag_=='NNP':
        # store token.lemma_
for ent in doc.ents:
    # store ent.text
</code></pre>

<p>However, it is very slow, as <code>spaCy</code> does the full analysis, which I do not need.</p>

<p>can I speed up <code>spaCy</code> to do this specific job?</p>
",Multilingual Language Processing & Language Identification,natural language processing fast detection noun long text need extract noun use however slow doe full analysis need speed specific job
What is Natural Language Processing Doing Exactly in This Code?,"<p>I am new to natural language processing and I want to use it to write a news aggregator(in Node.js in my case). Rather than just use a prepackage framework, I want to learn the nuts and bolts and I am starting with the NLP portion. I found this one tutorial that has been the most helpful so far:</p>

<p><a href=""http://www.p-value.info/2012/12/howto-build-news-aggregator-in-100-loc.html"" rel=""nofollow"">http://www.p-value.info/2012/12/howto-build-news-aggregator-in-100-loc.html</a></p>

<p>In it, the author gets the RSS feeds and loops through them looking for the elements(or fields) <code>title</code> and <code>description</code>. I know Python and understand the code. But what I don't understand is what NLP is doing here with <code>title</code> and <code>description</code> under the hood(besides scraping and tokenizing, which is apparent...and those tasks don't need a NLP).</p>

<pre><code>import feedparser
import nltk
corpus = []
titles=[]
ct = -1
for feed in feeds:
    d = feedparser.parse(feed)
    for e in d['entries']:
       words = nltk.wordpunct_tokenize(nltk.clean_html(e['description']))
       words.extend(nltk.wordpunct_tokenize(e['title']))
       lowerwords=[x.lower() for x in words if len(x) &gt; 1]
       ct += 1
       print ct, ""TITLE"",e['title']
       corpus.append(lowerwords)
       titles.append(e['title'])
</code></pre>
",Multilingual Language Processing & Language Identification,natural language processing exactly code new natural language processing want use write news aggregator node j case rather use prepackage framework want learn nut bolt starting nlp portion found one tutorial ha helpful far author get r feed loop looking element field know python understand code understand nlp hood besides scraping tokenizing apparent task need nlp
Algorithm to find english words meanings belonging (parent) categories,"<p>Lets say we have categories like below. </p>

<p>How would you check any given word and find which category this word belongs to?</p>

<p>Is there any API or ready solution for this?</p>

<pre><code>main_categories:[
""Abstract "",
""Animals/Wildlife "",
""Arts "",
""Backgrounds/Textures "",
""Beauty/Fashion "",
""Buildings/Landmarks "",
""Business/Finance"",
""Celebrities "",
""Education "",
""Food and drink "",
""Healthcare/Medical "",
""Holidays "",
""Industrial "",
""Interiors "",
""Miscellaneous "",
""Nature "",
""Objects "",
""Parks/Outdoor "",
""People "",
""Religion"",
""Sports/Recreation "",
""Technology Transportation""
  ]
</code></pre>
",Multilingual Language Processing & Language Identification,algorithm find english word meaning belonging parent category let say category like would check given word find category word belongs api ready solution
Transformers architecture for machine translation,"<p>I have adapted the base transformer model, for my corpus of aligned Arabic-English sentences. As such the model has trained for 40 epochs and accuracy (SparseCategoricalAccuracy) is improving by a factor of 0.0004 for each epoch.
To achieve good results, my estimate is to attain final accuracy anywhere around 0.5 and accuracy after 40 epochs is 0.0592.</p>

<p>I am running the model on the tesla 2 p80 GPU. Each epoch is taking ~2690 sec.
This implies I need at least 600 epochs and training time would be 15-18 days.
Should I continue with the training or is there something wrong in the procedure as the base transformer in the research paper was trained on an ENGLISH-FRENCH corpus?</p>

<p>Key highlights:</p>

<ol>
<li>Byte-pair(encoding) of sentences</li>
<li>Maxlen_len =100</li>
<li>batch_size= 64</li>
<li>No pre-trained embeddings were used.</li>
</ol>
",Multilingual Language Processing & Language Identification,transformer architecture machine translation adapted base transformer model corpus aligned arabic english sentence model ha trained epoch accuracy sparsecategoricalaccuracy improving factor epoch achieve good result estimate attain final accuracy anywhere around accuracy epoch running model tesla p gpu epoch taking sec implies need least epoch training time would day continue training something wrong procedure base transformer research paper wa trained english french corpus key highlight byte pair encoding sentence maxlen len batch size pre trained embeddings used
Natural Language Processing for Date/Time in Swift,"<p>Suppose when a user enters <em>buy milk tomorrow</em> in the text field, it should be classified as:</p>

<ol>
<li>task: buy milk</li>
<li>due date: tomorrow</li>
<li>tag: food</li>
</ol>

<p>Is there any recommended swift library or web api that supports this?</p>
",Multilingual Language Processing & Language Identification,natural language processing date time swift suppose user enters buy milk tomorrow text field classified task buy milk due date tomorrow tag food recommended swift library web api support
Convert a list of words to a list of integers in scikit-learn,"<p>I want to convert a list of words to a list of integers in scikit-learn, and do so for a corpus that consists of a list of lists of words. E.g. the corpus can be a bunch of sentences. </p>

<p>I can do as follows using  <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"" rel=""nofollow""><code>sklearn.feature_extraction.text.CountVectorizer</code></a>, but is there any simpler way? I suspect I may be missing some CountVectorizer functionalities, as it's a common pre-processing step in natural language processing. In this code I first fit CountVectorizer, then I have to iterate over each words of each list of words to generate the list of integers.</p>

<pre><code>import sklearn
import sklearn.feature_extraction
import numpy as np

def reverse_dictionary(dict):
    '''
    http://stackoverflow.com/questions/483666/python-reverse-inverse-a-mapping
    '''
    return {v: k for k, v in dict.items()}

vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=1)

corpus = ['This is the first document.',
        'This is the second second document.',
        'And the third one.',
        'Is this the first document? This is right.',]

X = vectorizer.fit_transform(corpus).toarray()

tokenizer = vectorizer.build_tokenizer()
output_corpus = []
for line in corpus: 
    line = tokenizer(line.lower())
    output_line = np.empty_like(line, dtype=np.int)
    for token_number, token in np.ndenumerate(line):
        output_line[token_number] = vectorizer.vocabulary_.get(token) 
    output_corpus.append(output_line)
print('output_corpus: {0}'.format(output_corpus))

word2idx = vectorizer.vocabulary_
print('word2idx: {0}'.format(word2idx))

idx2word = reverse_dictionary(word2idx)
print('idx2word: {0}'.format(idx2word))
</code></pre>

<p>outputs:</p>

<pre><code>output_corpus: [array([9, 3, 7, 2, 1]), # 'This is the first document.'
                array([9, 3, 7, 6, 6, 1]), # 'This is the second second document.'
                array([0, 7, 8, 4]), # 'And the third one.'
                array([3, 9, 7, 2, 1, 9, 3, 5])] # 'Is this the first document? This is right.'
word2idx: {u'and': 0, u'right': 5, u'third': 8, u'this': 9, u'is': 3, u'one': 4,
           u'second': 6, u'the': 7, u'document': 1, u'first': 2}
idx2word: {0: u'and', 1: u'document', 2: u'first', 3: u'is', 4: u'one', 5: u'right', 
           6: u'second', 7: u'the', 8: u'third', 9: u'this'}
</code></pre>
",Multilingual Language Processing & Language Identification,convert list word list integer scikit learn want convert list word list integer scikit learn corpus consists list list word e g corpus bunch sentence follows using simpler way suspect may missing countvectorizer functionality common pre processing step natural language processing code first fit countvectorizer iterate word list word generate list integer output
Feature extraction from a single word,"<p>Usually one wants to get a feature from a text by using the bag of words approach, counting the words and calculate different measures, for example tf-idf values, like this:  <a href=""https://stackoverflow.com/questions/4207057/how-to-include-words-as-numerical-feature-in-classification"">How to include words as numerical feature in classification</a></p>

<p>But my problem is different, I want to extract a feature vector from a single word. I want to know for example that potatoes and french fries are close to each other in the vector space, since they are both made of potatoes. I want to know that milk and cream also are close, hot and warm, stone and hard and so on.</p>

<p>What is this problem called? Can I learn the similarities and features of words by just looking at a large number documents?</p>

<p>I will not make the implementation in English, so I can't use databases.</p>
",Multilingual Language Processing & Language Identification,feature extraction single word usually one want get feature text using bag word approach counting word calculate different measure example tf idf value like href include word numerical feature classification problem different want extract feature vector single word want know example potato french fry close vector space since made potato want know milk cream also close hot warm stone hard problem called learn similarity feature word looking large number document make implementation english use database
How do I match tokens in similar (but not identical strings) so that I can share POS tags from one string to another?,"<p>I have a large corpus of text, split into sentences. I have two versions of each sentence, one version has POS-tagged tokens. I want to POS tag everything in version 1. I want to do this by replacing the words in version 1 with their POS-tagged counterparts from version 2.</p>

<p>There are some complications with this:</p>

<ol>
<li><p>Spelling for the same word can be different between the two versions (e.g. <code>'cafe'</code> vs. <code>'caf√©'</code>).</p></li>
<li><p>Spacing in the POS-tagged version doesn't always match spacing in the other (e.g. <code>""did"", ""n't""</code> vs. <code>""didn't""</code>).</p></li>
<li><p>One version uses symbols regularly while the other spells out the full word (e.g. <code>'&amp;'</code> vs. <code>'and'</code>).</p></li>
</ol>

<p>The language of the text isn't English, so the examples above are only a rough approximation of what's going on. Here are a couple of examples from the actual text. I hope it's easy to see how POS-tagged text in version 2 matches the text in version 1 closely, but not exactly; some words are missing, some are spelled differently, some are out of order, etc.</p>

<pre><code>Example 1.
Version 1: "".o. omi adov-ztu jo zn√≥yod sotfico pru &amp; bra""
Version 2: [['omi', '&lt;DET&gt;'], ['adov', '&lt;NOUN&gt;'], ['ztu', '&lt;PRON&gt;'], ['zn√≥yod', '&lt;VERB&gt;'],
           ['sotfico', '&lt;ADJ&gt;'], ['uont', '&lt;CCONJ&gt;'], ['jo', '&lt;ADP&gt;']]

Example 2.
Version 1: ""vomoyj z√≠y""
Version 2: [['v√≥', '&lt;SCONJ&gt;'], ['·πÅo', '&lt;PART&gt;'], ['yj', '&lt;PRON&gt;'], ['z√≠y', '&lt;ADJ&gt;']]

Example 3.
Version 1: "".o. fa-tistyjogot""
Version 2: [['fa', '&lt;PP&gt;'], ['t', '&lt;IP&gt;'], ['is', '&lt;UU&gt;'], ['fatistyjogot', '&lt;VERB&gt;']] 
</code></pre>

<p>In example 1 <code>'&amp;'</code> maps to <code>'uont'</code>. The words <code>'pru'</code> and <code>'bra'</code> in version 1 don't map to anything in version 2. The word, <code>'jo'</code>, is also in the wrong place in version 2, and needs to follow the word order of version 1.</p>

<p>In example 2 <code>'v√≥'</code>, <code>'·πÅo'</code>, and <code>'yj'</code> all map to <code>'vomoyj'</code>, even though some characters are different, and it's split in two places.</p>

<p>In example 3 there is only one word, but parts of it are repeated. <code>'fa'</code>, <code>'t'</code>, and <code>'is'</code> all appear in <code>'fatisyjogot'</code>, so i can ignore everything except <code>'fatisyjogot'</code> in version 2.</p>

<p>Where a word is tagged in version 2, I want to replace its counterpart in version 1 with the form from version 2 and the POS-tag. That way I can keep the word order of version 1. If no tagged form exists in version 2, I want to keep the word from version 1 and add the placeholder tag, <code>'&lt;X&gt;'</code>. I also need to leave out any content in version 2 if it is repeated like in example 3. So, from the examples above, I'd like to create the following lists:</p>

<pre><code>Example 1: [['.o.', '&lt;X&gt;'], ['omi', '&lt;DET&gt;'], ['adov', '&lt;NOUN&gt;'], ['ztu', '&lt;PRON&gt;'], ['jo', '&lt;ADP&gt;'],
           ['zn√≥yod', '&lt;VERB&gt;'], ['sotfico', '&lt;ADJ&gt;'], ['pru', '&lt;X&gt;'], ['uont', '&lt;CCONJ&gt;'], ['bra', '&lt;X&gt;']]
Example 2: [['v√≥', '&lt;SCONJ&gt;'], ['·πÅo', '&lt;PART&gt;'], ['yj', '&lt;PRON&gt;'], ['z√≠y', '&lt;ADJ&gt;']]
Example 3: [['.o.', '&lt;X&gt;'], ['fatistyjogot', '&lt;VERB&gt;']]
</code></pre>

<p>I've tried writing a function using RegEx and the edit distance method from the <code>nltk</code> module to identify similar strings. It works well for longer strings, but because some strings are so short, like <code>'v√≥'</code> above, it sometimes has difficulties. I've also looked at sequence alignment libraries, but found myself confused trying to apply them.</p>

<p>Is there any way to compare these strings and match every string in version 2 to some substring in version 1 with high accuracy? I can sort out the POS tags myself, I just need a way to find all of the corresponding tokens.</p>

<p>For example, can I write a function, give it the two versions as arguments, and get it to return all the related strings (and their index/placement in the sentence)?</p>

<pre><code>v1 = ""vomoyj z√≠y""
v2 = [['v√≥', '&lt;SCONJ&gt;'], ['·πÅo', '&lt;PART&gt;'], ['yj', '&lt;PRON&gt;'], ['z√≠y', '&lt;ADJ&gt;']]

def some_func(v1, v2):
    *do something*
    return comparison_list

print(some_func(v1, v2))

Output:
[['v√≥', 'vomoyj', 0], ['·πÅo', 'vomoyj', 1], ['yj', 'vomoyj', 2], ['z√≠y', 'z√≠y', 3]]
*OR*
[['v√≥', 'vo'], ['·πÅo', 'mo'], ['yj', 'yj'], ['z√≠y', 'z√≠y']]
</code></pre>

<p>EDIT: It's not feasible to translate this to English to simplify the problem. I really need to just compare strings.</p>
",Multilingual Language Processing & Language Identification,match token similar identical string share po tag one string another large corpus text split sentence two version sentence one version ha po tagged token want po tag everything version want replacing word version po tagged counterpart version complication spelling word different two version e g v spacing po tagged version always match spacing e g v one version us symbol regularly spell full word e g v language text english example rough approximation going couple example actual text hope easy see po tagged text version match text version closely exactly word missing spelled differently order etc example map word version map anything version word also wrong place version need follow word order version example map even though character different split two place example one word part repeated appear ignore everything except version word tagged version want replace counterpart version form version po tag way keep word order version tagged form exists version want keep word version add placeholder tag also need leave content version repeated like example example like create following list tried writing function using regex edit distance method module identify similar string work well longer string string short like sometimes ha difficulty also looked sequence alignment library found confused trying apply way compare string match every string version substring version high accuracy sort po tag need way find corresponding token example write function give two version argument get return related string index placement sentence edit feasible translate english simplify problem really need compare string
Replacing UNKNOWN words with the sorce word in MT,"<p>I'm working on  Machine learning AI translation system, and I want to make it more adaptable my code now when the word is new will place <em>UNK</em> which stands for <em>UNKNOWN</em> and leave it, but I want to copy the same word and past it back instead of printing <em>UNK</em>, so if a new word comes it should pass back the same word as translation instead of <em>UNK</em> my code looks like this for now:</p>

<p>any ideas what shall I change : </p>

<pre><code># Adding the word 'UNK' to the end of the array (stands for UNKNOWN words)
    X_ix_to_word.append('UNK')

    # Creating the word-to-index dictionary from the array created above
    X_word_to_ix = {word:ix for ix, word in enumerate(X_ix_to_word)}

    # Converting each word to its index value
    for i, sentence in enumerate(X):
        for j, word in enumerate(sentence):
            if word in X_word_to_ix:
                X[i][j] = X_word_to_ix[word]
            else:
                X[i][j] = X_word_to_ix['UNK']

    y_ix_to_word = [word[0] for word in y_vocab]
    y_ix_to_word.insert(0, 'ZERO')
    y_ix_to_word.append('UNK')
    y_word_to_ix = {word:ix for ix, word in enumerate(y_ix_to_word)}
    for i, sentence in enumerate(y):
        for j, word in enumerate(sentence):
            if word in y_word_to_ix:
                y[i][j] = y_word_to_ix[word]
            else:
                y[i][j] = y_word_to_ix['UNK']
    return (X, len(X_vocab)+2, X_word_to_ix, X_ix_to_word, y, len(y_vocab)+2, y_word_to_ix, y_ix_to_word)

def load_test_data(source, X_word_to_ix, max_len):
    f = open(source, 'r')
    X_data = f.read()
    f.close()

    X = [text_to_word_sequence(x)[::-1] for x in X_data.split('\n') if len(x) &gt; 0 and len(x) &lt;= max_len]
    for i, sentence in enumerate(X):
        for j, word in enumerate(sentence):
            if word in X_word_to_ix:
                X[i][j] = X_word_to_ix[word]
            else:
                X[i][j] = X_word_to_ix['UNK']
    return X

</code></pre>
",Multilingual Language Processing & Language Identification,replacing unknown word sorce word mt working machine learning ai translation system want make adaptable code word new place unk stand unknown leave want copy word past back instead printing unk new word come pas back word translation instead unk code look like idea shall change
"splitting words by syllable with CMU Pronunciation Dictionary, NLTK, and Python3","<p>I am working on a natural language processing project and am stuck on splitting words into syllables (using <code>nltk</code> and <code>cmudict.dict()</code> in <code>python 3</code>).</p>

<p>I currently count syllables by looking a word in my corpus up in the CMU Pronunciation Dictionary and counting the number of stresses in its list of phonemes. This appears to work pretty well.</p>

<p>What I am stuck on is how to use this information to split the accompanying grapheme after counting, as I do not understand how to either translate the phonemes back to the graphemes (seems error prone) or use the list of phonemes to somehow split the grapheme.</p>

<p>Here is the function I wrote to do this (word tokenization happens elsewhere):</p>

<pre><code>def getSyllables(self):
    pronunciation = cmudict.dict() # get the pronunciation dictionary
    syllableThreshold = 1 # we dont care about 1 syllable words right now
    for word in self.tokens:
        for grapheme, phonemes in pronunciation.items():
            if grapheme == word.lower(): # all graphemes are lowercase, we have to word.lower() to match
                syllableCounter = 0
                for x in phonemes[0]:
                    for y in x:
                        if y[-1].isdigit(): # an item ending in a number is a stress (syllable)
                            syllableCounter += 1
                if syllableCounter &gt; syllableThreshold:
                    output = ' '.join([word, ""="", str(syllableCounter)])
                    print(output)
                    print(phonemes)
                else:
                    print(word)
</code></pre>

<p>Just as an example, my current output is:</p>

<pre><code>Once
an
angry = 2
[['AE1', 'NG', 'G', 'R', 'IY0']]
man
</code></pre>

<p>How can I split the word <code>angry</code>, for example, into <code>an - gry</code>?</p>
",Multilingual Language Processing & Language Identification,splitting word syllable cmu pronunciation dictionary nltk python working natural language processing project stuck splitting word syllable using currently count syllable looking word corpus cmu pronunciation dictionary counting number stress list phoneme appears work pretty well stuck use information split grapheme counting understand either translate phoneme back grapheme seems error prone use list phoneme somehow split grapheme function wrote word tokenization happens elsewhere example current output split word example
Is there a way to give a StanfordCoreNLP pipeline raw text and a list of tokens as input?,"<p>I'm doing some natural language processing with Arabic. Since I'm working with a couple different NLP tools in tandem, I want to be able to be able to give raw text to a StanfordCoreNLP pipeline, but provide my own list of tokens rather than having it do the tokenization. Is there a way to do that? </p>
",Multilingual Language Processing & Language Identification,way give stanfordcorenlp pipeline raw text list token input natural language processing arabic since working couple different nlp tool tandem want able able give raw text stanfordcorenlp pipeline provide list token rather tokenization way
Tokenizing texts in both Chinese and English improperly splits English words into letters,"<p>When tokenizing texts that contain both Chinese and English, the result will split English words into letters, which is not what I want. Consider the following code:</p>

<pre><code>from nltk.tokenize.stanford_segmenter import StanfordSegmenter
segmenter = StanfordSegmenter()
segmenter.default_config('zh')
print(segmenter.segment('Âìà‰ΩõÂ§ßÂ≠¶ÁöÑMelissa Dell'))
</code></pre>

<p>The output will be <code>Âìà‰ΩõÂ§ßÂ≠¶ ÁöÑ M e l i s s a D e l l</code>. How do I modify this behavior?</p>
",Multilingual Language Processing & Language Identification,tokenizing text chinese english improperly split english word letter tokenizing text contain chinese english result split english word letter want consider following code output modify behavior
Dataset of students&#39; responses to programming task in text,"<p>I'm working on an Intelligent Tutoring System for programming where the tutor asks questions about code and the student answer in natural language (English). As a part of analyzing the answer, I'm using text similarity. However, this does not tell me what wrong with the answer i.e., misunderstand a concept. Therefore, I'm Thinking of using ML to classify the responses and identify any misconceptions.</p>

<p>My question is, where can I find a dataset that contains textual answers for programming tasks (JAVA)?  </p>
",Multilingual Language Processing & Language Identification,dataset student response programming task text working intelligent tutoring system programming tutor asks question code student answer natural language english part analyzing answer using text similarity however doe tell wrong answer e misunderstand concept therefore thinking using ml classify response identify misconception question find dataset contains textual answer programming task java
Is there an easy way with built-in functions to automatically retrain a keras NLP model?,"<p>I have a natural language processing model built with keras using  keras.preprocessing.text.Tokenizer. I know that I can retrain the old model by calling it's <code>.fit(...)</code> after importing it, but I need to update my tokenizer as well. The tokenizer does some things: tokenizes a string by spaces, eliminates symbols, converts to lower, keeps only the most used tokens after creating it's dictionary, hash the tokens and appends 0 if the sentence is too short.</p>

<p>Ex:</p>

<pre class=""lang-py prettyprint-override""><code>tokenizer = Tokenizer(num_words=vocab_size)

tokenizer.fit_on_texts(df_train['message'][0:100].values)

x_train = tokenizer.texts_to_sequences(df_train['message'][0:100].values)

x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)
</code></pre>

<p>This process is needed to be able to input the sequences to a nlp network. The problem appears when I try to automatically retrain this. Every time I retrain, the tokenizer must be updated. If I try to add new text, all the values from the dictionary that the tokenizer class uses(meaning the encoding of a word) changes.</p>

<p>Ex:</p>

<p>If I update like this: <code>tokenizer.fit_on_texts(df_train['message'][100:200].values)</code>,</p>

<p>then the </p>

<p><code>x_train = tokenizer.texts_to_sequences(df_train['message'][0:100].values)</code></p>

<p>will output different encodings for the sentences. I need the same encodings. In the official documentation it's said that the method ""def fit_on_texts(self, texts)"" ""Updates internal vocabulary based on a list of texts."". It updates, but also changes all the old values of the keys, old or new ones. </p>

<p>Is there an official method to keep the old values of the words and generate new values only for the new words?</p>
",Multilingual Language Processing & Language Identification,easy way built function automatically retrain kera nlp model natural language processing model built kera using kera preprocessing text tokenizer know retrain old model calling importing need update tokenizer well tokenizer doe thing tokenizes string space eliminates symbol convert lower keep used token creating dictionary hash token appends sentence short ex process needed able input sequence nlp network problem appears try automatically retrain every time retrain tokenizer must updated try add new text value dictionary tokenizer class us meaning encoding word change ex update like output different encoding sentence need encoding official documentation said method def fit text self text update internal vocabulary based list text update also change old value key old new one official method keep old value word generate new value new word
"How do you differentiate between names, places, and things?","<p>Here is a list of proper nouns taken from The Lord of the Rings. I was wondering if there is a good way to sort them based on whether they refer to a person, place or thing. Does there exist a natural language processing library that can do this? Is there a way to differentiate between places, names, and things?</p>

<p>Shire, Tookland, Bagginses, Boffins, Marches, Buckland, Fornost, Norbury, Hobbits, Took, Thain, Oldbucks, Hobbitry, Thainship, Isengrim, Michel, Delving, Midsummer, Postmaster, Shirriff, Farthing, Bounders, Bilbo, Frodo</p>
",Multilingual Language Processing & Language Identification,differentiate name place thing list proper noun taken lord ring wa wondering good way sort based whether refer person place thing doe exist natural language processing library way differentiate place name thing shire tookland bagginses boffin march buckland fornost norbury hobbit took thain oldbucks hobbitry thainship isengrim michel delving midsummer postmaster shirriff farthing bounder bilbo frodo
IBM Natural Language Processing Projects (Beginner getting started question),"<p>I've been digging into the IBM Cloud Services, Watson and NLP. Just installed the CLI and tried with Node SDKs, and a starterkit, unfortunately I did not succeed by trying to get a sample code by default to understand how it works. </p>

<p>After that, I did some research get a better open minded approach to how actually I could use some of their free services to get started, but there's actually to vague information, even though the IBM Docs are pretty extensive and well written, it can get very confusing.</p>

<p>I would appreciate any open source repo, or working/live project that you are willing to share to make a better image in my mind about it IBM cloud services. </p>
",Multilingual Language Processing & Language Identification,ibm natural language processing project beginner getting started question digging ibm cloud service watson nlp installed cli tried node sdks starterkit unfortunately succeed trying get sample code default understand work research get better open minded approach actually could use free service get started actually vague information even though ibm doc pretty extensive well written get confusing would appreciate open source repo working live project willing share make better image mind ibm cloud service
"Seq2Seq/ NLP/Translation: After generating the target sentence, does the last decoder hidden state carry any residual meaning?","<p>I am studying machine translation right now and I am interested in a question probing a bit more deeply into the internals of sentence representations.</p>

<p>Suppose we train an encoder-decoder Seq2Seq En-Fr translation system on parallel corpora, starting with pre-trained Eng and Fr word vectors. The system can use anything to form the sentence embedding (Transformers, LSTMs, etc). Then the job of the Seq2Seq translation system is to learn to build Eng sentence representations from Eng word vectors and learn to build French sentence representations from French word vectors and by the linking of the encoder and decoder, learn those two sentence representations in the same space. </p>

<p>After training the model, and encoding some English sentence with the model (Say, ""This is not a pipe.""), the sentence embedding in the joint representation space has some idea of the words 'this', 'is', 'not', 'a', 'pipe', etc and all their associations as well as the sequence in which they appear. (1)</p>

<p>When the decoder is run on the encoding, it is able to take out the aforementioned information due for a load of corpora that was fed to it during training and statistical associations between words, and output, correspondingly, 'Ceci', 'n', ''', 'est', 'pas', 'une', 'pipe', '(EOS)'. At each step, it extracts and outputs the next French word from the decoder hidden state and transforms it so that the heuristically ""most prominent"" word to be decoded next can be found by the decoder, and so on, until '(EOS)'.</p>

<p>My question is this: Is there any interpretation of the last decoder hidden state after (EOS) is the output? Is it useful for anything else? Of course, an easy answer is ""no, the model was trained to capture millions of lines of English text and process them until some word in conjunction with the hidden state produces (EOS) and last decoder hidden state is simply that, everything else not explicitly trained on is just noise and not signal"".</p>

<p>But I'm wondering if there's anything more to this? What I'm trying to get at is, if you have a sentence embedding generated in English, and have the meaning dumped out of it in French by the decoder model, does any residual meaning remain that is not translatable from English to French? Certainly, the last hidden state for any particular sentence's translation would be very hard to interpret, but how about in the aggregate (like some aggregation of the last hidden states of every single sentence to be translated that has the words 'French' in it, which means something slightly different in English because it can be paired with 'fries' etc. This is a silly example, but you can probably think of others exploiting cultural ambiguities, etc, that turn up in language.) Might this last embedding capture some statistical ""uncertainty"" or ambiguity about the translation (maybe of like the English possible ""meanings"" and associations that could have ended up in French but didn't?) or some other structural aspect of the language that might be used to help us understand, say, how English is different from French?</p>

<p>What category do you think the answer to this fall in?</p>

<ul>
<li>""There is no signal"",</li>
<li>""There probably is some signal but it would be
very hard to extract because of depends on the mechanics of how the
model was trained""</li>
<li>""There is a signal that can be reliably extracted,
even if we have to aggregate over millions of examples""?</li>
</ul>

<p>I'm not sure if this question is sensical at all but I'm curious about the answer and if any research been done on this front? I ask out of plain simple curiosity.</p>

<p><strong>Notes</strong>: <br>
I am aware that the last hidden state exists because it generates (EOS) in conjunction with the last word. That is its purpose, nothing else (?) makes it special. I'm wondering if we can get any more meaning out of it (even if it means transforming it like applying the decoder step one more time to it or something).</p>

<p>(1) (Of course, the ML model has no rich ides of 'concepts' as a human would with all its associations to thoughts and experiences and feelings, to the ML model the 'concept' only has associations with other words seen in the monolingual corpus for the word vector training and the bilingual corpus for translation training.)</p>
",Multilingual Language Processing & Language Identification,seq seq nlp translation generating target sentence doe last decoder hidden state carry residual meaning studying machine translation right interested question probing bit deeply internals sentence representation suppose train encoder decoder seq seq en fr translation system parallel corpus starting pre trained eng fr word vector system use anything form sentence embedding transformer lstms etc job seq seq translation system learn build eng sentence representation eng word vector learn build french sentence representation french word vector linking encoder decoder learn two sentence representation space training model encoding english sentence model say pipe sentence embedding joint representation space ha idea word pipe etc association well sequence appear decoder run encoding able take aforementioned information due load corpus wa fed training statistical association word output correspondingly ceci n est pa pipe eos step extract output next french word decoder hidden state transforms heuristically prominent word decoded next found decoder eos question interpretation last decoder hidden state eos output useful anything else course easy answer model wa trained capture million line english text process word conjunction hidden state produce eos last decoder hidden state simply everything else explicitly trained noise signal wondering anything trying get sentence embedding generated english meaning dumped french decoder model doe residual meaning remain translatable english french certainly last hidden state particular sentence translation would hard interpret aggregate like aggregation last hidden state every single sentence translated ha word french mean something slightly different english paired fry etc silly example probably think others exploiting ambiguity etc turn language might last embedding capture statistical uncertainty ambiguity translation maybe like english possible meaning association could ended french structural aspect language might used help u understand say english different french category think answer fall signal probably signal would hard extract depends mechanic model wa trained signal reliably extracted even aggregate million example sure question sensical curious answer research done front ask plain simple curiosity note aware last hidden state exists generates eos conjunction last word purpose nothing else make special wondering get meaning even mean transforming like applying decoder step one time something course ml model ha rich ides concept human would association thought experience feeling ml model concept ha association word seen monolingual corpus word vector training bilingual corpus translation training
"How to print twitter data stream to a file whener ever i try,i am getting a unicode error","<p>This is my python code to retrieve data from twitter.
but when I am trying to store the data to gannie.txt I am encountering the following error.</p>

<blockquote>
<pre><code>File ""D:\software\Anaconda\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 5-6: character maps to &lt;undefined&gt;
</code></pre>
  
  <p>Any help regarding this,i am new to this text mining and im trying to build project of sentiment analysis using natural language processing </p>
</blockquote>

<p>here's my code: </p>

<pre><code>outF = open(""gannie.txt"", ""a"")
for tweet in tweets:
    #print(tweet.text)
    Tweet = tweet.text
                #Convert www.* or https?://* to URL
    Tweet = re.sub('((www\.[\s]+)|(https?://[^\s]+))','URL',Tweet)


    Tweet = re.sub('@[^\s]+','TWITTER_USER',Tweet)

                #Remove additional white spaces
    Tweet = re.sub('[\s]+', ' ', Tweet)

                #Replace #word with word Handling hashtags
    Tweet = re.sub(r'#([^\s]+)', r'\1', Tweet)

                #trim
    Tweet = Tweet.strip('\'""')

                #Deleting happy and sad face emoticon from the tweet 
    a = ':)'
    b = ':('
    Tweet = Tweet.replace(a,'')
    Tweet = Tweet.replace(b,'')

                #Deleting the Twitter @username tag and reTweets
    tag = 'TWITTER_USER' 
    rt = 'RT'
    url = 'URL'
    Tweet = Tweet.replace(tag,'')
    tweetCount+=1
    if rt in Tweet:
        continue
    Tweet = Tweet.replace(url,'')
    print(Tweet)
    outF.write(Tweet)
    outF.write(""\n"")
outF.close()
</code></pre>
",Multilingual Language Processing & Language Identification,print twitter data stream file whener ever try getting unicode error python code retrieve data twitter trying store data gannie txt encountering following error help regarding new text mining im trying build project sentiment analysis using natural language processing code
Paragraph indentation of a column in a dataframe,"<p>I was trying webscraping, and managed to get the headlines and detailed content story of the news.
Code for the same is:</p>

<pre><code>webpage &lt;- read_html(""https://www.rediff.com/sports"")

headlines.node &lt;- html_nodes(webpage,'.relative h2 a')
headlines &lt;- html_text(headlines.node)
headlines &lt;- str_squish(headlines)

links &lt;- webpage %&gt;% html_nodes("".relative h2 a"") %&gt;% 
html_attr(""href"")

content &lt;- c()
for(i in 1:length(links)){
  newslink &lt;- links[i]
  webpage &lt;- read_html(newslink)
  story.node &lt;- html_nodes(webpage, ""p"")
  story &lt;-  html_text(story.node)
  story &lt;- str_squish(story)
  content[i] &lt;- paste(story, collapse = '')
}

df &lt;- data.frame(""Headlines""=headlines, ""Main Content""=content)
</code></pre>

<p>However, In order to store the detailed content of the news in a dataframe field, I had to collapse the detailed content story of the page, without which it was returning: ""In content[i] &lt;- story :
number of items to replace is not a multiple of replacement length""), as it was returning multiple rows of data of a paragraph.</p>

<p>The collapse argument created a column with data like:</p>

<pre><code>df$Main.Content[1] 
</code></pre>

<p>The above line returned:
[1] NewsApp (Free)Kohli gets 'Spirit of Cricket' gong; Stokes is ICC Cricketer of the Year.India's swashbuckling opener Rohit Sharma was on Wednesday named the ICC's 2019 ODI Cricketer of the Year for his incredible run of form, while English all-rounder Ben Stokes walked away with the overall honours.Indian skipper Virat Kohli was named captain of both the ICC's Test and ODI teams of the year besides winning the 'Spirit of Cricket' award for trying to stop fans from booing Steve Smith during a World Cup match at the Oval. Smith was returning to international cricket from a one-year suspension for ball-tampering at that time.England's World Cup-winning all-rounder Stokes got the biggest prize -- the 'Sir Garfield Sobers Trophy' for Player of the Year, while Australia fast bowler Pat Cummins was named the Test Player of the Year.India seamer Deepak Chahar won the T20 International Performance of the Year, Australia's Marnus Labuschagne was named as Emerging Cricketer of the Year, while Scotland's Kyle Coetzer was declared the Associate Cricketer of the Year.The 32-year-old  . . . . . . . . and the remaining story (not copying the complete thing here. . . )</p>

<p><strong>We lost the paragraph indentation and the text looks messy. Is there any way that we can maintain the paragraph indentations of each link and store it in a field of a dataframe?</strong></p>

<p>Example: like when i hit</p>

<pre><code>df$Main.Content[1]
</code></pre>

<p>It should return me a clean paragraph indented text as:</p>

<p>NewsApp (Free)Kohli gets 'Spirit of Cricket' gong; Stokes is ICC Cricketer of the Year.India's swashbuckling opener Rohit Sharma was on Wednesday named the ICC's 2019 ODI Cricketer of the Year for his incredible run of form, while English all-rounder Ben Stokes walked away with the overall honours.</p>

<p>Indian skipper Virat Kohli was named captain of both the ICC's Test and ODI teams of the year besides winning the 'Spirit of Cricket' award for trying to stop fans from booing Steve Smith during a World Cup match at the Oval. Smith was returning to international cricket from a one-year suspension for ball-tampering at that time.</p>

<p>(and so on. . . as in the original page)</p>

<p>I tried to best explain my requirement. Please ask if something is unclear about the question.</p>
",Multilingual Language Processing & Language Identification,paragraph indentation column dataframe wa trying webscraping managed get headline detailed content story news code however order store detailed content news dataframe field collapse detailed content story page without wa returning content story number item replace multiple replacement length wa returning multiple row data paragraph collapse argument created column data like line returned newsapp free kohli get spirit cricket gong stokes icc cricketer year india swashbuckling opener rohit sharma wa wednesday named icc odi cricketer year incredible run form english rounder ben stokes walked away overall honour indian skipper virat kohli wa named captain icc test odi team year besides winning spirit cricket award trying stop fan booing steve smith world cup match oval smith wa returning international cricket one year suspension ball tampering time england world cup winning rounder stokes got biggest prize sir garfield sobers trophy player year australia fast bowler pat cummins wa named test player year india seamer deepak chahar international performance year australia marnus labuschagne wa named emerging cricketer year scotland kyle coetzer wa declared associate cricketer year year old remaining story copying complete thing lost paragraph indentation text look messy way maintain paragraph indentation link store field dataframe example like hit return clean paragraph indented text newsapp free kohli get spirit cricket gong stokes icc cricketer year india swashbuckling opener rohit sharma wa wednesday named icc odi cricketer year incredible run form english rounder ben stokes walked away overall honour indian skipper virat kohli wa named captain icc test odi team year besides winning spirit cricket award trying stop fan booing steve smith world cup match oval smith wa returning international cricket one year suspension ball tampering time original page tried best explain requirement please ask something unclear question
"How to determine the language(English, Chinese...) of a given string in Oracle?","<p>How to determine the language (English, Chinese...) of a given sting (table column value) in Oracle(multi language environment)?</p>
",Multilingual Language Processing & Language Identification,determine language english chinese given string oracle determine language english chinese given sting table column value oracle multi language environment
What are the details of Sequence-to-sequence model for text summarization?,"<p>It is clear how to train encoder-decoder model for translation: each source sequence has its corresponding target sequence (translation). But in case of text summarization abstract is much shorter than its article. According to <a href=""https://cs224d.stanford.edu/reports/urvashik.pdf"" rel=""nofollow noreferrer"">Urvashi Khandelwal, Neural Text Summarization </a> each source sentence has its abstract (shorter or longer). But I hardly beleive there is any such dataset exists where each sentence has its corresponding abstract. So, if i am right, what are the possible ways to train sunch model? Otherwise are there any free datasets for text summarization?</p>
",Multilingual Language Processing & Language Identification,detail sequence sequence model text summarization clear train encoder decoder model translation source sequence ha corresponding target sequence translation case text summarization abstract much shorter article according urvashi khandelwal neural text summarization source sentence ha abstract shorter longer hardly beleive dataset exists sentence ha corresponding abstract right possible way train sunch model otherwise free datasets text summarization
Remove classes of word from document,"<p>Is it possible to remove specific types of words (e.g. articles, nouns, pronouns, adjectives, verbs, adverbs, conjunctions, prepositions, and possessive pronouns) from documents, like how one would remove stopwords from a body of text? If so, how? </p>

<p>I imagine a natural language processing library would have this functionality but I haven't found anything.</p>
",Multilingual Language Processing & Language Identification,remove class word document possible remove specific type word e g article noun pronoun adjective verb adverb conjunction preposition possessive pronoun document like one would remove stopwords body text imagine natural language processing library would functionality found anything
How do I extract noun/ verbal phrases for portuguese?,"<p>I've found various tools to extract verbal and noun phrases in English, including in some questions here in stackoverflow. Yet, the techniques I've found only seem to work for English texts. I've tried spacy and textblob but they won't return anything for Portuguese texts (works perfectly in English).</p>

<p>Here is what I've tried for Portuguese:
<a href=""https://stackoverflow.com/questions/44661200/spacy-to-extract-specific-noun-phrase"">Spacy to extract specific noun phrase</a>
The chunk in doc.noun_chunks works perfectly for English, but does anyone knows an already existent technique for Portuguese? I'm searching everywhere I know.</p>
",Multilingual Language Processing & Language Identification,extract noun verbal phrase portuguese found various tool extract verbal noun phrase english including question stackoverflow yet technique found seem work english text tried spacy textblob return anything portuguese text work perfectly english tried portuguese href extract specific noun phrase chunk doc noun chunk work perfectly english doe anyone know already existent technique portuguese searching everywhere know
Processing a Corpus For a word2vec Implementation,"<p>As part of a class project, I'm trying to write a word2vec implementation in Python and train it on a corpus of ~6GB. I'm trying to code a reasonably optimized solution so I don't have to let my PC sit for days.</p>

<p>Going through the C word2vec source code, I notice that there, each thread reads words from a file, and takes the time to look up the index of every word. At the end, it stores a ""sentence"" of word indexes.</p>

<p>Wouldn't it be logical to translate the whole corpus into one containing integer indexes of the appropriate words? That way, time isn't lost during training on hash-table lookups, while the translation process is a one-time expense.</p>

<p>I understand that for extremely large corpuses, you are effectively doubling the amount it takes on disk, which you might want to avoid.</p>

<p>However, if you do have the memory, wouldn't this offer a noticeable increase in efficiency? Or am I just overestimating the impact of a table lookup?</p>
",Multilingual Language Processing & Language Identification,processing corpus word vec implementation part class project trying write word vec implementation python train corpus gb trying code reasonably optimized solution let pc sit day going c word vec source code notice thread read word file take time look index every word end store sentence word index logical translate whole corpus one containing integer index appropriate word way time lost training hash table lookup translation process one time expense understand extremely large corpus effectively doubling amount take disk might want avoid however memory offer noticeable increase efficiency overestimating impact table lookup
How to predict action by processing multiple free texts in java,"<p>I have a multi-column data set as follows</p>

<pre><code>Id      Summary        Component       Description      Labels             Action

id1     free-text-11   free-text-12    free-text-13     label1, label2     action1
id2     free-text-11   free-text-22    free-text-23     label2, label3     action2

... so on

</code></pre>

<p>Here <strong>Summary</strong>, <strong>Component</strong>, <strong>Description</strong> contains user provided free text in english. <strong>Labels</strong> and <strong>Action</strong> columns contains system defined fixed texts. Now my job at hand is to train a model using java which will predict <strong>Action</strong> value after reading data from other columns - <strong>Summary</strong>, <strong>Component</strong>, <strong>Description</strong> and <strong>Labels</strong> and here some of the columns can be optional.</p>

<p>As a total newbie, I tried to use LDA using mallet, but all of the examples only handle one free text input column, and also I am not sure which algorithm would be best fit for my use case. So how do I solve this problem using java? Any help would be appreciated.</p>
",Multilingual Language Processing & Language Identification,predict action processing multiple free text java multi column data set follows summary component description contains user provided free text english label action column contains system defined fixed text job hand train model using java predict action value reading data column summary component description label column optional total newbie tried use lda using mallet example handle one free text input column also sure algorithm would best fit use case solve problem using java help would appreciated
Dose chinese need wordpiece?,"<p>I want to use Chinese bert model. In tokenization.py, I fond WordpieceTokenizer function(<a href=""https://github.com/google-research/bert/blob/master/tokenization.py"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/blob/master/tokenization.py</a>), but I don't think it is needed to use wordpiece for chinese, because the miminal unit of chinese is character.</p>

<p>WordpieceTokenizer is just for english text, am I right?</p>
",Multilingual Language Processing & Language Identification,dose chinese need wordpiece want use chinese bert model tokenization py fond wordpiecetokenizer function think needed use wordpiece chinese miminal unit chinese character wordpiecetokenizer english text right
NLP: Permute the order of constituent phrases in an English sentence,"<p>I'm looking for a solution that is not knowledge-base dependent or domain-specific, i.e. for general English text. </p>

<p>For the sentence, ""I went to the store today"", one can identify constituents</p>

<blockquote>
  <p>&lt;I went to the store&gt;, &lt;today&gt;</p>
</blockquote>

<p>and permute to ""Today I went to the store."" One could also identify constituents </p>

<blockquote>
  <p>&lt;I went&gt;, &lt;to the store&gt;, &lt;today&gt; </p>
</blockquote>

<p>and permute to ""to the store I went today"", or even ""to the store today I went"". The latter 2 permutations are less idiomatic but still preserve meaning.</p>

<p>My question -- what strategy could I use to identify a set of constituent phrases (e.g. from a parser) to which one could apply arbitrary re-ordering, whilst still preserving basic meaning?</p>

<p>The above examples are relatively simple; my use case is for more general sentences, as found in e.g. transcribed spoken English and email.</p>

<p>I allow that results may be less than grammatical, e.g. ""My friend's private green was ideal for yesterday's game"" might permute to ""for yesterday's game was ideal my friend's private green"", which is not grammatically sound, but the meaning is preserved.</p>

<p>If a strategy can reduce the likelihood of such ungrammatical permutations, that would be a plus, but not a requirement.</p>
",Multilingual Language Processing & Language Identification,nlp permute order constituent phrase english sentence looking solution knowledge base dependent domain specific e general english text sentence went store today one identify constituent went store today permute today went store one could also identify constituent went store today permute store went today even store today went latter permutation le idiomatic still preserve meaning question strategy could use identify set constituent phrase e g parser one could apply arbitrary ordering whilst still preserving basic meaning example relatively simple use case general sentence found e g transcribed spoken english email allow result may le grammatical e g friend private green wa ideal yesterday game might permute yesterday game wa ideal friend private green grammatically sound meaning preserved strategy reduce likelihood ungrammatical permutation would plus requirement
Clustering sentence vectors in a dictionary,"<p>I'm working with a kind of unique situation. I have words in Language1 that I've defined in English. I then took each English word, took its word vector from a pretrained GoogleNews w2v model, and average the vectors for every definition. The result, an example with a 3 dimension vector:</p>

<pre><code>L1_words={
'word1': array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01], dtype=float32),
'word2': array([ 5.09211312e-02, -2.67828571e-01, -1.49875201e-03], dtype=float32)
}

</code></pre>

<p>What I want to do is cluster (using K-means probably, but I'm open to other ideas) the keys of the dict by their numpy-array values. 
I've done this before with standard w2v models, but the issue I'm having is that this is a dictionary. Is there another data set I can convert this to? I'm inclined to write it to a csv/make it into a pandas datafram and use Pandas or R to work on it like that, but I'm told that floats are problem when it comes to things requiring binary (as in: they lose information in unpredictable ways). I tried saving my dictionary to hdf5, but dictionaries are not supported. </p>

<p>Thanks in advance!</p>
",Multilingual Language Processing & Language Identification,clustering sentence vector dictionary working kind unique situation word language defined english took english word took word vector pretrained googlenews w v model average vector every definition result example dimension vector want cluster using k mean probably open idea key dict numpy array value done standard w v model issue dictionary another data set convert inclined write csv make panda datafram use panda r work like told float problem come thing requiring binary lose information unpredictable way tried saving dictionary hdf dictionary supported thanks advance
Is there a NLP package or function that knows or can find locations from a document?,"<p>I am using Spacy as well as a bit of custom code to do some natural language processing for work. We want to do something where we can find a where a paper was written by using the locations sited in the paper and was curious if there is a package that could find locations such as countries, cities, states etc? Thanks for your time.</p>
",Multilingual Language Processing & Language Identification,nlp package function know find location document using spacy well bit custom code natural language processing work want something find paper wa written using location sited paper wa curious package could find location country city state etc thanks time
I want to remove non-English words from a sentence in Python 3.x,"<p>I have a bunch of user queries. In it there are certain queries which contain junk characters as well, eg. <code>I work in Google asdasb asnlkasn</code>
I need only <code>I work in Google</code></p>

<pre><code>import nltk
import spacy
import truecase
words = set(nltk.corpus.words.words())
nlp = spacy.load('en_core_web_lg')

def check_ner(word):
    doc = nlp(word)
    ner_list = []
    for token in doc.ents:
        ner_list.append(token.text)
    return ner_list



sent = ""I work in google asdasb asnlkasn""
sent = truecase.get_true_case(sent)
ner_list = check_ner(sent)

final_sent = "" "".join(w for w in nltk.wordpunct_tokenize(sent)if w.lower() in words or not 
w.isalpha() or w in ner_list)
</code></pre>

<p>I tried this but this doesn't remove the characters since ner is detecting <code>google asdasb asnlkasn</code> as <code>Work_of_Art</code> or sometimes <code>asdasb asnlkasn</code> as Person.
I had to include ner because <code>words = set(nltk.corpus.words.words())</code> doesn't have Google, Microsoft, Apple etc or any other NER value in the corpus.</p>
",Multilingual Language Processing & Language Identification,want remove non english word sentence python x bunch user query certain query contain junk character well eg need tried remove character since ner detecting sometimes person include ner google microsoft apple etc ner value corpus
Memory efficiently loading of pretrained word embeddings from fasttext library with gensim,"<p>I would like to load pretrained multilingual word embeddings from the fasttext library with gensim; here the link to the embeddings:</p>

<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>In particular, I would like to load the following word embeddings: </p>

<ul>
<li>cc.de.300.vec (4.4 GB) </li>
<li>cc.de.300.bin (7 GB)</li>
</ul>

<p>Gensim offers the following two options for loading fasttext files:</p>

<ol>
<li><p><code>gensim.models.fasttext.load_facebook_model(path, encoding='utf-8')</code>    </p>

<blockquote>
  <ul>
  <li><em>Load the input-hidden weight matrix from Facebook‚Äôs native fasttext
  .bin output file.</em></li>
  <li><em>load_facebook_model() loads the full model, not just
  word embeddings, and enables you to continue model training.</em></li>
  </ul>
</blockquote></li>
<li><p><code>gensim.models.fasttext.load_facebook_vectors(path, encoding='utf-8')</code></p>

<blockquote>
  <ul>
  <li><em>Load word embeddings from a model saved in Facebook‚Äôs native fasttext .bin format.</em></li>
  <li><em>load_facebook_vectors() loads the word embeddings only. Its faster, but does not enable you to continue training.</em></li>
  </ul>
</blockquote></li>
</ol>

<p>Source Gensim documentation: 
<a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>

<p>Since my laptop has only 8 GB RAM, I am continuing to get MemoryErrors or the loading takes a very long time (up to several minutes).</p>

<p>Is there an option to load these large models from disk more memory efficient?</p>
",Multilingual Language Processing & Language Identification,memory efficiently loading pretrained word embeddings fasttext library gensim would like load pretrained multilingual word embeddings fasttext library gensim link embeddings particular would like load following word embeddings cc de vec gb cc de bin gb gensim offer following two option loading fasttext file load input hidden weight matrix facebook native fasttext bin output file load facebook model load full model word embeddings enables continue model training load word embeddings model saved facebook native fasttext bin format load facebook vector load word embeddings faster doe enable continue training source gensim documentation since laptop ha gb ram continuing get memoryerrors loading take long time several minute option load large model disk memory efficient
how to get list of words whose parts of speech never change,"<p>I am working some NLP project . I need to find all the words in English whose parts of speech never change (ie: always have a single parts of speech in any sentence). Can anyone suggest how to find them and is there any specific name to these kind of words.</p>
",Multilingual Language Processing & Language Identification,get list word whose part speech never change working nlp project need find word english whose part speech never change ie always single part speech sentence anyone suggest find specific name kind word
NLTK Dutch alpino to english,"<p>I'm trying to find the quadgram English words I tried to change alpino which downloads the dutch words to en or English but its not working any idea ?</p>

<pre><code>from nltk.util import ngrams
from nltk.corpus import alpino
print(alpino.words())
quadgrams=ngrams(alpino.words(),4)
for i in quadgrams:
    print(i)

</code></pre>
",Multilingual Language Processing & Language Identification,nltk dutch alpino english trying find quadgram english word tried change alpino downloads dutch word en english working idea
"Given a sentence in english with a blank space, how can I estmate in python the probability of a specific word to fit into that blank space?","<p>Let's say we have a sentence in python3 with a blank space like the following:</p>

<p><code>sentence = ""Tomorrow I want to go _______.""</code></p>

<p>and we want to find out which word is more likely to appear in the blank space from a pool of words:</p>

<p><code>pool_of_words = ['eating', 'playing', 'thinking', 'jogging']</code></p>

<p><strong>Question: How can I estimate the probability of each word from the pool to appear in that blank space?</strong></p>

<p><strong>Example:</strong>
I guess the solution could be in the shape of a probability function that would give something like the examples below. How can I build such a function? </p>

<p><code>probability('jogging') = 0.98</code></p>

<p><code>probability('eating') = 0.81</code></p>

<p><code>probability('thinking') = 0.2</code></p>

<p>Thanks a lot in advance.</p>
",Multilingual Language Processing & Language Identification,given sentence english blank space estmate python probability specific word fit blank space let say sentence python blank space like following want find word likely appear blank space pool word question estimate probability word pool appear blank space example guess solution could shape probability function would give something like example build function thanks lot advance
How to change language of termDocumentmatrix in R text-mining?,"<h2>I need to change language to Turkish in the function of termDocumentmatrix. Could you possibly help me?</h2>
<h1>this code works. I reach result what i want for stemming, stopwords etc for Turkish.</h1>
<pre><code>dat&lt;-&quot;BirG√ºn, T√ºrkiye'de g√ºnl√ºk olarak yayƒ±mlanan ulusal bir gazete.
Gazetenin yazƒ± i≈üleri m√ºd√ºr√º Berkant G√ºltekin, yayƒ±n danƒ±≈ümanƒ± Barƒ±≈ü ƒ∞nce, 
sorumlu m√ºd√ºr√º Cansever Uƒüur ve haber koordinat√∂r√º ƒ∞brahim Varlƒ±'dƒ±r. Yayƒ±n 
hayatƒ±na 14 Nisan 2004'te ba≈ülayan gazetenin sahibi Birg√ºn Yayƒ±ncƒ±lƒ±k ve ƒ∞leti≈üim Ticaret A≈û'd&quot;

dat%&gt;% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)%&gt;%
  tokens_remove(stopwords(&quot;tr&quot;, source = &quot;stopwords-iso&quot;)) %&gt;%
  tokens_wordstem(language = &quot;turkish&quot;) %&gt;%
  tokens_tolower() 

Result:

[1] &quot;birg&quot;        &quot;t√ºrkiye'&quot;    &quot;g√ºnl√ºk&quot;      &quot;yayƒ±mlana&quot;   &quot;ulusal&quot;      &quot;gaze&quot;        &quot;gazete&quot;      &quot;yaz&quot;         &quot;i≈ü&quot;         
[10] &quot;m√ºd√ºr&quot;       &quot;berkant&quot;     &quot;g√ºltek&quot;      &quot;yay&quot;         &quot;danƒ±≈üma&quot;     &quot;barƒ±≈ü&quot;       &quot;ince&quot;        &quot;sorumlu&quot;     &quot;m√ºd√ºr&quot;      
[19] &quot;cansever&quot;    &quot;uƒüur&quot;        &quot;haber&quot;       &quot;koordinat√∂r&quot; &quot;ibrah&quot;       &quot;varlƒ±'&quot;      &quot;yay&quot;         &quot;hayat&quot;       &quot;nisa&quot;       
[28] &quot;te&quot;          &quot;ba≈ülaya&quot;     &quot;gazete&quot;      &quot;sahip&quot;       &quot;birgi&quot;       &quot;yayƒ±ncƒ±lƒ±k&quot;  &quot;ileti≈ü&quot;      &quot;ticaret&quot;     &quot;a≈ü'd&quot;       
</code></pre>
<p><strong>But, i dont integrate these processes into term document matrix which is below that i try to mine pdf file</strong></p>
<pre><code>library(pdftools)
library(tm)
library(SnowballC)
library(dplyr)
library(stringr)
library(tidytext)
library(quanteda)
</code></pre>
<h1>this part is reading pdf file in the working place</h1>
<pre><code>files &lt;- list.files(pattern = &quot;pdf$&quot;)
file&lt;-as.character(files)
opinions &lt;- lapply(files, pdf_text)
length(opinions)
lapply(opinions, length) 
</code></pre>
<h1>creating corpus</h1>
<pre><code>corp &lt;- Corpus(URISource(files),
               readerControl = list(reader = readPDF))

</code></pre>
<h1>in this part, &quot;language=&quot;turkish&quot; does not working, it still uses base language which is English</h1>
<pre><code>opinions.tdm &lt;- TermDocumentMatrix(corp, 
                                   control = 
                                     list(language=&quot;turkish&quot;,
                                          stopwords = TRUE,
                                          removePunctuation = TRUE,
                                          tolower = TRUE,
                                          stemming = TRUE,
                                          removeNumbers = TRUE,
                                          bounds = list(global = c(1, Inf)))) 

inspect(opinions.tdm[1:10,]) 

opinions.tdm &lt;- TermDocumentMatrix(corp, 
                                   control = 
                                     list(language=&quot;turkish&quot;,
                                          stopwords = TRUE,
                                          tolower = TRUE,
                                          stemming = TRUE,
                                          removePunctuation=TRUE,
                                          removeNumbers = TRUE,
                                          bounds = list(global = c(1, Inf))))    
</code></pre>
<pre><code>
findFreqTerms(opinions.tdm, lowfreq = 100, highfreq = Inf)

ft &lt;- findFreqTerms(opinions.tdm, lowfreq = 100, highfreq = Inf)
as.matrix(opinions.tdm[ft,]) 

ft.tdm &lt;- as.matrix(opinions.tdm[ft,])
sort(apply(ft.tdm, 1, sum), decreasing = TRUE)
a&lt;-sort(apply(ft.tdm, 1, sum), decreasing = TRUE)
a&lt;-as.data.frame(a)
a$word&lt;-rownames(a)
aa&lt;-a %&gt;%  filter(a &gt; 200) %&gt;%
  mutate(word = reorder(word, a))
</code></pre>
",Multilingual Language Processing & Language Identification,change language termdocumentmatrix r text mining need change language turkish function termdocumentmatrix could possibly help code work reach result want stemming stopwords etc turkish dont integrate process term document matrix try mine pdf file part reading pdf file working place creating corpus part language turkish doe working still us base language english
when apply word2vec just characters shown not a word ÿü,"<p>this my code you can see i am tokonize the sentence to word but i am still have a problem when i apply 
word2vec model in my sentences i use Arabic text 
anaconda version 4.7.12</p>

<pre><code>sentences = nltk.sent_tokenize(str(sentences1))
sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
for i in range(len(sentences)):
sentences[i] = [word for word in sentences[i] if word not in stopwords.words('arabic')]

sentences = re.sub(r'[^\w\s]','',(str(sentences)))
sentences = re.sub(""\d+"", """", sentences)
sentences =sentences.strip()
sentences = nltk.word_tokenize(sentences)
from gensim.models import Word2Vec
model = Word2Vec(sentences, min_count=1)
words1 = model.wv.vocab
</code></pre>

<p>in words1 the vocab just shown the letters    </p>
",Multilingual Language Processing & Language Identification,apply word vec character shown word code see tokonize sentence word still problem apply word vec model sentence use arabic text anaconda version word vocab shown letter
"Create subject-verb-object model of complex, fragmented sentences from police reports","<p>I am fairly new to spacy / textacy and I have a complicated task ahead. Your help is much appreciated.</p>

<p>In a nutshell, from a sentence like ""Did assault paramedic by kicking and pushing him"", I want to establish whether the reported abuse was against a police officer or other worker (ambulance, hospital staff, traffic warden, etc).</p>

<p>The challenges are:
- The language in which the officers write is not standard English, also the sentences have many punctuation and other errors.
- Subject is often omitted from the reports so using 'textacy.extract.subject_verb_object_triples' for example does not work as it cannot find a subject. (also subject is not necessary here as we already know that the individual has been charged with the abuse, we only want to know what category worker they assaulted from the text provided)
- The text can comprise of a number of sentences that give other context to the crime or it might list a number of abuse charges to multiple types of workers in one text.</p>

<p>Examples:
1. ""Did shout, swear and threaten her neighbours, assault A Police Officer.""
2. ""Did get ejected from a liecenced premises thereafter act aggressively towards his wife and push her.Did act in an aggressive threatening manner towards door staff and other persons.Did resist arrest.Did assault Police by biting and kicking.""
3. ""Accused did punch PC Smith then in the execution of his duty by throwing a punch towards his face to his non injury.""
4. ""Did throw a mobile phone at witness constable Smith""</p>

<p>What I am expecting to get is something like VERB,OBJECT (punch, PC Smith) which would then need to be learned to mean yes, this is a police officer. The compound objects could be PC (Police Constable), Sgt (Sargent), etc</p>

<p>I tried this:</p>

<pre><code>import spacy
import textacy


nlp = spacy.load('en')
text = nlp(u'Did assault paramedic by kicking and pushing him')

text_ext = textacy.extract.subject_verb_object_triples(text)
</code></pre>

<p>But that only works after adding a subject (which i do not need), as well as 'the' in front of the object (paramedic). So the sentence becomes ""Accused did assault the paramedic by kicking and pushing him). I have 55k statements to begin with so correcting the language is not feasible.</p>

<p>How can I work this issue? Thanks</p>
",Multilingual Language Processing & Language Identification,create subject verb object model complex fragmented sentence police report fairly new spacy textacy complicated task ahead help much appreciated nutshell sentence like assault paramedic kicking pushing want establish whether reported abuse wa police officer worker ambulance hospital staff traffic warden etc challenge language officer write standard english also sentence many punctuation error subject often omitted report using textacy extract subject verb object triple example doe work find subject also subject necessary already know individual ha charged abuse want know category worker assaulted text provided text comprise number sentence give context crime might list number abuse charge multiple type worker one text example shout swear threaten neighbour assault police officer get ejected liecenced premise thereafter act aggressively towards wife push act aggressive threatening manner towards door staff person resist arrest assault police biting kicking accused punch pc smith execution duty throwing punch towards face non injury throw mobile phone witness constable smith expecting get something like verb object punch pc smith would need learned mean yes police officer compound object could pc police constable sgt sargent etc tried work adding subject need well front object paramedic sentence becomes accused assault paramedic kicking pushing k statement begin correcting language feasible work issue thanks
Can we use Generative Adversarial Network for Neural Machine Translation?,"<p>I'm going to implement a translator based on NMT(Neural Machine Translation). In here I hope to use only monolingual corpora without using parallel corpus data for my dataset. Is it possible to train the model using only monolingual corpora data? I'm grateful if someone can share your idea regarding this.</p>
",Multilingual Language Processing & Language Identification,use generative adversarial network neural machine translation going implement translator based nmt neural machine translation hope use monolingual corpus without using parallel corpus data dataset possible train model using monolingual corpus data grateful someone share idea regarding
How to find path_similarity for lemmas in spanish?,"<p>When I try to find path_similarity or whether two English words are synonyms, I am able to execute for English, but when I try the same for Spanish, it shows an error.</p>

<p>Please find the attached the screenshot:</p>

<p><img src=""https://i.sstatic.net/PG9eV.png"" alt=""Screenshot of the error""></p>
",Multilingual Language Processing & Language Identification,find path similarity lemma spanish try find path similarity whether two english word synonym able execute english try spanish show error please find attached screenshot
Swedish lemmatization,"<p>I am trying to find a lemmatizer for Swedish without any success. Does anyone know anything about this? I am aware for the WordNet in the ntlk package but that's for the English only.</p>
",Multilingual Language Processing & Language Identification,swedish lemmatization trying find lemmatizer swedish without success doe anyone know anything aware wordnet ntlk package english
Can I pre-trained BERT model from scratch using tokenized input file and custom vocabulary file for Khmer language,"<p>I would like to know if it's possible for me to use my own tokenized/segmented documents (with my own vocab file as well) as the input file to the <code>create_pretraining_data.py</code> script (git source: <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a>).   </p>

<p>The main reason for this question is because the segmentation/tokenization for the Khmer language is different than that of English. </p>

<pre><code>Original:
·ûú·û∂‚Äã·ûò·û∂·ûì‚Äã·ûò·ûÄ‚Äã·ûá·û∂·ûò·ûΩ·ûô‚Äã·ûì·ûº·ûú

Segmented/Tokenized:
·ûú·û∂ ·ûò·û∂·ûì ·ûò·ûÄ ·ûá·û∂·ûò·ûΩ·ûô ·ûì·ûº·ûú
</code></pre>

<p>I tried something on my own and managed to get some results after running the <code>create_pretraining_data.py</code> and <code>run_pretraining.py</code> script.  However, I'm not sure if what I'm doing can be considered correct.  </p>

<p>I also would like to know the method that I should use to verify my model.  </p>

<p>Any help is highly appreciated!  </p>

<h2>Script Modifications</h2>

<p>The modifications that I did were:</p>

1. Make input file in a list format

<p>Instead of a normal plain text, my input file is from my custom Khmer tokenization output where I then make it into a list format, mimicking the output that I get when running the sample English text.</p>

<pre><code>[[['·ûä·üÜ·ûé·û∂·üÜ', '·ûü·û∂·ûú·ûò·üâ·û∂·ûú', '·ûá·û∂', '·ûî·üí·ûö·ûó·üÅ·ûë', '·ûà·ûæ', '·û†·ûº·ûî', '·ûï·üí·ûõ·üÇ'],  
['·ûú·û∂', '·ûï·üí·ûè·ûõ·üã', '·ûï·ûî·üí·ûö·ûô·üÑ·ûá·ûì·üç', '·ûô·üâ·û∂·ûÑ', '·ûÖ·üí·ûö·ûæ·ûì', '·ûä·ûõ·üã', '·ûü·ûª·ûÅ·ûó·û∂·ûñ']],  
[['cmt', '$', '270', '·ûì·û∂·üÜ', '·ûõ·û∂·ûó', '·ûì·û∂·üÜ', '·ûü·üÜ·ûé·û∂·ûÑ', '·û†·üÅ·ûÑ', '·û†·üÅ·ûÑ']]]
</code></pre>

<p><em>* The outer bracket indicates a source file, the first nested bracket indicates a document and the second nested bracket indicates a sentence. Exactly the same structure as the variable <code>all_documents</code> inside the <code>create_training_instances()</code> function</em></p>

2. Vocab file from unique segmented words

<p>This is the part that I'm really really having some serious doubt with. To create my vocab file, all I did was find the unique tokens from the whole documents. I then add the core token requirement <code>[CLS], [SEP], [UNK] and [MASK]</code>. I'm not sure if this the correct way to do it.  </p>

<p>Feedback on this part is highly appreciated!</p>

3. Skip tokenization step inside the create_training_instances() function

<p>Since my input file already matches what the variable <code>all_documents</code> is, I skip line 183 to line 207. I replaced it with reading my input as-is:</p>

<pre><code>  for input_file in input_files:
      with tf.gfile.GFile(input_file, ""r"") as reader:
          lines = reader.read()
      all_documents = ast.literal_eval(lines)
</code></pre>

<h2>Results/Output</h2>

<p>The raw input file (before custom tokenization) is from random web-scraping.  </p>

<p>Some information on the raw and vocab file:</p>

<pre><code>Number of documents/articles: 5
Number of sentences: 78
Number of vocabs: 649 (including [CLS], [SEP] etc.)
</code></pre>

<p>Below is the output (tail end of it) after running the  <code>create_pretraining_data.py</code></p>

<p><a href=""https://i.sstatic.net/eoW3p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eoW3p.png"" alt=""create_pretraining_data_khmer_output""></a></p>

<p>And this is what I get after running the <code>run_pretraining.py</code></p>

<p><img src=""https://user-images.githubusercontent.com/24283367/69701076-06944b00-1127-11ea-9845-318a0c9520ca.PNG"" alt=""20191119_sample_4""></p>

<p>As shown in the diagram above I'm getting a very low accuracy from this and hence my concern if I'm doing it correctly.</p>
",Multilingual Language Processing & Language Identification,pre trained bert model scratch using tokenized input file custom vocabulary file khmer language would like know possible use tokenized segmented document vocab file well input file script git source main reason question segmentation tokenization khmer language different english tried something managed get result running script however sure considered correct also would like know method use verify model help highly appreciated script modification modification make input file list format instead normal plain text input file custom khmer tokenization output make list format mimicking output get running sample english text outer bracket indicates source file first nested bracket indicates document second nested bracket indicates sentence exactly structure variable inside function vocab file unique segmented word part really really serious doubt create vocab file wa find unique token whole document add core token requirement sure correct way feedback part highly appreciated skip tokenization step inside create training instance function since input file already match variable skip line line replaced reading input result output raw input file custom tokenization random web scraping information raw vocab file output tail end running get running shown diagram getting low accuracy hence concern correctly
Spacy token-based matching with &#39;n&#39; number of tokens between tokens,"<p>I am using spacy to match a particular expression in some text (in italian). My text can appear in multiple forms and I am trying to learn what's the best way to write a general rule. I have 4 cases as below,, and I would like to write a general patter that could work with all of the cases. Something like:</p>

<pre><code># case 1
text = 'Superfici principali e secondarie: 90 mq'
# case 2
# text = 'Superfici principali e secondarie di 90 mq'
# case 3
# text = 'Superfici principali e secondarie circa 90 mq'
# case 4
# text = 'Superfici principali e secondarie di circa 90 mq'

nlp = spacy.load('it_core_news_sm')
doc = nlp(text)

matcher = Matcher(nlp.vocab) 

pattern = [{""LOWER"": ""superfici""}, {""LOWER"": ""principali""}, {""LOWER"": ""e""}, {""LOWER"": ""secondarie""},  &lt;&lt; ""some token here that allows max 3 tokens or a IS_PUNCT or nothing at all"" &gt;&gt;, {""IS_DIGIT"": True}, {""LOWER"": ""mq""}]

matcher.add(""Superficie"", None, pattern)

matches = matcher(doc)
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  # Get string representation
    span = doc[start:end]  # The matched span
    print(match_id, string_id, start, end, span.text)

</code></pre>
",Multilingual Language Processing & Language Identification,spacy token based matching n number token token using spacy match particular expression text italian text appear multiple form trying learn best way write general rule case would like write general patter could work case something like
Training a computer for word auto-segmentation (non-english language),"<p>I have been given a set of 80 non-english words in an excel file..the first column contains the resulting word after a crude automatic segmentation has been applied to it and the second column contains the resulting word after being segmented manually. Below is a set of 3 rows of the file</p>

<p>Auto segmentation  .........    Manually segmented</p>

<ol>
<li>[%D-Ik--(is$)    ...........   [%D-Ik]--(is$)</li>
<li>[%D-Ip-t-eR]-(u$)   ....          [%D-I]-[pt-eR]-(u$)</li>
<li>[%D-Om-(a$) ...........       [%D-Om]-(a$)</li>
</ol>

<p>My question is: is there a way with which I can train a model with this set of examples in order to segment new words (that start from d) automatically? </p>
",Multilingual Language Processing & Language Identification,training computer word auto segmentation non english language given set non english word excel file first column contains resulting word crude automatic segmentation ha applied second column contains resulting word segmented manually set row file auto segmentation manually segmented ik ik ip er u pt er u om om question way train model set example order segment new word start automatically
What are the best algorithms to determine the language of text and to correct typos in python?,"<p>I am looking for algorithms that could tell the language of the text to me(e.g. Hello - English, Bonjour - French, Servicio - Spanish) and also correct typos of the words in english. I have already explored Google's TextBlob, it is very relevant but it got ""Too many requests"" error as soon as my code starts executing. I also started exploring Polyglot but I am facing a lot of issues to download the library on Windows.</p>

<p>Code for TextBlob</p>

<pre><code>*import pandas as pd
from tkinter import filedialog
from textblob import TextBlob
import time
from time import sleep
colnames = ['Word']
x=filedialog.askopenfilename(title='Select the word list')
print(""Data to be checked: "" + x)
df = pd.read_excel(x,sheet_name='Sheet1',header=0,names=colnames,na_values='?',dtype=str)
words = df['Word']
i=0
Language_detector=pd.DataFrame(columns=['Word','Language','corrected_word','translated_word'])
for word in words:

        b = TextBlob(word)
        language_word=b.detect_language()
        time.sleep(0.5)

        if language_word in ['en','EN']:
            corrected_word=b.correct()
            time.sleep(0.5)
            Language_detector.loc[i, ['corrected_word']]=corrected_word
        else:
             translated_word=b.translate(to='en')
             time.sleep(0.5)

        Language_detector.loc[i, ['Word']]=word
        Language_detector.loc[i, ['Language']]=language_word
        Language_detector.loc[i, ['translated_word']]=translated_word

        i=i+1

filename=""Language detector test v 1.xlsx""
Language_detector.to_excel(filename,sheet_name='Sheet1')
print(""Languages identified for the word list"")**
</code></pre>
",Multilingual Language Processing & Language Identification,best algorithm determine language text correct typo python looking algorithm could tell language text e g hello english bonjour french servicio spanish also correct typo word english already explored google textblob relevant got many request error soon code start executing also started exploring polyglot facing lot issue download library window code textblob
How can I test my natural language processing model with &quot;real&quot; cases?,"<p>I am introducing myself to Natural Languaje Processing and artificial neural networks and I have followed this wonderful <a href=""https://realpython.com/python-keras-text-classification/"" rel=""nofollow noreferrer"">tutorial</a>
Once finished it, I would like to know if there is any way to test the model with phrases that I can invent, (That film entertained me a lot) for example.
Because it is very good to know the percentage of success on the test set, but I want to know how to test it.</p>
",Multilingual Language Processing & Language Identification,test natural language processing model real case introducing natural languaje processing artificial neural network followed wonderful tutorial finished would like know way test model phrase invent film entertained lot example good know percentage success test set want know test
Python and Name Entity Recognition for Arabic Language,"<p>I am performing some NER on Arabic language.  The code is as follows:</p>

<pre><code>from polyglot.text import Text
blob = ""ŸÖÿ±ÿ≠ÿ®ÿß ÿßÿ≥ŸÖŸä rahul agnihotri ÿ£ŸÜÿß ÿπŸÖÿ±Ÿä 41 ÿ≥ŸÜÿ© Ÿà ÿßŸÑŸáŸÜÿØŸäÿ©""
text = Text(blob)
text = Text(blob, hint_language_code='ar') #ar stands for arabic
print(text.entities)
</code></pre>

<p>After executing above given code in ubuntu i get below given error:</p>

<blockquote>
  <p>SyntaxError: Non-ASCII character '\xd9' in file ./ner.py on line 4,
  but no encoding declared; see <a href=""http://python.org/dev/peps/pep-0263/"" rel=""nofollow noreferrer"">http://python.org/dev/peps/pep-0263/</a> for
  details</p>
</blockquote>

<p>However, if I include # -<em>- coding: utf-8 -</em>- it works and here is the output:</p>

<pre><code>[I-LOC([u'\u0627\u0644\u0647\u0646\u062f\u064a\u0629'])]
</code></pre>

<p>This is not the desired ouptut i am looking for.  The desired output should in Arabic language not this way.</p>

<p>FYI:  All required libraries are installed.</p>
",Multilingual Language Processing & Language Identification,python name entity recognition arabic language performing ner arabic language code follows executing given code ubuntu get given error syntaxerror non ascii character xd file ner py line encoding declared see detail however include coding utf work output desired ouptut looking desired output arabic language way fyi required library installed
How to compute efficiently an orthogonality vector ? (NLP),"<p>this is my first question here so I am sorry if I am not doing this right.</p>

<p>So, I have a problem computing an orthogonality matrix. I mean, I want to compute an array, similar to co-occurrence matrix, but not having co-occurrences: I want occurrences alone. The array has to be of size <em>(number of words, number of words)</em>, and I am fully aware that this array will be very sparse.</p>

<p>The problem begins with the fact that the 'count' function has to differenciate words on the right and words on the left of the target within a window of 5 words: this is problem number 1. How to do this efficiently ? Here is my code for that :
<em>brokensentence is how I named my tokenized text</em></p>

<pre><code>import collections 
def contextDefiner(word,brokensentence,side):
    brokensentence = np.array(brokensentence)
    count = collections.Counter()
    if side == 'right':
        context = [[b,c,d,e,f] for (a,b,c,d,e,f) in  
                   zip(brokensentence[:],brokensentence[1:],brokensentence[2:],brokensentence[3:],brokensentence[4:],brokensentence[5:]) if a==word]  
    if side == 'left':
        context = [[a,b,c,d,e] for (a,b,c,d,e,f) in 
                   zip(brokensentence[:],brokensentence[1:],brokensentence[2:],brokensentence[3:],brokensentence[4:],brokensentence[5:]) if f==word]
    for ngram in context:
        count.update(ngram)
    return (count)
</code></pre>

<p>OK, now, my second problem with this is that my output is a dictionary. I have to define the context for each word in my vocabulary. This is how I defined my vocabulary :</p>

<pre><code>def vocabularyCreator(corpus):
    vocabulary = sorted(set(wordBreaker(corpus)))
    count=0
    dictionary={}
    for word in vocabulary:
         dictionary[word]=count
         count+=1
    return dictionary
</code></pre>

<p>Now, I have to translate the results for each word into an array. Keep in mind that the indexes of my array have to be the same indexes in my vocabulary. I mean, if the word ""bear"" is the 23rd word in my vocabulary, the context has to be stored in the 23rd row. This is what I did, but it takes SO LONG to compute ! </p>

<pre><code>from scipy import sparse
from scipy.sparse import csr_matrix
from scipy.sparse import lil_matrix

def matrixFiller(vocab,brokensentence,side):
    words = vocab.keys()
    matrix = lil_matrix((len(vocab),len(vocab)))
    for word in words:
        wordindex = vocab.get(word)
        context = contextDefiner(word,brokensentence,side)
        for item in context:
            itemindex = vocab.get(item)
            matrix[wordindex,itemindex]=context.get(item)
    return sparse.csr_matrix(matrix)
</code></pre>

<p>I think that a better idea would be to define the context on the left (or the right) for each word at the same time, instead of having my contextDefiner function iterate *(number of words)** times. But how to do that ? Also, I would need it to be the most efficient possible, because I have to work on a large corpus of texts.</p>

<p>I hope this makes sense, and that you will be able to help me !
Thank you so much. Tell me if you need anything else to understand my problem ! 
Also, the final output has to be compatible with the KMeansClustering function...</p>
",Multilingual Language Processing & Language Identification,compute efficiently orthogonality vector nlp first question sorry right problem computing orthogonality matrix mean want compute array similar co occurrence matrix co occurrence want occurrence alone array ha size number word number word fully aware array sparse problem begin fact count function ha differenciate word right word left target within window word problem number efficiently code brokensentence named tokenized text ok second problem output dictionary define context word vocabulary defined vocabulary translate result word array keep mind index array index vocabulary mean word bear rd word vocabulary context ha stored rd row take long compute think better idea would define context left right word time instead contextdefiner function iterate number word time also would need efficient possible work large corpus text hope make sense able help thank much tell need anything else understand problem also final output ha compatible kmeansclustering function
Ask dynamic questions for information acquisition,"<p>I am developing a chatbot that asks the user the information that is not there in the database. </p>

<p>Consider the database has 40 details for every person: Name, Age, Fav food, Fav Restaurant, Fav city, Reason for Fav City, Four the most liked things in the city,etc.</p>

<p>So, the questions can be 
""What is our name?""
""Why do you like Paris?""
""Name four places in Paris that you like the most?""</p>

<p>etc.</p>

<p>I want these questions to be generated by the bot on the fly but have no idea how to formulate these questions in English. 
Any help or direction (research papers/libraries/codes, etc) would be appreciated.</p>
",Multilingual Language Processing & Language Identification,ask dynamic question information developing chatbot asks user information database consider database ha detail every person name age fav food fav restaurant fav city reason fav city four liked thing city etc question name like paris name four place paris like etc want question generated bot fly idea formulate question english help direction research paper library code etc would appreciated
How can I use the Stanford NLP Part-of-speech tagging in Spanish?,"<p>I am working with Stanford CoreNLP and I have a doubt.
I want to determinate the grammatical category of each word and when I execute the text in the command line with:</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-spanish.properties  -annotators tokenize,ssplit,pos, ner  -file entrada.txt -outputFormat conll
</code></pre>

<p>the output is like :</p>

<pre><code>1       tomar   _       VERB    _       _       _
2       una     _       DET     _       _       _
3       cerveza _       NOUN    _       _       _
4       en      _       ADP     _       _       _
5       Madrid  _       PROPN   _       _       _

</code></pre>

<p>But when I execute from NetBeans with this code: </p>

<pre><code>Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");
        props.setProperty(""tokenize.language"", ""es"");
        props.setProperty(""pos.model"", ""edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger"");
        props.setProperty(""ner.model"", ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"");
        props.setProperty(""ner.applyNumericClassifiers"", ""true"");
        props.setProperty(""ner.useSUTime"", ""false"");
        props.setProperty(""ner.applyFineGrained"", ""false"");
        props.setProperty(""ner.language"", ""es"");

        String text = ""Ver una pel√≠cula de miedo, pasear por un parque"";
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        Annotation document = new Annotation(text);


        pipeline.annotate(document);
        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
        for(CoreMap sentence: sentences) {
            for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
              String g = token.tag();
              String word = token.get(TextAnnotation.class);
              String pos = token.get(PartOfSpeechAnnotation.class);
              String ne = token.get(NamedEntityTagAnnotation.class);
              String lema = token.get(LemmaAnnotation.class);


              System.out.println(String.format(""[%s] ""
                      + ""[%s] ""
                      + ""[%s] ""
                      + ""[%s] "" , word, pos, ne, lema));
            }
        }
</code></pre>

<p>the output is like this:</p>

<pre><code>[Ver] [vmn0000] [O] [ver] 
[una] [di0000] [O] [una] 
[pel√≠cula] [nc0s000] [O] [pel√≠cula] 
[de] [sp000] [O] [de] 
[miedo] [nc0s000] [O] [miedo] 
[,] [fc] [O] [,] 
[pasear] [vmn0000] [O] [pasear] 
[por] [sp000] [O] [por] 
[un] [di0000] [O] [un] 
[parque] [nc0s000] [O] [parque] 
</code></pre>

<p>So, How can I convert the tags like ""vmn0000"" in ""Verb"" ?</p>

<p>Thank you in advance !!</p>
",Multilingual Language Processing & Language Identification,use stanford nlp part speech tagging spanish working stanford corenlp doubt want determinate grammatical category word execute text command line output like execute netbeans code output like convert tag like vmn verb thank advance
How can I create a string in english letters from another language word?,"<p>I need to find a way to rewrite words(translit) from some languages into English language. For example <code>–ø—Ä–∏–≤–µ—Ç</code> (in Russian) sounds like <code>privet</code> (in English).</p>

<p>Meaning and grammar don't matter, but I'd like it to have a more similar sounding. Everything should be in Python, I have diligently looked up on the internet and haven't found a good approach.</p>

<p>For example, something similar to this:</p>

<pre><code>translit(""—é—É —Å–æ –±–µ—É—Ç–∏—Ñ—É–ª"", ""ru"") = juu so beutiful

translit(""–∫–∞—Ä"", ""ru"") = kar
</code></pre>
",Multilingual Language Processing & Language Identification,create string english letter another language word need find way rewrite word translit language english language example russian sound like english meaning grammar matter like similar sounding everything python diligently looked internet found good approach example something similar
Separating English text and non-English Text from a file,"<p>I have a .csv file and I want to separate Non-English Text and English Text in two different files. Below is the code, I tried:</p>

<pre><code>  import string
  def isEnglish(s):
      return s.translate(None, string.punctuation).isalnum()
  file=open('File1.csv','r',encoding='UTF-8')
  outfile1=open('Eng.csv','w', encoding='utf-8')
  outfile2=open('Noneng.csv','w', encoding='utf-8')
  for line in file.readlines():
       r = isEnglish(line)
       if r:
          outfile1.write(line+""\n"")
       else:
          outfile2.write(line+""\n"")
</code></pre>

<p>The code is not producing the desired result. There is repetitive English text in both the files. I have attached a snapshot of one output file.</p>
",Multilingual Language Processing & Language Identification,separating english text non english text file csv file want separate non english text english text two different file code tried code producing desired result repetitive english text file attached snapshot one output file
Python - pyparsing unicode characters,"<p>:) I tried using w = Word(printables), but it isn't working. How should I give the spec for this. 'w' is meant to process Hindi characters (UTF-8)</p>

<p>The code specifies the grammar and parses accordingly. </p>

<pre><code>671.assess  :: ‡§Ö‡§π‡§∏‡§æ‡§∏  ::2
x=number + ""."" + src + ""::"" + w + ""::"" + number + ""."" + number
</code></pre>

<p>If there is only english characters it is working so the code is correct for the ascii format but the code is not working for the unicode format.</p>

<p>I mean that the code works when we have something of the form
671.assess  :: ahsaas  ::2</p>

<p>i.e. it parses words in the english format, but I am not sure how to parse and then print characters in the unicode format. I need this for English Hindi word alignment for purpose.</p>

<p>The python code looks like this:</p>

<pre><code># -*- coding: utf-8 -*-
from pyparsing import Literal, Word, Optional, nums, alphas, ZeroOrMore, printables , Group , alphas8bit , 
# grammar 
src = Word(printables)
trans =  Word(printables)
number = Word(nums)
x=number + ""."" + src + ""::"" + trans + ""::"" + number + ""."" + number
#parsing for eng-dict
efiledata = open('b1aop_or_not_word.txt').read()
eresults = x.parseString(efiledata)
edict1 = {}
edict2 = {}
counter=0
xx=list()
for result in eresults:
  trans=""""#translation string
  ew=""""#english word
  xx=result[0]
  ew=xx[2]
  trans=xx[4]   
  edict1 = { ew:trans }
  edict2.update(edict1)
print len(edict2) #no of entries in the english dictionary
print ""edict2 has been created""
print ""english dictionary"" , edict2 

#parsing for hin-dict
hfiledata = open('b1aop_or_not_word.txt').read()
hresults = x.scanString(hfiledata)
hdict1 = {}
hdict2 = {}
counter=0
for result in hresults:
  trans=""""#translation string
  hw=""""#hin word
  xx=result[0]  
  hw=xx[2]
  trans=xx[4]
  #print trans
  hdict1 = { trans:hw }
  hdict2.update(hdict1)

print len(hdict2) #no of entries in the hindi dictionary
print""hdict2 has been created""
print ""hindi dictionary"" , hdict2
'''
#######################################################################################################################

def translate(d, ow, hinlist):
   if ow in d.keys():#ow=old word d=dict
    print ow , ""exists in the dictionary keys""
        transes = d[ow]
    transes = transes.split()
        print ""possible transes for"" , ow , "" = "", transes
        for word in transes:
            if word in hinlist:
        print ""trans for"" , ow , "" = "", word
                return word
        return None
   else:
        print ow , ""absent""
        return None

f = open('bidir','w')
#lines = [""'\
#5# 10 # and better performance in business in turn benefits consumers .  # 0 0 0 0 0 0 0 0 0 0 \
#5# 11 # vHyaapaar mEmn bEhtr kaam upbhOkHtaaomn kE lIe laabhpHrdd hOtaa hAI .  # 0 0 0 0 0 0 0 0 0 0 0 \
#'""]
data=open('bi_full_2','rb').read()
lines = data.split('!@#$%')
loc=0
for line in lines:
    eng, hin = [subline.split(' # ')
                for subline in line.strip('\n').split('\n')]

    for transdict, source, dest in [(edict2, eng, hin),
                                    (hdict2, hin, eng)]:
        sourcethings = source[2].split()
        for word in source[1].split():
            tl = dest[1].split()
            otherword = translate(transdict, word, tl)
            loc = source[1].split().index(word)
            if otherword is not None:
                otherword = otherword.strip()
                print word, ' &lt;-&gt; ', otherword, 'meaning=good'
                if otherword in dest[1].split():
                    print word, ' &lt;-&gt; ', otherword, 'trans=good'
                    sourcethings[loc] = str(
                        dest[1].split().index(otherword) + 1)

        source[2] = ' '.join(sourcethings)

    eng = ' # '.join(eng)
    hin = ' # '.join(hin)
    f.write(eng+'\n'+hin+'\n\n\n')
f.close()
'''
</code></pre>

<p>if an example input sentence for the source file is:</p>

<pre><code>1# 5 # modern markets : confident consumers  # 0 0 0 0 0 
1# 6 # AddhUnIk baajaar : AshHvsHt upbhOkHtaa .  # 0 0 0 0 0 0 
!@#$%
</code></pre>

<p>the ouptut would look like this :-</p>

<pre><code>1# 5 # modern markets : confident consumers  # 1 2 3 4 5 
1# 6 # AddhUnIk baajaar : AshHvsHt upbhOkHtaa .  # 1 2 3 4 5 0 
!@#$%
</code></pre>

<p>Output Explanation:-
This achieves bidirectional alignment.
It means the first word of english 'modern' maps to the first word of hindi 'AddhUnIk' and vice versa. Here even characters are take as words as they also are an integral part of bidirectional mapping. Thus if you observe the hindi WORD '.' has a null alignment and it maps to nothing with respect to the English sentence as it doesn't have a full stop.
The 3rd line int the output basically represents a delimiter when we are working for a number of sentences for which your trying to achieve bidirectional mapping.</p>

<p>What modification should i make for it to work if the I have the hindi sentences in Unicode(UTF-8) format.</p>
",Multilingual Language Processing & Language Identification,python pyparsing unicode character tried using w word printables working give spec w meant process hindi character utf code specifies grammar par accordingly english character working code correct ascii format code working unicode format mean code work something form ass ahsaas e par word english format sure parse print character unicode format need english hindi word alignment purpose python code look like example input sentence source file ouptut would look like output explanation achieves bidirectional alignment mean first word english modern map first word hindi addhunik vice versa even character take word also integral part bidirectional mapping thus observe hindi word ha null alignment map nothing respect english sentence full stop rd line int output basically represents delimiter working number sentence trying achieve bidirectional mapping modification make work hindi sentence unicode utf format
SOLR and Natural Language Parsing - Can I use it?,"<h3>Requirements</h3>

<p><a href=""https://stackoverflow.com/questions/90580/word-frequency-algorithm-for-natural-language-processing"">Word frequency algorithm for natural language processing</a></p>

<h3>Using Solr</h3>

<p>While the answer for that question is excellent, I was wondering if I could make use of all the time I spent getting to know SOLR for my NLP.</p>

<p>I thought of SOLR because:</p>

<ol>
<li>It's got a bunch of tokenizers and performs a lot of NLP.</li>
<li>It's pretty use to use out of the box.</li>
<li>It's restful distributed app, so it's easy to hook up</li>
<li>I've spent some time with it, so using could save me time.</li>
</ol>

<h3>Can I use Solr?</h3>

<p>Although the above reasons are good, I don't know SOLR THAT well, so I need to know if it would be appropriate for my requirements.</p>

<h3>Ideal Usage</h3>

<p>Ideally, I'd like to configure SOLR, and then be able to send SOLR some text, and retrieve the indexed tonkenized content.</p>

<h3>Context</h3>

<p>I'm working on a small component of a bigger recommendation engine.</p>
",Multilingual Language Processing & Language Identification,solr natural language parsing use requirement href frequency algorithm natural language processing using solr answer question excellent wa wondering could make use time spent getting know solr nlp thought solr got bunch tokenizers performs lot nlp pretty use use box restful distributed app easy hook spent time using could save time use solr although reason good know solr well need know would appropriate requirement ideal usage ideally like configure solr able send solr text retrieve indexed tonkenized content context working small component bigger recommendation engine
Is there any way to detect and differentiate between English language and Roman-Urdu language in python?,"<p>Basically I have a RomanUrduDataSet (Urdu written with the help of English alphabets e.g Sahi-right) which also includes some English language words. And I have to detect how many words of the English language are included and what are they. In other words, wants to differentiate between two languages i.e English and roman-Urdu both use the same set of alphabets. e.g ""Prime Minister Wazeer-azam""</p>

<p>I have tried spacy and spacy_langdetect packages in colab using python it's working good for all other languages but unfortunately including the Roman Urdu words as English language words. Such as for a text <b>""This is English text sai kaha"" in which ""sai kaha""</b> (well said) belongs to roman Urdu but my code below is including it as English language words.</p>

<pre><code>import spacy
from spacy_langdetect import LanguageDetector

nlp = spacy.load(""en"")
nlp.add_pipe(LanguageDetector(), name=""language_detector"", last=True)
text = ""This is English text Er lebt mit seinen Eltern und seiner Schwester in Berlin. Yo me divierto todos los d√≠as en el parque. Je m'appelle Ang√©lica Summer, j'ai 12 ans et je suis canadienne.""
doc = nlp(text)
# document level language detection. Think of it like average language of document!
print(doc._.language['language'])
# sentence level language detection
for i, sent in enumerate(doc.sents):
    print(sent, sent._.language)
</code></pre>

<p><b>OUTPUT:</b><br>
This is English text sai kaha {'language': 'en', 'score': 0.9999982400559537}
Er lebt mit seinen Eltern und seiner Schwester in Berlin. {'language': 'de', 'score': 0.9999979601967207}
Yo me divierto todos los d√≠as en el parque. {'language': 'es', 'score': 0.9999976130316337}
Je m'appelle Ang√©lica Summer, j'ai 12 ans et je suis canadienne. {'language': 'fr', 'score': 0.9999962796815557}</p>

<p><b><i>but my desired result is:</i></b></p>

<p>This English text {'language': 'en', 'score':<br>
sai kaha {'language': 'roman-urdu', 'score':</p>
",Multilingual Language Processing & Language Identification,way detect differentiate english language roman urdu language python basically romanurdudataset urdu written help english alphabet e g sahi right also includes english language word detect many word english language included word want differentiate two language e english roman urdu use set alphabet e g prime minister wazeer azam tried spacy spacy langdetect package colab using python working good language unfortunately including roman urdu word english language word text english text sai kaha sai kaha well said belongs roman urdu code including english language word output english text sai kaha language en score er lebt mit seinen eltern und seiner schwester berlin language de score yo divierto todos los en el parque language e score je appelle ang lica summer j ai et je suis canadienne language fr score desired result english text language en score sai kaha language roman urdu score
Recognizing new words with Freeling,"<p>I'm using Freeling to analyse text in Spanish, but I have a question when it comes to customize the used dictionary. The specific example is that the word</p>

<pre><code>morelos
</code></pre>

<p>is a singular masculine noun but is being split in two words and classified as follows:</p>

<pre><code>more morar VMM03S0 1 -  
los lo PP3MPA0 1 -
</code></pre>

<p>I've tried a wide variety of things from adding the word in the dictionary, which entry the following </p>

<pre><code>morelos morelos NPMSS00
</code></pre>

<p>I've tried not using multiwords, but is also unsuccessful.</p>

<p>Can anyone recommend me what to do?</p>

<p>(Is there anywhere a comprehensive tutorial to understand use freeling?)</p>
",Multilingual Language Processing & Language Identification,recognizing new word freeling using freeling analyse text spanish question come customize used dictionary specific example word singular masculine noun split two word classified follows tried wide variety thing adding word dictionary entry following tried using multiwords also unsuccessful anyone recommend anywhere comprehensive tutorial understand use freeling
Using Arabic WordNet for synonyms in python?,"<p>I am trying to get the synonyms for arabic words in a sentence</p>

<p>If the word is in English it works perfectly, and the results are displayed in Arabic language, I was wondering if its possible to get the synonym of an Arabic word right away without writing it in english first.</p>

<p>I tried that but it didn't work &amp; I would prefer without tashkeel ÿßŸÜÿ™ÿ∏ÿßÿ± instead of ÿßŸêŸÜŸíÿ™Ÿêÿ∏ÿßÿ±</p>

<pre><code>from nltk.corpus import wordnet as omw
jan = omw.synsets('ÿßŸÜÿ™ÿ∏ÿßÿ± ')[0]
print(jan)
print(jan.lemma_names(lang='arb'))
</code></pre>
",Multilingual Language Processing & Language Identification,using arabic wordnet synonym python trying get synonym arabic word sentence word english work perfectly result displayed arabic language wa wondering possible get synonym arabic word right away without writing english first tried work would prefer without tashkeel instead
Python Library to find out valid English words in a paragraph,"<p>I have a list of paragraphs, I would like to check if these words are valid English words or not. Sometimes, due to some external issues, i might not get valid English words in these paragraphs. I am aware of libraries like pyenchant and nltk which have a set of dictionaries and provide accuracy of some level but both of these have few drawbacks. I wonder if there exists another library or procedure that can provide me with what I am looking for with at-most accuracy possible.</p>
",Multilingual Language Processing & Language Identification,python library find valid english word paragraph list paragraph would like check word valid english word sometimes due external issue might get valid english word paragraph aware library like pyenchant nltk set dictionary provide accuracy level drawback wonder exists another library procedure provide looking accuracy possible
TextBlob translator cannot detect the different language in dataframe,"<p>I run the language translator using TextBlob. It can translate from a string. However, I tried to loop the textblob translator for the data in a dataframe which in dataframe might have a mixed of different languages (en and es).</p>

<p>The code I used is : </p>

<pre class=""lang-py prettyprint-override""><code>for content in data:
  blob = TextBlob(content)

for i in data:
  blob = TextBlob(i)

blob.translate(from_lang = 'en', to = 'es')
</code></pre>

<p>The error is :</p>

<pre><code>    83             result = result.encode('utf-8')
    84         if result.strip() == source.strip():
---&gt; 85             raise NotTranslated('Translation API returned the input string unchanged.')
    86 
    87     def _request(self, url, host=None, type_=None, data=None):

NotTranslated: Translation API returned the input string unchanged.
</code></pre>
",Multilingual Language Processing & Language Identification,textblob translator detect different language dataframe run language translator using textblob translate string however tried loop textblob translator data dataframe dataframe might mixed different language en e code used error
Counting the frequency of a words in Wikipedia,"<p>I need to extract information from wikipedia, but I have no idea on how to proceed. What I have to do is the following:</p>

<p>Given a word 'w', how can I count the number of times 'w' appears in the whole English Wikipedia? Is there a list already available online? If not, how could I do such thing? I am new to coding and I'm trying to do some experiments in some NLP-related tasks.</p>
",Multilingual Language Processing & Language Identification,counting frequency word wikipedia need extract information wikipedia idea proceed following given word w count number time w appears whole english wikipedia list already available online could thing new coding trying experiment nlp related task
Training data converted to feature vectors but test data is string or text,"<p><code>clean_train_reviews</code> is a <code>list</code> of <code>strings</code>. </p>

<p>Each string is a review, an example is included below:</p>

<blockquote>
  <p>classic war worlds timothy hines entertaining film obviously goes
  great effort lengths faithfully recreate h g wells classic book mr
  hines succeeds watched film appreciated fact standard predictable
  hollywood fare comes every year e g spielberg version tom cruise
  slightest resemblance book obviously everyone looks different things
  movie envision amateur critics look criticize everything others rate
  movie important bases like entertained people never agree critics
  enjoyed effort mr hines put faithful h g wells classic novel found
  entertaining made easy overlook critics perceive shortcomings</p>
</blockquote>

<p>Using the <code>vectorizer</code> initialized below, the above string is converted into a feature vector of the form: </p>

<blockquote>
  <p>(sentence_index, feature_index) count</p>
</blockquote>

<p>An example is:   </p>

<blockquote>
  <p>(0, 1905)     3</p>
</blockquote>

<p>This means ""a sentence with id of 0 and feature with id or index of 1905 occurs 3 times in this string.</p>

<pre><code>vectorizer = CountVectorizer(analyzer = ""word"",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 5000)
</code></pre>

<p><code>train[""sentiment""]</code> is a string of 1's and 0's (1=positive sentiment, 0=negative sentiment)</p>

<pre><code>train_data_features = vectorizer.fit_transform(clean_train_reviews)
forest = RandomForestClassifier(n_estimators = 100)
forest = forest.fit( train_data_features, train[""sentiment""] )
</code></pre>

<p>My question is: </p>

<p>The random forest is trained on the feature vector (all numeric values) and the sentiment (which is again numeric). But the test data set is plain text english. When the trained model is run on the test data, how does the model know what to make of plain text in the test data because the model was 
only trained on feature vectors, which were only numbers? Or does the <code>forest</code> object retain information about the plain text in the training data?</p>
",Multilingual Language Processing & Language Identification,training data converted feature vector test data string text string review example included classic war world timothy hines entertaining film obviously go great effort length faithfully recreate h g well classic book mr hines succeeds watched film appreciated fact standard predictable hollywood fare come every year e g spielberg version tom cruise slightest resemblance book obviously everyone look different thing movie envision amateur critic look criticize everything others rate movie important base like entertained people never agree critic enjoyed effort mr hines put faithful h g well classic novel found entertaining made easy overlook critic shortcoming using initialized string converted feature vector form sentence index feature index count example mean sentence id feature id index occurs time string string positive sentiment negative sentiment question random forest trained feature vector numeric value sentiment numeric test data set plain text english trained model run test data doe model know make plain text test data model wa trained feature vector number doe object retain information plain text training data
Java: how to validate natural language text,"<p>I'm using OCR to recognize (German) text in an image. It works well but not perfectly. Sometimes a word gets messed-up. Therefore, I want to implement some sort of validation. Of course, I can just use a word list and find words that are similar to the messed-up word, but is there a way to check if the sentence is plausible with these words? </p>

<p>After all, my smartphone can give me good suggestions on how to complete a sentence.</p>
",Multilingual Language Processing & Language Identification,java validate natural language text using ocr recognize german text image work well perfectly sometimes word get messed therefore want implement sort validation course use word list find word similar messed word way check sentence plausible word smartphone give good suggestion complete sentence
Can I customize the dictionary of a pre-trained transformer neural machine translation model?,"<p>There are many pre-trained machine translations models available, but it seems like they all need to be run with the dictionary they are trained with. The dictionaries sometimes can have less coverage for my data set (even BPE based ones), and sometimes miss important words as unknowns. What are the best ways to customize the pretrained models to a dictionary learned from my own data set? For example, some way to transfer learn, like unfreezing the encoder layers and retraining? </p>
",Multilingual Language Processing & Language Identification,customize dictionary pre trained transformer neural machine translation model many pre trained machine translation model available seems like need run dictionary trained dictionary sometimes le coverage data set even bpe based one sometimes miss important word unknown best way customize pretrained model dictionary learned data set example way transfer learn like unfreezing encoder layer retraining
grammarly alternative - NLP,"<p>I'm trying to learn NLP with python. Although I work with a variety of programming languages I'm looking for some kind of from the ground up solution that I can put together to come up with a product that has a high standard of spelling and grammer like grammerly?</p>

<p>I've tried some approaches with python. <a href=""https://pypi.org/project/inflect/"" rel=""nofollow noreferrer"">https://pypi.org/project/inflect/</a> 
Spacy for parts of speech. </p>

<p>Could someone point me in the direction of some kind of fully fledged API, that I can pull apart and try and work out how to get to a decent standard of english, like grammerly.</p>

<p>Many thanks,
Vince.</p>
",Multilingual Language Processing & Language Identification,grammarly alternative nlp trying learn nlp python although work variety programming language looking kind ground solution put together come product ha high standard spelling grammer like grammerly tried approach python spacy part speech could someone point direction kind fully fledged api pull apart try work get decent standard english like grammerly many thanks vince
What approach is best for urgency detection in a statement (Natural Language Processing)?,"<p>I am working on this problem where i need to understand how urgent it is to respond to a mail. My goal is such that this system should work in any case, meaning the focus must be on the words that show urgency instead of domain specific. </p>

<p>My initial approach was to use a set of words that generally show urgency and then see the similarity of that between the words present in the input statement. It worked very well for simple statements but obviously when it got complicated like ""i thought this was urgent , but you can take time"" couldn't get classified correctly. </p>

<pre class=""lang-py prettyprint-override""><code>def score(string):
sum = 0
   #calculate average of similarities for words above threshold (0.6)
    for word in string:
        for element in ms:
            score = models.similarity(word,ps.stem(element))
             if pol and score &gt; 0.6:
                 sum+=score
    return sum/len(string)

#my trained MNB model

model = pickle.load(open(""MyClassifier-MNB.model"",""rb""))
#new mail --&gt; ""I thought this was urgent, but take your time""
item = account.inbox.filter(is_read=False).order_by('-datetime_received')[0]
score(item.body)

#it is obvious that this will return a good score because i am only checking if the key words are present. What would be a good way to check negation. I also tried using polarity_score from nltk but it couldn't do much.
</code></pre>

<p>for <b>""I thought it was urgent but you can take your time""</b> <br>
<b>""Expected Output :</b>"" <br>
Not Urgent <br>
<b>""My Output :</b>""  <br>
Urgent</p>
",Multilingual Language Processing & Language Identification,approach best urgency detection statement natural language processing working problem need understand urgent respond mail goal system work case meaning focus must word show urgency instead domain specific initial approach wa use set word generally show urgency see similarity word present input statement worked well simple statement obviously got complicated like thought wa urgent take time get classified correctly thought wa urgent take time expected output urgent output urgent
Python NLP differentiation of British English and American English,"<p>Currently i am working on a project using nlp and python. i have content and need to find the language. I am using spacy to detect the language. The libraries are providing only language as English language. i need to find whether it is British or American English? Any suggestions?</p>

<p>I tried with Spacy, NLTK, lang-detect. but this libraries provide only English. but i need to display as en-GB for British and en-US for american.</p>
",Multilingual Language Processing & Language Identification,python nlp differentiation british english american english currently working project using nlp python content need find language using spacy detect language library providing language english language need find whether british american english suggestion tried spacy nltk lang detect library provide english need display en gb british en u american
Recognition of first and last name as one entity,"<p>I am interested in Natural Language processing.
I am wondering if there is a good known algorithm that in a text one can determine first and last name as one entity.</p>

<p>For example If we have this:</p>

<p><code>Last week John Wayne went to Europe.</code></p>

<p>I want to have a tokenizer that gives: ""Last"", ""Week"", John Wayne"", ""went"", ""to"", ""Europe"".</p>

<p>Any help is appreciated.</p>
",Multilingual Language Processing & Language Identification,recognition first last name one entity interested natural language processing wondering good known algorithm text one determine first last name one entity example want tokenizer give last week john wayne went europe help appreciated
Fitting a model give error (ValueError: could not convert string to float:),"<p>Using Naive Bayes Alorithm</p>

<pre><code>from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
</code></pre>

<p>The code is working till this line but when i fit the model then it shows error.</p>

<pre><code>nb.fit(X_train, y_train)
</code></pre>

<p>Output:</p>

<pre><code>ValueError: could not convert string to float: 'My fiance and 
I tried the place because of a Groupon.  We live in the same neighborhood 
and see the place all the time but the look of the place was never enough 
to draw us in.  There is nothing eye catching about the business front at 
all.  It\'s in a strip mall and looks old..........
</code></pre>

<p>I'm using yelp.csv dataset for natural language processing</p>

<p>Expected answer should be like this</p>

<pre><code>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
</code></pre>
",Multilingual Language Processing & Language Identification,fitting model give error valueerror could convert string float using naive bayes alorithm code working till line fit model show error output using yelp csv dataset natural language processing expected answer like
Detecting language using Stanford NLP,"<p>I'm wondering if it is possible to use <code>Stanford CoreNLP</code> to detect which language a sentence is written in? If so, how precise can those algorithms be?</p>
",Multilingual Language Processing & Language Identification,detecting language using stanford nlp wondering possible use detect language sentence written precise algorithm
FastText - Cannot load model.bin due to C++ extension failed to allocate the memory,"<p>I'm trying to use the FastText Python API <a href=""https://pypi.python.org/pypi/fasttext"" rel=""noreferrer"">https://pypi.python.org/pypi/fasttext</a> Although, from what I've read, this API can't load the newer .bin model files at <a href=""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"" rel=""noreferrer"">https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</a> as suggested in <a href=""https://github.com/salestock/fastText.py/issues/115"" rel=""noreferrer"">https://github.com/salestock/fastText.py/issues/115</a></p>

<p>I've tried everything that is suggested at that issue, and furthermore <a href=""https://github.com/Kyubyong/wordvectors"" rel=""noreferrer"">https://github.com/Kyubyong/wordvectors</a> doesn't have the .bin for English, otherwise the problem would be solved. Does anyone know of a work-around for this?</p>
",Multilingual Language Processing & Language Identification,fasttext load model bin due c extension failed allocate memory trying use fasttext python api although read api load newer bin model file suggested tried everything suggested issue furthermore bin english otherwise problem would solved doe anyone know work around
Sentiment analysis in R for cyrillic,"<p>I can't comment on this page where i found a function <a href=""https://stackoverflow.com/questions/47339479/sentiment-analysis-text-analytics-in-russian-cyrillic-languages/48642615#48642615"">Sentiment Analysis Text Analytics in Russian / Cyrillic languages</a></p>

<pre><code>get_sentiment_rus &lt;- function(char_v, method=""custom"", lexicon=NULL, path_to_tagger = NULL, cl = NULL, language = ""english"") {
  language &lt;- tolower(language)
  russ.char.yes &lt;- ""[\u0401\u0410-\u044F\u0451]""
  russ.char.no &lt;- ""[^\u0401\u0410-\u044F\u0451]""

    if (is.na(pmatch(method, c(""syuzhet"", ""afinn"", ""bing"", ""nrc"", 
                             ""stanford"", ""custom"")))) 
    stop(""Invalid Method"")
  if (!is.character(char_v)) 
    stop(""Data must be a character vector."")
  if (!is.null(cl) &amp;&amp; !inherits(cl, ""cluster"")) 
    stop(""Invalid Cluster"")
  if (method == ""syuzhet"") {
    char_v &lt;- gsub(""-"", """", char_v)
  }
  if (method == ""afinn"" || method == ""bing"" || method == ""syuzhet"") {
    word_l &lt;- strsplit(tolower(char_v), ""[^A-Za-z']+"")
    if (is.null(cl)) {
      result &lt;- unlist(lapply(word_l, get_sent_values, 
                              method))
    }
    else {
      result &lt;- unlist(parallel::parLapply(cl = cl, word_l, 
                                           get_sent_values, method))
    }
  }
  else if (method == ""nrc"") {
#    word_l &lt;- strsplit(tolower(char_v), ""[^A-Za-z']+"")
    word_l &lt;- strsplit(tolower(char_v), paste0(russ.char.no, ""+""), perl=T)
    lexicon &lt;- dplyr::filter_(syuzhet:::nrc, ~lang == tolower(language), 
                              ~sentiment %in% c(""positive"", ""negative""))
    lexicon[which(lexicon$sentiment == ""negative""), ""value""] &lt;- -1
    result &lt;- unlist(lapply(word_l, get_sent_values, method, 
                            lexicon))
  }
  else if (method == ""custom"") {
#    word_l &lt;- strsplit(tolower(char_v), ""[^A-Za-z']+"")
    word_l &lt;- strsplit(tolower(char_v), paste0(russ.char.no, ""+""), perl=T)
    result &lt;- unlist(lapply(word_l, get_sent_values, method, 
                            lexicon))
  }
  else if (method == ""stanford"") {
    if (is.null(path_to_tagger)) 
      stop(""You must include a path to your installation of the coreNLP package.  See http://nlp.stanford.edu/software/corenlp.shtml"")
    result &lt;- get_stanford_sentiment(char_v, path_to_tagger)
  }
  return(result)
}
</code></pre>

<p>It gives an error</p>

<pre><code>&gt; mysentiment &lt;- get_sentiment_rus(as.character(corpus))
 Show Traceback

 Rerun with Debug
 Error in UseMethod(""filter_"") : 
  no applicable method for 'filter_' applied to an object of class ""NULL"" 
</code></pre>

<p>And the sentiment scores are equal to 0</p>

<pre><code>&gt; SentimentScores &lt;- data.frame(colSums(mysentiment[,]))
&gt; SentimentScores
             colSums.mysentiment.....
anger                               0
anticipation                        0
disgust                             0
fear                                0
joy                                 0
sadness                             0
surprise                            0
trust                               0
negative                            0
positive                            0
</code></pre>

<p>Could you please point out where a problem might be? Or suggest any other working method for sentiment analysis –≤ R? Just wonder what package supports russian language.</p>

<p>I am looking for any working method for sentiment analysis of a text in russian.</p>
",Multilingual Language Processing & Language Identification,sentiment analysis r cyrillic comment page found function href analysis text analytics russian cyrillic language give error sentiment score equal could please point problem might suggest working method sentiment analysis r wonder package support russian language looking working method sentiment analysis text russian
"Python, URL NLP: How to count all English words in a URL string?","<p>I have an interesting problem. I have a list of billions of URLs. Something like:</p>

<pre><code>www.fortune.com
www.newyorktimes.com
www.asdf.com
</code></pre>

<p>I also have an English dictionary as a JSON file. <a href=""https://github.com/dwyl/english-words"" rel=""nofollow noreferrer"">https://github.com/dwyl/english-words</a>. How can I count the number of English words detected in the URL?</p>

<p>For example, for the URLS above, the counts should be: 1,3,0 for the words (fortune, new york times). The ideal output is a Pandas dataframe with the URLs and the count of English words in the URL.</p>

<p>The problem is challenging because there isn't a delimiter between words in the URL. It's also kind of a brute force search.</p>
",Multilingual Language Processing & Language Identification,python url nlp count english word url string interesting problem list billion url something like also english dictionary json file count number english word detected url example url count word fortune new york time ideal output panda dataframe url count english word url problem challenging delimiter word url also kind brute force search
Sequence translation mapping proper nouns to entity types,"<p>I'm trying to perform a sequence to sequence translation with biological scientific text which has a LOT of proper nouns in it into a structured language for biology. I can pretty easily tag the terms (proteins, chemical compounds, etc) with their entity types. </p>

<p>I was wondering if there is a way to replace the nouns with their entity types (in a way that would let me match and add them back after translation) or annotate them in a way that would enhance the translation process. </p>

<p>For example, converting ""Simvastatin-induced apoptosis is accompanied by specific induction of caveolin-1 expression"" to ""simvastin increases rna expression of caveolin-1""  - simvastin == compound, caveolin-1 == RNA   </p>

<p>My expectation is that I would need less training data if I could pre-process the input sentences, but only if I can convert "" induced apoptosis is accompanied by specific induction of  expression"" in a way that I can match up the original entities in the output (easy with one entity of each type, but what happens if I have multiple compounds or RNA's?).</p>
",Multilingual Language Processing & Language Identification,sequence translation mapping proper noun entity type trying perform sequence sequence translation biological scientific text ha lot proper noun structured language biology pretty easily tag term protein chemical compound etc entity type wa wondering way replace noun entity type way would let match add back translation annotate way would enhance translation process example converting simvastatin induced apoptosis accompanied specific induction caveolin expression simvastin increase rna expression caveolin simvastin compound caveolin rna expectation would need le training data could pre process input sentence convert induced apoptosis accompanied specific induction expression way match original entity output easy one entity type happens multiple compound rna
Python - Search large list of keywords in a large unstructured data,"<p>I have large bag of keywords that I want to search in large unstructured data and auto-tag this content.</p>

<p>First few steps that I took was to eliminate to pre-process the data like: removing stop words, punctuation, case-sentivity, check high-frequency words and delete them if I don't need. I adapted few things from this article: <a href=""https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34</a>.</p>

<p>After the pre-processing, I am un-sure how I should be searching the keywords and how should I be auto-tagging them in a efficient way. Are there any good machine learning tutorials, algorithms, and/or Natural Language Processing that can do such task much better than what I am doing now? </p>

<p>Here is an example:
Bag of keywords:</p>

<pre><code>keywords_to_search = ['Harry Potter','LOTR','Lord of the Rings','Secret Garden','Pinocchio'] # Some example, I have over 100K list of keywords to search
l1 = ['abc','def','ghi','jkl']
l2 = ['Pinocchio and Harry Potter is a famous children\'s book..','LOTR was written by J. R. R. Tolkien','Fordo Baggins is a character in Lord of the Rings Book Series.','blank']
df = pd.DataFrame({'some_col':l1,'text':l2})
</code></pre>

<p>Try:</p>

<pre><code>df[df.text.str.contains('|'.join(keywords_to_search))] # for searching ... very slow for large unstructured texts
</code></pre>

<p>Looking for this: </p>

<pre><code>some_col    text                                                                tags
abc         Pinocchio and Harry Potter is a famous children's book.             Pinocchio,Harry Potter
def         LOTR was written by J. R. R. Tolkien                                LOTR
ghi         Fordo Baggins is a character in Lord of the Ri...                   Lord of the Rings
</code></pre>
",Multilingual Language Processing & Language Identification,python search large list keywords large unstructured data large bag keywords want search large unstructured data auto tag content first step took wa eliminate pre process data like removing stop word punctuation case sentivity check high frequency word delete need adapted thing article pre processing un sure searching keywords auto tagging efficient way good machine learning tutorial algorithm natural language processing task much better example bag keywords try looking
Finding full taxonomy (heirarchical hypernymy sequence) of a given DBpedia resource using SPARQL,"<p>Given a DBpedia resource, I want to find the entire taxonomy till root.</p>

<p>For example, if I were to say in plain English, for Barack Obama I want to know the entire taxonomy which goes as <em>Barack Obama ‚Üí Politician ‚Üí Person ‚Üí Being</em>. </p>

<p>I have written the following recursive function for the same:    </p>

<pre><code>import requests
import json
from SPARQLWrapper import SPARQLWrapper, JSON
sparql = SPARQLWrapper(""http://dbpedia.org/sparql"")

def get_taxonomy(results,entity,hypernym_list):

    '''This recursive function keeps on fetching the hypernyms of the 
    DBpedia resource recursively till the highest concept or root is reached'''

    if entity == 'null':
        return hypernym_list
    else :
        query = ''' SELECT ?hypernyms WHERE {&lt;'''+entity+'''&gt; &lt;http://purl.org/linguistics/gold/hypernym&gt; ?hypernyms .}
        '''
        sparql.setQuery(query)
        sparql.setReturnFormat(JSON)
        results = sparql.query().convert()
        for result in results[""results""][""bindings""]:
            hypernym_list.append(result['hypernyms']['value'])
        if len(results[""results""][""bindings""]) == 0:
            return get_taxonomy(results,'null',hypernym_list)
        return get_taxonomy(results,results[""results""][""bindings""][0]['hypernyms']['value'],hypernym_list)

def get_taxonomy_of_resource(dbpedia_resource):
    list_for_hypernyms=[]
    results = {}
    results[""results""]={}
    results[""results""][""bindings""]=[1,2,3]
    taxonomy_list = get_taxonomy(results,dbpedia_resource,list_for_hypernyms)
    return taxonomy_list
</code></pre>

<p>The code works for the following input:</p>

<pre><code>get_taxonomy_of_resource('http://dbpedia.org/resource/Barack_Obama')
</code></pre>

<p>Output:</p>

<pre><code>['http://dbpedia.org/resource/Politician', 
'http://dbpedia.org/resource/Person', 'http://dbpedia.org/resource/Being']
</code></pre>

<p>Problem :</p>

<p>But for following output it only gives hypernym till one level above and stops:</p>

<pre><code>get_taxonomy_of_resource('http://dbpedia.org/resource/Steve_Jobs')
</code></pre>

<p>Output:</p>

<pre><code>['http://dbpedia.org/resource/Entrepreneur']
</code></pre>

<p>Research:</p>

<p>On doing some research on their site <code>dbpedia.org/page/&lt;term&gt;</code> I realized that the reason it stopped at Entrepreneur is that when I click on this resource on their site, it takes me to resource 'Entrepreneurship' and state its hypernym as 'Process'. So now my problem has been directed to the question:</p>

<p>How do I know that Entrepreneur is directing to Entrepreneurship even though both are valid DBpedia entities? My recursive function fails due to this as in next iteration it attempts to find hypernym for Entrepreneur rather than Entrepreneurship.</p>

<p>Any help is duly appreciated</p>
",Multilingual Language Processing & Language Identification,finding full taxonomy heirarchical hypernymy sequence given dbpedia resource using sparql given dbpedia resource want find entire taxonomy till root example say plain english barack obama want know entire taxonomy go barack obama politician person written following recursive function code work following input output problem following output give hypernym till one level stop output research research site realized reason stopped entrepreneur click resource site take resource entrepreneurship state hypernym process problem ha directed question know entrepreneur directing entrepreneurship even though valid dbpedia entity recursive function fails due next iteration attempt find hypernym entrepreneur rather entrepreneurship help duly appreciated
Implementation of Word2Vec in Non-English Based languages?,"<p>Is it possible to implement Word2Vec models in languages which isn't based on the English alphabet like Urdu, Tamil etc? If so can someone suggest me a pathway. </p>
",Multilingual Language Processing & Language Identification,implementation word vec non english based language possible implement word vec model language based english alphabet like urdu tamil etc someone suggest pathway
what is the best way to remove non-ASCII characters from a text Corpus when using Quanteda in R?,"<p>I am in dire need. I have a corpus that I have converted into a common language, but some of the words were not properly converted into English. Therefore, my corpus has non-ASCII characters such as <code>U+00F8</code>. </p>

<p>I am using Quanteda and I have imported my text using this code: </p>

<pre><code> EUCorpus &lt;- corpus(textfile(file=""/Users/RiohBurke/Documents/RStudio/PROJECT/*.txt""), encodingFrom = ""UTF-8-BOM"")
</code></pre>

<p>My corpus consists of 166 documents. Having imported the documents into R, what would be the best way to get rid of these non-ASCII characters?</p>
",Multilingual Language Processing & Language Identification,best way remove non ascii character text corpus using quanteda r dire need corpus converted common language word properly converted english therefore corpus ha non ascii character using quanteda imported text using code corpus consists document imported document r would best way get rid non ascii character
How to do a Python split() on languages (like Chinese) that don&#39;t use whitespace as word separator?,"<p>I want to split a sentence into a list of words.</p>

<p>For English and European languages this is easy, just use split()</p>

<pre><code>&gt;&gt;&gt; ""This is a sentence."".split()
['This', 'is', 'a', 'sentence.']
</code></pre>

<p>But I also need to deal with sentences in languages such as Chinese that don't use whitespace as word separator.</p>

<pre><code>&gt;&gt;&gt; u""ËøôÊòØ‰∏Ä‰∏™Âè•Â≠ê"".split()
[u'\u8fd9\u662f\u4e00\u4e2a\u53e5\u5b50']
</code></pre>

<p>Obviously that doesn't work.</p>

<p>How do I split such a sentence into a list of words?</p>

<p><strong>UPDATE:</strong></p>

<p>So far the answers seem to suggest that this requires natural language processing techniques and that the word boundaries in Chinese are ambiguous. I'm not sure I understand why. The word boundaries in Chinese seem very definite to me. Each Chinese word/character has a corresponding unicode and is displayed on screen as an separate word/character.</p>

<p>So where does the ambiguity come from. As you can see in my Python console output Python has no problem telling that my example sentence is made up of 5 characters:</p>

<pre><code>Ëøô - u8fd9
ÊòØ - u662f
‰∏Ä - u4e00
‰∏™ - u4e2a
Âè• - u53e5
Â≠ê - u5b50
</code></pre>

<p>So obviously Python has no problem telling the word/character boundaries. I just need those words/characters in a list.</p>
",Multilingual Language Processing & Language Identification,python split language like chinese use whitespace word separator want split sentence list word english european language easy use split also need deal sentence language chinese use whitespace word separator obviously work split sentence list word update far answer seem suggest requires natural language processing technique word boundary chinese ambiguous sure understand word boundary chinese seem definite chinese word character ha corresponding unicode displayed screen separate word character doe ambiguity come see python console output python ha problem telling example sentence made character obviously python ha problem telling word character boundary need word character list
Hunspell for Portuguese shows correctly spelled words as misspelled,"<p>I am using the latest version of <a href=""https://spacy.io/universe/project/spacy_hunspell"" rel=""nofollow noreferrer"">spacy_hunspell</a> with Portuguese <a href=""https://github.com/elastic/hunspell/tree/master/dicts/pt_PT"" rel=""nofollow noreferrer"">dictionaries</a>. And, I realized that when I have inflected verbs containing special characters, such as the acute accent (`) and the tilde (~), the spellchecker fails to retrieve the correct verification:</p>

<pre><code>import hunspell

spellchecker = hunspell.HunSpell('/usr/share/hunspell/pt_PT.dic',
                                 '/usr/share/hunspell/pt_PT.aff')

#Verb: fazer
spellchecker.spell('fazer') # True, correct
spellchecker.spell('faremos') # True, correct
spellchecker.spell('far√°') # False, incorrect
spellchecker.spell('fara') # True, incorrect
spellchecker.spell('far√£o') # False, incorrect

#Verb: andar
spellchecker.spell('andar') # True, correct
spellchecker.spell('andamos') # True, correct
spellchecker.spell('andar√°') # False, incorrect
spellchecker.spell('andara') # True, correct

#Verb: ouvir
spellchecker.spell('ouvir') # True, correct
spellchecker.spell('ou√ßo') # False, incorrect
</code></pre>

<p>Another problem is when the verb is irregular, like <code>ir</code>: </p>

<pre><code>spellchecker.spell('vamos') # False, incorrect
spellchecker.spell('vai') # False, incorrect
spellchecker.spell('iremos') # True, correct
spellchecker.spell('ir√°') # False, incorrect
</code></pre>

<p>As far as noticed, the problem does not happen with nouns with special characters:</p>

<pre><code>spellchecker.spell('cora√ß√£o') # True, correct
spellchecker.spell('√≥rg√£o') # True, correct
spellchecker.spell('√≥bvio') # True, correct
spellchecker.spell('piv√¥') # True, correct
</code></pre>

<p>Any suggestions?</p>
",Multilingual Language Processing & Language Identification,hunspell portuguese show correctly spelled word misspelled using latest version spacy hunspell portuguese dictionary realized inflected verb containing special character acute accent tilde spellchecker fails retrieve correct verification another problem verb irregular like far noticed problem doe happen noun special character suggestion
Best approach to feature engineering in natural language processing?,"<p>I am trying to cluster a large corpus of documents and would like to also subsequently explain what characterises each cluster in terms of the most common shared keywords or key-phrases in each cluster. To make clustering feasible, I will use a dimensionality reduction method like for example LSA (SVD) or doc2vec.</p>

<p>I can see several possible paths to the dimensionality reduced feature matrix:</p>

<ol>
<li><p>POS-tagging, chunking (shallow parsing) for nounal phrases (NPs), TF-IDF, LSA (SVD)</p></li>
<li><p>n-grams, TF-IDF, LSA (SVD)</p></li>
<li><p>doc2vec all the way.</p></li>
</ol>

<p>What are some of the advantages and disadvantages of these and perhaps other approaches? And what is ultimately the best way to go?</p>
",Multilingual Language Processing & Language Identification,best approach feature engineering natural language processing trying cluster large corpus document would like also subsequently explain characterises cluster term common shared keywords key phrase cluster make clustering feasible use dimensionality reduction method like example lsa svd doc vec see several possible path dimensionality reduced feature matrix po tagging chunking shallow parsing nounal phrase np tf idf lsa svd n gram tf idf lsa svd doc vec way advantage disadvantage perhaps approach ultimately best way go
Best way to make meaningful sentence from one sentence,"<p>I am trying to create sub-sentences from a combined English sentence using Python. I need help in figuring out the best way and right way to do so. </p>

<p>I have looked into a similar question asked on this <a href=""https://stackoverflow.com/questions/8974090/finding-meaningful-sub-sentences-from-a-sentence"">Finding meaningful sub-sentences from a sentence</a>. The reason of asking a new question is because I wanted to rephrase the newly created sentence, once I identify a set of sub-statements from the original sentences.</p>

<p>Posting the minimal code, which I am currently using to do my task:</p>

<pre class=""lang-py prettyprint-override""><code>def create_SLEW_Messages(req):
    print(req)

    # Pass Fail statement will be something like
    # ""Verify that "" + InitPart
    # first split the requirement with SHALL keyword, which is the subject
    InitPart = re.split('shall', req, flags=re.IGNORECASE)[0]

    # to get the exact word after shall
    verbWord = re.split('shall', req, flags=re.IGNORECASE)[1].split()[0]

    TempPassFailStatement1 = re.split('and', req, flags=re.IGNORECASE)[0]
    PassFailStatement1 = re.split('shall', TempPassFailStatement1, flags=re.IGNORECASE)[1]
    PassFailStatement2 = re.split('and', req, flags=re.IGNORECASE)[1].split("","")[0]

    returnPassFailStatement1 = ""Verify that "" + InitPart + "" will "" + PassFailStatement1

    print(returnPassFailStatement1)
    returnPassFailStatement2 = """"
    if verbWord in PassFailStatement2:
        returnPassFailStatement2 = ""Verify that "" + InitPart + "" will "" + PassFailStatement2
        print(returnPassFailStatement2)
    else:
        returnPassFailStatement2 = ""Verify that "" + InitPart + "" will "" + verbWord + "" "" + PassFailStatement2
        print(returnPassFailStatement2)

    return returnPassFailStatement1, returnPassFailStatement2

</code></pre>

<p><strong>Pre-condition - The statement which is provided to the above function will always have a ""shall"" keyword</strong></p>

<p><strong>Data 1 ==></strong> 
Input 1 - <em>The alpha tape shall move down for increasing xyz_alphaBeta and up for decreasing xyz_alphaBeta.</em></p>

<p><strong>Ouptut (actual):</strong></p>

<p>Verify that The alpha tape will  move down for increasing xyz_alphaBeta</p>

<p>Verify that The alpha tape will move  up for decreasing xyz_alphaBeta.</p>

<p><em>the above output is as per my requirement, however when I pass a sentence of similar sort and different complexity, my algo fails to detect correct sub-sentences and frames incorrect or incomplete sentences as shown below for Data 2</em></p>

<p><strong>Data 2 ==></strong>
Input 2 - <em>The Minimum Data Bug shall be positioned on the alpha tape, move upwards for increasing fac_alphaV3 and downwards for decreasing fac_alphaV3.</em></p>

<p><strong>Ouptut (actual):</strong></p>

<p>Verify that The Minimum Data Bug  will  be positioned on the alpha tape, move upwards for increasing fac_alphaV3 </p>

<p>Verify that The Minimum Data Bug  will be  downwards for decreasing fac_alphaV3.</p>

<p><strong>Ouptut (required):</strong></p>

<p>Verify that The Minimum Data Bug  will  be positioned on the alpha tape, <strong>and</strong> move upwards for increasing fac_alphaV3 </p>

<p>Verify that The Minimum Data Bug  will be <strong>positioned on the alpha tape</strong>, <strong>and move</strong> downwards for decreasing fac_alphaV3.</p>

<p><strong>PS. I understand, that regex or splitting technique is not a good technique in terms of splitting a language text, which can vary from one form to another. That is the reason, I am looking for some suggestions on this</strong></p>

<p>Any suggestions or inputs are welcome !!!</p>
",Multilingual Language Processing & Language Identification,best way make meaningful sentence one sentence trying create sub sentence combined english sentence using python need help figuring best way right way looked similar question asked pre condition statement provided function always shall keyword data input alpha tape shall move increasing xyz alphabeta decreasing xyz alphabeta ouptut actual verify alpha tape move increasing xyz alphabeta verify alpha tape move decreasing xyz alphabeta output per requirement however pas sentence similar sort different complexity algo fails detect correct sub sentence frame incorrect incomplete sentence shown data data input minimum data bug shall positioned alpha tape move upwards increasing fac alphav downwards decreasing fac alphav ouptut actual verify minimum data bug positioned alpha tape move upwards increasing fac alphav verify minimum data bug downwards decreasing fac alphav ouptut required verify minimum data bug positioned alpha tape move upwards increasing fac alphav verify minimum data bug positioned alpha tape move downwards decreasing fac alphav p understand regex splitting technique good technique term splitting language text vary one form another reason looking suggestion suggestion input welcome
Does PorterStemmer supports languages other than english?,"<p>Snowball stemmer supports many languages other than english, but does porter also?</p>
",Multilingual Language Processing & Language Identification,doe porterstemmer support language english snowball stemmer support many language english doe porter also
Vectorising tokenised french text,"<p>I have <code>Tokenised</code> french text using <code>Spacy</code> but not able to Vectorise using TFidfvectoriser</p>

<p>I tried this code but it gives error</p>

<p>vectorizer.fit_transform(data.spacyd)</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from spacy.tokenizer import Tokenizer
vectorizer=CountVectorizer()
</code></pre>

<p>'spacy.tokens.doc.Doc' object has no attribute 'lower'</p>
",Multilingual Language Processing & Language Identification,vectorising tokenised french text french text using able vectorise using tfidfvectoriser tried code give error vectorizer fit transform data spacyd spacy token doc doc object ha attribute lower
separating an Arabic sentence into words results in a different number of words with different functions,"<p>I am trying to separate one Arabic sentence, Verse 38:1 of Quran, with the <code>tm</code> and <code>tokenizers</code> packages but they split the sentence differently into 3 and 4 words, respectively. Can someone explain (1) why this is and (2) what is the meaning of this difference from NLP  and Arabic-language points of view? Also, is one of them wrong? I am by no means expert in NLP nor Arabic but trying to run the codes.</p>

<p>Here are the codes I tried:</p>

<pre class=""lang-r prettyprint-override""><code>library(tm)
library(tokenizers)
# Verse 38:1
verse&lt;- ""ÿµ ŸàÿßŸÑŸÇÿ±ÿ¢ŸÜ ÿ∞Ÿä ÿßŸÑÿ∞ŸÉÿ±""

# This separates into to 3 words by tm library 
a &lt;- colnames(DocumentTermMatrix(Corpus(VectorSource(verse) )))
a
# ""ÿßŸÑÿ∞ŸÉÿ±""   ""ÿ∞Ÿä""      ""ŸàÿßŸÑŸÇÿ±ÿ¢ŸÜ""

# This separates into 4 words by 
b &lt;- tokenizers::tokenize_words(verse)
b
# ""ÿµ""       ""ŸàÿßŸÑŸÇÿ±ÿ¢ŸÜ"" ""ÿ∞Ÿä""      ""ÿßŸÑÿ∞ŸÉÿ±""  
</code></pre>

<p>I would expect them to be equal but they are different. Can someone explain what is going on here?</p>
",Multilingual Language Processing & Language Identification,separating arabic sentence word result different number word different function trying separate one arabic sentence verse quran package split sentence differently word respectively someone explain meaning difference nlp arabic language point view also one wrong mean expert nlp arabic trying run code code tried would expect equal different someone explain going
Python: TaggedCorpusReader how to get from a STTS to a Universal tagset,"<p>I'm working on a POS Tagger using Python and Keras. The data I've got is using the STTS Tags, but I'm supposed to create a Tagger for the universal tagset. So I need to translate this.</p>

<p>First I thought of making a dictionary and simply search replace the tags, but then I saw the option of setting a tagset using the TaggedCorpusReader. (e.g. 'brown')</p>

<p>But I miss a list of possible tagsets that can be used there. Can I use the STTS Tagset somehow or do I have to make a dictionary myself?</p>

<p><strong>Example Source:</strong>
Code #3 : map corpus tags to the universal tagset
<a href=""https://www.geeksforgeeks.org/nlp-customization-using-tagged-corpus-reader/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/nlp-customization-using-tagged-corpus-reader/</a></p>

<pre><code>corpus = TaggedCorpusReader(filePath, ""standard_pos_tagged.txt"", tagset='STTS') #?? doesn't work sadly
# ....
trainingCorpus.tagged_sents(tagset='universal')[1]
</code></pre>

<hr>

<p><strong>In the end it looked something like this:</strong> (big thanks to <a href=""https://stackoverflow.com/users/699305/alexis"">alexis</a>)</p>

<pre><code>with open(resultFileName, ""w"") as output:
    for sent in stts_corpus.tagged_sents():
        for word, tag in sent:
            try:
                newTag = mapping_dict[tag];
                output.write(word+""/""+newTag+"" "")               
            except:
                print(""except ""  + str(word) + "" - "" + str(tag))
        output.write(""\n"")
</code></pre>
",Multilingual Language Processing & Language Identification,python taggedcorpusreader get stts universal tagset working po tagger using python kera data got using stts tag supposed create tagger universal tagset need translate first thought making dictionary simply search replace tag saw option setting tagset using taggedcorpusreader e g brown miss list possible tagsets used use stts tagset somehow make dictionary example source code map corpus tag universal tagset end looked something like big thanks href pre
Model returning wrong predictions every single time for language translation,"<p>I have used LSTM for designing a model for language translation. But even after 10000 entries in my data set, the model is not able to translate properly.</p>

<p>I have converted every word to small caps and have not removed any stop word. The training data is used as it is from the data set.</p>

<pre><code>model = Sequential()
model.add(Embedding(src_vocab, n_units, input_length=src_timesteps,input_shape=trainX.shape,mask_zero=True))
model.add(LSTM(n_units))
model.add(RepeatVector(tar_timesteps))
model.add(LSTM(n_units, return_sequences=True))
model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))
</code></pre>

<p>Ideally, this should have shown some translation if not actual but most of the times, it does not return maximum probability for any of the tokenized words and the string I get in the end is empty.
Here is the link for my <a href=""https://www.kaggle.com/amythshekhar/language-translation-error"" rel=""nofollow noreferrer"">kernel</a>. Any help is appreciated.</p>
",Multilingual Language Processing & Language Identification,model returning wrong prediction every single time language translation used lstm designing model language translation even entry data set model able translate properly converted every word small cap removed stop word training data used data set ideally shown translation actual time doe return maximum probability tokenized word string get end empty link kernel help appreciated
A PHP Library / Class to Count Words in Various Languages?,"<p>Some time in the near future I will need to implement a cross-language word count, or if that is not possible, a cross-language character count.</p>

<p>By word count I mean an accurate count of the words contained within the given text, taking the language of the text. The language of the text is set by a user, and will be assumed to be correct.</p>

<p>By character count I mean a count of the ""possibly in a word"" characters contained within the given text, with the same language information described above.</p>

<p>I would much prefer the former count, but I am aware of the difficulties involved. I am also aware that the latter count is much easier, but very much prefer the former, if at all possible.</p>

<p>I'd love it if I just had to look at English, but I need to consider every language here, Chinese, Korean, English, Arabic, Hindi, and so on.</p>

<p>I would like to know if Stack Overflow has any leads on where to start looking for an existing product / method to do this in PHP, as I am a good lazy programmer*</p>

<p><a href=""http://www.ecpod.com/blog/TEMP/word-count/index.php?string=%E4%B8%80%E4%B8%AA%E6%9C%89%E5%8D%81%E7%9A%84%E5%AD%97%E7%AC%A6%E7%9A%84%E5%8F%A5%E5%AD%90&amp;locale=zh_CN.UTF8"" rel=""noreferrer"">A simple test</a> showing how str_word_count with set_locale doesn't work, and a function from php.net's str_word_count page.</p>

<p>*<a href=""http://blogoscoped.com/archive/2005-08-24-n14.html"" rel=""noreferrer"">http://blogoscoped.com/archive/2005-08-24-n14.html</a></p>
",Multilingual Language Processing & Language Identification,php library class count word various language time near future need implement cross language word count possible cross language character count word count mean accurate count word contained within given text taking language text language text set user assumed correct character count mean count possibly word character contained within given text language information described would much prefer former count aware difficulty involved also aware latter count much easier much prefer former possible love look english need consider every language chinese korean english arabic hindi would like know stack overflow ha lead start looking existing product method php good lazy programmer simple test showing str word count set locale work function php net str word count page
Where can I find documentation on Berkeley Parser?,"<p>I consider using the Berkeley Parser in my research on parsing German since <a href=""http://aclweb.org/anthology/W/W08/W08-1008.pdf"" rel=""nofollow"">some sources</a> propose it to be by far the best PCFG parser for this language. Unfortunatelly, we don't see any valuable documentation. Compared to Stanford Parser the Berkeley Parser's documentation is minimalistic.</p>

<p>Are there any information sources on training and tuning apart from its <a href=""https://code.google.com/p/berkeleyparser/"" rel=""nofollow"">Google Code Repository</a>, README and console help?  </p>
",Multilingual Language Processing & Language Identification,find documentation berkeley parser consider using berkeley parser research parsing german since source propose far best pcfg parser language unfortunatelly see valuable documentation compared stanford parser berkeley parser documentation minimalistic information source training tuning apart google code repository readme console help
Correct or complete a word based on context,"<p>I am working on text normalization. I have descriptions of variables/attributes, which I need to convert to correct english.
A an example is shown below:
""This is the sta of the customer's order""
The word 'sta' above needs to be converted to 'status' based on the error and the context.</p>

<p>I tried out a character level encoder decoder architecture, but did not get good results.I need some direction on how to approach this problem. </p>

<p>input :""This is the sta of the customer's order""</p>

<p>output: ""This is the status of the customer's order""</p>
",Multilingual Language Processing & Language Identification,correct complete word based context working text normalization description variable attribute need convert correct english example shown sta customer order word sta need converted status based error context tried character level encoder decoder architecture get good result need direction approach problem input sta customer order output status customer order
Confusion between adjective and participle,"<p>Consider the three sentences: 1. The experienced pain is acute. 2. Experienced candidates are invited to apply. 3. This candidate is experienced in all phases of SDLC.</p>

<p>In sentence 1, what is actually meant is: ""the pain that is experienced"" and not, repeat not, ""pain having experience"". In sentences 2 and 3, what is meant is: ""candidates having experience"".</p>

<p>In sentence 1, the word 'experienced' does not work as an adjective but works as a participle. In sentences 2 and 3, the word essentially functions as an adjective.</p>

<p>However, the Stanford POS-Tagger does exactly the opposite: it tags the word as ""JJ"" in sentence 1 and ""VBN"" in sentences 2 and 3.</p>

<p>The problem seems to arise mainly because the word 'experience' in English is both a noun and a verb.</p>
",Multilingual Language Processing & Language Identification,confusion adjective participle consider three sentence experienced pain acute experienced candidate invited apply candidate experienced phase sdlc sentence actually meant pain experienced repeat pain experience sentence meant candidate experience sentence word experienced doe work adjective work participle sentence word essentially function adjective however stanford po tagger doe exactly opposite tag word jj sentence vbn sentence problem seems arise mainly word experience english noun verb
matching high number of different sentences (using regexp patterns parsing),"<p>I want to use <strong>regexps</strong> to build a text sentence classifier (for a chatbot natural language processing).</p>

<p>I have a very large number (e.g. <strong>>> 100</strong>) of different kind of text sentences to match regexps patterns. </p>

<p>When a sentence matches a regexp (say, an <em>intent</em>), activates a specific <em>action</em> (a function handler).</p>

<p>I preset specific regexps to match any different different set of sentences, by example: </p>

<pre><code>     // I have a long list of regexps (also many regexp for a many intents)

    const regexps = [ 
      /from (?&lt;fromCity&gt;.+)/,  // ---&gt; actionOne()
      /to (?&lt;toCity&gt;.+)/,      // ---&gt; actionTwo()
      /.../,                   // ---&gt; anotherAction()
      /.../                   // ---&gt; yetAnotherAction()
    ]

   // I have a long list of actions (function handlers)

   const actions = [
     actionOne(),
     actionTwo(),
     ...,
     ...
   ]      
</code></pre>

<p><strong>How can I build the fastest (multi-regexp) classifier (in Javascript)?</strong> </p>

<p>My current quick and dirty solution is to just check each regexp sequentially:</p>

<pre><code>    // at run time        
    ...
    sentence = 'from Genova'
    ...

    if (sentence.match(/from (?&lt;fromCity&gt;.+)/)
      actionOne()

    else if(sentence.match(/to (?&lt;toCity&gt;.+)/)
      actionTwo()

    else if ...
    else if ...
    else 
      fallback()
</code></pre>

<p>The above <em>if-then sequence</em> approach is not much scalable and above all is slow in term of performances (even if most frequency-used regexp sort could help). </p>

<p>An alternative approach to improve performances could be:
to create <strong>a <em>single</em> (big) regexp composed by named group (one for each matcher-regexp) alternation</strong>?</p>

<p>As in the minimal example:</p>

<pre><code>   const regexp = /(?&lt;one&gt;from (?&lt;toCity&gt;.+))|(?&lt;two&gt;to (?&lt;toCity&gt;.+))/
</code></pre>

<p>So I create the <em>regexp classifier</em> simply with (please take the code here below as javascript pseudo-code):</p>

<pre><code>    // at build time

    // I collect all possible regexps, each one as a named group
    const intents = [
      '(?&lt;one&gt;from (?&lt;fromCity&gt;.+))',
      '(?&lt;two&gt;to (?&lt;toCity&gt;.+))',
      '...',
      '...'
    ]

    const classifier = new RegExp(intents.join('|'))

    // collection of functions handlers, one for each regexp
    const Actions = {
     'one': 'actionOne',
     'two': 'actionTwo',
     ...,
     ...
    }

    // at run time

    const match = sentence.match(classifier)

    // if match, call corresponding function handler
    // match.groups contains the matching named group
    const action = Actions[match.groups]

    if ( action )
      action()
    else
      fallback() // no match
</code></pre>

<p>Does it make sense?
Any suggestion for a better approach?</p>
",Multilingual Language Processing & Language Identification,matching high number different sentence using regexp pattern parsing want use regexps build text sentence classifier chatbot natural language processing large number e g different kind text sentence match regexps pattern sentence match regexp say intent activates specific action function handler preset specific regexps match different different set sentence example build fastest multi regexp classifier javascript current quick dirty solution check regexp sequentially sequence approach much scalable slow term performance even frequency used regexp sort could help alternative approach improve performance could create single big regexp composed named group one matcher regexp alternation minimal example create regexp classifier simply please take code javascript pseudo code doe make sense suggestion better approach
Seq2Seq for string reversal,"<p>If I have a string, say ""abc"" and target of that string in reverse, say ""cba"". </p>

<p>Can a neural network, in particular an encoder-decoder model, learn this mapping? If so, what is the best model to accomplish this.</p>

<p>I ask, as this is a structural translation rather than a simple character mapping as in normal machine translation</p>
",Multilingual Language Processing & Language Identification,seq seq string reversal string say abc target string reverse say cba neural network particular encoder decoder model learn mapping best model accomplish ask structural translation rather simple character mapping normal machine translation
"How to write a code for a dataset in which one of the columns contains punctuations, spaces and to delete the corresponding row to it?","<p>I am trying to clean a dataset which contain some chinese characters and get rid of those rows which contain chinese characters.</p>

<p>I have first tried replacing chinese characters with a space and then tried to use regex to fill the dataset with only those rows and columns which don't have the spaces and punctuations.</p>

<pre><code>        df[""reviewer_name""] = df[""reviewer_name""].str.replace(r'[^\x00-\x7F]+','')
        df['comments'] = df['comments'].str.replace(r'[^\x00-\x7F]+', '')
        df = df[df['comments'].str.contains(r'\W+', na=False)]
        df
</code></pre>

<p>The data is like this - </p>

<ul>
<li><p>data -<br>
title_id             date        Reviewer_name           comments</p>

<p>258716           2019-04-21       Heap Chuan        Êñ∞ÂÖ¨ÂØì,ÂæàÂπ≤ÂáÄ,ËøòÊúâÁÆ°ÁêÜÂëòÊé•ÂæÖ</p></li>
</ul>

<p>-Expected-
All the rows with chinese character to be gone from dataset</p>
",Multilingual Language Processing & Language Identification,write code dataset one column contains punctuation space delete corresponding row trying clean dataset contain chinese character get rid row contain chinese character first tried replacing chinese character space tried use regex fill dataset row column space punctuation data like data title id date reviewer name comment heap chuan expected row chinese character gone dataset
"Natural-Language-Processing, Machine Learning, Data Science","<p>How to extract multiple tweets from different twitter accounts with the help of sentiment function. (Use python language, Natural Language Processing)
Create a graph using matplot to represent positive &amp; negative output. Also find the probability and total number of tweets and find the future coming tweets.</p>
",Multilingual Language Processing & Language Identification,natural language processing machine learning data science extract multiple tweet different twitter account help sentiment function use python language natural language processing create graph using matplot represent positive negative output also find probability total number tweet find future coming tweet
Can I use Natural Language Processing while identifying given words in a paragraph Or do I need to use machine learning algorithms,"<p>I need to identify some given words using NLP. 
As an example,</p>

<p>Mary Lives in <strong>France</strong></p>

<p>If we consider in here the given words are Australia, Germany,France. But in this sentence it include only France. </p>

<p>So Among the above 3 given words I need to identify the sentence is include only France</p>
",Multilingual Language Processing & Language Identification,use natural language processing identifying given word paragraph need use machine learning algorithm need identify given word using nlp example mary life france consider given word australia germany france sentence include france among given word need identify sentence include france
How to remove xml or html command lines and retrieve actual text data using python?,"<p>How to remove html or xml command lines from a downloaded webpage to obtain only text data during text preprocessing using python-3.x</p>

<p>I have tried by first removing special characters and numbers using str.translate and then crosschecking the tokens from english dictionary but some html commands still get included.</p>

<pre><code>def rmpunctuation(text):

    chars_to_remove = ""!\""¬∑‚Äî#$%&amp;'‚Äì()*+,-.‚Ä¢‚àí‚ü®‚ü©/:;&lt;=&gt;? 
    @[\]^_`{|}~0123456789""

    tr = str.maketrans("""", """", chars_to_remove)

    return text.translate(tr)


def dictcheck(text):

    a = []

    for i in range(0,len(text)):

        if(d.check(text[i]) == True):

            a.append(text[i])

    return a
</code></pre>

<p>i expect the output to be a list of all words which are actual text from the webpage and not some xml or html code.</p>
",Multilingual Language Processing & Language Identification,remove xml html command line retrieve actual text data using python remove html xml command line downloaded webpage obtain text data text preprocessing using python x tried first removing special character number using str translate crosschecking token english dictionary html command still get included expect output list word actual text webpage xml html code
Natural Language Processing in PHP,"<p>Given, say, a recipe (list of ingredients, steps, etc.) in free text form, how could I parse that in such a way I can pull out the ingredients (e.g. quantity, unit of measurements, ingredient name, etc.) usin PHP?</p>

<p>Assume that the free text is <em>somewhat</em> formatted.</p>
",Multilingual Language Processing & Language Identification,natural language processing php given say recipe list ingredient step etc free text form could parse way pull ingredient e g quantity unit measurement ingredient name etc usin php assume free text somewhat formatted
Algorithm for translating MLB play-by-play records into descriptive text,"<p>I'm trying to collect a dataset that could be used for automatically generating baseball articles. </p>

<p>I have play-by-play records of MLB games from <a href=""https://www.retrosheet.org/"" rel=""nofollow noreferrer"">retrosheet.org</a> that I would like to be written out to plain text, as those that could possibly appear as part of a recap news article. </p>

<p>Here are some examples of the play-by-play records: </p>

<pre><code>play,2,0,semim001,32,.CBFFFBBX,9/F
play,2,0,phegj001,01,FX,S7/G
play,2,0,martn003,01,CX,3/G
play,2,1,youne003,00,,NP
</code></pre>

<p>The following is what I would like to achieve: 
For the first example 
<code>play,2,0,semim001,32,.CBFFFBBX,9/F</code>, </p>

<p>I want it to be written out as something like: </p>

<p><em>""semim001 (Marcus Semien) was on three balls and two strikes in the second inning as the away player. He hit the ball into play after one called strike, one ball, three fouls, and another two balls. The fly ball was caught by the right outfielder.""</em></p>

<p>The plays are formatted in the following way:</p>

<ol>
<li><p>The first field is the inning, an integer starting at 1.</p></li>
<li><p>The second field is either 0 (for visiting team) or 1 (for home team).</p></li>
<li><p>The third field is the Retrosheet player id of the player at the plate.</p></li>
<li><p>The fourth field is the count on the batter when this particular event (play) occurred. Most Retrosheet games do not have this information, and in such cases, ""??"" appears in this field.</p></li>
<li><p>The fifth field is of variable length and contains all pitches to this batter in this plate appearance and is described below. If pitches are unknown, this field is left empty, nothing is between the commas.</p></li>
<li><p>The sixth field describes the play or event that occurred.</p></li>
</ol>

<p>Explanations for all the symbols in the fifth and sixth field can be found on this <a href=""https://www.retrosheet.org/eventfile.htm"" rel=""nofollow noreferrer"">Retrosheet page.</a> </p>

<p>With Python 3, I've been able to format all the info of invariable length into a formatted sentence, which is all but the last two fields. I'm having difficulty in thinking of an efficient way to unparse (correct me if this is the wrong term to use here) the fifth and sixth fields, the pitches and the events that occurred, due to their variable length and wide variety of things that can occur. </p>

<p>I think I could write out all the rules based on the info on the Retrosheet website, but I'm looking for suggestions for a smarter way to do this. I wrote natural language processing as tags, hoping this could be a trivial problem in that field. Any pointers will be greatly appreciated!</p>
",Multilingual Language Processing & Language Identification,algorithm translating mlb play play record descriptive text trying collect dataset could used automatically generating baseball article play play record mlb game retrosheet org would like written plain text could possibly appear part recap news article example play play record following would like achieve first example want written something like semim marcus semien wa three ball two strike second inning away player hit ball play one called strike one ball three foul another two ball fly ball wa caught right outfielder play formatted following way first field inning integer starting second field either visiting team home team third field retrosheet player id player plate fourth field count batter particular event play occurred retrosheet game information case appears field fifth field variable length contains pitch batter plate appearance described pitch unknown field left empty nothing comma sixth field describes play event occurred explanation symbol fifth sixth field found retrosheet page python able format info invariable length formatted sentence last two field difficulty thinking efficient way unparse correct wrong term use fifth sixth field pitch event occurred due variable length wide variety thing occur think could write rule based info retrosheet website looking suggestion smarter way wrote natural language processing tag hoping could trivial problem field pointer greatly appreciated
How to convert Speech to text using google api,"<p>what solution will you suggest
I want to Convert speech to text, not in English and then translate text to English look for specific keywords save data in the database
<a href=""https://i.sstatic.net/0ckZb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0ckZb.png"" alt=""enter image description here""></a></p>
",Multilingual Language Processing & Language Identification,convert speech text using google api solution suggest want convert speech text english translate text english look specific keywords save data database
Machine Learning and Natural Language Processing,"<p>Assume you know a student who wants to study Machine Learning and Natural Language Processing.</p>

<p>What specific computer science subjects should they focus on and which programming languages are specifically designed to solve these types of problems?</p>

<p>I am not looking for your favorite subjects and tools, but rather industry standards.</p>

<p><b>Example</b>: I'm guessing that knowing Prolog and Matlab might help them.  They also might want to study Discrete Structures*, Calculus, and Statistics.</p>

<p>*Graphs and trees. Functions: properties, recursive definitions, solving recurrences. Relations: properties, equivalence, partial order. Proof techniques, inductive proof. Counting techniques and discrete probability.  Logic: propositional calculus, first-order predicate calculus. Formal reasoning: natural deduction, resolution. Applications to program correctness and automatic reasoning. Introduction to algebraic structures in computing.</p>
",Multilingual Language Processing & Language Identification,machine learning natural language processing assume know student want study machine learning natural language processing specific computer science subject focus programming language specifically designed solve type problem looking favorite subject tool rather industry standard example guessing knowing prolog matlab might help also might want study discrete structure calculus statistic graph tree function property recursive definition solving recurrence relation property equivalence partial order proof technique inductive proof counting technique discrete probability logic propositional calculus first order predicate calculus formal reasoning natural deduction resolution application program correctness automatic reasoning introduction algebraic structure computing
Text analysis problems with german grammar,"<p>How do solutions like DeepL deal with the german grammar, especially with partical verbs that are consisting of two parts. I want to use the Apple NLP for text analysis and it works great for detecting nouns, adjectives and verbs but if there is a partical verb with 2 parts in the beginning and the end of the sentence it will not detect them as one verb. But DeepL does that. </p>

<p>For example the word ""Anfangen"" (to  start) is a German particle verb. Consisting of two parts, a base verb (‚Äúfangen‚Äù) and particle (‚Äúan‚Äù). But ""fangen"" as a single word means to catch. So NLP need to check that there is a particle in the sentence that belongs to ""fangen"". It's hard to explain. Or is it all about machine learning?</p>
",Multilingual Language Processing & Language Identification,text analysis problem german grammar solution like deepl deal german grammar especially partical verb consisting two part want use apple nlp text analysis work great detecting noun adjective verb partical verb part beginning end sentence detect one verb deepl doe example word anfangen start german particle verb consisting two part base verb fangen particle fangen single word mean catch nlp need check particle sentence belongs fangen hard explain machine learning
Part of Speech Tagging - where to start?,"<p>Hello I would like to know how to implement the solution to such a task:</p>

<p>There's a 500Mb file of plain English texts.<br><br></p>

<p>I'd like to collect the statistics about the frequency of words,
but additionally to be sure that each word is recognized correctly (or the majority of words).<br></p>

<p>In terms that 'cry' in the sentence ""she gave a loud CRY"" would 
be classified as a noun and ""Do not cry"" would give a verb to the statistics.</p>

<p>Also, it would be good to filter proper names, so that they formed another dictionary.</p>

<p>The other task would be more difficult. I would like to find the occurences of words that
come together quite often and to build the list of such occurences.</p>

<p>Let's say, ""green grass"", ""beautiful girl"", ""handle carefully"", ""you are right"".
So that we could say exactly, what word sequences are often used together in the language.</p>

<p>HOW WOULD I START? Are there open Java tools &amp; good books on the subject?</p>
",Multilingual Language Processing & Language Identification,part speech tagging start hello would like know implement solution task mb file plain english text like collect statistic frequency word additionally sure word recognized correctly majority word term cry sentence gave loud cry would classified noun cry would give verb statistic also would good filter proper name formed another dictionary task would difficult would like find occurences word come together quite often build list occurences let say green grass beautiful girl handle carefully right could say exactly word sequence often used together language would start open java tool good book subject
List of &quot;regular&quot; english words,"<p>So I found these 3 resources:</p>

<ul>
<li><a href=""https://github.com/dwyl/english-words"" rel=""nofollow noreferrer"">479k english words</a></li>
<li><a href=""https://gist.github.com/h3xx/1976236"" rel=""nofollow noreferrer"">100k most popular english words from Wiktionary</a></li>
<li><a href=""https://github.com/first20hours/google-10000-english"" rel=""nofollow noreferrer"">Google's most frequent 10,000 words</a></li>
</ul>

<p>I don't mind that in the list of 479k words there are words like <code>b</code> and <code>u</code> (words of which I've never seen), because this includes super obscure stuff. But what I'm surprised to find is that the Wiktionary frequent list includes these words:</p>

<pre><code>b
be
bel
bl
bleu
bu
eu
l
lb
le
leu
lu
lub
</code></pre>

<p>In addition, the wiki list <em>doesn't</em> include these words!</p>

<pre><code>lube
neg
tab
</code></pre>

<p>That is, there are many words that the wiki list has that I wouldn't think of as ""regular words"", and there are many ""regular words"" that the wiki list doesn't have.</p>

<p>So my question is if there is a list of ""regular"" words for download on the web in the public domain. By regular, I just mean words that you learn through reading books and such, not these words like <code>leu</code> and <code>bl</code> which for some reason are in the wiki featured list.</p>

<p>This is for an autocomplete component.</p>
",Multilingual Language Processing & Language Identification,list regular english word found resource k english word k popular english word wiktionary google frequent word mind list k word word like word never seen includes super obscure stuff surprised find wiktionary frequent list includes word addition wiki list include word many word wiki list ha think regular word many regular word wiki list question list regular word download web public domain regular mean word learn reading book word like reason wiki list autocomplete component
How to get phrase-level sentiment from Stanford Core NLP package,"<p>This might not be a very relevant question to this community. But I thought it would let me reach out to the wider computer science community and get help.</p>

<p>I am using the Stanford Core NLP package, more specifically the <a href=""http://nlp.stanford.edu/sentiment/index.html"" rel=""nofollow noreferrer"">Sentiment module</a> of it. I am getting sentence level sentiment by using the following command. </p>

<pre><code>java -cp stanford-corenlp-3.4.jar:stanford-corenlp-3.4-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-0.23.jar -mx2g edu.stanford.nlp.sentiment.SentimentPipeline -stdin &lt; input.txt
</code></pre>

<p>But I need the phrase-level sentiment, like we see in the <a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow noreferrer"">online demo</a>. I am not being able to figure out how.</p>

<p>EDIT: </p>

<p>After looking into the source code, I figured that just by adding another argument to the above-mentioned command, it is possible to get sentiment score for each node of the parse tree representation of a sentence. However, this gives only a numeric sentiment score as opposed to a positive/negative sentiment. But I think it is fairly trivial to translate this score to a binary positive/negative sentiment. The command is:</p>

<pre><code>java -cp stanford-corenlp-3.4.jar:stanford-corenlp-3.4-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-0.23.jar -mx2g edu.stanford.nlp.sentiment.SentimentPipeline -stdin -output PENNTREES &lt; input.txt
</code></pre>
",Multilingual Language Processing & Language Identification,get phrase level sentiment stanford core nlp package might relevant question community thought would let reach wider computer science community get help using stanford core nlp package specifically sentiment module getting sentence level sentiment using following command need phrase level sentiment like see online demo able figure edit looking source code figured adding another argument mentioned command possible get sentiment score node parse tree representation sentence however give numeric sentiment score opposed positive negative sentiment think fairly trivial translate score binary positive negative sentiment command
does python3 nltk word_tokenize() have a string length limit?,"<p>I have a dictionary consisting of a database primary key with a string.  </p>

<pre><code>self.mydict = {
1:'a small example'
,2:'some sentence'
,3:'a very long string around 30k characters'
}
</code></pre>

<p>For key value pairs where the string is length&lt;1000, everything tokenizes as I would expect.<br>
For a few very large strings (length=30,000), the tokenizer returns multiple broken lines in my csv output.</p>

<pre><code>def write_data(self):
    headers=[]
    for x,y in self.mydict.items():
        headers.append([word_tokenize(y)])
        print(len(y))

    with open(self.outputdata, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        for item in headers:
            writer.writerow(item)
</code></pre>

<p>Writing my results to a csv, I get the following:  </p>

<pre><code>['a','small','example']
['some','sentence']
['a','very','long',
string','around','30k','characters']""
</code></pre>

<p>So the 30k length string breaks for some reason, and appears to split onto another line. I can truncate the first ~1000 characters of the strings and this problem goes away, but I'd prefer to keep the long strings as I'm doing natural language processing. Is this bug due to the length of the string or the way I'm writing my csv?</p>
",Multilingual Language Processing & Language Identification,doe python nltk word tokenize string length limit dictionary consisting database primary key string key value pair string length everything tokenizes would expect large string length tokenizer return multiple broken line csv output writing result csv get following k length string break reason appears split onto another line truncate first character string problem go away prefer keep long string natural language processing bug due length string way writing csv
separate subsentences inside sentence without any coordination,"<p>I want to separate all subsentences inside a sentence. If the sentence have  punctuation or any coordination, I am able to separate them with spacy. But in case where there is no separation, have you any idea to deal with it?
For example, I have the sentence (in french):</p>

<pre><code>Je suis Linda je veux savoir votre nom.
</code></pre>

<p>I want to get:</p>

<pre><code>Je suis Linda
je veux savoir votre nom.
</code></pre>
",Multilingual Language Processing & Language Identification,separate subsentences inside sentence without coordination want separate subsentences inside sentence sentence punctuation coordination able separate spacy case separation idea deal example sentence french want get
JSONDecodeError: Expecting value: line 1 column 1 (char 0) using Translate API,"<p>While parsing a large text, and translating English language into Hindi, this error arises.
JSONDecodeError: Expecting value: line 1 column 1 (char 0)</p>

<p>I have tried initializing the api in every loop. Still no result. Tried again after 2 days, the error still arises. How to know if my IP is getting blocked? </p>

<pre><code>eng_words= []
for i in nltk_tokens:
    translator = Translator()
    langs = translator.detect(i)
    if all( [langs.lang == ""en"", langs.confidence &gt; 0.91] ):
        eng_words.append(i)
    else:
        language = translator.translate(i, dest='hi')
        word = language.text

 Error :  
 &lt;ipython-input-12-a89f1985a6a7&gt; in detect_lang(nltk_tokens)
  4     translator = Translator()
  5     for i in nltk_tokens:
  ----&gt; 6         langs = translator.detect(i)
  7         if all( [langs.lang == ""en"", langs.confidence &gt; 0.91] ):
  8             eng_words.append(i)

  JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></pre>
",Multilingual Language Processing & Language Identification,jsondecodeerror expecting value line column char using translate api parsing large text translating english language hindi error arises jsondecodeerror expecting value line column char tried initializing api every loop still result tried day error still arises know ip getting blocked
how to format my text dataset for training?,"<p>I'm new to python and machine learning, 
I'm working on training a chatbot</p>

<p>I collected (or wrote) large number of possible inputs in an excel file (.xlsx), I will train my dataset using LSTM and IOBES lableing, I will do the same as here :
<a href=""https://www.depends-on-the-definition.com/guide-sequence-tagging-neural-networks-python/"" rel=""nofollow noreferrer"">https://www.depends-on-the-definition.com/guide-sequence-tagging-neural-networks-python/</a></p>

<p>In the link you can see a snapshot of the dataset, I want to make my dataset like it.</p>

<p>my questions are :</p>

<p>1- Is there a way to split a sentence into words so I can do the tagging for words ? (there is a tool in Excel, I tried it but it is very exhausted).
2- I tried to convert my file to .cvs, but I've faced a lot of problems (it is with utf-8, because my dataset is not in english) , is there another extension ?</p>

<p>I really appreciate your help and advice.
Thank you</p>
",Multilingual Language Processing & Language Identification,format text dataset training new python machine learning working training chatbot collected wrote large number possible input excel file xlsx train dataset using lstm iobes lableing link see snapshot dataset want make dataset like question way split sentence word tagging word tool excel tried exhausted tried convert file cv faced lot problem utf dataset english another extension really appreciate help advice thank
Arabic NER get multiple entity,"<p>I have an Arabic  NER algorithm in Python and it classified as  below list  </p>

<p>[('ŸÑŸÑÿ£ŸÖŸÖ', 'B-ORG'), ('ÿßŸÑŸÖÿ™ÿ≠ÿØÿ©', 'I-ORG'), ('ÿ£ŸÜÿ∑ŸàŸÜŸäŸà', 'B-PERS'), ('ÿØŸàŸÜÿßŸÑÿØ', 'I-PERS')]</p>

<p>How can I get a combination of B-ORG and I-ORG as ORGANIZATION ..
Also B-PERS and I-PERS as PERSON </p>

<p>please</p>
",Multilingual Language Processing & Language Identification,arabic ner get multiple entity arabic ner algorithm python classified list b org org b pers pers get combination b org org organization also b pers pers person please
Intelligent text parsing and translation,"<p>What would be an intelligent way to store text, so that it can be intelligently parsed and translated later on.</p>

<p>For example, The employee is outstanding as he can identify his own strengths and weaknesses and is comfortable with himself.</p>

<p>The above could be the generic text which is shown to the user prior to evaluation. If the user is a Male (say Shaun) or female (say Mary), the above text should be translated as follows.</p>

<p>Mary is outstanding as she can identify her own strengths and weaknesses and is comfortable with herself.</p>

<p>Shaun is outstanding as he can identify his own strengths and weaknesses and is comfortable with himself.</p>

<ol>
<li><p>How do we store the evaluation criteria in the first place with appropriate place or token holders. (In the above case employee should be translated to employee name and based on his gender the words he or she, himself or herself needs to be translated)</p></li>
<li><p>Is there a mechanism to automatically translate the text with the above information.</p></li>
</ol>
",Multilingual Language Processing & Language Identification,intelligent text parsing translation would intelligent way store text intelligently parsed translated later example employee outstanding identify strength weakness comfortable could generic text shown user prior evaluation user male say shaun female say mary text translated follows mary outstanding identify strength weakness comfortable shaun outstanding identify strength weakness comfortable store evaluation criterion first place appropriate place token holder case employee translated employee name based gender word need translated mechanism automatically translate text information
Get Stanford NER result through NLTK with IOB format,"<p>i'm using nltk as interface for Stanford NER Tagger. I have question that are there any options to <strong>get NER result as IOB format using NLTK</strong>? I've read this <a href=""https://stackoverflow.com/questions/21469082/how-do-i-use-iob-tags-with-stanford-ner"">question</a> but it's for java user</p>

<p>NLTK version:  3.4</p>

<p>Java version:  jdk1.8.0_211/bin</p>

<p>Stanford NER model:  english.conll.4class.distsim.crf.ser.gz</p>

<p><strong>Input</strong>: My name is Donald Trumph</p>

<p><strong>Expected output</strong>: My/O name/O is/O Donald/B-PERSON Trumph/I-PERSON</p>
",Multilingual Language Processing & Language Identification,get stanford ner result nltk iob format using nltk interface stanford ner tagger question option get ner result iob format using nltk read href java user p nltk version java version jdk bin stanford ner model english conll class distsim crf ser gz input name donald trumph expected output name donald b person trumph person
How to detect the dominant language of a text word?,"<p>It's looks good for <code>string</code> but it's not working for me for a <code>word</code>. I am working with search as per as my requirement when user typing any 3 character in the meantime looking to check which language user typing. if I think it should not work with <code>detec0t</code> word but i expect it should be working with <code>Islam</code> word. </p>

<pre><code>let tagger = NSLinguisticTagger(tagSchemes:[.tokenType, .language, .lexicalClass, .nameType, .lemma], options: 0)

func determineLanguage(for text: String) {
    tagger.string = text
    let language = tagger.dominantLanguage
    print(""The language is \(language!)"")
}


//Test case
determineLanguage(for: ""I love Islam"") // en -pass
determineLanguage(for: ""‡¶Ü‡¶Æ‡¶ø ‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø"") // bn -pass
determineLanguage(for: ""ÿ£ŸÜÿß ÿ£ÿ≠ÿ® ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖ"") // ar -pass
determineLanguage(for: ""Islam"") // und - failed
</code></pre>

<p><strong>Result:</strong></p>

<blockquote>
  <p>The language is en <br>
  The language is bn <br>
  The language is ar <br>
  The language is und</p>
</blockquote>

<p>What I missed for ""Unknown language"" </p>
",Multilingual Language Processing & Language Identification,detect dominant language text word look good working working search per requirement user typing character meantime looking check language user typing think work word expect working word result language en language bn language ar language und missed unknown language
How to find which document is belong to which cluster?,"<p>I am new in natural language processing and I found <a href=""https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"" rel=""nofollow noreferrer"">this</a> interesting tutorial which describes how to do the topic modeling.</p>

<p>Available <a href=""https://www.kaggle.com/therohk/million-headlines/data"" rel=""nofollow noreferrer"">data</a> for this tutorial </p>

<p>Source code: <a href=""https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"" rel=""nofollow noreferrer"">here</a></p>

<p>The above code can provide topic modeling using LDA and generates the k number of topic. My question is how can I find which document belongs to which topic (cluster)? Like the example shown in figure <a href=""https://imgur.com/hA0dqUL"" rel=""nofollow noreferrer"">here</a>. I wondering something like:</p>

<blockquote>
  <p>publish_date:20030219 with text (aba ...) belongs to topic 1 cluster
  or ..</p>
</blockquote>

<p>I already read the post such as:
<a href=""https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi/20991190"">[1]</a> or <a href=""https://stackoverflow.com/questions/51448833/topicmodel-how-to-query-documents-by-topic-model-topic"">[2]</a> but still, I couldn't get my answer.</p>

<p>I also tried Matlab text analytic toolbox but I couldn't figure that out yet. </p>

<p>It would be great if you can provide me any help.</p>
",Multilingual Language Processing & Language Identification,find document belong cluster new natural language processing found interesting tutorial describes topic modeling available data tutorial source code code provide topic modeling using lda generates k number topic question find document belongs topic cluster like example shown figure wondering something like publish date text aba belongs topic cluster already read post also tried matlab text analytic toolbox figure yet would great provide help
Python beginner : Preprocessing a french text in python and calculate the polarity with a lexicon,"<p>I am writing an algorithm in python which processes a column of sentences and then gives the polarity (positive or negative) of each cell of my column of sentences. The script uses a list of negative and positive word from the NRC emotion lexicon (French version) I am having a problem writing the preprocess function. I have already written the count function and the polarity function but since I have some difficulty writing the preprocess function, I am not really sure if those functions works. </p>

<p>The positive and negative words were in the same file (lexicon) but I export positive and negztive words separately because I did not know how to use the lexicon as it was. </p>

<p>My function count occurrence of positive and negative does not work and I do not know why it Always sends me 0. I Added positive word in each sentence so  the should appear in the dataframe:</p>

<p>stacktrace :</p>

<pre><code>
[4 rows x 6 columns]
   id                                           Verbatim      ...       word_positive  word_negative
0  15  Je n'ai pas bien compris si c'√©tait destin√© a ...      ...                   0              0
1  44  Moi a√©rien affable affaire agent de conservati...      ...                   0              0
2  45  Je affectueux affirmative te hais et la Foret ...      ...                   0              0
3  47  Je absurde accidentel accusateur accuser affli...      ...                   0              0

=&gt;  
def count_occurences_Pos(text, word_list):
    '''Count occurences of words from a list in a text string.'''
    text_list = process_text(text)

    intersection = [w for w in text_list if w in word_list]


    return len(intersection)
csv_df['word_positive'] = csv_df['Verbatim'].apply(count_occurences_Pos, args=(lexiconPos, ))
</code></pre>

<p>This my csv_data : line 44 , 45  contains positive words and line 47 more negative word but in the column of positive and negative word , it is alwaqys empty, the function does not return the number of words  and the final column is Always positive whereas the last sentence is negative</p>

<pre><code>id;Verbatim
15;Je n'ai pas bien compris si c'√©tait destin√© a rester
44;Moi a√©rien affable affaire agent de conservation qui ne agraffe connais rien, je trouve que c'est s'emmerder pour rien, il suffit de mettre une multiprise
45;Je affectueux affirmative te hais et la Foret enchant√©e est belle de milles faux et les jeunes filles sont assises au bor de la mer
47;Je absurde accidentel accusateur accuser affliger affreux agressif allonger allusionne admirateur admissible adolescent agent de police Comprends pas la vie et je suis perdue 
</code></pre>

<p>Here the full code :</p>

<pre><code># -*- coding: UTF-8 -*-
import codecs 
import re
import os
import sys, argparse
import subprocess
import pprint
import csv
from itertools import islice
import pickle
import nltk
from nltk import tokenize
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import pandas as pd
try:
    import treetaggerwrapper
    from treetaggerwrapper import TreeTagger, make_tags
    print(""import TreeTagger OK"")
except:
    print(""Import TreeTagger pas Ok"")

from itertools import islice
from collections import defaultdict, Counter



csv_df = pd.read_csv('test.csv', na_values=['no info', '.'], encoding='Cp1252', delimiter=';')
#print(csv_df.head())

stopWords = set(stopwords.words('french'))  
tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')     
def process_text(text):
    '''extract lemma and lowerize then removing stopwords.'''

    text_preprocess =[]
    text_without_stopwords= []

    text = tagger.tag_text(text)
    for word in text:
        parts = word.split('\t')
        try:
            if parts[2] == '':
                text_preprocess.append(parts[1])
            else:
                text_preprocess.append(parts[2])
        except:
            print(parts)


    text_without_stopwords= [word.lower() for word in text_preprocess if word.isalnum() if word not in stopWords]
    return text_without_stopwords

csv_df['sentence_processing'] = csv_df['Verbatim'].apply(process_text)
#print(csv_df['word_count'].describe())
print(csv_df)


lexiconpos = open('positive.txt', 'r', encoding='utf-8')
print(lexiconpos.read())
def count_occurences_pos(text, word_list):
    '''Count occurences of words from a list in a text string.'''

    text_list = process_text(text)

    intersection = [w for w in text_list if w in word_list]

    return len(intersection)


#csv_df['word_positive'] = csv_df['Verbatim'].apply(count_occurences_pos, args=(lexiconpos, ))
#print(csv_df)

lexiconneg = open('negative.txt', 'r', encoding='utf-8')

def count_occurences_neg(text, word_list):
    '''Count occurences of words from a list in a text string.'''
    text_list = process_text(text)

    intersection = [w for w in text_list if w in word_list]

    return len(intersection)
#csv_df['word_negative'] = csv_df['Verbatim'].apply(count_occurences_neg, args= (lexiconneg, ))
#print(csv_df)

def polarity_score(text):   
    ''' give the polarity of each text based on the number of positive and negative word '''
    positives_text =count_occurences_pos(text, lexiconpos)
    negatives_text =count_occurences_neg(text, lexiconneg)
    if positives_text &gt; negatives_text :
        return ""positive""
    else : 
        return ""negative""
csv_df['polarity'] = csv_df['Verbatim'].apply(polarity_score)
#print(csv_df)
print(csv_df)
</code></pre>

<p>If you could also see if the rest of the code is good to thank you.</p>
",Multilingual Language Processing & Language Identification,python beginner preprocessing french text python calculate polarity lexicon writing algorithm python process column sentence give polarity positive negative cell column sentence script us list negative positive word nrc emotion lexicon french version problem writing preprocess function already written count function polarity function since difficulty writing preprocess function really sure function work positive negative word file lexicon export positive negztive word separately know use lexicon wa function count occurrence positive negative doe work know always sends added positive word sentence appear dataframe stacktrace csv data line contains positive word line negative word column positive negative word alwaqys empty function doe return number word final column always positive whereas last sentence negative full code could also see rest code good thank
Special characters training NLP models,"<p>I just started with NLP in bots, where a user ask a question that is classified by LUIS and then forwarded to QnAMaker to get an answer, and I have noticed that it behaves strangely with Spanish since we have accented characters and double question marks (¬ø?). For example:</p>

<pre><code>[1] ¬øqu√© es NLP?
[2] que es NLP
</code></pre>

<p>If I train my model with the first one and test it with the second one, the model won't identify both of them with the same intent. This is a very common way to communicate in Spanish since some people tend to save time by avoiding accented charactes and punctuation.</p>

<p>My questions are: </p>

<ul>
<li>Should I normalize every utterance in my model (removing accents,
punctuation, etc.)? Or should I train it with every different example?</li>
<li>Are there any guidelines for training NLP models that I can base my work in?</li>
</ul>
",Multilingual Language Processing & Language Identification,special character training nlp model started nlp bot user ask question classified luis forwarded qnamaker get answer noticed behaves strangely spanish since accented character double question mark example train model first one test second one model identify intent common way communicate spanish since people tend save time avoiding accented charactes punctuation question normalize every utterance model removing accent punctuation etc train every different example guideline training nlp model base work
How to compute the perplexity in text classification?,"<p>I'm doing dialect text classification with scikit learn, naive bayes and countvectorizer. So far I'm only doing 3 dialects text classification. I'm going to add a new dialect(or actually, the formal language for those dialects). The problem is, the new text that I'm going to add, shares a lot of words with the other 3 dialects. So I read the following in a research document:</p>

<blockquote>
  <p>We train an n-gram model for each dialect from the collected data. To
  train the MSA model, we select sentences from Arabic UN corpus and
  news collections. All the dialect and MSA models share the same
  vocabulary, thus perplexity can be compared properly. At
  classification time, given an input sentence, the classifier computes
  the perplexity for each dialect type and choose the one with minimum
  perplexity as the label.</p>
</blockquote>

<p>They mean by MSA(Modern Standard Arabic) which is the formal language for those dialects. How are they  calculating the perplexity? Are they just using naive bayes or there's more to it?</p>
",Multilingual Language Processing & Language Identification,compute perplexity text classification dialect text classification scikit learn naive bayes countvectorizer far dialect text classification going add new dialect actually formal language dialect problem new text going add share lot word dialect read following research document train n gram model dialect collected data train msa model select sentence arabic un corpus news collection dialect msa model share vocabulary thus perplexity compared properly classification time given input sentence classifier computes perplexity dialect type choose one minimum perplexity label mean msa modern standard arabic formal language dialect calculating perplexity using naive bayes
WMT2018 news commentary Chinese dataset weird character,"<p>I'm working on the WMT2018 dataset for Chinese to do machine translation. I found that there are a lot of weird characters in the data which look like the following:</p>

<p><a href=""https://i.sstatic.net/BAvg1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BAvg1.png"" alt=""enter image description here""></a></p>

<p>I thought this was the encoding problem, so I converted it to a text file with UTF-8 that supports Chinese characters, but this problem remains.</p>

<p>Any idea on how to solve this issue? </p>
",Multilingual Language Processing & Language Identification,wmt news commentary chinese dataset weird character working wmt dataset chinese machine translation found lot weird character data look like following thought wa encoding problem converted text file utf support chinese character problem remains idea solve issue
How to train a classifier to detect vernacular from grammatical language?,"<p>I'm using text classification to classify Arabic dialects, so far I have 4 dialects. However, now I want the classifier to detect the formal(standard or grammatical) language of those dialects which is called MSA(Modern Standard Arabic). </p>

<p>Should I use grammatical analysis? build a language model? or I do the same as I did with the dialects by collecting MSA tweets and then train them?</p>
",Multilingual Language Processing & Language Identification,train classifier detect vernacular grammatical language using text classification classify arabic dialect far dialect however want classifier detect formal standard grammatical language dialect called msa modern standard arabic use grammatical analysis build language model dialect collecting msa tweet train
how does BLEU score works ? . And how does it differ from the simple Jaccard score based similarity prediction,"<p>I was trying to understand the concept of BLEU score . 
And then i found this article that had written </p>

<p>""The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where 1-gram or unigram would be each token and a bigram comparison would be each word pair.""</p>

<p>But that is almost same as when calculating the jaccard score for the similarity scoe b/w two texts .</p>

<p>Could someone explain me how the BLEU score works , and the difference b/w it and the jaccard score based similarity measures .</p>
",Multilingual Language Processing & Language Identification,doe bleu score work doe differ simple jaccard score based similarity prediction wa trying understand concept bleu score found article written approach work counting matching n gram candidate translation n gram reference text gram unigram would token bigram comparison would word pair almost calculating jaccard score similarity scoe b w two text could someone explain bleu score work difference b w jaccard score based similarity measure
Predict value for multiple columns in Natural language processing,"<p>I have a dataset of emails contains 4 fields. One is exact number of people, second is date, person need laptop or not, person interested in cafeteria or not. And, want to predict values for all these features using natural language processing. So, how can I create the dataset and apply natural language processing on that?</p>
",Multilingual Language Processing & Language Identification,predict value multiple column natural language processing dataset email contains field one exact number people second date person need laptop person interested cafeteria want predict value feature using natural language processing create dataset apply natural language processing
Multi-intent natural language processing and classification,"<p>So, I'm making my own home assistant and I'm trying to make a multi-intent classification system. However, I cannot find a way to split the query said by the user into the multiple different intents in the query.</p>

<p>For example:</p>

<pre><code>I have my data for one of my intents (same format for all) 

{""intent_name"": ""music.off"" , ""examples"": [""turn off the music"" , ""kill 
the music"" , ""cut the music""]}
</code></pre>

<p>and the query said by the user would be:</p>

<p><code>'dim the lights, cut the music and play Black Mirror on tv'</code></p>

<p>I want to split the sentence into their individual intents such as :</p>

<p><code>['dim the lights', 'cut the music', 'play black mirror on tv']</code></p>

<p>however, I can't just use <code>re.split</code> on the sentence with <code>and</code> and <code>,</code> as delimiters to split with as if the user asks :</p>

<p><code>'turn the lights off in the living room, dining room, kitchen and bedroom'</code> </p>

<p>this will be split into </p>

<p><code>['turn the lights off in the living room', 'kitchen', 'dining room', 'bedroom']</code></p>

<p>which would not be usable with my intent detection</p>

<p>this is my problem, thank you in advance</p>

<h2>UPDATE</h2>

<p>okay so I've got this far with my code, it can get the examples from my data and identify the different intents inside as I wished however it is not splitting the parts of the original query into their individual intents and is just matching. </p>

<pre><code>import nltk
import spacy
import os
import json
#import difflib
#import substring
#import re
#from fuzzysearch import find_near_matches
#from fuzzywuzzy import process

text = ""dim the lights, shut down the music and play White Collar""

commands = []

def get_matches():

    for root, dirs, files in os.walk(""./data""):  

        for filename in files:

            f = open(f""./data/{filename}"" , ""r"")
            file_ = f.read()
            data = json.loads(file_)

            choices.append(data[""examples""])

        for set_ in choices:

            command = process.extract(text, set_ , limit=1)

            commands.append(command)

    print(f""all commands : {commands}"")
</code></pre>

<p>this returns <code>[('dim the lights') , ('turn off the music') , ('play Black Mirror')]</code> which is the correct intents but I have no way of knowing which part of the query relates to each intent - this is the main problem</p>

<p>my data is as follows , very simple for now until I figure out a method:</p>

<p><code>play.json</code></p>

<p><code>{""intent_name"": ""play.device"" , ""examples"" : [""play Black Mirror"" , ""play Netflix on tv"" , ""can you please stream Stranger Things""]}</code></p>

<hr>

<p><code>music.json</code></p>

<p><code>{""intent_name"": ""music.off"" , ""examples"": [""turn off the music"" , ""cut the music"" , ""kill the music""]}</code></p>

<hr>

<p><code>lights.json</code></p>

<p><code>{""intent_name"": ""lights.dim"" , ""examples"" : [""dim the lights"" , ""turn down the lights"" , ""lower the brightness""]}</code></p>
",Multilingual Language Processing & Language Identification,multi intent natural language processing classification making home assistant trying make multi intent classification system however find way split query said user multiple different intent query example query said user would want split sentence individual intent however use sentence delimiters split user asks split would usable intent detection problem thank advance update okay got far code get example data identify different intent inside wished however splitting part original query individual intent matching return correct intent way knowing part query relates intent main problem data follows simple figure method
Shuffling pairs of lines in two text files,"<p>I'm working on a machine translation project in which I have 4.5 million lines of text in two languages, <a href=""https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en"" rel=""nofollow noreferrer"">English</a> and <a href=""https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de"" rel=""nofollow noreferrer"">German</a>. I would like to shuffle these lines prior to dividing the data into shards on which I will train my model. I know the <code>shuf</code> command described <a href=""https://stackoverflow.com/questions/2153882/how-can-i-shuffle-the-lines-of-a-text-file-on-the-unix-command-line-or-in-a-shel/30133294"">here</a> allows one to shuffle lines in one file, but how can I ensure that corresponding lines in the second file are also shuffled into the same order? Is there a command to shuffle lines in both files?</p>
",Multilingual Language Processing & Language Identification,shuffling pair line two text file working machine translation project million line text two language english german would like shuffle line prior dividing data shard train model know command described href allows one shuffle line one file ensure corresponding line second file also shuffled order command shuffle line file p
NLP: Validate a sentence against a given grammar,"<p>I have a corpus of English sentences  </p>

<pre><code>sentences = [
    ""Mary had a little lamb."",
    ""John has a cute black pup."",
    ""I ate five apples.""
]
</code></pre>

<p>and a grammar (for the sake of simplicity)</p>

<pre><code>grammar = ('''
    NP: {&lt;NNP&gt;&lt;VBZ|VBD&gt;&lt;DT&gt;&lt;JJ&gt;*&lt;NN&gt;&lt;.&gt;} # NP
    ''')
</code></pre>

<p>I wish to filter out the sentences which don't conform to the grammar. 
Is there a built-in NLTK function which can achieve this?
In the above example, first two sentences follow the pattern of my grammar, but not the last one.</p>
",Multilingual Language Processing & Language Identification,nlp validate sentence given grammar corpus english sentence grammar sake simplicity wish filter sentence conform grammar built nltk function achieve example first two sentence follow pattern grammar last one
How to detect non-words in a text?,"<p>I would like to write an algorithm that let me to detect the non-words in a sentence. I'm working with french documents and I'm using an OCR to recognize sentences in it. </p>

<p>Sometimes, the OCR (Tess4J - Tesseract OCR for Java) recognize some parts of the document as words like signature or other non-words things. </p>

<p>In this way, I have got some weird result. See below an example:</p>

<pre><code>l'int√©ress√©(e) devra contacter le Minist√®re du_D√©veloppement durable et des Infrastructures pour se faire d√©livrer un permis de conduire. √¶ P C√Ç_√§'‚Äù‚Äú"":√¶‚Äú""‚Äîg fin de document.
</code></pre>

<p>Here the sequence <code>√¶ P C√Ç_√§'‚Äù‚Äú"":√¶‚Äú""‚Äîg</code> is not a word. </p>

<p>So, how can I detect these ""weird"" words and remove it?</p>
",Multilingual Language Processing & Language Identification,detect non word text would like write algorithm let detect non word sentence working french document using ocr recognize sentence sometimes ocr te j tesseract ocr java recognize part document word like signature non word thing way got weird result see example sequence word detect weird word remove
Is there any Namespace or Package in Any Language that Check English Grammar Check,"<p>I want to make many large software systems which requires the functions that have features like English Grammar Check and all other validations/manipulations that can be done in English. Is there any package available in any programming language that gives me these resources? Just imagine I am making a software like MSword, and I want to add Grammar Validations in my software same as in MSword, is there anything available or I have to code everything from scratch? How hard it would be according to you? How much time would it take?</p>
",Multilingual Language Processing & Language Identification,namespace package language check english grammar check want make many large software system requires function feature like english grammar check validation manipulation done english package available programming language give resource imagine making software like msword want add grammar validation software msword anything available code everything scratch hard would according much time would take
Starting out NLP - Python + large data set,"<p>I've been wanting to learn python and do some NLP, so have finally gotten round to starting. Downloaded the english wikipedia mirror for a nice chunky dataset to start on, and have been playing around a bit, at this stage just getting some of it into a sqlite db (havent worked with dbs in the past unfort).</p>

<p>But I'm guessing sqlite is not the way to go for a full blown nlp project(/experiment :) - what would be the sort of things I should look at ? HBase (.. and hadoop) seem interesting, i guess i could run then im java, prototype in python and maybe migrate the really slow bits to java... alternatively just run Mysql.. but the dataset is 12gb, i wonder if that will be a problem? Also looked at lucene, but not sure how (other than breaking the wiki articles into chunks) i'd get that to work.. </p>

<p>What comes to mind for a really flexible NLP platform (i dont really know at this stage WHAT i want to do.. just want to learn large scale lang analysis tbh) ?</p>

<p>Many thanks.</p>
",Multilingual Language Processing & Language Identification,starting nlp python large data set wanting learn python nlp finally gotten round starting downloaded english wikipedia mirror nice chunky dataset start playing around bit stage getting sqlite db havent worked db past unfort guessing sqlite way go full blown nlp project experiment would sort thing look hbase hadoop seem interesting guess could run im java prototype python maybe migrate really slow bit java alternatively run mysql dataset gb wonder problem also looked lucene sure breaking wiki article chunk get work come mind really flexible nlp platform dont really know stage want want learn large scale lang analysis tbh many thanks
Filtering list of Arabic sentences based on language test: Why so slow?,"<p>I'm trying to go through a list of (mostly) Arabic sentences, and remove those that are not Arabic. I've got a hack for telling if a character is Arabic or not: Arabic has no case, so if the character is alpha but isn't upper case or lower case, it's Arabic.</p>

<p>I've got the code below, which works, but the language identification part is very slow, compared to the other filter. It doesn't seem to me like it's doing anything particularly complex, so I don't understand why it's taking so long. (The corpus is size is about 300K sentences before filtering.)</p>

<p>Is there something I can do to make it more efficient?</p>

<p>Thanks!</p>

<pre><code>def test_lang(string):
    """"""Takes a string and determines if it is written in Arabic 
    characters or foreign, by testing whether the first character 
    has a case attribute. This is intended for Arabic texts that  
    may have English or French words added. If it encounters another 
    case-less language (Chinese for instance), it will falsely 
    identify it as Arabic.""""""

    if not string or not string.isalpha():
        return None
    char = string[0]
    if char.isalpha() and not (char.islower() or char.isupper()):
        lang = 'AR'
    else:
        lang = 'FW'
    return lang
</code></pre>

<p>...</p>

<pre><code># remove sentences that are in English or French - THIS IS SLOW (takes a few mins)
for sent in sents:
    if sent and test_lang(sent[0]) != 'AR':
        sents.remove(sent)

# remove clearly MSA sentences -- THIS IS FAST (takes a few seconds)
msa_features = ['ŸÑŸäÿ≥','ŸÑÿ≥ÿ™','ŸÑŸäÿ≥ÿ™','ŸÑŸäÿ≥Ÿàÿß','ÿßŸÑÿ∞Ÿä','ÿßŸÑÿ∞ŸäŸÜ','ÿßŸÑÿ™Ÿä','ŸÖÿßÿ∞ÿß', 'ÿπŸÜ']
p = re.compile('|'.join(msa_features))
for sent in sents:
    if re.search(p, sent):
        sents.remove(sent)
</code></pre>
",Multilingual Language Processing & Language Identification,filtering list arabic sentence based language test slow trying go list mostly arabic sentence remove arabic got hack telling character arabic arabic ha case character alpha upper case lower case arabic got code work language identification part slow compared filter seem like anything particularly complex understand taking long corpus size k sentence filtering something make efficient thanks
List of greetings phrases in english for NLP task?,"<p>I want lists of greetings phrases in English for natural language processing tasks, so I wonder where can I find something like that ?</p>
",Multilingual Language Processing & Language Identification,list greeting phrase english nlp task want list greeting phrase english natural language processing task wonder find something like
Someone can give a simple explanation about the elements of Natural Language Processing?,"<p>I'm new to Natural Language Processing and I'm a confused about the terms used.</p>

<p>What is tokenization? POS tagging? Entity Identify?</p>

<p>Tokenization is only split the text in parts that can have a meaning or give a meaning for these parts? And the meaning, what is the name when I determine that something is a noun, verb or adjetive. And if I want to divide into dates, names, currency?</p>

<p>I need a simple explanation about the areas/terms used in NLP.</p>
",Multilingual Language Processing & Language Identification,someone give simple explanation element natural language processing new natural language processing confused term used tokenization po tagging entity identify tokenization split text part meaning give meaning part meaning name determine something noun verb adjetive want divide date name currency need simple explanation area term used nlp
Best way for synonyms if data is stored outside dialogflow?,"<p>I want to create a chatbot. One of the intents is <code>who-is</code> which allows users to ask ""Who is"" for Arabic names to get information on a person. I have people stored in a database (firestore). I would like the user to say ""Who is Saalih Uthaymeen?"" But they may ask ""Who is <strong>Shaykh</strong> Saalih <strong>al</strong>-'Uthaymeen?"" as well. The names are different, but they're the same really. And they're close in spelling.</p>

<p>I noticed a synonyms feature in dialogflow, but I don't have the names stored in dialogflow so I don't know if it's useful. Since the synonyms have similar spelling, can't dialogflow help? Otherwise, I see the following options:</p>

<h2>Option 1. Manually create every name and its synonyms in dialogflow.</h2>

<p>This is manual hard work. Even if I programmatically enter every name, I have to manually enter every synonym. And I have 2 or 3 hundred names.</p>

<h2>Option 2. Manually create synonyms in my database.</h2>

<p>Basically, I have a table of people... so I would create a new table mapping every person to every synonym possible for his name. Since the spellings will be very similar, I'm confident this is a waste of work and time.</p>

<p>Is there any other option Dialogflow offers?</p>
",Multilingual Language Processing & Language Identification,best way synonym data stored outside dialogflow want create chatbot one intent allows user ask arabic name get information person people stored database firestore would like user say saalih uthaymeen may ask shaykh saalih al uthaymeen well name different really close spelling noticed synonym feature dialogflow name stored dialogflow know useful since synonym similar spelling dialogflow help otherwise see following option option manually create every name synonym dialogflow manual hard work even programmatically enter every name manually enter every synonym hundred name option manually create synonym database basically table people would create new table mapping every person every synonym possible name since spelling similar confident waste work time option dialogflow offer
Some problems with loading models in Stanford Neural Network Dependency Parser,"<p>I'm using the Stanford Neural network Dependency Parser. I've trained some models on French treebanks (GSD, ParTUT, Sequoia, Spoken), and now I'm trying to generate the output of the model on the test segments of the treebanks. It all works fine with ParTUT, Sequoia and Spoken, but GSD gives me some problems. The command I run is:</p>

<pre><code>java -Xmx1g -cp ""*"" edu.stanford.nlp.parser.nndep.DependencyParser \ -model Stf_ud_gsd_2200.model.txt.gz.gz -testFile fr_gsd_ud_test_new.conllu -outFile FR/Stf_gsd_ud.conllu
</code></pre>

<p>I receive the following error:</p>

<pre><code>SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""358,6""
        at sun.misc.FloatingDecimal.readJavaFormatString(Unknown Source)
        at sun.misc.FloatingDecimal.parseDouble(Unknown Source)
        at java.lang.Double.parseDouble(Unknown Source)
        at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:570)
        at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:508)
        at edu.stanford.nlp.parser.nndep.DependencyParser.main(DependencyParser.java:1284)
</code></pre>

<p>If I understand this correctly, the problem does not stem from the test-treebank, but rather from the model itself, and the way some numbers have been saved there.</p>

<p>Does anyone have any tips on how to overcome it? I'd be very grateful for help!</p>
",Multilingual Language Processing & Language Identification,problem loading model stanford neural network dependency parser using stanford neural network dependency parser trained model french treebanks gsd partut sequoia spoken trying generate output model test segment treebanks work fine partut sequoia spoken gsd give problem command run receive following error understand correctly problem doe stem test treebank rather model way number saved doe anyone tip overcome grateful help
NGram based Language detection William B. Cavnar and John M. Trenkle,"<p>I am trying to implement the NGram based Langauage detection paper by William B. Cavnar and John M. Trenkle using <a href=""https://github.com/z0mbiehunt3r/ngrambased-textcategorizer/blob/master/ngramfreq.py"" rel=""nofollow noreferrer"">https://github.com/z0mbiehunt3r/ngrambased-textcategorizer/blob/master/ngramfreq.py</a></p>

<pre><code>import operator
import string
import glob
import os.path
from nltk.util import ngrams

#file which contains the language to be detected
filename = raw_input(""Enter the file name: "")
fp = open(filename)
text = str(fp.read())
fp.close()

#tokenize the text
rawtext = text.translate(None, string.punctuation)
words = [w.lower() for w in rawtext.split("" "")]

#generate ngrams for the text
gen_ngrams=[]
for word in words:
    for i in range(1,6):
        temp = ngrams(word, i, pad_left = True, pad_right = True, left_pad_symbol = ' ', right_pad_symbol =' ')
    #join the characters of individual ngrams
        for t in temp:
            ngram = ' '.join(t)
            gen_ngrams.append(ngram)

#calculate ngram frequencies of the text
ngram_stats = {}
for n in gen_ngrams:
    if not ngram_stats.has_key(n):
        ngram_stats.update({n:1})
    else:
        ng_count = ngram_stats[n]
        ngram_stats.update({n:ng_count+1})

#now sort them, add an iterator to dict and reverse sort based on second column(count of ngrams)
ngrams_txt_sorted = sorted(ngram_stats.iteritems(), key=operator.itemgetter(1), reverse = True)[0:300]

#Load ngram language statistics
lang_stats={}
for filepath in glob.glob('./langdata/*.dat'):
    filename = os.path.basename(filepath)
    lang = os.path.splitext(filename)[0]
    ngram_stats = open(filepath,""r"").readlines()
    ngram_stats = [x.rstrip() for x in ngram_stats]
    lang_stats.update({lang:ngram_stats})


#compare ngram frequency statistics by doing a rank order lookup
lang_ratios = {}
txt_ng = [ng[0] for ng in ngrams_txt_sorted]
print txt_ng
max_out_of_place = len(txt_ng)
for lang, ngram_stat in lang_stats.iteritems():
    lang_ng = [ng[0] for ng in lang_stats]
    doc_dist = 0
    for n in txt_ng:
        try:
            txt_ng_index = txt_ng.index(n)
            lang_ng_index = lang_ng.index(n)
        except ValueError:
            lang_ng_index = max_out_of_place
        doc_dist += abs(lang_ng_index - txt_ng_index)
    lang_ratios.update({lang:doc_dist})
for i in lang_ratios.iteritems():
    print i
predicted_lang = min(lang_ratios, key=lang_ratios.get)
print ""The language is"",predicted_lang
</code></pre>

<p>It outputs 'English' every time I execute it. The computed distances are always the same for all the languages. I am not able to figure out the logical error in the above code. Kindly help me.</p>
",Multilingual Language Processing & Language Identification,ngram based language detection william b cavnar john trenkle trying implement ngram based langauage detection paper william b cavnar john trenkle using output english every time execute computed distance always language able figure logical error code kindly help
Calculate TD-IDF for a single word in Textacy,"<p>I'm trying to use <a href=""https://github.com/chartbeat-labs/textacy"" rel=""noreferrer"">Textacy</a> to calculate the TF-IDF score for a single word across the standard corpus, but am a bit unclear about the result I am receiving.</p>

<p>I was expecting a single float which represented the frequency of the word in the corpus. So why am I receiving a list (?) of 7 results?</p>

<p>""acculer"" is actually a French word, so was expecting a result of 0 from an English corpus.</p>

<pre class=""lang-py prettyprint-override""><code>word = 'acculer'
vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=True, idf_type='smooth')
tf_idf = vectorizer.fit_transform(word)
logger.info(""tf_idf:"")
logger.info(tfidf)
</code></pre>

<p>Output</p>

<pre><code>tf_idf:
(0, 0)  2.386294361119891
(1, 1)  1.9808292530117262
(2, 1)  1.9808292530117262
(3, 5)  2.386294361119891
(4, 3)  2.386294361119891
(5, 2)  2.386294361119891
(6, 4)  2.386294361119891
</code></pre>

<p>The second part of the question is how can I provide my own corpus to the TF-IDF function in Textacy, esp. one in a different language?</p>

<p><strong>EDIT</strong></p>

<p>As mentioned by @Vishal I have logged the ouput using this line:</p>

<pre><code>logger.info(vectorizer.vocabulary_terms)
</code></pre>

<p>It seems the provided word <code>acculer</code> has been split into characters. </p>

<pre><code>{'a': 0, 'c': 1, 'u': 5, 'l': 3, 'e': 2, 'r': 4}
</code></pre>

<p>(1) How can I get the TF-IDF for this word against the corpus, rather than each character?</p>

<p>(2) How can I provide my own corpus and point to it as a param?</p>

<p>(3) Can TF-IDF be used at a sentence level? ie: what is the relative frequency of this sentence's terms against the corpus.</p>
",Multilingual Language Processing & Language Identification,calculate td idf single word textacy trying use textacy calculate tf idf score single word across standard corpus bit unclear result receiving wa expecting single float represented frequency word corpus receiving list result acculer actually french word wa expecting result english corpus output second part question provide corpus tf idf function textacy esp one different language edit mentioned vishal logged ouput using line seems provided word ha split character get tf idf word corpus rather character provide corpus point param tf idf used sentence level ie relative frequency sentence term corpus
How to get a list of parts of speech and words from Princeton English WordNet?,"<p>I'd like a complete list of parts of speech (e.g., adj., adv., and .v) for English words. All I need is just a TSV table with two columns, with the first column the word and the second column POS.</p>

<p>I know that wordnet should contain such information. But it contains more than what I needed. I am not sure which file I should use.</p>

<p><a href=""https://wordnet.princeton.edu/download"" rel=""nofollow noreferrer"">https://wordnet.princeton.edu/download</a></p>

<p>Does anybody know a convenience-to-use file that contains English words and their POS? Thanks.</p>
",Multilingual Language Processing & Language Identification,get list part speech word princeton english wordnet like complete list part speech e g adj adv v english word need tsv table two column first column word second column po know wordnet contain information contains needed sure file use doe anybody know convenience use file contains english word po thanks
Integrating a new language with dialog flow agent,"<p>I see multiple supported language in the dialog flow but I want to ask if I can integrate a new language as Arabic, 
Is there the ability to add a new language and how ?
If I want to implement the new package for the Arabic NLP by myself  How I can start learning, it will be based on what?</p>
",Multilingual Language Processing & Language Identification,integrating new language dialog flow agent see multiple supported language dialog flow want ask integrate new language arabic ability add new language want implement new package arabic nlp start learning based
Why is my program throwing java.io.StreamCorruptedException: invalid type code: 3F?,"<p>I'm trying to tokenize a piece of Chinese text with Stanford NLP but the program throws exceptions all the time.</p>

<p>I tried different ways to load the properties file but they didn't work.</p>

<pre><code>import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import java.io.InputStream;
import java.util.*;

public class Spider {
public static void main(String[] args) {
    try {
            StanfordCoreNLP ppl;
            Properties prop = new Properties();
            InputStream in = Spider.class.getClassLoader().getResourceAsStream(""StanfordCoreNLP-chinese.properties"");
            prop.load(in);
            ppl = new StanfordCoreNLP(prop);
            Annotation doc = new Annotation(""ÊµÆ‰∫ëÁôΩÊó•ÔºåÂ±±Â∑ùÂ∫Ñ‰∏•Ê∏©Êüî„ÄÇ"");
            ppl.annotate(doc);
            ppl.prettyPrint(doc, System.out);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre>

<p>The exceptions are as follows:</p>

<blockquote>
  <p>java.io.StreamCorruptedException: invalid type code: 3F   at
  java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1622)
    at
  java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:1993)
    at
  java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1588)
    at
  java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:430)
    at
  edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2642)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1505)
    at
  edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2939)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:286)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:270)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.(ClassifierCombiner.java:142)
    at
  edu.stanford.nlp.ie.NERClassifierCombiner.(NERClassifierCombiner.java:108)
    at
  edu.stanford.nlp.pipeline.NERCombinerAnnotator.(NERCombinerAnnotator.java:125)
    at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$5(StanfordCoreNLP.java:523)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
    at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)  at
  edu.stanford.nlp.util.Lazy.get(Lazy.java:31)  at
  edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:251)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:192)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:188)
    at Spider.main(Spider.java:13)
  edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Couldn't
  load classifier from
  edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz   at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:70)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$5(StanfordCoreNLP.java:523)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
    at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)  at
  edu.stanford.nlp.util.Lazy.get(Lazy.java:31)  at
  edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:251)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:192)
    at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:188)
    at Spider.main(Spider.java:13) Caused by: java.io.IOException:
  Couldn't load classifier from
  edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz   at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:296)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:270)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.(ClassifierCombiner.java:142)
    at
  edu.stanford.nlp.ie.NERClassifierCombiner.(NERClassifierCombiner.java:108)
    at
  edu.stanford.nlp.pipeline.NERCombinerAnnotator.(NERCombinerAnnotator.java:125)
    at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
    ... 9 more Caused by: java.lang.ClassCastException: class
  java.util.ArrayList cannot be cast to class
  edu.stanford.nlp.classify.LinearClassifier (java.util.ArrayList is in
  module java.base of loader 'bootstrap';
  edu.stanford.nlp.classify.LinearClassifier is in unnamed module of
  loader 'app')     at
  edu.stanford.nlp.ie.ner.CMMClassifier.loadClassifier(CMMClassifier.java:1095)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1505)
    at
  edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1495)
    at
  edu.stanford.nlp.ie.ner.CMMClassifier.getClassifier(CMMClassifier.java:1141)
    at
  edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:292)
    ... 14 more</p>
</blockquote>
",Multilingual Language Processing & Language Identification,program throwing java io streamcorruptedexception invalid type code f trying tokenize piece chinese text stanford nlp program throw exception time tried different way load property file work exception follows java io streamcorruptedexception invalid type code f java base java io objectinputstream readobject objectinputstream java java base java io objectinputstream readarray objectinputstream java java base java io objectinputstream readobject objectinputstream java java base java io objectinputstream readobject objectinputstream java edu stanford nlp ie crf crfclassifier loadclassifier crfclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie crf crfclassifier getclassifier crfclassifier java edu stanford nlp ie classifiercombiner loadclassifierfrompath classifiercombiner java edu stanford nlp ie classifiercombiner loadclassifiers classifiercombiner java edu stanford nlp ie classifiercombiner classifiercombiner java edu stanford nlp ie nerclassifiercombiner nerclassifiercombiner java edu stanford nlp pipeline nercombinerannotator nercombinerannotator java edu stanford nlp pipeline annotatorimplementations ner annotatorimplementations java edu stanford nlp pipeline stanfordcorenlp lambda getnamedannotators stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp lambda null stanfordcorenlp java edu stanford nlp util lazy compute lazy java edu stanford nlp util lazy get lazy java edu stanford nlp pipeline annotatorpool get annotatorpool java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java spider main spider java edu stanford nlp io runtimeioexception java io ioexception load classifier edu stanford nlp model ner chinese misc distsim crf ser gz edu stanford nlp pipeline annotatorimplementations ner annotatorimplementations java edu stanford nlp pipeline stanfordcorenlp lambda getnamedannotators stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp lambda null stanfordcorenlp java edu stanford nlp util lazy compute lazy java edu stanford nlp util lazy get lazy java edu stanford nlp pipeline annotatorpool get annotatorpool java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java spider main spider java caused java io ioexception load classifier edu stanford nlp model ner chinese misc distsim crf ser gz edu stanford nlp ie classifiercombiner loadclassifierfrompath classifiercombiner java edu stanford nlp ie classifiercombiner loadclassifiers classifiercombiner java edu stanford nlp ie classifiercombiner classifiercombiner java edu stanford nlp ie nerclassifiercombiner nerclassifiercombiner java edu stanford nlp pipeline nercombinerannotator nercombinerannotator java edu stanford nlp pipeline annotatorimplementations ner annotatorimplementations java caused java lang classcastexception class java util arraylist cast class edu stanford nlp classify linearclassifier java util arraylist module java base loader bootstrap edu stanford nlp classify linearclassifier unnamed module loader app edu stanford nlp ie ner cmmclassifier loadclassifier cmmclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie abstractsequenceclassifier loadclassifier abstractsequenceclassifier java edu stanford nlp ie ner cmmclassifier getclassifier cmmclassifier java edu stanford nlp ie classifiercombiner loadclassifierfrompath classifiercombiner java
Breaking a command into components using Natural Language Processing,"<p>I want to convert a variable assignment command into code. For example:
""create a variable alpha equal to the number 7"" or
""define a new variable alpha and set it to 7"", and either should convert into:</p>

<pre><code>int alpha = 7;
</code></pre>

<p>How I plan on accomplishing this task is by looking for specific components in the string (e.g. variable name and variable value). Once I figure out that the variable name is ""alpha"" and the variable value is ""7"", I can construct the code seen above. But how would I go about finding the variable name and variable value through natural language processing? The user might add extra words or omit some words (or maybe even invert the order).</p>
",Multilingual Language Processing & Language Identification,breaking command component using natural language processing want convert variable assignment command code example create variable alpha equal number define new variable alpha set either convert plan accomplishing task looking specific component string e g variable name variable value figure variable name alpha variable value construct code seen would go finding variable name variable value natural language processing user might add extra word omit word maybe even invert order
"should I build own NLP engine for rare language or use cloud services for chatbots (azure, GCP) and translations?","<p>So, I would like to build a <code>chatbot</code> for a language not widely supported (i.e. <code>goole</code>/<code>azure</code> don't have support for building chatbots, but only translation service). Translation works well from that language to English (and vice versa).</p>

<p>So, is it easier:</p>

<ol>
<li>To build a new NLP engine for that specific language to recognize context of user question when using the chatbot?</li>
<li>To translate user questions to English and then internally use chatbot engine which supports English (of course, the chatbot needs to be
    programmed), i.e. azure/GCP engines. When context is recognized, and
    when used provide answer (it can again be translated to destination 
    language toward user) ?</li>
</ol>

<p>Method 2 seems easier (cloud api/services already available). Not sure how it works in practice when recognizing context is this chained?</p>
",Multilingual Language Processing & Language Identification,build nlp engine rare language use cloud service chatbots azure gcp translation would like build language widely supported e support building chatbots translation service translation work well language english vice versa easier build new nlp engine specific language recognize context user question using chatbot translate user question english internally use chatbot engine support english course chatbot need programmed e azure gcp engine context recognized used provide answer translated destination language toward user method seems easier cloud api service already available sure work practice recognizing context chained
Bi-LSTM: How to handle unigram and bigrams for a NLP classification?,"<p>I have a chinese text and I use a Bi-LSTM to predict if each character of the text belongs to one of these classes: </p>

<p>B (if the charatcer is at the beginning of the word), 
I (if it is inside the word), 
E (if it is at the end of the word)
S (if it is a single character).</p>

<p>For doing this, I have taken each character of the text and I have built a dictionary, thanks to this I was able to transform the sequence of characters into a sequences of numbers that I give to my network (after the phase of padding), for example:</p>

<p>dictionary={t:1, h:2, e:3, p:4, n:5}
The pen -> 123 435 -> network -> BIE BIE</p>

<p>Everything is ok if I'm working with unigram. However, my network should read also bigrams. How should I handle bigrams? I don't have specific labels for bigrams. (Maybe, my network for each bigram should gives two labels? It doesn't make sense to me)</p>
",Multilingual Language Processing & Language Identification,bi lstm handle unigram bigram nlp classification chinese text use bi lstm predict character text belongs one class b charatcer beginning word inside word e end word single character taken character text built dictionary thanks wa able transform sequence character sequence number give network phase padding example dictionary h e p n pen network bie bie everything ok working unigram however network read also bigram handle bigram specific label bigram maybe network bigram give two label make sense
Is it possible to adapt and existing NLP tool in english to Swedish? and what&#180;s the best approach?,"<p>Whats the best approach of using existing NLP tools in english with another language ex.spanish ?</p>
",Multilingual Language Processing & Language Identification,possible adapt existing nlp tool english swedish best approach whats best approach using existing nlp tool english another language ex spanish
Making sentences from words,"<p>Is there a way to get back sentences from broken up words? For example I have the text below:</p>

<pre><code>THIS IS
A TITLE
hello my 
name
is Sam
Some of the text is formatted.
But others
are
not
</code></pre>

<p>What I am doing right now is providing some structure to it.</p>

<ul>
<li>If I see upper case characters, I assume that it is a title and then I get all the upper case letters in sequence and form a title. </li>
<li>If I see an uppercase character and and full stop some characters later, then that would be one full sentence.</li>
</ul>

<p>The result I would like from above is this:</p>

<pre><code>THIS IS A TITLE
----
hello my name is Sam
----
Some of the text is formatted.
----
But others are not
----
</code></pre>

<p>Where above '----' is a sentence. (Formatting)</p>

<p>However, there seems to be a lot of corner cases I would have to account for.
Is there Natural Language processing I could do to make this easier?</p>
",Multilingual Language Processing & Language Identification,making sentence word way get back sentence broken word example text right providing structure see upper case character assume title get upper case letter sequence form title see uppercase character full stop character later would one full sentence result would like sentence formatting however seems lot corner case would account natural language processing could make easier
How to handle names/unknown words in neural machine translation?,"<p>Can anyone explain a best method to handle unknown words in <strong>Neural machine translation</strong> instead of removing it and to know how google translate is handling names while the sentence is getting translate between any two languages ?</p>

<p>I'd really appreciate your response...Thanks!</p>
",Multilingual Language Processing & Language Identification,handle name unknown word neural machine translation anyone explain best method handle unknown word neural machine translation instead removing know google translate handling name sentence getting translate two language really appreciate response thanks
Is there a good natural language processing library,"<p>I need to implement some NLP in my current module. I am looking for some good library that can help me here. I came across 'LingPipe' but could not completely follow on how to use it.<br>
Basically, we need to implement a feature where the application can decipher customer instructions (delivery instructions) typed in plain english. Eg:</p>

<ul>
<li>Will pick up at 12:00 noon tomorrow</li>
<li>Request delivery after 10th June</li>
<li>Please do not send before Wednesday</li>
<li>Add 10 more units of XYZ to the order</li>
</ul>
",Multilingual Language Processing & Language Identification,good natural language processing library need implement nlp current module looking good library help came across lingpipe could completely follow use basically need implement feature application decipher customer instruction delivery instruction typed plain english eg pick noon tomorrow request delivery th june please send wednesday add unit xyz order
English word segmentation in NLP?,"<p>I am new in the NLP domain, but my current research needs some text parsing (or called keyword extraction) from URL addresses, e.g. a fake URL,</p>
<pre><code>http://ads.goole.com/appid/heads
</code></pre>
<p>Two constraints are put on my parsing,</p>
<ol>
<li><p>The first &quot;ads&quot; and last &quot;heads&quot; should be distinct because &quot;ads&quot; in the &quot;heads&quot; means more suffix rather than an advertisement.</p>
</li>
<li><p>The &quot;appid&quot; can be parsed into two parts; that is 'app' and 'id', both taking semantic meanings on the Internet.</p>
</li>
</ol>
<p>I have tried the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">Stanford NLP</a> toolkit and Google search engine. The former tries to classify each word in a grammar meaning which is under my expectation. The Google engine shows more smartness about &quot;appid&quot; which gives me suggestions about &quot;app id&quot;.</p>
<p>I can not look over the reference of search history in Google search so that it gives me &quot;app id&quot; because there are many people have searched these words. Can I get some offline line methods to perform similar parsing??</p>
<hr />
<p><strong>UPDATE:</strong></p>
<p>Please skip the regex suggestions because there is a potentially unknown number of compositions of words like &quot;appid&quot; in even simple URLs.</p>
<p>Thanks,</p>
<p>Jamin</p>
",Multilingual Language Processing & Language Identification,english word segmentation nlp new nlp domain current research need text parsing called keyword extraction url address e g fake url two constraint put parsing first ad last head distinct ad head mean suffix rather advertisement appid parsed two part app id taking semantic meaning internet tried stanford nlp toolkit google search engine former try classify word grammar meaning expectation google engine show smartness appid give suggestion app id look reference search history google search give app id many people searched word get offline line method perform similar parsing update please skip regex suggestion potentially unknown number composition word like appid even simple url thanks jamin
Word vectorization in natural language processing,"<p>I have a data set. This data set consists of only words. I have to do the vectoring of these words. I've searched for word vectoring algorithms. Bag of words, word2wec, tf-idf Bag of words, word2wec, tf-idf are doing the process of vectoring the words in a sentences. But I don't have sentences. i have just words. So how do I make the process of vectoring the words?</p>
",Multilingual Language Processing & Language Identification,word vectorization natural language processing data set data set consists word vectoring word searched word vectoring algorithm bag word word wec tf idf bag word word wec tf idf process vectoring word sentence sentence word make process vectoring word
How does the Gensim Fasttext pre-trained model get vectors for out-of-vocabulary words?,"<p>I am using gensim to load pre-trained fasttext model. I downloaded the English wikipedia trained model from fasttext <a href=""https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md"" rel=""noreferrer"">website</a>. </p>

<p>here is the code I wrote to load the pre-trained model: </p>

<pre><code>from gensim.models import FastText as ft
model=ft.load_fasttext_format(""wiki.en.bin"")
</code></pre>

<p>I try to check if the following phrase exists in the vocal(which rare chance it would as these are pre-trained model). </p>

<pre><code>print(""internal executive"" in model.wv.vocab)
print(""internal executive"" in model.wv)

False
True
</code></pre>

<p>So the phrase ""internal executive"" is not present in the vocabulary but we still have the word vector corresponding to that. </p>

<pre><code>model.wv[""internal executive""]
Out[46]:
array([ 0.0210917 , -0.15233646, -0.1173932 , -0.06210957, -0.07288644,
       -0.06304111,  0.07833624, -0.17026938, -0.21922196,  0.01146349,
       -0.13639058,  0.17283678, -0.09251394, -0.17875175,  0.01339212,
       -0.26683623,  0.05487974, -0.11843193, -0.01982722,  0.37037706,
       -0.24370994,  0.14269598, -0.16363597,  0.00328478, -0.16560239,
       -0.1450972 , -0.24787527, -0.01318423,  0.03277111,  0.16175713,
       -0.19367714,  0.16955379,  0.1972683 ,  0.09044111,  0.01731548,
       -0.0034324 , -0.04834719,  0.14321515,  0.01422525, -0.08803893,
       -0.29411593, -0.1033244 ,  0.06278021,  0.16452256,  0.0650492 ,
        0.1506474 , -0.14194389,  0.10778475,  0.16008648, -0.07853138,
        0.2183501 , -0.25451994, -0.0345991 , -0.28843886,  0.19964759,
       -0.10923116,  0.26665714, -0.02544454,  0.30637854,  0.04568949,
       -0.04798719, -0.05769338,  0.25762403, -0.05158515, -0.04426906,
       -0.19901046,  0.00894193, -0.17269588, -0.24747233, -0.19061406,
        0.14322804, -0.10804397,  0.4002605 ,  0.01409482, -0.04675362,
        0.10039093,  0.07260711, -0.0938239 , -0.20434211,  0.05741301,
        0.07592541, -0.02921724,  0.21137556, -0.23188967, -0.23164661,
       -0.4569614 ,  0.07434579,  0.10841205, -0.06514647,  0.01220404,
        0.02679767,  0.11840229,  0.2247431 , -0.1946325 , -0.0990666 ,
       -0.02524677,  0.0801085 ,  0.02437297,  0.00674876,  0.02088535,
        0.21464555, -0.16240154,  0.20670174, -0.21640894,  0.03900698,
        0.21772243,  0.01954809,  0.04541844,  0.18990673,  0.11806394,
       -0.21336791, -0.10871669, -0.02197789, -0.13249406, -0.20440844,
        0.1967368 ,  0.09804545,  0.1440366 , -0.08401451, -0.03715726,
        0.27826542, -0.25195453, -0.16737154,  0.3561183 , -0.15756823,
        0.06724873, -0.295487  ,  0.28395334, -0.04908851,  0.09448399,
        0.10877471, -0.05020981, -0.24595442, -0.02822314,  0.17862654,
        0.06452435, -0.15105674, -0.31911567,  0.08166212,  0.2634299 ,
        0.17043628,  0.10063848,  0.0687021 , -0.12210461,  0.10803893,
        0.13644943,  0.10755012, -0.09816817,  0.11873955, -0.03881042,
        0.18548298, -0.04769253, -0.01511982, -0.08552645, -0.05218676,
        0.05387992,  0.0497043 ,  0.06922272, -0.0089245 ,  0.24790663,
        0.27209425, -0.04925154, -0.08621719,  0.15918174,  0.25831223,
        0.01654229, -0.03617229, -0.13490392,  0.08033483,  0.34922174,
       -0.01744722, -0.16894792, -0.10506647,  0.21708378, -0.22582002,
        0.15625793, -0.10860757, -0.06058934, -0.25798836, -0.20142137,
       -0.06613475, -0.08779443, -0.10732629,  0.05967236, -0.02455976,
        0.2229451 , -0.19476262, -0.2720119 ,  0.03687386, -0.01220259,
        0.07704347, -0.1674307 ,  0.2400516 ,  0.07338555, -0.2000631 ,
        0.13897157, -0.04637206, -0.00874449, -0.32827383, -0.03435039,
        0.41587186,  0.04643605,  0.03352945, -0.13700874,  0.16430037,
       -0.13630766, -0.18546128, -0.04692861,  0.37308362, -0.30846512,
        0.5535561 , -0.11573419,  0.2332801 , -0.07236694, -0.01018955,
        0.05936847,  0.25877884, -0.2959846 , -0.13610311,  0.10905041,
       -0.18220575,  0.06902339, -0.10624941,  0.33002165, -0.12087796,
        0.06742091,  0.20762768, -0.34141317,  0.0884434 ,  0.11247049,
        0.14748637,  0.13261876, -0.07357208, -0.11968047, -0.22124515,
        0.12290633,  0.16602683,  0.01055585,  0.04445777, -0.11142147,
        0.00004863,  0.22543314, -0.14342701, -0.23209116, -0.00003538,
        0.19272381, -0.13767233,  0.04850799, -0.281997  ,  0.10343244,
        0.16510887,  0.08671653, -0.24125539,  0.01201926,  0.0995285 ,
        0.09807415, -0.06764816, -0.0206733 ,  0.04697794,  0.02000999,
        0.05817033,  0.10478792,  0.0974884 , -0.01756372, -0.2466861 ,
        0.02877498,  0.02499748, -0.00370895, -0.04728201,  0.00107118,
       -0.21848503,  0.2033032 , -0.00076264,  0.03828803, -0.2929495 ,
       -0.18218371,  0.00628893,  0.20586628,  0.2410889 ,  0.02364616,
       -0.05220835, -0.07040054, -0.03744286, -0.06718048,  0.19264086,
       -0.06490505,  0.27364203,  0.05527219, -0.27494466,  0.22256687,
        0.10330909, -0.3076979 ,  0.04852265,  0.07411488,  0.23980476,
        0.1590279 , -0.26712465,  0.07580928,  0.05644221, -0.18824042],
</code></pre>

<p>Now my confusion is that Fastext creates vectors for character ngrams of a word too. So for a word ""internal"" it will create vectors for all its character ngrams including the full word and then the final word vector for the word is the sum of its character ngrams. </p>

<p>However, how it is still able to give me vector of a word or even the whole sentence? Isn't fastext vector is for a word and its ngram? So what are these vector I am seeing for the phrase when its clearly two words?</p>
",Multilingual Language Processing & Language Identification,doe gensim fasttext pre trained model get vector vocabulary word using gensim load pre trained fasttext model downloaded english wikipedia trained model fasttext website code wrote load pre trained model try check following phrase exists vocal rare chance would pre trained model phrase internal executive present vocabulary still word vector corresponding confusion fastext creates vector character ngrams word word internal create vector character ngrams including full word final word vector word sum character ngrams however still able give vector word even whole sentence fastext vector word ngram vector seeing phrase clearly two word
Spelling text in italian language using textblob,"<p>I want to do spelling for text in Italian language using textblob, but I find just the code for English language. how can do it?
this is the code for English
    from textblob import TextBlob
    text = ""I am gonig to schol""
    text = TextBlob(text)
    print(text.correct())
    I am going to school</p>
",Multilingual Language Processing & Language Identification,spelling text italian language using textblob want spelling text italian language using textblob find code english language code english textblob import textblob text gonig schol text textblob text print text correct going school
how to replace question mark to word,"<p>I have Arabic tweet and I want to replace question marks and exclamation into Arabic word synonymous I tried this code i used regular expression but nothing happens. I used jupyter notebook</p>

<pre><code>def replace_questionmark(tweet):
text = re.sub(""!"", ""ÿ™ÿπÿ¨ÿ®"",tweet)
text = re.sub('ÿßÿ≥ÿ™ŸÅŸáÿßŸÖ','ÿü' ,tweet)
return tweet

data_df['clean text'] = data_df['Text'].apply(lambda x: replace_questionmark(x))
</code></pre>
",Multilingual Language Processing & Language Identification,replace question mark word arabic tweet want replace question mark exclamation arabic word synonymous tried code used regular expression nothing happens used jupyter notebook
Errbot Natural Language Processing,"<p>I would like to add some basic Natural Language Processing or Natural Language understanding into a bot I have implemented with the errbot library. This is to add in basic conversation to the bot. So that the operator can have some basic chat with the chatbot. Perhaps leveraging NTLK. </p>

<p>Is this something anyone has done already or has any god pointers? </p>

<p>Much appreciated.</p>
",Multilingual Language Processing & Language Identification,errbot natural language processing would like add basic natural language processing natural language understanding bot implemented errbot library add basic conversation bot operator basic chat chatbot perhaps leveraging ntlk something anyone ha done already ha god pointer much appreciated
How to get context out of a (Single) sentence?,"<p>I am fairly new to NLP,</p>

<p>I want to implement a python based clustering algorithm, it will be having : </p>

<ul>
<li><p>Context/Topic Extraction - From the Title Statement (Will probably contain not more than 6-7 words)</p></li>
<li><p>Clustering Algorithm</p></li>
</ul>

<p>So the problem is, that I have a bunch of statements(20 statements * 5-6 words per statement = 100-120 words) all related to a Title Statement. And an Algorithm should be able to cluster them.</p>

<p>For the (1) - As an input, first I will have a Title, from that title I want to extract various topics, for ex :</p>

<p><strong><em>TITLE : ""Problem in Manufacturing Assembly Line""</em></strong> - From this I want to extract something like</p>

<p><em>1. Mechanical Problems</em></p>

<p><em>2. Electrical Problems</em></p>

<p><em>3. Linemen Management</em></p>

<p><em>4. Supply Chain Management Problems</em>......</p>

<p>And use these extracted topics to cluster those statements. I can perform the second task of clustering, but how do I extract topics from a single statement that contains not more than 6-7 words?</p>

<p>Language : <strong><em>English</em></strong></p>

<p>Any idea how to go about the first problem??</p>
",Multilingual Language Processing & Language Identification,get context single sentence fairly new nlp want implement python based clustering algorithm context topic extraction title statement probably contain word clustering algorithm problem bunch statement statement word per statement word related title statement algorithm able cluster input first title title want extract various topic ex title problem manufacturing assembly line want extract something like mechanical problem electrical problem linemen management supply chain management problem use extracted topic cluster statement perform second task clustering extract topic single statement contains word language english idea go first problem
"Efficient Context-Free Grammar parser, preferably Python-friendly","<p>I am in need of parsing a small subset of English for one of my project, described as a context-free grammar with (1-level) feature structures (<a href=""http://code.google.com/p/nltk/source/browse/trunk/nltk/examples/grammars/book_grammars/feat0.fcfg?r=8260"">example</a>) and I need to do it efficiently .</p>

<p>Right now I'm using <a href=""http://www.nltk.org/"">NLTK</a>'s parser which produces the right output but is very slow. For my grammar of ~450 fairly ambiguous non-lexicon rules and half a million lexical entries, parsing simple sentences can take anywhere from 2 to 30 seconds, depending it seems on the number of resulting trees. Lexical entries have little to no effect on performance.</p>

<p>Another problem is that loading the (25MB) grammar+lexicon at the beginning can take up to a minute.</p>

<p>From what I can find in literature, the running time of the algorithm used to parse such a grammar (Earley or CKY) should be linear to the size of the grammar and cubic to the size of the input token list. My experience with NLTK indicates that ambiguity is what hurts the performance most, not the absolute size of the grammar.</p>

<p>So now I'm looking for a CFG parser to replace NLTK. I've been considering <a href=""http://www.dabeaz.com/ply/"">PLY</a> but I can't tell whether it supports feature structures in CFGs, which are required in my case, and the examples I've seen seem to be doing a lot of procedural parsing rather than just specifying a grammar. Can anybody show me an example of PLY both supporting feature structs and using a declarative grammar?</p>

<p>I'm also fine with any other parser that can do what I need efficiently. A Python interface is preferable but not absolutely necessary.</p>
",Multilingual Language Processing & Language Identification,efficient context free grammar parser preferably python friendly need parsing small subset english one project described context free grammar level feature structure another problem loading mb grammar lexicon beginning take minute find literature running time algorithm used parse grammar earley cky linear size grammar cubic size input token list experience nltk indicates ambiguity hurt performance absolute size grammar looking cfg parser replace nltk considering href tell whether support feature structure cfgs required case example seen seem lot procedural parsing rather specifying grammar anybody show example ply supporting feature structs using declarative grammar p also fine parser need efficiently python interface preferable absolutely necessary
Any publicly available word dictionary in a text file?,"<p>I am looking for a word dictionary for different languages(english, spanish, ...). However, almost all dictionaries that I could find are either provided by a program or on a website. </p>

<p>I want to get this word dictionary as a text file. (Also, this file should be publicly available.) Are there any public available word dictionaries in text files?</p>
",Multilingual Language Processing & Language Identification,publicly available word dictionary text file looking word dictionary different language english spanish however almost dictionary could find either provided program website want get word dictionary text file also file publicly available public available word dictionary text file
command line parameter in word2vec,"<p>I want to use word2vec to create my own word vector corpus with the current version of the english wikipedia, but I can't find an explanation of the command line parameter for using that program. In the demp-script you can find following:<br>
(text8 is an old wikipedia corpus of 2006)</p>

<pre><code>make
if [ ! -e text8 ]; then
wget http://mattmahoney.net/dc/text8.zip -O text8.gz
gzip -d text8.gz -f
fi
time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15
./distance vectors.bin
</code></pre>

<p>What is the meaning of the command line parameter:<br>
<code>vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15</code></p>

<p>And what are the most suitable values when I have a wikipedia text corpus of around 20GB(.txt file)? I read that for bigger corpora a vector size of 300 or 500 would be better.</p>
",Multilingual Language Processing & Language Identification,command line parameter word vec want use word vec create word vector corpus current version english wikipedia find explanation command line parameter using program demp script find following text old wikipedia corpus meaning command line parameter suitable value wikipedia text corpus around gb txt file read bigger corpus vector size would better
Unigram vs Bigram vs Posgram in Natural Language Processing,"<p>I want to know what is the meaning and difference between unigram, bigram and posgram. I have searched the Internet but I could not find a comprehensive answer. Any help would be very much appreciated.</p>
",Multilingual Language Processing & Language Identification,unigram v bigram v posgram natural language processing want know meaning difference unigram bigram posgram searched internet could find comprehensive answer help would much appreciated
About parsing parentheses in English model,"<p>This sentence is part of the Simplified Wikipedia:</p>

<p>There are three things in air, Nitrogen (79%), oxygen (20%), and other types of gases (1%).</p>

<p>The parenthetical percentages are not handled well in spaCy 2.0 and 2.1. What is the best way to handle this class of problem? </p>

<p>Here is the visualization:
<a href=""https://i.sstatic.net/vP6Yr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vP6Yr.png"" alt=""visualization of parse of above sample sentence""></a></p>
",Multilingual Language Processing & Language Identification,parsing parenthesis english model sentence part simplified wikipedia three thing air nitrogen oxygen type gas parenthetical percentage handled well spacy best way handle class problem visualization
Detection of Economic event on a french corpus,"<p>actually working in a project witch is the detection of economic event from french corpus .
I have more than 10000 corpus that I collected from a newspaper each Corpus talks about a company. for each company more than 10 corpus talks about it .
I need to find a way to extract these information so extract all economic event from a corpus .
I'm using Python 3.7 and mongodb to store my data .
I can't find any tuto talks about this . If you have ideas 
Thank you every one for help </p>
",Multilingual Language Processing & Language Identification,detection economic event french corpus actually working project witch detection economic event french corpus corpus collected newspaper corpus talk company company corpus talk need find way extract information extract economic event corpus using python mongodb store data find tuto talk idea thank every one help
How to perform language translation of a column (excel file) to english using Textblob?,"<p>My code produced the following error:</p>

<blockquote>
  <p>AttributeError: 'function' object has no attribute 'translate'</p>
</blockquote>

<p>More detail:</p>

<p><a href=""https://i.sstatic.net/uKYoJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uKYoJ.png"" alt=""error""></a></p>

<p>What is wrong with my code?</p>

<pre><code>import pandas as pd
import numpy as np
from textblob import TextBlob

df_file2= df_file['Repair Details']. apply.translate(from_lang='zh-CN',to ='en')
</code></pre>
",Multilingual Language Processing & Language Identification,perform language translation column excel file english using textblob code produced following error attributeerror function object ha attribute translate detail wrong code
How to obtain enhanced dependency parsing from Stanford NLP tools?,"<p>I'm working on a project about dependency parsing for Polish. We‚Äôre trying to train the Stanford Neural Network Dependency Parser on data from Polish language (using Universal Dependencies treebanks in .conllu format). The data is already tokenized and annotated, so we‚Äôve trained neither the tokenizer, nor the parser provided by CORE NLP. So far we‚Äôve been able to achieve some success with pl_lfg-ud Treebank in standard dependencies, by running the parser from the command line. But we would also like to train the parser to reproduce the enhanced Universal Dependencies, which are represented in the treebank as well. So far I have not been able to find a way to do so in the
documentation, and FAQ for both NNDEP and CORE NLP, even though, as far as I understand, it is possible with Stanford NLP parser. Is it the case that the enhanced dependencies parsing works only for English (or other officially supported langauges), or am I simply doing something wrong?</p>

<p>I'll be very grateful for any clues!</p>
",Multilingual Language Processing & Language Identification,obtain enhanced dependency parsing stanford nlp tool working project dependency parsing polish trying train stanford neural network dependency parser data polish language using universal dependency treebanks conllu format data already tokenized annotated trained neither tokenizer parser provided core nlp far able achieve success pl lfg ud treebank standard dependency running parser command line would also like train parser reproduce enhanced universal dependency represented treebank well far able find way documentation faq nndep core nlp even though far understand possible stanford nlp parser case enhanced dependency parsing work english officially supported langauges simply something wrong grateful clue
list of English words to refer to humans,"<p>I am trying to automatically process English sentences and detect the words which might be referring to humans. e.g. he, everybody, someone, niece, I, son, ...
I am already using NER, and have implemented some simple heuristic rules as well.
But I think, other than tricky cases which is fine if I mis-label them, the problem can be solved with a simple dictionary look-up. Is there any list of English words that I can use?</p>
",Multilingual Language Processing & Language Identification,list english word refer human trying automatically process english sentence detect word might referring human e g everybody someone niece son already using ner implemented simple heuristic rule well think tricky case fine mi label problem solved simple dictionary look list english word use
Deep Learning for Arabic Natural Language Processing project,"<p>I'm about to propose a graduation project in Deep Learning for Arabic Natural Language Processing field. However, since I'm a beginner student <strong>in this field</strong>, I have already started learning a helpful <a href=""https://www.coursera.org/specializations/deep-learning"" rel=""nofollow noreferrer"">series of courses</a>, and I have almost a full year to work on this individual project. So, what are some great ideas for an Android application in DL for Arabic NLP?</p>

<p>P.S. I will gather the needed dataset if it's not available.</p>

<p><strong>Edit:</strong> Recently, I read about <a href=""https://monkeylearn.com/blog/definitive-guide-natural-language-processing/"" rel=""nofollow noreferrer"">various problems</a> in this field, but most of them are very classic choices, and it seems to me that they don't give a good impression of my efforts while there exist many well-known and very efficient similar applications such as Google translate and Siri. However, Since my question is still too broad, kindly write down your suggested edits and recommendations.</p>
",Multilingual Language Processing & Language Identification,deep learning arabic natural language processing project propose graduation project deep learning arabic natural language processing field however since beginner student field already started learning helpful series course almost full year work individual project great idea android application dl arabic nlp p gather needed dataset available edit recently read various problem field classic choice seems give good impression effort exist many well known efficient similar application google translate siri however since question still broad kindly write suggested edits recommendation
How to implement &quot;do something when meeting &lt;EOS&gt;&quot;,"<p>The token <code>&lt;EOS&gt;</code> is ubiquitously used in NLP. As I haven't used it, the implementation of conditioning on it is a bit unclear to me. Could anyone provide a snippet of Python code. (If statements may be used.)</p>

<p>Example 1: There is a sequence of words with some <code>&lt;EOS&gt;</code> tokens interpolated. This sequence goes through a RNN to get encoded. Whenever encounters <code>&lt;EOS&gt;</code>, the timestep outputs its state. </p>

<p>Example 2: a machine translation task. When meets <code>&lt;EOS&gt;</code>, it stops generating tokens.</p>
",Multilingual Language Processing & Language Identification,implement something meeting eos token ubiquitously used nlp used implementation conditioning bit unclear could anyone provide snippet python code statement may used example sequence word token interpolated sequence go rnn get encoded whenever encounter timestep output state example machine translation task meet stop generating token
Classifying Text Based on Groups of Keywords?,"<p>I have a list of requirements for a software project, assembled from the remains of its predecessor. Each requirement should map to one or more categories. Each of the categories consists of a group of keywords. What I'm trying to do is find an algorithm that would give me a score ranking which of the categories each requirement is likely to fall into. The results would be use as a starting point to further categorize the requirements.</p>

<p>As an example, suppose I have the requirement:</p>

<blockquote>
  <p>The system shall apply deposits to a customer's specified account.</p>
</blockquote>

<p>And categories/keywords:</p>

<ol>
<li>Customer Transactions: deposits, deposit, customer, account, accounts</li>
<li>Balance Accounts: account, accounts, debits, credits</li>
<li>Other Category: foo, bar</li>
</ol>

<p>I would want the algorithm to score the requirement highest in category 1, lower in category 2, and not at all in category 3. The scoring mechanism is mostly irrelevant to me, but needs to convey how much more likely category 1 applies than category 2.</p>

<p>I'm new to NLP, so I'm kind of at a loss. I've been reading <em>Natural Language Processing in Python</em> and was hoping to apply some of the concepts, but haven't seen anything that quite fits. I don't think a simple frequency distribution would work, since the text I'm processing is so small (a single sentence.)</p>
",Multilingual Language Processing & Language Identification,classifying text based group keywords list requirement software project assembled remains predecessor requirement map one category category consists group keywords trying find algorithm would give score ranking category requirement likely fall result would use starting point categorize requirement example suppose requirement system shall apply deposit customer specified account category keywords customer transaction deposit deposit customer account account balance account account account debit credit category foo bar would want algorithm score requirement highest category lower category category scoring mechanism mostly irrelevant need convey much likely category applies category new nlp kind loss reading natural language processing python wa hoping apply concept seen anything quite fit think simple frequency distribution would work since text processing small single sentence
Split lines separated by tab (or spaces) in a file,"<p>I have a big file (europarl corpus) containing on each line a sentence in English and in German</p>

<pre><code>Es gab Tote.    They killed people.
Ich stimme Herrn Mayer in allem zu.     I agree with everything Mr Mayer has said.
</code></pre>

<p>Above are two example lines. <strong>I just need, for each line, to separate each language sentences</strong>. </p>

<p>As you can see, sentences are separated by a little space (a tab I think ?) consisting of multiple spaces, the problem is that the number of spaces is inconsistent through the file. </p>

<p>Also, sometimes the english version is totally missing</p>
",Multilingual Language Processing & Language Identification,split line separated tab space file big file europarl corpus containing line sentence english german two example line need line separate language sentence see sentence separated little space tab think consisting multiple space problem number space inconsistent file also sometimes english version totally missing
NLP(natural language processing) How to detect question with any method?,"<p>I search a machine learning method detecting some question. </p>

<p>Example, </p>

<pre><code> User: Please tell me your name ?     
 AI  : (AI find User want to know his name)   
       My name is [AI's name]. 
</code></pre>

<p>My dataset is as follows.    </p>

<pre><code>[label], [question]    
   1   , What's your name?    
   1   , Tell me your name.
   ...
</code></pre>

<p>But the problem is to include something that is not a question in the input.    </p>

<p>Example,</p>

<pre><code>User: Hello, my name is [User name]
AI  : (this is not a question)    
      (throw another process)
      (-&gt;) Nice to meet you.
</code></pre>

<p>The number of Question's categories is 10~20, but the number of sentences which is not a question is too many. </p>

<p>Do you know how to solve this question Or any task related to this?</p>
",Multilingual Language Processing & Language Identification,nlp natural language processing detect question method search machine learning method detecting question example dataset follows problem include something question input example number question category number sentence question many know solve question task related
German verbs lemmatization with Tiger corpus,"<p>recently I'm traing to build service for lemmatization of german words.</p>

<p>I found very good article <a href=""https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/"" rel=""nofollow noreferrer"">here</a> </p>

<p>After I've done all the steps described in the article my service works quite good, but while testing I noticed that some of verbs cannot be converted to infinitive form.</p>

<p>E.g. kochst -> kochen. 
The root cause is that my POS tagger gives me ADV for 'kochst' while should VVFIN or at least V... since this is a verb.</p>

<p>I also found that original TIGER corpus file doesn't contain 'kochst' form but only 'kocht'.</p>

<p>I am not familiar with conll format, but have added one additional row which is shown below</p>

<pre><code>50475_11    kochst  kochen  _   VVFIN   _   number=sg|person=2|tense=pres|mood=ind  _   0   _   --  _   _   _   _
</code></pre>

<p>and retrained the tagger without any success, see listing below</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; corp = nltk.corpus.ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',
...                                      ['ignore', 'words', 'ignore', 'ignore', 'pos'],
...                                      encoding='utf-8')
&gt;&gt;&gt; 
&gt;&gt;&gt; tagged_sents = corp.tagged_sents()
&gt;&gt;&gt; 
&gt;&gt;&gt; from ClassifierBasedGermanTagger.ClassifierBasedGermanTagger import ClassifierBasedGermanTagger
&gt;&gt;&gt; tagger = ClassifierBasedGermanTagger(train=tagged_sents)
&gt;&gt;&gt; tagger.tag(['kochst'])
[('kochst', u'ADV')]
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; tagged_sents[-1]
[(u'kochst', u'VVFIN')]
</code></pre>

<p>So it might be I added 'kochst' record incorrectly or TIGER corpus is not complete (I found that many verbs in second person form are not there) or I simply don't understand something here, how to train POS tagger to return verb for conjugated verbs.</p>

<p>'kochst' just an example,  I guess there a lot other verbs cannot be recognized</p>

<pre><code>&gt;&gt;&gt; tagger.tag(['fahre'])
[('fahre', u'XY')]
&gt;&gt;&gt; tagger.tag(['musst'])
[('musst', u'PPER')]
</code></pre>
",Multilingual Language Processing & Language Identification,german verb lemmatization tiger corpus recently traing build service lemmatization german word found good article done step described article service work quite good testing noticed verb converted infinitive form e g kochst kochen root cause po tagger give adv kochst vvfin least v since verb also found original tiger corpus file contain kochst form kocht familiar conll format added one additional row shown retrained tagger without success see listing might added kochst record incorrectly tiger corpus complete found many verb second person form simply understand something train po tagger return verb conjugated verb kochst example guess lot verb recognized
"If a small neural network were used as a scoring function for Attention model, what label/value it is trained against?","<p>I am reading up a paper on Attention mechanism of encoder-decoder architecture for machine translation. There were several proposals for the scoring function for decoding step, such as cosine similarity (between an encoder state and a decoder state), simple dot-product, etc... One of them is using a neural network to train to get a score. But what i don't get is what are we going to train it against? By that I mean the output ""Y"" label/value. The equation for the network is given below.</p>

<p>score(s,h)= v . tanh(W[s;h])</p>

<p><a href=""https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"" rel=""nofollow noreferrer"">https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</a></p>
",Multilingual Language Processing & Language Identification,small neural network used scoring function attention model label value trained reading paper attention mechanism encoder decoder architecture machine translation several proposal scoring function decoding step cosine similarity encoder state decoder state simple dot product etc one using neural network train get score get going train mean output label value equation network given score h v tanh w h
"Fasttext representation for short phrase, but not for longer phrase containing the short one","<p>I'm using <code>Gensim</code> for loading the german <code>.bin</code> files from <code>Fasttext</code> in order to get vector representations for out-of-vocabulary words and phrases. So far it works fine and I achieve good results overall.<br>
I am familiar with the <code>KeyError :'all ngrams for word &lt;word&gt; absent from model'.</code> Clearly the model doesn't provide a vector representation for every possible ngram combination.<br>
But now I ran into a confusing (at least for me) issue.<br>
I'll just give a quick example:<br>
the model provides a representation for the phrase <code>AuM Wert</code>.<br>
But when I want to get a representation for <code>AuM Wert 50 Mio. Eur</code>, I'll get the <code>KeyError</code> mentioned above. So the model obviously has a representation for the shorter phrase but not for the extended one.<br>
It even returns a representation for <code>AuM Wert 50 Mio.Eur</code> (I just removed the space between 'Mio' and 'Eur')<br>
I mean, the statement in the Error is simply not true, because the first example shows that it knows some of the ngrams. Can someone explain that to me? What don't I understand here? Is my understanding of ngrams wrong?</p>

<p>Heres the code: </p>

<pre><code>from gensim.models.wrappers import FastText
model = FastText.load_fasttext_format('cc.de.300.bin')
model.wv['AuM Wert'] #returns a vector
model.wv['AuM Wert 50 Mio.EUR'] #returns a vector
model.wv['AuM Wert 50 Mio. EUR'] #triggers the error
</code></pre>

<p>Thanks in advance,<br>
Amos</p>
",Multilingual Language Processing & Language Identification,fasttext representation short phrase longer phrase containing short one using loading german file order get vector representation vocabulary word phrase far work fine achieve good result overall familiar clearly model provide vector representation every possible ngram combination ran confusing least issue give quick example model provides representation phrase want get representation get mentioned model obviously ha representation shorter phrase extended one even return representation removed space mio eur mean statement error simply true first example show know ngrams someone explain understand understanding ngrams wrong code thanks advance amos
Swift NSLinguisticTagger results for languages other than English,"<p>I'm currently checking out Swift's NSLinguisticTagger. For test purposes I used the code from appcoda <a href=""https://www.appcoda.com/natural-language-processing-swift/"" rel=""nofollow noreferrer"">Introduction to Natural Language Processing</a>.</p>

<p>For the English language it works as expected and described in the tutorial. But when I use NSLinguisticTagger on languages other than English the Lemmatization, Parts of Speech and Named Entity Recognition produces no useful results. I can understand this for the Named Entity Recognition but for the first two options I thought at least a basic result should be possible. Did I miss a language specific setting or is NSLinguisticTagger only good for language detection and Tokenization when used for languages other than English?</p>

<p>Here's the code Sai Kambampati uses in his tutorial:</p>

<pre><code>import Foundation

let quote = ""Here's to the crazy ones. The misfits. The rebels. The troublemakers. The round pegs in the square holes. The ones who see things differently. They're not fond of rules. And they have no respect for the status quo. You can quote them, disagree with them, glorify or vilify them. About the only thing you can't do is ignore them. Because they change things. They push the human race forward. And while some may see them as the crazy ones, we see genius. Because the people who are crazy enough to think they can change the world, are the ones who do. - Steve Jobs (Founder of Apple Inc.)""

let tagger = NSLinguisticTagger(tagSchemes:[.tokenType, .language, .lexicalClass, .nameType, .lemma], options: 0)
let options: NSLinguisticTagger.Options = [.omitPunctuation, .omitWhitespace, .joinNames]

func determineLanguage(for text: String) {
  tagger.string = text
  let language = tagger.dominantLanguage
  print(""The language is \(language!)"")
}

determineLanguage(for: quote)

func tokenizeText(for text: String) {
  tagger.string = text
  let range = NSRange(location: 0, length: text.utf16.count)
  tagger.enumerateTags(in: range, unit: .word, scheme: .tokenType, options: options) { tag, tokenRange, stop in
      let word = (text as NSString).substring(with: tokenRange)
      print(word)
  }
}

tokenizeText(for: quote)

func partsOfSpeech(for text: String) {
  tagger.string = text
  let range = NSRange(location: 0, length: text.utf16.count)
  tagger.enumerateTags(in: range, unit: .word, scheme: .lexicalClass, options: options) { tag, tokenRange, _ in
      if let tag = tag {
          let word = (text as NSString).substring(with: tokenRange)
          print(""\(word): \(tag.rawValue)"")
      }
  }
}

partsOfSpeech(for: quote)

func namedEntityRecognition(for text: String) {
  tagger.string = text
  let range = NSRange(location: 0, length: text.utf16.count)
  let tags: [NSLinguisticTag] = [.personalName, .placeName, .organizationName]
  tagger.enumerateTags(in: range, unit: .word, scheme: .nameType, options: options) { tag, tokenRange, stop in
      if let tag = tag, tags.contains(tag) {
          let name = (text as NSString).substring(with: tokenRange)
          print(""\(name): \(tag.rawValue)"")
      }
  }
}

namedEntityRecognition(for: quote)
</code></pre>

<p>For the English sentence the result is exactly as expected.</p>

<p>e.g. for the Parts of Speech Tagging and the Named Entity Recognition:</p>

<p>The: Determiner</p>

<p>troublemakers: Noun</p>

<p>The: Determiner</p>

<p>round: Noun</p>

<p>pegs: Noun</p>

<p>...</p>

<p>Apple Inc.: Noun</p>

<p>Steve Jobs: PersonalName</p>

<p>Apple Inc.: OrganizationName</p>

<p>But for a German sentence </p>

<pre><code>let quote = ""Apple f√ºhrt die Hitliste der Silicon-Valley-Unternehmen an, bei denen sich Ingenieure das Wohnen in der N√§he nicht mehr leisten k√∂nnen. Dahinter folgen das Portal Reddit (San Francisco), der Suchriese Google (Mountain View) und die sozialen Netzwerke Twitter (San Francisco) und Facebook (Menlo Park)""
</code></pre>

<p>only the language detection and the tokenization seems to work correct. For the Parts of Speech Tagging only ""OtherWord"" and for the Named Entity Recognition no result at all is returned:</p>

<p>Apple: OtherWord</p>

<p>f√ºhrt: OtherWord</p>

<p>die: OtherWord</p>

<p>Hitliste: OtherWord</p>

<p>...</p>

<p>Did anyone tried to use the Class in other languages than English or is it only seriously usable when working with English text. I couldn't find any Apple documentation explaining the language capabilities beside from a list of languages that should be supported. Or am I doing something wrong?</p>

<p>Any comment pointing me to a solution is greatly appreciated. </p>

<p>Krid</p>
",Multilingual Language Processing & Language Identification,swift nslinguistictagger result language english currently checking swift nslinguistictagger test purpose used code appcoda introduction natural language processing english language work expected described tutorial use nslinguistictagger language english lemmatization part speech named entity recognition produce useful result understand named entity recognition first two option thought least basic result possible miss language specific setting nslinguistictagger good language detection tokenization used language english code sai kambampati us tutorial english sentence result exactly expected e g part speech tagging named entity recognition determiner troublemaker noun determiner round noun peg noun apple inc noun steve job personalname apple inc organizationname german sentence language detection tokenization seems work correct part speech tagging otherword named entity recognition result returned apple otherword f hrt otherword die otherword hitliste otherword anyone tried use class language english seriously usable working english text find apple documentation explaining language capability beside list language supported something wrong comment pointing solution greatly appreciated krid
How can I remove the non-alphanumeric (English) characters in a series containing strings while retaining spaces?,"<p>Currently, I have:</p>

<pre><code>[re.sub(r'\W', '', i) for i in training_data.loc[:, 'Text']]
</code></pre>

<p>However with this the Hindi characters remain and all the spaces are removed. Any ideas?</p>
",Multilingual Language Processing & Language Identification,remove non alphanumeric english character series containing string retaining space currently however hindi character remain space removed idea
Strategies for recognizing proper nouns in NLP,"<p>I'm interested in learning more about <a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""noreferrer"">Natural Language Processing</a> (NLP) and am curious if there are currently any strategies for recognizing proper nouns in a text that aren't based on dictionary recognition? Also, could anyone explain or link to resources that explain the current dictionary-based methods? Who are the authoritative experts on NLP or what are the definitive resources on the subject?</p>
",Multilingual Language Processing & Language Identification,strategy recognizing proper noun nlp interested learning natural language processing nlp curious currently strategy recognizing proper noun text based dictionary recognition also could anyone explain link resource explain current dictionary based method authoritative expert nlp definitive resource subject
Context Free Grammar for Tamarian Language,"<p>I am trying to figure out the CFG for <a href=""https://memory-alpha.fandom.com/wiki/Tamarian_language"" rel=""nofollow noreferrer"">Tamarian</a> language. I think for English, the starting symbol <code>S</code> usually starts with the production rule <code>S -&gt; NP VP</code>. Which means we can divide a typical sentence into parts of <code>Noun Phrase</code> and <code>Verb Phrase</code> and derivation goes from there.<br>
My question is, what would be a CFG or at least the first production rule for the Tamarian language. Some of the example sentences are below:   </p>

<p>'Sinda his face black his eyes red'<br>
'Darmok and Jalad at Tanagra'<br>
'Picard and Dathan at Eladrel'<br>
'Marab with sails unfurled'   </p>
",Multilingual Language Processing & Language Identification,context free grammar tamarian language trying figure cfg tamarian language think english starting symbol usually start production rule mean divide typical sentence part derivation go question would cfg least first production rule tamarian language example sentence sinda face black eye red darmok jalad tanagra picard dathan eladrel marab sail unfurled
"NLTK his &amp; hers are tagged differently,","<p>I just started experimenting with Natural Language Processing. My first few lines of code with NLTK gave confusing results.</p>

<pre><code>import nltk
exampleArray = ['The book is hers',
               'The book is his']
for item in exampleArray:
    tokenized = nltk.word_tokenize(item)
    tagged = nltk.pos_tag(tokenized)
    print(tagged)
</code></pre>

<p>Output:</p>

<pre><code>[('The', 'DT'), ('book', 'NN'), ('is', 'VBZ'), ('hers', 'NNS')]
[('The', 'DT'), ('book', 'NN'), ('is', 'VBZ'), ('his', 'PRP$')]
</code></pre>

<p>Why are ""his"" and ""hers"" tagged so differently?</p>
",Multilingual Language Processing & Language Identification,nltk tagged differently started experimenting natural language processing first line code nltk gave confusing result output tagged differently
Write own stemmer for stemming,"<p>I have a dataset of 27 files, each containing opcodes. I want to use stemming to map all versions of similar opcodes into the same opcode. For example: push, pusha, pushb, etc would all be mapped to push.</p>

<p>My dictionary contains 27 keys and each key has a list of opcodes as a value. Since the values contain opcodes and not normal english words, I cannot use the regular stemmer module. I need to write my own stemmer code. Also I cannot hard-code a custom dictionary that maps different versions of the opcodes to the root opcode because I have a huge dataset. </p>

<p>I think regex expression would be a good idea but I do not know how to use it. Can anyone help me with this or any other idea to write my own stemmer code?</p>
",Multilingual Language Processing & Language Identification,write stemmer stemming dataset file containing opcodes want use stemming map version similar opcodes opcode example push pusha pushb etc would mapped push dictionary contains key key ha list opcodes value since value contain opcodes normal english word use regular stemmer module need write stemmer code also hard code custom dictionary map different version opcodes root opcode huge dataset think regex expression would good idea know use anyone help idea write stemmer code
How to read a text file word by word and compare these words with existing English dictionary in Python?,"<p>I would like to read each word from a given text file and then want to compare these word with an existing English dictionary which may be a system dictionary or any other way.  Here is the code I have tried, but in the following code, there is a problem. The following codes reading brackets or any other unnecessary characters.</p>

<pre><code>f=open('words.txt')
M=[word for line in f for word in line.split()]
S=list(set(M))

for i in S:
    print i
</code></pre>

<p>How can I do the job?</p>
",Multilingual Language Processing & Language Identification,read text file word word compare word existing english dictionary python would like read word given text file want compare word existing english dictionary may system dictionary way code tried following code problem following code reading bracket unnecessary character job
Creating corpora for a tribal language POS tagging,"<p>I am using NLTK to extract nouns from a text-string and each of its word  already have POS tags in them in (ibaloi) language which be later used on creating grammar:</p>

<pre><code>sentence = ""this is a tribal language""
words = nltk.word_tokenize(sentence)
taggedWords = tagged_text = nltk.pos_tag(nltk.Text(words))
</code></pre>

<p>There is no problem in english. <strong>Is there a way to make it work in tribal(ibaloi) language as well?</strong></p>

<p>(I am new to natural language process taking some tutorials which is great by the way.)</p>
",Multilingual Language Processing & Language Identification,creating corpus tribal language po tagging using nltk extract noun text string word already po tag ibaloi language later used creating grammar problem english way make work tribal ibaloi language well new natural language process taking tutorial great way
Why the classifier gives me the input without any changes,"<p>I'm trying to build a classifier that segment Arabic sentences (transliterated in Buckwalter). I have attributes that read from files and I'm applying +/- 10 context window (i.e. for each word, I check 10 words before and 10 words after). The classification classes are:</p>

<p>B-S for the first word of the sentence, 
E-S for the last word of the sentence, 
I-S represents words in the middle,
and S for sentences that contain only one word. </p>

<pre><code>FastVector fClasse = new FastVector();
fClasse.addElement( ""I-S"" );
fClasse.addElement( ""B-S"" );
fClasse.addElement( ""E-S"" );
fClasse.addElement( ""S"" );

if (str.length == 1) {
    element.setValue( attClasse , ""S"" );
} else {
    if (i == 0) {
        element.setValue( attClasse , ""B-S"" );
    } else {
        if (i == str.length - 1) {
            element.setValue( attClasse , ""E-S"" );
        } else {
            element.setValue( attClasse , ""I-S"" );
        }
    }
}
</code></pre>

<p>First, I used 10 attributes that check the previous classes for each word. So the results were almost 100%, especially for B-S and S classes. But even when I test the model with a non-segmented file. The model result is also the same. Literally, the model gives me the input paragraph without any changes. 
When I removed those attributes (that check the previous 10 classes), the evaluation results hugely decreased (B-S 33%).</p>

<p>Below is my code: </p>

<p><a href=""https://drive.google.com/open?id=1-NiA_C-2YRkG19J-mZYg4BEdNnnVWbyG"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1-NiA_C-2YRkG19J-mZYg4BEdNnnVWbyG</a></p>

<p>it gives me an arff file that I use with weka GUI to generate a model. I'm using PART classifier.</p>

<p>Any ideas what I'm doing wrong? Would greatly appreciate any help.</p>
",Multilingual Language Processing & Language Identification,classifier give input without change trying build classifier segment arabic sentence transliterated buckwalter attribute read file applying context window e word check word word classification class b first word sentence e last word sentence represents word middle sentence contain one word first used attribute check previous class word result almost especially b class even test model non segmented file model result also literally model give input paragraph without change removed attribute check previous class evaluation result hugely decreased b code give arff file use weka gui generate model using part classifier idea wrong would greatly appreciate help
How to Properly Decode Hex Values in RTF,"<p>Unfortunately this one goes down two rabbit holes, text encodings and RTF. But here it is.</p>

<h2>Background</h2>

<p>I am working on an NLP text pipeline where we need to convert RTF to plain text, in other words we need to remove the RTF control characters and leave the text content intact. We are building the pipeline in python and it has several requirements that prevent us from using something like Apache Tikka in production. </p>

<p>I know that RTF can contain hex values such as <code>\'a9</code> if the author of the document typed a non-ascii character. I also know that the first sequence of control characters in the document specifies how to decode these hex values, e.g. <code>\ansicpg1252</code>. For example, in this case the presence of <code>\ansicpg1252</code> at the beginning of the document means that <code>\'a9</code> should be interpreted as unicode code point <code>00A9 (COPYRIGHT SIGN)</code> as per <a href=""https://en.wikipedia.org/wiki/Windows-1252"" rel=""nofollow noreferrer"">the windows-1252 encoding</a>.</p>

<h2>Question</h2>

<p>I came across an RTF document with <code>\ansicpg1252</code> in the first set of control characters, however there are several places in the document where the following hex literals appear, <code>\'81\'aa</code>. This is confusing becuase <code>0x81</code> is undefined in the <code>windows-1252</code> encoding. I thought maybe it could be <code>utf-8</code>, however it isn't defined in <code>utf-8</code> either. </p>

<p>WordPad.exe represents these two bytes with this character: ‚Üë</p>

<p>Apache Tikka uses the same character, ‚Üë</p>

<p>This character corresponds to unicode code point <code>2191 (Upwards Arrow)</code>, and as it turns out our mystery bytes, <code>0x81AA</code>, are the result of encoding this character using the <a href=""https://en.wikipedia.org/wiki/Code_page_932_(Microsoft_Windows)"" rel=""nofollow noreferrer"">Windows Code Page 932 encoding</a>, which contains Japanese characters.</p>

<p>For reference, the full context of those two bytes in the RTF document is </p>

<pre><code>\plain\f1\fs20 \'81\'aa\plain\f0\fs20
</code></pre>

<p>and the document contains this entry in the <code>\fonttbl</code> group:</p>

<pre><code>{\f1\fmodern\fcharset128\fprq1 MS Mincho;}
</code></pre>

<p>which, as far as I understand, means that any text following <code>\f1</code> should be rendered using the <code>MS Mincho</code> font, which kind of makes sense since <code>MS Mincho</code> contains Japanese glyphs. But how would an RTF parser know that <code>0x81AA</code> should be decoded using <code>Windows Code Page 932</code> and not <code>ansicpg1252</code> as specified in the first line of the file? Do I need to know that certain fonts imply certain encodings? </p>

<p>My best guess is that it has something to do with the part of the <code>\fonttbl</code> entry that says <code>\fcharset128</code>, but I'm not sure.</p>
",Multilingual Language Processing & Language Identification,properly decode hex value rtf unfortunately one go two rabbit hole text encoding rtf background working nlp text pipeline need convert rtf plain text word need remove rtf control character leave text content intact building pipeline python ha several requirement prevent u using something like apache tikka production know rtf contain hex value author document typed non ascii character also know first sequence control character document specifies decode hex value e g example case presence beginning document mean interpreted unicode code point per window encoding question came across rtf document first set control character however several place document following hex literal appear confusing becuase undefined encoding thought maybe could however defined either wordpad exe represents two byte character apache tikka us character character corresponds unicode code point turn mystery byte result encoding character using window code page encoding contains japanese character reference full context two byte rtf document document contains entry group far understand mean text following rendered using font kind make sense since contains japanese glyph would rtf parser know decoded using specified first line file need know certain font imply certain encoding best guess ha something part entry say sure
Fixing Perl errors when I try to pass a hash (by reference) AND a variable to a sub to print the corresponding value in the hash,"<p>I am banging my head over a Perl task in my Natural Language Processing course that we have been assigned to solve.</p>

<p>What they require us to be able to solve with Perl is the following:</p>

<ul>
<li><p>Input: the program takes two inputs from stdin in the form and type of;
  perl program.pl  </p></li>
<li><p>Processing and Output:</p>

<p>Part 1: the program tokenizes words in filename.txt and stores these words in a hash with their frequency of occurrence</p>

<p>Part 2: the program uses the  input for hashing purposes. If the word cannot be found in the hash (thus in the text), prints out zero as the frequency of the word. If the word CAN indeed be
found in the hash, prints out the corresponding frequency value of the word in the hash.</p></li>
</ul>

<p>I am sure from experience that my script is already able to DO ""Part 1"" stated above.</p>

<p>Part 2 needs to be accomplished using a Perl sub (subroutine) which takes the hash by reference, along with the  to hash for. This was the part that I had some serious trouble with.</p>

<p>First version before major changes Stefan Becker suggested;</p>

<pre><code>#!/usr/bin/perl                                                                           

use warnings;
use strict;

sub hash_4Frequency
{
    my ($hashWord, $ref2_Hash) = @_;                       
    print $ref2_Hash -&gt; {$hashWord}, ""\n"";  # thank you Stefan Becker, for sobriety
}

my %f = ();  # hash that will contain words and their frequencies                              
my $wc = 0;  # word-count                                       

my ($stdin, $word_2Hash) = @ARGV;  # corrected, thanks to Silvar

while ($stdin)
{
    while (""/\w+/"")
    {
        my $w = $&amp;;
        $_ = $"";
        $f{lc $w} += 1;
        $wc++;
    }
}

my @args = ($word_2Hash, %f);
hash_4Frequency(@args);
</code></pre>

<p>The second version after some changes;</p>

<pre><code>#!/usr/bin/perl

use warnings;
use strict;

sub hash_4Frequency
{
    my $ref2_Hash = %_;
    my $hashWord = $_;

    print $ref2_Hash -&gt; {$hashWord}, ""\n"";
}

my %f = ();  # hash that will contain words and their frequencies
my $wc = 0;  # word-count

while (&lt;STDIN&gt;) 
{
    while (/\w+/)
    {
        chomp;
        my $w = $&amp;;
        $_ = $"";

        $f{$_}++ foreach keys %f;
        $wc++;
    }
}

hash_4Frequency($_, \%f);
</code></pre>

<p>When I execute ' ./script.pl &lt; somefile.txt someWord ' in Terminal, Perl complains (Perl's output for the first version)</p>

<pre><code> Use of uninitialized value $hashWord in hash element at   
 ./word_counter2.pl line 35.

 Use of uninitialized value in print at ./word_counter2.pl line 35.
</code></pre>

<p>What Perl complains for the second version;</p>

<pre><code> Can't use string (""0"") as a HASH ref while ""strict refs"" in use at ./word_counter2.pl line 13, &lt;STDIN&gt; line 8390.
</code></pre>

<p>At least now I know the script can successfully work until this very last point, and it seems something semantic rather than syntactical.</p>

<p>Any further advice on this last part? Would be really appreciated.</p>

<p>P.S.: Sorry pilgrims, I am just a novice in the path of Perl.</p>
",Multilingual Language Processing & Language Identification,fixing perl error try pas hash reference variable sub print corresponding value hash banging head perl task natural language processing course assigned solve require u able solve perl following input program take two input stdin form type perl program pl processing output part program tokenizes word filename txt store word hash frequency occurrence part program us input hashing purpose word found hash thus text print zero frequency word word indeed found hash print corresponding frequency value word hash sure experience script already able part stated part need accomplished using perl sub subroutine take hash reference along hash wa part serious trouble first version major change stefan becker suggested second version change execute script pl somefile txt someword terminal perl complains perl output first version perl complains second version least know script successfully work last point seems something semantic rather syntactical advice last part would really appreciated p sorry pilgrim novice path perl
How can we identify inputs and outputs of a logic gate application problem using Natural Language Processing?,"<p>At the moment I have identified the entities in a given scenario, and I gave it a try to classify inputs entities and output entities using a neighborhood analysis technique for each identified entity. That method is not much accurate, since the way of providing the scenario might change time to time.</p>

<p>For an example:</p>

<ul>
<li>There is a <strong>door</strong>,<strong>window</strong> and a <strong>safe</strong>. The safe is unlocked , if the door
is closed and the window is opened.</li>
</ul>

<p>To find the Boolean expression for this we need to consider the safe, door and the window are entities.Furthermore the safe is an output entity and other two are input entities.</p>

<p>Can anyone suggest me a proper logic using python to identify which are input entities and which are output entities?</p>
",Multilingual Language Processing & Language Identification,identify input output logic gate application problem using natural language processing moment identified entity given scenario gave try classify input entity output entity using neighborhood analysis technique identified entity method much accurate since way providing scenario might change time time example door window safe safe unlocked door closed window opened find boolean expression need consider safe door window entity furthermore safe output entity two input entity anyone suggest proper logic using python identify input entity output entity
How to separate a mixed word (Persian and English) in python,"<p>Hi i have a dataset of strings, and some strings have mixed words such as below:</p>

<pre><code>    ÿ≥ŸÑÿßŸÖ12World
    ÿØŸàÿ±ÿ®€åŸÜdigital
    ÿ≥ÿßŸÑ2012good
</code></pre>

<p>...
and my desired output is :</p>

<pre><code>   12 ÿ≥ŸÑÿßŸÖ world
   ÿØŸàÿ±ÿ®€åŸÜ digital
   2012 ÿ≥ÿßŸÑ good
</code></pre>

<p>here is my code :</p>

<pre><code> def spliteKeyWord(str):
     regex = r""[\u200b-\u200c]|[0-9]+|[a-zA-Z]+\'*[a-z]*""
     matches = re.findall(regex, str, re.UNICODE)
     return matches
</code></pre>

<p>but this code doesnt show my desired output. Is it possible to get something like that output?</p>
",Multilingual Language Processing & Language Identification,separate mixed word persian english python hi dataset string string mixed word desired output code code doesnt show desired output possible get something like output
detect English words and nltk&#39;s words corpus,"<p>Just trying to see of a word is English or not. This:</p>

<pre><code>english_words = set(nltk.corpus.words.words())
print(""revised"" in english_words)
</code></pre>

<p>results in False. Am I doing something wrong? Is this to be expected? Are there better ways of doing this? Thanks. </p>
",Multilingual Language Processing & Language Identification,detect english word nltk word corpus trying see word english result false something wrong expected better way thanks
How to extract DateTime from German Sentences?,"<p>I am developing a chatbot with the Microsoft Bot Framework with LUIS (language understanding). Since the provided datetime entity is not sufficient I am currently struggeling with the best way how to detect dates in German. Dates can be in the format <code>'01.01.2019'</code> but also as relative date like <code>'n√§chste Woche'</code> (next week), <code>'morgen'</code> (tomorrow), <code>'am Montag'</code> (on Monday), etc.</p>

<p>Since a new DateTime Entity (v2) is already available for some languages in LUIS I tried to integrate the library <a href=""https://github.com/Microsoft/Recognizers-Text"" rel=""nofollow noreferrer"">https://github.com/Microsoft/Recognizers-Text</a> which is also used for the LUIS DateTimeV2 Entity within my code. Even though this is an improvement to the provided date from LUIS the result is still not sufficient. I already implemented some kind of backup logic but I am wondering if anybody knows a good library for German date extraction.</p>
",Multilingual Language Processing & Language Identification,extract datetime german sentence developing chatbot microsoft bot framework luis language understanding since provided datetime entity sufficient currently struggeling best way detect date german date format also relative date like next week tomorrow monday etc since new datetime entity v already available language luis tried integrate library also used luis datetimev entity within code even though improvement provided date luis result still sufficient already implemented kind backup logic wondering anybody know good library german date extraction
How can we derive the entities related to verb in a sentence using Natural Language Processing techniques?,"<p>If we are given text and we need to extract the noun phrases which are dominated by the verb in each sentence of that text. How can we figure out such entities?  </p>
",Multilingual Language Processing & Language Identification,derive entity related verb sentence using natural language processing technique given text need extract noun phrase dominated verb sentence text figure entity
Why does a Sentence parse throw an exception only the first time?,"<p>I am using the <code>edu.stanford.nlp.simple</code> package to generate parse trees for sentences in several different languages. The English and Chinese models produce the expected results, e.g.</p>

<pre><code>&gt; val s = new Sentence(""The quick brown fox jumps over the lazy dog."")
&gt; s.parse

res1: edu.stanford.nlp.trees.Tree = (ROOT (NP (NP (DT The) (JJ quick) (JJ brown) (NN fox)) (NP (NP (NNS jumps)) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog))))))
</code></pre>

<p>(I am using Scala here but that shouldn't make a difference.)</p>

<p>Other languages like German, however, exhibit a strange behavior:</p>

<pre><code>&gt; val p = new Properties()
&gt; p.load(IOUtils.readerFromString(""StanfordCoreNLP-german.properties""))
&gt; val s = new Sentence(""Ich hoffe, dass es funktionieren wird."")
&gt; s.parse(p)

10:48:34.127 [main] INFO  e.s.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/germanFactored.ser.gz ... done  [1.4sec].                                                                                                                                    
java.lang.NullPointerException                                                                                                                            
  edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:672)                                                   
  edu.stanford.nlp.simple.Document.runParse(Document.java:933)                                                                                            
  edu.stanford.nlp.simple.Sentence.parse(Sentence.java:637)                                                                                               
  ammonite.$sess.cmd3$.&lt;init&gt;(cmd3.sc:1)                                                                                                                  
  ammonite.$sess.cmd3$.&lt;clinit&gt;(cmd3.sc)

&gt; s.parse(p)
res4: edu.stanford.nlp.trees.Tree = (ROOT (S (PPER Ich) (VVFIN hoffe) ($, ,) (S (KOUS dass) (PPER es) (VP (VVINF funktionieren)) (VAFIN wird)) ($. .)))
</code></pre>

<p>I have examined the properties <code>p</code> to verify that they haven't changed -- the parser consistently throws a <code>NullPointerException</code> on the first invocation and then works correctly afterwards for the same sentence.</p>

<p>I have had a look through the source for CoreNLP but can't find an explicit reason why this might be happening... I wonder if I'm missing something?</p>

<p>I am using Stanford-CoreNLP version <code>3.9.1</code>. The foreign language models I'm referencing are the ones from the Maven repository, also discussed on the <a href=""https://stanfordnlp.github.io/CoreNLP/human-languages.html"" rel=""nofollow noreferrer"">Stanford CoreNLP website</a>.</p>
",Multilingual Language Processing & Language Identification,doe sentence parse throw exception first time using package generate parse tree sentence several different language english chinese model produce expected result e g using scala make difference language like german however exhibit strange behavior examined property verify changed parser consistently throw first invocation work correctly afterwards sentence look source corenlp find explicit reason might happening wonder missing something using stanford corenlp version foreign language model referencing one maven repository also discussed stanford corenlp website
How to use the regular expression to transform the &lt;U+5C16&gt; into \u5c16?,"<p>I am doing a sentiment analysis project and firstly, I need to clean the text data. Some text contains Chinese, Tagalog and what I am doing now is trying to translate them to English. But until now, all the Chinese characters in this datafile have some Unicode representation like:</p>

<pre><code>&lt;U+5C16&gt;
</code></pre>

<p>which could not be coped with using the Python Encoding&amp;Decoding path. So I want to transform this kind of pattern to:</p>

<pre><code>\u5c16
</code></pre>

<p>Then I think we could use the following code to get the Chinese characters I want:</p>

<pre><code>text.encode('latin-1').decode('unicode_escape')
</code></pre>

<p>So the question now is how to use the regex to transform <code>&lt;U+5C16&gt;</code> into<code>\u5c16</code>?</p>

<p>Thank you very much!</p>

<hr>

<p>Update: I think the most difficult thing here is that I need to let the <code>5c16</code> part in <code>\u5c16</code> be equivalent to the lowercase of the <code>5C16</code> in <code>&lt;U+5C16&gt;</code>. And in my social media dataset, what I see most is the text data like the following:</p>

<p><code>&lt;U+5C16&gt;&lt;U+6C99&gt;&lt;U+5480&gt;&lt;U+9418&gt;&lt;U+6A13&gt;</code></p>

<p>If I could transform the above text to <code>'\u5c16\u6c99\u5480\u9418\u6a13'</code> and print it in Python, I could get what I really want:</p>

<pre><code>Â∞ñÊ≤ôÂíÄÈêòÊ®ì
</code></pre>

<p>But how could I do this? Any insights and hints would be appreciated!</p>
",Multilingual Language Processing & Language Identification,use regular expression transform u c u c sentiment analysis project firstly need clean text data text contains chinese tagalog trying translate english chinese character datafile unicode representation like could coped using python encoding decoding path want transform kind pattern think could use following code get chinese character want question use regex transform thank much update think difficult thing need let part equivalent lowercase social medium dataset see text data like following could transform text print python could get really want could insight hint would appreciated
How to convert an English string of a number into a float (e.g. &quot;twenty-six&quot;=&gt;26.0),"<p>I'm building a chatbot and I want to be able to parse strings representing numerical amounts that represent a human age. When I ask <code>""How old are you?""</code> the user should be able to input <code>""26""</code> or <code>""twenty-six""</code> and have the bot understand it as <code>26.0</code>. The method should handle converting fractions to decimals.</p>

<p>Examples:</p>

<pre><code>parse(""six"")              # =&gt; 6.0
parse(""twenty-five"")      # =&gt; 25.0
parse(""three and a half"") # =&gt; 3.5
</code></pre>

<p>Is there a gem that handles this in Ruby already, or is it easy enough to write a simple <code>parse</code> method?</p>
",Multilingual Language Processing & Language Identification,convert english string number float e g twenty six building chatbot want able parse string representing numerical amount represent human age ask user able input bot understand method handle converting fraction decimal example gem handle ruby already easy enough write simple method
Spacy - Intepret text with stripped accents,"<p>I am working with spacy to analyze some text in french.</p>

<p>Somehow, in my data, all accented characters have been replaced by their non accented equivalents (ex: r√©serve --> reserve).</p>

<p>Hence, when I try to tokenize my strings and retrieve lemmas, spacy fails to identify my non accented terms.</p>

<pre><code>import spacy
nlp = spacy.load('fr')

doc_accented = u'r√©serve moi une chambre'
[token.lemma_ for token in nlp_test(doc_accented) if token.is_punct==False and token.is_space==False and token.is_stop==False]
# Returns : [u'r√©server', u'chambrer']

doc_not_accented = u'reserve moi une chambre'
[token.lemma_ for token in nlp_test(doc_not_accented) if token.is_punct==False and token.is_space==False and token.is_stop==False]
# Returns : [u'reserve', u'chambrer']
</code></pre>

<p>I tried to add new entries to the vocabulary by removing all accents from the existing entries and copying their attributes.</p>

<pre><code>import unicodedata

def strip_accents(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

attr_list = ['a list of attributes']

for el in nlp.vocab.strings:
     normalized_str = strip_accents(el)
     lexeme = nlp_test.vocab[normalized_str]

     for attr in attr_list:
         setattr(lexeme, attr, getattr(nlp.vocab[el], attr))
</code></pre>

<p>Do you know if there is a way to edit the vocabulary such as <code>u'reserve</code> can be interpreted by spacy as <code>u'r√©serve'</code> ?</p>

<p>Xavier</p>
",Multilingual Language Processing & Language Identification,spacy intepret text stripped accent working spacy analyze text french somehow data accented character replaced non accented equivalent ex r serve reserve hence try tokenize string retrieve lemma spacy fails identify non accented term tried add new entry vocabulary removing accent existing entry copying attribute know way edit vocabulary interpreted spacy xavier
Determine if a list of words is in a sentence?,"<p>Is there a way (Pattern or Python or NLTK, etc) to detect of a sentence has a list of words in it.</p>

<p>i.e. </p>

<p><code>The cat ran into the hat, box, and house.</code> |  <code>The list would be hat, box, and house</code></p>

<p>This could be string processed but we may have more generic lists:</p>

<p>i.e. </p>

<p><code>The cat likes to run outside, run inside, or jump up the stairs.</code> | </p>

<p><code>List=run outside, run inside, or jump up the stairs.</code></p>

<p>This could be in the middle of a paragraph or the end of the sentence which further complicates things.</p>

<p>I've been working with Pattern for python for awhile and I'm not seeing a way to go about this and was curious if there is a way with pattern or nltk (natural language tool kit).</p>
",Multilingual Language Processing & Language Identification,determine list word sentence way pattern python nltk etc detect sentence ha list word e could string processed may generic list e could middle paragraph end sentence complicates thing working pattern python awhile seeing way go wa curious way pattern nltk natural language tool kit
"Using Python &amp; NLP, how can I extract certain text strings &amp; corresponding numbers preceding the strings from Excel column having a lot of free text?","<p>I am relatively new to Python and very new to NLP (and nltk) and I have searched the net for guidance but not finding a complete solution. Unfortunately the sparse code I have been playing with is on another network, but I am including an example spreadsheet. I would like to get suggested steps in plain English (more detailed than I have below) so I could first try to script it myself in Python 3. Unless it would simply be easier for you to just help with the scripting... in which case, thank you.</p>

<p>Problem: A few columns of an otherwise robust spreadsheet are very unstructured with anywhere from 500-5000 English characters that tell a story. I need to essentially make it a bit more structured by pulling out the quantifiable data. I need to:</p>

<p>1) Search for a string in the user supplied unstructured free text column (The user inputs the column header) (I think I am doing this right)</p>

<p>2) Make that string a NEW column header in Excel (I think I am doing this right)</p>

<p>3) Grab the number before the string (This is where I am getting stuck. And as you will see in the sheet, sometimes there is no space between the number and text and of course, sometimes there are misspellings)</p>

<p>4) Put that number in the NEW column on the same row (Have not gotten to this step yet)</p>

<p>I will have to do this repeatedly for multiple keywords but I can figure that part out, I believe, with a loop or something. Thank you very much for your time and expertise...</p>

<p><a href=""https://i.sstatic.net/E9q4Z.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E9q4Z.jpg"" alt=""Example of spreadsheet with unstructured free text""></a></p>
",Multilingual Language Processing & Language Identification,using python nlp extract certain text string corresponding number preceding string excel column lot free text relatively new python new nlp nltk searched net guidance finding complete solution unfortunately sparse code playing another network including example spreadsheet would like get suggested step plain english detailed could first try script python unless would simply easier help scripting case thank problem column otherwise robust spreadsheet unstructured anywhere english character tell story need essentially make bit structured pulling quantifiable data need search string user supplied unstructured free text column user input column header think right make string new column header excel think right grab number string getting stuck see sheet sometimes space number text course sometimes misspelling put number new column row gotten step yet repeatedly multiple keywords figure part believe loop something thank much time expertise
Add a language in the Stanford parser,"<p>I would like to use the Stanford parser in another language not already implemented.</p>

<p>I looked on the website but found nothing that could help me with that.</p>

<p>I guess what I have to do is ""just"" create a new languagePCFG.ser but to do that?</p>

<p>Also, if anyone knows if French and Spanish are supposed to be released?</p>
",Multilingual Language Processing & Language Identification,add language stanford parser would like use stanford parser another language already implemented looked website found nothing could help guess create new languagepcfg ser also anyone know french spanish supposed released
Neural network for text generation - Reverse summarizer (Python / Keras),"<p>I am about to start working on a neural network for text generation. Inputs will be some words from a user (e.g. Brexit vote tomorrow chance of UK staying within EU slim) and the output will be a nice, well-written sentence (e.g. The Brexit vote will take place tomorrow and the UK is unlikely to stay within the European Union).</p>

<p>For the implementation, I am thinking about a sequence2sequence model but, before starting to code, I would like to check whether this subject has not been addressed before. After many Google searches, it seems that nobody has done a similar project before (although there's a lot of papers about text translation), which surprises me because such a tool would be useful for many people, such as journalists, etc. </p>

<p>Has any of you seen some useful Python code or relevant articles somewhere?</p>
",Multilingual Language Processing & Language Identification,neural network text generation reverse summarizer python kera start working neural network text generation input word user e g brexit vote tomorrow chance uk staying within eu slim output nice well written sentence e g brexit vote take place tomorrow uk unlikely stay within european union implementation thinking sequence sequence model starting code would like check whether subject ha addressed many google search seems nobody ha done similar project although lot paper text translation surprise tool would useful many people journalist etc ha seen useful python code relevant article somewhere
Python - Google Natural Language Processing API return HttpError 502 bad gateway,"<p>I'm working on a project using Python(3.6) and Django(2) in which I need to process too many(~ 50k) files from a directory by using Google cloud natural language processing API, but after processing ~400 files it returns an error as:</p>

<blockquote>
  <p><strong>Note:</strong> I have googled a lot, so don't mark this as duplicate, please!</p>
  
  <p>raise HttpError(resp, content, uri=self.uri)</p>
  
  <p>googleapiclient.errors.HttpError: https://language.googleapis.com/v1beta2/documents:analyzeSentiment?alt=json returned ""Bad Gateway""></p>
  
  <p>[30/Dec/2018 05:10:03] ""POST / HTTP/1.1"" 500 15162</p>
</blockquote>

<p><strong>Here's from <code>views.py</code>:</strong></p>

<pre><code>def nlp_text_manager(text_path, name):
    text = text_path
    # encoding = predict_encoding(text_path)
    # print('encoding is: {}'.format(encoding))
    txt = Path(text_path).read_text(encoding='utf8')
    service = discovery.build('language', 'v1beta2', credentials=credentials)
    service_request = service.documents().analyzeSentiment(
        body={
            'document': {
                'type': 'PLAIN_TEXT',
                'content': txt
            }
        }
    )
    response = service_request.execute()
    return response
</code></pre>
",Multilingual Language Processing & Language Identification,python google natural language processing api return httperror bad gateway working project using python django need process many k file directory using google cloud natural language processing api processing file return error note googled lot mark duplicate please raise httperror resp content uri self uri googleapiclient error httperror returned bad gateway dec post http
"‚ÄúIs there an R function(text-analysis) for tagging the subject of word( eg : noun,adj)?‚Äù","<p>I am doing a text-mining about sentiment analysis. However, i have met a problem when I was using text-mining with English articles. I want to ask is there any function similar to the function ""worker(type='tag')"" of Jieba package, but it is using in the English text-mining package (eg: tidytext)?</p>

<p>In the following, it is part of my code. This code is using for Chinese text-mining. However, i want to do a English text-mining by the similar way. What function can I use to replace the function  of worker(type=""tag"")</p>

<pre><code>library(jiebaRD)
library(jiebaR)
library(dplyr)
jieba &lt;- worker(type=""tag"",user=""C:/Users/User/Desktop/dict/bbb.txt"",symbol = TRUE)

ecal&lt;-function(str){
  result &lt;- jieba &lt;= str    
  winfront &lt;- 1L 
  count &lt;- 1  
  winvalue &lt;- c()  
  posvalue &lt;- c()  
  negvalue &lt;-c ()  
  pvalue &lt;- 0L     
  nvalue &lt;- 0L    
  ppcount &lt;- 1
  nncount &lt;- 1
  rheflag &lt;- FALSE
  for (i in 1:length(result)){
    if(names(result[i])==""positive""){      
      #cat(""find positive wordÔºö"",result[i],""\n"")      
      if(i==1)
        winvalue[count] &lt;- 1
      else{
        winvalue[count] &lt;- 1
        for (j in (i-1):winfront) {
          if(!is.na((as.numeric(names(result[j])))))
            winvalue[count] = winvalue[count]*as.numeric(names(result[j]))
          else if(names(result[j])==""deny"")
            winvalue[count] = winvalue[count]*(-1)
          else if(names(result[j])==""rhe"")
            rheflag &lt;- TRUE
        }
      }
      #cat(""the value of window isÔºö"",winvalue[count],""\n"")      
      count = count+1
      winfront &lt;- i+1
    }
</code></pre>
",Multilingual Language Processing & Language Identification,r function text analysis tagging subject word eg noun adj text mining sentiment analysis however met problem wa using text mining english article want ask function similar function worker type tag jieba package using english text mining package eg tidytext following part code code using chinese text mining however want english text mining similar way function use replace function worker type tag
Python- Google NLP Api returns ssl.SSLEOFError: EOF occurred in violation of protocol,"<p>I'm working on a project using Python(3.6) and Django(2) in which I need to process too many files from a directory by using Google cloud natural language processing API, but after processing ~100 files it returns an error as:</p>

<blockquote>
  <p>ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:852)
  [29/Dec/2018 13:27:33] ""POST / HTTP/1.1"" 500 17751</p>
</blockquote>

<p><strong>Here's from <code>views.py</code>:</strong></p>

<pre><code>def nlp_text_manager(text_path, name):
    text = text_path
    # encoding = predict_encoding(text_path)
    # print('encoding is: {}'.format(encoding))
    txt = Path(text_path).read_text(encoding='utf8')
    service = discovery.build('language', 'v1beta2', credentials=credentials)
    service_request = service.documents().analyzeSentiment(
        body={
            'document': {
                'type': 'PLAIN_TEXT',
                'content': txt
            }
        }
    )
    response = service_request.execute()
    return response
</code></pre>
",Multilingual Language Processing & Language Identification,python google nlp api return ssl ssleoferror eof occurred violation protocol working project using python django need process many file directory using google cloud natural language processing api processing file return error ssl ssleoferror eof occurred violation protocol ssl c dec post http
Wikipedia model training parameters,"<p>Pretrained models of English and other language wikipedia are available here... </p>

<p><a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a></p>

<p>What is the difference between 100d and 500d in case of English wikipedia? </p>

<p>And what does these parameters mean to training (window=5, iteration=10, negative=15)</p>
",Multilingual Language Processing & Language Identification,wikipedia model training parameter pretrained model english language wikipedia available difference case english wikipedia doe parameter mean training window iteration negative
Module import issue with a Japanese Tokenizer,"<p>I am trying to get the JapaneseTokenizer working in python, but I am having trouble with one of the modules it depends on. Here is the trace of the errors I am getting:</p>

<pre><code>/Users/home/PycharmProjects/SubLingo/application/tokenizerTest.py
    Traceback (most recent call last):
      File ""/Users/home/PycharmProjects/SubLingo/application/tokenizerTest.py"", line 1, in &lt;module&gt;
        import JapaneseTokenizer
      File ""/Users/home/PycharmProjects/SubLingo/venv/lib/python3.7/site-packages/JapaneseTokenizer/__init__.py"", line 6, in &lt;module&gt;
        from JapaneseTokenizer.jumanpp_wrapper import JumanppWrapper
      File ""/Users/home/PycharmProjects/SubLingo/venv/lib/python3.7/site-packages/JapaneseTokenizer/jumanpp_wrapper/__init__.py"", line 1, in &lt;module&gt;
        from .jumanpp_wrapper import JumanppWrapper
      File ""/Users/home/PycharmProjects/SubLingo/venv/lib/python3.7/site-packages/JapaneseTokenizer/jumanpp_wrapper/jumanpp_wrapper.py"", line 2, in &lt;module&gt;
        from pyknp import Jumanpp
    ImportError: cannot import name 'Jumanpp' from 'pyknp' (/Users/home/PycharmProjects/SubLingo/venv/lib/python3.7/site-packages/pyknp/__init__.py)
</code></pre>

<p>As you can see Jumanpp_wrapper is trying to import the module Jumanpp from pyknp. I have looked into the pyknp package currently installed on my machine and it does not have a module with this name. This leads me to conclude that the version of pyknp I have installed is not compatible with Jumanpp, so there must be another version available somewhere. The trouble is I install pyknp using the pip installer on my Mac, as recommended on the pyknp official site, so it should be the most current version. I'm not sure how to get an alternative version that contains the necessary module. I hope someone can point me in the right direction.</p>
",Multilingual Language Processing & Language Identification,module import issue japanese tokenizer trying get japanesetokenizer working python trouble one module depends trace error getting see jumanpp wrapper trying import module jumanpp pyknp looked pyknp package currently installed machine doe module name lead conclude version pyknp installed compatible jumanpp must another version available somewhere trouble install pyknp using pip installer mac recommended pyknp official site current version sure get alternative version contains necessary module hope someone point right direction
"OpenNLP-Document Categorizer- how to classify documents based on status; language of docs not English, also default features?","<p>I want to classify my documents using OpenNLP's Document Categorizer, based on their status: pre-opened, opened, locked, closed etc.</p>

<p>I have 5 classes and I'm using the Naive Bayes algorithm, 60 documents in my training set, and trained my set on 1000 iterations with 1 cut off param.</p>

<p>But no success, when I test them I don't get good results. I was thinking maybe it is because of the language of the documents (is not in English) or maybe I should somehow add the statuses as features. I have set the default features in the categorizer, and also I'm not very familiar with them.</p>

<p>The result should be locked, but its categorized as opened.</p>

<pre><code>InputStreamFactory in=null;
try {
in= new MarkableFileInputStreamFactory(new 
File(""D:\\JavaNlp\\doccategorizer\\doccategorizer.txt""));
}
catch (FileNotFoundException e2) {
System.out.println(""Creating new input stream"");
e2.printStackTrace();
}

ObjectStream lineStream=null;
ObjectStream sampleStream=null;

try {
lineStream = new PlainTextByLineStream(in, ""UTF-8"");
sampleStream = new DocumentSampleStream(lineStream);            
}
catch (IOException e1) {
System.out.println(""Document Sample Stream"");
e1.printStackTrace();
}


TrainingParameters params = new TrainingParameters();
params.put(TrainingParameters.ITERATIONS_PARAM, 1000+"""");
params.put(TrainingParameters.CUTOFF_PARAM, 1+"""");
params.put(AbstractTrainer.ALGORITHM_PARAM, 
NaiveBayesTrainer.NAIVE_BAYES_VALUE);


DoccatModel model=null;
try {
model = DocumentCategorizerME.train(""en"", sampleStream, params, new 
DoccatFactory());
} 
catch (IOException e) 
{
System.out.println(""Training..."");
e.printStackTrace();
}


System.out.println(""\nModel is successfully trained."");


BufferedOutputStream modelOut=null;

try {
modelOut = new BufferedOutputStream(new 
FileOutputStream(""D:\\JavaNlp\\doccategorizer\\classifier-maxent.bin""));
} 
catch (FileNotFoundException e) {

System.out.println(""Creating output stream"");
e.printStackTrace();
}
try {
model.serialize(modelOut);
}
catch (IOException e) {

System.out.println(""Serialize..."");
e.printStackTrace();
}
System.out.println(""\nTrained model is kept in: 
""+""model""+File.separator+""en-cases-classifier-maxent.bin"");

DocumentCategorizer doccat = new DocumentCategorizerME(model);
String[] docWords = ""Some text here..."".replaceAll(""[^A-Za-z]"", "" "").split("" "");
double[] aProbs = doccat.categorize(docWords);


System.out.println(""\n---------------------------------\nCategory : 
Probability\n---------------------------------"");
for(int i=0;i&lt;doccat.getNumberOfCategories();i++){
System.out.println(doccat.getCategory(i)+"" : ""+aProbs[i]);
}
System.out.println(""---------------------------------"");

System.out.println(""\n""+doccat.getBestCategory(aProbs)+"" : is the category 
for the given sentence"");
</code></pre>

<p><img src=""https://photos.app.goo.gl/GEgrCSJjk8NqaSe7A"" alt=""results""> 
<img src=""https://photos.app.goo.gl/j2qDeLNm6wiw9nzx8"" alt=""results2""></p>

<p>Can someone make a suggestion for me how to categorize my documents well, like should I add a language detector first, or add new features?</p>

<p>Thanks in advance</p>
",Multilingual Language Processing & Language Identification,opennlp document categorizer classify document based status language doc english also default feature want classify document using opennlp document categorizer based status pre opened opened locked closed etc class using naive bayes algorithm document training set trained set iteration cut param success test get good result wa thinking maybe language document english maybe somehow add status feature set default feature categorizer also familiar result locked categorized opened someone make suggestion categorize document well like add language detector first add new feature thanks advance
Clean text data in Python,"<p>I want to create a new column for the text data (every row for that column is one description) after removing all numbers (such as 189, 98001), special characters ( ‚Äò  ,  _, ‚Äú, (, ) ), and letters with numbers or special characters (e21x16, e267, e4, e88889, entry778, id2, n27th, pv3,  ). </p>

<p>So I wrote the function below. However, the returned results still contain numbers, and special characters. Basically, my goal is to keep only English words, and abbreviations. Does anyone know why my function is not working.</p>

<pre><code>def standardize_text(df, text_field):
  df[text_field] = df[text_field].str.lower()
  df[text_field] = df[text_field].str.replace(r'(', '') 
  df[text_field] = df[text_field].str.replace(r')', '')
  df[text_field] = df[text_field].str.replace(r',', '')
  df[text_field] = df[text_field].str.replace(r'_', '')
  df[text_field] = df[text_field].str.replace(r""'"", """")
  df[text_field] = df[text_field].str.replace(r""^[a-z]+\[0-9]+$"", """")
  df[text_field] = df[text_field].str.replace(r""^[0-9]{1,2,3,4,5}$"", """")
  return df
</code></pre>
",Multilingual Language Processing & Language Identification,clean text data python want create new column text data every row column one description removing number special character letter number special character e x e e e entry id n th pv wrote function however returned result still contain number special character basically goal keep english word abbreviation doe anyone know function working
SMS Text normalization,"<p>I am looking for a good library or some project that has been done in the area of SMS text normalization. I have found some good research projects like <a href=""http://www-nlp.stanford.edu/sms/"" rel=""nofollow"">this</a> one.</p>

<p>I am using Java as the programming language.</p>

<p>The concept in a nutshell is to handle SMS based text like ""<strong>tel him 2 go home nw</strong>"" and convert it to normal english language text ""<strong>tell him to go home now</strong>"".</p>
",Multilingual Language Processing & Language Identification,sm text normalization looking good library project ha done area sm text normalization found good research project like one using java programming language concept nutshell handle sm based text like tel go home nw convert normal english language text tell go home
MeCab Not Parsing Correctly,"<p>I downloaded MeCab to parse some Japanese text. To test it out, I tried doing what some examples online showed.</p>

<p>For example, I followed this guy's tips verbatim: <a href=""http://www.robfahey.co.uk/blog/japanese-text-analysis-in-python/"" rel=""nofollow noreferrer"">http://www.robfahey.co.uk/blog/japanese-text-analysis-in-python/</a></p>

<p>The code is as follows:</p>

<pre><code>import MeCab

test = ""‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ""
mt = MeCab.Tagger(""-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd"")
parsed = mt.parseToNode(test)

components = []
while parsed:
    components.append(parsed.surface)
    parsed = parsed.next

print(components)
</code></pre>

<p>The output that I'm expecting is:</p>

<pre><code>['', '‰ªäÊó•', '„ÅØ', '„ÅÑ„ÅÑ', 'Â§©Ê∞ó', '„Åß„Åô', '„Å≠', '„ÄÇ', 'ÈÅä„Å≥', '„Å´', 'Ë°å„Åã', '„Å™„ÅÑ', 'Ôºü', 'Êñ∞ÂÆø', '„Åß', 'Á•≠„Çä', '„Åå', '„ÅÇ„Çã', 'ÔºÅ', '']
</code></pre>

<p>However, I'm getting this:</p>

<pre><code>['‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', 'Â§©Ê∞ó„Åß„Åô„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„Åß„Åô„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„Å≠„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„ÄÇÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', 'ÈÅä„Å≥„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„Å´Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', 'Ë°å„Åã„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„Å™„ÅÑÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', 'ÔºüÊñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', 'Êñ∞ÂÆø„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„ÅßÁ•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', 'Á•≠„Çä„Åå„ÅÇ„ÇãÔºÅ', '„Åå„ÅÇ„ÇãÔºÅ', '„ÅÇ„ÇãÔºÅ', 'ÔºÅ', '']
</code></pre>

<p>To anyone familiar with MeCab or parsing node in general, what exactly am I doing wrong? Thanks again for your help!</p>
",Multilingual Language Processing & Language Identification,mecab parsing correctly downloaded mecab parse japanese text test tried example online showed example followed guy tip verbatim code follows output expecting however getting anyone familiar mecab parsing node general exactly wrong thanks help
Creating Words Dictionary and Mapping to other languages,"<p>I am creating an app, where user can search in multiple languages. Let's say I have a website which host a very big novel. may be consisting of thousands of para graphs. some where between 20,000 to 30,000 para graphs. </p>

<p>A user can read the novel in multiple languages. He can search in multiple languages. For example he searches ""LOVE"" in English, I will show him all the paragraphs containing ""LOVE"" in them. </p>

<p>Now, if user switches to French, and searches ""Amour"" (French for Love), I will show him all paragraphs containing ""Amour"" in them. </p>

<p>I can do it by creating two versions of the novel, one in french and other in English. Indeed, I will have the translations. But, when user is reading the novel I will give him an option to click on any word and see it's translation to other languages. In this case, if he is reading in English, I will show him translation of that particular word in French and vise versa. </p>

<p>This means I want to keep a word to word mapping between different languages. </p>

<p>One way of doing is to create a map my self, which is a lot of work. another way could be some API calls, for example Google Translator. 
Can you suggest best approach? Any existing API?
Some Google Terms to go into the right direction for this task would also be helpful. </p>
",Multilingual Language Processing & Language Identification,creating word dictionary mapping language creating app user search multiple language let say website host big novel may consisting thousand para graph para graph user read novel multiple language search multiple language example search love english show paragraph containing love user switch french search amour french love show paragraph containing amour creating two version novel one french english indeed translation user reading novel give option click word see translation language case reading english show translation particular word french vise versa mean want keep word word mapping different language one way create map self lot work another way could api call example google translator suggest best approach existing api google term go right direction task would also helpful
Find the most frequent words in a file,"<p>I have a file and I want to find the 10 most frequent words in that. I omitted stop words and punctuation and then put the result in a list. Each line contains a Persian sentence, a tab and then an English word. The problem is, the code below returns one word of each line. for example if the number of lines is 12, it returns 12 words. I think the indentation has problem. How can I fix it?</p>

<pre><code>.
.
.
def train ():
    RemStopWords (file1, file2)  # the function for removing stop words and punctuation at the start of the code
    for line in witoutStops:
        line = line.strip().split(""\t"")
        words = line[0].split()
        uniques = []
        q = []
        for word in words:
            if word not in uniques:
                uniques.append(word)
        counts = []
        for unique in uniques:
            count = 0              
            for word in words:     
                if word == unique:   
                    count += 1         
            counts.append((count, unique))
            counts.sort()        
            counts.reverse()
            for i in range(min(10, len(counts))):
                count, word = counts[i]
            print('%s %d' % (word, count))
            #q.append(word)
            #print (q)
</code></pre>
",Multilingual Language Processing & Language Identification,find frequent word file file want find frequent word omitted stop word punctuation put result list line contains persian sentence tab english word problem code return one word line example number line return word think indentation ha problem fix
How to decide if a webpage is about a specific topic or not?,"<p>I am trying to write a code which can take the source html of a web page then decide what kind of web page it is. I am intrested in deciding if the web page is about academic courses or not. A naive first approach that I have is to check if the text has words which can be related like (course, instructor, teach,...) and decide that it is about an academic course if it achieves enough hits.</p>

<p>Even though, I need some ideas how to achieve that more efficiently.</p>

<p>Any ideas would be appreciated.</p>

<p>Thanks in advance :)</p>

<p>Sorry for my English.</p>
",Multilingual Language Processing & Language Identification,decide webpage specific topic trying write code take source html web page decide kind web page intrested deciding web page academic course naive first approach check text ha word related like course instructor teach decide academic course achieves enough hit even though need idea achieve efficiently idea would appreciated thanks advance sorry english
Arabic WordNet Specifications,"<p>Is there any documentation or tutorial for Arabic WordNet? either for this 
 <a href=""http://globalwordnet.org/arabic-wordnet/"" rel=""nofollow noreferrer"">Arabic WordNet</a> or from the <a href=""http://compling.hss.ntu.edu.sg/omw/"" rel=""nofollow noreferrer"">Open Multilingual Wordnet</a>.</p>

<p>The answer in this <a href=""https://stackoverflow.com/questions/34620627/using-arabic-wordnet-for-synonyms-in-python"">link</a> shows an example, but I want a full description such as in <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""nofollow noreferrer"">English WordNet</a>. </p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,arabic wordnet specification documentation tutorial arabic wordnet either arabic wordnet open multilingual wordnet answer english wordnet thanks
How to add known words tokenizer keras python?,"<p>I want to convert text to sequence using keras with indonesian languages. but the keras tokenizer only detect the known word.</p>

<p>How to add known words in keras? or any solution for me to convert text to sequence?</p>

<pre><code>from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=n_most_common_words, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(concated['TITLE'].values)
txt = [""bisnis di indonesia sangat maju""]
seq = list(tokenizer.texts_to_sequences_generator(txt))
</code></pre>

<p>the ""seq"" variable resulting empty array if i used indonesian languages, its work perfectly if i used the english word. how to use keras for different languages? or anyway to add some known word to keras?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,add known word tokenizer kera python want convert text sequence using kera indonesian language kera tokenizer detect known word add known word kera solution convert text sequence seq variable resulting empty array used indonesian language work perfectly used english word use kera different language anyway add known word kera thanks
NLP to SQl query mapping algorithm which is independent of the database on which it is applied,"<p>user gives me an excel sheet and asks questions pertaining to that sheet in english. I need to construct  one/set of sql queries from his question so that i can execute them and return the output to the user. 
Identifying the column names, data values,arithmetic operators in the user entered question did work for some questions.But my code returns the same output for the following two queries which is wrong</p>

<p>show me states and their sales for state = texas</p>

<p>show me all the states which have sales equal to that of texas</p>

<p>*here 'state' , 'sales' are column names. 'texas' is a data value in the column 'states'</p>

<p>Whatever papers i have read so far only do NLP to SQL mapping for a specific database.I have also tried stanford lexicalized parser for dependencies, clauseIE algorithm for clause extraction. But nothing that i have tried gave me desired results.</p>

<p>any lead in the right direction will be appreciated</p>
",Multilingual Language Processing & Language Identification,nlp sql query mapping algorithm independent database applied user give excel sheet asks question pertaining sheet english need construct one set sql query question execute return output user identifying column name data value arithmetic operator user entered question work question code return output following two query wrong show state sale state texas show state sale equal texas state sale column name texas data value column state whatever paper read far nlp sql mapping specific database also tried stanford lexicalized parser dependency clauseie algorithm clause extraction nothing tried gave desired result lead right direction appreciated
Removing parentheses and everything in them with Regex,"<p>Having a bit of trouble with some code I'm working through. Basically, I have transcripts (txt files) for a few Japanese anime, of which I want to remove everything but the spoken lines (Japanese sentences) in order to do some NLP experiments.</p>

<p>I've managed to accomplish a good bit of cleaning, but where I'm stuck is with parentheses. A majority of the elements in my list start with a character's name inside parentheses (i.e. (Armin)). I want to remove these, but all the regex code I've found online doesn't seem to work.</p>

<p>Here's a snippet of the list I'm working with:</p>

<pre><code>['Ôºà„Ç¢„É´„Éü„É≥Ôºâ„Åù„ÅÆÊó•', '‰∫∫È°û„ÅØÊÄù„ÅÑÂá∫„Åó„Åü', 'Ôºà„Ç¢„É´„Éü„É≥ÔºâÂ•¥„Çâ„Å´', 'ÊîØÈÖç„Åï„Çå„Å¶„ÅÑ„ÅüÊÅêÊÄñ„Çí', 'Ôºà„Ç¢„É´„Éü„É≥ÔºâÈ≥•Á±†„ÅÆ‰∏≠„Å´', '„Å®„Çâ„Çè„Çå„Å¶„ÅÑ„Åü‚Äï', 'Â±àËæ±„Çí', 'Ôºà„Ç≠„Éº„ÇπÔºâÁ∑èÂì°', 'Êà¶ÈóòÁî®ÊÑèÔºÅ', 'ÁõÆÊ®ô„ÅØÔºë‰Ωì„Å†', 'ÂøÖ„Åö‰ªïÁïô„ÇÅ‚Äï', '„Åì„Åì„Çí', 'Êàë„ÄÖ', '‰∫∫È°û', 'ÊúÄÂàù„ÅÆÂ£ÅÂ§ñÊã†ÁÇπ„Å®„Åô„ÇãÔºÅ', 'Ôºà„Ç®„É´„É¥„Ç£„É≥Ôºâ„ÅÇ„Å£‚Ä¶', 'ÁõÆÊ®ôÊé•ËøëÔºÅ', 'Ôºà„Ç≠„Éº„ÇπÔºâË®ìÁ∑¥„Å©„Åä„ÇäÔºï„Å§„Å´ÂàÜ„Åã„Çå„ÇçÔºÅ', 'ÂõÆ„ÅØÊàë„ÄÖ„ÅåÂºï„ÅçÂèó„Åë„ÇãÔºÅ', 'ÂÖ®ÊîªÊíÉÁè≠', 'Á´ã‰ΩìÊ©üÂãï„Å´Áßª„ÇåÔºÅ', 'Ôºà„Ç®„É´„É¥„Ç£„É≥ÔºâÂÖ®ÊñπÂêë„Åã„Çâ', 'ÂêåÊôÇ„Å´Âè©„Åè„ÅûÔºÅ', 'Ôºà„É¢„Éº„Çº„ÇπÔºâ„ÇÑ„ÅÇ„Éº„Å£ÔºÅ']
</code></pre>

<p>I've tried the following code (it's as close as I could get):</p>

<pre><code>no_parentheses = []

for line in mylist:

    if '(' in line:
        line = re.sub('\(.*\)','', line)
        no_parentheses.append(line)

    else:
        no_parentheses.append(line)
</code></pre>

<p>But when I view the results, those pesky parentheses remain in my list <em>mockingly</em>.</p>

<p>Could anyone offer suggestions to resolve this issue?</p>

<p>Thanks again!</p>
",Multilingual Language Processing & Language Identification,removing parenthesis everything regex bit trouble code working basically transcript txt file japanese anime want remove everything spoken line japanese sentence order nlp experiment managed accomplish good bit cleaning stuck parenthesis majority element list start character name inside parenthesis e armin want remove regex code found online seem work snippet list working tried following code close could get view result pesky parenthesis remain list mockingly could anyone offer suggestion resolve issue thanks
How is the decoder conditioned in seq2seq learning?,"<p>I am trying to understand sequence-to-sequence learning with a RNN. What I understand so far, is that the output of the encoder is used to condition the decoder. </p>

<p>Yet, I have two sources which, in my opinion, do the conditioning differently and I would like to know which way is valid (may be both) or if I miss something.</p>

<p><strong>Source: Neural Network Methods for Natural Language Processing by Yoav Goldberg</strong></p>

<p>As far as I understand the author, the decoder operates in every step with a state vector AND the decoder output AND the next part of the sequence.
Thus, the state vector of the decoder is seperated from the result of the encoder.</p>

<p><a href=""https://i.sstatic.net/Ifoz5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ifoz5.png"" alt=""enter image description here""></a></p>

<p><strong>Source: A ten-minute introduction to sequence-to-sequence learning in Keras by Francois Chollet</strong></p>

<p>As far as I understand the author and the source, the decoder is provided with the encoder state as intial state.
Thus, the state vector of the decoder is the output of the decoder. The decoder steps only depend on the encoder output through the state vector.</p>

<p><a href=""https://i.sstatic.net/cGqc7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cGqc7.png"" alt=""enter image description here""></a></p>
",Multilingual Language Processing & Language Identification,decoder conditioned seq seq learning trying understand sequence sequence learning rnn understand far output encoder used condition decoder yet two source opinion conditioning differently would like know way valid may miss something source neural network method natural language processing yoav goldberg far understand author decoder operates every step state vector decoder output next part sequence thus state vector decoder seperated result encoder source ten minute introduction sequence sequence learning kera francois chollet far understand author source decoder provided encoder state intial state thus state vector decoder output decoder decoder step depend encoder output state vector
Getting wrong answer byy langdetect.detect,"<p>I am using both Nltk and Scikit Learn to do some text processing. I have a data set containing of sentences that some of them has explained the situation in French and English(French part is duplicated)  which I want to delete french part. Following in one of my sentence:</p>

<p>""quipage de    Global Express     en provenance deTokyo  Japon  vers Dorval      a d   effectuer une remise des gaz sur la piste           cause d un probl  me de volets   Il fut autoris      se poser sur la piste     Les services d urgence n ont pas   t   demand  s     appareil s est pos   sans encombre   D  lai d environ   minutes sur l exploitation     The crew of    Global Express     from Tokyo  Japan  to Dorval      had to pull up on Rwy   at   because of a flap problem  It was cleared to land on Rwy    Emergency services were not requested  The aircraft touched down without incident  Delay of about   minutes to operations         Regional Report of  m d y        with   record s ""</p>

<p>I want to remove all words that are in French. I have tried following code so far but the result is not good enough.</p>

<pre><code>x=sentence
x=x.split()
import langdetect      
from langdetect import detect 
for word in x:
lang=langdetect.detect(word)
if lang=='fr':
    print(word)
    x.remove(word)
</code></pre>

<p>the following is my output:</p>

<pre><code>l
un
sur
une
oiseaux
avoir
un
le
du
un
est
</code></pre>

<p>Is this a good approach? how I can improve its performance in order to reach better results. </p>
",Multilingual Language Processing & Language Identification,getting wrong answer byy langdetect detect using nltk scikit learn text processing data set containing sentence ha explained situation french english french part duplicated want delete french part following one sentence quipage de global express en provenance detokyo japon vers dorval effectuer remise de gaz sur la piste cause un probl de volets il fut autoris se poser sur la piste le service urgence n ont pa demand appareil est po sans encombre lai environ minute sur l exploitation crew global express tokyo japan dorval pull rwy flap problem wa cleared land rwy emergency service requested aircraft touched without incident delay minute operation regional report record want remove word french tried following code far result good enough following output good approach improve performance order reach better result
Anyone know of any real systems using Computational Semantics with Lambda Calculus?,"<p>I was wondering if Computational Semantics is actually used in any real-world system?  (Simple examples <a href=""http://courses.washington.edu/ling571/ling571_fall_2010/slides/compsem_lambda.pdf"" rel=""nofollow"">here</a> and <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.9969&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">here</a>).  I would like to see how an actual system works. </p>

<blockquote>
  <p>It seems like there are a bunch of issues with actually using Computational Semantics in any real world system:</p>
  
  <ul>
  <li>It seems just labeling sentences with part-of-speech tags is error prone.  </li>
  <li>But you also need a reliable parse tree which is error prone and there can be many valid trees for one sentence.  </li>
  <li>Finding what pronouns are referring to what entities is error prone.  </li>
  <li>Word disambiguation is also another source of errors and multiple meanings could be valid in the same context.</li>
  <li>Any context-free-grammar of English I can find seems to be incomplete.</li>
  </ul>
  
  <p>Finally, after all these sources of error are dodged, we can finally convert the sentence to FOL with Computation Semantics!</p>
</blockquote>

<p>Also, I can't seem to figure out how to deal with prepositions in Computation Semantics.</p>

<p>Is this really just an academic exercise or is Computational Semantics actually useful?</p>
",Multilingual Language Processing & Language Identification,anyone know real system using computational semantics lambda calculus wa wondering computational semantics actually used real world system simple example would like see actual system work seems like bunch issue actually using computational semantics real world system seems labeling sentence part speech tag error prone also need reliable parse tree error prone many valid tree one sentence finding pronoun referring entity error prone word disambiguation also another source error multiple meaning could valid context context free grammar english find seems incomplete finally source error dodged finally convert sentence fol computation semantics also seem figure deal preposition computation semantics really academic exercise computational semantics actually useful
Force Stanford CoreNLP Parser to Prioritize &#39;S&#39; Label at Root Level,"<p>Greetings NLP Experts,</p>
<p>I am using the Stanford CoreNLP software package to produce constituency parses, using the most recent version (3.9.2) of the English language models JAR, downloaded from the <a href=""https://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">CoreNLP Download page</a>. I access the parser via the Python interface from the NLTK module nltk.parse.corenlp. Here is a snippet from the top of my main module:</p>
<pre><code>import nltk
from nltk.tree import ParentedTree
from nltk.parse.corenlp import CoreNLPParser

parser = CoreNLPParser(url='http://localhost:9000')
</code></pre>
<p>I also fire up the server using the following (fairly generic) call from the terminal:</p>
<pre><code>java -mx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer
-annotators &quot;parse&quot; -port 9000 -timeout 30000
</code></pre>
<p>The parser that CoreNLP selects by default (when the full English model is available) is the Shift-Reduce (SR) parser, which <a href=""https://stackoverflow.com/questions/29241123/pcfg-vs-sr-parser"">is sometimes claimed</a> to be both more accurate and faster than the CoreNLP PCFG parser. Impressionistically, I can corroborate that with my own experience, where I deal almost exclusively with Wikipedia text.</p>
<p>However, I have noticed that often the parser will erroneously opt for parsing what is in fact a complete sentence (i.e., a finite, matrix clause) as a subsentential constituent instead, often an <code>NP</code>. In other words, the parser should be outputting an <code>S</code> label at root level <code>(ROOT (S ...))</code>, but something in the complexity of the sentence's syntax pushes the parser to say a sentence is not a sentence <code>(ROOT (NP ...))</code>, etc.</p>
<p>The parses for such problem sentences also always contain another (usually glaring) error further down in the tree. Below are a few examples. I'll just paste in the top few levels of each tree to save space. Each is a perfectly acceptable English sentence, and so the parses should all begin <code>(ROOT (S ...))</code>. However, in each case some other label takes the place of <code>S</code>, and the rest of the tree is garbled.</p>
<blockquote>
<p><strong>NP:</strong> An estimated 22‚Äì189 million school days are missed annually due to a cold. <code>(ROOT (NP (NP An estimated 22) (: --) (S 189 million school days are missed annually due to a cold) (. .)))</code></p>
<p><strong>FRAG:</strong> More than one-third of people who saw a doctor received an antibiotic prescription, which has implications for antibiotic resistance. <code>(ROOT (FRAG (NP (NP More than one-third) (PP of people who saw a doctor received an antibiotic prescription, which has implications for antibiotic resistance)) (. .)))</code></p>
<p><strong>UCP:</strong> Coffee is a brewed drink prepared from roasted coffee beans, the seeds of berries from certain Coffea species. <code>(ROOT (UCP (S Coffee is a brewed drink prepared from roasted coffee beans) (, ,) (NP the seeds of berries from certain Coffea species) (. .)))</code></p>
</blockquote>
<p>At long last, here is my question, which I trust the above evidence proves is a useful one: <strong>Given that my data contains a negligible number of fragments or otherwise ill-formed sentences, how can I impose a high-level constraint on the CoreNLP parser such that its algorithm gives priority to assigning an <code>S</code> node directly below <code>ROOT</code>?</strong></p>
<p>I am curious to see whether imposing such a constraint when processing data (that one knows to satisfy it) will also cure other myriad ills observed in the parses produced. From what I understand, the solution would not lie in specifying a <code>ParserAnnotations.ConstraintAnnotation</code>. Would it?</p>
",Multilingual Language Processing & Language Identification,force stanford corenlp parser prioritize label root level greeting nlp expert using stanford corenlp software package produce constituency par using recent version english language model jar downloaded corenlp download page access parser via python interface nltk module nltk parse corenlp snippet top main module also fire server using following fairly generic call terminal parser corenlp selects default full english model available shift reduce sr parser href sometimes claimed accurate faster corenlp pcfg parser impressionistically corroborate experience deal almost exclusively wikipedia text however noticed often parser opt parsing fact complete sentence e finite matrix clause subsentential constituent instead often word parser outputting label root level something complexity sentence syntax push parser say sentence sentence etc par problem sentence also always contain another usually glaring error tree example paste top level tree save space perfectly acceptable english sentence par begin however case label take place rest tree garbled np estimated million school day missed annually due cold frag one third people saw doctor received antibiotic prescription ha implication antibiotic resistance ucp coffee brewed drink prepared roasted coffee seed berry certain coffea specie long last question trust evidence prof useful one given data contains number fragment otherwise ill formed sentence impose high level constraint corenlp parser algorithm give priority assigning node directly curious see whether imposing constraint processing data one know satisfy also cure myriad ill observed par produced understand solution would lie specifying would
regex to match only English alphabet letter text,"<p>I was trying to write regular expression that <strong>only matches text</strong> consists of English alphabet text that are more than 3 letters in python. I tried:</p>

<pre><code> regex = r'[a-z][a-z][a-z]+'
</code></pre>

<p>but it can't filter out strings like </p>

<pre><code> how@@
</code></pre>

<p>Any ideas would be appreciated:)</p>
",Multilingual Language Processing & Language Identification,regex match english alphabet letter text wa trying write regular expression match text consists english alphabet text letter python tried filter string like idea would appreciated
How to detect numerical value of a text?,"<p>We have data for survey question (e.g. rate us between 1-5) that's supposed to be numerical. However, we find that the response also includes</p>

<ul>
<li>üëç repeated 5 times </li>
<li>‚ù§Ô∏è repeated 4 times </li>
<li>Great! </li>
<li>four </li>
<li>3 and a half</li>
</ul>

<p>I'd like a way to turn the user response into a numerical value. e.g. the text above should translate into 5, 4, 5, 4, 3.5 respectively. Obviously this won't work 100% of the time so I'm looking for an optimal solution (perhaps a text analysis approach) that gets me over 80%. </p>
",Multilingual Language Processing & Language Identification,detect numerical value text data survey question e g rate u supposed numerical however find response also includes repeated time repeated time great four half like way turn user response numerical value e g text translate respectively obviously work time looking optimal solution perhaps text analysis approach get
How can I analyze pieces of text for positive or negative words?,"<p>I'm looking for some sort of module (preferably for python) that would allow me to give that module a string about 200 characters long. The module should then return how many positive or negative words that string had. (e.g. love, like, enjoy vs. hate, dislike, bad)</p>

<p>I'd really like to avoid having to reinvent the wheel in natural language processing, so if there is anything you guys know of that would allow me to do what I described above, it'd be a huge time-saver if you could share.</p>

<p>Thanks for the help!</p>
",Multilingual Language Processing & Language Identification,analyze piece text positive negative word looking sort module preferably python would allow give module string character long module return many positive negative word string e g love like enjoy v hate dislike bad really like avoid reinvent wheel natural language processing anything guy know would allow described huge time saver could share thanks help
How can I detect a verb order with Stanford CoreNLP Dependency Parser?,"<p>I have a RDF/Turtle Resource as below: </p>

<pre><code>@prefix factory: &lt;http://linkedfactory.iwu.fraunhofer.de/vocab#&gt; .
@prefix : &lt;http://linkedfactory.iwu.fraunhofer.de/data/&gt; .
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .
@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .

&lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory&gt; factory:contains &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/demofactory&gt; .

&lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU&gt; factory:contains &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/Rollex&gt; .

&lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim&gt; factory:contains &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab&gt; .

&lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab&gt; factory:contains &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/Aximus&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/BDM2000&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/Fliesspressen&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/GMX_Entgraten&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/GMX_Spanen1&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/GMX_Spanen2&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/GMX_Spanen3&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/GMX_Spanen4&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/HA100&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/Karobau&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/PRD40&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/PWZ&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/Querwalzen&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/E3-Sim/FoFab/Rollex&gt; .

&lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab&gt; factory:contains &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/BHKW&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/GLT&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/GMX&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/NSHV&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/NSHV-Buero&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/Rollex&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/SolarPlant&gt; .

&lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/BHKW&gt; factory:contains &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/BHKW/CoolingWater&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/BHKW/EmergencyCooling&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/BHKW/Generator&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/BHKW/HeatMeter&gt; , &lt;http://linkedfactory.iwu.fraunhofer.de/linkedfactory/IWU/FoFab/BHKW/HeatingWater&gt; .
</code></pre>

<p>I transform from a natural language to a Sparql request with constituency parser for the following situations</p>

<p>Question : What linkedfactory contains?</p>

<pre><code>PREFIX vocab: &lt;http://linkedfactory.iwu.fraunhofer.de/vocab#&gt;
select * where {
                       &lt;http://localhost:10080/linkedfactory/demofactory&gt;  vocab:contains ?o .
                        }
</code></pre>

<p>For the following question, I need to detect ""contains"" verb if it is coming before the noun ""linkedfactory"".</p>

<p>Question: What contains linkedfactory? or Can you give me which one contains linkedfactory?</p>

<pre><code>PREFIX vocab: &lt;http://linkedfactory.iwu.fraunhofer.de/vocab#&gt;
select * where {
                    ?s    vocab:contains &lt;http://localhost:10080/linkedfactory/demofactory&gt;  .
                        }
</code></pre>

<p>How can I handle with this situation? (Note: It is used rdflib-python to create a Sparql and to parse a sentence, I use Stanford CoreNLP-Python)</p>

<p>Thanks in advance,</p>
",Multilingual Language Processing & Language Identification,detect verb order stanford corenlp dependency parser rdf turtle resource transform natural language sparql request constituency parser following situation question linkedfactory contains following question need detect contains verb coming noun linkedfactory question contains linkedfactory give one contains linkedfactory handle situation note used rdflib python create sparql parse sentence use stanford corenlp python thanks advance
Natural Language Processing and Keyword finding for Java,"<p>Assume we have a forum where a user can create topics and discuss about things. It is in my interest that the forum is serious and does not contain exchange of illegal things or organization of illegal meetings e.g. drug trade or child pornography. My application is written in Java, is there a Framework or an WebApi that can find and identify words or semantic meanings of the things user wrote to check  there are no illegal things going on? </p>
",Multilingual Language Processing & Language Identification,natural language processing keyword finding java assume forum user create topic discus thing interest forum serious doe contain exchange illegal thing organization illegal meeting e g drug trade child pornography application written java framework webapi find identify word semantic meaning thing user wrote check illegal thing going
Why &quot;add one smoothing&quot; in language model does not count the &lt;/s&gt; in denominator,"<p>English is not my native language , Sorry for any grammatical mistakes.</p>

<p>I saw many documents for add one smoothing in language model, and I still very confused about the variable V in the formula:</p>

<pre><code>P (wi |w_i-1 ) = c(w_i-1 ,wi )+1  / c(w_i-1 )+V
</code></pre>

<p>as for this example corpus and I use bigram</p>

<pre><code>&lt;s&gt; John read Moby Dick &lt;/s&gt;
&lt;s&gt; Mary read a different book &lt;/s&gt;
&lt;s&gt; She read a book by Cher &lt;/s&gt;
</code></pre>

<p>if i want to calculate any P(wi | w_i-1) . The V will be 11
because the count of combination of [ w_i-1 , w ] is 11
. But I found it does not include the case   [w_i-1 , ""&lt;""/s"">""]  (or the V will be 12)
Why we do not need to include this case ? Isn't it the case that w_i-1 is in the end of an article or sentence ?</p>
",Multilingual Language Processing & Language Identification,add one smoothing language model doe count denominator english native language sorry grammatical mistake saw many document add one smoothing language model still confused variable v formula example corpus use bigram want calculate p wi w v count combination w w found doe include case w v need include case case w end article sentence
Error while extracting lemma from a sentence using stanford core NLP for .NET,"<p>I'm trying <a href=""https://www.nuget.org/packages?q=Stanford.NLP"" rel=""nofollow noreferrer"">stanford nlp</a> in .NET . I want to extract Lemma from given sentences. When I execute this code I'm running into the following error. 
*Unable to open <em>""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as class path, filename or URL</em>. </p>

<blockquote>
  <p>Can anyone please tell me where will I get that file and where should
  I place that file.?</p>
</blockquote>

<p>I have added the sample code..</p>

<pre><code>               var text = ""what is humidity"";
                var doc = new edu.stanford.nlp.simple.Document(text);
                var sentences = doc.sentences().toArray();

                foreach (edu.stanford.nlp.simple.Sentence sent in sentences)
                {
                    var temp = sent.lemma(2); //getting error here
                }
</code></pre>
",Multilingual Language Processing & Language Identification,error extracting lemma sentence using stanford core nlp net trying stanford nlp net want extract lemma given sentence execute code running following error unable open edu stanford nlp model po tagger english left word english left word distsim tagger class path filename url anyone please tell get file place file added sample code
How to change the end of the text content of an lxml etree.Element in Python3?,"<p>I am currently working on a natural language processing project in Python. We have html texts of scientific articles, which we parse with Pythons lxml.etree, and store as Elements and ElementTrees. Some of our html texts are actually converted pfs using pdf2htmlEX (<a href=""https://github.com/coolwanglu/pdf2htmlEX"" rel=""nofollow noreferrer"">https://github.com/coolwanglu/pdf2htmlEX</a>). This converter turns every row of the article into a div, in a way that strips spaces from endlines.</p>

<p>So this html:</p>

<pre><code>&lt;div&gt;This is a&lt;/div&gt;
&lt;div&gt;sample text.&lt;/div&gt;
</code></pre>

<p>... gets parsed like this:</p>

<blockquote>
  <p>This is asample text.</p>
</blockquote>

<p>Notice the absence of whitsepace between 'a' and 'sample'.</p>

<p>I need it to look like this:</p>

<blockquote>
  <p>This is a sample text.</p>
</blockquote>

<p>I have to somehow add the whitespace manually to the end of the line. Etree does have an Element.text attribute, and Element.text can be changed as any other strings could be. Element.text += ' ' results with adding whitespace to the end of the .text. However, .text will only go until the first child tag. Consider the following code:</p>

<pre><code>html = '&lt;div&gt;This is a &lt;strong&gt;sample&lt;/strong&gt; text.&lt;/div&gt;'
el = etree.fromstring(html)
print(el.text)
</code></pre>

<p>The printed result is:</p>

<blockquote>
  <p>'This is a '</p>
</blockquote>

<p>Sadly, Element does not have an attribute to reach the <strong>end</strong> of the text. If you want to use the whole text content of an Element, you need to use ''.join(el.itertext()). Sadly, for a couple of reasons, I need to change the elements text content directly. Storing the results of the itertext and using it in the rest of the code is <strong>not an option</strong> for me at this time.</p>

<p>To add some further challange, there are different variations of the same problem:</p>

<pre><code>html_1 = &lt;div&gt;This is a &lt;strong&gt;sample&lt;/strong&gt; text.&lt;/div&gt;
html_2 = &lt;div&gt;This is a sample &lt;strong&gt;text.&lt;/strong&gt;&lt;/div&gt;
</code></pre>

<p>These are different cases: html_1 has the endline in the div element, while html2 has it in the strong.</p>

<p>Is there a solution where I can add a space to the end of the string in an Element?</p>
",Multilingual Language Processing & Language Identification,change end text content lxml etree element python currently working natural language processing project python html text scientific article parse python lxml etree store element elementtrees html text actually converted pfs using pdf htmlex converter turn every row article div way strip space endlines html get parsed like asample text notice absence whitsepace sample need look like sample text somehow add whitespace manually end line etree doe element text attribute element text changed string could element text result adding whitespace end text however text go first child tag consider following code printed result sadly element doe attribute reach end text want use whole text content element need use join el itertext sadly couple reason need change element text content directly storing result itertext using rest code option time add challange different variation problem different case html ha endline div element html ha strong solution add space end string element
Is there any NLP tools for semantic parsing for languages other than English,"<p>I want to parse Malayalam(Indian Language) text corpora for developing a question answering system.Is there any NLP tools for semantic parsing for languages other than English.</p>
",Multilingual Language Processing & Language Identification,nlp tool semantic parsing language english want parse malayalam indian language text corpus developing question answering system nlp tool semantic parsing language english
Semantic representation of text,"<p>I am trying to expand my knowledge in natural language processing and I recently came across concept of semantic representation of text. </p>

<p>I understand its definition ""an abstract language in which meanings can be represented"", but what are some ways to create semantics representation of text? I can't find a satisfying answer for that. </p>

<p>Thank you for your answers. </p>
",Multilingual Language Processing & Language Identification,semantic representation text trying expand knowledge natural language processing recently came across concept semantic representation text understand definition abstract language meaning represented way create semantics representation text find satisfying answer thank answer
Stuck on cryptopals challenge 4 in Go,"<p>Problem: <a href=""http://www.cryptopals.com/sets/1/challenges/4"" rel=""nofollow noreferrer"">http://www.cryptopals.com/sets/1/challenges/4</a></p>

<p>I've previously completed this problem in C but I wanted to do a more generalised solution in Go (I just stopped checking strings for englishness when I reached one that matched my arbitrary goal in C, now I want the MOST english of all).</p>

<p>My solution works fine for the challenge 3 string, but when I try with challenge 4 I just get garbage out. More disconcertingly, I can't even see the correct string in the collection of strings generated by my XORs (I printed them all to a file). I've tried changing how I extract the strings from the text file (which is why I'm using a less standard method of getting the strings out) with no effect.</p>

<p>Challenge 3's string is ""1b37373331363f78151b7f2b783431333d78397828372d363c78373e783a393b3736"" which should output ""Cooking MC's like a pound of bacon"" which functions correctly. Challenge 4 is a file with 300 odd lines, only one of which is the correct one to decode. It should decrypt to ""Now that the party is jumping"", but I just get ""U+)Ex(unprintable)NSqhe/]PuSE7Nr;Rw;OUqeas"". I've been able to get a couple of different outputs but never the correct one.</p>

<pre><code> func main() {
    filebytes, err := ioutil.ReadFile(""4.txt"")
    if err != nil {
        log.Fatal(err)
    }
    filestring := string(filebytes)
    lines := strings.Split(filestring, ""\n"")
    bestGuess := challenge4.GuessFile(lines)
    fmt.Println(bestGuess)
}

func GuessFile(lines []string) string {
    guessArray := make([]string, len(lines))
    for i, line := range lines {
        bytes, err := hex.DecodeString(line)
        if err != nil {
            log.Fatal(err)
        }
        guessArray[i] = challenge3.GuessString(bytes)
    }
    return utilities.MostEnglish(guessArray)
}


func GuessString(b []byte) string {
    guessArray := make([]string, 256)
    for i := 0; i &lt; 256; i++ {
        guessArray[i] = string(utilities.SbXor(b, byte(i)))
    }
    return utilities.MostEnglish(guessArray)
}


    // MostEnglish takes a slice of strings and returns the string most likely to
// be an English sentence.
func MostEnglish(s []string) string {
    var maxVal uint64
    var maxStr string
    for _, line := range s {

        val := EnglishFreq(line)
        if val != 0 {

            if val &gt; maxVal {
                maxVal, maxStr = val, line
            }
        }
    }
    return maxStr
}

// EnglishFreq takes a string and returns the  value representing
// the likelihood the string is a valid English sentence based on word frequency
    func EnglishFreq(s string) uint64 {
    var total uint64
    alphaFreq := map[rune]uint64{
        'A': 816,
        'B': 149,
        'C': 278,
        'D': 425,
        'E': 1270,
        'F': 222,
        'G': 201,
        'H': 609,
        'I': 696,
        'J': 15,
        'K': 77,
        'L': 402,
        'M': 240,
        'N': 674,
        'O': 750,
        'P': 192,
        'Q': 9,
        'R': 598,
        'S': 632,
        'T': 905,
        'U': 275,
        'V': 236,
        'W': 20,
        'X': 15,
        'Y': 197,
        'Z': 7,
    }

    for _, char := range s {
        if !unicode.IsPrint(char) {
            return 0
        }
        if val, ok := alphaFreq[unicode.ToUpper(char)]; ok {
            total += val
        }
    }
    return total
}
//SbXor does a single byte xor against a provided byte array
func SbXor(arr []byte, b byte) []byte {
    for i := range arr {
        arr[i] ^= b
    }
    return arr
}
</code></pre>
",Multilingual Language Processing & Language Identification,stuck cryptopals challenge go problem previously completed problem c wanted generalised solution go stopped checking string englishness reached one matched arbitrary goal c want english solution work fine challenge string try challenge get garbage disconcertingly even see correct string collection string generated xors printed file tried changing extract string text file using le standard method getting string effect challenge string b f b f b c e b output cooking mc like pound bacon function correctly challenge file odd line one correct one decode decrypt party jumping get u ex unprintable nsqhe puse nr rw ouqeas able get couple different output never correct one
Is it possible using NLP? Natural Language processing,"<p>I have a set of Project Names, a set of keywords and a set of paragraphs. </p>

<p>Now my task is, to check whether keywords match any project names , and keywords match any word in any paragraph. </p>

<p>If any set of paragraphs are matched with a keyword and any project matched with same keyword, then I have to assign these paragraphs to that project. </p>

<p>I have been using String Regex for this. But can this be implemented using Natural Language Processing concepts. </p>

<p>If yes... Please let me know how can it be implemented. It would be very helpful for me. </p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,possible using nlp natural language processing set project name set keywords set paragraph task check whether keywords match project name keywords match word paragraph set paragraph matched keyword project matched keyword assign paragraph project using string regex implemented using natural language processing concept yes please let know implemented would helpful thanks advance
Berkeley Parser English - different results online vs offline,"<p>I am using the Berkeley parser for English, from <a href=""https://github.com/slavpetrov/berkeleyparser"" rel=""nofollow noreferrer"">here</a>. This seems to give me different results than their online <a href=""http://tomato.banatao.berkeley.edu:8080/parser/parser.html"" rel=""nofollow noreferrer"">demo</a>.</p>
<p>For ex, consider the sentence:</p>
<p><code>I am a platinum member. I want someone to fix this for me immediately.</code></p>
<p>Running the parser on my machine gives me this parse tree (which is incorrect):
<a href=""https://i.sstatic.net/KAMNT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KAMNT.png"" alt=""enter image description here"" /></a></p>
<p>While running it through the demo I get this (which is correct):
<a href=""https://i.sstatic.net/BVqkd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BVqkd.png"" alt=""enter image description here"" /></a></p>
<p>Can someone point out why is there a difference here? The parser download has one english grammar file only, <code>eng_sm6.gr</code>, so it cannot be a wrong grammar file. Can this be a problem with a commandline switch? This is what I am running:
<code>java -jar BerkeleyParser-1.7.jar -gr eng_sm6.gr</code></p>
<p>Any help appreciated, thanks!</p>
",Multilingual Language Processing & Language Identification,berkeley parser english different result online v offline using berkeley parser english seems give different result online demo ex consider sentence running parser machine give parse tree incorrect running demo get correct someone point difference parser download ha one english grammar file wrong grammar file problem commandline switch running help appreciated thanks
"Gensim Keywords, how to load a german model?","<p>I'm try to get started with the gensim library. My goal is pretty simple. I want to use the keywords extraction provided by gensim on a german text. Unfortunately, i'm failing hard.</p>

<p>Gensim comes with a keywords extraction build in, it is build on TextRank. While the results look good on english text, it seems not to work on german. I simple installed gensim via pypi and used it out of the box. Well such AI Products are usually driven by a model. My guess is that gensim comes with a english model. A word2vec model for german is available on a <a href=""https://github.com/devmount/GermanWordEmbeddings"" rel=""nofollow noreferrer"">github page</a>.</p>

<p>But here i'm stuck, i can't find a way how the summarization module of gensim, which provides the <a href=""https://radimrehurek.com/gensim/summarization/keywords.html"" rel=""nofollow noreferrer"">keywords function</a> i'm looking for, can work with a external model.</p>

<p>So the basic question is, how do i load the german model and get keywords from german text?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,gensim keywords load german model try get started gensim library goal pretty simple want use keywords extraction provided gensim german text unfortunately failing hard gensim come keywords extraction build build textrank result look good english text seems work german simple installed gensim via pypi used box well ai product usually driven model guess gensim come english model word vec model german available github page stuck find way summarization module gensim provides keywords function looking work external model basic question load german model get keywords german text thanks
natural language processing: text corpus format for word2vec,"<p>I found a tutorial that uses word2vec on a large Wikipedia DataSet
<a href=""http://danielfrg.github.io/blog/2013/09/21/word2vec-yhat/"" rel=""nofollow"">http://danielfrg.github.io/blog/2013/09/21/word2vec-yhat/</a><br>
I would like to build a yhat rest API similar to the one he Daniel has demonstrated in his tutorial.</p>

<p>Today I put together some Spanish newspaper articles I wish to analyze. The web-site I retrieved my data formats its articles very regularly, so I have 1000 articles stored as strings, e.g. </p>

<pre><code>""Otros se dan a conocer por la simpleza, como¬†Sonya Cort√©s, 
quien expres√≥ que¬†atesora compartir en familia y gozar de salud.   
En el ambiente del reggaeton, Khriz, del d√∫o √Ångel &amp; Khriz, 
aprovechar√° para estrenar su nueva piscina ya que por su agenda 
de trabajo no ha podido darse un chapuz√≥n todav√≠a. Mientras,¬†
Daddy Yankee se tomar√° un descanso con la familia luego de una larga gira.""
</code></pre>

<p>I am comfortable with Python and was hoping to use the python wrapper listed in the tutorial:
<a href=""https://github.com/danielfrg/word2vec"" rel=""nofollow"">https://github.com/danielfrg/word2vec</a></p>

<p>How do I load my corpus into word2vec?  Right now I have an array of strings.</p>

<p>At the moment my corpus fits into memory.  Is word2vec still the right tool?</p>
",Multilingual Language Processing & Language Identification,natural language processing text corpus format word vec found tutorial us word vec large wikipedia dataset would like build yhat rest api similar one daniel ha demonstrated tutorial today put together spanish newspaper article wish analyze web site retrieved data format article regularly article stored string e g comfortable python wa hoping use python wrapper listed tutorial load corpus word vec right array string moment corpus fit memory word vec still right tool
Setting path in heidelTime property file to use Stanford POS Tagger for German?,"<p>I am trying to detect temporal information in German text. I tried using the Stanford CoreNLP pipeline as it would be very helpful to make use of dependency parse information in later stages (after temporal tagging) but to my understanding there is no way of setting the integrated temporal tagger of CoreNLP to German. Am I right about this or is there, in fact, a way to  do this.</p>

<p>Now I'm trying to use HeidelTime to retrieve tamporal tags seperately. I want to use the Stanford POS tagger with it. In the Heideltime config.props file, I am setting the path to the Stanford POS tagger like this (using windows): </p>

<pre><code>model_path = C:\\Users\\milu\\Documents\\stanford-postagger-full-2017-06-09\\stanford-postagger-full-2017-06-09\\models
# leave this unset if you do not need one (e.g., /home/jannik/stanford-postagger-full-2014-01-04/tagger.config)
config_path =   
</code></pre>

<p>This is the code I'm running on NetBeans, followed by the error I get. Is there something wrong with the way I am specifying the path to the POS tagger?</p>

<pre><code>public class RunHeideltimeInJava {


public static void main(String[] args) throws
        DocumentCreationTimeMissingException, ParseException {

    OutputType outtype = OutputType.XMI;
    POSTagger postagger = POSTagger.STANFORDPOSTAGGER;
    String conffile = ""C:\\Users\\milu\\Documents\\NetBeansProjects\\TimeTagging\\src\\config.props"";

    HeidelTimeStandalone hsNarratives = new HeidelTimeStandalone(Language.GERMAN,
            DocumentType.NARRATIVES, outtype, conffile, postagger);

    String narrativeText = ""Ich habe letztes Wochenende neue Schuhe gekauft."";

    String xmiNarrativeOutput = hsNarratives.process(narrativeText);
    System.err.println(""NARRATIVE*****"" + xmiNarrativeOutput);
    String dctString = ""2016-04-29"";
    DateFormat df = new SimpleDateFormat(""yyyy-MM-dd"");
    Date dct = df.parse(dctString);
 }
}
</code></pre>

<p>Output:</p>

<pre><code>run:
Aug 25, 2017 9:54:31 AM de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone initialize
INFORMATION: HeidelTimeStandalone initialized with language german
Aug 25, 2017 9:54:31 AM de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone readConfigFile
INFORMATION: trying to read in file C:\Users\milue\Documents\NetBeansProjects\TimeTagging\src\config.props
Aug 25, 2017 9:54:33 AM de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone initialize
INFO: HeidelTime initialized
Aug 25, 2017 9:54:33 AM de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone initialize
INFO: JCas factory initialized
Aug 25, 2017 9:54:33 AM de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone process
INFO: Processing started
Exception in thread ""main"" java.lang.NoClassDefFoundError: edu/stanford/nlp/tagger/maxent/TaggerConfig
    at de.unihd.dbs.heideltime.standalone.components.impl.StanfordPOSTaggerWrapper.&lt;init&gt;(StanfordPOSTaggerWrapper.java:12)
    at de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone.establishPartOfSpeechInformation(HeidelTimeStandalone.java:391)
    at de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone.establishHeidelTimePreconditions(HeidelTimeStandalone.java:332)
    at de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone.process(HeidelTimeStandalone.java:516)
    at de.unihd.dbs.heideltime.standalone.HeidelTimeStandalone.process(HeidelTimeStandalone.java:449)
    at RunHeideltimeInJava.main(RunHeideltimeInJava.java:29)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.tagger.maxent.TaggerConfig
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 6 more
C:\Users\milu\AppData\Local\NetBeans\Cache\8.2\executor-snippets\run.xml:53: Java returned: 1
BUILD FAILED (total time: 2 seconds)
</code></pre>
",Multilingual Language Processing & Language Identification,setting path heideltime property file use stanford po tagger german trying detect temporal information german text tried using stanford corenlp pipeline would helpful make use dependency parse information later stage temporal tagging understanding way setting integrated temporal tagger corenlp german right fact way trying use heideltime retrieve tamporal tag seperately want use stanford po tagger heideltime config prop file setting path stanford po tagger like using window code running netbeans followed error get something wrong way specifying path po tagger output
The sum of all bigrams that start with a particular word must be equal to the unigram count for that word?,"<p>In <a href=""https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf"" rel=""nofollow noreferrer"">this</a> chapter (at the end of page 4) of a stanford book on natural language processing it says:</p>

<blockquote>
  <p>The sum of all bigrams that start with a particular word must be equal to the unigram count for that word.</p>
</blockquote>

<p>The book leaves why this is the case as an exercise to the reader but I don't understand why this would be true.<br />
For example in the following corpus:</p>

<ul>
<li>A ball is red.</li>
<li>All balls are red.</li>
</ul>

<p>As far as I understand the word red would occur twice in unigrams however not appear at all as the first element of a bi-gram?</p>

<p>Am I missing something like that the end of sentence is used as a token? (but then the problem reemerges for trigrams?)</p>
",Multilingual Language Processing & Language Identification,sum bigram start particular word must equal unigram count word chapter end page stanford book natural language processing say sum bigram start particular word must equal unigram count word book leaf case exercise reader understand would true example following corpus ball red ball red far understand word red would occur twice unigrams however appear first element bi gram missing something like end sentence used token problem reemerges trigram
Why no programming in English? What is the difference between natural languages and programming languages?,"<p>What is the key difference between natural languages (such as English and French) and programming languages like C++ and Perl?</p>

<p>I am familiar with the ambiguity problem, but can't it be solved using an interactive compiler or using a subset of the natural language using a strict grammar but all the time still retaining the essence of the language?</p>

<p>Another issue is context. But lawyers have ways to solve this issue. (This question is not about reducing the programming complexity, it's simply about concise reasons and roadblock in using natural languages for instructing computer.)</p>

<p>Is there any other significant problem besides these two? Or do these two have greater consequences than I mentioned above? Is the interactive solution and lawyers language technically not feasible for programming?</p>
",Multilingual Language Processing & Language Identification,programming english difference natural language programming language key difference natural language english french programming language like c perl familiar ambiguity problem solved using interactive compiler using subset natural language using strict grammar time still retaining essence language another issue context lawyer way solve issue question reducing programming complexity simply concise reason roadblock using natural language instructing computer significant problem besides two two greater consequence mentioned interactive solution lawyer language technically feasible programming
How to remove English words from a file containing Dari words?,"<p>How to find English words and remove them from the file which contains Dari words? I tried this code, but I do not know how to improve it.</p>

<pre><code>inp = open('Dari.pos', 'r')
out = open('DariNER.txt', 'w')

for line in iter(inp):
   ------------?
   out.write(word)
inp.close()
out.close()
</code></pre>
",Multilingual Language Processing & Language Identification,remove english word file containing dari word find english word remove file contains dari word tried code know improve
Where can I find a list of English phrases?,"<p>I'm tasked with searching for the use of cliches and common phrases in text. The phrases are similar to the phrases you might see for the phrase puzzles on Wheel of Fortune. Here are a few examples:</p>

<ul>
<li>Easy Come Easy Go</li>
<li>Too Good To be True</li>
<li>Winning Isn't Everything</li>
</ul>

<p>I cannot find a list of phrases however. Does anybody know of such a list?</p>

<p>Seriously, even a list of all Wheel of Fortune solutions would suffice.</p>
",Multilingual Language Processing & Language Identification,find list english phrase searching use cliche common phrase text phrase similar phrase might see phrase puzzle wheel fortune example easy come easy go good true winning everything find list phrase however doe anybody know list seriously even list wheel fortune solution would suffice
Find minimum Levenshtein Distance between one word and an array of thousands,"<p>So my users wrote their addresses in a registration form, but a lot of them have typos. I have another list retrieved from the city records with the correct spelling of those addresses. So let's say I have ""Brooklny"" typed by them and I have the list of correct names: Brooklyn, Manhattan, Bronx, Staten Island, Queens (this is an example, the actual addresses are in Spanish and refer to neighborhoodS in Mexico City).</p>

<p>I want to find the edit distance between Brooklyn and each of the borough names and then find the word to whick Brooklyn has the minimum edit distance.</p>

<p>So edit distance between: Brooklny-Brooklyn is 2, Brooklny-Bronx is 4 and so on. The minimum of course is 2 with Brooklyn. </p>

<p>Imagine that I have Brooklny in cell A1 and Brooklyn, Manhattan, Bronx, Staten Island, Queens each in a cell from B1:B6</p>

<p>Im doing this in VBA for a user defined function in Excel and so far I have this code but it doesnt work.</p>

<pre><code>Function Minl(ByVal string1 As String, ByVal correctos As Range) As Variant

Dim distancias(3) As Integer
Dim i, minimo As Integer
i = 0
For Each c In correctos.Cells
    distancias(i) = Levenshtein(string1, c.Value)
    i = i + 1
Next c

Minl = Minrange(distancias)

End Function

Function Levenshtein(ByVal string1 As String, ByVal string2 As String) As Long

Dim i As Long, j As Long
Dim string1_length As Long
Dim string2_length As Long
Dim distance() As Long

string1_length = Len(string1)
string2_length = Len(string2)
ReDim distance(string1_length, string2_length)

For i = 0 To string1_length
distance(i, 0) = i
Next

For j = 0 To string2_length
    distance(0, j) = j
Next

For i = 1 To string1_length
    For j = 1 To string2_length
        If Asc(Mid$(string1, i, 1)) = Asc(Mid$(string2, j, 1)) Then
            distance(i, j) = distance(i - 1, j - 1)
        Else
            distance(i, j) = Application.WorksheetFunction.Min _
            (distance(i - 1, j) + 1, _
            distance(i, j - 1) + 1, _
            distance(i - 1, j - 1) + 1)
        End If
    Next
Next

Levenshtein = distance(string1_length, string2_length)

End Function

Function Minrange(ParamArray values() As Variant) As Variant
Dim minValue, Value As Variant
minValue = values(0)
For Each Value In values
   If Value &lt; minValue Then minValue = Value
Next
Minrange = minValue
End Function
</code></pre>

<p>I think the algorithm is right but I think I might be having trouble with the syntax. The levenshtein function works but im not sure about the other two.</p>
",Multilingual Language Processing & Language Identification,find minimum levenshtein distance one word array thousand user wrote address registration form lot typo another list retrieved city record correct spelling address let say brooklny typed list correct name brooklyn manhattan bronx staten island queen example actual address spanish refer neighborhood mexico city want find edit distance brooklyn borough name find word whick brooklyn ha minimum edit distance edit distance brooklny brooklyn brooklny bronx minimum course brooklyn imagine brooklny cell brooklyn manhattan bronx staten island queen cell b b im vba user defined function excel far code doesnt work think algorithm right think might trouble syntax levenshtein function work im sure two
nltk Arabic Text Output Disconnected,"<p>I've been trying to write a code for sentences splitting. And it worked very well in English and other left-to-right Latin-lettered languages. When I tried to do the same with Arabic, the text came up totally disconnected, like each letter individually. I'm not sure what the problem is.</p>

<p>My input text:</p>

<blockquote>
  <p>ÿπŸÜÿØŸÖÿß Ÿäÿ±ŸäÿØ ÿßŸÑÿπÿßŸÑŸÖ ÿ£ŸÜ Ÿäÿ™ŸÉŸÑŸëŸÖÿå ŸÅŸáŸà Ÿäÿ™ÿ≠ÿØŸëÿ´ ÿ®ŸÑÿ∫ÿ© ŸäŸàŸÜŸäŸÉŸàÿØ. ÿ≥ÿ¨ŸëŸÑ ÿßŸÑÿ¢ŸÜ ŸÑÿ≠ÿ∂Ÿàÿ± ÿßŸÑŸÖÿ§ÿ™ŸÖÿ± ÿßŸÑÿØŸàŸÑŸä ÿßŸÑÿπÿßÿ¥ÿ± ŸÑŸäŸàŸÜŸäŸÉŸàÿØÿå ÿßŸÑÿ∞Ÿä ÿ≥ŸäÿπŸÇÿØ ŸÅŸä ÿ¢ÿ∞ÿßÿ± ÿ®ŸÖÿØŸäŸÜÿ© ŸÖŸéÿßŸäŸêŸÜŸíÿ™Ÿíÿ≥ÿå ÿ£ŸÑŸÖÿßŸÜŸäÿß. Ÿà ÿ≥Ÿäÿ¨ŸÖÿπ ÿßŸÑŸÖÿ§ÿ™ŸÖÿ± ÿ®ŸäŸÜ ÿÆÿ®ÿ±ÿßÿ° ŸÖŸÜ ŸÉÿßŸÅÿ© ŸÇÿ∑ÿßÿπÿßÿ™ ÿßŸÑÿµŸÜÿßÿπÿ© ÿπŸÑŸâ ÿßŸÑÿ¥ÿ®ŸÉÿ© ÿßŸÑÿπÿßŸÑŸÖŸäÿ© ÿßŸÜÿ™ÿ±ŸÜŸäÿ™ ŸàŸäŸàŸÜŸäŸÉŸàÿØÿå ÿ≠Ÿäÿ´ ÿ≥ÿ™ÿ™ŸÖÿå ÿπŸÑŸâ ÿßŸÑÿµÿπŸäÿØŸäŸÜ ÿßŸÑÿØŸàŸÑŸä ŸàÿßŸÑŸÖÿ≠ŸÑŸä ÿπŸÑŸâ ÿ≠ÿØ ÿ≥Ÿàÿßÿ° ŸÖŸÜÿßŸÇÿ¥ÿ© ÿ≥ÿ®ŸÑ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸäŸàŸÜŸÉŸàÿØ ŸÅŸä ÿßŸÑŸÜÿ∏ŸÖ ÿßŸÑŸÇÿßÿ¶ŸÖÿ© ŸàŸÅŸäŸÖÿß ŸäÿÆÿµ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ®Ÿäÿ©ÿå ÿßŸÑÿÆÿ∑Ÿàÿ∑ÿå ÿ™ÿµŸÖŸäŸÖ ÿßŸÑŸÜÿµŸàÿµ ŸàÿßŸÑÿ≠Ÿàÿ≥ÿ®ÿ© ŸÖÿ™ÿπÿØÿØÿ© ÿßŸÑŸÑÿ∫ÿßÿ™.</p>
</blockquote>

<p>My code:</p>

<pre><code># -*- coding: utf-8 -*-

import nltk
from nltk import sent_tokenize

import codecs
import csv

sentences = codecs.open('SampleArabic.txt', 'r', 'utf-8-sig').read()

def split_sentences(sentences):
    with codecs.open('Output_AR.txt', 'w', encoding='utf-8') as writer:
        newcount = 0
        for sent in sent_tokenize(sentences):
            print(sent.encode('utf-8'))
            wr = csv.writer(writer,delimiter='\n')
            wr.writerow(str(sent))
            newcount = sentences.count(sentences)+newcount
        print(newcount)
    pass

split_sentences(sentences)
</code></pre>

<p>My first issue is that the console prints the text in code:</p>

<pre><code>b'\xd8\xb9\xd9\x86\xd8\xaf\xd9\x85\xd8\xa7 \xd9\x8a\xd8\xb1\xd9\x8a\xd8\xaf \xd8\xa7\xd9\x84\xd8\xb9\xd8\xa7\xd9\x84\xd9\x85 \xd8\xa3\xd9\x86 \xd9\x8a\xd8\xaa\xd9\x83\xd9\x84\xd9\x91\xd9\x85 \xe2\x80\xac \xd8\x8c \xd9\x81\xd9\x87\xd9\x88 \xd9\x8a\xd8\xaa\xd8\xad\xd8\xaf\xd9\x91\xd8\xab \xd8\xa8\xd9\x84\xd8\xba\xd8\xa9 \xd9\x8a\xd9\x88\xd9\x86\xd9\x8a\xd9\x83\xd9\x88\xd8\xaf.'
b'\xd8\xb3\xd8\xac\xd9\x91\xd9\x84 \xd8\xa7\xd9\x84\xd8\xa2\xd9\x86 \xd9\x84\xd8\xad\xd8\xb6\xd9\x88\xd8\xb1 \xd8\xa7\xd9\x84\xd9\x85\xd8\xa4\xd8\xaa\xd9\x85\xd8\xb1 \xd8\xa7\xd9\x84\xd8\xaf\xd9\x88\xd9\x84\xd9\x8a \xd8\xa7\xd9\x84\xd8\xb9\xd8\xa7\xd8\xb4\xd8\xb1 \xd9\x84\xd9\x8a\xd9\x88\xd9\x86\xd9\x8a\xd9\x83\xd9\x88\xd8\xaf\xd8\x8c \xd8\xa7\xd9\x84\xd8\xb0\xd9\x8a \xd8\xb3\xd9\x8a\xd8\xb9\xd9\x82\xd8\xaf \xd9\x81\xd9\x8a \xd8\xa2\xd8\xb0\xd8\xa7\xd8\xb1 \xd8\xa8\xd9\x85\xd8\xaf\xd9\x8a\xd9\x86\xd8\xa9 \xd9\x85\xd9\x8e\xd8\xa7\xd9\x8a\xd9\x90\xd9\x86\xd9\x92\xd8\xaa\xd9\x92\xd8\xb3\xd8\x8c \xd8\xa3\xd9\x84\xd9\x85\xd8\xa7\xd9\x86\xd9\x8a\xd8\xa7.'
b'\xd9\x88 \xd8\xb3\xd9\x8a\xd8\xac\xd9\x85\xd8\xb9 \xd8\xa7\xd9\x84\xd9\x85\xd8\xa4\xd8\xaa\xd9\x85\xd8\xb1 \xd8\xa8\xd9\x8a\xd9\x86 \xd8\xae\xd8\xa8\xd8\xb1\xd8\xa7\xd8\xa1 \xd9\x85\xd9\x86 \xd9\x83\xd8\xa7\xd9\x81\xd8\xa9 \xd9\x82\xd8\xb7\xd8\xa7\xd8\xb9\xd8\xa7\xd8\xaa \xd8\xa7\xd9\x84\xd8\xb5\xd9\x86\xd8\xa7\xd8\xb9\xd8\xa9 \xd8\xb9\xd9\x84\xd9\x89 \xd8\xa7\xd9\x84\xd8\xb4\xd8\xa8\xd9\x83\xd8\xa9 \xd8\xa7\xd9\x84\xd8\xb9\xd8\xa7\xd9\x84\xd9\x85\xd9\x8a\xd8\xa9 \xd8\xa7\xd9\x86\xd8\xaa\xd8\xb1\xd9\x86\xd9\x8a\xd8\xaa \xd9\x88\xd9\x8a\xd9\x88\xd9\x86\xd9\x8a\xd9\x83\xd9\x88\xd8\xaf\xd8\x8c \xd8\xad\xd9\x8a\xd8\xab \xd8\xb3\xd8\xaa\xd8\xaa\xd9\x85\xd8\x8c \xd8\xb9\xd9\x84\xd9\x89 \xd8\xa7\xd9\x84\xd8\xb5\xd8\xb9\xd9\x8a\xd8\xaf\xd9\x8a\xd9\x86 \xd8\xa7\xd9\x84\xd8\xaf\xd9\x88\xd9\x84\xd9\x8a \xd9\x88\xd8\xa7\xd9\x84\xd9\x85\xd8\xad\xd9\x84\xd9\x8a \xd8\xb9\xd9\x84\xd9\x89 \xd8\xad\xd8\xaf \xd8\xb3\xd9\x88\xd8\xa7\xd8\xa1 \xd9\x85\xd9\x86\xd8\xa7\xd9\x82\xd8\xb4\xd8\xa9 \xd8\xb3\xd8\xa8\xd9\x84 \xd8\xa7\xd8\xb3\xd8\xaa\xd8\xae\xd8\xaf\xd8\xa7\xd9\x85 \xd9\x8a\xd9\x88\xd9\x86\xd9\x83\xd9\x88\xd8\xaf \xd9\x81\xd9\x8a \xd8\xa7\xd9\x84\xd9\x86\xd8\xb8\xd9\x85 \xd8\xa7\xd9\x84\xd9\x82\xd8\xa7\xd8\xa6\xd9\x85\xd8\xa9 \xd9\x88\xd9\x81\xd9\x8a\xd9\x85\xd8\xa7 \xd9\x8a\xd8\xae\xd8\xb5 \xd8\xa7\xd9\x84\xd8\xaa\xd8\xb7\xd8\xa8\xd9\x8a\xd9\x82\xd8\xa7\xd8\xaa \xd8\xa7\xd9\x84\xd8\xad\xd8\xa7\xd8\xb3\xd9\x88\xd8\xa8\xd9\x8a\xd8\xa9\xd8\x8c \xd8\xa7\xd9\x84\xd8\xae\xd8\xb7\xd9\x88\xd8\xb7\xd8\x8c \xd8\xaa\xd8\xb5\xd9\x85\xd9\x8a\xd9\x85 \xd8\xa7\xd9\x84\xd9\x86\xd8\xb5\xd9\x88\xd8\xb5 \xd9\x88\xd8\xa7\xd9\x84\xd8\xad\xd9\x88\xd8\xb3\xd8\xa8\xd8\xa9 \xd9\x85\xd8\xaa\xd8\xb9\xd8\xaf\xd8\xaf\xd8\xa9 \xd8\xa7\xd9\x84\xd9\x84\xd8\xba\xd8\xa7\xd8\xaa.'
3
</code></pre>

<p>But I think it is the minor problem.</p>

<p>The main issue, as I mentioned before, is that the output text file has the text totally disconnected.</p>

<p>In Notepad it looks like this:
<a href=""https://i.sstatic.net/Fhmqh.png"" rel=""nofollow noreferrer""></a> <a href=""https://i.sstatic.net/Fhmqh.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/Fhmqh.png</a></p>

<p>And in NotePad++ it looks like this:
<a href=""https://i.sstatic.net/gcA6z.png"" rel=""nofollow noreferrer""></a> <a href=""https://i.sstatic.net/gcA6z.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/gcA6z.png</a></p>

<p>I'm using Python 3.4. And this is only my 2nd attempt with Python. So, I might need some extra details.</p>
",Multilingual Language Processing & Language Identification,nltk arabic text output disconnected trying write code sentence splitting worked well english left right latin lettered language tried arabic text came totally disconnected like letter individually sure problem input text code first issue console print text code think minor problem main issue mentioned output text file ha text totally disconnected notepad look like notepad look like using python nd attempt python might need extra detail
Does graphaware support Dutch nlp in neo4j?,"<p>I want to use the <a href=""https://github.com/graphaware/neo4j-nlp"" rel=""nofollow noreferrer"">graphaware nlp package</a> to automatically perform nlp feature extraction on Dutch texts in neo4j.</p>

<p>For this purpose I wanted to use <a href=""https://github.com/graphaware/neo4j-nlp-opennlp"" rel=""nofollow noreferrer"">OpenNLP</a> as it should have <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">support for Dutch</a>.
The installation worked well, and I can annotate English texts, but for Dutch texts, the following error is thrown:</p>

<pre><code>Neo.ClientError.Procedure.ProcedureCallFailed
Failed to invoke procedure `ga.nlp.annotate`: Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Unsupported language : nl
</code></pre>

<p>I called the opennlp package using</p>

<pre><code>MATCH (n:News)
CALL ga.nlp.annotate({text:n.text, id: n.uuid, textProcessor: ""com.graphaware.nlp.processor.opennlp.OpenNLPTextProcessor"", pipeline: ""tokenizer""}) YIELD result
MERGE (n)-[:HAS_ANNOTATED_TEXT]-&gt;(result)
RETURN n, result
</code></pre>

<p>So it sucessfully detects that the fragment is Dutch, but it can not annotate this.</p>

<p>As a solution I was trying to manually download <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">the dutch models</a>, but I don't know how to load these up and connect them in a pipeline. It also seems weird that they would not come as default.</p>
",Multilingual Language Processing & Language Identification,doe graphaware support dutch nlp neo j want use graphaware nlp package automatically perform nlp feature extraction dutch text neo j purpose wanted use opennlp support dutch installation worked well annotate english text dutch text following error thrown called opennlp package using sucessfully detects fragment dutch annotate solution wa trying manually download dutch model know load connect pipeline also seems weird would come default
Automatically determine the natural language of a website page given its URL,"<p>I'm looking for a way to automatically determine the natural language used by a website page, given its URL.</p>

<p>In Python, a function like:</p>

<pre><code>def LanguageUsed (url):
    #stuff
</code></pre>

<p>Which returns a language specifier (e.g. 'en' for English, 'jp' for Japanese, etc...)</p>

<p>Summary of Results:
I have a reasonable solution working in Python using <a href=""http://pypi.python.org/pypi/oice.langdet/1.0dev-r781"" rel=""noreferrer"">code from the PyPi for oice.langdet</a>.
It does a decent job in discriminating English vs. Non-English, which is all I require at the moment.  Note that you have to fetch the html using Python urllib.  Also, oice.langdet is GPL license.</p>

<p>For a more general solution using Trigrams in Python as others have suggested, see this <a href=""http://code.activestate.com/recipes/326576/"" rel=""noreferrer"">Python Cookbook Recipe from ActiveState</a>.</p>

<p>The Google Natural Language Detection API works very well (if not the best I've seen).  However, it is Javascript and their TOS forbids automating its use.</p>
",Multilingual Language Processing & Language Identification,automatically determine natural language website page given url looking way automatically determine natural language used website page given url python function like return language specifier e g en english jp japanese etc summary result reasonable solution working python using code pypi oice langdet doe decent job discriminating english v non english require moment note fetch html using python urllib also oice langdet gpl license general solution using trigram python others suggested see python cookbook recipe activestate google natural language detection api work well best seen however javascript tos forbids automating use
Expanding english contractions using regular expressions in python,"<p>I am trying to expand the English contractions like won't, 
i'll etc.
I have kept all the contractions with their abbreviations in the dictionary form inside a file.
ex:</p>

<pre><code>CONTRACTION_MAP ={""ain't"": ""is not"",""aren't"": ""are not"",""can't"": ""cannot"",""can't've"": ""cannot have"",""'cause"": ""because"",""could've"": ""could have"",""couldn't"": ""could not"",""couldn't've"": ""could not have""..........etc}
</code></pre>

<p>Below is the code to expand the contractions </p>

<pre><code>from contractions import CONTRACTION_MAP
import re
    def expand_contractions(sentence,contraction_map):
        contractions_pattern = re.compile('({})'.format('|'.join(contraction_map.keys())),flags=re.IGNORECASE|re.DOTALL)
        def expand_match(contraction):
             match=contraction.group(0)
             expanded_contraction = contraction_map.get(match)
             return expanded_contraction
        expanded_sentence = contractions_pattern.sub(expand_match,sentence)
        return expanded_sentence
</code></pre>

<p>My question is if I use can't've in my text, the can't've word is expanded as cannot've instead of cannot have.</p>

<pre><code>sentence = ""Paul can't've ice cream as he is suffering with cough""
print(expand_contractions(sentence,CONTRACTION_MAP))
output = &gt; Paul cannot've ice cream as he is suffering with cough
</code></pre>

<p>Can anyone help me in figuring out what changes need to be done in the code to get the expected output.</p>
",Multilingual Language Processing & Language Identification,expanding english contraction using regular expression python trying expand english contraction like etc kept contraction abbreviation dictionary form inside file ex code expand contraction question use text word expanded instead anyone help figuring change need done code get expected output
Good way to add terms to python pattern singularize,"<p>I am using python pattern to get the singular form of English nouns. </p>

<pre><code>    In [1]: from pattern.en import singularize
    In [2]: singularize('patterns')
    Out[2]: 'pattern'
    In [3]: singularize('gases')
    Out[3]: 'gase'
</code></pre>

<p>I am solving the problem in the second example by defining</p>

<pre><code>    def my_singularize(strn):
        '''
        Return the singular of a noun. Add special cases to correct pattern generic rules.
        '''
        exceptionDict = {'gases':'gas','spectra':'spectrum','cross':'cross','nuclei':'nucleus'}
        try:
            return exceptionDict[strn]
        except:
            return singularize(strn)
</code></pre>

<p>Is there a better way to do this, e.g. add to the rules of pattern, or make the <code>exceptionDict</code> somehow internal to pattern?</p>
",Multilingual Language Processing & Language Identification,good way add term python pattern singularize using python pattern get singular form english noun solving problem second example defining better way e g add rule pattern make somehow internal pattern
Best evaluation method for real-time machine translation?,"<p>I'm aware that there are many different methods like BLEU, NIST, METEOR etc. They all have their pros and cons, and their effectiveness differs from corpus to corpus. I'm interested in real-time translation, so that two people could have a conversation by typing out a couple sentences at a time and having it immediately translated.</p>

<p>What kind of corpus would this count as? Would the text be considered too short for proper evaluation by most conventional methods? Would the fact that the speaker is constantly switching make the context more difficult?</p>
",Multilingual Language Processing & Language Identification,best evaluation method real time machine translation aware many different method like bleu nist meteor etc pro con effectiveness differs corpus corpus interested real time translation two people could conversation typing couple sentence time immediately translated kind corpus would count would text considered short proper evaluation conventional method would fact speaker constantly switching make context difficult
How can I concatenate the lines of dialogue while doing Natural Language Processing on a book,"<p>I am working on a sentiment analysis project of a book. I am using nltk.vader.sentimentintensityanalyzer to record the sentiment polarity of paragraphs in the Harry Potter series. </p>

<p>To create paragraphs and remove the line breaks I did: </p>

<pre><code>text_file = open('HP1 Sorcerer of Stone.txt', 'r')
text = str(text_file.readlines())   
text.replace('\\n""', """").replace(""\'"", """").replace("" , "","""")
</code></pre>

<p>This breaks the book down into paragraphs. The problem arises when it comes to dialogue.</p>

<p>Dialogue has the same paragraph breaks in between each character's words </p>

<pre><code>' ""So?"" snapped Mrs. Dursley. ',
' ""Well, I just thought... maybe... it was something to do with... you 
know... her crowd."" ',
' Mrs. Dursley sipped her tea through pursed lips. Mr. Dursley wondered 
whether he dared tell her he\\d heard the name ""Potter."" He decided he 
didn\\t dare. Instead he said, as casually as he could, ""Their son -- 
he\\d be about Dudley\\s age now, wouldn\\t he?"" ',
' ""I suppose so,"" said Mrs. Dursley stiffly. ',
' ""What\\s his name again? Howard, isn\\t it?"" ',
' ""Harry. Nasty, common name, if you ask me."" ',
</code></pre>

<p>How can I edit my breakdown methods so dialogue stays together as one element? The dialogue as a whole will then be used as a single input into the intensity analyzer.</p>
",Multilingual Language Processing & Language Identification,concatenate line dialogue natural language processing book working sentiment analysis project book using nltk vader sentimentintensityanalyzer record sentiment polarity paragraph harry potter series create paragraph remove line break break book paragraph problem arises come dialogue dialogue ha paragraph break character word edit method dialogue stay together one element dialogue whole used single input intensity analyzer
How to use SyntaxNet Output with another RNN algorithm,"<p>currently I‚Äôm building an NLU utilizing ‚ÄãArabic Language I‚Äôm using SyntaxNet, and I will use the RNN as an algorithm of machine learning for text classification and chatbot.</p>

<p>My Question is <strong>how can I link between the output of SyntaxNet and RNN ??</strong></p>
",Multilingual Language Processing & Language Identification,use syntaxnet output another rnn algorithm currently building nlu utilizing arabic language using syntaxnet use rnn algorithm machine learning text classification chatbot question link output syntaxnet rnn
"NLTK Lemmatizer, Extract meaningful words","<p>Currently, I am going to create a machine learning based code that automatically maps categories.</p>

<p>I am going to do natural language processing before that.</p>

<p>There are several words list.</p>

<pre><code>      sent ='The laughs you two heard were triggered 
             by memories of his own high j-flying 
             moist moisture moisturize moisturizing '.lower().split()
</code></pre>

<p>I made the following code.
I referenced this url. <a href=""https://stackoverflow.com/questions/35870282/nltk-lemmatizer-and-pos-tag"">NLTK: lemmatizer and pos_tag</a></p>

<pre><code>from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
def lemmatize_all(sentence):
    wnl = WordNetLemmatizer()
    for word, tag in pos_tag(word_tokenize(sentence)):
        if tag.startswith(""NN""):
            yield wnl.lemmatize(word, pos='n')
        elif tag.startswith('VB'):
            yield wnl.lemmatize(word, pos='v')
        elif tag.startswith('JJ'):
            yield wnl.lemmatize(word, pos='a')



words = ' '.join(lemmatize_all(' '.join(sent)))
</code></pre>

<p>The resulting values are shown below.</p>

<pre><code>laugh heard be trigger memory own high j-flying moist moisture moisturize moisturizing
</code></pre>

<p>I am satisfied with the following results.</p>

<pre><code>laughs -&gt; laugh 
were -&gt; be
triggered -&gt; trigger 
memories -&gt; memory 
moist -&gt; moist 
</code></pre>

<p>However, the following values are not satisfied.</p>

<pre><code>heard -&gt; heard 
j-flying -&gt; j-flying 
moisture -&gt; moisture 
moisturize -&gt; moisturize 
moisturizing -&gt; moisturizing 
</code></pre>

<p>Although it was better than the initial values, I would like the following results.</p>

<pre><code>heard -&gt; hear
j-flying -&gt; fly
moisture -&gt; moist
moisturize -&gt; moist
moisturizing -&gt; moist
</code></pre>

<p>If you have any other good way to extract meaningful words, 
please let me know.
Thank you</p>
",Multilingual Language Processing & Language Identification,nltk lemmatizer extract meaningful word currently going create machine learning based code automatically map category going natural language processing several word list made following code referenced url href lemmatizer po tag resulting value shown satisfied following result however following value satisfied although wa better initial value would like following result good way extract meaningful word please let know thank
Does Google engine penalize pages containing (machine or human) translated content?,"<p>Google SE has zero-tolerance policy against duplicate and spun content, but I am not sure how it deals with translated text? Any guesses on how it might detect translated content? The first thing occurs to my mind is they use their own Google Translate to back-translate the translated content into the source language, but if that's the case do they have to try back-translating into all languages? Are there any specific similarity metrics for such a task? Thank you!</p>
",Multilingual Language Processing & Language Identification,doe google engine penalize page containing machine human translated content google se ha zero tolerance policy duplicate spun content sure deal translated text guess might detect translated content first thing occurs mind use google translate back translate translated content source language case try back translating language specific similarity metric task thank
What is Part of speech (POS) tag in natural language processing,"<p>I am trying to get introduce by Stanford NLP package. I tried to execute few examples on my system.</p>

<p>for sentense: <code>I like it</code> it gives following result:</p>

<p><img src=""https://i.sstatic.net/fucNu.png"" alt=""Result""></p>

<p>Can some one please tell me what is <code>PRP</code> , <code>VBP</code>?
for sentence <code>It was very fantastic experience</code> it gives:</p>

<p><img src=""https://i.sstatic.net/sR96q.png"" alt=""Example 2""></p>

<p><strong>Can someone please elaborate this result?</strong></p>

<p>It can be tested here: <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>

<p>I want to get sentiment result of the sentence. Whether +ve or -ve? with its score value.</p>
",Multilingual Language Processing & Language Identification,part speech po tag natural language processing trying get introduce stanford nlp package tried execute example system sentense give following result one please tell sentence give someone please elaborate result tested want get sentiment result sentence whether score value
Abbreviation Reference for NLTK Parts of Speech,"<p>I'm using nltk to find the parts of speech for each word in a sentence.  It returns abbreviations that I both can't fully intuit and can't find good documentation for.</p>

<p>Running:</p>

<pre><code>import nltk
sample = ""There is no spoon.""
tokenized_words = nltk.word_tokenize(sample)
tagged_words = nltk.pos_tag(tokenized_words)
print tagged_words
</code></pre>

<p>Returns:</p>

<pre><code>[('There', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('spoon', 'NN'), ('.', '.')]
</code></pre>

<p>In the above example, I'm looking for what <code>DT</code>, <code>EX</code>, and the rest mean.</p>

<p>The best I have so far is to search for mentions of the abbreviations of concern in <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow"">Natural Language Processing with Python</a>, but there has to be something better.  I did also find a few literature-based resources, but I don't know how to tell which nltk is using.</p>
",Multilingual Language Processing & Language Identification,abbreviation reference nltk part speech using nltk find part speech word sentence return abbreviation fully intuit find good documentation running return example looking rest mean best far search mention abbreviation concern natural language processing python ha something better also find literature based resource know tell nltk using
What is Word Injection in Context of Natural Language Processing and Whats are its Applications?,"<p>Hello All Community Members,</p>

<p>After going through various articles and blogs, I came to understand that suppose, in a corpus (<code>C1</code>), let's assume there exists a word (<code>w</code>). Now that same word (<code>w</code>) also appears in another related corpus (<code>C2</code>). Similarly, there exists <code>n</code> number of words in corpus. </p>

<p>By <strong>word injection</strong>, what I understand is that :- the word (<code>w</code>) in Corpus (C1) is replaced by its modified version (<code>.w</code>) with the assignment of some special character like (<code>.</code>, <code>_</code> or <code>$</code>) in corpus (<code>C2.</code>).</p>

<p><strong>Note:</strong> The modified version of word (<code>.w</code>) does not belong to the corpus C1 and C2. The word <code>.w</code> is the same word as <code>w</code> used in other context.</p>

<p>IS THIS THE CORRECT EXPLANATION for WORD INJECTION in NLP?</p>

<p>For example, the word <code>system</code> is replaced by <code>_system</code>. </p>

<p>I hope this is correct definition of word injection. Does there exist any way in python to achieve the above mentioned concept?</p>

<p>Any help is appreciated. </p>
",Multilingual Language Processing & Language Identification,word injection context natural language processing whats application hello community member going various article blog came understand suppose corpus let assume exists word word also appears another related corpus similarly exists number word corpus word injection understand word corpus c replaced modified version assignment special character like corpus note modified version word doe belong corpus c c word word used context correct explanation word injection nlp example word replaced hope correct definition word injection doe exist way python achieve mentioned concept help appreciated
Combining attributes in spaCy&#39;s Matcher,"<p>Is it possible to combine different attributes in spaCy's matcher?</p>

<p>For example, we can match a lemma:</p>

<pre><code>{""LEMMA"": ""mylemma""}
</code></pre>

<p>and we can match a lowercase string:</p>

<pre><code>{""LOWER"": ""mylowercasestring""}
</code></pre>

<p>Is it possible to match a LOWER LEMMA?</p>

<p>EDIT: </p>

<p>At least when using the German model (de_core_news_sm), lemmas are not lowercased. Suppose I have the German phrase ""des Richters"", whose lemma sequence is ""der Richter"" -- I want a matcher to match lemmas independent of case, so that it matches the lemmas ""Richter"" and ""richter"" or any other case variation. </p>
",Multilingual Language Processing & Language Identification,combining attribute spacy matcher possible combine different attribute spacy matcher example match lemma match lowercase string possible match lower lemma edit least using german model de core news sm lemma lowercased suppose german phrase de richters whose lemma sequence der richter want matcher match lemma independent case match lemma richter richter case variation
Natural Language Process: Discover category of text?,"<p>What library exists that let's you determine whether a column full of text is a certain entity based on a list?</p>

<p>For example, given many lists consisting of text strings for training (each list may have seldom outlier strings that is noise), I want to establish some category for that list. </p>

<p>Now when there's a new text string given, I want to know which category or entity it belongs to.</p>

<p>What do you call this in natural language processing?</p>
",Multilingual Language Processing & Language Identification,natural language process discover category text library exists let determine whether column full text certain entity based list example given many list consisting text string training list may seldom outlier string noise want establish category list new text string given want know category entity belongs call natural language processing
Index of Substring in Original Text,"<p>I am working on one of Natural Language Processing Problem using <strong>Python</strong>. </p>

<p>My Problem:
Let's consider a string</p>

<pre><code>str1 = ""United, State is a very nice country""
</code></pre>

<p>We preprocess this string where we removed all special symbol, URLs, HTML content etc. Now the preprocessed string will become something like this:</p>

<pre><code>preprocessed_str = ""United State is a very nice country""
</code></pre>

<p>Then we pass this preprocessed string to my machine learning model which returns results like: </p>

<pre><code>Country: United State
</code></pre>

<p><strong>Now I want to take start and end index of the ""United State"" in the original string.</strong> </p>

<p>We tried this with <strong>python's str.find()</strong> function but it returns -1.</p>

<p>I will appreciate the approaches and solution through which we can solve this problem. Thanks in advance :)</p>
",Multilingual Language Processing & Language Identification,index substring original text working one natural language processing problem using python problem let consider string preprocess string removed special symbol url html content etc preprocessed string become something like pas preprocessed string machine learning model return result like want take start end index united state original string tried python str find function return appreciate approach solution solve problem thanks advance
Natural Language Processing Tools,"<p>Are there any good free NLP tools(for node)/APIs available for free? So far I found some github repos such as <code>natural</code>, that seem decent for basic senstence analysis, but not so good for more advanced stuff. Also I noticed the google NLP, but that isn't free. Anyone has experience working with these things and can recommend something free to use and powerful?</p>
",Multilingual Language Processing & Language Identification,natural language processing tool good free nlp tool node apis available free far found github repos seem decent basic senstence analysis good advanced stuff also noticed google nlp free anyone ha experience working thing recommend something free use powerful
The lemma of a noun in French,"<p>When I run the following code, I get the lemma of the noun ""Suppression"" being the same word ""Suppression"". </p>

<pre><code>import treetaggerwrapper as tt
tt_fr = tt.TreeTagger(TAGLANG='fr')
tag = tt_fr.TagText(u'Suppression')
</code></pre>

<p>The result I was waiting for is to get the actual verb which is ""Supprimer"". Is it because of the language (French)? Or is it Treetagger that doesn't do the work? Or is it me who don't understand the meaning of the lemma?</p>
",Multilingual Language Processing & Language Identification,lemma noun french run following code get lemma noun suppression word suppression result wa waiting get actual verb supprimer language french treetagger work understand meaning lemma
Stanford CoreNLP POS tagging in French,"<p>I am looking for a way to use Pos tagging for French sentences with Python. I saw that we could use Stanford CoreNLP but after several searches on google, I did not find real examples that could satisfy me ..
It would be great to have a piece of code that shows me how to solve my problem</p>
",Multilingual Language Processing & Language Identification,stanford corenlp po tagging french looking way use po tagging french sentence python saw could use stanford corenlp several search google find real example could satisfy would great piece code show solve problem
Translate unicode emojis to ascii emojis in Python,"<p>Is there a way to translate unicode emojis to an appropriate ascii emoticon in Python? I know the <a href=""https://pypi.org/project/emoji/"" rel=""nofollow noreferrer"">emoji library</a>  which can be used to convert unicode emojis to something like :crying_face:. But what I would need is to convert it to :'(</p>

<p>Is there an elegant way to do this without having to translate every possible emoji manually? Another option would be to convert the ascii emojis also to their textual representation, i.e. :'( should become :crying_face:.</p>

<p>My intermediate goal is to find a way to transform ascii and unicode emojis to a common representation. My final goal would be to replace emoticons (no matter if unicode or ascii) by the emotion they represent (if they do not represent an emotion, remove them)</p>
",Multilingual Language Processing & Language Identification,translate unicode emojis ascii emojis python way translate unicode emojis appropriate ascii emoticon python know emoji library used convert unicode emojis something like cry face would need convert elegant way without translate every possible emoji manually another option would convert ascii emojis also textual representation e become cry face intermediate goal find way transform ascii unicode emojis common representation final goal would replace emoticon matter unicode ascii emotion represent represent emotion remove
Meaning of feats in syntaxnet,"<p>I am using syntaxnet in Spanish and I have found that all the words have a field called ""feats"" whose format depends on the type of word (noun, pronoun, verb). There are some fields whose meaning is obvious, but in other cases I cannot figure out what it is showing. For example, this is the case of fields such as ""fPOS"" or ""Case"" in pronouns. Is there any guide or list with explanations avaiable?</p>
",Multilingual Language Processing & Language Identification,meaning feat syntaxnet using syntaxnet spanish found word field called feat whose format depends type word noun pronoun verb field whose meaning obvious case figure showing example case field fpos case pronoun guide list explanation avaiable
Stemming of the multilingual text corpus,"<p>I have a text corpus with item descriptions in English, Russian and Polish.</p>

<p>This text corpus has 68K observations. Some of these observations are written in English, some in Russian, and some in Polish.</p>

<p>Could you tell me how <strong><em>properly</em></strong> and <strong><em>cost-efficiently</em></strong> implement a word stemming in this case? I can not use an English stemmer on Russian words and vice versa.</p>

<p>Unfortunately, I could not find a good language identifier. E.g. <code>langdetect</code> works too slow and often incorrectly. For example, I try to identify language of english word 'today': </p>

<pre><code>detect(""today"") 
""so"" 
# i.e Somali 
</code></pre>

<p>So far my code implementation looks bad. I just use one stemmer on another:</p>

<pre><code>import nltk
# polish stemmer
from pymorfologik import Morfologik

clean_items = []

# create stemmers

snowball_en = nltk.SnowballStemmer(""english"")
snowball_ru = nltk.SnowballStemmer(""russian"")
stemmer_pl = Morfologik()

# loop over each item; create an index i that goes from 0 to the length
# of the item list 

for i in range(0, num_items):
    # Call our function for each one, and add the result to the list of
    # clean items

    cleaned = items.iloc[i]

    # to word stem
    clean_items.append(snowball_ru.stem(stemmer_pl(snowball_en.stem(cleaned))))
</code></pre>
",Multilingual Language Processing & Language Identification,stemming multilingual text corpus text corpus item description english russian polish text corpus ha k observation observation written english russian polish could tell properly cost efficiently implement word stemming case use english stemmer russian word vice versa unfortunately could find good language identifier e g work slow often incorrectly example try identify language english word today far code implementation look bad use one stemmer another
Stanford CoreNLP - lemmas are not recognised correctly,"<p>I‚Äôm using the coreNLP tools from the command line to tag some files containing text in German. I need to get the token, pos, lemma and ner annotations. For this purpose I‚Äôm using the following command:</p>

<p>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP <strong>-annotators tokenize,ssplit,pos,lemma,ner</strong> -filelist $dir/filelist.input  -outputFormat conll --add-modules java.se.ee -ner.useSUTime 0 <strong>-outputFormatOptions word,pos,lemma,ner</strong> -outputDirectory $dir/tagged_articles -replaceExtension -props StanfordCoreNLP-german.properties</p>

<p>However, the lemmas I‚Äôm getting are just not right. Here is an example of a tagged file:</p>

<p>Auch     ADV    auch    O</p>

<p>eine     ART     eine   O</p>

<p>ausgereifte ADJA    ausgereifte O</p>

<p>Technik NN  technik O</p>

<p>kann    VMFIN   kann    O</p>

<p>jedoch  ADV jedoch  O</p>

<p>an  APPR    a   O</p>

<p>ihre    PPOSAT  ihre    O</p>

<p>Grenzen NN  grenzen O</p>

<p>sto√üen  VVINF   sto√üen  O</p>

<p>The lemmas for some of those words should be: ist -> sein / Textmengen -> Textmenge / enormen -> enorm / Grenzen -> Grenze. So there is obviously something wrong and I‚Äôm wondering what it could be. Any hint is highly appreciated!</p>

<p>I am using the following German model: stanford-german-corenlp-2018-02-27-models.jar</p>

<p>According to the README file, the version of the coreNLP tools is ""2018-02-27    3.9.1‚Äù</p>

<p>java version ""10.0.1"" 2018-04-17</p>

<p>Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)</p>
",Multilingual Language Processing & Language Identification,stanford corenlp lemma recognised correctly using corenlp tool command line tag file containing text german need get token po lemma ner annotation purpose using following command java cp xmx g edu stanford nlp pipeline stanfordcorenlp annotator tokenize ssplit po lemma ner filelist dir filelist input outputformat conll add module java se ee ner usesutime outputformatoptions word po lemma ner outputdirectory dir tagged article replaceextension prop stanfordcorenlp german property however lemma getting right example tagged file auch adv auch eine art eine ausgereifte adja ausgereifte technik nn technik kann vmfin kann jedoch adv jedoch appr ihre pposat ihre grenzen nn grenzen sto en vvinf sto en lemma word ist sein textmengen textmenge enormen enorm grenzen grenze obviously something wrong wondering could hint highly appreciated using following german model stanford german corenlp model jar according readme file version corenlp tool java version java tm se runtime environment build
Python langdetect: choose between one language or the other only,"<p>I'm using <code>langdetect</code> to determine the language of a set of strings which I know are either in English or French.</p>

<p>Sometimes, <code>langdetect</code> tells me the language is Romanian for a string I know is in French.</p>

<p>How can I make <code>langdetect</code> choose between English or French only, and not all other languages?</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,python langdetect choose one language using determine language set string know either english french sometimes tell language romanian string know french make choose english french language thanks
Natural Language Processing in Python,"<p>How to find similar kind of issues for a new unseen issue based on past trained issues(includes summary and description of issue) using natural language processing in python</p>
",Multilingual Language Processing & Language Identification,natural language processing python find similar kind issue new unseen issue based past trained issue includes summary description issue using natural language processing python
How to extract an word and all his dependencies,"<p>I'm not data-scientist but I'm on a project where i need to do aspect based sentiments analysis, I've already done an classifier for sentiment analysis but now, I need to do the ""aspect based"" part.</p>

<p>I have a list of aspects (4) and I need to find this aspect in a text, get all his depencies and analyse the sentiment of this group of words.</p>

<blockquote>
  <p>The cake had good taste but the tea wasn't good at all</p>
</blockquote>

<p>""The cake had good taste"" = POS / ""the tea wasn't good at all"" = NEG</p>

<p>I've already explore stanford CoreNLP depencies parser but in french (because i've to do this in french) it's not so good (maybe I need to only keep Nouns and Adjectives for the parsing).</p>

<p>If you've any suggestions...</p>
",Multilingual Language Processing & Language Identification,extract word dependency data scientist project need aspect based sentiment analysis already done classifier sentiment analysis need aspect based part list aspect need find aspect text get depencies analyse sentiment group word cake good taste tea good cake good taste po tea good neg already explore stanford corenlp depencies parser french french good maybe need keep noun adjective parsing suggestion
Looping through Lemmas in NLTK Wordnet,"<p>Have a script for getting italian synonyms from Wordnet like this:</p>

<pre><code>from nltk.corpus import wordnet as wn

it_lemmas = wn.lemmas(""problema"", lang=""ita"")

hypernyms = it_lemmas[0].synset().hypernyms()

print(hypernyms[0].lemmas(lang=""ita""))
</code></pre>

<p>When I do the looping I get message <code>that list indices must be integers or slices, not Lemma</code></p>

<p>How should I do the looping to get not only one value ([0]) but all the values in this dictionary (the amount can be different) and print them all?</p>
",Multilingual Language Processing & Language Identification,looping lemma nltk wordnet script getting italian synonym wordnet like looping get message looping get one value value dictionary amount different print
Is there any api to get all words regarding a particular topic available in english grammar,"<p>I am working on NLP with python and my next step is to gather huge-huge data regarding specific topics available in English grammar. </p>

<p>For example : all words that can define a ""Department"" say  ""Accounts"".</p>

<p>So can any tell me how I can gather such data (if possible, through any API).</p>
",Multilingual Language Processing & Language Identification,api get word regarding particular topic available english grammar working nlp python next step gather huge huge data regarding specific topic available english grammar example word define department say account tell gather data possible api
Checking if word segmentation is possible,"<p>This is a follow up question to <a href=""https://stackoverflow.com/questions/3466972/how-to-split-a-string-into-words-ex-stringintowords-string-into-words/3469228#3469228"">this response</a> and the pseudo-code algorithm that the user posted. I didn't comment on that question because of its age. I am only interested in validating whether or not a string can be split into words. The algorithm doesn't need to actually split the string. This is the response from the linked question:</p>

<blockquote>
  <p>Let S[1..length(w)] be a table with Boolean entries. S[i] is true if
  the word w[1..i] can be split. Then set S[1] = isWord(w[1]) and for
  i=2 to length(w) calculate</p>
  
  <p>S[i] = (isWord[w[1..i] or for any j in {2..i}: S[j-1] and
  isWord[j..i]).</p>
</blockquote>

<p>I'm translating this algorithm into simple python code, but I'm not sure if I'm understanding it properly. Code:</p>

<pre><code>def is_all_words(a_string, dictionary)):
    str_len = len(a_string)
    S = [False] * str_len
    S[0] = is_word(a_string[0], dictionary)
    for i in range(1, str_len):
        check = is_word(a_string[0:i], dictionary)
        if (check):
            S[i] = check
        else:
            for j in range(1, str_len):
                check = (S[j - 1] and is_word(a_string[j:i]), dictionary)
                if (check):
                    S[i] == True
                    break
    return S
</code></pre>

<p>I have two related questions. 1) Is this code a proper translation of the linked algorithm into Python, and if it is, 2) Now that I have S, how do I use it to tell if the string <em>is</em> only comprised of words? In this case, <code>is_word</code> is a function that simply looks a given word up in a list. I haven't implemented it as a trie yet. </p>

<p>UPDATE: After updating the code to include the suggested change, it doesn't work. This is the updated code:</p>

<pre><code>def is_all_words(a_string, dictionary)):
    str_len = len(a_string)
    S = [False] * str_len
    S[0] = is_word(a_string[0], dictionary)
    for i in range(1, str_len):
        check = is_word(a_string[0:i], dictionary)
        if (check):
            S[i] = check
        else:
            for j in range(1, i): #THIS LINE WAS UPDATED
                check = (S[j - 1] and is_word(a_string[j:i]), dictionary)
                if (check):
                    S[i] == True
                    break
    return S

a_string = ""carrotforever""
S = is_all_words(a_string, dictionary)
print(S[len(S) - 1]) #prints FALSE

a_string = ""hello""
S = is_all_words(a_string, dictionary)
print(S[len(S) - 1]) #prints TRUE
</code></pre>

<p>It should return <code>True</code> for both of these. </p>
",Multilingual Language Processing & Language Identification,checking word segmentation possible follow question href response pseudo code algorithm user posted comment question age interested validating whether string split word algorithm need actually split string response linked question let length w table boolean entry true word w split set isword w length w calculate isword w j j isword j translating algorithm simple python code sure understanding properly code two related question code proper translation linked algorithm python use tell string comprised word case function simply look given word list implemented trie yet update updating code include suggested change work updated code return
return the base word without POS tags,"<p>Is there in anyway to stem both words (watching and watchers) to ""watch"" without using POS tagging?</p>

<pre><code>from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
wordnet_lemmatizer.lemmatize('watchers', pos='n')
wordnet_lemmatizer.lemmatize('watching', pos='v')
</code></pre>

<p>This and English Stemmer mentioned below both return ('watcher', 'watch'). I need to return only ('watch') using nltk or spacy.</p>

<pre><code>from nltk.stem.snowball import EnglishStemmer
stemmer = EnglishStemmer()
stemmer.stem('watchers') , stemmer.stem('watching')
</code></pre>

<p>Is this possible?</p>
",Multilingual Language Processing & Language Identification,return base word without po tag anyway stem word watching watcher watch without using po tagging english stemmer mentioned return watcher watch need return watch using nltk spacy possible
Microsoft Azure Text Analytics Coginitive Service Encoding Issue,"<p>In order to use their text analytics, Azure requires a json file/document that looks like this:</p>

<pre><code>document = {
  ""documents"" :[
    {""id"": ""1"", ""language"": ""en"", ""text"": ""I had a wonderful experience! The rooms were wonderful and the staff was helpful.""},
    {""id"": ""2"", ""language"": ""en"", ""text"": ""I had a terrible time at the hotel. The staff was rude and the food was awful.""},
    {'id': '3', 'language': 'es', 'text': 'Los caminos que llevan hasta Monte Rainier son espectaculares y hermosos.'},  
    {'id': '4', 'language': 'es', 'text': 'La carretera estaba atascada. Hab√≠a mucho tr√°fico el d√≠a de ayer.'}]}
</code></pre>

<p>The issue I am getting at the moment is that the last record <code>id: 4</code> is causing this error:</p>

<pre><code>b'{""code"":""BadRequest"",""message"":""Invalid request"",""innerError"":{""code"":""InvalidRequestBodyFormat"",""message"":""Request body format is wrong. 
Make sure the json request is serialized correctly and there are no null members.""}}'
</code></pre>

<p>The formatting of the JSON is correct, it's straight from their site and it runs perfectly fine without the last record. I tested some more and then found out that the <code>√≠</code> and <code>√°</code> are the ones throwing the error. To make sure, I even tested it out with English words like resum√© or fianc√© but still the same error. But that doesn't make sense since Spanish is one of the supported languages for the text analysis and the text language is even define as Spanish before it's processed.</p>

<p>So my question is, am I missing something before passing my data through Azure? Am I suppose to convert, changing the encoding, or remove those characters or is this something that Azure's API should be able to handle?</p>

<p>EDIT: A little more background, I followed the instructions provided on their site to set it up to work with <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/Text-Analytics/quickstarts/python#KeyPhraseExtraction"" rel=""nofollow noreferrer"">python</a>. It works perfectly except for what I mentioned. </p>
",Multilingual Language Processing & Language Identification,microsoft azure text analytics coginitive service encoding issue order use text analytics azure requires json file document look like issue getting moment last record causing error formatting json correct straight site run perfectly fine without last record tested found one throwing error make sure even tested english word like resum fianc still error make sense since spanish one supported language text analysis text language even define spanish processed question missing something passing data azure suppose convert changing encoding remove character something azure api able handle edit little background followed instruction provided site set work python work perfectly except mentioned
Getting the basic form of the english word,"<p>I am trying to get the basic english word for an english word which is modified from its base form. This question had been asked here, but I didnt see a proper answer, so I am trying to put it this way. I tried 2 stemmers and one lemmatizer from NLTK package which are porter stemmer, snowball stemmer, and wordnet lemmatiser.</p>

<p>I tried this code:</p>

<pre><code>from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer

words = ['arrival','conclusion','ate']

for word in words:
    print ""\n\nOriginal Word =&gt;"", word
    print ""porter stemmer=&gt;"", PorterStemmer().stem(word)
    snowball_stemmer = SnowballStemmer(""english"")
    print ""snowball stemmer=&gt;"", snowball_stemmer.stem(word)
    print ""WordNet Lemmatizer=&gt;"", WordNetLemmatizer().lemmatize(word)
</code></pre>

<p>This is the output I get:</p>

<pre><code>Original Word =&gt; arrival
porter stemmer=&gt; arriv
snowball stemmer=&gt; arriv
WordNet Lemmatizer=&gt; arrival


Original Word =&gt; conclusion
porter stemmer=&gt; conclus
snowball stemmer=&gt; conclus
WordNet Lemmatizer=&gt; conclusion


Original Word =&gt; ate
porter stemmer=&gt; ate
snowball stemmer=&gt; ate
WordNet Lemmatizer=&gt; ate
</code></pre>

<p>but I want this output</p>

<pre><code>    Input : arrival
    Output: arrive

    Input : conclusion
    Output: conclude

    Input : ate
    Output: eat 
</code></pre>

<p>How can I achieve this? Are there any tools already available for this? This is called as morphological analysis. I am aware of that, but there must be some tools which are already achieving this. Help is appreciated :)</p>

<p><strong>First Edit</strong></p>

<p>I tried this code</p>

<pre><code>import nltk
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet as wn

query = ""The Indian economy is the worlds tenth largest by nominal GDP and third largest by purchasing power parity""

def is_noun(tag):
    return tag in ['NN', 'NNS', 'NNP', 'NNPS']

def is_verb(tag):
    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']

def is_adverb(tag):
    return tag in ['RB', 'RBR', 'RBS']

def is_adjective(tag):
    return tag in ['JJ', 'JJR', 'JJS']

def penn_to_wn(tag):
    if is_adjective(tag):
        return wn.ADJ
    elif is_noun(tag):
        return wn.NOUN
    elif is_adverb(tag):
        return wn.ADV
    elif is_verb(tag):
        return wn.VERB
    return wn.NOUN

tags = nltk.pos_tag(word_tokenize(query))
for tag in tags:
    wn_tag = penn_to_wn(tag[1])
    print tag[0]+""---&gt; ""+WordNetLemmatizer().lemmatize(tag[0],wn_tag)
</code></pre>

<p>Here, I tried to use wordnet lemmatizer by providing proper tags. Here is the output:</p>

<pre><code>The---&gt; The
Indian---&gt; Indian
economy---&gt; economy
is---&gt; be
the---&gt; the
worlds---&gt; world
tenth---&gt; tenth
largest---&gt; large
by---&gt; by
nominal---&gt; nominal
GDP---&gt; GDP
and---&gt; and
third---&gt; third
largest---&gt; large
by---&gt; by
purchasing---&gt; purchase
power---&gt; power
parity---&gt; parity
</code></pre>

<p>Still, words like ""arrival"" and ""conclusion"" wont get processed with this approach. Is there any solution for this?</p>
",Multilingual Language Processing & Language Identification,getting basic form english word trying get basic english word english word modified base form question asked didnt see proper answer trying put way tried stemmer one lemmatizer nltk package porter stemmer snowball stemmer wordnet lemmatiser tried code output get want output achieve tool already available called morphological analysis aware must tool already achieving help appreciated first edit tried code tried use wordnet lemmatizer providing proper tag output still word like arrival conclusion wont get processed approach solution
How to get better lemmas from Spacy,"<p>While ""PM"" can mean ""pm(time)"" it can also mean ""Prime Minister"".</p>

<p>I want to capture the latter. I want lemma of ""PM"" to return ""Prime Minister"". How can I do this using <code>spacy</code>?</p>

<p>Example returning unexpected lemma: </p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; #nlp = spacy.load('en')
&gt;&gt;&gt; nlp = spacy.load('en_core_web_lg')
&gt;&gt;&gt; doc = nlp(u'PM means prime minister')
&gt;&gt;&gt; for word in doc:
...     print(word.text, word.lemma_)
... 
PM pm
means mean
prime prime
minister minister
</code></pre>

<p>As per doc <a href=""https://spacy.io/api/annotation"" rel=""nofollow noreferrer"">https://spacy.io/api/annotation</a>, spacy uses WordNet for lemmas; </p>

<blockquote>
  <p>A lemma is the uninflected form of a word. The English lemmatization data is taken from WordNet..</p>
</blockquote>

<p>When I tried inputting ""pm"" in <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=pm&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=0000"" rel=""nofollow noreferrer"">Wordnet</a>, it shows ""Prime Minister"" as one of the lemmas.</p>

<p>What am I missing here?</p>
",Multilingual Language Processing & Language Identification,get better lemma spacy pm mean pm time also mean prime minister want capture latter want lemma pm return prime minister using example returning unexpected lemma per doc spacy us wordnet lemma lemma uninflected form word english lemmatization data taken wordnet tried inputting pm wordnet show prime minister one lemma missing
How to Convert English to Cron?,"<p>I did some searching but haven't landed anything that looks useful yet but I am wondering if anyone knows of something (tool,lib etc) that can parse English phrases and translate them into a cron string.</p>

<p>For example: <code>Every Tuesday at 15:00</code> converts to <code>0 15 * * 2</code></p>

<p>It seems like something that would have lots of gotchas and it would be preferable to benefit from someone elses work. You see it in a few nice sites/apps that can work out what you mean from a simple phrase rather than having some hideous user interface.</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,convert english cron searching landed anything look useful yet wondering anyone know something tool lib etc parse english phrase translate cron string example convert seems like something would lot gotchas would preferable benefit someone el work see nice site apps work mean simple phrase rather hideous user interface thanks advance
German stemmer is not removing feminine suffixes &quot;-in&quot; and &quot;-innen&quot;,"<p>In German, every job has a feminine and a masculine version. The feminine one is derived from the masculine one by adding an ""-in"" suffix. In the plural form, this turns into ""-innen"".</p>

<p>Example:</p>

<pre><code>      | English          | German
------+------------------+-----------------------
masc. | teacher  doctor  | Lehrer      Arzt
fem.  | teacher  doctor  | Lehrerin    √Ñrztin
masc. | teachers doctors | Lehrer      √Ñrzte
fem.  | teachers doctors | Lehrerinnen √Ñrztinnen
</code></pre>

<p>Currently, I'm using NLTK's <code>nltk.stem.snowball.GermanStemmer</code>. 
It returns these stems:</p>

<pre><code>Lehrer      -&gt; lehr      | Arzt      -&gt; arzt
Lehrerin    -&gt; lehrerin  | √Ñrztin    -&gt; arztin
Lehrer      -&gt; lehr      | √Ñrzte     -&gt; arzt
Lehrerinnen -&gt; lehrerinn | √Ñrztinnen -&gt; arztinn
</code></pre>

<p>Is there a way to make this stemmer return the same stems for all four versions, feminine and masculine ones? Alternatively, is there any other stemmer doing that?</p>

<h2>Update</h2>

<p>I ended up adding ""-innen"" and ""-in"" as the first entries in the step 1 suffix-tuple like so:</p>

<pre><code>stemmer = GermanStemmer()
stemmer._GermanStemmer__step1_suffixes = (""innen"", ""in"") + stemmer._GermanStemmer__step1_suffixes
</code></pre>

<p>This way all of the above words are stemmed to <code>lehr</code> and <code>arzt</code> respectively. Also, all other ""job-forms"" that I tried so far are stemmed correctly, meaning masculine and feminine forms have the same stem. Also, if the ""job-form"" is derived from a verb, like <code>Lehrer/in</code>, they have the same stem as the verb.</p>
",Multilingual Language Processing & Language Identification,german stemmer removing feminine suffix innen german every job ha feminine masculine version feminine one derived masculine one adding suffix plural form turn innen example currently using nltk return stem way make stemmer return stem four version feminine masculine one alternatively stemmer update ended adding innen first entry step suffix tuple like way word stemmed respectively also job form tried far stemmed correctly meaning masculine feminine form stem also job form derived verb like stem verb
Product Name Recognition from Informal Text,"<p>About 5 years ago, I re-trained Stanford NER and it works somewhat, but new products often get missed. At that time, I retrained the entire NER model. What I would really like to do is to fine tune the Stanford NER model. Can that be done now?  Someone asked that before but the answer is not clear to me.</p>

<p><a href=""https://stackoverflow.com/questions/46114476/how-to-create-incremental-ner-training-modelappending-in-existing-model"">How to create incremental NER training model(Appending in existing model)?</a></p>

<p>Also related:
<a href=""https://stackoverflow.com/questions/30047569/how-to-extract-brand-from-product-name"">How to extract brand from product name</a></p>

<p>The most recent paper I can find is this: <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.904.3818&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.904.3818&amp;rep=rep1&amp;type=pdf</a></p>

<p>I tried the NER command on <a href=""https://stanfordnlp.github.io/CoreNLP/caseless.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/caseless.html</a> and it gave an error indicating ""english-caseless-left3words-distsim.tagger"" etc. could not be found. Is there a place for download trained models?</p>

<p>(OK. The models are included in stanford-english-corenlp-2018-02-27-models.jar)</p>
",Multilingual Language Processing & Language Identification,product name recognition informal text year ago trained stanford ner work somewhat new product often get missed time retrained entire ner model would really like fine tune stanford ner model done someone asked answer clear recent paper find tried ner command gave error indicating english caseless left word distsim tagger etc could found place download trained model ok model included stanford english corenlp model jar
Corrupted Rmarkdown script: How can I get the Cyrillic characters back?,"<p>I was working with a script with lots of Cyrillic characters (throughout chunks and out of them) for weeks. One day I have opened a new Rmarkdown script where I wrote English, while the other document is still in my R session. Afterwards, I have returned to the Cyrillic document and everything written turns to something like this <code>8 √ê¬∏√ë√ê¬ª√ë√Ç¬†1995 --&gt; √ê√ê¬ª√ê¬∞√ë√ë√ë - √ê¬Ω√ê¬∞√ë√ê¬æ√ê¬¥√ë</code> </p>

<p><strong>The question is</strong>: Where is the source of problem? And, how can the corrupted script turn to its original form (with the Cyrillic characters)?</p>

<hr>

<p><strong>UPDATE!!</strong></p>

<p>I have tried reopeining the Rstudio scrip with encoding CP1251, CP1252, windows1251 and UTF8, but it does not work. Certaintly the weird symbols change to another weird symbols. The problem is that I have saved the document with the default encoding CP1251 and windows1251) at the very begining. </p>

<hr>

<p><strong>Solution:</strong>
If working with cyrillic and lating characters, be sure you save the Rstudio script with UTF-8 encoding always, when you computer is windows (I do not know mac). If you close the script and open it again, re-open the file with UTF8 encoding.  </p>
",Multilingual Language Processing & Language Identification,corrupted rmarkdown script get cyrillic character back wa working script lot cyrillic character throughout chunk week one day opened new rmarkdown script wrote english document still r session afterwards returned cyrillic document everything written turn something like question source problem corrupted script turn original form cyrillic character update tried reopeining rstudio scrip encoding cp cp window utf doe work certaintly weird symbol change another weird symbol problem saved document default encoding cp window begining solution working cyrillic lating character sure save rstudio script utf encoding always computer window know mac close script open open file utf encoding
Python3 remove multiple hyphenations from a german string,"<p>I'm currently working on a neural network that evaluates students' answers to exam questions. Therefore, preprocessing the corpora for a Word2Vec network is needed. Hyphenation in german texts is quite common. There are mainly two different types of hyphenation:</p>

<p>1) End of line:  </p>

<p><code>The text reaches the end of the line so the last word is sepa-
rated.</code></p>

<p>2) Short form of enumeration:</p>

<p>in case of two ""elements"": </p>

<p><code>Geistes- und Sozialwissenschaften</code></p>

<p>more ""elements"": </p>

<p><code>Wirtschafts-, Geistes- und Sozialwissenschaften</code></p>

<p>The de-hyphenated form of these enumerations should be:</p>

<p><code>Geisteswissenschaften und Sozialwissenschaften</code></p>

<p><code>Wirtschaftswissenschaften, Geisteswissenschaften und Sozialwissenschaften</code></p>

<p>I need to remove all hyphenations and put the words back together. I already found several solutions for the first problem.</p>

<p>But I have absoluteley no clue how to get the second part (in the example above ""wissenschaften"") of the words in the enumeration problem. I don't even know if it is possible at all.</p>

<p>I hope that I have pointet out my problem properly.</p>

<p>So has anyone an idea how to solve this problem?</p>

<p>Thank you very much in advance!</p>
",Multilingual Language Processing & Language Identification,python remove multiple hyphenation german string currently working neural network evaluates student answer exam question therefore preprocessing corpus word vec network needed hyphenation german text quite common mainly two different type hyphenation end line short form enumeration case two element element de hyphenated form enumeration need remove hyphenation put word back together already found several solution first problem absoluteley clue get second part example wissenschaften word enumeration problem even know possible hope pointet problem properly ha anyone idea solve problem thank much advance
Toolkit to implement Text-to-Speech system for native language,"<p>I am creating a TTS system for a native language from which i would create a database of voice recordings from the native people. </p>

<p>I have no experience with Natural Language Processing, and so i would like to know if there are some current tools to achieve my aim?</p>

<p>I am not building from scratch with either laravel or python</p>

<p>Thanks in advance. </p>
",Multilingual Language Processing & Language Identification,toolkit implement text speech system native language creating tt system native language would create database voice recording native people experience natural language processing would like know current tool achieve aim building scratch either laravel python thanks advance
Stanford NLP CoreNLP don&#39;t do sentence split for chinese,"<p>My environment:</p>

<ul>
<li>CoreNLP 3.5.1</li>
<li>stanford-chinese-corenlp-2015-01-30-models</li>
<li>default property file for chinese :<code>StanfordCoreNLP-chinese.properties</code>
<ul>
<li><code>annotators = segment, ssplit</code></li>
</ul></li>
</ul>

<hr>

<p>My testing text is <code>""ÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂè•Â≠ê„ÄÇÈÄôÊòØÁ¨¨‰∫åÂÄãÂè•Â≠ê„ÄÇ""</code>
I get sentence from </p>

<pre><code>val sentences = annotation.get(classOf[SentencesAnnotation])
for (sent &lt;- sentences) {
  count+=1
  println(""sentence{$count} = "" + sent.get(classOf[TextAnnotation]))
}
</code></pre>

<p><strong>It always prints the whole testing text as one sentence , not the expected two here :</strong></p>

<pre><code>sentence1 = ÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂè•Â≠ê„ÄÇÈÄôÊòØÁ¨¨‰∫åÂÄãÂè•Â≠ê„ÄÇ
</code></pre>

<p>the expected are:</p>

<pre><code>expected sentence1 = ÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂè•Â≠ê„ÄÇ
expected sentence2 = ÈÄôÊòØÁ¨¨‰∫åÂÄãÂè•Â≠ê„ÄÇ
</code></pre>

<hr>

<p>Even the same result if I add more properties like :</p>

<pre><code>ssplit.eolonly = false
ssplit.isOneSentence = false
ssplit.newlineIsSentenceBreak = always
ssplit.boundaryTokenRegex = [.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+
</code></pre>

<hr>

<p>The CoreNLP logs are</p>

<pre><code>Registering annotator segment with class edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
Adding annotator segment
Loading Segmentation Model [edu/stanford/nlp/models/segmenter/chinese/ctb.gz]...Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... Loading Chinese dictionaries from 1 files:
  edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz

loading dictionaries from edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz...Done. Unique words in ChineseDictionary is: 423200
done [56.9 sec].
done. Time elapsed: 57041 ms
Adding annotator ssplit
Adding Segmentation annotation...output: [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]
INFO: TagAffixDetector: useChPos=false | useCTBChar2=true | usePKChar2=false
INFO: TagAffixDetector: building TagAffixDetector from edu/stanford/nlp/models/segmenter/chinese/dict/character_list and edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb
Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese/dict/character_list
Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb
ÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂè•Â≠ê„ÄÇÈÄôÊòØÁ¨¨‰∫åÂÄãÂè•Â≠ê„ÄÇ
---&gt;
[ÈÄôÊòØ, Á¨¨‰∏Ä, ÂÄã, Âè•Â≠ê, „ÄÇ, ÈÄôÊòØ, Á¨¨‰∫å, ÂÄã, Âè•Â≠ê, „ÄÇ]
done. Time elapsed: 419 ms
</code></pre>

<p>I once saw <a href=""http://webcache.googleusercontent.com/search?q=cache:O-n5_2O6x_AJ:stackoverflow.com.80bola.com/questions/28167258/stanford-nlp-filenotfoundexception-while-processing-chinese-text&amp;hl=zh-TW&amp;gl=tw&amp;strip=0"" rel=""nofollow"">someone</a> get the following log (CoreNLP 3.5.0) ; however oddly I do not have this log:</p>

<pre><code>Adding annotator ssplit edu.stanford.nlp.pipeline.AnnotatorImplementations:ssplit.boundaryTokenRegex=[.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+
</code></pre>

<hr>

<p>What's the problem ? Is there workaround? If unresolvable I can split it myself but I do not know how to integrate my splits into the CoreNLP pipeline.</p>
",Multilingual Language Processing & Language Identification,stanford nlp corenlp sentence split chinese environment corenlp stanford chinese corenlp model default property file chinese testing text get sentence always print whole testing text one sentence expected two expected even result add property like corenlp log saw someone get following log corenlp however oddly log problem workaround unresolvable split know integrate split corenlp pipeline
can I tokenize using spacy and then extract vectors for these token using pre trained word embeddings of fastext,"<p>I am tokenizing my text corpus which is in german language using the spacy's german model. 
Since currently, spacy only has small german model, I am unable to extract the word vectors using spacy itself. 
So, I am using fasttext's pre-trained word embeddings from here:<a href=""https://github.com/facebookresearch/fastText/blob/master/README.md#word-representation-learning"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/blob/master/README.md#word-representation-learning</a></p>

<p>Now facebook has used ICU tokenizer for tokenization process before extracting word embeddings for it. and i am using spacy
Can someone tell me if this is okay?
I feel spacy and ICU tokenizer might behave differently and if so then many tokens in my text corpus would not have a corresponding word vector</p>

<p>Thank for your help!</p>
",Multilingual Language Processing & Language Identification,tokenize using spacy extract vector token using pre trained word embeddings fastext tokenizing text corpus german language using spacy german model since currently spacy ha small german model unable extract word vector using spacy using fasttext pre trained word embeddings facebook ha used icu tokenizer tokenization process extracting word embeddings using spacy someone tell okay feel spacy icu tokenizer might behave differently many token text corpus would corresponding word vector thank help
NLP algorithm to extract part of sentence in language translation,"<p>I am trying to solve a problem but am not able to find a way other than training the data sets and making a classifier.</p>

<p><strong>Problem:</strong></p>

<p>The user says to translate a particular sentence from one language to another. I have the user speech in text part, and need to extract these 3 things from the text:</p>

<ul>
<li>Sentence to be translated.</li>
<li>The language in which its supposed to be translated.</li>
<li>The origin language.</li>
</ul>

<p>So, when we humans say, its usually in the form of these examples:</p>

<ul>
<li>What is I love you in French from English?</li>
<li>Can you translate I love you from English to French?</li>
<li>What is French for I love you in English? </li>
</ul>

<p>And any other possible way that a person can ask for translation.</p>

<p>I need to extract <code>I love you</code>, <code>French</code> (the language translated into) and <code>English</code> (the language translated from) from the sentence.
The first thing that came to my mind was to use <code>Regular Expessions</code>. But I found that it can only be used to detect the language and not the sentence part to be translated.</p>

<p>The other possible solution seems to have the various form of sentence as training data set and train a classifier, but I still feel that this NLP problem can be solved using some algorithm but am not able to get anything. </p>

<p>This seems to be a popular problem, so is there any way it can be done?</p>
",Multilingual Language Processing & Language Identification,nlp algorithm extract part sentence language translation trying solve problem able find way training data set making classifier problem user say translate particular sentence one language another user speech text part need extract thing text sentence translated language supposed translated origin language human say usually form example love french english translate love english french french love english possible way person ask translation need extract language translated language translated sentence first thing came mind wa use found used detect language sentence part translated possible solution seems various form sentence training data set train classifier still feel nlp problem solved using algorithm able get anything seems popular problem way done
fastText and word2vec: NaNs in accuracy computation code,"<p>I downloaded the pre-trained English Wikipedia vectors file (<code>wiki.en.vec</code>) from the fastText Github repository page, and I tried to compute the syntactic and semantic analogy task accuracies as described in the first of Mikolov's word2vec papers as follows:</p>

<p>I built the word2vec repository by simply doing <code>make</code>.</p>

<p>I ran <code>./compute-accuracy wiki.en.vec 0 &lt; questions-words.txt</code>, i.e., I pass the pre-trained vectors file to the compute-accuracy binary from word2vec along with a threshold of 0 in order to consider the entire vocabulary instead of by default restricting it to 30000, and I also send in the accuracy computation dataset <code>questions-words.txt</code> using <code>&lt;</code> because I noticed that the code reads the dataset from stdin.</p>

<p>In response, I simply get a bunch of NaNs like below. This doesn't change even if I change the threshold value to 30000 or anything else.</p>

<pre><code>&gt;capital-common-countries:
ACCURACY TOP1: 0.00 % (0 / 1)
Total accuracy: -nan % Semantic accuracy: -nan % Syntactic accuracy: -nan %
</code></pre>

<p>Can someone please explain why the English pre-trained vectors don't seem to work with word2vec's accuracy computation code? I took a look at <code>compute-accuracy.c</code> and it does look like it expects standard vector file formatting convention and I took a look at <code>wiki.en.vec</code> as well, and it does look like it is formatted in standard convention.</p>

<p>Also, in the fastText paper, word analogy accuracies with fastText vectors are presented and the paper cites Mikolov's word2vec paper there -- clearly, the same dataset was used, and presumably the same word2vec <code>compute-accuracy.c</code> file was used to obtain the presented numbers. So could someone please explain what's going wrong?</p>
",Multilingual Language Processing & Language Identification,fasttext word vec nan accuracy computation code downloaded pre trained english wikipedia vector file fasttext github repository page tried compute syntactic semantic analogy task accuracy described first mikolov word vec paper follows built word vec repository simply ran e pas pre trained vector file compute accuracy binary word vec along threshold order consider entire vocabulary instead default restricting also send accuracy computation dataset using noticed code read dataset stdin response simply get bunch nan like change even change threshold value anything else someone please explain english pre trained vector seem work word vec accuracy computation code took look doe look like expects standard vector file formatting convention took look well doe look like formatted standard convention also fasttext paper word analogy accuracy fasttext vector presented paper cite mikolov word vec paper clearly dataset wa used presumably word vec file wa used obtain presented number could someone please explain going wrong
How to extract meaning of colloquial phrases and expressions in English,"<p>I am looking into extracting the meaning of expressions used in everyday speaking. For an instance, it is apparent to a human that the sentence <code>The meal we had at restaurant A tasted like food at my granny's.</code> means that the food was tasty. </p>

<p>How can I extract this meaning using a tool or a technique?</p>

<p>The method I've found so far is to first extract phrases using Stanford CoreNLP POS tagging, and use a Word Sense Induction tool to derive the meaning of the phrase. However, as WSI tools are used to get the meaning of words when they have multiple meanings, I am not sure if it would be the best tool to use. </p>

<p>What would be the best method to extract the meanings? Or is there any tool that can both identify phrases and extract their meanings? </p>

<p>Any help is much appreciated. Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,extract meaning colloquial phrase expression english looking extracting meaning expression used everyday speaking instance apparent human sentence mean food wa tasty extract meaning using tool technique method found far first extract phrase using stanford corenlp po tagging use word sense induction tool derive meaning phrase however wsi tool used get meaning word multiple meaning sure would best tool use would best method extract meaning tool identify phrase extract meaning help much appreciated thanks advance
Fixing error output from seq2seq model,"<p>I want to ask you how we can effectively re-train a trained seq2seq model to remove/mitigate a specific observed error output. I'm going to give an example about Speech Synthesis, but any idea from different domains, such as Machine Translation and Speech Recognition, using seq2seq model will be appreciated.</p>

<p>I learned the basics of seq2seq with attention model, especially for Speech Synthesis such as <a href=""https://github.com/Rayhane-mamah/Tacotron-2"" rel=""nofollow noreferrer"">Tacotron-2</a>.
Using a distributed well-trained model showed me how naturally our computer could speak with the seq2seq (end-to-end) model (you can listen to some audio samples <a href=""https://r9y9.github.io/blog/2018/05/20/tacotron2/"" rel=""nofollow noreferrer"">here</a>). But still, the model fails to read some words properly, e.g., it fails to read ""obey […ôÀàbƒÅ]"" in multiple ways like […ôÀàbƒ´] and […ôÀàbƒì].</p>

<p>The reason is obvious because the word ""obey"" appears too little, only three times out of 225,715 words, in our dataset (<a href=""https://keithito.com/LJ-Speech-Dataset/"" rel=""nofollow noreferrer"">LJ Speech</a>), and the model had no luck.</p>

<p>So, how can we re-train the model to overcome the error? Adding extra audio clips containing the ""obey"" pronunciation sounds impractical, but reusing the three audio clips has the danger of overfitting. And also, I suppose we use a well-trained model and ""simply training more"" is not an effective solution.</p>

<p>Now, this is one of the drawbacks of seq2seq model, which is not talked much. The model successfully simplified the pipelines of the traditional models, e.g., for Speech Synthesis, it replaced an acoustic model and a text analysis frontend etc by a single neural network. But we lost the controllability of our model at all. It's impossible to make the system read in a specific way.</p>

<p>Again, if you use a seq2seq model in any field and get an undesirable output, how do you fix that? Is there a data-scientific workaround to this problem, or maybe a cutting-edge Neural Network mechanism to gain more controllability in seq2seq model?</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,fixing error output seq seq model want ask effectively train trained seq seq model remove mitigate specific observed error output going give example speech synthesis idea different domain machine translation speech recognition using seq seq model appreciated learned basic seq seq attention model especially speech synthesis tacotron using distributed well trained model showed naturally computer could speak seq seq end end model listen audio sample still model fails read word properly e g fails read obey b multiple way like b b reason obvious word obey appears little three time word dataset lj speech model luck train model overcome error adding extra audio clip containing obey pronunciation sound impractical reusing three audio clip ha danger overfitting also suppose use well trained model simply training effective solution one drawback seq seq model talked much model successfully simplified pipeline traditional model e g speech synthesis replaced acoustic model text analysis frontend etc single neural network lost controllability model impossible make system read specific way use seq seq model field get undesirable output fix data scientific workaround problem maybe cutting edge neural network mechanism gain controllability seq seq model thanks
Chatbot with node.js and python for NLP,"<p>I'm actually working on a chatbot project which must talk 2 languages <strong>(Dialectic Moroccan and french)</strong> , i'm willing to build the bot with node.js and host it in a server and build an NLP with python from scratch  then link both of codes . </p>

<p>Do you have any idea of how can i integrate python code to a <strong>node.js</strong> code? </p>
",Multilingual Language Processing & Language Identification,chatbot node j python nlp actually working chatbot project must talk language dialectic moroccan french willing build bot node j host server build nlp python scratch link code idea integrate python code node j code
How can I look up an english dictionary in python?,"<p>I am developing a Python program in order to find the etymology of words in a text. I have found out there are basically two options: parsing an online dictionary that provides etymology or using an API. I found this reply here but I don't seem to understand how to link the Oxford API with my Python program.</p>

<p>Can anyone explain me how to look up a word in an english dictionary? Thank you in advance.</p>

<p>Link to the question <a href=""https://stackoverflow.com/questions/21395011/python-module-with-access-to-english-dictionaries-including-definitions-of-words"">here</a></p>

<blockquote>
  <p>Note that while WordNet does not have all English words, what about the Oxford English Dictionary? (<a href=""http://developer.oxforddictionaries.com/"" rel=""noreferrer"">http://developer.oxforddictionaries.com/</a>). Depending on the scope of your project, it could be a killer API.
  Have you tried looking at Grady Ward's Moby? [link] (<a href=""http://icon.shef.ac.uk/Moby/"" rel=""noreferrer"">http://icon.shef.ac.uk/Moby/</a>).
  You could add it as a lexicon in NLTK (see notes on ""Loading your own corpus"" in Section 2.1).</p>
</blockquote>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from nltk.corpus import PlaintextCorpusReader
corpus_root = '/usr/share/dict'
wordlists = PlaintextCorpusReader(corpus_root, '.*')</code></pre>
</div>
</div>
</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from nltk.corpus import BracketParseCorpusReader
corpus_root = r""C:\corpora\penntreebank\parsed\mrg\wsj""
file_pattern = r"".*/wsj_.*\.mrg""
ptb = BracketParseCorpusReader(corpus_root, file_pattern)</code></pre>
</div>
</div>
</p>
",Multilingual Language Processing & Language Identification,look english dictionary python developing python program order find etymology word text found basically two option parsing online dictionary provides etymology using api found reply seem understand link oxford api python program anyone explain look word english dictionary thank advance link question depending scope project could killer api tried looking grady ward moby link could add lexicon nltk see note loading corpus section
Remove Telephone numbers using Regular Expressions in Python 3,"<p>I am trying to remove telephone numbers from a bunch of documents that i have parsed using <a href=""https://tika.apache.org/"" rel=""nofollow noreferrer"">tika</a> but I do not succeed.</p>

<p><a href=""https://i.sstatic.net/soGVL.png"" rel=""nofollow noreferrer"">Here is a screenshot</a> taken by <a href=""https://regex101.com/"" rel=""nofollow noreferrer"">regex101 validator</a>. As you can see, phone numbers are skipped.</p>

<p>The same example in text format is the following:</p>

<blockquote>
  <p>""Something here  </p>
  
  <p>and something here 9, but (I have something here as well), 123456, Hi guys!  </p>
  
  <p>+39.1234.325636     +39.321.1234567        </p>
  
  <p>sex male | date of birth 16/12/1927 | nationality italian  </p>
  
  <p>some stuff   ""</p>
</blockquote>

<p>This is my Regex (I am not an expert in this field):</p>

<p><code>(\(00\d{2}\)|\(\+\d{2}\)|00\d{2}|\+\d{2})[\. ]??3\d{2}[\. \-]??\d{2,4}[\. \-]??\d{2,4}$</code></p>

<p><strong>Notice that +39 (or 0039) is fixed and the first 3 in the second telephone number is also fixed.</strong></p>

<p>Do you have any suggestions?
Many thanks.</p>
",Multilingual Language Processing & Language Identification,remove telephone number using regular expression python trying remove telephone number bunch document parsed using tika succeed screenshot taken regex validator see phone number skipped example text format following something something something well hi guy sex male date birth nationality italian stuff regex expert field notice fixed first second telephone number also fixed suggestion many thanks
How does TreeTagger get the lemma of a word?,"<p>I am using TreeTagger to get the lemmas of words in Spanish, but I have observed there are too much words which are not transformed as should be. I would like to know how this operations works, if it is done with techniques such as decision trees or machine learning algorithms or it simply contains a list of words with its corresponding lemma. Does someone know it?
Thanks!!</p>
",Multilingual Language Processing & Language Identification,doe treetagger get lemma word using treetagger get lemma word spanish observed much word transformed would like know operation work done technique decision tree machine learning algorithm simply contains list word corresponding lemma doe someone know thanks
Removing duplicate lines and split parallel corpus,"<p>I have parallel translated corpus in English-French (text.en,text.fr),
each text includes around 500K of lines (sentences in source and target languge). what I want is to:
1- Remove the duplicated lines in both texts using python command; and avoid any alignment problem in both files. e.g: command deleted line 32 in text.en, then of course delete it in text.fr.
2- Then Split both files into Train/Dev/Test data, only 1K for dev, and 1K for test, and the rest for train.
I need to split text.en and text.fr using the same command, so I could keep the alignment and corresponding sentences in both files.
It would be better if I could extract test and dev data randomly, that will help getting better results.
How can I do that? please write the commands.
I appreciate any help, Thank you !</p>
",Multilingual Language Processing & Language Identification,removing duplicate line split parallel corpus parallel translated corpus english french text en text fr text includes around k line sentence source target languge want remove duplicated line text using python command avoid alignment problem file e g command deleted line text en course delete text fr split file train dev test data k dev k test rest train need split text en text fr using command could keep alignment corresponding sentence file would better could extract test dev data randomly help getting better result please write command appreciate help thank
Distant supervision algorithms for NLP,"<p>I am looking for <strong>algorithms for distant supervision</strong> (for Natural Language Processing applications). Can you indicate one applyable algorithm or method?</p>
",Multilingual Language Processing & Language Identification,distant supervision algorithm nlp looking algorithm distant supervision natural language processing application indicate one applyable algorithm method
How to make voice assistants to handle scientific terminology?,"<p>As a POC i would like to explore how i can make alexa skill or google home action to understand non English words,like chemical names ,scientific terminology.
Google Cloud Speech to text api allows to add a context but not a full grown solution.
How to approach this problem?</p>
",Multilingual Language Processing & Language Identification,make voice assistant handle scientific terminology poc would like explore make alexa skill google home action understand non english word like chemical name scientific terminology google cloud speech text api allows add context full grown solution approach problem
Natural Language dictionaries in Python,"<p>Does anyone know of a python Natural Language Processing Library or module that I could use to find synonyms (or antonyms, etc ..) of english words ?  </p>
",Multilingual Language Processing & Language Identification,natural language dictionary python doe anyone know python natural language processing library module could use find synonym antonym etc english word
Memory error when creating huge 3D sparse numpy array for one-hot vector,"<p>I am trying to create 3 tensors for my language translation LSTM network.</p>

<pre><code>import numpy as np
Num_samples=50000
Time_step=100
Vocabulary=5000
shape = (Num_samples,Time_step,Vocabulary)
encoder_input_data = np.zeros(shape,dtype='float32')
decoder_input_data = np.zeros(shape,dtype='float32')
decoder_target_data = np.zeros(shape,dtype='float32')
</code></pre>

<p>Obviously, my machine doesn't have enough memory to do so. Since the data is represented as one-hot vectors, it seems using the function <code>csc_matrix()</code> from <code>scipy.sparse</code> will be the solution, as suggested in this <a href=""https://stackoverflow.com/questions/37213750/memoryerror-when-creating-a-very-large-numpy-array"">tread</a> and this <a href=""https://stackoverflow.com/questions/19085012/numpy-memory-error-creating-huge-matrix"">tread</a>.</p>

<p>But after trying the <code>csc_matrix()</code> and <code>crc_matrix()</code>, it seems they only support 2D array.</p>

<p><a href=""https://stackoverflow.com/questions/7685128/sparse-3d-matrix-array-in-python"">Old treads</a> from 6 years ago did talk about this issue, but they are not machine learning orientated.</p>

<p><strong>My question is: Is there any python lib/tool that can help me to create sparse 3D arrays that allows me to store one-hot vectors for machine learning purpose later?</strong></p>
",Multilingual Language Processing & Language Identification,memory error creating huge sparse numpy array one hot vector trying create tensor language translation lstm network obviously machine enough memory since data represented one hot vector seems using function solution suggested trying seems support array href tread year ago talk issue machine learning orientated question python lib tool help create sparse array allows store one hot vector machine learning purpose later
How to relate the language model score of a whole sentence to those of the sentence&#39;s constituents,"<p>I trained a KENLM language model on around 5000 English sentences/paragraphs. I want to query this ARPA model with two or more segments and see if they can be concatenated to form a longer sentence, hopefully more ""grammatical."" Here as follows is the Python code that I have used to get the logarithmic scores - and the ten-based power value - of the segments and the ""sentence."" I have given two examples. Obviously, the sentence in the first example is more grammatical than the one in the second example. However, my question is not about this, but about how to relate the language model score of a whole sentence to those of the sentence's constituents. That is, if the sentence is grammatically better than its constituents.</p>

<pre><code>import math
import kenlm as kl
model = kl.LanguageModel(r'D:\seg.arpa.bin')
print ('************')
sentence = 'Mr . Yamada was elected Chairperson of'
print(sentence)
p1=model.score(sentence)
p2=math.pow(10,p1)
print(p1)
print(p2)
sentence = 'the Drafting Committee by acclamation .'
print(sentence)
p3=model.score(sentence)
p4=math.pow(10,p3)
print(p3)
print(p4)
sentence = 'Mr . Yamada was elected Chairperson of the Drafting Committee by acclamation .'
print(sentence)
p5=model.score(sentence)
p6=math.pow(10,p5)
print(p5)
print(p6)
print ('-------------')
sentence = 'Cases cited in the present volume ix'
print(sentence)
p1=model.score(sentence)
p2=math.pow(10,p1)
print(p1)
print(p2)
sentence = 'Multilateral instruments cited in the present volume xiii'
print(sentence)
p3=model.score(sentence)
p4=math.pow(10,p3)
print(p3)
print(p4)
sentence = 'Cases cited in the present volume ix Multilateral instruments cited in the present volume xiii'
print(sentence)
p5=model.score(sentence)
p6=math.pow(10,p5)
print(p5)
print(p6)
</code></pre>

<ul>
<li>************ Mr . Yamada was elected Chairperson of
-34.0706558228
8.49853715087e-35 the Drafting Committee by acclamation .
-28.3745193481
4.22163470933e-29 Mr . Yamada was elected Chairperson of the Drafting Committee by acclamation .
-55.5128440857
3.07012398337e-56
------------- Cases cited in the present volume ix
-27.7353248596
1.83939558773e-28 Multilateral instruments cited in the present volume xiii
-34.4523620605
3.52888852435e-35 Cases cited in the present volume ix Multilateral instruments cited in the present volume xiii
-60.7075233459
1.9609957573e-61</li>
</ul>
",Multilingual Language Processing & Language Identification,relate language model score whole sentence sentence constituent trained kenlm language model around english sentence paragraph want query arpa model two segment see concatenated form longer sentence hopefully grammatical follows python code used get logarithmic score ten based power value segment sentence given two example obviously sentence first example grammatical one second example however question relate language model score whole sentence sentence constituent sentence grammatically better constituent mr yamada wa elected chairperson e drafting committee acclamation e mr yamada wa elected chairperson drafting committee acclamation e case cited present volume ix e multilateral instrument cited present volume xiii e case cited present volume ix multilateral instrument cited present volume xiii e
How to read values from numbers written as words?,"<p>As we all know numbers can be written either in numerics, or called by their names. While there are a lot of examples to be found that convert 123 into one hundred twenty three, I could not find good examples of how to convert it the other way around.</p>

<p>Some of the caveats:</p>

<ol>
<li>cardinal/nominal or ordinal: ""one"" and ""first""</li>
<li>common spelling mistakes: ""forty""/""fourty""</li>
<li>hundreds/thousands: 2100 -> ""twenty one hundred"" and also ""two thousand and one hundred""</li>
<li>separators: ""eleven hundred fifty two"", but also ""elevenhundred fiftytwo"" or ""eleven-hundred fifty-two"" and whatnot</li>
<li>colloquialisms: ""thirty-something""</li>
<li>fractions: 'one third', 'two fifths'</li>
<li>common names: 'a dozen', 'half'</li>
</ol>

<p>And there are probably more caveats possible that are not yet listed.
Suppose the algorithm needs to be very robust, and even understand spelling mistakes.</p>

<p>What fields/papers/studies/algorithms should I read to learn how to write all this?
Where is the information?</p>

<blockquote>
  <p>PS: My final parser should actually understand 3 different languages, English, Russian and Hebrew. And maybe at a later stage more languages will be added. Hebrew also has male/female numbers, like ""one man"" and ""one woman"" have a different ""one"" ‚Äî ""ehad"" and ""ahat"". Russian also has some of its own complexities.</p>
</blockquote>

<p>Google does a great job at this. For example:</p>

<p><a href=""http://www.google.com/search?q=two+thousand+and+one+hundred+plus+five+dozen+and+four+fifths+in+decimal"" rel=""noreferrer"">http://www.google.com/search?q=two+thousand+and+one+hundred+plus+five+dozen+and+four+fifths+in+decimal</a></p>

<p>(the reverse is also possible <a href=""http://www.google.com/search?q=999999999999+in+english"" rel=""noreferrer"">http://www.google.com/search?q=999999999999+in+english</a>)</p>
",Multilingual Language Processing & Language Identification,read value number written word know number written either numerics called name lot example found convert one hundred twenty three could find good example convert way around caveat cardinal nominal ordinal one first common spelling mistake forty fourty hundred thousand twenty one hundred also two thousand one hundred separator eleven hundred fifty two also elevenhundred fiftytwo eleven hundred fifty two whatnot colloquialism thirty something fraction one third two fifth common name dozen half probably caveat possible yet listed suppose algorithm need robust even understand spelling mistake field paper study algorithm read learn write information p final parser actually understand different language english russian hebrew maybe later stage language added hebrew also ha male female number like one man one woman different one ehad ahat russian also ha complexity google doe great job example reverse also possible
How to construct a clean vocabulary from training text?,"<p>I am following the neural machine translation tutorial <a href=""https://www.tensorflow.org/tutorials/seq2seq"" rel=""nofollow noreferrer"">here</a> and notice that the datasets they use provide a <a href=""https://nlp.stanford.edu/projects/nmt/data/wmt15.en-cs/vocab.1K.en"" rel=""nofollow noreferrer"">clean vocab file</a>. But when I come across a dataset (e.g. <a href=""http://statmt.org/wmt17/translation-task.html"" rel=""nofollow noreferrer"">Europarl v8</a>) that does not provide a vocab file, I need to construct a vocabulary myself using the following function.</p>

<pre><code>def construct_vocab_from_file(file, vocab_file):
    # Read file, tokenize it and then sort it
    with open(file, 'r') as f:
        raw_data = f.read()
        tokens = nltk.wordpunct_tokenize(raw_data)
        words = [w.lower() for w in tokens]
        vocab = sorted(set(words))

    # Write vocab to file
    with open(vocab_file, 'w') as f:
        for w in vocab:
            f.write(w + ""\n"")
</code></pre>

<p>However, the vocabulary constructed this way looks a little bit messy. <a href=""https://i.sstatic.net/KfvzJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KfvzJ.png"" alt=""enter image description here""></a></p>

<p>The left one is from the clean vocab file while the right one with the black background (numbers are line number) is from the vocabulary constructed by me. This does not make me feel comfortable especially more than half of the vocabulary consist of these kind of special characters or numbers (e.g. 0, 00, 000, 0000, 0000003).</p>

<p>So my questions are:</p>

<p>1) Is this problematic?</p>

<p>2) Should I process it further and how?</p>
",Multilingual Language Processing & Language Identification,construct clean vocabulary training text following neural machine translation tutorial notice datasets use provide clean vocab file come across dataset e g europarl v doe provide vocab file need construct vocabulary using following function however vocabulary constructed way look little bit messy left one clean vocab file right one black background number line number vocabulary constructed doe make feel comfortable especially half vocabulary consist kind special character number e g question problematic process
"How to work out next most probable letter in a sequence, Natural Language Processing","<p>I have the sequence <code>AGCTTTCGA</code>and am asked to first list all unigrams and bigrams. As far as I am aware these are correct:</p>

<p>Unigrams:</p>

<pre><code>{A, G, C, T, T, T, C, G, A}
</code></pre>

<p>Bigrams:</p>

<pre><code>{AG, GC, CT, TT, TT, TC, CG, GA}
</code></pre>

<p>Now I need to calculate the next most probable character in this sequence for both the unigram and bigram mode. I know the formula for likelihood is </p>

<p>p(w2|w1) = count(w1, w2) / count(w1)</p>

<p>but am unsure how I would use this here?</p>
",Multilingual Language Processing & Language Identification,work next probable letter sequence natural language processing sequence asked first list unigrams bigram far aware correct unigrams bigram need calculate next probable character sequence unigram bigram mode know formula likelihood p w w count w w count w unsure would use
Is it possible to check if a word is really an english word via regex?,"<p>When I say english I really mean vs gobbledy gook. I'm not trying to filter out maitre'd or espanol or whatever.</p>

<p>So basically I'm trying to test whether a word is made of entirely of pronounceable syllables.</p>

<p>So here would be a regex:</p>

<pre><code>if re.match(r'^([^aeiouy]{1,3}[aeiouy]{1,3}[^aeiouy]{1,3}|[aeiouy]{1,3}[^aeiouy]{1,3})+
    print ""gobbledy gook!!!""
</code></pre>

<p>What its checking for: C=consonant V=vowel</p>

<p>CVC or VC groups of characters. groups are 1-3 characters in length</p>

<p>Does this make sense?,the_word) is None:
<em>xCodexBlockxPlacexHolderx</em></p>

<p>What its checking for: C=consonant V=vowel</p>

<p>CVC or VC groups of characters. groups are 1-3 characters in length</p>

<p>Does this make sense?</p>
",Multilingual Language Processing & Language Identification,possible check word really english word via regex say english really mean v gobbledy gook trying filter maitre espanol whatever basically trying test whether word made entirely pronounceable syllable would regex checking c consonant v vowel cvc vc group character group character length doe make sense word none xcodexblockxplacexholderx checking c consonant v vowel cvc vc group character group character length doe make sense
Is nltk wordnet lemmatizer language independent?,"<p>Is it true that <a href=""http://www.nltk.org/_modules/nltk/stem/wordnet.html"" rel=""nofollow noreferrer"">nltk's wordnet lemmatizer</a> does not depend on the language of the input text ? Would I use the same sequence of commands:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; print(wnl.lemmatize('dogs'))
dog
&gt;&gt;&gt; print(wnl.lemmatize('churches'))
church
&gt;&gt;&gt; print(wnl.lemmatize('aardwolves'))
aardwolf
&gt;&gt;&gt; print(wnl.lemmatize('abaci'))
abacus
&gt;&gt;&gt; print(wnl.lemmatize('hardrock'))
hardrock
</code></pre>

<p>for both english and french for instance ?</p>
",Multilingual Language Processing & Language Identification,nltk wordnet lemmatizer language independent true nltk wordnet lemmatizer doe depend language input text would use sequence command english french instance
Natural Programming Language.... what would you like to see?,"<p>I am looking at writing a compiler and after I complete something in a ""C"" style I am looking at adapting it to other models.  What are some syntactical constructs you would expect to see in a ""natural"" programming language?  </p>

<p>The target platform for this compiler will be the CLR and I am currently using Oslo+MGrammar for the lexer/parser (as you can probably tell this is really just an excuse to play)</p>

<p>One of the goals of my project would be to allow programming to feel more like a conversation than structured syntax and demands.</p>

<p>Guess I should extend this out a little.  One of the ideas I am working with is having a class declaration read like a paragraph. </p>

<pre><code>    A Dog is a mammal.  It may Bark and Run.  To Run it
uses its feet to move forward. It does Lay.
</code></pre>

<p>...would translate too...    </p>

<pre><code>public class Dog : Mammal{

    public Feet Feet { get; set;}

    public virtual void Bark() {}
    public virtual void Run() {
        this.Feet.MoveForward();
    }
    public void Lay(){}
}
</code></pre>
",Multilingual Language Processing & Language Identification,natural programming language would like see looking writing compiler complete something c style looking adapting model syntactical construct would expect see natural programming language target platform compiler clr currently using oslo mgrammar lexer parser probably tell really excuse play one goal project would allow programming feel like conversation structured syntax demand guess extend little one idea working class declaration read like paragraph would translate
What is a chunker in Natural Language Processing?,"<p>Does anyone know what is a chunker in the context of text processing and what is it's usage?</p>
",Multilingual Language Processing & Language Identification,chunker natural language processing doe anyone know chunker context text processing usage
"machine-learning, artificial-intelligence and computational-linguistics","<p>I would love to talk to people who have experience in machine-learning, computational-linguistics or artificial-intelligence in general but by the following example:</p>

<p>‚Ä¢   Which existing software would you apply for a manageable attempt building something like google translate by statistic linguistic, machine learning? 
<em>(Don‚Äôt get me wrong I don‚Äôt want to just do this, but solely trying to draw a conceptional framework for something most complex in this field, what would you think of if you had the chance to lead a team going to realize such...)</em></p>

<p>‚Ä¢   Which existent database(s)?  Which database technology to store results when those are terabytes of data</p>

<p>‚Ä¢   Which programming languages besides C++?</p>

<p>‚Ä¢   Apache mahunt?</p>

<p>‚Ä¢   And, how would those software components work together to power the effort as a whole? </p>
",Multilingual Language Processing & Language Identification,machine learning artificial intelligence computational linguistics would love talk people experience machine learning computational linguistics artificial intelligence general following example existing software would apply manageable attempt building something like google translate statistic linguistic machine learning get wrong want solely trying draw conceptional framework something complex field would think chance lead team going realize existent database database technology store result terabyte data programming language besides c apache mahunt would software component work together power effort whole
"In Natural language processing, how to find data set for scientific papers","<p>i'm new in data science and trying to make application that classify the scientific papers to (AI,Machine Learning, NLP,...) 
i spend a lot of time trying to find data set for scientific papers but not found.
any help to find the data set? </p>
",Multilingual Language Processing & Language Identification,natural language processing find data set scientific paper new data science trying make application classify scientific paper ai machine learning nlp spend lot time trying find data set scientific paper found help find data set
"Sentiment Analysis API for Multiple Dimensions i.e. Positivity, Emotionality etc","<p>I have big chucks of text in English (avg length 800 words) which I would like to evaluate with a good and reliable sentiment analysis API. </p>

<p>Some threads seem to suggest APIs like <a href=""http://www.alchemyapi.com/api/sentiment/"" rel=""nofollow"">Alchemy</a> but I would like an evaluation of the sentiment along <strong>multiple dimensions</strong> and not just a single score. Example of such dimensions could be Positivity and Emotionality etc.</p>

<p>Do you know any APIs that would provide such more elaborate results?</p>
",Multilingual Language Processing & Language Identification,sentiment analysis api multiple dimension e positivity emotionality etc big text english avg length word would like evaluate good reliable sentiment analysis api thread seem suggest apis like alchemy would like evaluation sentiment along multiple dimension single score example dimension could positivity emotionality etc know apis would provide elaborate result
%in% is returning FALSE when I know it&#39;s TRUE,"<p>Relevant files: </p>

<p><a href=""https://docs.google.com/spreadsheets/d/1iHo3WTV-WNF7m4Q3cTImAvf6dXSJITXpxEcfTPUWF7M/edit?usp=sharing"" rel=""nofollow noreferrer"">biggie</a></p>

<p><a href=""https://docs.google.com/spreadsheets/d/1hOCbTdfeCap-glmRj-Zt7l-qTmnkPMOAbQKQbw77MEo/edit?usp=sharing"" rel=""nofollow noreferrer"">positive</a></p>

<p>I'm working on some natural language processing and am trying to check if a word in one list is in another using the %in% check. Problem is, it returns everything as FALSE when I know there should be at least a few TRUE returns. I'm wondering if the issue is with the type of objects I am working with? Though when I run tests everything is a character object so I thought this shouldn't be an issue. Here is my code: </p>

<pre><code>library(dplyr)
library(tokenizers)
library(tidytext)

biggie &lt;- read.csv(""C:/Users/My.Name/Desktop/biggie.csv"", stringsAsFactors=FALSE)

colnames(biggie)[1] &lt;- 'biggie'



bigsplit &lt;- biggie %&gt;% 
  unnest_tokens(word, biggie)

pos &lt;- read.csv(""C:/Users/My.Name/Desktop/positive.csv"", stringsAsFactors = FALSE)

positive &lt;- function(data){
  pos_count &lt;- 0
  for(i in 1:nrow(data)){
    if (data[i,1] %in% pos){
      pos_count = pos_count + 1
    }
  }
  return(pos_count/nrow(data)
}
</code></pre>

<p>Here I found a workaround, but I feel like it adds unnecessary loops/steps into the function and takes a lot more computing power than I would like it to: </p>

<pre><code>#Tests
bigsplit[1,1] = ""abound""
bigsplit[1,1] %in% pos #Returns FALSE, but I would expect TRUE
bigsplit[1,1] %in% pos[1,1] #Returns TRUE

#NEW FUNCTION
positive &lt;- function(data){
  pos_count = 0
  for(i in 1:nrow(data)){
    match_this &lt;- data[i,1]
    for(i in 1:nrow(pos)){
      if(match_this %in% pos[i,1]){
        pos_count &lt;- pos_count + 1
      }
    }
  }
  return(pos_count/nrow(data))
}
</code></pre>

<p>If anyone has any tips on these issues, I would really appreciate hearing them. Thanks!</p>
",Multilingual Language Processing & Language Identification,returning false know true relevant file biggie positive working natural language processing trying check word one list another using check problem return everything false know least true return wondering issue type object working though run test everything character object thought issue code found workaround feel like add unnecessary loop step function take lot computing power would like anyone ha tip issue would really appreciate hearing thanks
How does Google Translate provide offline translations?,"<p>The thing I am specifically puzzled by is how each language corresponds to a single translation file. This means that there must be a common intermediate translation that connects the different languages to solve the NxM problem. How exactly does it work?</p>
",Multilingual Language Processing & Language Identification,doe google translate provide offline translation thing specifically puzzled language corresponds single translation file mean must common intermediate translation connects different language solve nxm problem exactly doe work
how to remove translation error in apertium language pair?,"<p>i m translating words in terminal by this command <br><code>echo ÿ®ÿßŸæ | apertium -d . urd-pan_Arab</code> <br> and it is giving me  <br>#Ÿæ€åŸà <br>which is correct but when i run <br> <code>echo ⁄©ÿ®⁄æ€å | apertium -d . urd-pan_Arab</code> <br>it is giving me same word again as if there is no word in dict file <br> *⁄©ÿ®⁄æ€å <br>instead of  #⁄©ÿØ€å
<br> this is repository (<a href=""https://github.com/hsumerf/apertium-urd-pan"" rel=""nofollow noreferrer"">https://github.com/hsumerf/apertium-urd-pan</a>) and words are on line no. 266 &amp; 267 in file  <strong>apertium-urd-pan/apertium-urd-pan.urd-pan_Arab.dix</strong> , how to overcome this problem?</p>
",Multilingual Language Processing & Language Identification,remove translation error apertium language pair translating word terminal command giving correct run giving word word dict file instead repository word line file apertium urd pan apertium urd pan urd pan arab dix overcome problem
LUIS - is it possible to &quot;merge&quot; two datetimeV2 entities,"<p>We have English LUIS model that has active entities of type <code>datetimeV2</code></p>

<p>It works fine but in sentennce </p>

<blockquote>
  <p>today 26th march</p>
</blockquote>

<p>there are two entities detected while only one is desired - see below.</p>

<p>Is it possible to detect this sample sentence as one entity? E.g. using Composite entity?</p>

<hr>

<pre><code>""entities"": [
{
    ""entity"": ""today"",
    ""type"": ""builtin.datetimeV2.date"",
    ""resolution"": ...
},
{
    ""entity"": ""26th april"",
    ""type"": ""builtin.datetimeV2.date"",
    ""resolution"": ...
    }   
]
</code></pre>
",Multilingual Language Processing & Language Identification,luis possible merge two datetimev entity english luis model ha active entity type work fine sentennce today th march two entity detected one desired see possible detect sample sentence one entity e g using composite entity
Google NLP API - Companies identified as &quot;Unknown&quot;,"<p>I try to identify entities in French newspaper articles and very often organizations, even famous ones such as Apple, are identified as ""Unknown"", whereas the linked Wikipedia page is correct.</p>

<p>You can try using the content of this article:
<a href=""http://www.lefigaro.fr/secteur/high-tech/2018/02/02/32001-20180202ARTFIG00030-trimestre-record-pour-apple-grace-a-ses-prix-eleves.php"" rel=""nofollow noreferrer"">http://www.lefigaro.fr/secteur/high-tech/2018/02/02/32001-20180202ARTFIG00030-trimestre-record-pour-apple-grace-a-ses-prix-eleves.php</a></p>

<p>Does anybody know how to make sure companies are correctly recognized as ""Organization"" entities?</p>
",Multilingual Language Processing & Language Identification,google nlp api company identified unknown try identify entity french newspaper article often organization even famous one apple identified unknown whereas linked wikipedia page correct try using content article doe anybody know make sure company correctly recognized organization entity
compiling error in apertium language pair,"<p>I was making english-spanish language pair by following these <a href=""http://wiki.apertium.org/wiki/How_to_bootstrap_a_new_pair"" rel=""nofollow noreferrer"">http://wiki.apertium.org/wiki/How_to_bootstrap_a_new_pair</a> apertium instructions but after scripting pair when I go: <br><code>./autogen.sh --with-lang1=../apertium-en --with-lang2=../apertium-es</code><br> With this command I got this error, how can I solve this problem?
<br><code>configure: WARNING: apertium-en looks like an apertium dependency, but couldn't find ../apertium-en/apertium-en.pc.in -- wrong directory, or not compiled yet?
configure: error: Could not find sources dir for apertium-en (AP_SRC1=""../apertium-en"")</code></p>
",Multilingual Language Processing & Language Identification,compiling error apertium language pair wa making english spanish language pair following apertium instruction scripting pair go command got error solve problem
How to build an accurate translation engine?,"<p>I found a formula few months ago, myself to translate any source language (computer characters) to destination (computer characters). Using Lua (desk top users) and C++ class (for native access) so that i can embed it in Web Browser etc etc. I am wondering if we have already better something for this in C++ or Lua.</p>

<p>Mine sometimes its really not translating grammars correctly or even rules, before building it i thought mine would be a best way to complete, but its taking way to long now, and i am afraid it may become wrong implementation. Now i want to check out others and compare mine.</p>

<p>I used Google translate or others which is not my target, i was building a translator engine (like google or others), where someone can put there dictionary and create rules.</p>

<p>Is there any existing translation framework or libraries (OpenCOG or Moses) to do Source language to Destination ? 
example: Arabic to Chinese or English to Japanese ? Or What else Google/others using ?</p>

<p>Any suggestion would be appreciated</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,build accurate translation engine found formula month ago translate source language computer character destination computer character using lua desk top user c class native access embed web browser etc etc wondering already better something c lua mine sometimes really translating grammar correctly even rule building thought mine would best way complete taking way long afraid may become wrong implementation want check others compare mine used google translate others target wa building translator engine like google others someone put dictionary create rule existing translation framework library opencog moses source language destination example arabic chinese english japanese else google others using suggestion would appreciated thanks advance
Finding path in a matrix for sentence alignment,"<p>Following problem: I am working with python and I have two texts in different languages (texta and textb) which have both a lot of sentences. The aim is to align each sentence in texta with a sentence of textb. The overall length is roughly equal, but the number of the sentences is not. It is somewhere about 1:1.3, so at times one sentence of texta might get two sentences of textb. The order of the sentences in both texts needs to be preserved. 
Now I have a CNN that outputs a matrix of all the sentences and a propability score of them being a translation of each other. 
This might in a simplified way look like this:</p>

<pre><code>[0.5,0.2,.0.0]
[0.1,0.6,0.4]
[0.2,0.3,0.8]
</code></pre>

<p>The accuracy of these predictions is about 50%. So when I just select the best result of each row I get a precision of 50%. 
Now one would expect the ideal path to be more or less diagonal, since both texts will contain more or less the same information at the same place. However in reality this is not always the case. A sentence can be short, a sentence can be long, sometimes a paragraph is missing (translation mistake). So I wonder what is a nice approach to solve this problem. My first idea is to just measure the distance to the diagonal center and give a penalty that reduces the propability score as further as an element is away from that center. However this approach is weak in the case that a paragraph is missing and thus a longer sequence of sentences lies further away from the center. So I am looking for a kind of optimization technique, but I do not know where to start yet.</p>
",Multilingual Language Processing & Language Identification,finding path matrix sentence alignment following problem working python two text different language texta textb lot sentence aim align sentence texta sentence textb overall length roughly equal number sentence somewhere time one sentence texta might get two sentence textb order sentence text need preserved cnn output matrix sentence propability score translation might simplified way look like accuracy prediction select best result row get precision one would expect ideal path le diagonal since text contain le information place however reality always case sentence short sentence long sometimes paragraph missing translation mistake wonder nice approach solve problem first idea measure distance diagonal center give penalty reduces propability score element away center however approach weak case paragraph missing thus longer sequence sentence lie away center looking kind optimization technique know start yet
Heuristics for determining whether something is a &quot;word&quot; or random data?,"<p>I am writing a web crawler in python that downloads a list of URLS, extracts all visible text from the HTML, tokenizes the text (using nltk.tokenize) and then creates a positional inverted index of words in each document for use by a search feature. </p>

<p>However, right now, the index contains a bunch of useless entries like:</p>

<p>1) <code>//roarmag.org/2015/08/water-conflict-turkey-middle-east/</code> </p>

<p>2) <code>‚Äî‚Äî‚Äî-</code></p>

<p>3) <code>ykgnwym+ccybj9z1cgzqovrzu9cni0yf7yycim6ttmjqroz3wwuxiseulphetnu2</code> </p>

<p>4) <code>iazl+xcmwzc3da==</code></p>

<p>Some of these, like #1, are where URLs appear in the text. Some, like #3, are excerpts from PGP keys, or other random data that is embedded in the text. </p>

<p>I am trying to understand how to filter out useless data like this.  But I don't just want to keep words that I would find in an English dictionary, but also things like names, places, nonsense words like ""Jabberwocky"" or ""Rumpelstiltskin"", acronyms like ""TANSTAAFL"", obscure technical/scientific terms, etc ... </p>

<p>That is, I'm looking for a way to heuristically strip out strings that are ""jibberish"". (1) exceedingly ""long"" (2) filled with a bunch of punctuation (3) composed of random strings of characters like afhdkhfadhkjasdhfkldashfkjahsdkfhdsakfhsadhfasdhfadskhkf ... I understand that there is no way to do this with 100% accuracy, but if I could remove even 75% of the junk I'd be happy.</p>

<p>Are there any techniques that I can use to separate ""words"" from junk data like this? </p>
",Multilingual Language Processing & Language Identification,heuristic determining whether something word random data writing web crawler python downloads list url extract visible text html tokenizes text using nltk tokenize creates positional inverted index word document use search feature however right index contains bunch useless entry like like url appear text like excerpt pgp key random data embedded text trying understand filter useless data like want keep word would find english dictionary also thing like name place nonsense word like jabberwocky rumpelstiltskin acronym like tanstaafl obscure technical scientific term etc looking way heuristically strip string jibberish exceedingly long filled bunch punctuation composed random string character like afhdkhfadhkjasdhfkldashfkjahsdkfhdsakfhsadhfasdhfadskhkf understand way accuracy could remove even junk happy technique use separate word junk data like
Python module with access to english dictionaries including definitions of words,"<p>I am looking for a python module that helps me get the definition(s) from an english dictionary for a word.</p>

<p>There is of course <code>enchant</code>, which helps me check if the word exists in the English language, but it does not provide definitions of them (at least I don't see anything like that in the docs)</p>

<p>There is also WordNet, which is accessible with NLTK. It has definitions and even sample sentences, but WordNet does not contain all English words. Common words like ""how"", ""I"", ""You"", ""should"", ""could""... are not part of WordNet.</p>

<p>Is there any python module that gives access to a full english dictionary including definitions of words?</p>
",Multilingual Language Processing & Language Identification,python module access english dictionary including definition word looking python module help get definition english dictionary word course help check word exists english language doe provide definition least see anything like doc also wordnet accessible nltk ha definition even sample sentence wordnet doe contain english word common word like could part wordnet python module give access full english dictionary including definition word
Rouge-N precision score for multiple reference summaries,"<p>I am writing the code for ROUGE-N score used in Machine Translation and Text Summarization domains. </p>

<p>The <a href=""http://www.aclweb.org/anthology/W04-1013"" rel=""nofollow noreferrer"">original paper for ROUGE Score</a> says that ROUGE-N is a recall score . The paper also mentions the formula for calculating the ROUGE-N score for a candidate summary when we have multiple reference summaries.</p>

<p>However after going through few blogs I found that , there is something called the ROUGE-N precision score which is equal to the fraction of n-grams present in the candidate summary that matches with reference summary. </p>

<p>However I have got no idea as to how to tweak this formula while we are having multiple reference summaries instead of one. </p>
",Multilingual Language Processing & Language Identification,rouge n precision score multiple reference summary writing code rouge n score used machine translation text summarization domain original paper rouge score say rouge n recall score paper also mention formula calculating rouge n score candidate summary multiple reference summary however going blog found something called rouge n precision score equal fraction n gram present candidate summary match reference summary however got idea tweak formula multiple reference summary instead one
Software Product Feature extraction from requirement text using Natural Language Processing,"<p>I am creating an application that will extract software product features from natural language text and this is mainly based on Information Extraction. Here feature is ""a user visible aspect in a product"". I have read few research papers and what I have gathered is that they use parts of speech to start of this process.</p>

<p>In feature extraction what I am trying to do is to develop the relation of Actor-Action-Object like ""System supports Credit Card"".</p>

<p>I am using Stanford's Open IE for this but results are not coming accurate. My question is that what Information Extraction technique I should use to get good results and some one has already suggested me to use RDF graphs.</p>
",Multilingual Language Processing & Language Identification,software product feature extraction requirement text using natural language processing creating application extract software product feature natural language text mainly based information extraction feature user visible aspect product read research paper gathered use part speech start process feature extraction trying develop relation actor action object like system support credit card using stanford open ie result coming accurate question information extraction technique use get good result one ha already suggested use rdf graph
Can the ANEW dictionary be used for sentiment analysis in quanteda?,"<p>I am trying to find a way to implement the Affective Norms for English Words (in dutch) for a longitudinal sentiment analysis with Quanteda. What I ultimately want to have is a ""mean sentiment"" per year in order to show any longitudinal trends.</p>

<p>In the data-set all words a scored on a 7-point Likert-scale by 64 coders on four categories, which provides a mean for each word. What I want to do is take one of the dimensions and use this to analyse changes in emotions over time. I realise that Quanteda has a function for implementing the LIWC-dictionary, but I would prefer using the open-source ANEW-data if possible.</p>

<p>Essentially, I need help with the implementation because I am new to coding and R</p>

<p>The ANEW file looks something like this (in .csv):</p>

<p>WORD/SCORE: cancer: 1.01, potato: 3.56, love: 6.56</p>
",Multilingual Language Processing & Language Identification,anew dictionary used sentiment analysis quanteda trying find way implement affective norm english word dutch longitudinal sentiment analysis quanteda ultimately want mean sentiment per year order show longitudinal trend data set word scored point likert scale coder four category provides mean word want take one dimension use analyse change emotion time realise quanteda ha function implementing liwc dictionary would prefer using open source anew data possible essentially need help implementation new coding r anew file look something like csv word score cancer potato love
How to use OpenNLP with Java?,"<p>I want to POStag an English sentence and do some processing. I would like to use openNLP. I have it installed </p>

<p>When I execute the command</p>

<pre><code>I:\Workshop\Programming\nlp\opennlp-tools-1.5.0-bin\opennlp-tools-1.5.0&gt;java -jar opennlp-tools-1.5.0.jar POSTagger models\en-pos-maxent.bin &lt; Text.txt
</code></pre>

<p>It gives output POSTagging the input in Text.txt</p>

<pre><code>    Loading POS Tagger model ... done (4.009s)
My_PRP$ name_NN is_VBZ Shabab_NNP i_FW am_VBP 22_CD years_NNS old._.


Average: 66.7 sent/s
Total: 1 sent
Runtime: 0.015s
</code></pre>

<p>I hope it installed properly?</p>

<p>Now how do i do this POStagging from inside a java application? I have added the openNLPtools, jwnl, maxent jar to the project but how do i invoke the POStagging?</p>
",Multilingual Language Processing & Language Identification,use opennlp java want postag english sentence processing would like use opennlp installed execute command give output postagging input text txt hope installed properly postagging inside java application added opennlptools jwnl maxent jar project invoke postagging
Difference between Fasttext .vec and .bin file,"<p>I recently downloaded fasttext pretrained model for english. I got two files:</p>

<ol>
<li>wiki.en.vec</li>
<li>wiki.en.bin</li>
</ol>

<p>I am not sure what is the difference between the two files?</p>
",Multilingual Language Processing & Language Identification,difference fasttext vec bin file recently downloaded fasttext pretrained model english got two file wiki en vec wiki en bin sure difference two file
Cleaning the data from csv file,"<p>I'm doing the sentiment analysis on crpytocurrency. My job is to clean the data from the csv file. The data was generated(From Twitter) and saved in a csv file. Before doing the sentiment Analysis part. I have to clean the data .For example, Delete the punctuations, the URLs, put the test in a lower case. Those are the tweets. </p>

<p>## I already imported useful libraries for example NLTK(Natural language processing), pandas, numpy, and others. </p>

<p>Here's the output for the 'Tweets' Column.</p>

<pre><code>   ctweet['Tweets'][0:6]



 Out[5]:


    0    RT @TheLTCnews: The @LTCFoundation has publish...
    1    RT @WildchildSings: ""https:/ "" + /t.co/""FZrGw6xsZU ac...""
    2    RT @HODL_Whale: 5 days until #LitePay launches...
    3    LTC to USD price $211.92 ""https:/"" + /t.co/""CFjg1mIg...""
    4    LTC to BTC price B0.020218 ""https:/"" +/t.co/""XPL8NI...""
    5    LTC to GBP price ¬£151.89 ""https:/"" +/t.co/""iOIbhgyd...""
    6    Litecoin dropped into the bear zone as sugges...
    Name: Tweets, dtype: object

# the output contains url. Because stackoverflow won't allow me to post the url. I have to change the method for url like adding ""quotes"" and ""//"".  
</code></pre>

<p>My next task was to clean the data. Here's the preprocessing code.</p>

<pre><code>#Preprocessing del RT @blablabla:
ctweet['tweetos'] = '' 

#add tweetos first part
for i in range(len(ctweet['Tweets'])):
    try:
        ctweet['tweetos'][i] = ctweet['Tweets'].str.split(' ')[i][0]
    except AttributeError:    
        ctweet['tweetos'][i] = 'other'

        #Preprocessing tweetos. select tweetos contains 'RT @'
        for i in range(len(ctweet['Tweets'])):
            if ctweet['tweetos'].str.contains('@')[i]  == False:
                ctweet['tweetos'][i] = 'other'

        # remove URLs, RTs, and twitter handles
        for i in range(len(ctweet['Tweets'])):
            ctweet['Tweets'][i] = "" "".join([word for word in ctweet['Tweets'][i].split()
                                        if 'http' not in word and '@' not in word and '&lt;' not in word])

  ctweet['Tweets'][0]
</code></pre>

<p>The code above it will delete the punctuation, the urls, put test in a lower case, extract username for examples. When I run that code it gives an error.</p>

<pre><code>TypeErrorTraceback (most recent call last)
&lt;ipython-input-3-8254e078073a&gt; in &lt;module&gt;()
      5 for i in range(len(ctweet['Tweets'])):
      6     try:
----&gt; 7         ctweet['tweetos'][i] = ctweet['Tweets'].str.split(' ')[i][0]
      8     except AttributeError:
      9         ctweet['tweetos'][i] = 'other'

TypeError: 'float' object has no attribute '__getitem__'
</code></pre>

<p>What does that error mean? How can I solve this problem. Im using Jupyter Notebook 5.4.1</p>

<h2>Update part</h2>

<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-7-bb6b24f62739&gt; in &lt;module&gt;()
     16 # remove URLs, RTs, and twitter handles
     17 for i in range(len(ctweet['Tweets'])):
---&gt; 18     ctweet['Tweets'][i] = "" "".join([word for word in ctweet['Tweets'][i].split()
     19                                 if 'http' not in word and '@' not in word and '&lt;' not in word])
     20 

AttributeError: 'float' object has no attribute 'split'
</code></pre>
",Multilingual Language Processing & Language Identification,cleaning data csv file sentiment analysis crpytocurrency job clean data csv file data wa generated twitter saved csv file sentiment analysis part clean data example delete punctuation url put test lower case tweet already imported useful library example nltk natural language processing panda numpy others output tweet column next task wa clean data preprocessing code code delete punctuation url put test lower case extract username example run code give error doe error mean solve problem im using jupyter notebook update part
Is there a programming language with semantics close to English?,"<p>Most languages allow to 'tweek' to certain extend parts of the syntax (<a href=""https://stackoverflow.com/questions/829125/c-loop-macros"">C++</a>,<a href=""http://msdn.microsoft.com/en-us/library/aa664765(v=vs.71).aspx"" rel=""nofollow noreferrer"">C#</a>) and/or semantics that you will be using in your code (<a href=""http://news.ycombinator.com/item?id=1945282"" rel=""nofollow noreferrer"">Katahdin</a>, <a href=""http://en.wikipedia.org/wiki/Lua_(programming_language)"" rel=""nofollow noreferrer"">lua</a>). But I have not heard of a language that can just completely define how your code will look like. So  isn't there some language which already exists that has such capabilities to override <strong>all</strong> syntax &amp; define semantics ? </p>

<p>Example of what I want to do is basically from the C# code below:</p>

<pre><code>foreach(Fruit fruit in Fruits)
{
  if(fruit is Apple)
  {
    fruit.Price =  fruit.Price/2;
  }
}
</code></pre>

<p>I want do be able to to write the above code in my perfect language like this:</p>

<pre><code>Check if any fruits are Macintosh apples and discount the price by 50%.
</code></pre>

<p>The advantages that come to my mind looking from a coder's perspective in this ""imaginary"" language are:</p>

<ol>
<li>It's very clear what is going on (self descriptive) - it's plain English after all even kid would understand my program</li>
<li>Hides all complexities which I have to write in C#. But why should I care to learn that
if statements, arithmetic operators etc since there are already implemented</li>
</ol>

<p>The disadvantages that I see for a coder who will maintain this program are:</p>

<ol>
<li>Maybe you would express  this program differently from me so you may not get all the
information that I've expressed in my sentence</li>
<li>Programs can be quite verbose and hard to debug but  if possible to even proximate this type of syntax above maybe more people would start programming right? That would be amazing I think. I can go to work and just write an essay to draw a square on a winform like this: </li>
</ol>

<p><code>Create a form called MyGreetingForm. Draw a square with in the middle of 
MyGreetingFormwith a side of 100 points. In the middle of the square write ""Hello! Click here to continue"" in Arial font.</code></p>

<p>In the above code the parser must basically guess that I want to use
    the unnamed square from the previous sentence, it'd be hard to write such a smart parser I guess, yet it's so simple what I want to do.</p>

<p><code>If the user clicks on square in the middle of MyGreetingForm show MyMainForm.</code>  </p>

<p>In the above code 'basically' the compiler must: 1)generate an event handler 2) check if there is any square in the middle of the form and if there is - 3) hide the form and show another form  </p>

<p>It looks very hard to do but it doesn't look impossible IMO to me at least approximate this (I can personally generate a parser to perform the 3 steps above np &amp; it's basically the same that it has to do any way when you add even in c# <code>a.MyEvent=+handler;</code> so I don't see a problem here) so I'm thinking maybe somebody already did something like this ? Or is there some practical burden of complexity to create such a 'essay style' programming language which I can't see ? I mean what's the worse that can happen if the parser is not that good? - your program will crash so you have to re-word it:)</p>
",Multilingual Language Processing & Language Identification,programming language semantics close english language allow tweek certain extend part syntax c semantics using code katahdin lua heard language completely define code look like language already exists ha capability override syntax define semantics example want basically c code want able write code perfect language like advantage come mind looking coder perspective imaginary language clear going self descriptive plain english even kid would understand program hide complexity write c care learn statement arithmetic operator etc since already implemented disadvantage see coder maintain program maybe would express program differently may get information expressed sentence program quite verbose hard debug possible even proximate type syntax maybe people would start programming right would amazing think go work write essay draw square winform like code parser must basically guess want use unnamed square previous sentence hard write smart parser guess yet simple want code basically compiler must generate event handler check square middle form hide form show another form look hard look impossible imo least approximate personally generate parser perform step np basically ha way add even c see problem thinking maybe somebody already something like practical burden complexity create essay style programming language see mean worse happen parser good program crash word
Where can I find a list of english part of speech constraints?,"<p>I'm looking for a list of English part of speech sequencing rules (e.g. ""a determiner cannot be followed by a verb""). 
Thought it would be easy but I couldn't find an actual list of more than several examples.
Any ideas?</p>

<p>Thanks. </p>
",Multilingual Language Processing & Language Identification,find list english part speech constraint looking list english part speech sequencing rule e g determiner followed verb thought would easy find actual list several example idea thanks
Is natural language Turing complete?,"<p>I'm pretty sure a human language (e.g. English) is powerful enough to simulate a Turing machine, which would make it Turing complete. However, that would imply natural languages are no more or less expressive than programming languages, which seems questionable.</p>

<p>Is natural language Turing complete?</p>
",Multilingual Language Processing & Language Identification,natural language turing complete pretty sure human language e g english powerful enough simulate turing machine would make turing complete however would imply natural language le expressive programming language seems natural language turing complete
Issue in Arabic preprocessing techniques,"<p>I'm trying to apply preprocessing techniques on Arabic string list but I'm not getting the correct results. </p>

<p>This is my code:</p>

<pre><code>import re
import sys
import itertools
from nltk.tokenize import TweetTokenizer
from nltk.stem.isri import ISRIStemmer

foo = 'ÿßŸÑÿ≥ŸÄŸÑÿßÿßŸÖ ÿπŸÄŸÄŸÑŸäŸÉŸÖ 32 Ÿáÿ∞Ÿá ÿ™ÿ¨Ÿëÿ±ÿ®ÿ©'
TATWEEL = u""\u0640""
stemmer = ISRIStemmer()
tknzr = TweetTokenizer()
text = tknzr.tokenize(foo)

for index in text:
    newList = [i for i in text if not i.isdigit()] # Remove digit 
    newList = ' '.join([i.lower() for i in text if not i.startswith(('@', '#'))]) # Remove mentions and hashtags
    newList = re.sub(r""http\S+"", """",index) # Remove links
    newList = stemmer.norm(index, num=1) # #emove diacritics
    newList = re.sub(r'[^\w\s]','', index)  # Remove punctuation
    newList = index.replace(TATWEEL, '')
    newList = ''.join(i for i, _ in itertools.groupby(index)) # Remove consecutive duplicate

print (newList)
</code></pre>

<p>The list I should get is:</p>

<pre><code>ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿáÿ∞Ÿá ÿ™ÿ¨ÿ±ÿ®ÿ©
</code></pre>

<p>but What I got is:</p>

<pre><code>ÿ±ÿ®ÿ©
</code></pre>

<p>When I try to test each method alone it works but when I gather it together it messes up. </p>

<p>-I'm using Python 3</p>

<p>Thank you.</p>
",Multilingual Language Processing & Language Identification,issue arabic preprocessing technique trying apply preprocessing technique arabic string list getting correct result code list get got try test method alone work gather together mess using python thank
Is there any library to perform semantic role labeling in english?,"<p>I want to perform semantic role labelling on the user query in python. I searched online, but SRL is available for Portuguese. Is there any SRL library for english language ?</p>
",Multilingual Language Processing & Language Identification,library perform semantic role labeling english want perform semantic role labelling user query python searched online srl available portuguese srl library english language
Can not identify text in Spanish with Lingpipe,"<p>Some days ago, I am developing an java server to keep a bunch of data and identify its language, so I decided to use lingpipe for such task. But I have facing an issue, after training code and evaluating it with two languages(English and Spanish) by getting that I can't identify spanish text, but I got a successful result with english and french.</p>

<p>The tutorial that I have followed in order to complete this task is: 
<a href=""http://alias-i.com/lingpipe/demos/tutorial/langid/read-me.html"" rel=""nofollow noreferrer"">http://alias-i.com/lingpipe/demos/tutorial/langid/read-me.html</a></p>

<p>An the next steps I have made in order to complete the task:
Steps followed to train a Language Classifier</p>

<p>~1.First place and unpack the english and spanish metadata inside a folder named leipzig, as follow (Note: Metadata and Sentences are provided from <a href=""http://wortschatz.uni-leipzig.de/en/download"" rel=""nofollow noreferrer"">http://wortschatz.uni-leipzig.de/en/download</a>):</p>

<pre><code>leipzig       //Main folder
   1M sentences             //Folder with data of the last trial 
     eng_news_2015_1M
     eng_news_2015_1M.tar.gz
     spa-hn_web_2015_1M
     spa-hn_web_2015_1M.tar.gz
   ClassifyLang.java                //Custom program to try the trained code
   dist                                        //Folder
     eng_news_2015_300K.tar.gz              //unpackaged english sentences
     spa-hn_web_2015_300K.tar.gz            //unpackaged spanish sentences
   EvalLanguageId.java
   langid-leipzig.classifier            //trained code
   lingpipe-4.1.2.jar
   munged                                      //Folder
     eng                    //folder containing the sentences.txt for english
        sentences.txt
     spa                    //folder containing the sentences.txt for spanish
        sentences.txt
   Munge.java
   TrainLanguageId.java
   unpacked                                    //Folder
     eng_news_2015_300K         //Folder with the english metadata 
        eng_news_2015_300K-co_n.txt
        eng_news_2015_300K-co_s.txt
        eng_news_2015_300K-import.sql
        eng_news_2015_300K-inv_so.txt
        eng_news_2015_300K-inv_w.txt
        eng_news_2015_300K-sources.txt
        eng_news_2015_300K-words.txt
        sentences.txt
     spa-hn_web_2015_300K                   //Folder with the spanish metadata 
        sentences.txt
        spa-hn_web_2015_300K-co_n.txt
        spa-hn_web_2015_300K-co_s.txt
        spa-hn_web_2015_300K-import.sql
        spa-hn_web_2015_300K-inv_so.txt
        spa-hn_web_2015_300K-inv_w.txt
        spa-hn_web_2015_300K-sources.txt
        spa-hn_web_2015_300K-words.txt
</code></pre>

<p>~2.Second unpack the language metadata compressed into a unpack folder</p>

<pre><code>unpacked                                    //Folder
    eng_news_2015_300K          //Folder with the english metadata 
        eng_news_2015_300K-co_n.txt
        eng_news_2015_300K-co_s.txt
        eng_news_2015_300K-import.sql
        eng_news_2015_300K-inv_so.txt
        eng_news_2015_300K-inv_w.txt
        eng_news_2015_300K-sources.txt
        eng_news_2015_300K-words.txt
        sentences.txt
    spa-hn_web_2015_300K                    //Folder with the spanish metadata 
        sentences.txt
        spa-hn_web_2015_300K-co_n.txt
        spa-hn_web_2015_300K-co_s.txt
        spa-hn_web_2015_300K-import.sql
        spa-hn_web_2015_300K-inv_so.txt
        spa-hn_web_2015_300K-inv_w.txt
        spa-hn_web_2015_300K-sources.txt
        spa-hn_web_2015_300K-words.txt
</code></pre>

<p>~3.Then Munge the sentences of each one in order to remove the line numbers, tabs and replacing line breaks with single space characters. The output is uniformly written using the UTF-8 unicode encoding (Note:the munge.java at Lingpipe site). </p>

<pre><code>/-----------------Command line----------------------------------------------/

javac -cp lingpipe-4.1.2.jar: Munge.java
java -cp lingpipe-4.1.2.jar: Munge /home/samuel/leipzig/unpacked /home/samuel/leipzig/munged
----------------------------------------Results-----------------------------
spa
reading from=/home/samuel/leipzig/unpacked/spa-hn_web_2015_300K/sentences.txt charset=iso-8859-1
writing to=/home/samuel/leipzig/munged/spa/spa.txt charset=utf-8
total length=43267166

eng
reading from=/home/samuel/leipzig/unpacked/eng_news_2015_300K/sentences.txt charset=iso-8859-1
writing to=/home/samuel/leipzig/munged/eng/eng.txt charset=utf-8
total length=35847257

/---------------------------------------------------------------/

&lt;---------------------------------Folder-------------------------------------&gt;
   munged                                      //Folder
    eng                     //folder containing the sentences.txt for english
        sentences.txt
    spa                 //folder containing the sentences.txt for spanish
        sentences.txt
&lt;--------------------------------------------------------------------------&gt;
</code></pre>

<p>~4.Next we start by training the language(Note:the TrainLanguageId.java at Lingpipe LanguageId tutorial).</p>

<pre><code>/---------------Command line--------------------------------------------/

javac -cp lingpipe-4.1.2.jar: TrainLanguageId.java
java -cp lingpipe-4.1.2.jar: TrainLanguageId /home/samuel/leipzig/munged /home/samuel/leipzig/langid-leipzig.classifier 100000 5
-----------------------------------Results-----------------------------------
nGram=100000 numChars=5
Training category=eng
Training category=spa

Compiling model to file=/home/samuel/leipzig/langid-leipzig.classifier

/----------------------------------------------------------------------------/
</code></pre>

<p>~5. We evaluated our trained code with the next result, having some issues on the confusion matrix  (Note:the EvalLanguageId.java at Lingpipe LanguageId tutorial).</p>

<pre><code>/------------------------Command line---------------------------------/

javac -cp lingpipe-4.1.2.jar: EvalLanguageId.java
java -cp lingpipe-4.1.2.jar: EvalLanguageId /home/samuel/leipzig/munged /home/samuel/leipzig/langid-leipzig.classifier 100000 50 1000
-------------------------------Results-------------------------------------

Reading classifier from file=/home/samuel/leipzig/langid-leipzig.classifier
Evaluating category=eng
Evaluating category=spa
TEST RESULTS
BASE CLASSIFIER EVALUATION
Categories=[eng, spa]
Total Count=2000
Total Correct=1000
Total Accuracy=0.5
95% Confidence Interval=0.5 +/- 0.02191346617949794
Confusion Matrix
reference \ response
  ,eng,spa
  eng,1000,0                                &lt;---------- not diagonal sampling
  spa,1000,0
Macro-averaged Precision=NaN
Macro-averaged Recall=0.5
Macro-averaged F=NaN
Micro-averaged Results
         the following symmetries are expected:
           TP=TN, FN=FP
           PosRef=PosResp=NegRef=NegResp
           Acc=Prec=Rec=F
  Total=4000
  True Positive=1000
  False Negative=1000
  False Positive=1000
  True Negative=1000
  Positive Reference=2000
  Positive Response=2000
  Negative Reference=2000
  Negative Response=2000
  Accuracy=0.5
  Recall=0.5
  Precision=0.5
  Rejection Recall=0.5
  Rejection Precision=0.5
  F(1)=0.5
  Fowlkes-Mallows=2000.0
  Jaccard Coefficient=0.3333333333333333
  Yule's Q=0.0
  Yule's Y=0.0
  Reference Likelihood=0.5
  Response Likelihood=0.5
  Random Accuracy=0.5
  Random Accuracy Unbiased=0.5
  kappa=0.0
  kappa Unbiased=0.0
  kappa No Prevalence=0.0
  chi Squared=0.0
  phi Squared=0.0
  Accuracy Deviation=0.007905694150420948
Random Accuracy=0.5
Random Accuracy Unbiased=0.625
kappa=0.0
kappa Unbiased=-0.3333333333333333
kappa No Prevalence =0.0
Reference Entropy=1.0
Response Entropy=NaN
Cross Entropy=Infinity
Joint Entropy=1.0
Conditional Entropy=0.0
Mutual Information=0.0
Kullback-Liebler Divergence=Infinity
chi Squared=NaN
chi-Squared Degrees of Freedom=1
phi Squared=NaN
Cramer's V=NaN
lambda A=0.0
lambda B=NaN

ONE VERSUS ALL EVALUATIONS BY CATEGORY


CATEGORY[0]=eng VERSUS ALL

First-Best Precision/Recall Evaluation
  Total=2000
  True Positive=1000
  False Negative=0
  False Positive=1000
  True Negative=0
  Positive Reference=1000
  Positive Response=2000
  Negative Reference=1000
  Negative Response=0
  Accuracy=0.5
  Recall=1.0
  Precision=0.5
  Rejection Recall=0.0
  Rejection Precision=NaN
  F(1)=0.6666666666666666
  Fowlkes-Mallows=1414.2135623730949
  Jaccard Coefficient=0.5
  Yule's Q=NaN
  Yule's Y=NaN
  Reference Likelihood=0.5
  Response Likelihood=1.0
  Random Accuracy=0.5
  Random Accuracy Unbiased=0.625
  kappa=0.0
  kappa Unbiased=-0.3333333333333333
  kappa No Prevalence=0.0
  chi Squared=NaN
  phi Squared=NaN
  Accuracy Deviation=0.011180339887498949


CATEGORY[1]=spa VERSUS ALL

First-Best Precision/Recall Evaluation
  Total=2000
  True Positive=0
  False Negative=1000
  False Positive=0
  True Negative=1000
  Positive Reference=1000
  Positive Response=0
  Negative Reference=1000
  Negative Response=2000
  Accuracy=0.5
  Recall=0.0
  Precision=NaN
  Rejection Recall=1.0
  Rejection Precision=0.5
  F(1)=NaN
  Fowlkes-Mallows=NaN
  Jaccard Coefficient=0.0
  Yule's Q=NaN
  Yule's Y=NaN
  Reference Likelihood=0.5
  Response Likelihood=0.0
  Random Accuracy=0.5
  Random Accuracy Unbiased=0.625
  kappa=0.0
  kappa Unbiased=-0.3333333333333333
  kappa No Prevalence=0.0
  chi Squared=NaN
  phi Squared=NaN
  Accuracy Deviation=0.011180339887498949

/-----------------------------------------------------------------------/
</code></pre>

<p>~6.Then we tried to make a real evaluation with spanish text:</p>

<pre><code>/-------------------Command line----------------------------------/

javac -cp lingpipe-4.1.2.jar: ClassifyLang.java
java -cp lingpipe-4.1.2.jar: ClassifyLang

/-------------------------------------------------------------------------/

&lt;---------------------------------Result------------------------------------&gt;
Text:   Yo soy una persona incre√≠ble y muy inteligente, me admiro a mi mismo lo que me hace sentir ansiedad de lo que viene, por que es algo grandioso lleno de cosas buenas y de ahora en adelante estar√© enfocado y optimista aunque tengo que aclarar que no lo har√© por querer algo, sino por que es mi pasi√≥n. 
Best    Language:   eng     &lt;------------- Wrong Result

&lt;-----------------------------------------------------------------------&gt;
</code></pre>

<p>Code for ClassifyLang.java:</p>

<pre><code>import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.JointClassification;
import com.aliasi.classify.JointClassifier;
import com.aliasi.classify.JointClassifierEvaluator;
import com.aliasi.classify.LMClassifier;

import com.aliasi.lm.NGramProcessLM;

import com.aliasi.util.AbstractExternalizable;

import java.io.File;
import java.io.IOException;

import com.aliasi.util.Files;

public class ClassifyLang {

    public static String text   =   ""Yo soy una persona incre√≠ble y muy inteligente, me admiro a mi mismo""
                +   "" estoy ansioso de lo que viene, por que es algo grandioso lleno de cosas buenas""
                +   "" y de ahora en adelante estar√© enfocado y optimista""
                +   "" aunque tengo que aclarar que no lo har√© por querer algo, sino por que no es dif√≠cil serlo.    "";

    private static File MODEL_DIR
        = new File(""/home/samuel/leipzig/langid-leipzig.classifier"");

    public static void main(String[] args)
        throws ClassNotFoundException, IOException {

    System.out.println(""Text:   ""   +   text);

    LMClassifier    classifier  =   null;
    try {
        classifier  =   (LMClassifier)  AbstractExternalizable.readObject(MODEL_DIR);
        }   catch   (IOException    |   ClassNotFoundException  ex) {
                    //  Handle  exceptions
            System.out.println(""Problem with the Model"");
        }

    Classification  classification  =   classifier.classify(text);
    String  bestCategory    =   classification.bestCategory();
    System.out.println(""Best    Language:   ""   +   bestCategory);

        }
}
</code></pre>

<p>~7.I tried with a 1 million metadata file, but it got the same result and also changing the ngram number by getting the same results.
I will be so thankfull for your help.</p>
",Multilingual Language Processing & Language Identification,identify text spanish lingpipe day ago developing java server keep bunch data identify language decided use lingpipe task facing issue training code evaluating two language english spanish getting identify spanish text got successful result english french tutorial followed order complete task next step made order complete task step followed train language classifier first place unpack english spanish metadata inside folder named leipzig follow note metadata sentence provided second unpack language metadata compressed unpack folder munge sentence one order remove line number tab replacing line break single space character output uniformly written using utf unicode encoding note munge java lingpipe site next start training language note trainlanguageid java lingpipe languageid tutorial evaluated trained code next result issue confusion matrix note evallanguageid java lingpipe languageid tutorial tried make real evaluation spanish text code classifylang java tried million metadata file got result also changing ngram number getting result thankfull help
Java library that finds sentence boundaries,"<p>Does anyone know of a Java library that handles finding sentence boundaries? I'm thinking that it would be a smart StringTokenizer implementation that knows about all of the sentence terminators that languages can use.</p>

<p>Here's my experience with BreakIterator:</p>

<p>Using the example <a href=""http://java.sun.com/docs/books/tutorial/i18n/text/examples/BreakIteratorDemo.java"" rel=""nofollow noreferrer"">here</a>:
I have the following Japanese:</p>

<pre><code>‰ªäÊó•„ÅØ„Éë„ÇΩ„Ç≥„É≥„ÇíË≤∑„Å£„Åü„ÄÇÈ´òÊÄßËÉΩ„ÅÆ„Éû„ÉÉ„ÇØ„ÅØÊó©„ÅÑÔºÅ„Å®„Å¶„ÇÇÂø´ÈÅ©„Åß„Åô„ÄÇ
</code></pre>

<p>In ascii, it looks like this:</p>

<pre><code>\ufeff\u4eca\u65e5\u306f\u30d1\u30bd\u30b3\u30f3\u3092\u8cb7\u3063\u305f\u3002\u9ad8\u6027\u80fd\u306e\u30de\u30c3\u30af\u306f\u65e9\u3044\uff01\u3068\u3066\u3082\u5feb\u9069\u3067\u3059\u3002
</code></pre>

<p>Here's the part of that sample that I changed:
  static void sentenceExamples() {</p>

<pre><code>  Locale currentLocale = new Locale (""ja"",""JP"");
  BreakIterator sentenceIterator = 
     BreakIterator.getSentenceInstance(currentLocale);
  String someText = ""‰ªäÊó•„ÅØ„Éë„ÇΩ„Ç≥„É≥„ÇíË≤∑„Å£„Åü„ÄÇÈ´òÊÄßËÉΩ„ÅÆ„Éû„ÉÉ„ÇØ„ÅØÊó©„ÅÑÔºÅ„Å®„Å¶„ÇÇÂø´ÈÅ©„Åß„Åô„ÄÇ"";
</code></pre>

<p>When I look at the Boundary indices, I see this:</p>

<pre><code>0|13|24|32
</code></pre>

<p>But those indices don't correspond to any sentence terminators.</p>
",Multilingual Language Processing & Language Identification,java library find sentence boundary doe anyone know java library handle finding sentence boundary thinking would smart stringtokenizer implementation know sentence terminator language use experience breakiterator using example following japanese ascii look like part sample changed static void sentenceexamples look boundary index see index correspond sentence terminator
Splitting a string by text language,"<p>I am working with a corpus of text documents (stored as one character string each) that are predominantly in English, but include some documents in Spanish and some that repeat the same information in both English and Spanish. I used the <code>cld2</code> and <code>cld3</code> packages (which implement Chrome's language detection functionality in R) to estimate the languages contained in each string in the corpus. My goal is to process all the strings containing text in both English and Spanish such that the English portions are retained and the Spanish portions are removed.</p>

<p>Here is an example of three of the strings I'm working with:</p>

<pre><code>mixed.language.strings &lt;- c(""Department of Cultural Affairs and Special Events: Today will be the First Annual Mariachi and Folklorico Festival! Local groups begin at 1:00pm and world renowned headliners start at 3:00pm. It will be located in Millennium Park. Invite your friends, family, and neighbors to participate in this FREE event! \nEnjoy the weather on this beautiful Sunday! \n ************** \n Departamento de Asuntos Culturales y Eventos Especiales: Hoy ser√° el Primer Festival Anual de Mariachi y Balet Folkl√≥rico! Los grupos locales comienzan a las 1:00 pm y los grupos de renombre mundial empiezan a las 3:00 pm. Ser√° en el Millennium Park. Inviten a su familia, amigo@s, y vecin@s a este evento completamente GRATIS!"", 
""Call or walk into our office for information on the Emergency Heating Repair Program which provides eligible low-income, owner-occupied homes grants for a new heating system.\n\nLlame o visite nuestra oficina para m√°s informaci√≥n sobre un programa de la Ciudad ofreciendo dinero hac√≠a la reparaci√≥n o instalaci√≥n de sistemas de calefacci√≥n. Due√±os de casa de ingresos bajos son elegibles.   \n\n 3476 S. Archer Ave. \n (773) 523-8250"", 
""Join me and other local elected officials for a workshop on appealing your property taxes. Homes in West &amp; South townships of Cook County are currently eligible to appeal. See flier for more info, or call my office at 773-523-8250.\n\nLos invito a un taller sobre el proceso de apalear sus impuestos de propiedad. Hogares en los West y East ‚Äútownships‚Äù del Condado de Cook son elegibles ahora para apalear sus impuestos. Por favor refi√©ranse al volante a√±adido a este mensaje, o llame mi oficina al 773-523-8250, para m√°s informaci√≥n.""
)
</code></pre>

<p>As far as I've been able to determine, <code>cld2</code> and <code>cld3</code> can estimate the languages contained within a string, but cannot extract portions of the string based on language.</p>

<p>Is there a different package in R that I can use to identify the portions of each string that are in each language, and split the string in two based on that?</p>

<p>Thanks! Sorry if this isn't clear; it's my first time posting.</p>
",Multilingual Language Processing & Language Identification,splitting string text language working corpus text document stored one character string predominantly english include document spanish repeat information english spanish used package implement chrome language detection functionality r estimate language contained string corpus goal process string containing text english spanish english portion retained spanish portion removed example three string working far able determine estimate language contained within string extract portion string based language different package r use identify portion string language split string two based thanks sorry clear first time posting
NLP extracting associate word,"<p>I'm new to the NLP algorithm. I'm working wit python 3 in french.
I would like to extract a group of word  from a text that belong together. For example ""left foot"" ""bottle of water"". 
how can i found a rule that will extract a group of word from a text/sentence. </p>

<p>(ps: I'm french, sorry if i don't express myself well)</p>

<p>Thank you</p>
",Multilingual Language Processing & Language Identification,nlp extracting associate word new nlp algorithm working wit python french would like extract group word text belong together example left foot bottle water found rule extract group word text sentence p french sorry express well thank
Which API does Instagram use to perform &#39;see translation&#39;,"<p>In Instagram people write comments in in  mixed language, for instance Hindi-English mixed, i.e Hindi transliterated text mixed with English words. Instagram provides a 'see translation' feature which converts the bilingual text into English.</p>

<p>Which API does Instagram use in order to translate the transliterated text into English?</p>

<pre><code>Example comment:    'Mujhe office mein work hai'
Translated comment:  I have got work in office.
</code></pre>
",Multilingual Language Processing & Language Identification,api doe instagram use perform see translation instagram people write comment mixed language instance hindi english mixed e hindi transliterated text mixed english word instagram provides see translation feature convert bilingual text english api doe instagram use order translate transliterated text english
How to quickly check and count English grammar errors in Python?,"<p>I'm looking for a fast way to count grammar errors in Python. For example, I need something along these lines:</p>

<pre><code>matches = grammar_checker.check('You is awesome!')
assert len(matches) == 1
</code></pre>

<p>Speed is much more an issue than accuracy. I could live with a few false positives or negatives as a trade-off for fast checks. I am talking here about problem sizes on the order of 100k documents (with roughly 5-10k characters) in only a few minutes. </p>

<p>I came across <a href=""https://languagetool.org/"" rel=""nofollow noreferrer"">LanguageTool</a>  in Java and its Python wrapper <a href=""https://pypi.python.org/pypi/language-check"" rel=""nofollow noreferrer"">language_check</a>. However, these are unfortunately too slow for my purposes. Any other suggestions or ideas? Thanks!</p>
",Multilingual Language Processing & Language Identification,quickly check count english grammar error python looking fast way count grammar error python example need something along line speed much issue accuracy could live false positive negative trade fast check talking problem size order k document roughly k character minute came across languagetool java python wrapper language check however unfortunately slow purpose suggestion idea thanks
Should I create a higher-level regular expression language to express common patterns?,"<p>I'm using regular expressions to find certain patterns in natural language processing.</p>

<p>I find myself using the same patterns over and over. and since these patterns can be hard to read in a terse regular expression, I'm asking myself if I should develop a higher order regular language that captures that?</p>

<p>will I be digging myself into a hole trying to define a DSL like that?
what's a good framework for developing such a language, and what can I expect in terms of effort of building it?
what are some common pitfalls for defining and building such a language?</p>

<p>it could look something like this
[views] overlooking [the] ($object)</p>

<p>that would capture text such as ""overlooking the ocean"" or ""overlooking cityscapes"". </p>

<p>or another example could be 
($granite) counter[- ]tops
that would capture text such as ""granite countertops"" or ""quartz counter-tops"" (but not ""granite counter"" or just ""countertops""</p>
",Multilingual Language Processing & Language Identification,create higher level regular expression language express common pattern using regular expression find certain pattern natural language processing find using pattern since pattern hard read terse regular expression asking develop higher order regular language capture digging hole trying define dsl like good framework developing language expect term effort building common pitfall defining building language could look something like view overlooking object would capture text overlooking ocean overlooking cityscape another example could granite counter top would capture text granite countertop quartz counter top granite counter countertop
Modern dependency parser for Russian,"<p>Is there any modern part-of-speech tagger + dependency parser for Russian language?
I need a tool or service that will be able to process plain text and output:</p>

<ul>
<li>division into sentences</li>
<li>division into tokens</li>
<li>part-of-speech tags (fine-grained MSD tags are welcome)</li>
<li>lemmas (base forms)</li>
<li>dependency role labels</li>
</ul>

<p>I need the tool for commercial purposes.
It could be either an open-source project with a trained statistical model that can be used for commercial purposes (purchased if needed) or an web API.
Eventually it could be a proprietary closed-source binary with a proprietary model.
The parsing models for Russian than I've found online all require the use of TreeTagger, which 1) has a very unfriendly licence, 2) is over 20 years old.</p>
",Multilingual Language Processing & Language Identification,modern dependency parser russian modern part speech tagger dependency parser russian language need tool service able process plain text output division sentence division token part speech tag fine grained msd tag welcome lemma base form dependency role label need tool commercial purpose could either open source project trained statistical model used commercial purpose purchased needed web api eventually could proprietary closed source binary proprietary model parsing model russian found online require use treetagger ha unfriendly licence year old
Multilingual Entities Wit.AI (import app),"<h1>ImportApp #Multilingual</h1>
<p>I want to make multilingual chatbot.Currently made a chatbot using the English language with 30+ entities.
Query: How you doing
Entity: E1</p>
<p>Want to check results for the Spanish language.So I exported app and imported to a new app with language equals to Spanish.</p>
<p>Query : comment vas-tu
(Using google translate &quot;how you doing&quot;)</p>
<p>No entity detected by wit.</p>
<p>Please let me know how to make multilingual chatbot using wit.ai</p>
",Multilingual Language Processing & Language Identification,multilingual entity wit ai import app importapp multilingual want make multilingual chatbot currently made chatbot using english language entity query entity e want check result spanish language exported app imported new app language equal spanish query comment va tu using google translate entity detected wit please let know make multilingual chatbot using wit ai
Chat-Bot sentence parsing methods,"<p>I have created a WhatsApp chat-bot using 'Selenium' and 'BeautifulSoup' in <em>Python</em>. I parse the incoming messages using Regex by matching it with pre defined data-sets.</p>

<p>However, I feel the parsing is mundane and lacks natural sense. The same match keeps happening again and again and hence the same message will be sent each time. I tried to solve this issue by using array of strings for the same match and sending it randomly each time.</p>

<p>I have heard of NLTK and its Natural Language Processing. Can I use that instead of Regex to make the chat-bot feel more natural? I have no clue of other string parsing methods. Which method would best suit my needs? Any suggestion would help. Thanks.</p>
",Multilingual Language Processing & Language Identification,chat bot sentence parsing method created whatsapp chat bot using selenium beautifulsoup python parse incoming message using regex matching pre defined data set however feel parsing mundane lack natural sense match keep happening hence message sent time tried solve issue using array string match sending randomly time heard nltk natural language processing use instead regex make chat bot feel natural clue string parsing method method would best suit need suggestion would help thanks
"Natural language identification and assign as like &quot;en&quot;, &quot;fr&quot;, &quot;tr&quot;","<p>Is there any package for identifying which language is a text in R? 
I have many rows including text in different languages like ""en"", ""es"", ""fr"", ""ja"" and so on.. Is it possible to get result with language column like below?</p>

<pre><code>id text                 language
1  ""I am a musician""    en 
2  ""—è –∏–Ω–∂–µ–Ω–µ—Ä""          ru 
3  ""Je suis un po√®te""   fr
</code></pre>

<p>Or any other possible help to define type of natural language?</p>
",Multilingual Language Processing & Language Identification,natural language identification assign like en fr tr package identifying language text r many row including text different language like en e fr ja possible get result language column like possible help define type natural language
Lemmatization of Spanish sentences In Stanford CoreNLP,"<p>How can I use Stanford-NLP to lemmatize words or is this even a possibility in coreNLP? </p>

<p>According to this website (<a href=""https://stanfordnlp.github.io/CoreNLP/human-languages.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/human-languages.html</a>) Lemmatization is not an option--but I'm hoping that this is a neglected page that needs to be updated. </p>

<p>Additionally, I've seen related questions but about German: <a href=""https://stackoverflow.com/questions/29861925/does-stanford-core-nlp-support-lemmatization-for-german"">Does Stanford Core NLP support lemmatization for German?</a></p>

<p><strong>How can I lemmatize spanish words in CoreNLP?</strong></p>
",Multilingual Language Processing & Language Identification,lemmatization spanish sentence stanford corenlp use stanford nlp lemmatize word even possibility corenlp according website lemmatization option hoping neglected page need updated additionally seen related question german href stanford core nlp support lemmatization german lemmatize spanish word corenlp
How to use Tregex on not English texts with StanfordCoreNLP Server?,"<p>I am trying to use Tregex with StanfordCoreNLP Server for processing french texts. The server is configured with french properties, but the /tregex endpoint seems to process the text automatically with english parsing.</p>

<p>However when I am trying to parse the text with the regular parser, everything works pretty well and the french parser is correctly applied.</p>

<p>Here is an example:</p>

<p><strong>Input Sentence</strong></p>

<blockquote>
  <p>Pierre et Jean sont dans la cuisine</p>
</blockquote>

<p><strong>Regular Parsing :</strong> </p>

<p><code>http://localhost:8082/?properties=outputFormat&amp;properties=depparse.model+%3D+edu%2Fstanford%2Fnlp%2Fmodels%2Fparser%2Fnndep%2FUD_French.gzpos.model&amp;properties=parse.model&amp;properties=tokenize.language&amp;prop
erties=depparse.language&amp;properties=annotators</code></p>

<blockquote>
  <p>(ROOT
   (SENT
     (NP (NPP Pierre)
       (COORD (CC et)
         (NP (NPP Jean))))
     (VN (V sont))
     (PP (P dans)
       (NP (DET la) (NC cuisine)))))</p>
</blockquote>

<p><strong>Tregex Parsing</strong></p>

<p><code>http://localhost:8082/tregex?properties=annotators&amp;properties=outputFormat&amp;properties=depparse.model+%3D+edu%2Fstanford%2Fnlp%2Fmodels%2Fparser%2Fnndep%2FUD_French.gzpos.model&amp;properties=parse.model&amp;properties=tokenize.language&amp;properties=depparse.language&amp;pattern=NP%3Dnoun1+%24+NP%3Dnoun2</code></p>

<blockquote>
  <p>{
   ""sentences"": [
     {
       ""0"": {
         ""match"": ""(NP (NNP Pierre)\n  (CC et)\n  (NNP Jean))\n"",
         ""namedNodes"": [
           {
             ""noun1"": ""(NP (NNP Pierre)\n  (CC et)\n  (NNP Jean))\n""
  &lt;          },
           {
             ""noun2"": ""(NP\n  (NP (JJ sont) (NNS dans))\n  (PP (FW la)\n    >(NP (NN cuisine))))\n""
           }
         ]
       },
       ""1"": {
         ""match"": ""(NP\n  (NP (JJ sont) (NNS dans))\n  (PP (FW la)\n    (NP >(NN cuisine))))\n"",
         ""namedNodes"": [
           {
             ""noun1"": ""(NP\n  (NP (JJ sont) (NNS dans))\n  (PP (FW la)\n    >(NP (NN cuisine))))\n""
          },
           {
             ""noun2"": ""(NP (NNP Pierre)\n  (CC et)\n  (NNP Jean))\n""
           }
         ]
       }
     }
   ]
  }</p>
</blockquote>

<p>As you can see, the parsing is not the same in both cases. </p>

<p>After a quick check into the server logs, I noticed that the annotators are automatically discarded and replaced by the default english models :</p>

<pre><code>[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""coref"" with signature [coref.language:fr;coref.mode:statistical;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ssplit"" with signature [tokenize.language:fr;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""depparse"" with signature [depparse.language:french;depparse.model:edu/stanford/nlp/models/parser/nndep/UD_French.gz;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""tokenize"" with signature [tokenize.language:fr;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""mention"" with signature [mention.type:dep;coref.language:fr;coref.mode:statistical;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""pos"" with signature [pos.model:edu/stanford/nlp/models/pos-tagger/french/french.tagger;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""openie"" with signature [openie.strip_entailments:true;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.AnnotatorPool - Replacing old annotator ""parse"" with signature [parse.model:edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz;parse.binaryTrees:true;] with new annotator with signature []
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-2-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[pool-2-thread-3] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.2 sec].
</code></pre>

<p>I tried the same example on the CoreNLP test server (<a href=""http://corenlp.run/"" rel=""nofollow noreferrer"">http://corenlp.run/</a>) and it works perfectly.</p>

<p>I suppose I am doing something wrong with the server configuration but I don't know what :) </p>

<p>Many thanks for your help!</p>

<p>Louis</p>
",Multilingual Language Processing & Language Identification,use tregex english text stanfordcorenlp server trying use tregex stanfordcorenlp server processing french text server configured french property tregex endpoint seems process text automatically english parsing however trying parse text regular parser everything work pretty well french parser correctly applied example input sentence pierre et jean sont dans la cuisine regular parsing root sent np npp pierre coord cc et np npp jean vn v sont pp p dans np det la nc cuisine tregex parsing sentence match np nnp pierre n cc et n nnp jean n namednodes noun np nnp pierre n cc et n nnp jean n noun np n np jj sont nns dans n pp fw la n np nn cuisine n match np n np jj sont nns dans n pp fw la n np nn cuisine n namednodes noun np n np jj sont nns dans n pp fw la n np nn cuisine n noun np nnp pierre n cc et n nnp jean n see parsing case quick check server log noticed annotator automatically discarded replaced default english model tried example corenlp test server work perfectly suppose something wrong server configuration know many thanks help louis
Extracting sentenses belongs to different category from text using NLTK,"<p>I am new to Natural Language Processing (NLP) and I came across a problem where from the given text, I have to extract sentences belongs to different category like</p>

<p>1) sentences related to commitments (like sentences including 'will', 'shall' etc)</p>

<p>2) sentences related to cost or budget</p>

<p>3) and so on....</p>

<p>I need to know which features of NLTK should I use to implement this. How easy to add more and more category to extract more subjective information?</p>

<p>Any examples are even more helpful.  </p>
",Multilingual Language Processing & Language Identification,extracting sentenses belongs different category text using nltk new natural language processing nlp came across problem given text extract sentence belongs different category like sentence related commitment like sentence including shall etc sentence related cost budget need know feature nltk use implement easy add category extract subjective information example even helpful
Compare unequal text parts in natural language processing,"<p>I have two texts, one which is read in from a DOCX file and the other from a TXT file. The layout of the DOCX file is like this:</p>

<blockquote>
  <p>{NAME} </p>
  
  <p>{ADDRESS}</p>
  
  <p>Dear Mr. Doe,</p>
  
  <p>In spite of your application on our website, we regret to inform you
  that you don't get the job.</p>
  
  <p>{END}</p>
  
  <p>TheCompany </p>
  
  <p>Registered under number 12345</p>
</blockquote>

<p>The layout of the TXT file is the same, but whatever is in brackets is filled in, which makes it impossible to just compare string one on one. It would look like this:</p>

<blockquote>
  <p>Jessy</p>
  
  <p>Hillington road 23</p>
  
  <p>Dear Mr. Doe,</p>
  
  <p>In spite of your application on our website, we regret to inform you
  that you >don't get the job.</p>
  
  <p>Best Regards,</p>
  
  <p>TheCompany</p>
  
  <p>Registered under number 12345</p>
</blockquote>

<p>I tried to split the text parts and compare those parts with each other. So a simple:</p>

<pre><code>' '.join(text1.split(split_after)[1:]).split(split_before)[0]
</code></pre>

<p>which is for the DOCX and the same for text2 (TXT) and then compare it. But as the sentence can change, so that it starts with </p>

<blockquote>
  <p>Despite of your application on our website (...)</p>
</blockquote>

<p>which then breaks all the code. Imagine this scenario for DOCX files up to 20 pages. I need something more useful and programmable.Any ideas?</p>
",Multilingual Language Processing & Language Identification,compare unequal text part natural language processing two text one read docx file txt file layout docx file like name address dear mr doe spite application website regret inform get job end thecompany registered number layout txt file whatever bracket filled make impossible compare string one one would look like jessy hillington road dear mr doe spite application website regret inform get job best regard thecompany registered number tried split text part compare part simple docx text txt compare sentence change start despite application website break code imagine scenario docx file page need something useful programmable idea
Wordnet (Word Sense Annotated) Corpus,"<p>I've been utilizing lots of different corpora for natural language processing, and I've been looking for a corpus that has been annotated with Wordnet Word Senses.</p>

<p>I understand that there probably is not a big corpus with this information, since the corpus needs to be built up manually, but there has to be something to go off of.</p>

<p>Also if there isn't a corpus in existence, is there at least a sense annotated ngram database (with what percentage of the time a word is each of its definitions, or a numerical count of each wordnet definition depending on how common the word sense is)?</p>
",Multilingual Language Processing & Language Identification,wordnet word sense annotated corpus utilizing lot different corpus natural language processing looking corpus ha annotated wordnet word sens understand probably big corpus information since corpus need built manually ha something go also corpus existence least sense annotated ngram database percentage time word definition numerical count wordnet definition depending common word sense
Find the corresponding nouns or verbs of adjectives and adverbs in an English sentence,"<p>My goal is to build an algorithm that given an adjective or adverb within a sentence, indicates the corresponding noun or verb. </p>

<p>For example: </p>

<blockquote>
  <p>The boy threw the heavy stone angrily to the window that was very far.</p>
</blockquote>

<p>heavy (adj) -> stone </p>

<p>angrily (adv) -> threw</p>

<p>far (adj) -> window</p>

<p>So far I was able to tag Parts of Speech for each word and identify adjectives, nouns, verbs, and adverbs in a given sentence. </p>

<ol>
<li>In Natural Language Processing, is there a terminology for what I'm trying to do? </li>
<li>Does a well known algorithm or approach exist for my goal? </li>
<li>I wonder if it makes sense to manually train several sentences and build a machine learning model for this? or is it over-engineering the problem? </li>
</ol>
",Multilingual Language Processing & Language Identification,find corresponding noun verb adjective adverb english sentence goal build algorithm given adjective adverb within sentence indicates corresponding noun verb example boy threw heavy stone angrily window wa far heavy adj stone angrily adv threw far adj window far wa able tag part speech word identify adjective noun verb adverb given sentence natural language processing terminology trying doe well known algorithm approach exist goal wonder make sense manually train several sentence build machine learning model engineering problem
How do I get started with a project on Text Summarization using NLP?,"<p>My final year engineering project requires me to build an application using Java or Python which summarizes a text document using Natural Language Processing. How do I even begin with the programming of such an application? </p>

<p>Based on some research, I've just noted down that extraction-based summarization will be the best bet for me since it isn't so complex as abstraction based algorithms. Even then, it'd be really helpful if someone would guide me in the right direction to go about this.  </p>
",Multilingual Language Processing & Language Identification,get started project text summarization using nlp final year engineering project requires build application using java python summarizes text document using natural language processing even begin programming application based research noted extraction based summarization best bet since complex abstraction based algorithm even really helpful someone would guide right direction go
Data Mining Using Large XML Files,"<p>Haven't used many .xml files, so looking for some expert help in determining how to start.  </p>

<p>Trying to mine Stackexchange data from xml files located <a href=""https://archive.org/download/stackexchange"" rel=""nofollow noreferrer"">here</a></p>

<p>There are separate files such as Posts, Users, Badges, PostHistory etc. that will ultimately need to be ""joined"" in order to derive any insights. Also, will need to apply NLP (Natural Language Processing techniques) to the comments, Posts etc.  </p>

<p>What are my available options for (a) storing, and (b)processing the data? I know R, C, and SQL/databases. </p>

<p>Thanks in advance!  </p>
",Multilingual Language Processing & Language Identification,data mining using large xml file used many xml file looking expert help determining start trying mine stackexchange data xml file located separate file post user badge posthistory etc ultimately need joined order derive insight also need apply nlp natural language processing technique comment post etc available option storing b processing data know r c sql database thanks advance
semantically similar word for natural language processing in German,"<p>I am working on natural language programming in the German Language in which I need to categorize words according to the meaning of the words. E.g 'Communication', 'Social skills', 'Interpersonal Skills' belongs to 'Communication skills' and so forth. </p>

<p>Basically, the words need to sort based on the similarity of the meaning it has with given set of standard words. 
I have tried <code>Levenstein-distance</code>, <code>edit-distance</code> and open-source <code>fuzzy</code> string matching technique but the result are not satisfying.</p>

<p>Best results come from using <code>Longest-common Subsequence</code> the list of words but I want to match the words based on the underlying meaning of the words. </p>
",Multilingual Language Processing & Language Identification,semantically similar word natural language processing german working natural language programming german language need categorize word according meaning word e g communication social skill interpersonal skill belongs communication skill forth basically word need sort based similarity meaning ha given set standard word tried open source string matching technique result satisfying best result come using list word want match word based underlying meaning word
How does language detection work?,"<p>I have been wondering for some time how does Google translate(or maybe a hypothetical translator) detect language from the string entered in the ""from"" field. I have been thinking about this and only thing I can think of is looking for words that are unique to a language in the input string. The other way could be to check sentence formation or other semantics in addition to keywords. But this seems to be a very difficult task considering different languages and their semantics. I did some research to find that there are ways that use n-gram sequences and use some statistical models to detect language. Would appreciate a high level answer too.</p>
",Multilingual Language Processing & Language Identification,doe language detection work wondering time doe google translate maybe hypothetical translator detect language string entered field thinking thing think looking word unique language input string way could check sentence formation semantics addition keywords seems difficult task considering different language semantics research find way use n gram sequence use statistical model detect language would appreciate high level answer
spaCy similarity - Setting sense2vec or word2vec as default,"<p>I came across one of spaCy's blog's <a href=""https://explosion.ai/blog/sense2vec-with-spacy"" rel=""nofollow noreferrer"">post</a> which introduces the sense2vec idea and there's also an online interface to play with it's <a href=""https://demos.explosion.ai/sense2vec/?word=natural%20language%20processing&amp;sense=auto"" rel=""nofollow noreferrer"">most similar function</a> while there's also an <a href=""https://demos.explosion.ai/similarity"" rel=""nofollow noreferrer"">online tool</a> to play with the similarity function, brought by spaCy.</p>

<p>I just couldn't understand if those tools are based on the sense2vec algorithm or just normal pre-trained word2vec model (I'm talking about the large English model). </p>

<p>Is there a way to define with which model to I would like to work with? </p>

<p>The point is that </p>
",Multilingual Language Processing & Language Identification,spacy similarity setting sense vec word vec default came across one spacy blog post introduces sense vec idea also online interface play similar function also online tool play similarity function brought spacy understand tool based sense vec algorithm normal pre trained word vec model talking large english model way define model would like work point
Translation to Predicate Logic (PL) with lexicon,"<p>I have the following statement that I would like to translate to predicate logic:</p>

<p><em>The performance of a push system will vary according to the environment, but the performance of a pull system will always be better.</em></p>

<p>The following lexicon is given:</p>

<p>P(x, y): x is the performance of y</p>

<p>H(x): x is a push system</p>

<p>L(x): x is a pull system</p>

<p>E(x): x is the environment</p>

<p>V(x, y): x will vary according to y</p>

<p>B(x, y): x will always be better than y</p>

<p>I thought of this translation:</p>

<p>‚àÉx(P(x, H(x)) ‚áí V(x, E(y)) ‚àß ‚àÄx(P(x, L(y)) ‚áí B(L(x), H(y))</p>

<p>But, I was told that I cannot supply predicates as arguments to other predicates, and thus P(x, H) as I have done above is not correct. Any suggestions on how to correctly translate the statement? Thanks.</p>
",Multilingual Language Processing & Language Identification,translation predicate logic pl lexicon following statement would like translate predicate logic performance push system vary according environment performance pull system always better following lexicon given p x x performance h x x push system l x x pull system e x x environment v x x vary according b x x always better thought translation x p x h x v x e x p x l b l x h wa told supply predicate argument predicate thus p x h done correct suggestion correctly translate statement thanks
Convert adjective to adverb,"<p>Does anyone know <strong>how to convert an english adjective to its respective adverb</strong>?  Python would be ideal, but really any programmatic approach would be great.</p>

<p>I've tried <a href=""https://www.clips.uantwerpen.be/pages/pattern-en"" rel=""noreferrer"">pattern.en</a>, <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""noreferrer"">nltk wordnet</a>, and <a href=""http://spacy.io/"" rel=""noreferrer"">spacy</a> to no avail.</p>

<p>Converting adverbs to their root adjective form is no problem.  I'm using the SO solution <a href=""https://stackoverflow.com/questions/17245123/getting-adjective-from-an-adverb-in-nltk-or-other-nlp-library"">here</a>.</p>

<p>What I want is to go the other way.  From adjective to adverb.</p>

<p><a href=""https://stackoverflow.com/a/48218093/8870055"">Here is nltk wordnet code</a> that kind of converts words between different word forms, but fails for adjective &lt;--> adverb conversions. </p>

<p>Specifically, I'd like a function <code>getAdverb</code> like this:</p>

<pre><code>getAdverb('quick')
&gt;&gt;&gt; quickly
getAdverb('noteable')
&gt;&gt;&gt; notably
getAdverb('happy')
&gt;&gt;&gt; happily
</code></pre>

<p>Any code, resources, or suggestions would be greatly appreciated!</p>
",Multilingual Language Processing & Language Identification,convert adjective adverb doe anyone know convert english adjective respective adverb python would ideal really programmatic approach would great tried pattern en nltk wordnet spacy avail converting adverb root adjective form problem using solution kind convert word different word form fails adjective adverb conversion specifically like function like code resource suggestion would greatly appreciated
Customized tag and lemmas for URLs using spaCy,"<p>Consider the sentence </p>

<pre><code>msg = 'I got this URL https://stackoverflow.com/questions/47637005/handmade-estimator-modifies-parameters-in-init/47637293?noredirect=1#comment82268544_47637293 freed'
</code></pre>

<p>Next, I process the sentence using out-of-the-box <code>spaCy</code> for English:</p>

<pre><code>import spacy
nlp = spacy.load('en')
doc = nlp(msg)
</code></pre>

<p>Let's review the output of: <code>[(t, t.lemma_, t.pos_, t.tag_, t.dep_) for t in doc]</code>:</p>

<pre><code>[(I, '-PRON-', 'PRON', 'PRP', 'nsubj'),
 (got, 'get', 'VERB', 'VBD', 'ROOT'),
 (this, 'this', 'DET', 'DT', 'det'),
 (URL, 'url', 'NOUN', 'NN', 'compound'),
 (https://stackoverflow.com/questions/47637005/handmade-estimator-modifies-parameters-in-init/47637293?noredirect=1#comment82268544_47637293,
  'https://stackoverflow.com/questions/47637005/handmade-estimator-modifies-parameters-in-init/47637293?noredirect=1#comment82268544_47637293',
  'NOUN',
  'NN',
  'nsubj'),
 (freed, 'free', 'VERB', 'VBN', 'ccomp')]
</code></pre>

<p>I would like to improve the handling of the URL piece. In particular, I want to:</p>

<ol>
<li>Set its <code>lemma</code> to <code>stackoverflow.com</code></li>
<li>Set the <code>tag</code> to <code>URL</code></li>
</ol>

<p>How can I do it using <code>spaCy</code>? I want to use a regex (as suggested <a href=""https://stackoverflow.com/a/31952097/671013"">here</a>) to decide whether a string is a URL or not and get the domain. So far, I failed to find the way to do it.</p>

<p><em>EDIT</em> I guess a custom component is what I need. However, it seems like there's no way of placing a regex-based (or any other) callable as the <a href=""https://github.com/explosion/spaCy/blob/0cf0fca17486201049c915dab1e2ed220d6be373/examples/pipeline/custom_component_countries_api.py#L62"" rel=""nofollow noreferrer""><code>patterns</code></a>.</p>
",Multilingual Language Processing & Language Identification,customized tag lemma url using spacy consider sentence next process sentence using box english let review output would like improve handling url piece particular want set set using want use regex suggested
"In natural language processing (NLP), how do you make an efficient dimension reduction?","<p>In NLP, it's always the case that the dimension of the features are very huge. For example, for one project at hand, the dimension of features  is almost 20 thousands (p = 20,000), and each feature is a 0-1 integer to show whether a specific word or bi-gram is presented in a paper (one  paper is a data point $x \in R^{p}$).</p>

<p>I know the redundancy among the features is huge, so dimension reduction is necessary. I have three questions:</p>

<p>1) I have 10 thousands data points (n = 10,000), and each data points has 10 thousands features (p = 10,000). What is the effieient  way to conduct dimension reduction? The matrix $X \in R^{n \times p}$ is so huge that both PCA (or SVD, truncated SVD is OK, but I don't think SVD is a good way to reduce dimention for binary features) and Bag of Words (or K-means) is hard be be directly conducted on $X$ (Sure, it is sparse). I don't have a server, I just use my PC:-(.</p>

<p>2) How to judge the similarity or distance among two data points? I think the Euclidean distance may not work well for binary features. How about L0 norm? What do you use?</p>

<p>3) If I want to use SVM machine (or other kernel methods) to conduct classification, which kernel should I use?</p>

<p>Many Thanks!</p>
",Multilingual Language Processing & Language Identification,natural language processing nlp make efficient dimension reduction nlp always case dimension feature huge example one project hand dimension feature almost thousand p feature integer show whether specific word bi gram presented paper one paper data point x r p know redundancy among feature huge dimension reduction necessary three question thousand data point n data point ha thousand feature p effieient way conduct dimension reduction matrix x r n time p huge pca svd truncated svd ok think svd good way reduce dimention binary feature bag word k mean hard directly conducted x sure sparse server use pc judge similarity distance among two data point think euclidean distance may work well binary feature l norm use want use svm machine kernel method conduct classification kernel use many thanks
Dictionary vs nested dictionaries ... for ~7 milion lexical definitions (MULTITEXT v5),"<p>I'm using lexical resource text file, formatted in MULTITEXT v5 format that actually look like this:</p>

<blockquote>
  <p>≈ævakah    ≈ævakati Vme1s   0   0.000000</p>
  
  <p>≈ævakahu   ≈ævakati Vme3p   0   0.000000</p>
  
  <p>≈ævakala   ≈ævakati Vmp-pn  0   0.000000</p>
  
  <p>≈ævakala   ≈ævakati Vmp-sf  45  0.000081</p>
  
  <p>≈ævakale   ≈ævakati Vmp-pf  11  0.000020</p>
  
  <p>≈ævakali   ≈ævakati Vmp-pm  66  0.000119</p>
  
  <p>≈ævakalo   ≈ævakati Vmp-sn  10  0.000018</p>
  
  <p>≈ævakan    ≈ævakati Appmsann    0   0.000000</p>
  
  <p>≈ævakan    ≈ævakati Appmsnn 0   0.000000</p>
  
  <p>≈ævakan    ≈ævakati Appmsvn 0   0.000000</p>
</blockquote>

<p>The format meaning:</p>

<blockquote>
  <p>[inflected word]  [lemma form] [grammatical context]  ... and tf-idf info that I don't use</p>
</blockquote>

<p>So, in typical scenario I have to match ~5000 of various [inflected word]s to retrieve [lemma form]s and more importantly: [grammatical context]s, where single [inflected word] may actually have more matching lines (like in the case of <strong>≈ævakan</strong>). The lexical resource to search has about 7 milion lines.</p>

<p>So far, I tried with loading the complete file into List and then running all ~5000 Regexes against each line (List item) using Parallel.ForEach. Regex was used for flexibility to query via [lemma form] or using only part of the word, but for sake of performance I can give up on that. And it took something like 30 minutes to find about 350 entries. So, obviously my approach was completely wrong.</p>

<p>Now I'm thinking to load the complete file into Dictionary where the key would be [inflected word] (so I give up on flexibility) but I wonder:</p>

<ul>
<li>Would it make sense (for greater execution time) to nest two dictionaries like this:</li>
</ul>

<blockquote>
  <p>Dictionary&lt;[first letter], Dictionary&lt;[inflected word], List&lt;[definition
     line]>>></p>
</blockquote>

<p>Would it do any better then loading all into:</p>

<blockquote>
  <p>Dictionary&lt;[inflected word], List&lt;[definition line]>></p>
</blockquote>

<ul>
<li>Is there some better idea?</li>
</ul>

<p>I'm not using ConcurrentDictionary since the content is written into data structure only once, before the use starts. </p>

<p>My preference is solely query execution time - RAM seems not to be an issue - with current code I have 19Gb of RAM available, and I have 8 core CPU so any comments on Parallel execution are also welcome. </p>

<p>In case someone wonders: this is a Natural Language Processing application. </p>
",Multilingual Language Processing & Language Identification,dictionary v nested dictionary milion lexical definition multitext v using lexical resource text file formatted multitext v format actually look like vakah vakati vme vakahu vakati vme p vakala vakati vmp pn vakala vakati vmp sf vakale vakati vmp pf vakali vakati vmp pm vakalo vakati vmp sn vakan vakati appmsann vakan vakati appmsnn vakan vakati appmsvn format meaning inflected word lemma form grammatical context tf idf info use typical scenario match various inflected word retrieve lemma form importantly grammatical context single inflected word may actually matching line like case vakan lexical resource search ha milion line far tried loading complete file list running regexes line list item using parallel foreach regex wa used flexibility query via lemma form using part word sake performance give took something like minute find entry obviously approach wa completely wrong thinking load complete file dictionary key would inflected word give flexibility wonder would make sense greater execution time nest two dictionary like dictionary first letter dictionary inflected word list definition line would better loading dictionary inflected word list definition line better idea using concurrentdictionary since content written data structure use start preference solely query execution time ram seems issue current code gb ram available core cpu comment parallel execution also welcome case someone wonder natural language processing application
Error loading openNLP Spanish model POS tagger in R,"<p>I am trying to run a POS tagger function for Spanish text using R's openNLP package. I previously run the same function using a model for English text, but it seems there is not an official model for Spanish POS tagging in the openNLP page (<a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">http://opennlp.sourceforge.net/models-1.5/</a>)</p>

<p>I found a previous question that pointed to a Spanish POS tagging model (<a href=""https://stackoverflow.com/questions/40959983/java-opennlp-version-1-5-3-spanish-models"">Java OpenNLP version 1.5.3. Spanish models</a>), which I tried to use, but I got the following error message when I tried to use any of the models available there:</p>

<pre><code>word_token_annotator &lt;- Maxent_Word_Token_Annotator(model = 'opennlp-es-perceptron-pos-es.bin')
</code></pre>

<p>However:</p>

<pre><code>Error in .jnew(""opennlp.tools.tokenize.TokenizerModel"", .jcast(.jnew(""java.io.FileInputStream"",  : 
  java.lang.IllegalArgumentException: opennlp.tools.util.InvalidFormatException: The TokenizerME cannot load a model for the POSTaggerME!
</code></pre>

<p>I suppose that the binaries available in that github repo are not in the format that is expected by ""Maxent_Word_Token_Annotator"".</p>

<p>Do you know how I could solve this issue, or are you aware of any other Spanish POS tagging model that I can plug into my code?</p>

<p>Thank you very much in advance for your help.</p>
",Multilingual Language Processing & Language Identification,error loading opennlp spanish model po tagger r trying run po tagger function spanish text using r opennlp package previously run function using model english text seems official model spanish po tagging opennlp page found previous question pointed spanish po tagging model know could solve issue aware spanish po tagging model plug code thank much advance help
Natural Language Processing,"<p>I have thousands of sentences in a file. I want to find only right/useful English Language words. Is it possible with Natural Language Processing?</p>

<p>Sample Sentence:</p>

<p>~@^.^@~ tic but sometimes world good famous tac Zorooooooooooo</p>

<p>I just want to extract only English Words like</p>

<p>tic world good famous </p>

<p>Any Advice how can I achieve this. Thanks in Advance</p>
",Multilingual Language Processing & Language Identification,natural language processing thousand sentence file want find right useful english language word possible natural language processing sample sentence tic sometimes world good famous tac zorooooooooooo want extract english word like tic world good famous advice achieve thanks advance
Text Analytics Vs Natural Language Processing What is the difference?,"<p>I had a tough evening today trying to convince one of my colleagues that NLP or <strong>Natural Language Processing</strong> is the super set and <strong><em>Text Analytics</em></strong> is a sub set of it.
At the best probably both are synonymous and can be used interchangeably.</p>

<p>Is that correct? Anybody who has a crystal clarity as to whether these terms have a boundary well defined or can be used interchangeably?</p>
",Multilingual Language Processing & Language Identification,text analytics v natural language processing difference tough evening today trying convince one colleague nlp natural language processing super set text analytics sub set best probably synonymous used interchangeably correct anybody ha crystal clarity whether term boundary well defined used interchangeably
How to split a string of characters/alphabets without space/separator into dictionary words?,"<p>I have a string that consists of two or more dictionary English words, but the spaces between the words is missing. How can I separate the words in R or Python?</p>

<p><strong>Example:</strong></p>

<pre><code>Input_string = ""thequickbrownfox""

Desired_output_string = ""the quick brown fox""
</code></pre>

<p>Is there an algorithm to do such text processing?</p>
",Multilingual Language Processing & Language Identification,split string character alphabet without space separator dictionary word string consists two dictionary english word space word missing separate word r python example algorithm text processing
lemmatizer for Spanish in python 3,"<p>I'm trying to apply lemmatization over some Spanish words in Python 3, I found a nice library ""pattern"" to do it in python 2. However, this one has no support for python 3 yet and cannot really find a good one. It's not enough for my work to apply a steamer such as SnowBall from nltk. Any help or advice on libraries will be greatly appreciated it.</p>
",Multilingual Language Processing & Language Identification,lemmatizer spanish python trying apply lemmatization spanish word python found nice library pattern python however one ha support python yet really find good one enough work apply steamer snowball nltk help advice library greatly appreciated
stemDocment in tm package not working on past tense word,"<p>I have a file 'check_text.txt' that contains ""<strong>said say says make made</strong>"". I'd like to perform stemming on it to get ""say say say make make"". I tried to use <code>stemDocument</code> in <code>tm</code> package, as the following, but only get ""said say say make made"". Is there a way to perform stemming on past tense words? Is it necessary to do so in real-world natural language processing? Thanks!</p>

<pre><code>filename = 'check_text.txt'
con &lt;- file(filename, ""rb"")
text_data &lt;- readLines(con,skipNul = TRUE)
close(con)
text_VS &lt;- VectorSource(text_data)
text_corpus &lt;- VCorpus(text_VS)
text_corpus &lt;- tm_map(text_corpus, stemDocument, language = ""english"")
as.data.frame(text_corpus)$text
</code></pre>

<p><strong>EDIT</strong>: I also tried <code>wordStem</code> in <code>SnowballC</code> package</p>

<pre><code>&gt; library(SnowballC)
&gt; wordStem(c(""said"", ""say"", ""says"", ""make"", ""made""))
[1] ""said"" ""sai""  ""sai""  ""make"" ""made""
</code></pre>
",Multilingual Language Processing & Language Identification,stemdocment tm package working past tense word file check text txt contains said say say make made like perform stemming get say say say make make tried use package following get said say say make made way perform stemming past tense word necessary real world natural language processing thanks edit also tried package
"Given two words and their corresponding pieces-of-speech, how to return them in the order that makes most grammatical sense?","<p>I have an app that creates two randomly generated words. My goal is to return them in the order that makes the most sense grammatically.</p>

<p>Getting their corresponding pieces-of-speech to each word should not be difficult, but my main question is to how to implement the logic to handle the rules. It seems like it would be an absurd number of relational rules given any two pieces of speech. IE, given a noun, a list of rules for which POS goes before or after it, then repeat by making another exhaustive list for each other POS and its relationships to other pieces-of-speech. How even do you structure something like that in an elegant way, if there is no better way to accomplish this?</p>

<p>If a given word can be multiple pieces-of-speech (""duck"", ""novel"", etc), is there an ideal way to handle that?</p>

<p>I'm aware english is complicated and there will always be exceptions, but a ""best guess"" would be satisfactory. </p>
",Multilingual Language Processing & Language Identification,given two word corresponding piece speech return order make grammatical sense app creates two randomly generated word goal return order make sense grammatically getting corresponding piece speech word difficult main question implement logic handle rule seems like would absurd number relational rule given two piece speech ie given noun list rule po go repeat making another exhaustive list po relationship piece speech even structure something like elegant way better way accomplish given word multiple piece speech duck novel etc ideal way handle aware english complicated always exception best guess would satisfactory
Finding best speed and accuracy combination for parsing,"<p>I have experience on different cases englishPCFG pos-tagger is more accurate then any others. The model used on <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a> </p>

<p>Due to speed, I would like to get parse result with depparse (english_UD)</p>

<p>Is there a way to use englishPCFG pos-tagger (without parse as it is slow to compare with english_UD) with depparse to combine pos-tagger accuracy and depparse speed ?</p>
",Multilingual Language Processing & Language Identification,finding best speed accuracy combination parsing experience different case englishpcfg po tagger accurate others model used due speed would like get parse result depparse english ud way use englishpcfg po tagger without parse slow compare english ud depparse combine po tagger accuracy depparse speed
Is there a Java implementation of Constraint Grammar?,"<p>Anybody knows a Java implementation of <a href=""http://en.wikipedia.org/wiki/Constraint_Grammar"" rel=""nofollow"">Constraint Grammar</a> for natural language processing? I know the <a href=""http://beta.visl.sdu.dk/cg3.html"" rel=""nofollow"">VISL CG3</a> implementation, that is in C++, and I could interface it from Java, but it would be easier if I could find a Java implementation since it will be integrated to a legacy Java code.
This will be used in a <a href=""http://cogroo.sourceforge.net/"" rel=""nofollow"">Portuguese open source grammar checker</a> and should be compatible with LGPL license. </p>
",Multilingual Language Processing & Language Identification,java implementation constraint grammar anybody know java implementation constraint grammar natural language processing know visl cg implementation c could interface java would easier could find java implementation since integrated legacy java code used portuguese open source grammar checker compatible lgpl license
How do I suggest tags to the user based only on the title of a list?,"<p>The Problem:</p>

<p>I need to suggest tags to the user based only on the title (5-15 words) of a list they are about to create.</p>

<p>We have around 30 pre-determined tags -</p>

<pre><code>Gaming, Movies, TV shows, Documentaries, Books, Music, Art, History, People, Adventure, Sports, Cooking, Travel, Places, Food, Drinks, Fitness, DIY, Technology, Science, Cars, Bikes, Comedy, Shopping, Clothes, Fashion, Photography, Nature, etc.
</code></pre>

<p>So, for example, for a list with title <em>'Most expensive fine-dine restaurants around the world'</em> suggested tags could be
<em>(Food, Places, Drinks, Travel)</em>.</p>

<p>It does not need to be super accurate, just needs to work satisfactorily well, I am sure it would get better as more training data comes in from our users. I don't have any training data for supervised learning yet.</p>

<p>I find myself lost in the vast space of Machine Learning and Natural Language Processing. It would be very helpful if someone can suggest what methods/algorithms/libraries I should use for this specific task, and the background reading I should do before it.</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,suggest tag user based title list problem need suggest tag user based title word list create around pre determined tag example list title expensive fine dine restaurant around world suggested tag could food place drink travel doe need super accurate need work satisfactorily well sure would get better training data come user training data supervised learning yet find lost vast space machine learning natural language processing would helpful someone suggest method algorithm library use specific task background reading thanks
Natural Language Processing in Java (NLP),"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/870460/java-is-there-a-good-natural-language-processing-library"">Java : Is there a good natural language processing library</a>  </p>
</blockquote>



<p>Can anybody tell me about a library for NLP in java? It would really be nice if is properly documented too. I have tried to work with lingpipe but I am not able to understand it completely.</p>
",Multilingual Language Processing & Language Identification,natural language processing java nlp possible duplicate href good natural language processing library anybody tell library nlp java would really nice properly documented tried work lingpipe able understand completely
How to get group of words as Rationale using Natural Language processing/NLTK?,"<p>consider below sentence, I want know how to extract rational from below sentence. Rationale meaning in this context = <code>single/group of words which gives some meanigful information to sentense</code></p>
<blockquote>
<p>I am play hockey.</p>
<blockquote>
<p><strong>play</strong></p>
</blockquote>
<p>I am feeling cold.</p>
<blockquote>
<p>rational: <strong>feeling cold</strong></p>
</blockquote>
<p>Sam is going to do Master of Science in Stanford next year.</p>
<blockquote>
<p>rational: <strong>Master of Science.</strong></p>
</blockquote>
</blockquote>
<p>Those words are mix combination of adverbs, verbs and adjective. So how to extract those in nltk?</p>
<p>I want to know how to extract group of words as rational using nltk ?</p>
",Multilingual Language Processing & Language Identification,get group word rationale using natural language processing nltk consider sentence want know extract rational sentence rationale meaning context play hockey play feeling cold rational feeling cold sam going master science stanford next year rational master science word mix combination adverb verb adjective extract nltk want know extract group word rational using nltk
TypeError: &#39;module&#39; object is not callable in Spacy Python,"<p>I want to print <code>Parse Tree</code> using <code>Spacy</code>. But the code below is giving the error </p>

<blockquote>
  <p>en_nlp = spacy.language('English')
  TypeError: 'module' object is not callable</p>
</blockquote>

<p>The error is on this <code>en_nlp = spacy.loads('en')</code> line. I tried to shake off as <code>en_nlp = spacy.language(English)</code> by importing <code>from spacy.en import English</code> But still it is not working. Can someone help?</p>

<p>Code: </p>

<pre><code>import spacy
from nltk import Tree

en_nlp = spacy.loads('en')

doc = en_nlp(""The quick brown fox jumps over the lazy dog."")

def to_nltk_tree(node):
    if node.n_lefts + node.n_rights &gt; 0:
        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])
    else:
        return node.orth_


[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]
</code></pre>
",Multilingual Language Processing & Language Identification,typeerror module object callable spacy python want print using code giving error en nlp spacy language english typeerror module object callable error line tried shake importing still working someone help code
Apertium translator. Is there a way to get the original phrase,"<p>Is there a way in apertium translator to get the original phrase for a translation?</p>

<p>I.E. get something like:</p>

<pre><code>phrase: {
  original: { Hola, buenos d√≠as},
  translated: {Hello, good morning}
}
</code></pre>

<p>I need that in order to make a mechanism to improve the translations.</p>
",Multilingual Language Processing & Language Identification,apertium translator way get original phrase way apertium translator get original phrase translation e get something like need order make mechanism improve translation
What programming language is most like natural language?,"<p>I got the idea for this question from numerous situations where I don't understand what the person is talking about and when others don't understand me.</p>

<p>So, a ""smart"" solution would be to speak a computer language. :)</p>

<p>I am interested how far a programming language can go to get near to (English) natural language. When I say near, I mean not just to use words and sentences, but to be able to ""do"" things a natural language can ""do"" and by ""do"" I mean that it can be used (in a very limited way) as a replacement for natural language.</p>

<p>I know that this is impossible (is it?) but I think that this can be interesting.</p>
",Multilingual Language Processing & Language Identification,programming language like natural language got idea question numerous situation understand person talking others understand smart solution would speak computer language interested far programming language go get near english natural language say near mean use word sentence able thing natural language mean used limited way replacement natural language know impossible think interesting
Could Lojban be used to perform better at natural language understanding than English?,"<p>Not sure if this is a good place for the question but thought I would try since there are lots of smart people here.</p>

<p>I am just wondering if using a constructed (i.e. not developed naturally) and syntactically unambiguous human language like <a href=""http://en.wikipedia.org/wiki/Lojban"" rel=""nofollow noreferrer"">Lojban</a> could be used to perform better at natural language understanding than, say, English, since it is a more logical language.</p>

<p>If anyone has explored this idea or has a better understanding of NLP I'd love some feedback.</p>
",Multilingual Language Processing & Language Identification,could lojban used perform better natural language understanding english sure good place question thought would try since lot smart people wondering using constructed e developed naturally syntactically unambiguous human language like lojban could used perform better natural language understanding say english since logical language anyone ha explored idea ha better understanding nlp love feedback
Spacy 2.0 Matcher: add() takes at least 4 positional arguments (3 given),"<p>I'm trying to make <a href=""http://spacy.io"" rel=""nofollow noreferrer"">Spacy's</a> <a href=""https://spacy.io/api/matcher"" rel=""nofollow noreferrer"">matcher example</a> from the <a href=""https://spacy.io/api"" rel=""nofollow noreferrer"">official documentation</a> work on my machine.</p>

<pre><code>from spacy.matcher import Matcher

matcher = Matcher(nlp.vocab) 
pattern = [{'LOWER': ""hello""}, {'LOWER': ""world""}] 
matcher.add(""HelloWorld"", None, pattern) 
doc = nlp(u'hello world!') 
matches = matcher(doc)
</code></pre>

<p>Unfortunately I encounter the following error:</p>

<p><strong>TypeError: add() takes at least 4 positional arguments (3 given)</strong></p>

<p>The corresponding source code can be found <a href=""https://github.com/explosion/spaCy/blob/master/spacy/matcher.pyx"" rel=""nofollow noreferrer"">here</a>, the important part is</p>

<pre><code>def add(self, key, on_match, *patterns):
    """"""Add a match-rule to the matcher. A match-rule consists of: an ID
        key, an on_match callback, and one or more patterns.
</code></pre>

<p>Spacy has recently been updated to version 2.0, I installed that version and also successfully linked the English model to it. Feels like I'm missing something very obvious here, but I don't see what I'm doing wrong.</p>
",Multilingual Language Processing & Language Identification,spacy matcher add take least positional argument given trying make spacy matcher example official documentation work machine unfortunately encounter following error typeerror add take least positional argument given corresponding source code found important part spacy ha recently updated version installed version also successfully linked english model feel like missing something obvious see wrong
"sequence to sequence learning for language translation, what about unseen words","<p>sequence to sequence learning is a powerful mechanism for language translation, especially using it locally in a context specific case.</p>

<p>I am following <a href=""http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""nofollow noreferrer"">this <code>pytorch</code> tutorial</a> for the task.</p>

<p>However, the tutorial did not split the data into training and testing.
You might think its not a big deal, just split it up, use one chunk for training and the other for testing. But it is not that simple.</p>

<p>Essentially, the tutorial creates the indices of the seen words while leading the dataset. The indices are simply stored in the dictionary. This is before going to the encoder RNN, just a simple conversion kind of task from words to the numbers.</p>

<p>If data is split up at random, what happens is, one of the keyword may not appear in the sentences from the training set, and so may not have an index at all. If it shows up at the time of testing, what should be done?</p>

<p>Extend the dictionary? </p>
",Multilingual Language Processing & Language Identification,sequence sequence learning language translation unseen word sequence sequence learning powerful mechanism language translation especially using locally context specific case following tutorial task however tutorial split data training testing might think big deal split use one chunk training testing simple essentially tutorial creates index seen word leading dataset index simply stored dictionary going encoder rnn simple conversion kind task word number data split random happens one keyword may appear sentence training set may index show time testing done extend dictionary
Approaches to improve Microsoft ChatBot with each user conversation by learning from it?,"<p>I am building a Microsoft ChatBot using LUIS for natural language processing. I would like LUIS to improve by learning new utterences for the intents identified. </p>

<p>For example, if my 'Greeting' intent has utterences 'Hi', 'Hello', 'Hello, how are you?', the next time it encounters 'How are you?', it may predict the intent as 'Greeting' with a low accuracy. If that utterance is learnt as part of the intent, then in future, this utterence will be predicted with better accuracy and also help us in recognizing utterences closer to this utterence.</p>

<p>The learning could be based on:</p>

<blockquote>
  <ol>
  <li>All input for which intent was identified. (I understand this can cause wrong learnings).</li>
  <li>Inputs identified by LUIS and verified by user or agent real-time or later offline.</li>
  <li>Inputs identified right/wrong but verified and corrected by the agent or  support team later via an easier UI.</li>
  </ol>
</blockquote>

<p>I understand LUIS gives a 'Suggested Utterences' tab which takes care of point 3. I am trying to understand how we can automate this learning by minimal user intervention. </p>

<p>What are the various approaches used in projects?</p>
",Multilingual Language Processing & Language Identification,approach improve microsoft chatbot user conversation learning building microsoft chatbot using luis natural language processing would like luis improve learning new utterences intent identified example greeting intent ha utterences hi hello hello next time encounter may predict intent greeting low accuracy utterance learnt part intent future utterence predicted better accuracy also help u recognizing utterences closer utterence learning could based input intent wa identified understand cause wrong learning input identified luis verified user agent real time later offline input identified right wrong verified corrected agent support team later via easier ui understand luis give suggested utterences tab take care point trying understand automate learning minimal user intervention various approach used project
OpenFST Create Morphological Analyzer,"<p>I want to create a morphological analyzer (only for suffixes) for English language with OpenFST.
For example,</p>
<blockquote>
<p>Input: computerization</p>
<p>Output:  compute + -er + -ize + -ation + Noun</p>
</blockquote>
<p>My question is how should I create input and output symbols using shell commands. Is there a way to do this in OpenFST?</p>
",Multilingual Language Processing & Language Identification,openfst create morphological analyzer want create morphological analyzer suffix english language openfst example input computerization output compute er ize ation noun question create input output symbol using shell command way openfst
python: nlp: expand english contractions like don&#39;t that&#39;s etc,"<p>I am trying some preprocessing hence words like dont etc. want to simply to do not so that its  algo works better.  I checked nltk didnt find something handy.  I could use crude method of lookup but the issue will be the one used with proper noun  like Jon's  etc. Please suggest</p>

<p>The earlier question 
<a href=""https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"">Expanding English language contractions in Python</a></p>

<p>doesnt have good answer for proper noun usage</p>
",Multilingual Language Processing & Language Identification,python nlp expand english contraction like etc trying preprocessing hence word like dont etc want simply algo work better checked nltk didnt find something handy could use crude method lookup issue one used proper noun like jon etc please suggest earlier question href english language contraction python doesnt good answer proper noun usage
Determine what tree bank type can come next,"<p>I am use <code>Apache NLP</code> and its <code>POSTaggerME</code>. I have it breaking down words into their <code>Penn Treebank tag set</code> values. Is there any functionality out there (doesn't have to be in Apache NLP) that lets you know what kind of word can come next using the English language structure and semantics?</p>

<p>For example, I have sentence <code>""Most large ""</code> which breaks down to <code>JJS</code> and <code>JJ</code>. Or an <code>Adjective, superlative</code>, and an <code>Adjective</code>. What I want to know, is using the structure of the English language, if there is a tool that can tell me if something like an <code>NNS</code>, or a <code>Noun, plural</code> can come next and the sentence is still valid?</p>
",Multilingual Language Processing & Language Identification,determine tree bank type come next use breaking word value functionality apache nlp let know kind word come next using english language structure semantics example sentence break want know using structure english language tool tell something like come next sentence still valid
Search Engine suggestion - link (form complete),"<p>I need to create search engine in my app. When user types e.g. ""Buy 100 White Batman T-shirts"", it should automatically open page with filled inputs (Page Buy -> Article: Batman T-shirt, Qty: 100, Color: White). </p>

<p>Also, if user types ""Buy"", it should be some suggestions like ""Buy  "" or similar.. </p>

<p>I was trying to accomplish that wit NLP (Apache openNLP + english language), but I can't figure it out completely. Any suggestions, what should I use and which approach should I take?</p>
",Multilingual Language Processing & Language Identification,search engine suggestion link form complete need create search engine app user type e g buy white batman shirt automatically open page filled input page buy article batman shirt qty color white also user type buy suggestion like buy similar wa trying accomplish wit nlp apache opennlp english language figure completely suggestion use approach take
"Natural Language Processing(syntatctic,semantic,progmatic) Analysis","<p>My text contains text=<strong>""Ravi beated Ragu""</strong></p>

<p>My Question will be <strong>""Who beated Ragu?""</strong></p>

<p>The Answer Should come <strong>""Ravi""</strong> Using NLP</p>

<p>How to do this by natural language processing.
Kindly guide me to proceed with this by syntactic,semantic and progmatic analysis using python</p>
",Multilingual Language Processing & Language Identification,natural language processing syntatctic semantic progmatic analysis text contains text ravi beated ragu question beated ragu answer come ravi using nlp natural language processing kindly guide proceed syntactic semantic progmatic analysis using python
how do I create my own training corpus for stanford tagger?,"<p>I have to analyze informal english text with lots of short hands and local lingo. Hence I was thinking of creating the model for the stanford tagger.</p>

<p>How do i create my own set of labelled corpus for the stanford tagger to train on?</p>

<p>What is the syntax of the corpus and how long should my corpus be in order to achieve a desirable performance?</p>
",Multilingual Language Processing & Language Identification,create training corpus stanford tagger analyze informal english text lot short hand local hence wa thinking creating model stanford tagger create set labelled corpus stanford tagger train syntax corpus long corpus order achieve desirable performance
How to extract Information?,"<p>Objective: I am trying to do a project on Natural Language Processing (NLP), where I want to extract information and represent it in graphical form.</p>

<p>Description:</p>

<ol>
<li>I am considering news article as input to my project.<br></li>
<li>Removing unwanted data in the input &amp; making it in Clean Format.<br></li>
<li>Performing NLP &amp; Extracting Information/Knowledge<br></li>
<li>Representing Information/Knowledge in Graphical Format.<br></li>
</ol>

<p>Is it Possible?</p>
",Multilingual Language Processing & Language Identification,extract information objective trying project natural language processing nlp want extract information represent graphical form description considering news article input project removing unwanted data input making clean format performing nlp extracting information knowledge representing information knowledge graphical format possible
CoreNLP for other languages like Arabic,"<p>I am using CoreNLP I have successfully executed it for English. I need to do the same for other languages like Arabic. May I please know how to train the system and use it for other languages?
please give me the steps in details</p>
",Multilingual Language Processing & Language Identification,corenlp language like arabic using corenlp successfully executed english need language like arabic may please know train system use language please give step detail
CoreNLP for other languages like Arabic,"<p>I am using CoreNLP I have successfully executed it for English. I need to do the same for other languages like Arabic. May I please know how to train the system and use it for other languages?
please give me the steps in details</p>
",Multilingual Language Processing & Language Identification,corenlp language like arabic using corenlp successfully executed english need language like arabic may please know train system use language please give step detail
Find the most suitable sentence for array of tokens,"<p>I have the following DataFrame for text mining:</p>

<pre><code>df = pd.DataFrame({'text':[""Anyone who reads Old and Middle English literary texts will be familiar with the mid-brown volumes of the EETS, with the symbol of Alfreds jewel embossed on the front cover"",
                     ""Most of the works attributed to King Alfred or to Aelfric, along with some of those by bishop Wulfstan and much anonymous prose and verse from the pre-Conquest period, are to be found within the Society's three series"",
                     ""all of the surviving medieval drama, most of the Middle English romances, much religious and secular prose and verse including the English works of John Gower, Thomas Hoccleve and most of Caxton's prints all find their place in the publications"",
                     ""Without EETS editions, study of medieval English texts would hardly be possible.""]})



text
0   Anyone who reads Old and Middle English litera...
1   Most of the works attributed to King Alfred or...
2   all of the surviving medieval drama, most of t...
3   Without EETS editions, study of medieval Engli...
</code></pre>

<p>And I have lists of tokens:</p>

<pre><code>tokens = [['middl engl', 'mid-brown', 'symbol'], [""king"", 'anonym', 'series'], ['mediev', 'romance', 'relig'], ['hocclev', 'edit', 'publ']]
</code></pre>

<p>I'm trying to find the most suitable sentence for each array of tokens from the list <strong>tokens</strong> above.</p>

<p>UPDATE: I was asked to explain my problem in more details.</p>

<p>The problem is that I'm doing it on non-English texts, so it is very problematic to illustrate a bit more of my problem. </p>

<p>I'm looking for some function x which takes as input each element of my <strong>tokens</strong> list and for each element of <strong>tokens</strong> list, it searches for most suitable(maybe in some metric sense) sentences in <code>df.text</code>. This is the main idea the output doesn't matter. I just want it to work:)</p>
",Multilingual Language Processing & Language Identification,find suitable sentence array token following dataframe text mining list token trying find suitable sentence array token list token update wa asked explain problem detail problem non english text problematic illustrate bit problem looking function x take input element token list element token list search suitable maybe metric sense sentence main idea output matter want work
How to identify patterns inside a text and categorize them,"<p>From a table that stores medicine descriptions I need to identify the product name, strength, product quantity and pharmaceutical company of each entry. The goal is to have a copy of the table with a predefined structure.</p>

<p><strong>Current table:</strong>
<img src=""https://i.sstatic.net/uPx2k.png"" alt=""current table""></p>

<p><strong>Normalized table:</strong>
<img src=""https://i.sstatic.net/XxrL8.png"" alt=""normalized table""></p>

<p>So far I've read a little of Natural Language Processing, but I want to know another approach; I was thinking of using Regex but there are plenty of cases.</p>

<p>Any kind of insight would be appreciated.</p>
",Multilingual Language Processing & Language Identification,identify pattern inside text categorize table store medicine description need identify product name strength product quantity pharmaceutical company entry goal copy table predefined structure current table normalized table far read little natural language processing want know another approach wa thinking using regex plenty case kind insight would appreciated
Stanford CoreNLP example on Arabic Language,"<p>I am having a project on  Arabic NLP,Can I have a java example on how to use (coreNLP) Stanford  Arabic Modules (segmenter,POS tagger...)?</p>
",Multilingual Language Processing & Language Identification,stanford corenlp example arabic language project arabic nlp java example use corenlp stanford arabic module segmenter po tagger
Natural Language Processing for data extraction from PDF,"<p>I have many different formats of scanned pdfs with many different fields. Think of it as an invoice that has been scanned. I need to extract the information from the scanned pdf and output the fields and the texts that are in each of the fields. </p>

<p>I have an OCR tool that does a good job in extracting all the texts in the raw format. I somehow using NLP have to be able to extract the fields and their values from the raw text. As there are many formats of the invoice, using OCR is not an option in this case. How could NLP help me in solving this problem?</p>
",Multilingual Language Processing & Language Identification,natural language processing data extraction pdf many different format scanned pdfs many different field think invoice ha scanned need extract information scanned pdf output field text field ocr tool doe good job extracting text raw format somehow using nlp able extract field value raw text many format invoice using ocr option case could nlp help solving problem
Dictionary for multi-word term like low fat milk?,"<p>I am not a fluent English speaker, so maybe this question is trivial for native English speakers.</p>

<p>I am implementing a natural language processing system. I need to extract terms which have meanings by combining with several another words.</p>

<p>For example, I am supposed to extract [""I"", ""am"", ""drinking"", ""low fat"", ""low fat milk""] from ""I am drinking low fat milk"". Is it possible?</p>
",Multilingual Language Processing & Language Identification,dictionary multi word term like low fat milk fluent english speaker maybe question trivial native english speaker implementing natural language processing system need extract term meaning combining several another word example supposed extract drinking low fat low fat milk drinking low fat milk possible
convert pdf (with special characters) to text,"<p>Hi I'm trying to convert multiple pdfs to text, my code is working, however most of mine file are in spanish, with characters such as (√±,√≠,√≥,√∫,√©) and these (√±,√≠,√≥,√∫,√©) are getting corrupted. Also I need the text file to be in lower case for text analysis later:</p>

<pre><code>library(XML)
  library(httr)
  library(dplyr)
  library(tidyr)
  library(stringr)
  library(tm)

  # Get a list of all of the document names of the downloaded PDFs
    pdf_files &lt;- list.files(path = paste(getwd(), '/pdf', sep = ''),
                            pattern = 'pdf',
                            full.names = TRUE)

    # Check there are pdf files in directory
    if( length(pdf_files) &gt; 0 ){

      # Loop through each PDF and create a txt version in the same folder

      for(i in pdf_files){

        system(
          paste(
            paste('""', getwd(), '/dependencies/xpdf/bin64/pdftotext.exe""', sep = ''), 
            paste0('""', i, '""')),
          wait = FALSE)

      }
    }


  cat( '\nConversion to text complete.\n\n' )
</code></pre>
",Multilingual Language Processing & Language Identification,convert pdf special character text hi trying convert multiple pdfs text code working however mine file spanish character getting corrupted also need text file lower case text analysis later
Where are technical papers or documents for Watson Natural Language Understanding like sentiment analysis?,"<p>I am reading Bluemix Natural Language Understanding <a href=""https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/#methods"" rel=""nofollow noreferrer"">api</a>. They are very interesting. What I want to ask is where I can find the technical documents or papers that describe the basic theories or methods behind computations or codes of sentient, emotion, concepts, categories...? How I can know what these apis return measures are based on? There are quite a number of Natural Language Processing (NLP) tools available now. Some tools (e.g. stanford nlp tools -> <a href=""http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"" rel=""nofollow noreferrer"">paper: Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank</a>) show links to the papers or documents that address what theories and methods their nlp tools are based on or they show THEIR methods of computing sentiment. Will you direct me to the resources that deal with these basic theoretical methods behind computations of Waston NLU sentiment,emotion, concepts, categories...?</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,technical paper document watson natural language understanding like sentiment analysis reading bluemix natural language understanding api interesting want ask find technical document paper describe basic theory method behind computation code sentient emotion concept category know apis return measure based quite number natural language processing nlp tool available tool e g stanford nlp tool paper recursive deep model semantic compositionality sentiment treebank show link paper document address theory method nlp tool based show method computing sentiment direct resource deal basic theoretical method behind computation waston nlu sentiment emotion concept category thanks
How to disambiguate words in Conceptnet,"<p><a href=""http://conceptnet5.media.mit.edu/"" rel=""nofollow"">Conceptnet</a> contains two basic types of nodes, words (e.g. /c/en/cat) and senses (e.g. /c/en/cat/n/domestic_cat). Unfortunately, the vast majority of edges use word nodes. This makes inferring difficult, because I can't be sure which sense a word-to-word edge is referring to. </p>

<p>For example, Conceptnet contains 9 senses that use the word ""cat"", most being proper nouns (/c/en/cat/n/musical, /c/en/cat/n/magazine, /c/en/cat/n/a_spiteful_woman_gossip, etc). If an edge says ""/c/en/cat /r/HasA /c/en/tail"", I know by using my own experience that that's probably referring to /c/en/cat/n/domestic_cat and no other senses. Whereas if I see an edge that says ""/c/en/cat /r/IsA /c/en/fun_to_watch"", I know it's probably referring to /c/en/cat/n/musical, but it also still might be referring to /c/en/cat/n/domestic_cat.</p>

<p>How do I automate this process? How do I translate edges that only use word nodes so that they use sense nodes?</p>
",Multilingual Language Processing & Language Identification,disambiguate word conceptnet conceptnet contains two basic type node word e g c en cat sens e g c en cat n domestic cat unfortunately vast majority edge use word node make inferring difficult sure sense word word edge referring example conceptnet contains sens use word cat proper noun c en cat n musical c en cat n magazine c en cat n spiteful woman gossip etc edge say c en cat r hasa c en tail know using experience probably referring c en cat n domestic cat sens whereas see edge say c en cat r isa c en fun watch know probably referring c en cat n musical also still might referring c en cat n domestic cat automate process translate edge use word node use sense node
Custom Keyboard in Android,"<p>I want to develop a Keyboard in Android. How Should I proceed? Also I wanted to know whether all the popular keyboards used today in Android implement Machine Learning or Natural Language Processing.</p>
",Multilingual Language Processing & Language Identification,custom keyboard android want develop keyboard android proceed also wanted know whether popular keyboard used today android implement machine learning natural language processing
"python2.7: Why does printing a list of chinese look like [u&#39;\u4ed6&#39;, u&#39;\u6765\u5230&#39;, u&#39;\u4e86&#39;, u&#39;\u7f51\u6613&#39;]?","<p>I used the jieba chinese dictionary for word segmentation.<br>
When I print a list of words, the result is the following:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*- 
import jieba
import sys
import jieba


s1 = ""‰ªñÊù•Âà∞‰∫ÜÁΩëÊòìÊù≠Á†îÂ§ßÂé¶!""
seg_list = jieba.cut(s1)
lst1 = "", "".join(seg_list)
print lst1
m =lst1.split(', ')
print m[2]
punct = set(u''':!),.:;?]}¬¢'""„ÄÅ„ÄÇ„Äâ„Äã„Äç„Äè„Äë„Äï„Äó„ÄûÔ∏∞Ô∏±Ô∏≥ÔπêÔΩ§ÔπíÔπîÔπïÔπñÔπóÔπöÔπúÔπûÔºÅÔºâÔºåÔºéÔºöÔºõÔºüÔΩúÔΩùÔ∏¥Ô∏∂Ô∏∏Ô∏∫Ô∏ºÔ∏æÔπÄÔπÇÔπÑÔπèÔΩ§ÔΩûÔø†„ÄÖ‚Äñ‚Ä¢¬∑ÀáÀâ‚Äï--‚Ä≤‚Äô‚Äù([{¬£¬•'""‚Äµ„Äà„Ää„Äå„Äé„Äê„Äî„ÄñÔºàÔºªÔΩõÔø°Ôø•„ÄùÔ∏µÔ∏∑Ô∏πÔ∏ªÔ∏ΩÔ∏øÔπÅÔπÉÔπôÔπõÔπùÔºàÔΩõ‚Äú‚Äò-‚Äî_‚Ä¶''')

filterpuntl = list(filter(lambda x: x not in punct, m))

print filterpuntl[2]
</code></pre>

<p>The result is following:</p>

<pre><code>‰ªñ, Êù•Âà∞, ‰∫Ü, ÁΩëÊòì, Êù≠Á†î, Â§ßÂé¶, !
[u'\u4ed6', u'\u6765\u5230', u'\u4e86', u'\u7f51\u6613', u'\u676d\u7814', u'\u5927\u53a6', u'!']
[u'\u4ed6', u'\u6765\u5230', u'\u4e86', u'\u7f51\u6613', u'\u676d\u7814', u'\u5927\u53a6']
</code></pre>

<p>How to change the <code>[u'\u4ed6', u'\u6765\u5230'  ...]</code>  to Chinese characters?</p>

<p>When I print a single element of the list, it is Chinese:</p>

<pre><code>print m[2]
print filterpuntl[2]
</code></pre>

<p>The result is:</p>

<pre><code>‰ªñ, Êù•Âà∞, ‰∫Ü, ÁΩëÊòì, Êù≠Á†î, Â§ßÂé¶, !
‰∫Ü
‰∫Ü
</code></pre>
",Multilingual Language Processing & Language Identification,python doe printing list chinese look like u u ed u u u u u e u u f u used jieba chinese dictionary word segmentation print list word result following result following change chinese character print single element list chinese result
Better algorithm for shortening English words,"<p>I have some unique codes that are generated from strings (ex: website host names) in various independent components of my application.</p>

<p>These codes are meant to be used by machines only so i would like to keep them as short as possible.</p>

<p>The below algorithm would be applied to every word in the string. The output words would be concatenated with a dash to generate the unique code.</p>

<pre><code>The current algorithm I have used:

 - Skip word if length is less than 6

 - Leave first character as is

 - Remove every wowel in the word from the second character onwards
</code></pre>

<ol>
<li>architectural digest eu => archtctrl-dgst-eu</li>
<li>arizona foothills magazine => arzn-fthlls-mgzn</li>
</ol>

<p>Is there a better way to shorten an English word leaving it as recognisable as possible to a human reader?</p>

<p>The output should be deterministic and produce the same shortened version whenever it is run on the same input.</p>

<p>A good algorithm should also minimise the number of clashes for similarly spelt words.</p>
",Multilingual Language Processing & Language Identification,better algorithm shortening english word unique code generated string ex website host name various independent component application code meant used machine would like keep short possible algorithm would applied every word string output word would concatenated dash generate unique code architectural digest eu archtctrl dgst eu arizona foothill magazine arzn fthlls mgzn better way shorten english word leaving recognisable possible human reader output deterministic produce shortened version whenever run input good algorithm also minimise number clash similarly spelt word
What is the math requirement for natural language processing?,"<p>I've tried looking around but I can't seem to find an answer on what math I need before jumping into NLP. I was hoping to get a solid foundation in math before jumping into NLP. </p>

<p>From what I've gathered it's mostly:
Probability,
some Statistic,
Discrete Math </p>

<p>Thank you for your time.</p>
",Multilingual Language Processing & Language Identification,math requirement natural language processing tried looking around seem find answer math need jumping nlp wa hoping get solid foundation math jumping nlp gathered mostly probability statistic discrete math thank time
spaCy: what is NORM-part of tokenizer_exceptions?,"<p>I'm adding <code>tokenizer_exceptions</code> for my language. I was looking at <code>'gonna'</code> example for English language, so wrote the rule as follows:</p>

<pre><code>'—Ç.–ø.': [
    {ORTH: ""—Ç."", NORM: ""—Ç–æ–º—É"", LEMMA: ""—Ç–æ—Ç""},
    {ORTH: ""–ø."", NORM: ""–ø–æ–¥–æ–±–Ω–æ–µ"", LEMMA: ""–ø–æ–¥–æ–±–Ω—ã–π""}
],
</code></pre>

<p>Then when I tokenize, I expect that <code>NORM</code>-parts of rule will be in <code>token.norm_</code> (though there is no any documentation about <code>Token.norm_</code>). But instead I see <code>ORTH</code>-part in <code>token.norm_</code> and nowhere in the <code>token</code>-instance I could see the <code>NORM</code>-part of rule.</p>

<p>So what is <code>Token.norm_</code>-member and what is <code>NORM</code>-part of <code>tokenizer_exceptions</code>-rule for?</p>
",Multilingual Language Processing & Language Identification,spacy norm part tokenizer exception adding language wa looking example english language wrote rule follows tokenize expect part rule though documentation instead see part nowhere instance could see part rule member part rule
how to use an arabic stemmer on ubuntu,"<p>I'm trying to test out stemming Arabic text using the ISRIStemmer tool but the GNOME terminal doesn't properly render Arabic text which is RTL.</p>

<p>I assume this means I need to have the texts I need in external documents and reference them in the code.</p>

<p>Can any one show me an example of how I might go about doing this?</p>
",Multilingual Language Processing & Language Identification,use arabic stemmer ubuntu trying test stemming arabic text using isristemmer tool gnome terminal properly render arabic text rtl assume mean need text need external document reference code one show example might go
How to find keyword related parts in text?,"<p>I am looking for a way, to find contextually related parts in a document to a given keyword. For example, I want to find parts, that are related to 'happiness'. Which should yield results like:</p>

<ul>
<li>... the boy was very happy ...</li>
<li>... a lot of people were celebrating ...</li>
<li>... everybody was having a good time ...</li>
</ul>

<p>I thought about generating related keywords and synonyms and search the text for these, but that might result in a lot of false positives, depending on how many related keywords will be used and the quality of these.</p>

<p>Natural language processing sounded promising, but does not really match this case as far as I can tell. It can tell me the sentiment and extract entities, but I would not know how to solve my problem.</p>

<p>Also it seems really hard to search for, as I do not really know how to formulate the question.</p>

<p>What direction should I be looking in? Is there a science or even a service that I could leverage?</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,find keyword related part text looking way find contextually related part document given keyword example want find part related happiness yield result like boy wa happy lot people celebrating everybody wa good time thought generating related keywords synonym search text might result lot false positive depending many related keywords used quality natural language processing sounded promising doe really match case far tell tell sentiment extract entity would know solve problem also seems really hard search really know formulate question direction looking science even service could leverage thanks
Which natural languages are supported by Google Cloud Natural Language API?,"<p>I can't find this crucial information in the official docs or here.
Which natural languages are supported by the service? Which languages will be supported in the near future?
I'm interested in POS tagging with lemmatization and dependency parsing. I tried analysing Russian, Polish and even Italian sentences and got a 400s (‚ÄúThe language ru is not supported for syntax analysis‚Äù etc.).
If advertising a service as multilingual, it would be fair to admit which languages are supported before having to register and enter credit card details.</p>
",Multilingual Language Processing & Language Identification,natural language supported google cloud natural language api find crucial information official doc natural language supported service language supported near future interested po tagging lemmatization dependency parsing tried analysing russian polish even italian sentence got language ru supported syntax analysis etc advertising service multilingual would fair admit language supported register enter credit card detail
Natural Language Processing of Topics,"<p>I'm a part of a group working on a big data course project and we've run into what we see as a problem for NLP. Currently we have groups of data formatted in JSON as such:</p>

<pre><code>    ""wine"": {
        ""category"": ""socializing"",
        ""category_id"": 31,
        ""score"": 0.0,
        ""topic_id"": 611
    }
    ""dragons"": {
        ""category"": ""lifestyle"",
        ""category_id"": 17,
        ""score"": 0.279108277990115,
        ""topic_id"": 2137
    },
    ""furry-fandom"": {
        ""category"": ""lifestyle"",
        ""category_id"": 17,
        ""score"": 0.279108277990115,
        ""topic_id"": 48595
    },
    ""legendarycreatures"": {
        ""category"": ""lifestyle"",
        ""category_id"": 17,
        ""score"": 0.279108277990115,
        ""topic_id"": 10523
    }
</code></pre>

<p>The tags are topics associated with relevant info (a category, popularity score, and a category/topic ID #). We have associated categories for each topic already since the API we're pulling from handles it. Our problem though is that the categories are too broad, with only 33, for identifying any meaningful trends and the topics are too specific w/overlap (e.g. dragons/legendarycreatures) and there are too many with approx 22,000.</p>

<p>This is where NLP comes in; we want to create some sort of set of super-topics that aren't as broad as ""category"" but not as specific as the current topics. An example using ""dragons"" and ""legandarycreatures"" again would be both, along with others, fitting into a super-topic of ""fantasy"".</p>

<p>A little more background, we're using Python to grab/process our data, we'd like to continue to use it for this, and none of us have any practical experience with NLP. </p>

<p>With this all in mind, we'd love to have some suggestions and help with this area of struggle. If there are better ways or maybe it isn't feasible with NLP, we are open to them. What we're trying to avoid though is hard coding some sort of table for categorization.</p>

<p>TL;DR: We're trying to categorize a set of 22,000 topics to appropriate ""super-topics"" that are more specific than the current ones but less broad than the current categories. We're trying to do this with NLP while using Python but don't know how to go about it and are open to suggestions as well.</p>
",Multilingual Language Processing & Language Identification,natural language processing topic part group working big data course project run see problem nlp currently group data formatted json tag topic associated relevant info category popularity score category topic id associated category topic already since api pulling handle problem though category broad identifying meaningful trend topic specific w overlap e g dragon legendarycreatures many approx nlp come want create sort set super topic broad category specific current topic example using dragon legandarycreatures would along others fitting super topic fantasy little background using python grab process data like continue use none u practical experience nlp mind love suggestion help area struggle better way maybe feasible nlp open trying avoid though hard coding sort table categorization tl dr trying categorize set topic appropriate super topic specific current one le broad current category trying nlp using python know go open suggestion well
How to tune a Machine Translation model with huge language model?,"<p><code>Moses</code> is a software to build machine translation models. And <code>KenLM</code> is the defacto language model software that moses uses.</p>

<p>I have a textfile with 16GB of text and i use it to build a language model as such:</p>

<pre><code>bin/lmplz -o 5 &lt;text &gt; text.arpa
</code></pre>

<p>The resulting file (<code>text.arpa</code>) is 38GB. Then I binarized the language model as such:</p>

<pre><code>bin/build_binary text.arpa text.binary
</code></pre>

<p>And the binarized language model (<code>text.binary</code>) grows to 71GB.</p>

<p>In <code>moses</code>, after training the translation model, you should tune the weights of the model by using <code>MERT</code> algorithm. And this can simply be done with <a href=""https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl"">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl</a>. </p>

<p>MERT works fine with small language model but with the big language model, it takes quite some days to finish. </p>

<p>I did a google search and found KenLM's filter, which promises to filter the language model to a smaller size: <a href=""https://kheafield.com/code/kenlm/filter/"">https://kheafield.com/code/kenlm/filter/</a></p>

<p>But i'm clueless as to how to make it work. The command help gives:</p>

<pre><code>$ ~/moses/bin/filter
Usage: /home/alvas/moses/bin/filter mode [context] [phrase] [raw|arpa] [threads:m] [batch_size:m] (vocab|model):input_file output_file

copy mode just copies, but makes the format nicer for e.g. irstlm's broken
    parser.
single mode treats the entire input as a single sentence.
multiple mode filters to multiple sentences in parallel.  Each sentence is on
    a separate line.  A separate file is created for each sentence by appending
    the 0-indexed line number to the output file name.
union mode produces one filtered model that is the union of models created by
    multiple mode.

context means only the context (all but last word) has to pass the filter, but
    the entire n-gram is output.

phrase means that the vocabulary is actually tab-delimited phrases and that the
    phrases can generate the n-gram when assembled in arbitrary order and
    clipped.  Currently works with multiple or union mode.

The file format is set by [raw|arpa] with default arpa:
raw means space-separated tokens, optionally followed by a tab and arbitrary
    text.  This is useful for ngram count files.
arpa means the ARPA file format for n-gram language models.

threads:m sets m threads (default: conccurrency detected by boost)
batch_size:m sets the batch size for threading.  Expect memory usage from this
    of 2*threads*batch_size n-grams.

There are two inputs: vocabulary and model.  Either may be given as a file
    while the other is on stdin.  Specify the type given as a file using
    vocab: or model: before the file name.  

For ARPA format, the output must be seekable.  For raw format, it can be a
    stream i.e. /dev/stdout
</code></pre>

<p>But when I tried the following, it gets stuck and does nothing:</p>

<pre><code>$ ~/moses/bin/filter union lm.en.binary lm.filter.binary
Assuming that lm.en.binary is a model file
Reading lm.en.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
</code></pre>

<p><strong>What should one do to the Language Model after binarization? Is there any other steps to manipulate large language models to reduce the
computing load when tuning?</strong></p>

<p><strong>What is the usual way to tune on a large LM file?</strong></p>

<p><strong>How to use KenLM's filter?</strong></p>

<p>(more details on <a href=""https://www.mail-archive.com/moses-support@mit.edu/msg12089.html"">https://www.mail-archive.com/moses-support@mit.edu/msg12089.html</a>)</p>
",Multilingual Language Processing & Language Identification,tune machine translation model huge language model software build machine translation model defacto language model software moses us textfile gb text use build language model resulting file gb binarized language model binarized language model grows gb training translation model tune weight model using algorithm simply done clueless make work command help give tried following get stuck doe nothing one language model binarization step manipulate large language model reduce computing load tuning usual way tune large lm file use kenlm filter detail href
Using NLP to Extract Information from check-ins and Comments,"<p>We're CS students and we're working on a recommendation system for our GP. 
Our data set contains users and the places they have visited, we want to use NLP to translate those places into activities.
ex. (Mall -> Shopping, Club -> Playing ...) and so on. 
and also if the users write down a comment we may know what kind of activity they are doing. </p>

<p>We're kinda lost at this point and wanna know from where should we start or what should we start searching for?</p>

<p>Thanks. </p>
",Multilingual Language Processing & Language Identification,using nlp extract information check comment c student working recommendation system gp data set contains user place visited want use nlp translate place activity ex mall shopping club playing also user write comment may know kind activity kinda lost point wan na know start start searching thanks
Implementing linguistic rules to transliterate Thai words,"<p>I can't find any open code in order to transliterate Thai script to a phonetic equivalent.
For example:
<code>‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ</code> which can be transliterated to <code>saÃÄ-waÃÄt-di</code>
It's like pinyin for Chinese. This enables learners of a foreign script to get the proper pronunciation even if one can't read the script.</p>

<p>There is a defined standard by ISO, which isn't usable since it's an exact transliteration, char by char. But Thai uses chars in front above and after other chars.</p>

<p>There are websites and software that can do transliteration automatically, so it's possible to do this automatically. Nonetheless there's no open code available and I searched in English as well as Thai language. There are some research papers on that but always without logic or code, describing that topic in a high-level fashion. </p>

<p>My approach so far:
I define the phonemes that are possible in the Thai language and transcribe them to English:</p>

<p>We have some simple letters that always make the same sound:</p>

<pre><code>/*
x - consonants:
‡∏Å   -&gt;  g
‡∏Ç   -&gt;  kh
‡∏Ñ   -&gt;  kh
‡∏Ü   -&gt;  kh
‡∏á   -&gt;  ng
‡∏à   -&gt;  dsch
‡∏â   -&gt;  dsch
‡∏ä   -&gt;  dsch
‡∏ã   -&gt;  s
‡∏å   -&gt;  dsch
‡∏ç   -&gt;  i
‡∏é   -&gt;  d
‡∏è   -&gt;  dt
‡∏ê   -&gt;  th
‡∏ë   -&gt;  th
‡∏í   -&gt;  th
‡∏ì   -&gt;  n
‡∏î   -&gt;  d
‡∏ï   -&gt;  dt
‡∏ñ   -&gt;  th
‡∏ó   -&gt;  th
‡∏ò   -&gt;  th
‡∏ô   -&gt;  n
‡∏ö   -&gt;  b
‡∏õ   -&gt;  bp
‡∏ú   -&gt;  ph
‡∏ù   -&gt;  f
‡∏û   -&gt;  ph
‡∏ü   -&gt;  f
‡∏†   -&gt;  ph
‡∏°   -&gt;  m
‡∏¢   -&gt;  i
‡∏£   -&gt;  r
‡∏•   -&gt;  l
‡∏ß   -&gt;  w
‡∏®   -&gt;  s
‡∏©   -&gt;  s
‡∏™   -&gt;  s
‡∏´   -&gt;  h
‡∏¨   -&gt;  l
‡∏≠   -&gt;  (silent)
‡∏Æ   -&gt;  h
</code></pre>

<p>Then we have vowels that contain the above consonants and finally make a syllable. In the vowel compounds below, the possible position of a consonant is substituted by an x.
The Z is a substitution of the consonants that can come at the end of a syllable. Not all of the above consonants can appear at position Z, only the ones described further below under Z.</p>

<pre><code>Vowels With ‡πÄ
‡πÄx‡∏µ‡∏¢‡∏ß   -&gt;  iauw
‡πÄx‡∏∑‡∏≠‡∏¢   -&gt;  √º√ºai
‡πÄx‡πá‡∏≠    -&gt;  √∂√∂h
‡πÄx‡∏¥Z    -&gt;  √∂h
‡πÄx‡∏≤‡∏∞    -&gt;  o
‡πÄx‡∏∞ -&gt;  e
‡πÄx‡∏µ‡∏¢‡∏∞   -&gt;  ia
‡πÄx‡∏∑‡∏≠‡∏∞   -&gt;  √ºa
‡πÄx‡πá‡∏ß    -&gt;  eu
‡πÄx‡πáZ    -&gt;  e
‡πÄx‡∏≤ -&gt;  au
‡πÄx‡∏µ‡∏¢Z   -&gt;  iia
‡πÄx‡∏∑‡∏≠Z   -&gt;  √º√ºa
‡πÄx‡∏≠‡∏∞    -&gt;  √∂
‡πÄxZ -&gt;  eh
‡πÄx‡∏ß -&gt;  eeu
‡πÄx‡∏µ‡∏¢    -&gt;  iia
‡πÄx‡∏∑‡∏≠    -&gt;  √º√ºa
‡πÄx‡∏≠ -&gt;  √∂√∂
‡πÄx  -&gt;  ee
‡πÄx‡∏¢ -&gt;  √∂√∂i

Vowels With ‡πÅ
‡πÅx‡πá‡∏ß    -&gt;  √§u
‡πÅx‡∏ß -&gt;  √§√§u
‡πÅx‡∏∞ -&gt;  √§
‡πÅx‡πáZ    -&gt;  √§
‡πÅxZ -&gt;  √§√§
‡πÅx  -&gt;  √§√§

Vowels With ‡πÑ , ‡πÉ Or ‡πÇ
‡πÇxZ -&gt;  oo
‡πÇx  -&gt;  oo
‡πÇx‡∏ß -&gt;  oou
‡πÇx‡∏∞ -&gt;  o
‡πÑx  -&gt;  ai
‡πÉx  -&gt;  ai
‡πÑx‡∏¢ -&gt;  ai
‡πÇx‡∏¢ -&gt;  ooi

Vowels With ‡∏≠‡∏¥ Or ‡∏≠‡∏µ
x‡∏¥Z -&gt;  i
x‡∏¥  -&gt;  i
x‡∏¥‡∏ß -&gt;  iu
x‡∏µZ -&gt;  ii
x‡∏µ      -&gt;  ii

Vowels With ‡∏≠‡∏∏ Or ‡∏≠‡∏π
x‡∏∏Z -&gt;  u
x‡∏∏  -&gt;  u
x‡∏∏‡∏¢ -&gt;  ui
x‡∏πZ -&gt;  uu
x‡∏π  -&gt;  uu

Vowels With ‡∏≠‡∏∂ Or ‡∏≠‡∏∑
x‡∏∂Z -&gt;  √º
x‡∏∂  -&gt;  √º
x‡∏∑‡∏≠ -&gt;  √º√º
x‡∏∑Z -&gt;  √º√º

Vowels With ‡∏≤
x‡∏≤‡∏¢ -&gt;  aai
x‡∏≤‡∏ß -&gt;  aau
x‡∏≤Z -&gt;  aa
x‡∏≤  -&gt;  aa

Vowels With ‡∏≠
x‡πá‡∏≠Z    -&gt;  o
x‡πá‡∏≠‡∏¢    -&gt;  oi
x‡∏≠‡∏¢ -&gt;  ooi
x‡∏≠Z -&gt;  oo
x‡∏≠  -&gt;  oo

Vowels With ‡∏≠‡∏± Or ‡∏ß
x‡∏±‡∏ß‡∏∞    -&gt;  ua
x‡∏±‡∏¢ -&gt;  ai
x‡∏±‡∏ß -&gt;  ua
x‡∏±Z -&gt;  a
x‡∏ß‡∏¢ -&gt;  uai
x‡∏ßZ -&gt;  ua

Remaining Vowels
x‡∏∞  -&gt;  a
x‡πà‡∏≤ -&gt;  am
Inherent 'o'    -&gt;  o

Z - possible final consonants:
‡∏Å   -&gt;  g
‡∏á   -&gt;  ng
‡∏î   -&gt;  dt
‡∏ô   -&gt;  n
‡∏ö   -&gt;  b
‡∏°   -&gt;  m
*/
</code></pre>

<p>I am not sure what approach would be short enough to contain all of those rules, without retyping a lot of if/else or switch/cases. Furthermore a syllable string can contain an optional character: a tone mark. This is not always the case but often. So my approach would be, to first strip the vowel string from the tone mark and then process some rule like:</p>

<pre><code>// example ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß -&gt;     diau
string vowel001 = ""iau"";

string thaiword = ""‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"";
if( thaiword.substring(0,1)==""‡πÄ"" &amp;&amp;
    thaiword.substring(1,2)==""‡∏µ"" &amp;&amp; /*the unicode character moves above "" which we don't have to care about*/
    thaiword.substring(2,3)==""‡∏¢"" &amp;&amp; 
    thaiword.substring(3,4)==""‡∏ß""){
    return getConsonantTransliteration(string.pos.1) + vowel001;
}
</code></pre>

<p>Is there a faster and more oop approach to do that, instead of writing such a rule for every possibility?</p>

<p>Edit2:</p>

<p>Currently the fastest way seems to be a Regular Expression approach:</p>

<p>Given the example <code>‡πÄ‡∏≠‡∏µ‡∏¢‡∏ß</code> vowel and all it's possible variations:</p>

<pre><code>‡πÄ‡∏•‡∏µ‡∏¢‡∏ß
‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏ß
‡πÄ‡∏´‡∏•‡∏µ‡∏¢‡∏ß
‡πÄ‡∏´‡∏•‡∏µ‡πâ‡∏¢‡∏ß
‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏ß

find:       ‡πÄ(‡∏´{0,1})([‡∏Å-‡∏Æ])‡∏µ([‡πà‡πâ‡πä‡πã]{0,1})‡∏¢‡∏ß
replace:    silenth[\1] initialcons[\2] tone[\3]

‡πÄ‡∏•‡∏µ‡∏¢‡∏ß --&gt;   silenth[] initialcons[‡∏•] tone[]
‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏ß --&gt;  silenth[] initialcons[‡∏•] tone[‡πâ]
‡πÄ‡∏´‡∏•‡∏µ‡∏¢‡∏ß --&gt;  silenth[‡∏´] initialcons[‡∏•] tone[]
‡πÄ‡∏´‡∏•‡∏µ‡πâ‡∏¢‡∏ß --&gt; silenth[‡∏´] initialcons[‡∏•] tone[‡πâ]
‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏ß --&gt;  silenth[] initialcons[‡∏´] tone[‡πâ]
</code></pre>
",Multilingual Language Processing & Language Identification,implementing linguistic rule transliterate thai word find open code order transliterate thai script phonetic equivalent example transliterated like pinyin chinese enables learner foreign script get proper pronunciation even one read script defined standard iso usable since exact transliteration char char thai us char front char website software transliteration automatically possible automatically nonetheless open code available searched english well thai language research paper always without logic code describing topic high level fashion approach far define phoneme possible thai language transcribe english simple letter always make sound vowel contain consonant finally make syllable vowel compound possible position consonant substituted x z substitution consonant come end syllable consonant appear position z one described z sure approach would short enough contain rule without retyping lot else switch case furthermore syllable string contain optional character tone mark always case often approach would first strip vowel string tone mark process rule like faster oop approach instead writing rule every possibility edit currently fastest way seems regular expression approach given example vowel possible variation
Looking for a database or text file of english words with their different forms,"<p>I am working on a project and I need to get the root of a given word (stemming). As you know, the stemming algorithms that don't use a dictionary are not accurate. Also I tried the WordNet but it is not good for my project. I found phpmorphy project but it doesn't include API in Java. </p>

<p>At this time I am looking for a database or a text file of english words with their different forms. for example:</p>

<p>run running ran ...
include including included ...
...</p>

<p>Thank you for your help or advise. </p>
",Multilingual Language Processing & Language Identification,looking database text file english word different form working project need get root given word stemming know stemming algorithm use dictionary accurate also tried wordnet good project found phpmorphy project include api java time looking database text file english word different form example run running ran include including included thank help advise
NLP timezone issue,"<p>Currently i am using Bot Framework with Node for my chat bot and i am using API AI as my Natural Language Processing. </p>

<p>My question is, let say the user replied with ""yesterday"" or ""today"" or ""tomorrow"" and it is base on the server time. Now, it will go wrong if a person in Australia says today at 6 AM in the morning and their time which is in GMT will still be previous day and this gives a wrong date.</p>

<p>May i have suggestion on how do i solve this timezone issue?</p>
",Multilingual Language Processing & Language Identification,nlp timezone issue currently using bot framework node chat bot using api ai natural language processing question let say user replied yesterday today tomorrow base server time go wrong person australia say today morning time gmt still previous day give wrong date may suggestion solve timezone issue
Swedish NLP for product search engine with structured data,"<p>I‚Äôm currently working on a project in which I have a database with 1000 products (washing machines), where each one has 21 product attributes (such as weight, dimensions, colour, power consumption and so on.)
My aim to use NLP to make the users able to search the database of products by queries of natural language, like:</p>
<blockquote>
<p>‚ÄúFind a washing machine that can load at least 8 kg of laundry, and with height no more than 60 cm and with a front of stainless steel‚Äù</p>
<p>‚ÄúI‚Äôm looking for a washing machine that costs less than 6000 SEK and has the opening in the front, not in the top‚Äù</p>
</blockquote>
<p>This NL query needs to be translated to a SQL-query to be used with my database. The problem is that I would need it to work in the Swedish language.
I‚Äôve found a great API (<a href=""https://json-tagger.com/"" rel=""nofollow noreferrer"">https://json-tagger.com/</a>) that does the pre-processing of the sentences for me, tokenization and tagging Part of speech in Swedish. Thanks! But now I would really like some tips on how I best use this to translate it to SQL-queries?</p>
<p>I guess I would need to extract the relations and semantics of the user input in order to query the database, but I‚Äôm not sure how to do this. As it is a fairly limited area (washing machine product search) I hope I can construct some rules for doing this, but I‚Äôm not sure if that is the right way to go. Any help or ideas are very appreciated! :)</p>
<p>I kind of new to NLP and would really prefer working in Python3. Thank you!</p>
",Multilingual Language Processing & Language Identification,swedish nlp product search engine structured data currently working project database product washing machine one ha product attribute weight dimension colour power consumption aim use nlp make user able search database product query natural language like find washing machine load least kg laundry height cm front stainless steel looking washing machine cost le sek ha opening front top nl query need translated sql query used database problem would need work swedish language found great api doe pre processing sentence tokenization tagging part speech swedish thanks would really like tip best use translate sql query guess would need extract relation semantics user input order query database sure fairly limited area washing machine product search hope construct rule sure right way go help idea appreciated kind new nlp would really prefer working python thank
"Independent clause boundary disambiguation, and independent clause segmentation ‚Äì any tools to do this?","<p>I remember skimming the sentence segmentation section from the NLTK site a long time ago. </p>

<p>I use a crude text replacement of ‚Äúperiod‚Äù ‚Äúspace‚Äù with ‚Äúperiod‚Äù ‚Äúmanual line break‚Äù to achieve sentence segmentation, such as with a Microsoft Word replacement (<code>.</code> -> <code>.^p</code>) or a Chrome extension:</p>

<p><a href=""https://github.com/AhmadHassanAwan/Sentence-Segmentation"" rel=""nofollow noreferrer"">https://github.com/AhmadHassanAwan/Sentence-Segmentation</a></p>

<p><a href=""https://chrome.google.com/webstore/detail/sentence-segmenter/jfbhkblbhhigbgdnijncccdndhbflcha"" rel=""nofollow noreferrer"">https://chrome.google.com/webstore/detail/sentence-segmenter/jfbhkblbhhigbgdnijncccdndhbflcha</a></p>

<p>This is instead of an NLP method like the Punkt tokenizer of NLTK. </p>

<p>I segment to help me more easily locate and reread sentences, which can sometimes help with reading comprehension. </p>

<p>What about independent clause boundary disambiguation, and independent clause segmentation? Are there any tools that attempt to do this?</p>

<p>Below is some example text. If an independent clause can be identified within a sentence, there‚Äôs a split. Starting from the end of a sentence, it moves left, and greedily splits:</p>

<p>E.g.</p>

<blockquote>
  <p><strong>Sentence</strong> boundary disambiguation
  (SBD), also known as sentence
  breaking, is the problem in natural
  language processing of deciding where </p>
  
  <p>sentences begin and end. </p>
  
  <p><strong>Often</strong>, natural language processing
  tools </p>
  
  <p>require their input to be divided into
  sentences for a number of reasons. </p>
  
  <p><strong>However</strong>, sentence boundary
  identification is challenging because punctuation </p>
  
  <p>marks are often ambiguous.</p>
  
  <p><strong>For</strong> example, a period may </p>
  
  <p>denote an abbreviation, decimal point,
  an ellipsis, or an email address - not
  the end of a sentence. </p>
  
  <p><strong>About</strong> 47% of the periods in the Wall
  Street Journal corpus </p>
  
  <p>denote abbreviations.[1] </p>
  
  <p><strong>As</strong> well, question marks and
  exclamation marks may </p>
  
  <p>appear in embedded quotations,
  emoticons, computer code, and slang.</p>
  
  <p><strong>Another</strong> approach is to automatically </p>
  
  <p>learn a set of rules from a set of
  documents where the sentence</p>
  
  <p>breaks are pre-marked. </p>
  
  <p><strong>Languages</strong> like Japanese and Chinese </p>
  
  <p>have unambiguous sentence-ending
  markers.</p>
  
  <p><strong>The</strong> standard 'vanilla' approach to </p>
  
  <p>locate the end of a sentence:</p>
  
  <p>(a) <strong>If</strong> </p>
  
  <p>it's a period,  </p>
  
  <p>it ends a sentence.</p>
  
  <p>(b) <strong>If</strong> the preceding  </p>
  
  <p>token is on my hand-compiled list of
  abbreviations, then  </p>
  
  <p>it doesn't end a sentence.</p>
  
  <p>(c) <strong>If</strong> the next  </p>
  
  <p>token is capitalized, then  </p>
  
  <p>it ends a sentence.</p>
  
  <p><strong>This</strong> </p>
  
  <p>strategy gets about 95% of sentences
  correct.[2]</p>
  
  <p><strong>Solutions</strong> have been based on a maximum
  entropy model.[3] </p>
  
  <p><strong>The</strong> SATZ architecture uses a neural
  network to </p>
  
  <p>disambiguate sentence boundaries and
  achieves 98.5% accuracy.</p>
</blockquote>

<p>(I‚Äôm not sure if I split it properly.)</p>

<p>If there are no means to segment independent clauses, are there any search terms that I can use to further explore this topic?</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,independent clause boundary disambiguation independent clause segmentation tool remember skimming sentence segmentation section nltk site long time ago use crude text replacement period space period manual line break achieve sentence segmentation microsoft word replacement chrome extension instead nlp method like punkt tokenizer nltk segment help easily locate reread sentence sometimes help reading comprehension independent clause boundary disambiguation independent clause segmentation tool attempt example text independent clause identified within sentence split starting end sentence move left greedily split e g sentence boundary disambiguation sbd also known sentence breaking problem natural language processing deciding sentence begin end often natural language processing tool require input divided sentence number reason however sentence boundary identification challenging punctuation mark often ambiguous example period may denote abbreviation decimal point ellipsis email address end sentence period wall street journal corpus denote abbreviation well question mark exclamation mark may appear embedded quotation emoticon computer code slang another approach automatically learn set rule set document sentence break pre marked language like japanese chinese unambiguous sentence ending marker standard vanilla approach locate end sentence period end sentence b preceding token hand compiled list abbreviation end sentence c next token capitalized end sentence strategy get sentence correct solution based maximum entropy model satz architecture us neural network disambiguate sentence boundary achieves accuracy sure split properly mean segment independent clause search term use explore topic thanks
JSON to API.AI or Wit.AI,"<p>I have a decision tree in JSON. </p>

<p>Right now, I am using that JSON tree in the flow in my website. </p>

<p>The tree has a structure similar to below</p>

<p><code>{
    ""id"": 1,
    ""name"": ""Select the vehicle type"",
    ""children"": [{
        ""id"": ""2"",
        ""name"": ""Show the Cars"",
        ""parent"": ""1"",
        ""children"": [{
            ""id"": 3,
            ""name"": ""BMW"",
            ""parent"": 2
        }]
    }]
}</code></p>

<p>Now, I want to use the tree and convert to Natural Language Processing flow using <strong>API.AI</strong> or <strong>Wit.AI</strong>.</p>

<p>How can I do that? Is there a tool to do that? Or the only way is to manually  create again the flow? </p>
",Multilingual Language Processing & Language Identification,json api ai wit ai decision tree json right using json tree flow website tree ha structure similar want use tree convert natural language processing flow using api ai wit ai tool way manually create flow
Python - Nltk pos spanish,"<p>I'm new to <code>nlp</code> and i have been reading a lot about <code>nltk</code>. I'm from South America and I want to start using nltk with Spanish texts. I read the docs but i only found how to stem Spanish words. Maybe I am wrong or maybe I just could not find that part of the documentation. I also read a few blogs about the subject.</p>

<p>The question is: Is there an ""official"" or most practical way to use pos tags in Spanish?</p>
",Multilingual Language Processing & Language Identification,python nltk po spanish new reading lot south america want start using nltk spanish text read doc found stem spanish word maybe wrong maybe could find part documentation also read blog subject question official practical way use po tag spanish
How to format preg_match statement to receive remote text?,"<p>I am currently working on a database with words from the english dictionary (selling original usernames) . My objective is to be able to display the definition of the usernames added to the database. Right now I'm working on proof of concept before applying the words from my database , but I cannot figure out how to format the preg match statements to get whether the word is a verb or noun etc, and the definition.<br>My code so far:</p>

<pre><code>$content = file_get_contents(""http://www.dictionary.com/browse/abut"");

preg_match(?);
preg_match(?);

$class = $classmatch[];
$def = $defmatch[];


echo "" $class &lt;br&gt; $def"";
</code></pre>
",Multilingual Language Processing & Language Identification,format preg match statement receive remote text currently working database word english dictionary selling original usernames objective able display definition usernames added database right working proof concept applying word database figure format preg match statement get whether word verb noun etc definition code far
Improving google translator api accuracy by using domain specific knowledge,"<p>Ive got some french text data (20000 lines) and corresponding english translation.</p>

<p>This data is specific to insurance domain and hence will have some restrictions due 
to vocabulary. Now when Im using google to translate to do it, its giving me 90% accuracy
wrt translation. I want to improve this to 95+.</p>

<p>Is there a way/method wherein I can improve
the translation output which Im getting by google by passing it through another algorithm
based on insurance domain etc to improve upon the final accuracy?</p>
",Multilingual Language Processing & Language Identification,improving google translator api accuracy using domain specific knowledge ive got french text data line corresponding english translation data specific insurance domain hence restriction due vocabulary im using google translate giving accuracy wrt translation want improve way method wherein improve translation output im getting google passing another algorithm based insurance domain etc improve upon final accuracy
SnowballStemmer for Russian words list,"<p>I do know how to perform SnowballStemmer on a single word (in my case, on russian one). Doing the next things:</p>

<pre><code>from nltk.stem.snowball import SnowballStemmer 

stemmer = SnowballStemmer(""russian"") 
stemmer.stem(""–í–∞—Å–∏–ª–∏–π"")
'–í–∞—Å–∏–ª'
</code></pre>

<p>How can I do the following if I have a list of words like ['–í–∞—Å–∏–ª–∏–π', '–ì–µ–Ω–Ω–∞–¥–∏–π', '–í–∏—Ç–∞–ª–∏–π']?</p>

<p>My approach using for loop seems to be not working :( </p>

<pre><code>l=[stemmer.stem(word) for word in l]
</code></pre>
",Multilingual Language Processing & Language Identification,snowballstemmer russian word list know perform snowballstemmer single word case russian one next thing following list word like approach using loop seems working
Unexpected format when running StanfordPOSTagger with NLTK for Chinese,"<p>I have installed Python 3.6.0, NLTK 3.2.4, and downloaded Stanford POS Tagger 3.8.0.</p>

<p>Then I tried running the following script:</p>

<pre><code>#!/usr/bin/env python3

from nltk.tag import StanfordPOSTagger


st = StanfordPOSTagger('chinese-distsim.tagger')
print(st.tag('Ëøô ÊòØ ÊñØÂù¶Á¶è ‰∏≠Êñá ÂàÜËØçÂô® ÊµãËØï'.split()))
</code></pre>

<p>and the output is in an unexpected format:</p>

<pre><code>[('', 'Ëøô#PN'), ('', 'ÊòØ#VC'), ('', 'ÊñØÂù¶Á¶è#NR'), ('', '‰∏≠Êñá#NN'), ('', 'ÂàÜËØçÂô®#NN'), ('', 'ÊµãËØï#NN')]
</code></pre>

<p>The tagger does do its job, but the words and their parts of speech are not separated as a pair, but joined by a '#' to form single strings. Is this the format specially for Chinese, or is there something wrong?</p>
",Multilingual Language Processing & Language Identification,unexpected format running stanfordpostagger nltk chinese installed python nltk downloaded stanford po tagger tried running following script output unexpected format tagger doe job word part speech separated pair joined form single string format specially chinese something wrong
join quanteda dfm top ten 1grams with all dfm 2 thru 5grams,"<p>To conserve memory space when dealing with a very large corpus sample i'm looking to take just the top 10 1grams and combine those with all of the 2 thru 5grams to form my single quanteda::dfmSparse object that will be used in natural language processing [nlp] predictions. Carrying around all the 1grams will be pointless because only the top ten [ or twenty ] will ever get used with the simple back off model i'm using.  </p>

<p>I wasn't able to find a quanteda::dfm(corpusText, . . .) parameter that instructs it to only return the top ## features. So based on comments from package author @KenB in other threads i'm using the dfm_select/remove functions to extract the top ten 1grams and based on the ""quanteda dfm join"" search results hit ""<a href=""https://stackoverflow.com/questions/37363711/concatenate-dfm-matrices-in-quanteda-package"">concatenate dfm matrices in 'quanteda' package</a>"" i'm using rbind.dfmSparse??? function to join those results.</p>

<p>So far everything looks right from what i can tell. Thought i'd bounce this game plan off of SO community to see if i'm overlooking a more efficient route to arrive at this result or some flaw in solution I've arrived at thus far.</p>

<pre><code>corpusObject &lt;- quanteda::corpus(paste(""some corpus text of no consequence that in practice is going to be very large\n"",
    ""and so one might expect a very large number of ngrams but for nlp purposes only care about top ten\n"",
    ""adding some corpus text word repeats to ensure 1gram top ten selection approaches are working\n""))
corpusObject$documents
dfm1gramsSorted &lt;- dfm_sort(dfm(corpusObject, tolower = T, stem = F, ngrams = 1))
dfm2to5grams &lt;- quanteda::dfm(corpusObject, tolower = T, stem = F, ngrams = 2:5)
dfm1gramsSorted; dfm2to5grams 
#featnames(dfm1gramsSorted); featnames(dfm2to5grams)
#colSums(dfm1gramsSorted); colSums(dfm2to5grams)

dfm1gramsSortedLen &lt;- length(featnames(dfm1gramsSorted))
# option1 - select top 10 features from dfm1gramsSorted
dfmTopTen1grams &lt;- dfm_select(dfm1gramsSorted, pattern = featnames(dfm1gramsSorted)[1:10]) 
dfmTopTen1grams; featnames(dfmTopTen1grams)
# option2 - drop all but top 10 features from dfm1gramsSorted
dfmTopTen1grams &lt;- dfm_remove(dfm1gramsSorted, pattern = featnames(dfm1gramsSorted)[11:dfm1gramsSortedLen]) 
dfmTopTen1grams; featnames(dfmTopTen1grams)

dfmTopTen1gramsAndAll2to5grams &lt;- rbind(dfmTopTen1grams, dfm2to5grams)
dfmTopTen1gramsAndAll2to5grams;
#featnames(dfmTopTen1gramsAndAll2to5grams); colSums(dfmTopTen1gramsAndAll2to5grams)
data.table(ngram = featnames(dfmTopTen1gramsAndAll2to5grams)[1:50], frequency = colSums(dfmTopTen1gramsAndAll2to5grams)[1:50],
keep.rownames = F, stringsAsFactors = F)
</code></pre>

<p>/eoq</p>
",Multilingual Language Processing & Language Identification,join quanteda dfm top ten gram dfm thru gram conserve memory space dealing large corpus sample looking take top gram combine thru gram form single quanteda dfmsparse object used natural language processing nlp prediction carrying around gram top ten twenty ever get used simple back model using able find quanteda dfm corpustext parameter instructs return top feature based comment package author kenb thread using dfm select remove function extract top ten gram based quanteda dfm join search result hit far everything look right tell thought bounce game plan community see overlooking efficient route arrive result flaw solution arrived thus far eoq
Very simple text classification by machine learning?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/8136677/text-classification-into-categories"">Text Classification into Categories</a>  </p>
</blockquote>



<p>I am currently working on a solution to get the type of food served in a database with 10k restaurants based on their description. I'm using lists of keywords to decide which kind of food is being served.</p>

<p>I read a little bit about machine learning but I have no practical experience with it at all. Can anyone explain to me if/why it would a be better solution to a simple problem like this? I find accuracy more important than performance!</p>

<p>simplified example:</p>

<pre><code>[""China"", ""Chinese"", ""Rice"", ""Noodles"", ""Soybeans""]
[""Belgium"", ""Belgian"", ""Fries"", ""Waffles"", ""Waterzooi""]
</code></pre>

<p>a possible description could be:</p>

<p>""Hong's Garden Restaurant offers savory, reasonably priced <strong>Chinese</strong> to our customers. If you find that you have a sudden craving for
<strong>rice</strong>, <strong>noodles</strong> or <strong>soybeans</strong> at 8 o‚Äôclock on a Saturday evening, don‚Äôt worry! We‚Äôre open seven days a week and offer carryout service. You can get <strong>fries</strong> here as well!""</p>
",Multilingual Language Processing & Language Identification,simple text classification machine learning possible duplicate chinese customer find sudden craving rice noodle soybean clock saturday evening worry open seven day week offer carryout service get fry well
"When using word alignment tools like fast_align, does more sentences mean better accuracy?","<p>I am using fast_align <a href=""https://github.com/clab/fast_align"" rel=""nofollow noreferrer"">https://github.com/clab/fast_align</a> to get word alignments between 1000 German sentences and 1000 English translations of those sentences. So far the quality is not so good. </p>

<p>Would throwing more sentences into the process help fast_align to be more accurate? Say I take some OPUS data with 100k aligned sentence pairs and then add my 1000 sentences in the end of it and feed it to fast_align. Will that help? I can't seem to find any info on whether this would make sense.</p>
",Multilingual Language Processing & Language Identification,using word alignment tool like fast align doe sentence mean better accuracy using fast align get word alignment german sentence english translation sentence far quality good would throwing sentence process help fast align accurate say take opus data k aligned sentence pair add sentence end feed fast align help seem find info whether would make sense
extract a linguistic structure based on POS tagged sentence using Stanford nlp in JAVA,"<p>I am new in Natural Language Processing (NLP), I want to do part-of-speech tagging (POS) and then do find a specific structure within a text. I could manage POS tagging using Stanford-NLP but, I do not know how to extract this structure:</p>

<blockquote>
  <p><code>NN/NNS + IN + DT + NN/NNS/NNP/NNPS</code></p>
</blockquote>

<pre><code>public static void main(String args[]) throws Exception{
    //input File
    String contentFilePath = """";
    //outputFile
    String triplesFilePath = contentFilePath.substring(0, contentFilePath.length()-4)+""_postagg.txt"";

    //document to POS tagging
    String content = getFileContent(contentFilePath);

    Properties props = new Properties();

    props.setProperty(""annotators"",""tokenize, ssplit, pos"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    // Annotate the document.
    Annotation doc = new Annotation(content);
    pipeline.annotate(doc);


    // Annotate the document.
    List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
    for (CoreMap sentence : sentences) {
        for (CoreLabel token: sentence.get(CoreAnnotations.TokensAnnotation.class)) {
            String word = token.get(CoreAnnotations.TextAnnotation.class);
            // this is the POS tag of the token
            String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
            System.out.println(word + ""/"" + pos);
        } }}}
</code></pre>
",Multilingual Language Processing & Language Identification,extract linguistic structure based po tagged sentence using stanford nlp java new natural language processing nlp want part speech tagging po find specific structure within text could manage po tagging using stanford nlp know extract structure
I want to make a app which can extract information from the chat and add entry in calendar for specific messages using natural language processing,"<p>I want to make a app which can extract information from  the chat and add entry in calendar for specific messages using natural language processing.</p>

<p>for example if we get message something like this in our chat app 
   ""Meet me today at 8:00 pm on xyz street.""</p>

<p>then our app should make entry in calendar for time, location and title of an event.</p>

<p>So please suggest me how to proceed and what give idea about brief steps to implement this using natural language processing. </p>
",Multilingual Language Processing & Language Identification,want make app extract information chat add entry calendar specific message using natural language processing want make app extract information chat add entry calendar specific message using natural language processing example get message something like chat app meet today pm xyz street app make entry calendar time location title event please suggest proceed give idea brief step implement using natural language processing
Is it possible to guess a user&#39;s mood based on the structure of text?,"<p>I assume a natural language processor would need to be used to parse the text itself, but what suggestions do you have for an algorithm to detect a user's mood based on text that they have written? I doubt it would be very accurate, but I'm still interested nonetheless.</p>

<p>EDIT: I am by no means an expert on linguistics or natural language processing, so I apologize if this question is too general or stupid.</p>
",Multilingual Language Processing & Language Identification,possible guess user mood based structure text assume natural language processor would need used parse text suggestion algorithm detect user mood based text written doubt would accurate still interested nonetheless edit mean expert linguistics natural language processing apologize question general stupid
Splitting chinese document into sentences,"<p>I have to split Chinese text into multiple sentences. I tried the Stanford DocumentPreProcessor. It worked quite well for English but not for Chinese.</p>

<p>Please can you let me know any good sentence splitters for Chinese preferably in Java or Python.</p>
",Multilingual Language Processing & Language Identification,splitting chinese document sentence split chinese text multiple sentence tried stanford documentpreprocessor worked quite well english chinese please let know good sentence splitter chinese preferably java python
java and nlp to extract information (malware names) and filtering,"<p>I have been thinking and working on a homemade algorithm to extract malware names from a dataset. The results were not so promising.</p>

<p>My dataset looks like this: </p>

<pre><code>torrentlocker payment site
win32/somoto.e potentially unwanted
financial services;malicious sources/malnets;personal sites
tv/video streams;piracy/copyright concerns;entertainment;malicious     sources/malnets
locky;malware;dark;stealing;infected
</code></pre>

<p>What I would like to have as output is:</p>

<pre><code>torrentlocker payment site -&gt; torrentlocker
win32/somoto.e potentially unwanted -&gt; win32/somoto.e
financial services;malicious sources/malnets;personal sites -&gt; null 
tv/video streams;piracy/copyright concerns;entertainment;malicious sources/malnets -&gt; null 
locky;malware;dark;stealing;infected -&gt; locky
</code></pre>

<p>In general the approach that I think may work is to extract the words with meanings (copyright, payment site, unwanted...). Maybe there is an easy way (better than scraping a online dictionary) to have all the words with meaning and then compare them with my data? I found through searching that NLP (natural language processing) is a way to do it. I would like to have some recommendations if there's a better way to do this, especially that I prefer working with Java.</p>
",Multilingual Language Processing & Language Identification,java nlp extract information malware name filtering thinking working homemade algorithm extract malware name dataset result promising dataset look like would like output general approach think may work extract word meaning copyright payment site unwanted maybe easy way better scraping online dictionary word meaning compare data found searching nlp natural language processing way would like recommendation better way especially prefer working java
How to assign predefined tags to customer complaints,"<p>I have a problem in natural language processing:</p>

<p>I want to assign tags to customer complaints. For every complaint, it needs to be possible to assign several tags. </p>

<p>For example:
'Your food and service is terrible'
- I would want to assign the tags 'food' and 'service'</p>

<p>I have a dataset to which I can assign tags manually for every tag and line.</p>

<p>I could run seperate logistic regressions to assign a likelihood for every line and tag based on word count.</p>

<p>Is there a better approach?</p>

<p>I know I could do LDA, but I find it a bit weird that I would have to predefine the number of topics.</p>

<p>Thanks,
Felix</p>
",Multilingual Language Processing & Language Identification,assign predefined tag customer complaint problem natural language processing want assign tag customer complaint every complaint need possible assign several tag example food service terrible would want assign tag food service dataset assign tag manually every tag line could run seperate logistic regression assign likelihood every line tag based word count better approach know could lda find bit weird would predefine number topic thanks felix
Natural language processing. POS tagging and syntax analysis,"<p>I am currently working hard on implementing my own library for English language processing. The real challenge is to go through all abundance of theoretical material and get the quantum of understading how to put it all on rails of production.</p>

<p>I have made some progress so far. I implemented end-of-sentence detector and Early parser. But the fact is unless I include in my terminal dictionary the specific word the parser can not recognize it and build chart.</p>

<p>To be more explicit please review the following example of my CFGrammar:</p>

<pre><code>Production[] ppTerminals = { new Production(new Word[] { new Terminal(""Preposition""), new NonTerminal(""NP"") })};
AddProduction(ppTerminal, ""PP""); // Add production
...
DictionaryBuilder(Prepositions.SingleWord, ""Preposition""); //Where
Prepositions.SingleWord is a hard-coded list of possible prepositions.
</code></pre>

<p>As a result if Earley parser comes across, let's say, unknown two-word PP like ""up to"" it will fail to recognize it and build chart.</p>

<p>So I think I need something else prior to syntax parser that will handles my sentences and then forward relevant data to parser. The main idea is that the dictionary was built up dynamically at the stage of POS tagging, then Earley parser can recognize a word.</p>

<p>I implemented tokenizer and lexer. As an output I have got S-Expression tree like:</p>

<pre><code>(sentence
  (word BOND)
  (word TRADING)
  (word REVENUES)
  (word AT)
  (word GOLDMAN)
  (word SACHS)
  (word SLID)
  (value 40%)
  ...
  (word AND)
  (word CURRENCIES)
  (word WAS)
  (currency $1.16BN)
  ...
 )
</code></pre>

<p>But I'm familiar with Hidden Markov Model and such algorithms like the Viterbi algorithm for finding the most likely state sequence and the Baum-Welch algorithm for parameter estimation.</p>

<p>Could you please give me just a piece of advice how to link together Earley parser and POS tagging based on HMM. Or, propabably, I am going a wrong direction so then please point out where I run wrong.  Now I am a bit confused. Thank you!</p>
",Multilingual Language Processing & Language Identification,natural language processing po tagging syntax analysis currently working hard implementing library english language processing real challenge go abundance theoretical material get understading put rail production made progress far implemented end sentence detector early parser fact unless include terminal dictionary specific word parser recognize build chart explicit please review following example cfgrammar result earley parser come across let say unknown two word pp like fail recognize build chart think need something else prior syntax parser handle sentence forward relevant data parser main idea dictionary wa built dynamically stage po tagging earley parser recognize word implemented tokenizer lexer output got expression tree like familiar hidden markov model algorithm like viterbi algorithm finding likely state sequence baum welch algorithm parameter estimation could please give piece advice link together earley parser po tagging based hmm propabably going wrong direction please point run wrong bit confused thank
my wordnet lemmatizer is not lemmatizing words correctly,"<p>this code is running but it is not giving expected results after lemmatization. Most of the words are same after lemmatization. Can anyone highlight my mistake or suggest me some other lemmatizer for english.
Input is a column of a table e.g few rows are Alphabets, reveals, ,Bestowed , going Output: Alphabet, reveal, bestow, go.</p>

<pre><code>    import pymysql
    from nltk.stem import PorterStemmer
    from nltk.stem import WordNetLemmatizer
    conn=pymysql.connect(""localhost"",""root"",""root"",""qwn1"")
    cursor=conn.cursor()
    sql=""select lemma from quranic_words where new_word=1 ""
    cursor.execute(sql)
    results=cursor
    while results:
        results=cursor.fetchone()
        lemmatizer=WordNetLemmatizer()
        print( ""results"", repr(results))
        for w in results:
            print (lemmatizer.lemmatize(w))
    cursor.close()
    conn.close()  
</code></pre>
",Multilingual Language Processing & Language Identification,wordnet lemmatizer lemmatizing word correctly code running giving expected result lemmatization word lemmatization anyone highlight mistake suggest lemmatizer english input column table e g row alphabet reveals bestowed going output alphabet reveal bestow go
Is there any Portuguese Treebank with chunked sents for NLTK?,"<p>My problem is that I need to retrieve NP chunks from texts using NLTK, but I do not want use regex chunkers. I am new to NLP, but I think classification-based chunking (IOB tags) would be much better than regular expression chunking. I can only find chunked sents in languages that are not Portuguese (Brazil), most of them in English.</p>

<p>Actually, my main problem is to extract Person and Organization Entities from text, so I am first trying to get the NP chunks. But I am not sure if I am trailling the right path. Any tip would be great.</p>
",Multilingual Language Processing & Language Identification,portuguese treebank chunked sent nltk problem need retrieve np chunk text using nltk want use regex chunkers new nlp think classification based chunking iob tag would much better regular expression chunking find chunked sent language portuguese brazil english actually main problem extract person organization entity text first trying get np chunk sure trailling right path tip would great
Detect Language Text in PHP without huge dependences or third-party services,"<p>Looking for a composer package, which without a huge dependency (without knowledge bases more than 3MB) and third-party services will be able to determine the language of the text.</p>

<p>The text is very often consists of several words.</p>

<p>For example, I'd like to see this package with a high accuracy identified the languages of the following fragments:</p>

<ol>
<li><blockquote>
  <p>text on english</p>
</blockquote></li>
<li><blockquote>
  <p>–¢–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º</p>
</blockquote></li>
<li><blockquote>
  <p>–¢–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ some words on english</p>
</blockquote></li>
<li><blockquote>
  <p>ÁµêÂüéÂèãÂ•à„ÅØÂãáËÄÖ„Åß„ÅÇ„Çã -È∑≤Â∞æÈ†àÁæé„ÅÆÁ´†- Á¨¨2Á´† „Äå„Åü„Åæ„Åó„ÅÑ„Äç</p>
</blockquote></li>
<li><blockquote>
  <p>‡Æµ‡Æø‡Æµ‡Øá‡Æï‡ÆÆ‡Øç</p>
</blockquote></li>
<li><blockquote>
  <p>El aeropuerto se considera</p>
</blockquote></li>
<li><blockquote>
  <p>Wunderbar steht er da im Silberhaar.</p>
</blockquote></li>
<li><blockquote>
  <p>Ein wei√ü gl√§nzendes</p>
</blockquote></li>
<li><blockquote>
  <p>si les faits n‚Äôob√©issent pas</p>
</blockquote></li>
<li><blockquote>
  <p>4 8 15 16 23 42</p>
</blockquote></li>
</ol>

<p>Mainly interested in the qualitative determination for the following languages: <strong>English</strong>, <strong>Russian</strong>, <strong>German</strong>, Spanish, Dutch, Italian, <strong>French</strong>, <strong>Chinese</strong>, <strong>Japanese</strong>, Norwegian, Danish, <strong>Indian</strong>.</p>

<p>A big plus would be if this package is not outdated or abandoned.</p>

<p>PS: It is important that they do not take much memory when running.</p>
",Multilingual Language Processing & Language Identification,detect language text php without huge dependence third party service looking composer package without huge dependency without knowledge base mb third party service able determine language text text often consists several word example like see package high accuracy identified language following fragment text english word english el aeropuerto se considera wunderbar steht er da im silberhaar ein wei gl nzendes si le faits n ob issent pa mainly interested qualitative determination following language english russian german spanish dutch italian french chinese japanese norwegian danish indian big plus would package outdated abandoned p important take much memory running
Logical fallacy detection and/or identification with natural-language-processing,"<p>Is there a package or methodology in existence for the detection of flawed logical arguments in text? </p>

<p>I was hoping for something that would work for text that is not written in an academic setting (such as a logic class). It might be a stretch but I would like something that can identify where logic is trying to be used and identify the logical error. A possible use for this would be marking errors in editorial articles.</p>

<p>I don't need anything that is polished. I wouldn't mind working to develop something either so I'm really looking for what's out there in the wild now.</p>
",Multilingual Language Processing & Language Identification,logical fallacy detection identification natural language processing package methodology existence detection flawed logical argument text wa hoping something would work text written academic setting logic class might stretch would like something identify logic trying used identify logical error possible use would marking error editorial article need anything polished mind working develop something either really looking wild
Natural language de-identification,"<p>I am looking for a natural language tool that can automatically de-identify English text. For example, every email address should be renamed or obscured. But proper names should be de-identified, as should addresses and what not.</p>

<p>There is a <a href=""http://mist-deid.sourceforge.net/"" rel=""nofollow"">MITRE Identification Scrubber Toolkit</a>. I don't know how well it works.</p>

<p>My questions:</p>

<ul>
<li>Are there any other tools out there?</li>
<li>Does anyone have experience with the MITRE tool? How well does it work?</li>
</ul>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,natural language de identification looking natural language tool automatically de identify english text example every email address renamed obscured proper name de identified address mitre identification scrubber toolkit know well work question tool doe anyone experience mitre tool well doe work thanks
How to not split English into separate letters in the Stanford Chinese Parser,"<p>I am using the Stanford Segmenter at <a href=""http://nlp.stanford.edu/software/segmenter.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/segmenter.shtml</a> in Python. For the Chinese segmenter, whenever it encounters a English word, it will split the word into many characters one by one, but I want to keep the characters together after the segmentation is done.  </p>

<p>For example:</p>

<pre><code>‰Ω†Â•ΩabcÊàëÂ•Ω 
</code></pre>

<p>currently will become this after the segmentation </p>

<pre><code>‰Ω†Â•Ω a b c Êàë Â•Ω
</code></pre>

<p>but I want it to become </p>

<pre><code>‰Ω†Â•Ω abc Êàë Â•Ω
</code></pre>

<p>Is there a way to teach the segmenter to do that?  Is there a setting for this?</p>

<p>I Googlged this and to no answer, and tried to hack together a way(spending 6 hours on it) to do this by pulling out English characters from the text and then put them back in after the segmentation is done, but realized it is very difficult to do this in a efficient manner.  Any help on this would be greatly appreciated.</p>
",Multilingual Language Processing & Language Identification,split english separate letter stanford chinese parser using stanford segmenter python chinese segmenter whenever encounter english word split word many character one one want keep character together segmentation done example currently become segmentation want become way teach segmenter setting googlged answer tried hack together way spending hour pulling english character text put back segmentation done realized difficult efficient manner help would greatly appreciated
Searching through nested list,"<pre><code>from nltk import word_tokenize

list_1 = [a, b, c, d, e, f]
list_2 = [[aa, bb, cc], [dd, ee], [ff], [gg, hh, ii, jj], [kk, ll], [mm, nn, oo]]
text = 'The lazy aa moves along the hh'
text_token = word_tokenize(text)

for word in text:
     if word in [j for i in list_2 for j in i]:
        print(list_2.index(word))
     else:
        print(word)

ValueError: 'hh' is not in list
</code></pre>

<p>I'm attempting textual analysis on large bodies of French text. I have already tried using the NLTK Stemmer and it doesn't stem a significant amount of words that I'm analyzing. </p>

<p>I'm not very familiar with python or any of the other coding languages (I'm coming from the Humanities) so I'm also not entirely sure as to how to search for the issue that I'm looking for, and I apologize in  if this question is either redundant or easily solved. </p>

<p>I've attempted to approach this by finding a list of French words with their various grammatical forms. I already complied the list into two separate lists: the first list contains the root of the word that I would like the others to take on, while the second list is the variant forms that the item in the first list can take on. I've made it so that both lists correspond to each other. For example, <code>list_1[0]</code> would correspond to the words in <code>list_2[0]</code>. As a result, there are a lot of nested lists of different lengths within list_2. </p>

<p>My issue seems to arise when I'm searching through nested lists. I have to iterate through <code>text_token</code> to check if any of the words in <code>text_token</code> exist in <code>list_2</code>. If <code>text_token</code> exists in <code>list_2</code> then find <code>misc = list_2.index(word)</code> . After finding <code>misc</code>, I want to replace word with search <code>list_1[misc]</code>. However, after confirming that word exists in <code>list_2</code>, I try to <code>print(list_2.index(word))</code> to see if it works, but i keep receiving <code>ValueError: 'hh' is not in list</code> I'm not entirely sure how to cycle through nested lists after already having done so in order to avoid this error, because I'm assuming that when I search through the definitions a second time, it doesn't see the nested list as multiple items. I've tried to circumvent this by trying another list comprehension after checking it in the original one, but it ends up returning individual letters. I've also tried making these two lists into a dictionary, but I can't seem to figure out how to yield <code>.keys()</code> from <code>.values()</code>.</p>
",Multilingual Language Processing & Language Identification,searching nested list attempting textual analysis large body french text already tried using nltk stemmer stem significant amount word analyzing familiar python coding language coming humanity also entirely sure search issue looking apologize question either redundant easily solved attempted approach finding list french word various grammatical form already complied list two separate list first list contains root word would like others take second list variant form item first list take made list correspond example would correspond word result lot nested list different length within list issue seems arise searching nested list iterate check word exist exists find finding want replace word search however confirming word exists try see work keep receiving entirely sure cycle nested list already done order avoid error assuming search definition second time see nested list multiple item tried circumvent trying another list comprehension checking original one end returning individual letter also tried making two list dictionary seem figure yield
rows csv into nested list,"<p>I'm a beginner in programming, but for a natural language processing project I need to work with csv. 
I have this csv file with annotated text. The sentences are separated from each other with an empty row. Each row is a token (word or punctuation with it's annotation). What I need is a nested list like this <code>[[[I,pronoun],[need, verb], [you, pronoun]], [[Do, verb], [you, pronoun], [need, verb], [me, pronoun]]]</code></p>

<p>The text looks like this in the csv:</p>

<pre><code>I  pronoun
need  verb
you  pronoun

Do  pronoun
you  pronoun
need verb
me  pronoun
</code></pre>

<p>I tried the following code, but then I only get one big list and not a nested list. I don't know how to split the sentences into different lists on the empty row.</p>

<pre><code> sentences = []
    for row in text:
        sentences.append(list(row))
 print(sentences)
</code></pre>

<p>Any suggestions?</p>
",Multilingual Language Processing & Language Identification,row csv nested list beginner programming natural language processing project need work csv csv file annotated text sentence separated empty row row token word punctuation annotation need nested list like text look like csv tried following code get one big list nested list know split sentence different list empty row suggestion
spaCy sentence segmentation failing on quotes,"<p>I am parsing some news data with spaCy and am noticing a consistent failure regarding sentence segmentation where there is a quote.  Has anyone else solved this issue? </p>

<p>Here is a reproducible example - note sentence 4 in the output below.  spaCy fails to split at the start of the quote, and this is consistent through other news articles I'm working with. </p>

<p>Thanks a lot. </p>

<p>Example:</p>

<p>Raw data: </p>

<blockquote>
  <p>u'body': u'\n   LONDON Nov 4 Britons hurt by lower incomes and rising food prices after the financial crisis have cut back on fruit and vegetables and turned instead to fatty, sugary, processed food, an academic study showed on Monday.Britain has seen food prices rise much more sharply than most other developed economies between 2005 and 2012, while wage growth has been low and unemployment has risen.The net effect has been that Britons are spending 8.5 percent less in real terms on food purchased at home than before the recession - with the trend even greater for pensioners and families with young children.The research is likely to be politically sensitive at a time when Britain\'s Conservative-led government is under pressure from the opposition Labour Party, over declining standards of living and sharply rising demand at food banks which hand out free food to the poorest Britons.                                  People have economised by buying less food, measured in number of calories, but also on its quality, picking products that are less nutritious and higher in saturated fat and sugar.""Various measures of nutritional quality declined over this period, with bigger decreases for pensioner households and households with young children,"" said the Institute for Fiscal Studies, an economics research body.OBESITY                                   Families with children were prone to switching to more sugary food, while pensioners favoured food high in saturated fat, the study showed. Both groups often have lower incomes.While the economy is starting to show signs of growth after suffering the biggest hit to economic growth since records began during the 2008-09 recession, households\' disposable incomes are no higher than a decade ago.                                  However, the IFS said a lower-quality diet was not an inevitable consequence of having less money, and that some households had been able to eat as healthily as before while spending less. More research was needed to see why this was not the case for other households, the researchers added.The study looked at data on more than 15,000 households\' shopping habits collected by market research company Kantar Worldpanel between 2005 and 2012.The figures do not include meals purchased or provided away from home, for example in restaurants or at schools, which in England provide free lunches for poorer pupils.The study was released alongside a piece of longer-term research from the IFS, which showed the English now consume 15-30 percent fewer calories than in 1980, despite higher obesity rates probably due to less physical activity.This contrasts with the United States, where calorie consumption has risen as well as obesity. The IFS said it was were researching further into trends in Britons\' physical activity over the period.',</p>
</blockquote>

<p>Code to split: </p>

<pre><code>from __future__ import unicode_literals
import spacy
nlp = spacy.load('en')
doc1 = nlp(article_to_json['body'].decode('utf-8'), parse=True)

for number, sent in enumerate(doc1.sents):
    print number, sent, ""\n""
</code></pre>

<p>Output: </p>

<blockquote>
  <p>0     LONDON Nov 4 Britons hurt by lower incomes and rising food
  prices after the financial crisis have cut back on fruit and
  vegetables and turned instead to fatty, sugary, processed food, an
  academic study showed on Monday. </p>
  
  <p>1 Britain has seen food prices rise much more sharply than most other
  developed economies between 2005 and 2012, while wage growth has been
  low and unemployment has risen. </p>
  
  <p>2 The net effect has been that Britons are spending 8.5 percent less
  in real terms on food purchased at home than before the recession -
  with the trend even greater for pensioners and families with young
  children. </p>
  
  <p>3 The research is likely to be politically sensitive at a time when
  Britain's Conservative-led government is under pressure from the
  opposition Labour Party, over declining standards of living and
  sharply rising demand at food banks which hand out free food to the
  poorest Britons.                                   </p>
  
  <p><strong>4 People have economised by buying less food, measured in number of calories, but also on its quality, picking products that are less
  nutritious and higher in saturated fat and sugar.""Various measures of
  nutritional quality declined over this period, with bigger decreases
  for pensioner households and households with young children,"" said the
  Institute for Fiscal Studies, an economics research body.</strong> </p>
  
  <p>5 OBESITY                                   Families with children
  were prone to switching to more sugary food, while pensioners favoured
  food high in saturated fat, the study showed. </p>
  
  <p>6 Both groups often have lower incomes. </p>
  
  <p>7 While the economy is starting to show signs of growth after
  suffering the biggest hit to economic growth since records began
  during the 2008-09 recession, households' disposable incomes are no
  higher than a decade ago.                                   </p>
  
  <p>8 However, the IFS said a lower-quality diet was not an inevitable
  consequence of having less money, and that some households had been
  able to eat as healthily as before while spending less. </p>
  
  <p>9 More research was needed to see why this was not the case for other
  households, the researchers added. </p>
  
  <p>10 The study looked at data on more than 15,000 households' shopping
  habits collected by market research company Kantar Worldpanel between
  2005 and 2012.The figures do not include meals purchased or provided
  away from home, for example in restaurants or at schools, which in
  England provide free lunches for poorer pupils. </p>
  
  <p>11 The study was released alongside a piece of longer-term research
  from the IFS, which showed the English now consume 15-30 percent fewer
  calories than in 1980, despite higher obesity rates probably due to
  less physical activity. </p>
  
  <p>12 This contrasts with the United States, where calorie consumption
  has risen as well as obesity. </p>
  
  <p>13 The IFS said it was were researching further into trends in
  Britons' physical activity over the period.</p>
</blockquote>
",Multilingual Language Processing & Language Identification,spacy sentence segmentation failing quote parsing news data spacy noticing consistent failure regarding sentence segmentation quote ha anyone else solved issue reproducible example note sentence output spacy fails split start quote consistent news article working thanks lot example raw data u body u n london nov briton hurt lower income rising food price financial crisis cut back fruit vegetable turned instead fatty sugary processed food academic study showed monday britain ha seen food price rise much sharply developed economy growth ha low unemployment ha risen net effect ha briton spending percent le real term food purchased home recession trend even greater pensioner family young child research likely politically sensitive time britain conservative led government pressure opposition labour party declining standard living sharply rising demand food bank hand free food poorest briton people economised buying le food measured number calorie also quality picking product le nutritious higher saturated fat sugar various measure nutritional quality period bigger decrease pensioner household household young child said institute fiscal study economics research body obesity family child prone switching sugary food pensioner favoured food high saturated fat study showed group often lower income economy starting show sign growth suffering biggest hit economic growth since record began recession household disposable income higher decade ago however ifs said lower quality diet wa inevitable consequence le money household able eat healthily spending le research wa needed see wa case household researcher added study looked data household shopping habit collected market research company kantar worldpanel figure include meal purchased provided away home example restaurant school england provide free lunch poorer pupil study wa released alongside piece longer term research ifs showed english consume percent fewer calorie despite higher obesity rate probably due le physical activity contrast united state calorie consumption ha risen well obesity ifs said wa researching trend briton physical activity period code split output london nov briton hurt lower income rising food price financial crisis cut back fruit vegetable turned instead fatty sugary processed food academic study showed monday britain ha seen food price rise much sharply developed economy growth ha low unemployment ha risen net effect ha briton spending percent le real term food purchased home recession trend even greater pensioner family young child research likely politically sensitive time britain conservative led government pressure opposition labour party declining standard living sharply rising demand food bank hand free food poorest briton people economised buying le food measured number calorie also quality picking product le nutritious higher saturated fat sugar various measure nutritional quality period bigger decrease pensioner household household young child said institute fiscal study economics research body obesity family child prone switching sugary food pensioner favoured food high saturated fat study showed group often lower income economy starting show sign growth suffering biggest hit economic growth since record began recession household disposable income higher decade ago however ifs said lower quality diet wa inevitable consequence le money household able eat healthily spending le research wa needed see wa case household researcher added study looked data household shopping habit collected market research company kantar worldpanel figure include meal purchased provided away home example restaurant school england provide free lunch poorer pupil study wa released alongside piece longer term research ifs showed english consume percent fewer calorie despite higher obesity rate probably due le physical activity contrast united state calorie consumption ha risen well obesity ifs said wa researching trend briton physical activity period
Convert plain english to SQL,"<p>What would be the best way I could convert english text to sql queries.  For example, if I wanted a user to run a query by typing in ""show me college students with a GPA higher than 3."" or ""show mothers with 2 or more kids""  The closest link I found so far is this: <a href=""http://english2sql.com/demo.html"" rel=""nofollow"">http://english2sql.com/demo.html</a>, but it does not look like it is supported much since the email I sent them bounced back to me.  If there are some open source libraries I could use for this, preferably in C# that would be awesome.  Any suggestions?</p>
",Multilingual Language Processing & Language Identification,convert plain english sql would best way could convert english text sql query example wanted user run query typing show college student gpa higher show mother kid closest link found far doe look like supported much since email sent bounced back open source library could use preferably c would awesome suggestion
mixed language indexing in solr,"<p>I am indexing mixed-language pdf documents to solr, meaning one single document is made out of different languages, mainly english parts and french parts. I would like to stream each part to a specific field depending on the language.</p>

<p>So, lets say ""Hello, my name is nicolas. Je voudrais extraire du texte avec solr"" would be indexed into two fields, 
Field_en ""Hello my name is nicolas""
 and Field_fr ""Je voudrais extraire du texte avec SOLR"".</p>

<p>Currently, I am only able to do so at the document level (using LangDetect processor from solr), but it doesn't help for mixed language documents.</p>

<p>If it is too difficult, I would at least like to be able to selectively remove one language during indexing, and get one single language in a field.</p>
",Multilingual Language Processing & Language Identification,mixed language indexing solr indexing mixed language pdf document solr meaning one single document made different language mainly english part french part would like stream part specific field depending language let say hello name nicolas je voudrais extraire du texte avec solr would indexed two field field en hello name nicolas field fr je voudrais extraire du texte avec solr currently able document level using langdetect processor solr help mixed language document difficult would least like able selectively remove one language indexing get one single language field
Replacing more than one character in unicode python,"<pre><code>import re
test = unicode(""ÿ¥ÿØŸéÿØ"", encoding='utf-8')
test = test.replace(u""\u064e"", """")
</code></pre>

<p>This is the code to remove one character. I would like to replace any of the following unicode characters: 0622, 0623, 0625 with 0627. This is for the Arabic language. I know how to do it in multiple lines but is there a way to do it in one?</p>
",Multilingual Language Processing & Language Identification,replacing one character unicode python code remove one character would like replace following unicode character arabic language know multiple line way one
Why do we do padding in NLP tasks?,"<p>In NLP tasks, it's very common that people annotate a sentence with SOC (start of a sentence) and EOC(end of a sentence). Why do they do that? </p>

<p>Is it a task dependent performance? For instance, the reason you do padding in NER problems is different from the reason you do padding for translation problems? As in the NER problem you do padding as to extract more useful features from the context, however in a translation problem, you do padding to identify the end of a sentence because the decoder is trained sentence-by-sentence.</p>
",Multilingual Language Processing & Language Identification,padding nlp task nlp task common people annotate sentence soc start sentence eoc end sentence task dependent performance instance reason padding ner problem different reason padding translation problem ner problem padding extract useful feature context however translation problem padding identify end sentence decoder trained sentence sentence
Natural language processing keywords for building search engine,"<p>I'm recently interested in NLP, and would like to build up search engine for product recommendation. (Actually I'm always wondering about how search engine for Google/Amazon is built up)</p>

<p>Take Amazon product as example, where I could access all ""word"" information about one product:</p>

<pre><code>Product_Name    Description      ReviewText
""XXX brand""    ""Pain relief""    ""This is super effective""
</code></pre>

<p>By applying <code>nltk</code> and <code>gensim</code> packages I could easily compare similarity of different products and make recommendations.</p>

<p>But here's another question I feel very vague about:
How to build a search engine for such products? </p>

<p>For example, if I feel pain and would like to search for medicine online, I'd like to type-in <code>""pain relief""</code> or <code>""pain""</code>, whose searching results should include <code>""XXX brand""</code>.</p>

<p>So this sounds more like keyword extraction/tagging question? How should this be done in NLP? I know <strong>corpus</strong> should contain <strong>all</strong> but <strong>single</strong> words, so it's like:</p>

<pre><code>[""XXX brand"" : (""pain"", 1),(""relief"", 1)]
</code></pre>

<p>So if I typed in either <code>""pain""</code> or <code>""relief""</code> I could get <code>""XXX brand""</code>; but what about I searched <code>""pain relief""</code>?</p>

<p>I could come up with idea that directly call python in my javascript for calculate similarities of input words <code>""pain relief""</code> on browser-based server and make recommendation; but that's kind of do-able? </p>

<p>I still prefer to build up very big lists of keywords at backends, stored in datasets/database and directly visualized in web page of search engine.</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,natural language processing keywords building search engine recently interested nlp would like build search engine product recommendation actually always wondering search engine google amazon built take amazon product example could access word information one product applying package could easily compare similarity different product make recommendation another question feel vague build search engine product example feel pain would like search medicine online like type whose searching result include sound like keyword extraction tagging question done nlp know corpus contain single word like typed either could get searched could come idea directly call python javascript calculate similarity input word browser based server make recommendation kind able still prefer build big list keywords backends stored datasets database directly visualized web page search engine thanks
How to parse an xml file in python?,"<p>I have an xml file and it looks like this</p>

<pre><code>&lt;?xml version='1.0' encoding='UTF8'?&gt;
&lt;Reviews&gt;
  &lt;Review rid=""0"" book_title=""O-Apanhador-no-Campo-de-Centeio"" score=""4.0""&gt;
    &lt;sentences&gt;
      &lt;sentence id=""0:0:0"" place=""title"" polarity=""neutral""&gt;
        &lt;text&gt;Est√° provado:&lt;/text&gt;
        &lt;tokens&gt;
          &lt;word id=""1"" form=""Est√°"" base=""estar"" postag=""v-fin"" morf=""PR 3S IND VFIN"" extra=""fmc * vK mv"" head=""0"" deprel=""STA"" srl=""PRED"" obj=""O"" opinion=""O"" from=""0"" to=""4""/&gt;
          &lt;word id=""2"" form=""provado"" base=""provar"" postag=""v-fin"" morf=""PCP M S"" extra=""vH jh"" head=""1"" deprel=""Cs"" srl=""ATR"" obj=""O"" opinion=""O"" from=""5"" to=""12""/&gt;
          &lt;word id=""3"" form="":"" base=""--"" postag=""pu"" morf=""--"" extra=""--"" head=""0"" deprel=""PU"" srl="""" obj=""O"" opinion=""O"" from=""12"" to=""13""/&gt;
        &lt;/tokens&gt;
      &lt;/sentence&gt;
      &lt;sentence id=""0:0:1"" place=""title"" polarity=""neutral""&gt;
        &lt;text&gt;Pode existir um livro bom sem uma hist√≥ria boa.&lt;/text&gt;
        &lt;tokens&gt;
          &lt;word id=""1"" form=""Pode"" base=""poder"" postag=""v-fin"" morf=""PR 3S IND VFIN"" extra=""fmc * aux"" head=""0"" deprel=""STA"" srl="""" obj=""O"" opinion=""O"" from=""0"" to=""4""/&gt;
          &lt;word id=""2"" form=""existir"" base=""existir"" postag=""v-inf"" morf=""--"" extra=""mv"" head=""1"" deprel=""Oaux"" srl=""PRED"" obj=""O"" opinion=""O"" from=""5"" to=""12""/&gt;
          &lt;word id=""3"" form=""um"" base=""um"" postag=""pron-indef"" morf=""M S"" extra=""--"" head=""4"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""13"" to=""15""/&gt;
          &lt;word id=""4"" form=""livro"" base=""livro"" postag=""n"" morf=""M S"" sem=""sem-r"" extra=""--"" head=""1"" deprel=""S"" srl=""TH"" obj=""O"" opinion=""O"" from=""16"" to=""21""/&gt;
          &lt;word id=""5"" form=""bom"" base=""bom"" postag=""adj"" morf=""M S"" extra=""np-close"" head=""4"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""22"" to=""25""/&gt;
          &lt;word id=""6"" form=""sem"" base=""sem"" postag=""prp"" morf=""--"" extra=""--"" head=""2"" deprel=""fA"" srl="""" obj=""O"" opinion=""O"" from=""26"" to=""29""/&gt;
          &lt;word id=""7"" form=""uma"" base=""um"" postag=""pron-indef"" morf=""F S"" extra=""--"" head=""8"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""30"" to=""33""/&gt;
          &lt;word id=""8"" form=""hist√≥ria"" base=""hist√≥ria"" postag=""n"" morf=""F S"" sem=""per domain sem-r"" extra=""--"" head=""6"" deprel=""DP"" srl=""COM-ADV"" obj=""O"" opinion=""O"" from=""34"" to=""42""/&gt;
          &lt;word id=""9"" form=""boa"" base=""bom"" postag=""adj"" morf=""F S"" extra=""jh np-close"" head=""8"" deprel=""DN"" srl="""" obj=""O"" opinion=""O"" from=""43"" to=""46""/&gt;
          &lt;word id=""10"" form=""."" base=""--"" postag=""pu"" morf=""--"" extra=""--"" head=""0"" deprel=""PU"" srl="""" from=""46"" to=""47""/&gt;
        &lt;/tokens&gt;
</code></pre>

<p>I want to extract the text field and the polarity to a separate csv file.</p>

<p>I used this to extract polarity successfully, but I can't extract the text</p>

<pre><code>with open('output1.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(('text', 'polarity'))
    root = lxml.etree.fromstring(xmlstr)
    for sent in root.iter('sentence'):
        row = sent.get('text'), sent.get('polarity')
        writer.writerow(row)
</code></pre>

<p>where xmlstr is a string of the content of the xml file.</p>

<p>How can I extract the text field from the file !?</p>

<p>note: This is a link containing the file I'm working with
<a href=""https://raw.githubusercontent.com/pedrobalage/ABSA_Experiments/master/corpus/ReLiPalavras.xml"" rel=""nofollow noreferrer"">sentiment analysis in portuguese</a></p>

<p>can any one help !? </p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,parse xml file python xml file look like want extract text field polarity separate csv file used extract polarity successfully extract text xmlstr string content xml file extract text field file note link containing file working sentiment analysis portuguese one help thanks
decompose complex sentences to simple sentences,"<p>Having a sentence like: ""I was given tablets by my nurse to try and ease my pain."" I want to decompose it to:</p>

<ol>
<li><p>""I was given tablets by my nurse to try."" </p></li>
<li><p>""I was given tablets by my nurse to ease my pain.""</p></li>
</ol>

<p>I have tried English parser in OpenNLP (by accessing java from Python) (demo: <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a>). ""try"" and ""ease"" are at the same sub-level of (VP (TO to) below, how can I decompose the sentence? </p>

<pre><code>(ROOT
  (S
    (NP (PRP I))
    (VP (VBD was)
      (VP (VBN given)
        (NP (NNS tablets))
        (PP (IN by)
          (NP (PRP$ my) (NN nurse)
            (S
              (VP (TO to)
                (VP (VB try)
                  (CC and)
                  (VB ease)
                  (NP (PRP$ my) (NN pain)))))))))
    (. .)))
</code></pre>

<p>Do you know any way other than this java solution: <a href=""https://stackoverflow.com/questions/10020451/algorithm-to-extract-simple-sentences-from-complexmixed-sentences"">algorithm to extract simple sentences from complex(mixed) sentences?</a>  </p>
",Multilingual Language Processing & Language Identification,decompose complex sentence simple sentence sentence like wa given tablet nurse try ease pain want decompose wa given tablet nurse try wa given tablet nurse ease pain tried english parser opennlp accessing java python demo try ease sub level vp decompose sentence know way java solution href extract simple sentence complex mixed sentence
Break/Decompose complex and compound sentences in nltk,"<p>Is there a way to decompose complex sentences into simple sentences in nltk or other natural language processing libraries?</p>

<p>For example:</p>

<p>The park is so wonderful when the sun is setting and a cool breeze is blowing ==> The sun is setting. a cool breeze is blowing.  The park is so wonderful.</p>
",Multilingual Language Processing & Language Identification,break decompose complex compound sentence nltk way decompose complex sentence simple sentence nltk natural language processing library example park wonderful sun setting cool breeze blowing sun setting cool breeze blowing park wonderful
"Bing/Google/Flickr API: how would you find an image to go along each of 150,000 Japanese sentences?","<p>I'm doing part-of-speech &amp; morphological analysis project for Japanese sentences. Each sentence will have its own webpage. To make this page more visual, I want to show one picture which is somehow related to the sentence. For example, For the sentence ""ÁßÅ„ÅØÂ≠¶Áîü„Åß„Åô""„ÄÄÔºà""I'm a student""), the relevant pictures would be pictures of school, Japanese textbook, students, etc. What I have: part-of-speech tagging for every word. My approach now: use 2-3 nouns from every sentence and retrieve the first image from search results using Bing Images API. Note: all the sentence processing up to this point was done in Java. </p>

<hr>

<p>Have a couple of questions though:
1) what is better (richer corpus &amp; powerful search), Google Images API, Bing Images API, Flickr API, etc. for searching nouns in Japanese? 
2) how do you select the most important noun from the sentence to do the query in Image Search Engine without doing complicated topic modeling, etc.?
Thanks!</p>
",Multilingual Language Processing & Language Identification,bing google flickr api would find image go along japanese sentence part speech morphological analysis project japanese sentence sentence webpage make page visual want show one picture somehow related sentence example sentence student relevant picture would picture school japanese textbook student etc part speech tagging every word approach use noun every sentence retrieve first image search result using bing image api note sentence processing point wa done java couple question though better richer corpus powerful search google image api bing image api flickr api etc searching noun japanese select important noun sentence query image search engine without complicated topic modeling etc thanks
How to use Google Natural Language Processing Cloud API in Android,"<p>Hello I guys I want to use Google's Natural Language Processing Cloud API for my android Application. What are the steps for getting the reference to the api and what all things is to be downloaded and what are the steps to be followed</p>
",Multilingual Language Processing & Language Identification,use google natural language processing cloud api android hello guy want use google natural language processing cloud api android application step getting reference api thing downloaded step followed
Natural Language Generation - how to go beyond templates,"<p>We've build a system that analyzes some data and outputs some results in plain English (i.e. no charts etc.). The current implementation relies on lots of templates and some randomization in order to give as much diversity to the text as possible.</p>

<p>We'd like to switch to something more advanced with the hope that the produced text is less repetitive and sounds less robotic. I've searched a lot on google but I cannot find something concrete to start from. Any ideas?</p>

<p>EDIT: The data fed to the NLG mechanism are in JSON format. Here is an example about web analytics data. The json file may contain for example a metric (e.g. visits), it's value in the last X days, whether the last value is expected or not and which dimensions (e.g. countries or marketing channels) affected its change.</p>

<p>The current implementation could give something like this:</p>

<blockquote>
  <p>Overall visits in the UK mainly from ABC email campaign reached 10K (+20% DoD) and were above the expected value by 10%. Users were mainly landing on XXX page while the increase was consistent across devices.</p>
</blockquote>

<p>We're looking to finding a way to depend less on templates, sound even more natural and increase the vocabulary.</p>
",Multilingual Language Processing & Language Identification,natural language generation go beyond template build system analyzes data output result plain english e chart etc current implementation relies lot template randomization order give much diversity text possible like switch something advanced hope produced text le repetitive sound le robotic searched lot google find something concrete start idea edit data fed nlg mechanism json format example web analytics data json file may contain example metric e g visit value last x day whether last value expected dimension e g country marketing channel affected change current implementation could give something like overall visit uk mainly abc email campaign reached k dod expected value user mainly landing xxx page increase wa consistent across device looking finding way depend le template sound even natural increase vocabulary
How to deactivate the default stop words feature for sklearn TfidfVectorizer,"<p>I am trying to get the tf-idf values for Japanese words.
The problem I am having is that sklearn TfidfVectorizer removes some Japanese characters, which I want to keep, as stop words. </p>

<p>The following is the example:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer(stop_words = None)

words_list = [""Ê≠Ø"",""„Åå"",""Áóõ„ÅÑ""]
tfidf_matrix =  tf.fit_transform(words_list)
feature_names = tf.get_feature_names() 
print (feature_names)
</code></pre>

<p>The output is:<code>['Áóõ„ÅÑ']</code></p>

<p>However, I want to keep all those three characters in the list.
I believe TfidfVectorizer removes characters with length of 1 as stop words.
How could I deactivate the default stop words feature and keep all characters?</p>
",Multilingual Language Processing & Language Identification,deactivate default stop word feature sklearn tfidfvectorizer trying get tf idf value japanese word problem sklearn tfidfvectorizer remove japanese character want keep stop word following example output however want keep three character list believe tfidfvectorizer remove character length stop word could deactivate default stop word feature keep character
Identify a list of items using Natural Language Processing,"<p>Is there a way for NLP parsers to identify a list? <br>
For example, <em>""a tiger, a lion and a gorilla""</em> should be identified as a list<br>
(I don't need it to be identified as a list of animals; just a list would be sufficient).</p>

<p>My ultimate aim is to link a common verb/word to all the items in the list.
For example, consider the sentence <strong>""He found a pen, a book and a flashlight""</strong>. Here, <em>""found""</em> verb should be linked to all the 3 items.</p>

<p>Another example, <strong>""He was tested negative for cancer, anemia and diabetes""</strong>. Here, the word <em>""negative""</em> should be linked to the three diseases. </p>

<p>Is this possible with any of the open-source NLP packages like OpenNLP or Stanford CoreNLP? Any other solution?
<br><br><br>
<strong>EDIT:</strong><br>
Like mentioned in one of the answers, my initial idea was to manually parse the list and find the items by looking at the placement of commas, etc.</p>

<p>But then I discovered Stanford NLP's OpenIE model. This seems to be doing a pretty good job. <br>For example, <strong>""He has a pen and a book""</strong> gives the 2 relations <strong>(He;has;a pen)</strong> and <strong>(He;has;a book)</strong>. </p>

<p>The problem with the model is that it doesn't work for incomplete sentences like, <strong>""has a pen and a book""</strong>. <br>(From what I understood, this is because OpenIE can only extract triples)<br>
It also fails when negations are involved. Eg, <strong>""He has no pens""</strong>.</p>

<p>Is there a solution to these problems? What are the best solutions available currently for information extraction?</p>
",Multilingual Language Processing & Language Identification,identify list item using natural language processing way nlp parser identify list example tiger lion gorilla identified list need identified list animal list would sufficient ultimate aim link common verb word item list example consider sentence found pen book flashlight found verb linked item another example wa tested negative cancer anemia diabetes word negative linked three disease possible open source nlp package like opennlp stanford corenlp solution edit like mentioned one answer initial idea wa manually parse list find item looking placement comma etc discovered stanford nlp openie model seems pretty good job example ha pen book give relation ha pen ha book problem model work incomplete sentence like ha pen book understood openie extract triple also fails negation involved eg ha pen solution problem best solution available currently information extraction
Auto-completing Strings in Python?,"<p>I was wondering if there was some function where I could put in a string and if this string had words that it could auto-complete to (in the english language) it would return true (for example if the string is ""bl"" then this function would return true since ""blue"" is a word) and if not (say the string is ""blszc"") then it would return false.</p>
",Multilingual Language Processing & Language Identification,auto completing string python wa wondering wa function could put string string word could auto complete english language would return true example string bl function would return true since blue word say string blszc would return false
How to check to see if a string is contained in any english word?,"<p>Going off of this link: <a href=""https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python"">How to check if a word is an English word with Python?</a></p>

<p>Is there any way to see (in python) if a string of letters is contained in any word in the English language? For example, fun(wat) would return true since ""water"" is a word (and I'm sure there are multiple other words that contain wat) but fun(wayterlx) would be false since wayterlx is not contained in any English word. (and it is not a word itself)</p>

<p>Edit: A second example: d.check(""blackjack"") returns true but d.check(""lackjac"") returns false, but in the function I am looking for it would return true since it is contained in some english word.</p>
",Multilingual Language Processing & Language Identification,check see string contained english word going link edit second example check blackjack return true check lackjac return false function looking would return true since contained english word
Which tagset to use for training a POS tagger?,"<p>I'm designing a simple POS tagger using the Viterbi algorithm and a Trigram language model. How do I decide which tagset to use for training? (POS tagger for English)</p>
",Multilingual Language Processing & Language Identification,tagset use training po tagger designing simple po tagger using viterbi algorithm trigram language model decide tagset use training po tagger english
Natural language processing with elastic search,"<p>I wanted to integrate search functionality in my website. </p>

<p>I am using elastic search for it, If user searches ""Maruti suzkuki under 2 lac"" then it has to search cars having brand name ""Maruti Suzuki"" and price under <a href=""https://en.wikipedia.org/wiki/Indian_numbering_system"" rel=""nofollow noreferrer"">2 lac</a>.
How can I achieve this?</p>

<p>Types of searches</p>

<ol>
<li>maruti suzuki under 2 lac</li>
<li>maruti suzuki 20000km driven </li>
<li>cars 2015 year model 
etc.</li>
</ol>

<p>ES version 5.4</p>
",Multilingual Language Processing & Language Identification,natural language processing elastic search wanted integrate search functionality website using elastic search user search maruti suzkuki lac ha search car brand name maruti suzuki price lac achieve type search maruti suzuki lac maruti suzuki km driven car year model etc e version
Matching PoS tags with specific text with `testacy.extract.pos_regex_matches(...)`,"<p>I'm using <code>textacy</code>'s <code>pos_regex_matches</code> method to find certain chunks of text in sentences.</p>

<p>For instance, assuming I have the text: <code>Huey, Dewey, and Louie are triplet cartoon characters.</code>, I'd like to detect that <code>Huey, Dewey, and Louie</code> is an enumeration.</p>

<p>To do so, I use the following code (on <code>testacy 0.3.4</code>, the version available at the time of writing):</p>

<pre><code>import textacy

sentence = 'Huey, Dewey, and Louie are triplet cartoon characters.'
pattern = r'&lt;PROPN&gt;+ (&lt;PUNCT|CCONJ&gt; &lt;PUNCT|CCONJ&gt;? &lt;PROPN&gt;+)*'
doc = textacy.Doc(sentence, lang='en')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
</code></pre>

<p>which prints:</p>

<pre><code>Huey, Dewey, and Louie
</code></pre>

<p>However, if I have something like the following:</p>

<pre><code>sentence = 'Donald Duck - Disney'
</code></pre>

<p>then the <code>-</code> (dash) is recognised as <code>&lt;PUNCT&gt;</code> and the whole sentence is recognised as a list -- which it isn't.</p>

<p>Is there a way to specify that only <code>,</code> and <code>;</code> are valid <code>&lt;PUNCT&gt;</code> for lists? </p>

<p>I've looked for some reference about this regex language for matching PoS tags with no luck, can anybody help? Thanks in advance!</p>

<p>PS: I tried to replace <code>&lt;PUNCT|CCONJ&gt;</code> with <code>&lt;[;,]|CCONJ&gt;</code>, <code>&lt;;,|CCONJ&gt;</code>, <code>&lt;[;,]|CCONJ&gt;</code>, <code>&lt;PUNCT[;,]|CCONJ&gt;</code>, <code>&lt;;|,|CCONJ&gt;</code> and <code>&lt;';'|','|CCONJ&gt;</code> as suggested in the comments, but it didn't work...</p>
",Multilingual Language Processing & Language Identification,matching po tag specific text using method find certain chunk text sentence instance assuming text like detect enumeration use following code version available time writing print however something like following dash recognised whole sentence recognised list way specify valid list looked reference regex language matching po tag luck anybody help thanks advance p tried replace suggested comment work
sentiment analysis for English vs Urdu,"<p>I am new to NLP and have started working in Sentiment Analysis in one of the resource-poor languages - Urdu. I have a basic question. Work which has already been done in English is mature.How my working in sentiment analysis for the Urdu language can make a contribution as Both English and Urdu sentiment Analysis use the same machine learning algorithms. How the Sentiment Analysis techniques for English make them different from other languages. In simple words can I directly apply the Sentiment Analysis techniques developed for the English language on Urdu.</p>
",Multilingual Language Processing & Language Identification,sentiment analysis english v urdu new nlp started working sentiment analysis one resource poor language urdu basic question work ha already done english mature working sentiment analysis urdu language make contribution english urdu sentiment analysis use machine learning algorithm sentiment analysis technique english make different language simple word directly apply sentiment analysis technique developed english language urdu
Stanford NLP get the word contribution,"<p>I am fairly new to Natural Language Processing, but I need to dig in anyway. I am working with stanford NLP 3.7.0. I need to compute a few things and for that I need the word-contribution from the sentence for each word. So let's say the sentence is: ""I am an idiot"".</p>

<p>And the word contribution could be like: (""I"" : 5, ""am"" : 2, ""an"": 0, ""idiot"" : 10). That's just an example.
And the total contribution could be </p>

<pre><code>(5+2+0+10)/4; /*number of words(Just an example) */
</code></pre>

<p>If I have it correctly I am using the</p>

<pre><code>Constituent.score();
</code></pre>

<p>method to get the score of this word. This method returns a double.</p>

<p>However; I am not sure what exactly is the result computed against, and basically how is this score deduced? Also, what are the maximum and minimum values I can get. I tried googling for this, but could not find anything specific. Maybe you guys can help.</p>

<p>p.s. If it is important, I am working with Java.</p>

<p>Thank You</p>
",Multilingual Language Processing & Language Identification,stanford nlp get word contribution fairly new natural language processing need dig anyway working stanford nlp need compute thing need word contribution sentence word let say sentence idiot word contribution could like idiot example total contribution could correctly using method get score word method return double however sure exactly result computed basically score deduced also maximum minimum value get tried googling could find anything specific maybe guy help p important working java thank
Replacing year with English words in R,"<p>I need to pre-process a speech transcript for forced-alignment. However, I am having difficulty with replacing year with text representation. For example, 1984 needs to be replaced with ""nineteen eighty four"". I tried the replace_number function of the qdap package. The package is awesome, but it replaces 1984 with ""one thousand nine hundred eighty four"" instead. Are there other functions from any R packages that may I try? Thanks!</p>
",Multilingual Language Processing & Language Identification,replacing year english word r need pre process speech transcript forced alignment however difficulty replacing year text representation example need replaced nineteen eighty four tried replace number function qdap package package awesome replaces one thousand nine hundred eighty four instead function r package may try thanks
Python parse text from multiple txt file,"<p>Seeking advice on how to mine items from multiple text files to build a dictionary. </p>

<p>This text file: <a href=""https://pastebin.com/Npcp3HCM"" rel=""noreferrer"">https://pastebin.com/Npcp3HCM</a></p>

<p>Was manually transformed into this required data structure: <a href=""https://drive.google.com/file/d/0B2AJ7rliSQubV0J2Z0d0eXF3bW8/view"" rel=""noreferrer"">https://drive.google.com/file/d/0B2AJ7rliSQubV0J2Z0d0eXF3bW8/view</a></p>

<p>There are thousands of such text files and they may have different section headings as shown in these examples:</p>

<ol>
<li><a href=""https://pastebin.com/wWSPGaLX"" rel=""noreferrer"">https://pastebin.com/wWSPGaLX</a> </li>
<li><a href=""https://pastebin.com/9Up4RWHu"" rel=""noreferrer"">https://pastebin.com/9Up4RWHu</a></li>
</ol>

<p>I started off by reading the files</p>

<pre><code>from glob import glob

txtPth = '../tr-txt/*.txt'
txtFiles = glob(txtPth)

with open(txtFiles[0],'r') as tf:
    allLines = [line.rstrip() for line in tf]

sectionHeading = ['Corporate Participants',
                  'Conference Call Participiants',
                  'Presentation',
                  'Questions and Answers']

for lineNum, line in enumerate(allLines):
    if line in sectionHeading:
        print(lineNum,allLines[lineNum])
</code></pre>

<p>My idea was to look for the line numbers where section headings existed and try to extract the content in between those line numbers, then strip out separators like dashes. That didn't work and I got stuck in trying to create a dictionary of this kind so that I can later run various natural language processing algorithms on quarried items. </p>

<pre><code>{file-name1:{
    {date-time:[string]},
    {corporate-name:[string]},
    {corporate-participants:[name1,name2,name3]},
    {call-participants:[name4,name5]},
    {section-headings:{
        {heading1:[
            {name1:[speechOrderNum, text-content]},
            {name2:[speechOrderNum, text-content]},
            {name3:[speechOrderNum, text-content]}],
        {heading2:[
            {name1:[speechOrderNum, text-content]},
            {name2:[speechOrderNum, text-content]},
            {name3:[speechOrderNum, text-content]},
            {name2:[speechOrderNum, text-content]},
            {name1:[speechOrderNum, text-content]},
            {name4:[speechOrderNum, text-content]}],
        {heading3:[text-content]},
        {heading4:[text-content]}
        }
    }
}
</code></pre>

<p>The challenge is that different files may have different headings and number of headings. But there will always be a section called ""Presentation"" and very likely to have ""Question and Answer"" section. These section headings are always separated by a string of equal-to signs. And content of different speaker is always separated by string of dashes. The ""speech order"" for Q&amp;A section is indicated with a number in square brackets. The participants are are always indicated in the beginning of the document with an asterisks before their name and their tile is always on the next line. </p>

<p>Any suggestion on how to parse the text files is appreciated. The ideal help would be to provide guidance on how to produce such a dictionary (or other suitable data structure) for each file that can then be written to a database. </p>

<p>Thanks</p>

<p>--EDIT--</p>

<p>One of the files looks like this: <a href=""https://pastebin.com/MSvmHb2e"" rel=""noreferrer"">https://pastebin.com/MSvmHb2e</a></p>

<p>In which the ""Question &amp; Answer"" section is mislabeled as ""Presentation"" and there is no other ""Question &amp; Answer"" section. </p>

<p>And final sample text: <a href=""https://pastebin.com/jr9WfpV8"" rel=""noreferrer"">https://pastebin.com/jr9WfpV8</a></p>
",Multilingual Language Processing & Language Identification,python parse text multiple txt file seeking advice mine item multiple text file build dictionary text file wa manually transformed required data structure thousand text file may different section heading shown example started reading file idea wa look line number section heading existed try extract content line number strip separator like dash work got stuck trying create dictionary kind later run various natural language processing algorithm quarried item challenge different file may different heading number heading always section called presentation likely question answer section section heading always separated string equal sign content different speaker always separated string dash speech order q section indicated number square bracket participant always indicated beginning document asterisk name tile always next line suggestion parse text file appreciated ideal help would provide guidance produce dictionary suitable data structure file written database thanks edit one file look like question answer section mislabeled presentation question answer section final sample text
Does an algorithm exist to help detect the &quot;primary topic&quot; of an English sentence?,"<p>I'm trying to find out if there is a known algorithm that can detect the ""key concept"" of a sentence.</p>

<p>The use case is as follows:</p>

<ol>
<li>User enters a sentence as a query (Does chicken taste like turkey?)</li>
<li>Our system identifies the concepts of the sentence (chicken, turkey)</li>
<li>And it runs a search of our corpus content</li>
</ol>

<p>The area that we're lacking in is identifying what the core ""topic"" of the sentence is really about.  The sentence ""Does chicken taste like turkey"" has a primary topic of ""chicken"", because the user is asking about the taste of chicken.  While ""turkey"" is a helper topic of less importance.</p>

<p>So... I'm trying to find out if there is an algorithm that will help me identify the primary topic of a sentence... Let me know if you are aware of any!!! </p>
",Multilingual Language Processing & Language Identification,doe algorithm exist help detect primary topic english sentence trying find known algorithm detect key concept sentence use case follows user enters sentence query doe chicken taste like turkey system identifies concept sentence chicken turkey run search corpus content area lacking identifying core topic sentence really sentence doe chicken taste like turkey ha primary topic chicken user asking taste chicken turkey helper topic le importance trying find algorithm help identify primary topic sentence let know aware
Chunking with Python-Treetaggerwrapper,"<p>The Treetagger can do POS-tagging as well as text-chunking, which means extracting verbal and nominal clauses, as in this German example:</p>

<pre><code>$ echo 'Das ist ein Test.' | cmd/tagger-chunker-german
    reading parameters ...
    tagging ...
     finished.
&lt;NC&gt;
Das PDS die
&lt;/NC&gt;
&lt;VC&gt;
ist VAFIN   sein
&lt;/VC&gt;
&lt;NC&gt;
ein ART eine
Test    NN  Test
&lt;/NC&gt;
.   $.  .
</code></pre>

<p>I'm trying to figure out how to do this with the Treetaggerwrapper in Python (since it's faster than directly calling Treetagger), but I can't figure out how it's done. The documentation refers to chunking as preprocessing, so I tried using this:</p>

<pre><code>tags = tagger.tag_text(u""Dieser Satz ist ein Satz."",prepronly=True)
</code></pre>

<p>But the output is just a list of the words with no information added. I'm starting to think that what the Wrapper calls Chunking is something different than what the actual tagger calls Chunking, but maybe I'm just missing something? Any help would be appreciated.</p>
",Multilingual Language Processing & Language Identification,chunking python treetaggerwrapper treetagger po tagging well text chunking mean extracting verbal nominal clause german example trying figure treetaggerwrapper python since faster directly calling treetagger figure done documentation refers chunking preprocessing tried using output list word information added starting think wrapper call chunking something different actual tagger call chunking maybe missing something help would appreciated
Syntaxnet pretrained models for Italian language,"<p>I'm trying to use Syntaxnet pretrained models for Italian language. I just found one issue related to tokenization task on Italian and Spanish languages. I have got a preliminar problem. The posted link for pretrained dataset dowload does not work. Here it is : <a href=""https://github.com/tensorflow/models/blob/master/syntaxnet/universal.md"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/syntaxnet/universal.md</a>. Could someone provide me a working repository link? Thanks a lot</p>
",Multilingual Language Processing & Language Identification,syntaxnet pretrained model italian language trying use syntaxnet pretrained model italian language found one issue related tokenization task italian spanish language got preliminar problem posted link pretrained dataset dowload doe work could someone provide working repository link thanks lot
How can lemmatize arabic text using its POS tags?,"<p>when I use lemmatize for arabic the word dosn't change how can lemmatize arabic text using its <strong>POS</strong> tags?</p>

<p><a href=""https://i.sstatic.net/oWZVp.png"" rel=""nofollow noreferrer"">the output I get</a></p>
",Multilingual Language Processing & Language Identification,lemmatize arabic text using po tag use lemmatize arabic word dosn change lemmatize arabic text using po tag output get
How to use HeidelTime as UIMA AnalysisEngine with DKPro,"<p>I'm currently working on a project to extract biographical information from textual sources. One step is the annotation of the source to see what's actually in there. To do that, I'd like to use <a href=""https://github.com/HeidelTime/heideltime"" rel=""nofollow noreferrer"">HeidelTime</a> because its <a href=""https://github.com/HeidelTime/heideltime/wiki/Architectural-Overview#uima-interface"" rel=""nofollow noreferrer"">documentation</a> says it fits nicely into an UIMA pipeline. Because I'm still a beginner at NLP, I've dabbled with the <a href=""https://dkpro.github.io/dkpro-core/releases/1.8.0/docs/component-reference.html"" rel=""nofollow noreferrer"">DKPro Core Framework</a> which so far has provided convenient access to all components I wanted, including wrapping things up in pipelines like so:</p>

<pre><code>public static void main(String[] args) throws UIMAException, IOException {
    Path inputDir = Paths.get(args[0]);
    String language = args[1];
    String fileForm = String.format(""[+]*%s"", args[2]);
    Path outputFile = Paths.get(args[3]);

    CollectionReader reader = createReader(TextReader.class,
            TextReader.PARAM_SOURCE_LOCATION, inputDir.toString(),
            TextReader.PARAM_LANGUAGE, language,
            TextReader.PARAM_PATTERNS, new String[]{fileForm});
    AnalysisEngineDescription segmenter = createEngineDescription(StanfordSegmenter.class,
            StanfordSegmenter.PARAM_LANGUAGE, language,
            StanfordSegmenter.PARAM_WRITE_SENTENCE, true,
            StanfordSegmenter.PARAM_WRITE_TOKEN, true
    );
    AnalysisEngineDescription ner = createEngineDescription(StanfordNamedEntityRecognizer.class);
    AnalysisEngineDescription writer = createEngineDescription(TokenizedTextWriter.class,
            TokenizedTextWriter.PARAM_TARGET_LOCATION, outputFile.toString(),
            TokenizedTextWriter.PARAM_OVERWRITE, true,
            TokenizedTextWriter.PARAM_EXTENSION, "".txt""
    );
    runPipeline(reader, segmenter, ner, writer);
}
</code></pre>

<p>The documentation states that the main analysis class of HeidelTime implements the necessary interface, so I added it, including the suggested pre- and post-processing AnalysisEngines:</p>

<pre><code>public static void main(String[] args) throws UIMAException, IOException {
    Path inputDir = Paths.get(args[0]);
    String language = args[1];
    String fileForm = String.format(""[+]*%s"", args[2]);
    Path outputFile = Paths.get(args[3]);

    CollectionReader reader = createReader(TextReader.class,
            TextReader.PARAM_SOURCE_LOCATION, inputDir.toString(),
            TextReader.PARAM_LANGUAGE, language,
            TextReader.PARAM_PATTERNS, new String[]{fileForm});
    AnalysisEngineDescription segmenter = createEngineDescription(StanfordSegmenter.class,
            StanfordSegmenter.PARAM_LANGUAGE, language,
            StanfordSegmenter.PARAM_WRITE_SENTENCE, true,
            StanfordSegmenter.PARAM_WRITE_TOKEN, true
    );
    AnalysisEngineDescription ner = createEngineDescription(StanfordNamedEntityRecognizer.class);
    // ======= HeidelTime ======
    AnalysisEngineDescription treeTagger = createEngineDescription(TreeTaggerWrapper.class);
    AnalysisEngineDescription heidelTime = createEngineDescription(HeidelTime.class);
    AnalysisEngineDescription intervalTagger = createEngineDescription(IntervalTagger.class);
    // ======= HeidelTime ======
    AnalysisEngineDescription writer = createEngineDescription(TokenizedTextWriter.class,
            TokenizedTextWriter.PARAM_TARGET_LOCATION, outputFile.toString(),
            TokenizedTextWriter.PARAM_OVERWRITE, true,
            TokenizedTextWriter.PARAM_EXTENSION, "".txt""
    );
    runPipeline(reader, segmenter, ner, treeTagger, heidelTime, intervalTagger, writer);
}
</code></pre>

<p>However, when I run this, I encounter the following error:</p>

<pre><code>1016 [main] WARN  org.apache.uima.resource.metadata.TypeSystemDescription  - [jar:file:/C:/Users/User/.m2/repository/com/github/heideltime/heideltime/2.2.1/heideltime-2.2.1.jar!/desc/type/HeidelTime_TypeSystemStyleMap.xml] is not a type file. Ignoring.
org.apache.uima.util.InvalidXMLException: Invalid descriptor at jar:file:/C:/Users/User/.m2/repository/com/github/heideltime/heideltime/2.2.1/heideltime-2.2.1.jar!/desc/type/HeidelTime_TypeSystemStyleMap.xml.
    at org.apache.uima.util.impl.XMLParser_impl.parse(XMLParser_impl.java:218)
    at org.apache.uima.util.impl.XMLParser_impl.parseTypeSystemDescription(XMLParser_impl.java:729)
    at org.apache.uima.util.impl.XMLParser_impl.parseTypeSystemDescription(XMLParser_impl.java:718)
    at org.apache.uima.fit.factory.TypeSystemDescriptionFactory.createTypeSystemDescription(TypeSystemDescriptionFactory.java:107)
    at org.apache.uima.fit.factory.CollectionReaderFactory.createReader(CollectionReaderFactory.java:213)
    at de.uniba.minf.msc.stemper.corpus.pantheon.Pipeline.main(Pipeline.java:37)
Caused by: org.apache.uima.util.InvalidXMLException: The XML parser encountered an unknown element type: styleMap.
    at org.apache.uima.util.impl.XMLParser_impl.buildObject(XMLParser_impl.java:301)
    at org.apache.uima.util.impl.SaxDeserializer_impl.getObject(SaxDeserializer_impl.java:142)
    at org.apache.uima.util.impl.XMLParser_impl.parse(XMLParser_impl.java:209)
    ... 5 more
</code></pre>

<p>The HeidelTime component doesn't seem to properly translate with the other Analysis Engines. The documentation says it should, however the responsible class is missing from the repository and probably from the Maven artifact I pulled as well. I don't know where to begin looking for a fix, and so far I've found nothing hinting in the direction online, except for some old questions as how to use the standalone <a href=""https://stackoverflow.com/questions/27337268/how-to-use-heideltime-temporal-tagger-inside-a-java-project?s=1%7C2.2967"">here</a> and <a href=""https://stackoverflow.com/questions/17283557/do-i-need-to-rewrite-my-entire-java-project-if-i-want-to-use-a-single-uima-depen/17285139#17285139"">here</a>.</p>
",Multilingual Language Processing & Language Identification,use heideltime uima analysisengine dkpro currently working project extract biographical information textual source one step annotation source see actually like use heideltime documentation say fit nicely uima pipeline still beginner nlp dabbled dkpro core framework far ha provided convenient access component wanted including wrapping thing pipeline like documentation state main analysis class heideltime implement necessary interface added including suggested pre post processing analysisengines however run encounter following error heideltime component seem properly translate analysis engine documentation say however responsible class missing repository probably maven artifact pulled well know begin looking fix far found nothing hinting direction online except old question use standalone href href
Translation to Predicate Logic with Lexicon,"<p>How would one translate the following statement into predicate logic:</p>

<p>""<em>Even though the examiner hopes all students will satisfy the requirements for grade E or better, somebody will receive a lower grade and be disappointed</em>.""</p>
",Multilingual Language Processing & Language Identification,translation predicate logic lexicon would one translate following statement predicate logic even though examiner hope student satisfy requirement grade e better somebody receive lower grade disappointed
NLTK synset with other languages,"<p>Right now i'm trying to compare words from two different files, one english, one chinese. I have to identify if any of the english words are related to the chinese words and if they are, are they equal or is one a hypernym of the other. I can use synsets for english but what can i do about the chinese words?</p>
",Multilingual Language Processing & Language Identification,nltk synset language right trying compare word two different file one english one chinese identify english word related chinese word equal one hypernym use synset english chinese word
Preserve lines in Stanford CoreNLP,"<p>From <a href=""https://nlp.stanford.edu/software/tokenizer.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/tokenizer.html</a> , the Stanford tokenizer has the option <code>-preserveLines</code> to preserve the line breaks determined by the <code>\n</code> in a text file.</p>

<p>But in CoreNLP, there is the <code>ssplit</code> that splits the sentences and breaks the lines up. </p>

<p>So given the text file:</p>

<pre><code>ÁîüÂëäÊøÄÂäπÈõëÈ°åÈπøÈöõÂÜôÂæπËºù‰∏á„ÄÇÊúÄÂçîÈ†òÈáé‰∫¨ÈÉ®ÁøíÈ†ÇÁµåÊó•‰∫§Êó•ËÄÉÂΩìÂÇô„ÄÇ
foo foo bar bar.
</code></pre>

<p>The desired output to just tokenize the file should be:</p>

<pre><code>Áîü Âëä ÊøÄÂäπ ÈõëÈ°å ÈπøÈöõ ÂÜô ÂæπËºù‰∏á „ÄÇ ÊúÄÂçîÈ†ò Èáé‰∫¨ÈÉ® ÁøíÈ†ÇÁµå Êó• ‰∫§Êó• ËÄÉ ÂΩìÂÇô „ÄÇ
foo foo bar bar .
</code></pre>

<p>Using the Chinese CoreNLP models:</p>

<pre><code>wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip
cp stanford-corenlp-full-2016-10-31/stanford-corenlp-3.7.0.jar .
wget http://nlp.stanford.edu/software/stanford-english-corenlp-2016-10-31-models.jar
echo -e ""ÁîüÂëäÊøÄÂäπÈõëÈ°åÈπøÈöõÂÜôÂæπËºù‰∏á„ÄÇÊúÄÂçîÈ†òÈáé‰∫¨ÈÉ®ÁøíÈ†ÇÁµåÊó•‰∫§Êó•ËÄÉÂΩìÂÇô„ÄÇ\nfoo foo bar bar."" &gt; input.txt

java -mx10g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -props stanford-tools/StanfordCoreNLP-chinese.tokenize.properties -file input.txt -outputFormat text
</code></pre>

<p>And the <code>stanford-chinese.properties</code> file should look like this:</p>

<pre><code>annotators = segment, ssplit
customAnnotatorClass.segment = edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true
ssplit.boundaryTokenRegex = [.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+
</code></pre>

<p>(Note: the Stanford CoreNLP for chinese don't write to file if the <code>ssplit</code> isn't used, see <a href=""https://github.com/stanfordnlp/CoreNLP/issues/409"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP/issues/409</a>)</p>

<p>And the output produces 3 lines instead of 2:</p>

<pre><code>Áîü Âëä ÊøÄÂäπ ÈõëÈ°å ÈπøÈöõ ÂÜô ÂæπËºù‰∏á „ÄÇ 
ÊúÄÂçîÈ†ò Èáé‰∫¨ÈÉ® ÁøíÈ†ÇÁµå Êó• ‰∫§Êó• ËÄÉ ÂΩìÂÇô „ÄÇ
foo foo bar bar .
</code></pre>

<p>Using <code>-preserveLines</code> option with the following command yields the same result:</p>

<pre><code>java -mx10g -cp ""stanford-tools/*""  edu.stanford.nlp.pipeline.StanfordCoreNLP -props stanford-tools/StanfordCoreNLP-chinese.tokenize.properties -preserveLines -file input.txt
</code></pre>

<hr>

<p>There is no explicit option int he Stanford segmenter to preseve the lines from <a href=""https://nlp.stanford.edu/software/segmenter.shtml#Questions"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/segmenter.shtml#Questions</a> </p>

<p>I understand that I can simply use the Stanford segmenter without the fuller NLP suite to segment using:</p>

<pre><code>wget https://nlp.stanford.edu/software/stanford-segmenter-2016-10-31.zip
unzip stanford-segmenter-2016-10-31.zip
cd stanford-segmenter-2016-10-13
./segment.sh ctb input.txt utf-8 0
</code></pre>

<p>[out]:</p>

<pre><code>Áîü Âëä ÊøÄÂäπ ÈõëÈ°å ÈπøÈöõ ÂÜô ÂæπËºù‰∏á „ÄÇ ÊúÄÂçîÈ†ò Èáé‰∫¨ÈÉ® ÁøíÈ†ÇÁµå Êó• ‰∫§Êó• ËÄÉ ÂΩìÂÇô „ÄÇ
</code></pre>

<p>foo foo bar bar .</p>

<p>But the question still remains on Stanford CoreNLP, i.e. <strong>can Stanford CoreNLP with the Chinese model preserve linebreaks?</strong></p>
",Multilingual Language Processing & Language Identification,preserve line stanford corenlp stanford tokenizer ha option preserve line break determined text file corenlp split sentence break line given text file desired output tokenize file using chinese corenlp model file look like note stanford corenlp chinese write file used see output produce line instead using option following command yield result explicit option int stanford segmenter preseve line understand simply use stanford segmenter without fuller nlp suite segment using foo foo bar bar question still remains stanford corenlp e stanford corenlp chinese model preserve linebreaks
How to tokenize Spanish text with NLTK or Pattern-es,"<p>Basically the issue I am having is with separating object and direct object pronouns from verbs.</p>

<p>Ie 'aprenderlo' should ideally be tokenized as two separate entities, 'dimelo' should be tokenized as three.  I have tried a variety of taggers in both libraries, and so far nothing has produced the results I want.  However, I am sure this must be a common problem - any ideas?</p>
",Multilingual Language Processing & Language Identification,tokenize spanish text nltk pattern e basically issue separating object direct object pronoun verb ie aprenderlo ideally tokenized two separate entity dimelo tokenized three tried variety tagger library far nothing ha produced result want however sure must common problem idea
How to read multiple pages using pdfbox,"<p>I want to read a Sinhala eBook using PDF box. I can read English one page PDF using PDF box. what I want is read multiple pages at once and store them</p>
",Multilingual Language Processing & Language Identification,read multiple page using pdfbox want read sinhala ebook using pdf box read english one page pdf using pdf box want read multiple page store
"Given a dictionary and a list of letters, make a program learn to generate valid words | Javascript","<p>I'm working on a big machine learning/nlp project and I'm stuck at a small part of it. (PM me, if you want to know what I'm working on exactly.)</p>

<p>I try to code a program in Javascript that learns to generate valid words, only by using all letters of the alphabet.</p>

<p>What I have is a database of 500K different words. It's a big JS object, structured like this (the words are german):</p>

<pre><code>database = {
    ""um"": {id: 1, word: ""um"", freq: 10938},
    ""oder"": {id: 2, word: ""oder"", freq: 10257},
    ""Er"": {id: 3, word: ""Er"", freq: 9323},
    ...
}
</code></pre>

<p><code>""freq""</code> means frequency obviously. (Maybe this value sometimes gets important but I currently don't use it, so just ignore it.)
<hr>
The way my program currently works is:
In the first iteration, it generates a completely random word between 2 and 13 letters long and searches for it in the database. If it's there, every letter in the word gets a good rating, if it's not there, they get a bad rating. Also the word length gets rated. If the word is valid, its word length gets a good rating, if it's not, its word length gets a bad rating.</p>

<p>In the iterations after that first one, it doesn't generate a word with <strong>random</strong> letters and a <strong>random</strong> word length. It uses probabilities based on the ratings of the letters and the word length.</p>

<p>For example, let's say it found the words ""the"", ""so"" and ""if"" after the first 100 iterations. So the letters ""t"", ""h"", ""e"" and the letters ""s"", ""o"", and the letters ""i"", ""f"" are good rated, and the word length of 2 and 3 is also good rated. So the word generated in the next iteration will more likely contain these good rated letters than bad rated letters.</p>

<p>Of course, the program also checks if the currently generated word already was generated and if so, then this word doesn't get rated again and it generates a new one.</p>

<p>In theory it should learn the optimal letter frequency and the optimal word-length-frequency by its own and sometimes only generate valid words.</p>

<p>Yeah. Of course this doesn't work. It gets better for the first few iterations, but as soon as it has found all the 2-lettered words it gets worse. I think my whole way how I do this is wrong. I've actually tried it out and have a (not so beautiful) graph after 5000 iterations for you:</p>

<p><a href=""https://i.sstatic.net/bgT7T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bgT7T.png"" alt=""(not so beautiful) graph""></a></p>

<p>Red line: wrong words generated</p>

<p>Green line: right words generated
<hr>
Yeah. What is the problem here? Am I doing machine learning wrong? And do you have a solution? Some algorithm or trie system?</p>

<p>PS: I'm aware of <a href=""https://stackoverflow.com/questions/25298200/given-a-dictionary-and-a-list-of-letters-find-all-valid-words-that-can-be-built"">this</a>, but it's not in JS, I don't understand it and I can't comment on it.</p>
",Multilingual Language Processing & Language Identification,given dictionary list letter make program learn generate valid word javascript working big machine learning nlp project stuck small part pm want know working exactly try code program javascript learns generate valid word using letter alphabet database k different word big j object structured like word german mean frequency obviously maybe value sometimes get important currently use ignore way program currently work first iteration generates completely random word letter long search database every letter word get good rating get bad rating also word length get rated word valid word length get good rating word length get bad rating iteration first one generate word random letter random word length us probability based rating letter word length example let say found word first iteration letter h e letter letter f good rated word length also good rated word generated next iteration likely contain good rated letter bad rated letter course program also check currently generated word already wa generated word get rated generates new one theory learn optimal letter frequency optimal word length frequency sometimes generate valid word yeah course work get better first iteration soon ha found lettered word get worse think whole way wrong actually tried beautiful graph iteration red line wrong word generated green line right word generated yeah problem machine learning wrong solution algorithm trie system p aware href j understand comment p
Arabic text not showing in R-,"<p>Just started working with R in Arabic as I plan to do text analysis and text mining with Hadith corpus. I have been reading threads related to my question but nevertheless, still can't manage to get the REAL basics here (sorry, absolute beginner).</p>

<p>So, I entered:
textarabic.v &lt;- scan(""data/arabic-text.txt"", encoding=""UTF-8"", what= ""character"",sep=""\n"")</p>

<p>And what comes out <code>textarabic.v</code> is of course, symbols (pic). Prior to this, I saved my text in utf-8 as I read in a thread but still nothing shows in Arabic.</p>

<p>I can type in Arabic R but scan brings the text in symbols.</p>

<p><a href=""https://i.sstatic.net/vwwv5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vwwv5.png"" alt=""enter image description here""></a></p>

<p>Also read and tried to implement other user's are codes to make Arabic text function but I don't even know how and where to implement them.
I added to R, tm and NLP packages. </p>

<p>What do you suggest for me to do next?
Thanks in advance,</p>
",Multilingual Language Processing & Language Identification,arabic text showing r started working r arabic plan text analysis text mining hadith corpus reading thread related question nevertheless still manage get real basic sorry absolute beginner entered textarabic v scan data arabic text txt encoding utf character sep n come course symbol pic prior saved text utf read thread still nothing show arabic type arabic r scan brings text symbol also read tried implement user code make arabic text function even know implement added r tm nlp package suggest next thanks advance
Splitting german chunk of text into sentences by using Xamarin / Google Cloud Speech API,"<p>I'm using the Google Cloud Speech API in a Xamarin.Forms Application. Unfortunatlely, the API just returns spoken language in a chunk of text without punctuation, question marks and so on. I know that can be done by using the Google Cloud Natural Language API, but it doesn't exist for the german language yet. I also looked after the Stanford NLP but I couldn't find out how to split the text into sentences. </p>

<p>For example, i wanna turn</p>

<p>""Die Sonne scheint die Katze ist weiss"" (The sun is shining the cat is white)</p>

<p>into</p>

<p>""Die Sonne scheint. Die Katze ist weiss."" (The sun is shining. The cat is white.)</p>
",Multilingual Language Processing & Language Identification,splitting german chunk text sentence using xamarin google cloud speech api using google cloud speech api xamarin form application unfortunatlely api return spoken language chunk text without punctuation question mark know done using google cloud natural language api exist german language yet also looked stanford nlp find split text sentence example wan na turn die sonne scheint die katze ist wei sun shining cat white die sonne scheint die katze ist wei sun shining cat white
How can I extract dependency triple from a parse tree,"<p>How can I extract dependency triple from a parse tree generated by an OpenNLP parser for arabic texts?</p>

<p>for example, the dependency triples in ‚ÄúI have a brown dog‚Äù consist of:
  (have subj I), (have obj dog), (dog adj-mod brown),(dog det a)</p>
",Multilingual Language Processing & Language Identification,extract dependency triple parse tree extract dependency triple parse tree generated opennlp parser arabic text example dependency triple brown dog consist subj obj dog dog adj mod brown dog det
Genres classification of documents,"<p>I'm looking for library whatever it's machine learning or something else it doesn't matter which will help me categorize the content I have. Basically content I have is articles written and I wanna know which of them are politics or sport bla bla so I have categorize them.</p>

<p>I was trying openNLP but cannot get it working as I need, is there anything else that will solve my need?</p>

<p>I guess I need some kind of Machine learning with natural language processing NLP but I can't find something that will do my job at this point.</p>
",Multilingual Language Processing & Language Identification,genre classification document looking library whatever machine learning something else matter help categorize content basically content article written wan na know politics sport bla bla categorize wa trying opennlp get working need anything else solve need guess need kind machine learning natural language processing nlp find something job point
Word2Vec: Is it possible to train with respect to weight in NLP?,"<p>I used <code>Gensim</code>'s <code>Word2Vec</code> for training most similar words.</p>

<p>My dataset is all posts from my college community site.</p>

<p>Each dataset consists of like this:</p>

<pre><code>(title) + (contents) + (all comments)  // String
</code></pre>

<p>For example, </p>

<pre><code>data[0] =&gt; ""This is title. Contents is funny. What so funny?. Not funny for me""
</code></pre>

<p>So, I have around 400,000 datas like above and make them as a vector and try to train these data via <code>Word2Vec</code>. </p>

<p>I wonder that whether it is possible to make <code>Word2Vec</code> consider WEIGHT, which means, if I give an weight to certain data vector, <code>Word2Vec</code> train this data in a way that each word in this data vector has more strong relationship(similarity).</p>

<p>For example, If I gave a weight 5 to dataset, <code>I like Pizza, Chicken</code>, the word <code>Pizza</code> and <code>Chicken</code> (or <code>like</code> and <code>Pizza</code> etc) has strong relations than other data vector's words.</p>

<p>Would that be possible?</p>

<p>Sorry for poor explanation but I'm not native english speaker. If need more detailed info, please post comment.</p>
",Multilingual Language Processing & Language Identification,word vec possible train respect weight nlp used training similar word dataset post college community site dataset consists like example around data like make vector try train data via wonder whether possible make consider weight mean give weight certain data vector train data way word data vector ha strong relationship similarity example gave weight dataset word etc ha strong relation data vector word would possible sorry poor explanation native english speaker need detailed info please post comment
Natural Language Processing in Ruby,"<p>I'm looking to do some sentence analysis (mostly for twitter apps) and infer some general characteristics. Are there any good natural language processing libraries for this sort of thing in Ruby?</p>

<p>Similar to <a href=""https://stackoverflow.com/questions/870460/java-is-there-a-good-natural-language-processing-library"">Is there a good natural language processing library</a> but for Ruby. I'd prefer something very general, but any leads are appreciated!</p>
",Multilingual Language Processing & Language Identification,natural language processing ruby looking sentence analysis mostly twitter apps infer general characteristic good natural language processing library sort thing ruby similar href good natural language processing library ruby prefer something general lead appreciated
How to use keras mask layer correctly?,"<p>The Keras mask layer can be used to deal with variable-length sequence training of RNNs. When I use them I get lower accuracy with the mask layer than single batch training. I suspect that I'm not using mask layer correctly.</p>

<p>My goal is to train an LSTM to learn how to spell words. The sequences, which are different English words, are encoded with one-hot representation. Below is code of data encoding part: <code>chars</code> are the set of all letters that make up the sequences, <code>mylist</code> is list of the sequences, <code>MAXLEN</code> is the max length of sequences.</p>

<pre><code>char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

X = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)
y = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)

for i, sentence in enumerate(mylist):
    for t in range(len(sentence)-Data_end):
        X[i, t, char_indices[sentence[t]]] = 1
        y[i, t, char_indices[sentence[t+1]]] = 1
</code></pre>

<p>My network is defined as:</p>

<pre><code>model = Sequential()
model.add(Masking(mask_value=0., input_shape=(None, len(chars))))
model.add(LSTM(2000, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(2000, return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributed(Dense(len(chars))))
model.add(Activation('softmax'))

sgd = SGD(lr=lr_init, decay=decay_init, momentum=momentum_init, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
early_stopping = EarlyStopping(patience=2,verbose=1)
</code></pre>

<p>To train:</p>

<pre><code>model.fit(X, y, callbacks=[early_stopping],batch_size=32, nb_epoch=1)
</code></pre>

<p>Am I using the mask layer correctly?</p>
",Multilingual Language Processing & Language Identification,use kera mask layer correctly kera mask layer used deal variable length sequence training rnns use get lower accuracy mask layer single batch training suspect using mask layer correctly goal train lstm learn spell word sequence different english word encoded one hot representation code data encoding part set letter make sequence list sequence max length sequence network defined train using mask layer correctly
apply function on each row of the dataframe and return a dataframe,"<p>I want to use jiebar to segment some Chinese sentences, these sentences are stored in a data frame.One way to do it is to convert the data frame into a list, then use apply_list. </p>

<pre><code>library(jiebar)
mixseg = worker()
c &lt;- apply_list(as.list(weibo$weibo), mixseg)
</code></pre>

<p>dataset: weibo$weibo</p>

<pre><code>[1] ""ËØ∏‰∏ΩÂ®úKitty ÊâìÂç°Á¨¨Â§©ËøêÂä®Âè™‰∏∫Êõ¥Â•ΩÁöÑËá™Â∑±ËøêÂä®Â∞±ÊòØÂùöÊåÅÂø´Êù•Âä†ÂÖ•ÊâìÂç°Á§æÂå∫Âêß""                
[2] ""‰ªäÂ§©ÁöÑÈ´òÂ∞îÂ§´ÁêÉËØæÈ´òÂ∞îÂ§´Ô∏èÊïôÁªÉÂèØÊòØÂÖ®ÁæéÊéíÂêçÂâçÁöÑÊïôÁªÉÂì¶\U0001f3cc AmericanNew Jersey Hudson""
[3] ""ÈºìÊéåÈºìÊéåÁ¨¨‰∏ÄÊ¨°ÊªëÈõ™Â±ÖÁÑ∂ÊòØÂú®ÁæéÂõΩ‰ΩìÈ™åÂú®Èõ™‰∏äÁöÑÊøÄÊÉÖ‰∏éÈÄüÂ∫¶ËøáÁòæÈÖ∑ Mount Peter""              
[4] ""‰º™Ë£ÖÂπ∏Á¶èÂ≠¶ÁùÄËÆ©Ëá™Â∑±Êõ¥ÂùöÂº∫  ÊàëÊ≠£Âú®‰ΩøÁî®ÂÅáÈù¢Â•≥Â≠©Â∞ÅÈù¢ÂõæÂ•ΩÊºÇ‰∫Æ‰Ω†‰ª¨ÈÉΩÂø´Êù•ËØïËØïÂ∞ÅÈù¢ÂõæÈ¢ÑËßà""    
[5] ""Êä¢Âà∞Âï¶Â§ßÂÆ∂ÂÖÉÂÆµËäÇÂø´‰πêÈ°∫ÂøÉÂ¶ÇÊÑèÈ≤úËä±È≤úËä± AmericanNew York Queens""                        
[6] "" Happy Chinese New Year ÁæéÂõΩCity of New York Central Park Conservancy""  
</code></pre>

<p>However, the size of the return list is 3 times larger than the original dataset if I do it in this way. Therefore, I want to use ldply/laply to return the result as a data frame, but I encounter this error</p>

<pre><code>b&lt;- ldply(weibo2, segment(mixseg))
Error in match(x, table, nomatch = 0L) : 
  argument ""jiebar"" is missing, with no default
</code></pre>
",Multilingual Language Processing & Language Identification,apply function row dataframe return dataframe want use jiebar segment chinese sentence sentence stored data frame one way convert data frame list use apply list dataset weibo weibo however size return list time larger original dataset way therefore want use ldply laply return result data frame encounter error
How to convert text file to CoNLL format for malt parser?,"<p>I'm trying to use malt parser with the pre made english model. However, I do not know how to convert a text corpus of English sentences into the CoNLL format that is necessary for Malt Parser to operate on. I could not find any documentation on the site. How should I go about this?</p>

<p>Update. I am referring to this post <a href=""https://stackoverflow.com/questions/17450652/create-conll-file-as-output-of-stanford-parser"">Create .conll file as output of Stanford Parser</a> to create a .conll. However, this is using Stanford Parser.</p>
",Multilingual Language Processing & Language Identification,convert text file conll format malt parser trying use malt parser pre made english model however know convert text corpus english sentence conll format necessary malt parser operate could find documentation site go update referring post href conll file output stanford parser create conll however using stanford parser
Regex for NLP rules,"<p>I need to come up with several regular expressions for some natural language processing rules. I am a beginner in Java so some direction would be useful and good enough for me to come up with expressions for the rest of the rules. The rule is as follows:</p>

<pre><code>.... such NP as {NP ,}* {(or | and)} NP 
</code></pre>

<p>e.g. ... works by such authors as Herrick,
Goldsmith, and Shakespeare. </p>

<p>Here NP is a noun chunk in a sentence which I get from: </p>

<pre><code>for (NLChunk chunk : sentence.getChunks()){
                if(chunk.getType().equals(""NP"")...rest of regex here)
</code></pre>
",Multilingual Language Processing & Language Identification,regex nlp rule need come several regular expression natural language processing rule beginner java direction would useful good enough come expression rest rule rule follows e g work author herrick goldsmith shakespeare np noun chunk sentence get
How can I lemmatize english words (example: &#39;run&#39; and &#39;ran&#39;) using R to bring them all to the same tense?,"<p>I want to lemmatize english words such that all of them get converted to the same tense. For example:</p>

<pre><code>c(""ran"",""run"",""running"") 
</code></pre>

<p>should become <code>c(""run"",""run"",""run"")</code>.</p>

<p>I have already explored R packages such as tm, wordnet, RTextTools, and Snowball C; but all of them result in the output <code>c(""ran"",""run"",""run"")</code>. As you can see, they do not convert ""ran"" to ""run"". </p>
",Multilingual Language Processing & Language Identification,lemmatize english word example run ran using r bring tense want lemmatize english word get converted tense example become already explored r package tm wordnet rtexttools snowball c result output see convert ran run
Joining adjacent words (tokens) in a TDM for tidy analysis,"<p>I have documents that have strings similar to the following:</p>

<pre><code>    textForAnalysis &lt;- c(""non-ifrs earnings numbers are report to be..."")
</code></pre>

<p>Which I enter into a corpus</p>

<pre><code>    textCorpus &lt;- Corpus(VectorSource(textForAnalysis))
</code></pre>

<p>Then translate to a TDM</p>

<pre><code>    textTDM &lt;- TermDocumentMatrix(textCorpus)
</code></pre>

<p>Then translate the TDM into tidy format for analysis</p>

<pre><code>    textTidy &lt;- tidy(textTDM)
</code></pre>

<p>When I print the text, everything is ok, </p>

<pre><code>    textTidy

&gt; textTidy
# A tibble: 6 √ó 3
      term document count
     &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;
1      are        1     1
2 earnings        1     1
3     ifrs        1     1
4      non        1     1
5  numbers        1     1
6   report        1     1
</code></pre>

<p>Except, I want to preserve the ""non-ifrs"" item as a single token (word).  I do not want to separate the ""non-ifrs"" phrase into ""non"" and ""ifrs"".  </p>

<p>How can I maintain adjacent wordings, e.g. ""non-ifrs"" as a single ""term"" (non-ifrs) in my analysis/TDM?</p>
",Multilingual Language Processing & Language Identification,joining adjacent word token tdm tidy analysis document string similar following enter corpus translate tdm translate tdm tidy format analysis print text everything ok except want preserve non ifrs item single token word want separate non ifrs phrase non ifrs maintain adjacent wording e g non ifrs single term non ifrs analysis tdm
How to Break a sentence into a few words,"<p>I want to ask how to break a sentence into a few words, what this is using of NLP (Natural Language Processing) in python called NLTK or PARSER ? on python i confused with the method, what method should i take in my case.</p>
",Multilingual Language Processing & Language Identification,break sentence word want ask break sentence word using nlp natural language processing python called nltk parser python confused method method take case
Machine learning for natural language processing - Custom translation,"<p>Lets say I have the following very simplified training and testing observations. </p>

<p><strong>Training</strong></p>

<pre><code>input: her favourite dog was a huskey and her favourite cat was a leopard
output: dog=huskey, cat=leopard

input: her favourite dog was a beagle and her favourite cat was a lion
output: dog=beagle, cat=lion

input: her favourite dog was a poodle and her favourite cat was a burmese
output: dog=poodle, cat=burmese
</code></pre>

<p><strong>Testing</strong></p>

<pre><code>input: her favourite dog was a collie and her favourite cat was a moggie
desired output: dog=collie, cat=moggie
</code></pre>

<ul>
<li>What is the best machine learning approach in python to enable me to have the testing input translated into the desired output? </li>
<li>What are the steps involved from taking this raw data to making this prediction?</li>
</ul>

<p>From some research in the area it seems that a lot of the existing machine learning packages are around classification, regression and clustering (e.g <a href=""http://scikit-learn.org/stable/"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/</a>), while what I am trying to do is a form of translation.</p>

<p>I have also looked into a few NLP packages, and the functionality falls more into the keyword identification, word type identification and sentiment analysis (e.g <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">http://www.nltk.org/</a>). There are also some translation packages available, but these are for pre-existing languages (<a href=""http://pythonhosted.org/goslate/"" rel=""nofollow noreferrer"">http://pythonhosted.org/goslate/</a>)</p>

<p>I recognise that for this particular case machine learning is thoroughly unnecessary, however in practise there will be far more complicated, different and numerous inputs to translate.</p>
",Multilingual Language Processing & Language Identification,machine learning natural language processing custom translation let say following simplified training testing observation training testing best machine learning approach python enable testing input translated desired output step involved taking raw data making prediction research area seems lot existing machine learning package around classification regression clustering e g trying form translation also looked nlp package functionality fall keyword identification word type identification sentiment analysis e g also translation package available pre existing language recognise particular case machine learning thoroughly unnecessary however practise far complicated different numerous input translate
Multilingual Named Entity Linking,"<p>I am working on a clustering algorithm to cluster news articles from different sources into one event similar to what Google News does. Everything is working well, except for one problem.  </p>

<p>I am clustering Arabic news articles, and the algorithm is working very good, it is showing very good results on Politics and Sports articles, but when it comes to Games and Technology, the results are not that good. The problem is I am having a very low recall (fewer clusters than should have).  </p>

<p>After investigating, I found that the problem is with named entities. In Games and Tech, authors seem to be mixing between using English names, or Arabic equivalent name, and this is affecting the title terms weighing the most, which affect the final results in general.  </p>

<p>Now, I am looking for a way to find equal named entities even if they are in different languages. I still don't know how exactly, and I appreciate any help.</p>
",Multilingual Language Processing & Language Identification,multilingual named entity linking working clustering algorithm cluster news article different source one event similar google news doe everything working well except one problem clustering arabic news article algorithm working good showing good result politics sport article come game technology result good problem low recall fewer cluster investigating found problem named entity game tech author seem mixing using english name arabic equivalent name affecting title term weighing affect final result general looking way find equal named entity even different language still know exactly appreciate help
Calculating grammar similarity between two sentences,"<p>I'm making a program which provides some english sentences which user has to learn more.</p>

<p>For example:</p>

<blockquote>
  <p>First, I provide a sentence <em>""I have to go school today""</em> to user.
  Then if the user wants to learn more sentences like that, I find some sentences which have high grammar similarity with that sentence.</p>
</blockquote>

<p>I think the only way for providing sentences is to calculate similarity.</p>

<p>Is there a way to calculate grammar similarity between two sentences?</p>

<p>or is there a better way to make that algorithm?</p>

<p>Any advice or suggestions would be appreciated. Thank you.</p>
",Multilingual Language Processing & Language Identification,calculating grammar similarity two sentence making program provides english sentence user ha learn example first provide sentence go school today user user want learn sentence like find sentence high grammar similarity sentence think way providing sentence calculate similarity way calculate grammar similarity two sentence better way make algorithm advice suggestion would appreciated thank
Natural Language Processing: Find obscenities in English?,"<p>Given a set of words tagged for part of speech, I want to find those that are obscenities in mainstream English. How might I do this? Should I just make a huge list, and check for the presence of anything in the list? Should I try to use a regex to capture a bunch of variations on a single root?</p>

<p>If it makes it easier, I don't want to filter out, just to get a count. So if there are some false positives, it's not the end of the world, as long as there's a more or less uniformly over exaggerated rate.</p>
",Multilingual Language Processing & Language Identification,natural language processing find obscenity english given set word tagged part speech want find obscenity mainstream english might make huge list check presence anything list try use regex capture bunch variation single root make easier want filter get count false positive end world long le uniformly exaggerated rate
NLTK Performance,"<p>Alright, I've been pretty interested in natural language processing recently: however, I've used C until now for most of my work. I heard of NLTK, and I didn't know Python, but it seems quite easy to learn, and it's looking like a really powerful and interesting language. In particular, the NLTK module seems very, very adapted to what I need to do.</p>

<p>However, when using <a href=""https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use"">sample code for NLTK</a> and pasting that into a file called <code>test.py</code>, I've noticed it takes a very, very long time to run ! </p>

<p>I'm calling it from the shell like so:</p>

<pre><code>time python ./test.py
</code></pre>

<p>And on a 2.4 GHz machine with 4 GBs of RAM, it takes 19.187 seconds ! </p>

<p>Now, maybe this is absolutely normal, but I was under the impression that NTLK was <em>extremely</em> fast; I may have been mistaken, but is there anything obvious that I'm clearly doing wrong here?</p>
",Multilingual Language Processing & Language Identification,nltk performance alright pretty interested natural language processing recently however used c work heard nltk know python seems quite easy learn looking like really powerful interesting language particular nltk module seems adapted need however using href code nltk pasting file called noticed take long time run calling shell like ghz machine gb ram take second maybe absolutely normal wa impression ntlk wa extremely fast may mistaken anything obvious clearly wrong
Fix CoreNLP sentence splitting with European dates,"<p>Stanford CoreNLP does not work with many common European date formats (c.f. <a href=""https://en.wikipedia.org/wiki/Date_and_time_notation_in_Europe"" rel=""nofollow noreferrer"">Date and time notation in Europe (Wikipedia)</a>).</p>

<p>We all know that date parsing is a mess, in particular the <a href=""https://i.sstatic.net/VSr0j.jpg"" rel=""nofollow noreferrer"">crazy mm-dd-yyyy format the U.S. uses</a>. Nevertheless, CoreNLP is unable to process even basic European date formats, because it splits too aggressively on dots.</p>

<p>However, date information is often presented e.g. as:</p>

<blockquote>
  <p>Die Terroranschl√§ge am 11. September 2001, einem Dienstag, waren vier koordinierte Flugzeugentf√ºhrungen mit anschlie√üenden Selbstmordattentaten ... <a href=""https://de.wikipedia.org/wiki/Terroranschl%C3%A4ge_am_11._September_2001"" rel=""nofollow noreferrer"">(Wikipedia)</a></p>
</blockquote>

<p>Whatever I try, CoreNLP splits this into:</p>

<pre><code>1. Die Terroranschl√§ge am 11.
2. September 2001, einem Dienstag, waren vier [...]
</code></pre>

<p>making CoreNLP largely unusable for German NLP. The dot simply marks this as ordinal numbers, i.e. this is the German way of writing ""11th September 2001"". You will also see the Notation ""11. 9. 2001"" in German frequently, CoreNLP thinks <code>9.</code> is another sentence, and also ""11. Sept. 2001""</p>

<p>Is there any hook (e.g. overriding some method) that would allow me to define patterns to re-join incorrectly split sentences with own rules? For example, <code>[123]?\d\. 1?\d\.</code> would be typical for dates, and there are not many months...</p>
",Multilingual Language Processing & Language Identification,fix corenlp sentence splitting european date stanford corenlp doe work many common european date format c f date time notation europe wikipedia know date parsing mess particular crazy mm dd yyyy format u us nevertheless corenlp unable process even basic european date format split aggressively dot however date information often presented e g die terroranschl ge september einem dienstag waren vier koordinierte flugzeugentf hrungen mit anschlie enden selbstmordattentaten wikipedia whatever try corenlp split making corenlp largely unusable german nlp dot simply mark ordinal number e german way writing th september also see notation german frequently corenlp think another sentence also sept hook e g overriding method would allow define pattern join incorrectly split sentence rule example would typical date many month
How can I lemmatize words in languages that do not use the English alphabet?,"<p>By non-English alphabet I mean languages like Urdu, Hindi etc. 
Can someone suggest me pathway? </p>

<p>PS: Not to be marked duplicate of <a href=""https://stackoverflow.com/questions/22144324/lemmatization-of-non-english-words"">Lemmatization of non-English words?</a>. The context here is different. I mean languages that do not use the English alphabet at all. The other question refers in general to languages that are not English.</p>
",Multilingual Language Processing & Language Identification,lemmatize word language use english alphabet non english alphabet mean language like urdu hindi etc someone suggest pathway p marked duplicate href non english word context different mean language use english alphabet question refers general language english
Detecting if text is non-English,"<p>What is the most accurate method for detecting if a text (specifically Instagram comments) are non-English? I am happy to use any high-level language, such as Python, PHP, etc.</p>

<pre><code>$ sudo pip2 install guess_language
&gt;&gt;&gt; from guess_language import guessLanguage
&gt;&gt;&gt; guessLanguage('la vita e bella')
'UNKNOWN'
&gt;&gt;&gt; guessLanguage('today is a good day')
'UNKNOWN'
&gt;&gt;&gt; guessLanguage('„Éú„Ç¶„É™„É≥„Ç∞„Éª„Éï„Ç©„Éº„Éª„Ç≥„É≠„É≥„Éê„Ç§„É≥(Â≠óÂπïÁâà)')
'ja'
</code></pre>

<p>and with </p>

<pre><code>$ sudo apt-get install php5.6-mbstring

      if(strlen($comment-&gt;text) == mb_strlen($comment-&gt;text, 'utf-8')) {
         echo '- '.$comment-&gt;text.""\n"";
    }
</code></pre>

<p>I get many things with English character which are not English:
examples:</p>

<pre><code>- Khoda be khanevadehashon sabr bede tahamol konan
- Akhey...
- Eshghi
- K
- :-)
- Ey khodaa
- ...
- @samaneaghazamani1990 vaaaaay khoda chejoori payam dadan?
- :(
- Elahiiiii
- May Allah please with them and grant higher rank of jannah salutes to the  bravehearts @taraneh_alidoosti @fanpagemostafazamani
- Elaaaahiii
- Roohetoon shad.
- :'(
- Roheshon shad!! Yadeshon gerami!!
- .:'(
- :-(
- Oooo
- Awli
</code></pre>

<p>I don't want to use something like Google Translate as I am dealing with lots of data. </p>

<p>Update:</p>

<pre><code>$ sudo pip2 install langdetect
&gt;&gt;&gt; from langdetect import detect
&gt;&gt;&gt; detect(""War doesn't show who's right, just who's left."")
'en'
&gt;&gt;&gt; detect(""today is a good day."")
'so'
&gt;&gt;&gt; detect(""la vita e bella!"")
'it'
&gt;&gt;&gt; detect(""khoobi? khoshi?"")
'so'
&gt;&gt;&gt; detect(""wow"")
'pl'
&gt;&gt;&gt; detect(""what a day"")
'en'
&gt;&gt;&gt; detect(""yay!"")
'so'
</code></pre>

<p>Does 'so' refer to unknown? I was expecting that <code>today is a good day</code> be considered as <code>en</code>!</p>
",Multilingual Language Processing & Language Identification,detecting text non english accurate method detecting text specifically instagram comment non english happy use high level language python php etc get many thing english character english example want use something like google translate dealing lot data update doe refer unknown wa expecting considered
Recognizing language of a short text?,"<p>I have a list of articles, and each article has its own title and description. Unfortunately, from the sources I am using, there is no way to know what language they are written in.</p>

<p>Furthermore, the text is not entirely written in 1 language; almost always English words are present.</p>

<p>I reckon I would need dictionary databases stored on my machine, but it feels a bit impractical. What would you suggest I do?</p>
",Multilingual Language Processing & Language Identification,recognizing language short text list article article ha title description unfortunately source using way know language written furthermore text entirely written language almost always english word present reckon would need dictionary database stored machine feel bit impractical would suggest
How to make a TF vector?,"<p>I want to make a TF vector for a word sense disambiguation project. I have some files, each for an ambiguated word which contains a Persian sentence, a tab and then an English word (which shows the sense of the ambiguated word) in each line. I extracted the 1000 most frequent words of the file and I have to make a TF vector according to this words. (The TF formula is: ùëáùêπùëñ,ùëó=log(1+ùëõùëñ,ùëó)). So the output should be a file which has 1000 columns of numbers and a column for the English word. (1001 columns).</p>

<p>For that, first I omitted stop words and punctuation. Then I wrote a function to extract 1000 most frequent words. Then a function to calculate each word's frequency in the file and then I claculated ùëáùêπùëñ,ùëó=log(1+ùëõùëñ,ùëó). (ùëõùëñ,ùëó is each word's frequency in the file), and put the result in a dictionary (""TFvauesl"" in the code). The problem is the TF function in the code.</p>

<p>1) for example if my file has 10 lines, it should return 10 lines in the output, too . But the code returns more than 10 lines. How should I correct it?</p>

<p>2) To make the TF vector I have to say if the word in each line of the file (which I put them in the witouStops list), is in the most frequent words and also is in the TFvalues dictionary, instead of that word, it puts its value which is in TFvalues dictionary. The TF function which I wrote doesn't return correct answer. How can I change it?</p>

<p>3) As I said before, the output file should have 1000 columns of numbers and a column for the English word. my data file has some sentences in each line. They have between 5 to 9 words (more or less). for the words in each line, which are in the most frequent words, the code should put its TF value which is in TFvalues dictionary. Now the question is, for the rest of that 1000 most frequent words which are not in the lines, should I write ""0""? Is this the way we make TF vector? hint: I want to use weka for classifying, so the number of each line should be equall. All of them should have 1001 columns.</p>

<p>An example of a binary arff file:
<a href=""https://www.dropbox.com/s/3ltepz1rk3ia9md/golTest.arff?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/3ltepz1rk3ia9md/golTest.arff?dl=0</a></p>

<p>data file example:
<a href=""https://www.dropbox.com/s/f4z8neslw3ht9e8/golTest.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/f4z8neslw3ht9e8/golTest.txt?dl=0</a></p>

<pre><code>from hazm import*
from collections import Counter
import collections
import math

punctuation = '!""#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~ÿåÿü¬´ÿõ¬´'

file1 = ""stopwords.txt""
file2 = ""golTest.txt""


witoutStops = []
corpuslines = []

def RemStopWords(file1, file2): 
    .
    .
    .  # do stuffs for omitting stop words and punctuations 

    #print (witoutStops)

def mostFreqWords():
    RemStopWords (file1, file2)
    with open (""TFFile.txt"", ""w"", encoding=""utf-8"") as f:
        counter = Counter()
        for line in witoutStops:
            line = line.strip().split(""\t"")
            words = line[0].split()
            counter.update(words)
        top1000 = [word[0] for word in counter.most_common(1000)]
        return top1000

wordcount = {}
TFvalues = {}
def calculateValues():
    RemStopWords (file1, file2)
    for line in witoutStops:
        line = line.strip().split(""\t"")
        words = line[0].split()
        for word in words:
            if word not in wordcount:
                wordcount[word] = 1    # calculte the frequency of each word
            else:                      # in the file
                wordcount[word] += 1       
            value = wordcount.get(word, 1) # calculte Tf = log (1+ nij)
            result = math.log(1+value)
            TFvalues[word] = result

def TF():
    RemStopWords(file1, file2)
    mostfreq = mostFreqWords()
    calculateValues()
    for line in witoutStops:
        f =open (""abi.arff"", ""a"", encoding = ""utf-8"") 
        line = line.split(""\t"")
        words = line[0].split()
        for word in words:
            for i in mostfreq:
                for k, v in TFvalues.items():
                    if word == k:
                        if any([i == word for word in words]):
                            value2 = TFvalues.get(word, 1)
                            f.write(str(value2,))
                        else:
                            f.write(""0,"")
            f.write(line[1])
TF()                
</code></pre>
",Multilingual Language Processing & Language Identification,make tf vector want make tf vector word sense disambiguation project file ambiguated word contains persian sentence tab english word show sense ambiguated word line extracted frequent word file make tf vector according word tf formula log output file ha column number column english word column first omitted stop word punctuation wrote function extract frequent word function calculate word frequency file claculated log word frequency file put result dictionary tfvauesl code problem tf function code example file ha line return line output code return line correct make tf vector say word line file put witoustops list frequent word also tfvalues dictionary instead word put value tfvalues dictionary tf function wrote return correct answer change said output file column number column english word data file ha sentence line word le word line frequent word code put tf value tfvalues dictionary question rest frequent word line write way make tf vector hint want use weka classifying number line equall column example binary arff file data file example
"Java, Stanford NLP : Extract specific speech labels from parser","<p>I recently discovered the Stanford NLP parser and it seems quite amazing. I have currently a working instance of it running in our project but facing the below mentioned 2 problems. </p>

<ol>
<li>How can I parse text and then extract only specific speech-labels from the parsed data, for example, how can I extract only <code>NNPS</code> and <code>PRP</code> from the sentence. </li>
<li>Our platform works in both English and German, so there is always a possibility that the text is either in English or German. How can I accommodate this scenario. Thank you.</li>
</ol>

<p>Code :</p>

<pre><code> private final String PCG_MODEL = ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"";

    private final TokenizerFactory&lt;CoreLabel&gt; tokenizerFactory = PTBTokenizer.factory(new CoreLabelTokenFactory(), ""invertible=true"");

 public void testParser() {
  LexicalizedParser lp = LexicalizedParser.loadModel(PCG_MODEL);
        String sent=""Complete Howto guide to install EC2 Linux server in Amazon Web services cloud."";
        Tree parse;
        parse = lp.parse(sent);

        List taggedWords = parse.taggedYield();
        System.out.println(taggedWords);
}
</code></pre>

<p>The above example works, but as you can see I am loading the English data. Thank you. </p>
",Multilingual Language Processing & Language Identification,java stanford nlp extract specific speech label parser recently discovered stanford nlp parser seems quite amazing currently working instance running project facing mentioned problem parse text extract specific speech label parsed data example extract sentence platform work english german always possibility text either english german accommodate scenario thank code example work see loading english data thank
Python regex: tokenizing English contractions,"<p>I am trying to parse strings in such a way as to separate out all word components, even those that have been contracted.  For example the tokenization of  ""shouldn't"" would be [""should"", ""n't""].</p>

<p>The nltk module does not seem to be up to the task however as:</p>

<blockquote>
  <p>""I wouldn't've done that.""</p>
</blockquote>

<p>tokenizes as:</p>

<blockquote>
  <p>['I', ""wouldn't"", ""'ve"", 'done', 'that', '.']</p>
</blockquote>

<p>where the desired tokenization of ""wouldn't've"" was: ['would', ""n't"", ""'ve""]</p>

<p>After examining common English contractions, I am trying to write a regex to do the job but I am having a hard time figuring out how to match ""'ve"" only once.  For example, the following tokens can all terminate a contraction:</p>

<blockquote>
  <p>n't, 've, 'd, 'll, 's, 'm, 're</p>
</blockquote>

<p>But the token ""'ve"" can also follow other contractions such as:</p>

<blockquote>
  <p>'d've, n't've, and (conceivably) 'll've</p>
</blockquote>

<p>At the moment, I am trying to wrangle this regex:</p>

<blockquote>
  <p>\b[a-zA-Z]+(?:('d|'ll|n't)('ve)?)|('s|'m|'re|'ve)\b</p>
</blockquote>

<p>However, this pattern also matches the badly formed:</p>

<blockquote>
  <p>""wouldn't've've""</p>
</blockquote>

<p>It seems the problem is that the third apostrophe qualifies as a word boundary so that the final ""'ve"" token matches the whole regex.</p>

<p>I have been unable to think of a way to differentiate a word boundary from an apostrophe and, failing that, I am open to advice for alternative strategies.</p>

<p>Also, I am curious if there is any way to include the word boundary special character in a character class.  According to the Python documentation, \b in a character class matches a backspace and there doesn't seem to be a way around this.</p>

<p>EDIT:</p>

<p>Here's the output:</p>

<pre><code>&gt;&gt;&gt;pattern = re.compile(r""\b[a-zA-Z]+(?:('d|'ll|n't)('ve)?)|('s|'m|'re|'ve)\b"")
&gt;&gt;&gt;matches = pattern.findall(""She'll wish she hadn't've done that."")
&gt;&gt;&gt;print matches
[(""'ll"", '', ''), (""n't"", ""'ve"", ''), ('', '', ""'ve"")]
</code></pre>

<p>I can't figure out the third match.  In particular, I just realized that if the third apostrophe were matching the leading \b, then I don't know what would be matching the character class [a-zA-Z]+.</p>
",Multilingual Language Processing & Language Identification,python regex tokenizing english contraction trying parse string way separate word component even contracted example tokenization would n nltk module doe seem task however done tokenizes done desired tokenization wa would n examining common english contraction trying write regex job hard time figuring match example following token terminate contraction n token also follow contraction n conceivably moment trying wrangle regex b za z n b however pattern also match badly formed seems problem third apostrophe qualifies word boundary final token match whole regex unable think way differentiate word boundary apostrophe failing open advice alternative strategy also curious way include word boundary special character character class according python documentation b character class match backspace seem way around edit output figure third match particular realized third apostrophe matching leading b know would matching character class za z
How to change default value of ANNIE resources in GATE from java code?,"<p>In GATE default values for ANNIE is set during initialization but sometimes based on requirement it has to be changed.</p>

<p><strong>My Requirement</strong> : I want to extract English sentences without considering ""nextline character"" but ""full stop"" which gives correct sentences. For that I need to change the default value of <em>transducerURL</em> in SentenceSplitter in ANNIE. 
This can be done in two ways :</p>

<ol>
<li><p>Using <em>ANNIE_with_defaults.gapp</em> - changing initparams value in Sentencesplitter and accessing from java </p>

<pre><code>    Gate.setGateHome(new File(Configuration.GATE_HOME));                        
    Gate.init();
    // Load ANNIE with defaults from the plug-ins folder
    File pluginsHome = Gate.getPluginsHome();
    File anniePlugin = new File(pluginsHome, ANNIEConstants.PLUGIN_DIR);
    File annieGapp = new File(anniePlugin, ANNIEConstants.DEFAULT_FILE);
    annieController = (CorpusController) PersistenceManager.loadObjectFromFile(annieGapp);
</code></pre></li>
</ol>

<p><strong>I am not being able to find where to change its value in gapp file.</strong></p>

<ol start=""2"">
<li><p>After initialising GATE, access ResourceData using CreoleRegister and change the default value of parameter <em>transducerURL</em> as shown below</p>

<pre><code>String resourceClassName = ""gate.creole.splitter.SentenceSplitter"";
        ResourceData resData = Gate.getCreoleRegister().get(resourceClassName);
        //System.out.println(resData.getParameterList());
        ParameterList params = resData.getParameterList();
        List&lt;List&lt;Parameter&gt;&gt; param =  params.getInitimeParameters();
        System.out.println(param);
        //System.out.println(param.get(0));
        for(List&lt;Parameter&gt; plist : param)
        {
            for(Parameter pm : plist)
            {
                System.out.println(""Name : ""+pm.getName()+"", ""+pm.getDefaultValue());
            }
        }
</code></pre></li>
</ol>

<p><strong>But Parameter does not have setDefaultvalue method.</strong></p>

<p>How to fix this?</p>

<p><strong>Any kind of help is welcome.</strong></p>
",Multilingual Language Processing & Language Identification,change default value annie resource gate java code gate default value annie set initialization sometimes based requirement ha changed requirement want extract english sentence without considering nextline character full stop give correct sentence need change default value transducerurl sentencesplitter annie done two way using annie default gapp changing initparams value sentencesplitter accessing java able find change value gapp file initialising gate access resourcedata using creoleregister change default value parameter transducerurl shown parameter doe setdefaultvalue method fix kind help welcome
What Is the Difference Between POS Tagging and Shallow Parsing?,"<p>I'm currently taking a Natural Language Processing course at my University and still confused with some basic concept. I get the definition of POS Tagging from the <a href=""http://www-nlp.stanford.edu/fsnlp/"" rel=""noreferrer"">Foundations of Statistical Natural Language Processing</a> book:</p>

<blockquote>
  <p>Tagging is the task of labeling (or tagging) each word in a sentence
  with its appropriate part of speech. We decide whether each word is a
  noun, verb, adjective, or whatever.</p>
</blockquote>

<p>But I can't find a definition of Shallow Parsing in the book since it also describe shallow parsing as one of the utilities of POS Tagging. So I began to search the web and found no direct explanation of shallow parsing, but in <a href=""http://en.wikipedia.org/wiki/Shallow_parsing"" rel=""noreferrer"">Wikipedia</a>:</p>

<blockquote>
  <p>Shallow parsing (also chunking, ""light parsing"") is an analysis of a sentence which identifies the constituents (noun groups, verbs, verb groups, etc.), but does not specify their internal structure, nor their role in the main sentence.</p>
</blockquote>

<p>I frankly don't see the difference, but it may be because of my English or just me not understanding simple basic concept. Can anyone please explain the difference between shallow parsing and POS Tagging? Is shallow parsing often also called Shallow Semantic Parsing?</p>

<p>Thanks before.</p>
",Multilingual Language Processing & Language Identification,difference po tagging shallow parsing currently taking natural language processing course university still confused basic concept get definition po tagging foundation statistical natural language processing book tagging task labeling tagging word sentence appropriate part speech decide whether word noun verb adjective whatever find definition shallow parsing book since also describe shallow parsing one utility po tagging began search web found direct explanation shallow parsing wikipedia shallow parsing also chunking light parsing analysis sentence identifies constituent noun group verb verb group etc doe specify internal structure role main sentence frankly see difference may english understanding simple basic concept anyone please explain difference shallow parsing po tagging shallow parsing often also called shallow semantic parsing thanks
Is TextBlob scalable?,"<p>I've recently come across TextBlob, which seems like a very neat Natural Language Processing library. <a href=""http://textblob.readthedocs.org/en/dev/quickstart.html"" rel=""nofollow"">http://textblob.readthedocs.org/en/dev/quickstart.html</a></p>

<p>However, I am concerned because it seems to act as a regular Python string. I have a very large text file, and for example, calling <code>blob.correct()</code> for a very modest text amount takes a very long time. Any feedback on the scale of TextBlob or any alternatives for natural language parsing?</p>
",Multilingual Language Processing & Language Identification,textblob scalable recently come across textblob seems like neat natural language processing library however concerned seems act regular python string large text file example calling modest text amount take long time feedback scale textblob alternative natural language parsing
How Can I Add More Languages to Stopwords in NLTK?,"<p>I'm using NLTK with stopwords to detect the language of a document using the method described by Alejandro Nolla at <a href=""http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/"" rel=""noreferrer"">http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/</a> , and it works reasonably well.</p>

<p>I'm also working with some additional languages not included in the NLTK stopwords package, such as Czech and Romanian, and they get false matches as other languages. These are the languages in stopwords:</p>

<p>['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']</p>

<p>How can I expand the list of languages supported by NLTK? Are there other stopword lists available that I can add? Is there a documented method I can use to create an add my own stopword lists?</p>
",Multilingual Language Processing & Language Identification,add language stopwords nltk using nltk stopwords detect language document using method described alejandro nolla work reasonably well also working additional language included nltk stopwords package czech romanian get false match language language stopwords danish dutch english finnish french german hungarian italian norwegian portuguese russian spanish swedish turkish expand list language supported nltk stopword list available add documented method use create add stopword list
Phrase extraction algorithm for statistical machine translation,"<p>I have written the following code with the phrase extraction algorithm for SMT.</p>

<p><a href=""https://github.com/alvations/nltk/blob/develop/nltk/align/phrase_based.py"" rel=""noreferrer"">GitHub</a></p>

<pre><code># -*- coding: utf-8 -*-

def phrase_extraction(srctext, trgtext, alignment):
    """"""
    Phrase extraction algorithm. 
    """"""
    def extract(f_start, f_end, e_start, e_end):
        phrases = set()
        # return { } if f end == 0
        if f_end == 0:
            return
        # for all (e,f) ‚àà A do
        for e,f in alignment:
            # return { } if e &lt; e start or e &gt; e end
            if e &lt; e_start or e &gt; e_end:        
                return

        fs = f_start
        # repeat-
        while True:
            fe = f_end
            # repeat-
            while True:
                # add phrase pair ( e start .. e end , f s .. f e ) to set E
                trg_phrase = "" "".join(trgtext[i] for i in range(fs,fe))
                src_phrase = "" "".join(srctext[i] for i in range(e_start,e_end))
                phrases.add(""\t"".join([src_phrase, trg_phrase]))
                fe+=1 # fe++
                # -until fe aligned
                if fe in f_aligned or fe &gt; trglen:
                    break
            fs-=1 # fe--
            # -until fs aligned
            if fs in f_aligned or fs &lt; 0:
                break
        return phrases

    # Calculate no. of tokens in source and target texts.
    srctext = srctext.split()
    trgtext = trgtext.split()
    srclen = len(srctext)
    trglen = len(trgtext)
    # Keeps an index of which source/target words are aligned.
    e_aligned = [i for i,_ in alignment]
    f_aligned = [j for _,j in alignment] 

    bp = set() # set of phrase pairs BP
    # for e start = 1 ... length(e) do
    for e_start in range(srclen):
        # for e end = e start ... length(e) do       
        for e_end in range(e_start, srclen):
            # // find the minimally matching foreign phrase
            # (f start , f end ) = ( length(f), 0 )
            f_start, f_end = trglen, 0
            # for all (e,f) ‚àà A do
            for e,f in alignment:
                # if e start ‚â§ e ‚â§ e end then
                if e_start &lt;= e &lt;= e_end:
                    f_start = min(f, f_start)
                    f_end = max(f, f_end)
            # add extract (f start , f end , e start , e end ) to set BP
            phrases = extract(f_start, f_end, e_start, e_end)
            if phrases:
                bp.update(phrases)
    return bp

srctext = ""michael assumes that he will stay in the house""
trgtext = ""michael geht davon aus , dass er im haus bleibt""
alignment = [(0,0), (1,1), (1,2), (1,3), (2,5), (3,6), (4,9), (5,9), (6,7), (7,7), (8,8)]

phrases = phrase_extraction(srctext, trgtext, alignment)

for i in phrases:
    print i
</code></pre>

<p>The phrase extraction algorithm from Philip Koehn's <em>Statistical Machine Translation</em> book, page 133 is as such:</p>

<p><img src=""https://i.sstatic.net/0j556.png"" alt=""enter image description here""></p>

<p>And the desired output should be:</p>

<p><img src=""https://i.sstatic.net/qfR2m.png"" alt=""enter image description here""></p>

<p>However with my code, I am only able to get these output:</p>

<blockquote>
  <p>michael assumes that he will stay in the - michael geht davon aus ,
  dass er im haus</p>
  
  <p>michael assumes that he will stay in the - michael geht davon aus ,
  dass er im haus bleibt</p>
</blockquote>

<p><strong>Does anyone spot what is wrong with my implementation?</strong></p>

<p>The code does extract phrases but it's not the complete desired output as shown with the translation table above:</p>

<p><img src=""https://i.sstatic.net/E21Xq.png"" alt=""enter image description here""></p>
",Multilingual Language Processing & Language Identification,phrase extraction algorithm statistical machine translation written following code phrase extraction algorithm smt github phrase extraction algorithm philip koehn statistical machine translation book page desired output however code able get output michael assumes stay michael geht davon au das er im haus michael assumes stay michael geht davon au das er im haus bleibt doe anyone spot wrong implementation code doe extract phrase complete desired output shown translation table
Learning word alignment from nltk,"<p>I have a parallel corpus for english-german. Is there a way to extract word alignment table from this corpus using nltk? I don't know if nltk.align is supposed to do this. I am unable to figure out from the documentation.</p>
",Multilingual Language Processing & Language Identification,learning word alignment nltk parallel corpus english german way extract word alignment table corpus using nltk know nltk align supposed unable figure documentation
N-grams: Explanation + 2 applications,"<p>I want to implement some applications with n-grams (preferably in PHP). </p>

<hr>

<p>Which type of n-grams is more adequate for most purposes? A word level or a character level n-gram? How could you implement an n-gram-tokenizer in PHP?</p>

<hr>

<p>First, I would like to know what N-grams exactly are. Is this correct? It's how I understand n-grams:</p>

<p>Sentence: ""I live in NY.""</p>

<p>word level bigrams (2 for n): ""# I', ""I live"", ""live in"", ""in NY"", 'NY #'</p>

<p>character level bigrams (2 for n): ""#I"", ""I#"", ""#l"", ""li"", ""iv"", ""ve"", ""e#"", ""#i"", ""in"", ""n#"", ""#N"", ""NY"", ""Y#""</p>

<p>When you have this array of n-gram-parts, you drop the duplicate ones and add a counter for each part giving the frequency:</p>

<p>word level bigrams: [1, 1, 1, 1, 1]</p>

<p>character level bigrams: [2, 1, 1, ...]</p>

<p>Is this correct?</p>

<hr>

<p>Furthermore, I would like to learn more about what you can do with n-grams:</p>

<ul>
<li>How can I identify the language of a text using n-grams?</li>
<li>Is it possible to do machine translation using n-grams even if you don't have a bilingual corpus?</li>
<li>How can I build a spam filter (spam, ham)? Combine n-grams with a Bayesian filter?</li>
<li>How can I do topic spotting? For example: Is a text about basketball or dogs? My approach (do the following with a Wikipedia article for ""dogs"" and ""basketball""): build the n-gram vectors for both documents, normalize them, calculate Manhattan/Euclidian distance, the closer the result is to 1 the higher is the similarity</li>
</ul>

<p>What do you think about my application approaches, especially the last one?</p>

<hr>

<p>I hope you can help me. Thanks in advance!</p>
",Multilingual Language Processing & Language Identification,n gram explanation application want implement application n gram preferably php type n gram adequate purpose word level character level n gram could implement n gram tokenizer php first would like know n gram exactly correct understand n gram sentence live ny word level bigram n live live ny ny character level bigram n l li iv e n n ny array n gram part drop duplicate one add counter part giving frequency word level bigram character level bigram correct furthermore would like learn n gram identify language text using n gram possible machine translation using n gram even bilingual corpus build spam filter spam ham combine n gram bayesian filter topic spotting example text basketball dog approach following wikipedia article dog basketball build n gram vector document normalize calculate manhattan euclidian distance closer result higher similarity think application approach especially last one hope help thanks advance
What does ($pre =~ /\./ &amp;&amp; $pre =~ /\p{IsAlpha}/) mean in the Moses Tokenizer?,"<p><a href=""https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl#L333"" rel=""nofollow noreferrer"">Moses Tokenizer</a> is the tokenizer widely used in machine translation and natural language processing experiments.</p>

<p>There is a line of regex that checks for:</p>

<pre><code>if (($pre =~ /\./ &amp;&amp; $pre =~ /\p{IsAlpha}/) || 
   ($NONBREAKING_PREFIX{$pre} &amp;&amp; $NONBREAKING_PREFIX{$pre}==1) || 
   ($i&lt;scalar(@words)-1 &amp;&amp; ($words[$i+1] =~ /^[\p{IsLower}]/)))
</code></pre>

<p>Please correct me if I'm wrong, the 2nd and 3rd conditions are to check</p>

<ul>
<li>whether the prefix is in a list of nonbreaking prefixes </li>
<li>whether the word is not the last token and there is still a lowercased token as the next word.</li>
</ul>

<p>The question is on the first condition where it checks for:</p>

<pre><code>($pre =~ /\./ &amp;&amp; $pre =~ /\p{IsAlpha}/)
</code></pre>

<ol>
<li><p>Is the <code>$pre =~ /\./</code> checking whether the prefix is a single fullstop?</p></li>
<li><p>And is <code>$pre =~ /\p{IsAlpha}/</code> checking whether the prefix is an alpha from the list of alphabet in the <a href=""http://perldoc.perl.org/perluniprops.html"" rel=""nofollow noreferrer"">perluniprop</a>?</p></li>
<li><p>One related question is whether the fullstop is already inside the perluniprop alphabet? If so, wouldn't this condition never be true?</p></li>
</ol>
",Multilingual Language Processing & Language Identification,doe pre pre p isalpha mean moses tokenizer moses tokenizer tokenizer widely used machine translation natural language processing experiment line regex check please correct wrong nd rd condition check whether prefix list nonbreaking prefix whether word last token still lowercased token next word question first condition check checking whether prefix single fullstop checking whether prefix alpha list alphabet perluniprop one related question whether fullstop already inside perluniprop alphabet condition never true
Part-of-speech without Python,"<p>I am trying to do tagging of a <em>french text</em>, but <code>TreeTagger</code> needs <code>Python</code> which is impossible to install on my PC at work. For security reasons, it is impossible to install other programs (only <code>R</code>).</p>

<p>Is it possible to use <code>R</code> code for tagging which does not require neither <code>java</code> nor <code>Python</code>?</p>
",Multilingual Language Processing & Language Identification,part speech without python trying tagging french text need impossible install pc work security reason impossible install program possible use code tagging doe require neither
Input format for proto-spaCy 500 line of Python English parser,"<p>I am seeking to understand the <a href=""https://gist.github.com/syllog1sm/10343947"" rel=""nofollow noreferrer"">A simple Python dependency parser</a> written by Matthew Honnibal and described in his blog post <a href=""https://explosion.ai/blog/parsing-english-in-python"" rel=""nofollow noreferrer"">Parsing English in 500 Lines of Python</a>, but I am unclear as to the format of the input files required.</p>
<p>The arguments to the <code>main()</code> function, as well as a directory to store the model, are three input files:</p>
<ol>
<li><code>train_loc</code> is a training set of dependency parsed sentences</li>
<li><code>heldout_in</code> is the held-out test set of POS-tagged sentences</li>
<li><code>heldout_gold</code> is the same held-out sentences dependency parsed</li>
</ol>
<p>From reading the source code it appears that <code>train_loc</code> and <code>heldout_gold</code> are in a CONLL format, although I am unsure which one. I am also unsure what is the format of the POS tagged file <code>heldout_in</code>.</p>
<p>I have downloaded the Penn treebank datasets &quot;Dependency Parsed Treebank&quot; and &quot;Penn Treebank Sample&quot; from the NLTK Corpora page for the dependency parsed and POS-tagged inputs respectively, but neither of these is accepted as input by the parser, producing tuple unpacking errors.</p>
<p>I am quite happy to work on understanding the code and converting the files I have downloaded into a format that the parser will accept, but wondered if the format was widely known and if files of the correct format are already available.</p>
",Multilingual Language Processing & Language Identification,input format proto spacy line python english parser seeking understand simple python dependency parser written matthew honnibal described blog post parsing english line python unclear format input file required argument function well directory store model three input file training set dependency parsed sentence held test set po tagged sentence held sentence dependency parsed reading source code appears conll format although unsure one also unsure format po tagged file downloaded penn treebank datasets dependency parsed treebank penn treebank sample nltk corpus page dependency parsed po tagged input respectively neither accepted input parser producing tuple unpacking error quite happy work understanding code converting file downloaded format parser accept wondered format wa widely known file correct format already available
Stanford NLP sentiment ambiguous result,"<p>I am using <strong>Stanford NLP v3.6</strong> (<strong>JAVA</strong>) to calculate <strong>sentiment</strong> of <strong>English</strong>  sentences.</p>

<p>Stanford NLP calculates the polarity of the sentence from 0 to 4.</p>

<ul>
<li>0 very negative</li>
<li>1 negative</li>
<li>2 neutral</li>
<li>3 positive</li>
<li>4 very positive </li>
</ul>

<p>I run some very simple test cases but got very strange result.</p>

<p>Example : </p>

<ol>
<li>Text = Jhon is good person, Sentiment = 3 (i.e <strong>positive</strong>)</li>
<li>Text = David is good person, Sentiment = 2 (i.e <strong>neutral</strong>)</li>
</ol>

<p>In above example, the sentences are same, other that the name <code>David</code>, <code>Jhon</code>, but sentiment values are different. <strong>Isn't this ambiguity</strong>?</p>

<p>I used this Java code for calculating sentiment: </p>

<pre><code> public static float calSentiment(String text) {

            // pipeline must get initialized before proceeding further
            Properties props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit,   parse, sentiment"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

            int mainSentiment = 0;
            if (text != null &amp;&amp; text.length() &gt; 0) {
                int longest = 0;
                Annotation annotation = pipeline.process(text);

                for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
                    Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
                    int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                    String partText = sentence.toString();

                    if (partText.length() &gt; longest) {
                        mainSentiment = sentiment;
                        longest = partText.length();
                    }
                }
            }
            if (mainSentiment &gt; 4 || mainSentiment &lt; 0) {
                return -9999;
            }
            return mainSentiment;

        }
</code></pre>

<p><strong>Is I am missing something in java code</strong>?</p>

<p>I also got negative sentiment (i.e less than 2) when the sentence was positive and vice-versa.</p>

<p>Thanks.</p>

<p>Below are results that I got with simple English sentences:</p>

<pre><code>Sentence: Tendulkar is a great batsman
Sentiment: 3
Sentence: David is a great batsman
Sentiment: 3
Sentence: Tendulkar is not a great batsman
Sentiment: 1
Sentence: David is not a great batsman
Sentiment: 2
Sentence: Shyam is not a great batsman
Sentiment: 1
Sentence: Dhoni loves playing football
Sentiment: 3
Sentence: John, Julia loves playing football
Sentiment: 3
Sentence: Drake loves playing football
Sentiment: 3
Sentence: David loves playing football
Sentiment: 2
Sentence: Virat is a good boy
Sentiment: 2
Sentence: David is a good boy
Sentiment: 2
Sentence: Virat is not a good boy
Sentiment: 1
Sentence: David is not a good boy
Sentiment: 2
Sentence: I love every moment of life
Sentiment: 3
Sentence: I hate every moment of life
Sentiment: 2
Sentence: I like dancing and listening to music
Sentiment: 3
Sentence: Messi does not like to play cricket
Sentiment: 1
Sentence: This was the worst movie I have ever seen
Sentiment: 0
Sentence: I really appreciated the movie
Sentiment: 1
Sentence: I really appreciate the movie
Sentiment: 3
Sentence: Varun talks in a condescending way
Sentiment: 2
Sentence: Ram is angry he did not win the tournament
Sentiment: 1
Sentence: Today's dinner was awful
Sentiment: 1
Sentence: Johny is always complaining
Sentiment: 3
Sentence: Modi's demonetisation has been very controversial and confusing
Sentiment: 1
Sentence: People are left devastated by floods and droughts
Sentiment: 2
Sentence: Chahal did a fantastic job by getting the 6 wickets
Sentiment: 3
Sentence: England played terribly bad
Sentiment: 1
Sentence: Rahul Gandhi is a funny man
Sentiment: 3
Sentence: Always be grateful to those who are generous towards you
Sentiment: 3
Sentence: A friend in need is a friend indeed
Sentiment: 3
Sentence: Mary is a jubilant girl
Sentiment: 2
Sentence: There is so much of love and hatred in this world
Sentiment: 3
Sentence: Always be positive
Sentiment: 3
Sentence: Always be negative
Sentiment: 1
Sentence: Never be negative
Sentiment: 1
Sentence: Stop complaining and start doing something
Sentiment: 2
Sentence: He is a awesome thief
Sentiment: 3
Sentence: Ram did unbelievably well in this year's exams
Sentiment: 2
Sentence: This product is well designed and easy to use
Sentiment: 3
</code></pre>
",Multilingual Language Processing & Language Identification,stanford nlp sentiment ambiguous result using stanford nlp v java calculate sentiment english sentence stanford nlp calculates polarity sentence negative negative neutral positive positive run simple test case got strange result example text jhon good person sentiment e positive text david good person sentiment e neutral example sentence name sentiment value different ambiguity used java code calculating sentiment missing something java code also got negative sentiment e le sentence wa positive vice versa thanks result got simple english sentence
English verbs processing ending with &#39;e&#39;,"<p>I am implementing few string replacers, with these conversions in mind</p>

<pre><code>'thou sittest' ‚Üí 'you sit'
'thou walkest' ‚Üí 'you walk'
'thou liest' ‚Üí 'you lie'
'thou risest' ‚Üí 'you rise'
</code></pre>

<p>If I keep it naive it is possible to use regex for this case to find &amp; replace, like <code>thou [a-z]+est</code></p>

<p>But the trouble comes in English verbs that end with <code>e</code> because based on the context I need to trim the <code>est</code> in some &amp; trim just <code>st</code> in the rest</p>

<p>What is the quick-dirty solution to achieve this? </p>
",Multilingual Language Processing & Language Identification,english verb processing ending e implementing string replacers conversion mind keep naive possible use regex case find replace like trouble come english verb end based context need trim trim rest quick dirty solution achieve
Extract relevent keywords from job advertisements,"<p>My friend asked me if I could write a program capable of identifying relevant keywords from job adverts knowing 3 variables: Industry, job title and the job posting text (example below).</p>

<p>The problem we are trying to address, from a job seeker's point of view, evolves around having the correct keywords in your resume for each job application hereby increasing your chances of getting shortlisted for an interview. This is especially important when the first stage screening is done by bots scanning for keywords.</p>

<p>Initially I was considering a relational database containing all industries, all job titles and their related keywords. This however is an enormous task and the data in progressive fields like information and bio technology would quickly become stale.</p>

<p>It seems machine learning and natural language processing is unavoidable.</p>

<p>Consider below job advert for a bank seeking a teller:</p>

<blockquote>
  <p>Are you an experienced Bank Teller seeking that perfect work life
  balance? If you‚Äôre looking for Casual Hours and have an absolute
  passion for customer service then this is the role for you! </p>
  
  <p>Our client services Queensland Public Servants (particularly
  Queensland Police); and is currently seeking a Bank Teller to join
  their Brisbane CBD team to start ASAP. </p>
  
  <p>The successful candidate will be required to work from 9:30am to
  2:30pm, Monday to Friday therefore 25 hours per week. Based on
  experience the successful candidate will be paid (approximately) $25 -
  $27 + superannuation per hour.</p>
  
  <p>This position is casual/temporary with the potential to for a
  permanent placement (based on performance/length of assignment etc.). </p>
  
  <p>DUTIES &amp; RESPONSIBILITIES: </p>
  
  <p>As a Bank Teller your will be required to:</p>
  
  <p>Attend to customers in a exceptional professional and efficient
  manner; Processing basic transactions such as deposits and
  withdrawals; Complete complex transactions such as loans and
  mortgages; Pass referrals onto sales team (NO SALES); Large amounts of
  cash handling; and Ensuring high attention to detail is at the top of
  your list! SKILLS &amp; EXPERIENCED:</p>
  
  <p>The successful candidate will have the following:</p>
  
  <p>Previous teller experience (within last 5 years) IDEAL; Previous
  customer service experience (within finance) IDEAL; Ability to work in
  a fast paced and time pressured environment; Excellent presentation
  and attitude; Exceptional attention to detail; Ability to quickly
  ‚Äòmaster‚Äô multiple software packages; and Strong time management skills
  and ability to work autonomously. If you boast to have fantastic
  customer service skills, a professional manner, and preferrably teller
  experience we would LOVE to hear from you!</p>
</blockquote>

<p>If I was the hiring manager (or a bot) I would probably look for these keywords in the resume:</p>

<blockquote>
  <p>teller, transactions, deposits, withdrawals, loans, mortgages, customer
  service, time management</p>
</blockquote>

<p>How would you attack this problem?</p>
",Multilingual Language Processing & Language Identification,extract relevent keywords job advertisement friend asked could write program capable identifying relevant keywords job advert knowing variable industry job title job posting text example problem trying address job seeker point view evolves around correct keywords resume job application hereby increasing chance getting shortlisted interview especially important first stage screening done bot scanning keywords initially wa considering relational database containing industry job title related keywords however enormous task data progressive field like information bio technology would quickly become stale seems machine learning natural language processing unavoidable consider job advert bank seeking teller experienced bank teller seeking perfect work life balance looking casual hour absolute passion customer service role client service queensland public servant particularly queensland police currently seeking bank teller join brisbane cbd team start asap successful candidate required work pm monday friday therefore hour per week based experience successful candidate paid approximately superannuation per hour position casual temporary potential permanent placement based performance length assignment etc duty responsibility bank teller required attend customer exceptional professional efficient manner processing basic transaction deposit withdrawal complete complex transaction loan mortgage pas referral onto sale team sale large amount cash handling ensuring high attention detail top list skill experienced successful candidate following previous teller experience within last year ideal previous customer service experience within finance ideal ability work fast paced time pressured environment excellent presentation attitude exceptional attention detail ability quickly master multiple software package strong time management skill ability work autonomously boast fantastic customer service skill professional manner preferrably teller experience would love hear wa hiring manager bot would probably look keywords resume teller transaction deposit withdrawal loan mortgage customer service time management would attack problem
NLTK stemming does not pass a simple case,"<p>I am new to NLTK, and I'm using stemmer function on stemming cases.</p>

<p>I have a simple example sentence to process, which is: ""Turn on the lightin."" I want to see if NLTK stemmer could help me filter out the typo ""lightin"". I've tested stemmer with ""lighting"", and snowBall stemmer can return the correct word ""light"" for me, but snowBall stemmer returns ""lightin"" in my test. </p>

<p>My stemming process is very trivial:</p>

<pre><code>tokens = ""Turn on the lightin""
for token in tokens:
    print(""SnowBall Lemmatizer: ""+snowBallStemmer.stem(token))
</code></pre>

<p>According to NTLK's doc, snowBallStemmer could be used to stem English. I want to know why snowBallStemmer failed to stem ""lightin"" and what could I do to fix this.</p>
",Multilingual Language Processing & Language Identification,nltk stemming doe pas simple case new nltk using stemmer function stemming case simple example sentence process turn lightin want see nltk stemmer could help filter typo lightin tested stemmer lighting snowball stemmer return correct word light snowball stemmer return lightin test stemming process trivial according ntlk doc snowballstemmer could used stem english want know snowballstemmer failed stem lightin could fix
"How to detect standard language constructions, like negations or questions?","<p>There are some words, phrases and language constructions which should be available without any additional training or definition. At least, I was expecting to have that from NLP.</p>

<p>For example:  </p>

<p>How to detect negations?<br>
- no, I don't need gift card<br>
- no gift card<br>
- gift card is not required  </p>

<p>How to detect that the sentence is question and what type?<br>
- How many colors do you have?<br>
- When will you call me?</p>

<p>Do I need to cover all that with my custom entities?</p>
",Multilingual Language Processing & Language Identification,detect standard language construction like negation question word phrase language construction available without additional training definition least wa expecting nlp example detect negation need gift card gift card gift card required detect sentence question type many color call need cover custom entity
How to prepare data for weka in word sense disambiguation,"<p>I want to use weka for word sense diasambiguation. I prepared some files containing a Persian sentence, a tab, a Persian word, a tab and then an English word. they are in notepad++ in txt format. Now how should I use these files for weka? How should I change them? </p>

<p>The sample file:
<a href=""https://www.dropbox.com/s/o7wtvrvkiir80la/F.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/o7wtvrvkiir80la/F.txt?dl=0</a></p>
",Multilingual Language Processing & Language Identification,prepare data weka word sense disambiguation want use weka word sense diasambiguation prepared file containing persian sentence tab persian word tab english word notepad txt format use file weka change sample file
toString methode for class inherrited from arrayList,"<p>I am doing some Natural Language Processing and therefore implemented a Sentence class is basically an ArrayList with some meta-information. I would like to write overwrite its toString() method, which simply pastes the Strings separated by spaces. I got it working by using the inherited get method, but I was wondering if it would be possible and better (in terms of efficiency and coding praxis) to access the fields directly.</p>

<p>Here is a simplified version of my class:</p>

<pre><code>public class Sentence extends ArrayList&lt;String&gt; {
    int sentimentScore;
    String Speaker;

    @Override
    public String toString(){
        StringBuilder sb = new StringBuilder();
        for(int i =0; i &lt; super.size(); i++){
            sb.append(super.get(i));
            sb.append("" "");
        }
        return sb.toString();
    }
</code></pre>
",Multilingual Language Processing & Language Identification,tostring methode class inherrited arraylist natural language processing therefore implemented sentence class basically arraylist meta information would like write overwrite tostring method simply paste string separated space got working using inherited get method wa wondering would possible better term efficiency coding praxis access field directly simplified version class
How to read the cedict (a space separated file) with regex groups?,"<p><a href=""https://www.mdbg.net/chindict/chindict.php?page=cc-cedict"" rel=""nofollow noreferrer"">CEDICT</a> is a resource for Chinese text analysis</p>

<p>The file plaintext file looks like this:</p>

<pre><code># CC-CEDICT
# Community maintained free Chinese-English dictionary.
# 
# Published by MDBG
% % [pa1] /percent (Tw)/
21‰∏âÈ´îÁ∂úÂêàÁóá 21‰∏â‰ΩìÁªºÂêàÁóá [er4 shi2 yi1 san1 ti3 zong1 he2 zheng4] /trisomy/Down's syndrome/
3C 3C [san1 C] /abbr. for computers, communications, and consumer electronics/China Compulsory Certificate (CCC)/
3P 3P [san1 P] /(slang) threesome/
A A [A] /(slang) (Tw) to steal/
</code></pre>

<p>There are 4 columns to the files and they are separated by spaces. Any spaces after the 4th is considered as one. Lines that starts with <code>#</code> needs to be skipped. </p>

<p>E.g. for the line: </p>

<blockquote>
  <p>3C 3C [san1 C] /abbr. for computers, communications, and consumer electronics/China Compulsory Certificate (CCC)/</p>
</blockquote>

<p>The content in the columns would be </p>

<ul>
<li>3C</li>
<li>3C</li>
<li>[san1 C]</li>
<li>/abbr. for computers, communications, and consumer electronics/China Compulsory Certificate (CCC)/</li>
</ul>

<p>Currently to read the file I've been tried using a mix of <code>str.split</code> and <code>re.findall</code> and skipping lines by <code>str.startswith()</code>, i.e.:</p>

<pre><code>import re
from collections import namedtuple


DictEntry = namedtuple('Dictionary', 'traditional simplified pinyin glosses')

dictfile = 'cedict_ts.u8'
cedict = {}

with open(dictfile, 'r', encoding='utf8') as fin:
    for line in fin:
        if line.startswith('#'):
            continue
        # Note: lines are NOT separated by tabs.
        line = line.strip()
        trad, sim, *stuff = line.split()
        pinyin = re.findall(r'\[([^]]*)\]',line)[0]
        glosses = re.findall(r'\/.*\/', line)[0].strip('/').split('/')
        entry = DictEntry(traditional=trad, simplified=sim, pinyin=pinyin, glosses=glosses)
        cedict[sim] = entry
</code></pre>

<p>It looks like the str and regex operations can be simiplified into a single regex and the columns can be extracted using groups. <strong>How to read the cedict (a space separated file) with regex groups?</strong></p>

<hr>

<p>I've also tried this regex with 4 groups:</p>

<pre><code>(.*)\s(.*)\s(\[([^]]*)\])\s(\/.*\/)
</code></pre>

<p>But somehow the first <code>(.*)\s</code> is greedy and it captures the whole line: <a href=""https://regex101.com/r/1c0O0E/1"" rel=""nofollow noreferrer"">https://regex101.com/r/1c0O0E/1</a></p>

<hr>

<p>I've tried this:</p>

<pre><code>.+\s(\[([^]]*)\])\s(\/.*\/)
</code></pre>

<p>And the first <code>.+\s</code> captures till it sees <code>[</code>. But that means that I'll have to use <code>str.split()</code> to get the first 2 columns. </p>
",Multilingual Language Processing & Language Identification,read cedict space separated file regex group cedict resource chinese text analysis file plaintext file look like column file separated space space th considered one line start need skipped e g line c c san c abbr computer communication consumer electronics china compulsory certificate ccc content column would c c san c abbr computer communication consumer electronics china compulsory certificate ccc currently read file tried using mix skipping line e look like str regex operation simiplified single regex column extracted using group read cedict space separated file regex group also tried regex group somehow first greedy capture whole line tried first capture till see mean use get first column
Speech to Text Method Using Python,"<p>Good day. I am currently working on Machine Translation (Speech-(Text--Text)-Speech) with our local dialects and I already have the speech and text corpus. However, I am facing a problem in recording the speech as input and transcribing it to a text file because the modules available for speech recognition did not cover our dialects, mostly it just supports English and other major languages.</p>

<p>Is there anyone who know how I can fix it? I would be honored to accept your valuable suggestions and it will help me a lot on my studies. Thanks!</p>
",Multilingual Language Processing & Language Identification,speech text method using python good day currently working machine translation speech text text speech local dialect already speech text corpus however facing problem recording speech input transcribing text file module available speech recognition cover dialect mostly support english major language anyone know fix would honored accept valuable suggestion help lot study thanks
machine learning - check &amp; parse sentence related to previous sentence,"<p>During natural language processing, what should be the logic to determine if the current sentence has ""cues"" which needs to be taken from the previous sentence while parsing.</p>

<p>I am not sure how to aptly describe this issue or what sub-topic of NLP it is referred as, so I would present an example.</p>

<p>If <em>previous sentence</em> was <code>Find me an Italian restaurant in New York city</code><br>
and the <em>current sentence</em> is <code>What's the weather there tomorrow</code></p>

<p>How should the parser proceed to understand that <code>there</code> in the current sentence refers to <code>New york</code> which was mentioned in the previous sentence.</p>

<p><em>PS - I am fairly new to NLP, so please be kind here with your explanations.</em></p>
",Multilingual Language Processing & Language Identification,machine learning check parse sentence related previous sentence natural language processing logic determine current sentence ha cue need taken previous sentence parsing sure aptly describe issue sub topic nlp referred would present example previous sentence wa current sentence parser proceed understand current sentence refers wa mentioned previous sentence p fairly new nlp please kind explanation
Getting wrong results NLTK and REGEX,"<p>I want to match all English words that contain
each of the letters ‚Äòa‚Äô, ‚Äòe‚Äô, ‚Äòi‚Äô, ‚Äòo‚Äô and ‚Äòu‚Äô exactly once and in that
order, and that does not match any other English words from the words corpus.
So far i am able to get words with all a,e,i,o,u from the corpus but some words have the vowels appearing more than once for example i'm getting results such as 'abietineous' whereas i'm looking for only words like 'abstemious'.</p>

<p>Here is the snippet of my code. Please assist with fixing my RE to get those results. </p>

<pre><code>[w for w in wordlist if re.search('[a].* [e].* [i].* [o].* [u].', w)]
</code></pre>

<p>Note: I want words containing only a,e,i,o,u - in that order and the a,e,i,o,u must only appear once. (Sorry for the spaces in my code but the format wasn't capturing my asterisks * unless i put a space)</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,getting wrong result nltk regex want match english word contain letter e u exactly order doe match english word word corpus far able get word e u corpus word vowel appearing example getting result abietineous whereas looking word like abstemious snippet code please assist fixing get result note want word containing e u order e u must appear sorry space code format capturing asterisk unless put space thanks
Why does Naive Bayes fail to solve XOR,"<p>I have recently started understanding algorithms related to natural language processing, and have come across various sites which indicate that Naive Bayes cannot capture the XOR concept. Firstly I do not understand what exactly is the XOR problem. Can someone please explain, what the XOR problem is with a simple classification example if possible.</p>
",Multilingual Language Processing & Language Identification,doe naive bayes fail solve xor recently started understanding algorithm related natural language processing come across various site indicate naive bayes capture xor concept firstly understand exactly xor problem someone please explain xor problem simple classification example possible
Solr Numerical Trie vs traditional trie (prefix tree),"<p>I'm reading Taming Text by Ingersoll, Morton &amp; Farris and I don't get how solr's implementation of numerical trie would help search for text? I'm a bit confused with the explanation for solr.TrieField fieldType for solr. </p>

<p>What I am confused is: Traditional prefix tree stores alphabet (if it's English), solr's is storing numerical, I don't get how someone is able to search for text in a numerical tree.</p>

<p>Does the tree convert characters into number and then range it? </p>

<p>It's apparently a range field. From what I've gathered, say we type a prefix Tamp, then from there we can stem to different possibility and a range query works with prefix + [range of character in numeric]? </p>

<p>Thank you for you time</p>
",Multilingual Language Processing & Language Identification,solr numerical trie v traditional trie prefix tree reading taming text ingersoll morton farris get solr implementation numerical trie would help search text bit confused explanation solr triefield fieldtype solr confused traditional prefix tree store alphabet english solr storing numerical get someone able search text numerical tree doe tree convert character number range apparently range field gathered say type prefix tamp stem different possibility range query work prefix range character numeric thank time
how to add matrices as values in a dictionary?,"<p>I have a dictionary which its values are matrices and its keys are the most frequent words in the train file. I have a test file, I have to see if the words in each line of that are in the dictionary it gets their values which are matrices and add the matrices and then divide them to the number of words. the answer should be one matrix. I tried ""sum(val)"" but it doesn't add them together. How can I do it? (The file contains a Persian sentence, a tab and then an English word). The output of the dictionary is as like as below:</p>

<p><a href=""https://i.sstatic.net/Gxj0t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Gxj0t.png"" alt=""enter image description here""></a></p>

<pre><code>keys = [p[0] for p in freq.most_common(4)]            
array = numpy.array([[wordVector[0,:]] , [wordVector[1,:]],    [wordVector[2,:]], [wordVector[3,:]]])
dic = dict(zip(keys, zip(array)))
#print (dic)

# test part
with open (""test2.txt"", encoding = ""utf-8"") as f2:
    for line in f2:
        line = line.split(""\t"")
        lin = line[0].split()
        for i in lin:
            for key, val in dic.items():
                if i == key:
                    print ((sum(val))/
</code></pre>
",Multilingual Language Processing & Language Identification,add matrix value dictionary dictionary value matrix key frequent word train file test file see word line dictionary get value matrix add matrix divide number word answer one matrix tried sum val add together file contains persian sentence tab english word output dictionary like
it-idf with TfidfVectorizer on Japanese text,"<p>I am working with a huge collection of documents written in several languages. I want to compute cosine distance between documents from their tf-idf scores. So far I have:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

# The documents are located in the same folder as the script
text_files = [r'doc1', r'doc2', r'doc3'] 
files = [open(f) for f in text_files]
documents = [f.read() for f in files]

vectorizer = TfidfVectorizer(ngram_range=(1,1))
tfidf = vectorizer.fit_transform(documents)
vocabulary = vectorizer.vocabulary_
</code></pre>

<p>When the three documents <code>doc1</code>, <code>doc2</code> and <code>doc3</code> contain English text, the algorithm works like a charm and <code>vocabulary</code> does indeed contains unigrams from the different bodies of text. I tried with Russian too, and it also worked great. However, when I try with some Japanese text, the algorithm does not work as intended anymore.</p>

<p>The problem arises from the fact that Japanese language does not have spaces, so that TfidfVectorizer does not understand what's a word and what isn't. For example I would have something like this in my unigram vocabulary:</p>

<blockquote>
  <p>Ë®∫Â§öÁ¥¢„ÅÑ„ÅªÊ®©ËæºÁúü„Åπ„Åµ„ÇäÂëäËªä„ÇØ„ÉéËà¨ÂÆÆ„Åà„Åº„Åú„ÇÜÊ≥®Êê∫„ÇÜ„ÇØ„Åè‰æõ9ÊôÇ„ÇØËª¢ÁµÑ„Åë„ÅåÊÑèË¶ã„Å†„Å£„ÅÇÁ®éÊñ∞„ÉàÂæ©Áîü„Å≤„ÇäÊïôÂè∞Ë©±Ëæû„ÇÉ„Å´</p>
</blockquote>

<p>Whic is clearly a sentence and not a word. How can I solve this problem?</p>
",Multilingual Language Processing & Language Identification,idf tfidfvectorizer japanese text working huge collection document written several language want compute cosine distance document tf idf score far three document contain english text algorithm work like charm doe indeed contains unigrams different body text tried russian also worked great however try japanese text algorithm doe work intended anymore problem arises fact japanese language doe space tfidfvectorizer doe understand word example would something like unigram vocabulary whic clearly sentence word solve problem
How can I detect whether a given line in a file is a proper English sentence?,"<p>I need to detect if a given ""line"" in a file is an English sentence or not. I am using Python. An approximate answer would do. I understand this is an NLP question but is there a lightweight tool that gives a reasonable approximation? I do not want to use a full-fledged NLP toolkit for this though if that is the only way then it is fine.</p>

<p>If NLP toolkit is the answer then the one that I am reading about is the <a href=""http://www.nltk.org/"" rel=""nofollow"">Natural Language Toolkit</a>. If anyone has a simple example on how to detect a sentence handy, please point me to it.</p>
",Multilingual Language Processing & Language Identification,detect whether given line file proper english sentence need detect given line file english sentence using python approximate answer would understand nlp question lightweight tool give reasonable approximation want use full fledged nlp toolkit though way fine nlp toolkit answer one reading natural language toolkit anyone ha simple example detect sentence handy please point
Proper way of eliminating letter repetitions from English words?,"<p>As the title clearly describes, I wonder what is the right way to eliminate character repetitions in English that are commonly used in social media to exaggerate the feeling. Since I am developing a software solution to correct mistyped words, I need a global algorithm that can be applied to most majority of English words. So, I am asking experts to learn the right way to eliminate additional letters in English words without using learning-based approachs?</p>

<p>ps. (1) I check programmatically if the word is valid or not using the <a href=""https://wordnet.princeton.edu"" rel=""nofollow"">WordNet 3.0 database</a>. So far so good except some examples such as the word <code>veery</code> which is defined as <code>tawny brown North American trush noted for its song</code> in WordNet 3.0. I interrupt letter elimination process when the word is found in WordNet. So are there any other knowledge bases that can be used instead of WordNet?</p>

<p>ps. (2) Actually I asked this question at <a href=""http://english.stackexchange.com"">English Language &amp; Usage community</a>. But they guided me to ask it here.</p>

<p><strong>Some examples:</strong></p>

<pre><code>haappyy --&gt; happy
amaaazzinng --&gt; amazing
veeerry --&gt; very
</code></pre>

<p>As you see in the examples, the place of letter repetition various through the word.</p>
",Multilingual Language Processing & Language Identification,proper way eliminating letter repetition english word title clearly describes wonder right way eliminate character repetition english commonly used social medium exaggerate feeling since developing software solution correct mistyped word need global algorithm applied majority english word asking expert learn right way eliminate additional letter english word without using learning based approach p check programmatically word valid using wordnet database far good except example word defined wordnet interrupt letter elimination process word found wordnet knowledge base used instead wordnet p actually asked question href language amp usage community guided ask example see example place letter repetition various word
Determining gender of words,"<p>I'm looking to anonymise text, and one of the tasks is to remove references to the gender of a person being referenced. It is OK, but not preferable, if a little bit of meaning is lost (a pragmatic consequence of the fact this would be automated).</p>

<p>Words like he/she/him/her imply the gender of the person being referred to. These are the words I'm looking for.</p>

<p>Given one of these words, I can normalise (likely by just picking one of the words), but I need to be able to identify them first.</p>

<p>My naive thought was to search through the dictionary for any word with a definition that specifies a gender. That works, but is quite noisy.</p>

<p>Is there a better method for determining gender-specific words (in English), or a (preferably automated) method for determining them?</p>
",Multilingual Language Processing & Language Identification,determining gender word looking anonymise text one task remove reference gender person referenced ok preferable little bit meaning lost pragmatic consequence fact would automated word like imply gender person referred word looking given one word normalise likely picking one word need able identify first naive thought wa search dictionary word definition specifies gender work quite noisy better method determining gender specific word english preferably automated method determining
Any (rough) equivalent to iOS NSLinguisticTagger for android?,"<p>In iOS, you can use the NSLinguisticTagger to do things like part-of-speech tagging, and even lemmatizing a word (like recognizing that ""went"" is a form of the verb ""to go"") for several different languages.</p>

<p>Is there anything like this for use with android development, that's available for several different languages, as is the case for iOS? I'm aware of stuff like the Stanford NLP, but it seems to only work with a limited number of languages (English + 5 others), which is not sufficient for my purposes. Plus, I'm not even sure if it could work on android. (but correct me if I'm wrong)</p>

<p>It would need to work without accessing the network, so it must do everything on the device without connecting to a service.</p>

<p>I'm afraid the answer is ""no""...but asking anyway just in case, before I spend a huge amount of time trying to develop my own workaround.</p>
",Multilingual Language Processing & Language Identification,rough equivalent io nslinguistictagger android io use nslinguistictagger thing like part speech tagging even lemmatizing word like recognizing went form verb go several different language anything like use android development available several different language case io aware stuff like stanford nlp seems work limited number language english others sufficient purpose plus even sure could work android correct wrong would need work without accessing network must everything device without connecting service afraid answer asking anyway case spend huge amount time trying develop workaround
What are trained models in NLP?,"<p>I am new to Natural language processing. Can anyone tell me what are the trained models in either OpenNLP or Stanford CoreNLP? While coding in java using apache openNLP package, we always have to include some trained models (found here <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow noreferrer"">http://opennlp.sourceforge.net/models-1.5/</a> ). What are they?</p>
",Multilingual Language Processing & Language Identification,trained model nlp new natural language processing anyone tell trained model either opennlp stanford corenlp coding java using apache opennlp package always include trained model found
delete specific words in a file,"<p>I want to delete stop words in a file (it contains a sentence, a tab and then an English word). The stop words are in a separate file and the language is Persian. The code below works, but the problem is, it deletes for example a stop word in a line but it doesn't delete the same stop word in some other line. it happened almost for every stop words. I guessed maybe it can be for normalizing. So I normalized the 2 files by importing the hazm module (hazm is like NLTK, for Persian). But there was no change in the problem. Can some body help?</p>

<pre><code>from hazm import*
punctuation = '!""#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~ÿåÿü¬´ÿõ'

file1 = ""stopwords.txt""
file2 = ""test/Ÿæÿ±.txt""


witoutStops = []
corpuslines = []

def RemStopWords (file1, file2):  
    with open(file1, encoding = ""utf-8"") as stopfile:
        normalizer = Normalizer()
        stopwords = stopfile.read()
        stopwords = normalizer.normalize(stopwords)
        with open(file2, encoding = ""utf-8"") as trainfile:
            with open (""y.txt"", ""w"", encoding = ""utf-8"") as newfile:
                for line in trainfile:
                    tmp = line.strip().split(""\t"")
                    tmp[0] = normalizer.normalize(tmp[0])
                    corpuslines.append(tmp)
                    for row in corpuslines:
                        line = """"
                        tokens = row[0].split()
                        for token in tokens:
                            if token not in stopwords:
                                line += token + "" ""
                    line = line.strip() + ""\n""
                    for i in punctuation:  # deletes punctuations
                        if i in line:
                            line = line.replace(i, """")
                    newfile.write(line)
                    witoutStops.append (line)
</code></pre>

<p>stop words file:
<a href=""https://www.dropbox.com/s/irjkjmwkzwnnpnk/stopwords.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/irjkjmwkzwnnpnk/stopwords.txt?dl=0</a></p>

<p>file:
<a href=""https://www.dropbox.com/s/p4m8san3xhr0pdj/%D9%BE%D8%B1.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/p4m8san3xhr0pdj/%D9%BE%D8%B1.txt?dl=0</a></p>
",Multilingual Language Processing & Language Identification,delete specific word file want delete stop word file contains sentence tab english word stop word separate file language persian code work problem deletes example stop word line delete stop word line happened almost every stop word guessed maybe normalizing normalized file importing hazm module hazm like nltk persian wa change problem body help stop word file file
NLP: Determine whether a specific semantic meaning is conveyed in a sentence,"<p>I have a collection of sentences from which I would like to extract those that express the following semantic meaning:</p>

<p>I like Italian cuisine.</p>

<p>There are many variations of how such a sentence can be structured and worded. Some examples:</p>

<ul>
<li>I enjoy Italian, Chinese, and Indian food.</li>
<li>Cuisines I love are Chinese, Italian, and Indian.</li>
<li>Some cuisines I like include Indian, Italian, and Chinese.</li>
<li>I like all kinds of cuisines around the world, such as Italian, Chinese, and Indian.</li>
</ul>

<p>What is a good way to approach this problem?</p>

<p>I am no expert in NLP. Here is just something I could think of: </p>

<ul>
<li>Find synonyms for 'like' and 'cuisine'</li>
<li>Build dependency trees for sentences using a parser (Stanford or Parsey McParseface)</li>
<li>Trim the dependency tree to only include the subject (e.g. 'I'), the verb keyword (e.g. 'like'), the noun keyword (e.g. 'food'), and the noun modifier (e.g. 'Italian'). This can be done by finding a path covering all these nodes in the tree.</li>
<li>Store a collection of dependency trees of training sentences.</li>
<li>Check if the dependency tree of a testing sentence exists in training</li>
</ul>

<p>Any ideas, suggestions, and/or comments would be much appreciated!</p>
",Multilingual Language Processing & Language Identification,nlp determine whether specific semantic meaning conveyed sentence collection sentence would like extract express following semantic meaning like italian cuisine many variation sentence structured worded example enjoy italian chinese indian food cuisine love chinese italian indian cuisine like include indian italian chinese like kind cuisine around world italian chinese indian good way approach problem expert nlp something could think find synonym like cuisine build dependency tree sentence using parser stanford parsey mcparseface trim dependency tree include subject e g verb keyword e g like noun keyword e g food noun modifier e g italian done finding path covering node tree store collection dependency tree training sentence check dependency tree testing sentence exists training idea suggestion comment would much appreciated
NLP: Classification giving wrong result. How to find out that the result from NLP Classification is wrong?,"<p>I have started learning Natural Language Processing and have already started stumbling.</p>

<p>I am using <code>NodeJs</code> for creating my application with the help of <code>NaturalNode library</code>
<a href=""https://github.com/NaturalNode/natural"" rel=""nofollow noreferrer"">Natural Node GitHub project</a></p>

<p><strong>Problem</strong></p>

<p>I am training my document with several scenarios as shown below</p>

<pre><code>/// importing package
var natural = require('natural');
var classifier = new natural.BayesClassifier();



/// traning document
classifier.addDocument(""h"", ""greetings"");
classifier.addDocument(""hi"", ""greetings"");
classifier.addDocument(""hello"", ""greetings"");
classifier.addDocument(""data not working"", ""internet_problem"");
classifier.addDocument(""browser not working"", ""internet_problem"");
classifier.addDocument(""google not working"", ""internet_problem"");
classifier.addDocument(""facebook not working"", ""internet_problem"");
classifier.addDocument(""internet not working"", ""internet_problem"");
classifier.addDocument(""websites not opening"", ""internet_problem"");
classifier.addDocument(""apps not working"", ""internet_problem"");
classifier.addDocument(""call drops"", ""voice_problem"");
classifier.addDocument(""voice not clear"", ""voice_problem"");
classifier.addDocument(""call not connecting"", ""voice_problem"");
classifier.addDocument(""calls not going through"", ""voice_problem"");
classifier.addDocument(""disturbance"", ""voice_problem"");
classifier.addDocument(""bye"", ""close"");
classifier.addDocument(""thank you"", ""feedback_positive"");
classifier.addDocument(""thanks"", ""voice_problem"");
classifier.addDocument(""shit"", ""feedback_negeive"");
classifier.addDocument(""shit"", ""feedback_negeive"");
classifier.addDocument(""useless"", ""feedback_negetive"");
classifier.addDocument(""siebel testing"", ""siebel_testing"")


classifier.train();


/// running classification
console.log('result for hi');
console.log(classifier.classify('hi'));
console.log('result for hii');
console.log(classifier.classify('hii'));
console.log('result for h');
console.log(classifier.classify('h'));
</code></pre>

<blockquote>
  <p>Output</p>

<pre><code>result for hi:
greetings


result for hii:
internet_problem

result for h:
internet_problem
</code></pre>
</blockquote>

<p>As you can see in the result of the key work <code>hi</code> the value is coming correct but if I misspelled <code>hi</code> for <code>hii</code> or <code>ih</code> then it's giving a wrong result. I am not able to understand how does classification works and how should I train the classifier or is there a way to find out that the result of classification is wrong so that I can request an user to input again.</p>

<p>Any help or explanation or anything is highly appreciated. Many thanks in advance.</p>

<p>Please consider me as a noob and forgive for any mistake.</p>
",Multilingual Language Processing & Language Identification,nlp classification giving wrong result find result nlp classification wrong started learning natural language processing already started stumbling using creating application help natural node github project problem training document several scenario shown output see result key work value coming correct misspelled giving wrong result able understand doe classification work train classifier way find result classification wrong request user input help explanation anything highly appreciated many thanks advance please consider noob forgive mistake
How to test SyntaxNet trained model (Spanish UD)?,"<p>Training my own SyntaxNet model for Spanish-Ancora UD corpus following the instructions from <a href=""https://github.com/tensorflow/models/tree/master/syntaxnet"" rel=""nofollow"">here</a> did not give me errors.</p>

<p>After all the steps the final files it creates were:</p>

<pre><code>-category-map  
-char-map  
-checkpoint  
-context  
-graph  
-label-map  
-latest-model  
-latest-model.meta  
-lcword-map  
-model  
-model.meta  
-prefix-table  
-status  
-suffix-table  
-tag-map  
-tag-to-category  
-tagged-dev-corpus  
-tagged-training-corpus  
-tagged-tunning-corpus  
-word-map 
</code></pre>

<p>The context.pbtxt file used for the training was the one from syntaxnet/models/parsey_universal.</p>

<p>Then when I try to test it calling <code>parser.sh</code> from syntaxnet/models/parsey_universal return a couple errors:  </p>

<pre><code>F syntaxnet/term_frequency_map.cc:63] Check failed: ::tensorflow::Status::OK() == (tensorflow::Env::Default()-&gt;NewRandomAccessFile(filename, &amp;file)) (OK vs. Not found: syntaxnet/models/parsey_universal/modeltest/char-ngram-map)
F syntaxnet/term_frequency_map.cc:63] Check failed: ::tensorflow::Status::OK() == (tensorflow::Env::Default()-&gt;NewRandomAccessFile(filename, &amp;file)) (OK vs. Not found: syntaxnet/models/parsey_universal/modeltest/morphology-map) 
</code></pre>

<p>Then I downloaded the Spanish pretrained model from <a href=""https://github.com/tensorflow/models/blob/master/syntaxnet/universal.md"" rel=""nofollow"">here</a>. And I checked the files. It seems like there are two missing files, the pretrained model has it by default but in the one I trained, these files were missing.</p>

<p>So my questions are how to get these files in the SyntaxNet training phase?<br>
there are other ways to produce them?<br>
should I test it in a different way?</p>
",Multilingual Language Processing & Language Identification,test syntaxnet trained model spanish ud training syntaxnet model spanish ancora ud corpus following instruction give error step final file creates context pbtxt file used training wa one syntaxnet model parsey universal try test calling syntaxnet model parsey universal return couple error downloaded spanish pretrained model checked file seems like two missing file pretrained model ha default one trained file missing question get file syntaxnet training phase way produce test different way
State of the art language translation toolkit,"<p>I need to translate Spanish tweets into english for my research. I find some toolkit. Among them, <a href=""http://www.statmt.org/moses/"" rel=""nofollow noreferrer"">Moses</a> is used by some research papers and other emerging toolkits used them as a baseline for evaluation purpose. So i am considering it as a candidate. Also, I found a toolkit from Stanford university called <a href=""http://nlp.stanford.edu/phrasal/"" rel=""nofollow noreferrer"">Phrsal</a>, which also seems to be good. The last one I found is from renowned <code>nltk</code> library. It also has a <a href=""http://www.nltk.org/api/nltk.translate.html"" rel=""nofollow noreferrer"">translate</a> package. Every one of them states that they used <code>phrase based statistical machine translation</code> technique along with some other techinques. Now my question is, from a practical or theoretical point of view, which will be best to use for tweets translation. Or <code>google translator api</code> would be the best solution?              </p>
",Multilingual Language Processing & Language Identification,state art language translation toolkit need translate spanish tweet english research find toolkit among moses used research paper emerging toolkits used baseline evaluation purpose considering candidate also found toolkit stanford university called phrsal also seems good last one found renowned library also ha translate package every one state used technique along techinques question practical theoretical point view best use tweet translation would best solution
Natural language processing API for iOS 5,"<p>I am looking for NLP capable library or API which will be able to identify number, date, person, place name, and parts of speech from a given sentence.</p>
",Multilingual Language Processing & Language Identification,natural language processing api io looking nlp capable library api able identify number date person place name part speech given sentence
Detect language changes in file using Python,"<p>I need to detect language changes in a file, and tag each word accordingly. I've come up with a hacky way, that works for 2 languages (english and greek).</p>

<p>The script is this:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys

#open file
filename = sys.argv[1]
f = open(filename,'r')
content = f.read()
f.close()


#initialize new content
newSentence=''
#for every line, if the first letter of the token isn't ascii, it's nonsense, tag it.
for line in content.split('\n'):
    newSentence+='\n'
    for token in line.split():
        try:
            result = token[0].decode('ascii','ignore')
            newSentence += ' /en'+token
        except:
            newSentence += ' /gr'+token


print newSentence

f=open(filename+'_new.txt','w')
f.write(newSentence)
f.close()
</code></pre>

<p>The main idea is that if the first letter of each word isn't ascii decodeable it mustn't be english,so it's the only other option.</p>

<p>Now i realize this is awfully hacky and I'd like to know how would I go about doing it in a more pythonic way? Even in a way that works for multiple languages in a document.</p>

<p>PS. I know how to detect language in a document in general, however I was wondering if there was faster way to detecting just the changes without invoking tools such as nltk etc.</p>
",Multilingual Language Processing & Language Identification,detect language change file using python need detect language change file tag word accordingly come hacky way work language english greek script main idea first letter word ascii decodeable english option realize awfully hacky like know would go pythonic way even way work multiple language document p know detect language document general however wa wondering wa faster way detecting change without invoking tool nltk etc
Programming tips with Japanese Language/Characters,"<p>I have an idea for a few web apps to write to help me, and maybe others, learn Japanese better since I am studying the language.</p>

<p>My problem is the site will be in mostly english, so it needs to mix fluently Japanese Characters, usually hirigana and katakana, but later kanji. I am getting closer to accomplishing this; I have figured out that the pages and source files need to be unicode and utf-8 content types.</p>

<p>However, my problem comes in the actual coding. What I need is to manipulate strings of text that are kana.  One example is:</p>

<p>„Åë„Åô I need to take that verb and convert it to the te-form „Åë„Åó„Å¶. I would prefer to do this in javascript as it will help down the road to do more manipulation, but if I have to will just do DB calls and hold everything in a DB.</p>

<p>My question is not only how to do it in javascript, but what are some tips and strategies to doing these kinds of things in other languages, too.  I am hoping to get more into doing language learning apps, but am lost when it comes to this.</p>
",Multilingual Language Processing & Language Identification,programming tip japanese language character idea web apps write help maybe others learn japanese better since studying language problem site mostly english need mix fluently japanese character usually hirigana katakana later kanji getting closer accomplishing figured page source file need unicode utf content type however problem come actual coding need manipulate string text kana one example need take verb convert te form would prefer javascript help road manipulation db call hold everything db question javascript tip strategy kind thing language hoping get language learning apps lost come
how to use regex with arabic text,"<p>I'm trying to find the main stem of arabic word the user will enter ŸÑÿßÿπÿ®ŸàŸÜ and the program will try to remove ŸàŸÜ from the word, the remain part of the word will be ŸÑÿßÿπÿ® and then try to find the main stem ŸÑÿπÿ® in my list of stems, can i do that with regex or any advice. Thanks</p>
",Multilingual Language Processing & Language Identification,use regex arabic text trying find main stem arabic word user enter program try remove word remain part word try find main stem list stem regex advice thanks
How to extract nouns from a German text with the StanfordNLP tool in Scala?,"<p>I want to extract the nouns of a German text with the StanfordNLP tool. Therefore I added the dependencies for German texts.</p>

<hr>

<p>My dependencies:</p>

<pre><code>     &lt;dependency&gt;
                &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
                &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
                &lt;version&gt;3.6.0&lt;/version&gt;
            &lt;/dependency&gt;

            &lt;!--BEGIN: NLP For German Text --&gt;
        &lt;dependency&gt;
              &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
              &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
              &lt;version&gt;3.6.0&lt;/version&gt;
              &lt;classifier&gt;models-german&lt;/classifier&gt;
            &lt;/dependency&gt;
            &lt;!--END: NLP For German Text --&gt;

            &lt;!-- Other dependencies --&gt;
</code></pre>

<p>In my Scala class I want to extract the nouns of a tweet. Here the following code snipped:</p>

<pre><code>// Start a new processor to use the NLP tools
    val proc: Processor = new FastNLPProcessor

    // TODO: Explain the val doc
    val doc = proc.annotate(text)

    // Is a String where the keywords are stored that we want to extract (e.g. Nouns - ""N""; etc.)
    var keywords: String = """"

    // Iterate throgh each sentence
    for (sentence &lt;- doc.sentences) {

      // i - contains the word of each sentence in a text in the current loop
      // x - saves the position of the word in the current loop
      // E.g. for tweet text :: ""new scala update xyz"" -&gt; first loop: i = new ; x = 0
      for ((i, x) &lt;- sentence.tags.get.view.zipWithIndex) {
        // ""N"" - is the abbreviation for Nouns
        if (i.toString().startsWith(""N"")) {
          // Append to keywords the noun of a text
          keywords = keywords + "" "" + sentence.words.array(x)
          // Print the current state of the keyword string
          println(keywords)
        }
      }
    }
</code></pre>

<p>But it only works for english texts. These are my imports:</p>

<pre><code>import org.clulab.processors.Processor
import org.clulab.processors.fastnlp.FastNLPProcessor
import play.api.libs.json._
import scala.util.parsing.json.JSONObject
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkConf
import com.mongodb.casbah.Imports._
import com.mongodb.casbah.MongoConnection
import com.mongodb.casbah.commons.MongoDBObject
import org.clulab.struct.DirectedGraphEdgeIterator
</code></pre>

<hr>

<p><strong>The complete Scala class:</strong></p>

<pre><code>object KeywordExtractor {

   // Creates a connection to the MongoDB client
   val mongoConn = MongoClient(""localhost"", 27017)

   // Names the DB where the data should be saved
   val mongoDB = mongoConn(""dbtest"")

   // Defines the collection in which the text should be stored
   val mongoColl = mongoDB(""testcollection"")

  def extractKey(tweet: String) = {

    // tweet is sent as a Json-String. jsonObject -&gt; stores the sent tweet as a Json Object
    val jsonObject = Json.parse(tweet)

    // text String to store the text from the tweet
    var text = """"
    try {
      // try to parse the text from the jsonObject into the text variable
      text = (jsonObject \ ""text"").as[String]
    } catch {
      case e: JsResultException =&gt; println(""Limit reached"")
    }

    // Start a new processor to use the NLP tools
    val proc: Processor = new FastNLPProcessor

    // TODO: Explain the val doc
    val doc = proc.annotate(text)

    // Is a String where the keywords are stored that we want to extract (e.g. Nouns - ""N""; etc.)
    var keywords: String = """"

    // Iterate throgh each sentence
    for (sentence &lt;- doc.sentences) {

      // i - contains the word of each sentence in a text in the current loop
      // x - saves the position of the word in the current loop
      // E.g. for tweet text :: ""new scala update xyz"" -&gt; first loop: i = new ; x = 0
      for ((i, x) &lt;- sentence.tags.get.view.zipWithIndex) {
        // ""N"" - is the abbreviation for Nouns
        if (i.toString().startsWith(""N"")) {
          // Append to keywords the noun of a text
          keywords = keywords + "" "" + sentence.words.array(x)
          // Print the current state of the keyword string
          println(keywords)
        }
      }
    }

    // Create a new MongoDB builder
    val builder = MongoDBObject.newBuilder

    // Creates a monogDb object to store the keywords (a bison file is created and text is the key for the value keywords)
    builder += ""text"" -&gt; keywords

    // TODO: .result not clear
    val newObj = builder.result

    // Insert the new Object to the MonogDB
    mongoColl.insert(newObj)

    // Clear the memory of the doc to avoid a out of memory error (doc allocates a lot of memory)
    doc.clear();
  }
}
</code></pre>
",Multilingual Language Processing & Language Identification,extract noun german text stanfordnlp tool scala want extract noun german text stanfordnlp tool therefore added dependency german text dependency scala class want extract noun tweet following code snipped work english text import complete scala class
NLTK cannot find the file,"<p>I am having problems with a php script that says that it cannot find file /root/nltk_data/tokenizers/punkt/english.pickle . However I confirmed that the file is here. II downloaded the whole data set multiple times)</p>

<p>The php script actually runs a python script and the nltk (a python module) says that it cannot find /root/nltk_data/tokenizers/punkt/english.pickle</p>

<pre><code>$dir = dirname(__FILE__);
$command =  ""/usr/bin/python "". $dir . ""/test.py"";
exec($command, $output);
</code></pre>

<p>On the other hands when i run the python script from command line it works perfectly fine and is able to access the file.</p>

<pre><code>python test.py
</code></pre>

<p>Is it possible to enable php to see those files? I chmod 777 the file but this didn't help.</p>

<p>the script contains:</p>

<pre><code>#!/usr/bin/env/ python
import  nltk
try:
    tokens = nltk.word_tokenize(""I like apples."")
    tagged = nltk.pos_tag(tokens)
    print ""OK!""
    #print ' * '.join(tokens)
except Exception:
    print ""error!""
    pass
</code></pre>

<p>Error log:</p>

<pre><code>Traceback (most recent call last):
File ""/var/zpanel/hostdata/zadmin/public_html/my_domain_com/test.py"", line 39, in &lt;module&gt;
tagged = nltk.pos_tag(tokens)
File ""/usr/local/lib/python2.7/site-packages/nltk-2.0.4-py2.7.egg/nltk/tag/__init__.py"", line   99, in pos_tag
tagger = load(_POS_TAGGER)
File ""/usr/local/lib/python2.7/site-packages/nltk-2.0.4-py2.7.egg/nltk/data.py"", line 605, in load
resource_val = pickle.load(_open(resource_url))
File ""/usr/local/lib/python2.7/site-packages/nltk-2.0.4-py2.7.egg/nltk/data.py"", line 686, in _open
return find(path).open()
File ""/usr/local/lib/python2.7/site-packages/nltk-2.0.4-py2.7.egg/nltk/data.py"", line 467, in find
raise LookupError(resource_not_found)
LookupError:
Resource taggers/maxent_treebank_pos_tagger/english.pickle not found. Please use the NLTK Downloader to obtain the resource:
&gt;&gt;&gt; nltk.download()
Searched in:
- '/root/nltk_data'
- '/usr/share/nltk_data'
- '/usr/local/share/nltk_data'
- '/usr/lib/nltk_data'
- '/usr/local/lib/nltk_data'
</code></pre>

<hr>
",Multilingual Language Processing & Language Identification,nltk find file problem php script say find file root nltk data tokenizers punkt english pickle however confirmed file ii downloaded whole data set multiple time php script actually run python script nltk python module say find root nltk data tokenizers punkt english pickle hand run python script command line work perfectly fine able access file possible enable php see file chmod file help script contains error log
How can I stop Stanford CoreNLP from segmenting my sentence,"<p>I have segmented resources and resources that match my segmented sentences.</p>

<p>How can I stop Stanford CoreNLP from segmenting my sentence before generating the parsing tree?</p>

<p>I am doing works on Chinese.</p>
",Multilingual Language Processing & Language Identification,stop stanford corenlp segmenting sentence segmented resource resource match segmented sentence stop stanford corenlp segmenting sentence generating parsing tree work chinese
Java OpenNLP version 1.5.3. Spanish models,"<p>I have a question concerning OpenNLP. I am looking for Spanish sentence detection and tokenization models that run with OpenNLP version 1.5.3. So far, I only found a pos tagging model that works with this version (<a href=""https://github.com/utcompling/OpenNLP-Models/tree/master/models"" rel=""nofollow noreferrer"">https://github.com/utcompling/OpenNLP-Models/tree/master/models</a>)</p>

<p>Additionally to the Spanish models, I need to use also English, French and Italian models in one java project. So far, I could find all models (sentence detection, tokenization and pos tagging) for every language, except for Spanish, for which I just found the pos tagging model. I don't want to switch to OpenNLP 1.4. (<a href=""http://opennlp.sourceforge.net/models.html"" rel=""nofollow noreferrer"">http://opennlp.sourceforge.net/models.html</a>) because I cannot find French and Italian models for that.</p>

<p>All the best and many thanks in advance,
Bina</p>
",Multilingual Language Processing & Language Identification,java opennlp version spanish model question concerning opennlp looking spanish sentence detection tokenization model run opennlp version far found po tagging model work version additionally spanish model need use also english french italian model one java project far could find model sentence detection tokenization po tagging every language except spanish found po tagging model want switch opennlp find french italian model best many thanks advance bina
Detecting sarcasm in a statement,"<p>How can I interpret whether a statement is a sarcastic one while dealing with various reviews from users? Stanford NLP (Natural Language Processing) can only tell whether it's a negative or a positive statement but sarcasm cannot be interpreted.</p>
",Multilingual Language Processing & Language Identification,detecting sarcasm statement interpret whether statement sarcastic one dealing various review user stanford nlp natural language processing tell whether negative positive statement sarcasm interpreted
How to use basic BLEU score in Asiya Machine Translation Evaluation toolkit?,"<p>Asiya is the machine translation evaluation toolkit to score machine translation outputs (<a href=""http://asiya.lsi.upc.edu/"" rel=""nofollow"">http://asiya.lsi.upc.edu/</a>). It is largely written in Perl.</p>

<p><strong>How do I use Asiya to perform BLEU metrics?</strong></p>

<p>I have followed the youtube introduction video: <a href=""https://www.youtube.com/watch?v=rA5De9Z4uWI"" rel=""nofollow"">https://www.youtube.com/watch?v=rA5De9Z4uWI</a></p>

<p>And created a config file (Asiya.config):</p>

<pre><code>input=raw
srclang=en
srccase=en
trglang=ja
trgcase=ja

src=corpus.tok/test.tok.en
ref=corpus.tok/hyp.tok.ja
sys=corpus.tok/test.tok.ja

some_metrics= BLEU NIST METEOR-ex Ol
</code></pre>

<p>My machine translation output file is in <code>corpus.tok/hyp.tok.ja</code>, the source file is in <code>corpus.tok/test.tok.en</code> and the reference file (correct translation) is at <code>corpus.tok/test.tok.ja</code>. They are tokenized plain text files, each line is a sentence.</p>

<p>And when I ran:</p>

<pre><code>/home/expert/asiya/bin/Asiya.pl -eval single -g all -metric_set some_metrics Asiya.config
</code></pre>

<p>I got this error:</p>

<pre><code>Smartmatch is experimental at /home/expert/asiya/bin/../lib/IQ/Common.pm line 784.
Smartmatch is experimental at /home/expert/asiya/bin/../lib/IQ/Common.pm line 791.
Smartmatch is experimental at /home/expert/asiya/bin/../lib/IQ/Scoring/ESA.pm line 339.
Use of uninitialized value in concatenation (.) or string at /home/expert/asiya/bin/../lib/IQ/Config.pm line 251.
[ASIYA] directory &lt;/tools&gt; does not exist!
</code></pre>

<p>The tools directory does exists as a subdirectory in the current directory that I ran the command. <strong>What went wrong? Is there a parameter that I could add for the Asiya tools directory?</strong></p>

<p>How do I use Asiya to perform BLEU evaluation?</p>

<p><strong>If I don't use Asiya, how else can I get the BLEU score per sentence and system BLEU scores for my machine translation output?</strong></p>

<p>(more details on <a href=""http://nlp.lsi.upc.edu/redmine/boards/11/topics/138"" rel=""nofollow"">http://nlp.lsi.upc.edu/redmine/boards/11/topics/138</a>)</p>
",Multilingual Language Processing & Language Identification,use basic bleu score asiya machine translation evaluation toolkit asiya machine translation evaluation toolkit score machine translation output largely written perl use asiya perform bleu metric followed youtube introduction video created config file asiya config machine translation output file source file reference file correct translation tokenized plain text file line sentence ran got error tool directory doe exists subdirectory current directory ran command went wrong parameter could add asiya tool directory use asiya perform bleu evaluation use asiya else get bleu score per sentence system bleu score machine translation output detail
What are features generators in natural language processing,"<p>Can anyone tell me what feature generators are with respect to natural language processors?</p>
",Multilingual Language Processing & Language Identification,feature generator natural language processing anyone tell feature generator respect natural language processor
How can I learn *practical* natural language processing?,"<p>I have some background in Java, Pascal, PERL, SQL, &amp; R and would like to find a reasonably least resistance path from that background to doing automated or semi-automated extraction of concepts from text and turning the result into something statistically analyzable (willing to learn new languages if needed).  I imagine I will need to perform some NLP tasks on a few thousand pages of text, particularly POS processing, identification of noun phrases, word sense disambiguation.  The latter, I believe, may require semi-supervised machine learning for accuracy.  The question I have is where to start learning <strong>practical</strong> NLP?  Taking a course or reading NLP books seem to involve getting into far more detail about how NLP tasks are conducted than I need now--I just need to know what it does, how accurate it is, and what alternatives there are.  Jumping into some existing NLP framework seems to get me stuck.  I've used GATE for POS processing, but the output was either in XML, which I have no idea how to further process, or in postgresql, which was a bear to manipulate w/ SQL to generate statistical data.  Also, at the time, GATE had no good method for extracting word sense.</p>
",Multilingual Language Processing & Language Identification,learn practical natural language processing background java pascal perl sql r would like find reasonably least resistance path background automated semi automated extraction concept text turning result something statistically analyzable willing learn new language needed imagine need perform nlp task thousand page text particularly po processing identification noun phrase word sense disambiguation latter believe may require semi supervised machine learning accuracy question start learning practical nlp taking course reading nlp book seem involve getting far detail nlp task conducted need need know doe accurate alternative jumping existing nlp framework seems get stuck used gate po processing output wa either xml idea process postgresql wa bear manipulate w sql generate statistical data also time gate good method extracting word sense
"Identify Subject ,Object,Verb in a English Sentence?","<p>I'm working in a machine translation  project. I need to identify subject,verb,object of a sentence in order continue my work. currently I'm using Stanford NLP parser to analyze the sentence. But I don't know how to extract SVO. Any ideas that I can consider?</p>
",Multilingual Language Processing & Language Identification,identify subject object verb english sentence working machine translation project need identify subject verb object sentence order continue work currently using stanford nlp parser analyze sentence know extract svo idea consider
Programmatic parsing and understanding of language (English),"<p>I am looking for some resources pertaining to the parsing and understanding of English (or just human language in general). While this is obviously a fairly complicated and wide field of study, I was wondering if anyone had any book or internet recommendations for study of the subject. I am aware of the basics, such as searching for <a href=""http://en.wikipedia.org/wiki/To_be"" rel=""nofollow noreferrer"">copulas</a> to draw word relationships, but anything you guys recommend I will be sure to thoroughly read.</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,programmatic parsing understanding language english looking resource pertaining parsing understanding english human language general obviously fairly complicated wide field study wa wondering anyone book internet recommendation study subject aware basic searching copula draw word relationship anything guy recommend sure thoroughly read thanks
Training nltk tagger with Indian POS data,"<p>I am trying to train nltk trainer with indian corpus. I mainly targetting <code>telugu.pos</code></p>

<p>I followed <a href=""http://nltk-trainer.readthedocs.io/en/latest/train_tagger.html"" rel=""nofollow noreferrer"">http://nltk-trainer.readthedocs.io/en/latest/train_tagger.html</a> and trained. Here is the snapshot<a href=""https://i.sstatic.net/plUBl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/plUBl.png"" alt=""enter image description here""></a></p>

<p>When I tried to test it with telugu text. <code>‡∞®‡∞æ ‡∞™‡±á‡∞∞‡±Å ‡∞ï‡∞∞‡±Ä‡∞Ç ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞®‡±á‡∞®‡±Å ‡∞≠‡∞æ‡∞∞‡∞§ ‡∞Ü‡∞π‡∞æ‡∞∞ ‡∞™‡±ç‡∞∞‡±á‡∞Æ.</code> which is in English <code>My name is Karim. I love Indian food.</code>. It gives this error.</p>

<p><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xe0 in position 0: ordinal not in range(128)</code></p>

<p>Am I doning somethign wrong?</p>

<p><a href=""https://i.sstatic.net/EercU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EercU.png"" alt=""enter image description here""></a></p>

<p>Edit</p>

<p>I edited the text</p>

<pre><code>sent = ""‡∞®‡∞æ ‡∞™‡±á‡∞∞‡±Å ‡∞ï‡∞∞‡±Ä‡∞Ç ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞®‡±á‡∞®‡±Å ‡∞≠‡∞æ‡∞∞‡∞§ ‡∞Ü‡∞π‡∞æ‡∞∞ ‡∞™‡±ç‡∞∞‡±á‡∞Æ."".decode('utf-8')
</code></pre>

<p>Now it gives result like</p>

<pre><code>&gt;&gt;&gt; text = nltk.word_tokenize(sent)
&gt;&gt;&gt; text
[u'\u0c28\u0c3e', u'\u0c2a\u0c47\u0c30\u0c41', u'\u0c15\u0c30\u0c40\u0c02', u'\u0c09\u0c02\u0c26\u0c3f', u'.', u'\u0c28\u0c47\u0c28\u0c41', u'\u0c2d\u0c3e\u0c30\u0c24', u'\u0c06\u0c39\u0c3e\u0c30', u'\u0c2a\u0c4d\u0c30\u0c47\u0c2e', u'.']
&gt;&gt;&gt; nltk.pos_tag(text)
[(u'\u0c28\u0c3e', 'JJ'), (u'\u0c2a\u0c47\u0c30\u0c41', 'NNP'), (u'\u0c15\u0c30\u0c40\u0c02', 'NNP'), (u'\u0c09\u0c02\u0c26\u0c3f', 'NNP'), (u'.', '.'), (u'\u0c28\u0c47\u0c28\u0c41', 'VB'), (u'\u0c2d\u0c3e\u0c30\u0c24', 'JJ'), (u'\u0c06\u0c39\u0c3e\u0c30', 'NNP'), (u'\u0c2a\u0c4d\u0c30\u0c47\u0c2e', 'NNP'), (u'.', '.')]
</code></pre>

<p>How can I print this content into original language?</p>
",Multilingual Language Processing & Language Identification,training nltk tagger indian po data trying train nltk trainer indian corpus mainly targetting followed trained snapshot tried test telugu text english give error doning somethign wrong edit edited text give result like print content original language
How to do POS tagging on Telugu text?,"<p>I am doing English POS tagging for so long. It's staright forward like</p>

<pre><code>&gt;&gt;&gt; text = word_tokenize(""And now for something completely different"")
&gt;&gt;&gt; nltk.pos_tag(text)
[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),
('completely', 'RB'), ('different', 'JJ')]
</code></pre>

<p>But I want to do it in Telugu. </p>

<p>I followed this article - <a href=""http://jaganadhg.freeflux.net/blog/archive/2009/10/12/nltk-and-indian-language-corpus-processing-part-ii.html"" rel=""nofollow noreferrer"">http://jaganadhg.freeflux.net/blog/archive/2009/10/12/nltk-and-indian-language-corpus-processing-part-ii.html</a></p>

<p>And could test few inbuilt sentences. 
<a href=""https://i.sstatic.net/syBjY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/syBjY.png"" alt=""enter image description here""></a></p>

<p>But I could not figure out the way to test any random Telugu text. Could some one please guide if he has experience in using NLTK for non english text.</p>

<p>I have total number words</p>

<pre><code>telugu.pos
    9999
</code></pre>

<p>sentences</p>

<pre><code>1197
telugu.pos
</code></pre>
",Multilingual Language Processing & Language Identification,po tagging telugu text english po tagging long staright forward like want telugu followed article could test inbuilt sentence could figure way test random telugu text could one please guide ha experience using nltk non english text total number word sentence
Is Arabic supported by all annotations in Stanford NLP?,"<p>I've used almost all provided annotations by Stanford NLP on English, but I want to work on Arabic now.</p>

<p>The question is, <strong>Could I use all annotations like</strong> <code>tokenize, ssplit, pos, lemma, ner, parse,dcoref</code> <strong>on Arabic too</strong>?</p>

<p><strong>Update:</strong></p>

<p>I want to know if Arabic has supported annotations like English or not, or is the annotations work on Arabic as well on English?</p>
",Multilingual Language Processing & Language Identification,arabic supported annotation stanford nlp used almost provided annotation stanford nlp english want work arabic question could use annotation like arabic update want know arabic ha supported annotation like english annotation work arabic well english
Is it possible to seperate a string into sentences using context?,"<p>Is it possible to seperate string to multiple sentences using NLP in python by string contexts.</p>

<p>Input:</p>

<p>string=""Chinese people use social media called weibo in China it is the same as Twitter.""</p>

<p>Output:</p>

<ul>
<li>Chinese people use social media called weibo in China</li>
<li>it is the same as Twitter.</li>
</ul>

<p>Or can we set the boundary of the sentences.</p>

<p>I have tried using tokenization, but using that I can only seperate string by punctuations, whitespace, blanklines etc.</p>
",Multilingual Language Processing & Language Identification,possible seperate string sentence using context possible seperate string multiple sentence using nlp python string context input string chinese people use social medium called weibo china twitter output chinese people use social medium called weibo china twitter set boundary sentence tried using tokenization using seperate string punctuation whitespace blanklines etc
API for check spelling in Android,"<p>I'm learning about natural language processing, and are required to make a project.
I'm trying to build an application that checks spelling for Vietnamese. However, I just stopped in putting data into the software. I do not know how to split the input data and how to write the code to checks spelling.
I know, Android have APIs support this. Who can recommend me an API and how to use it? Thanks a lot!</p>
",Multilingual Language Processing & Language Identification,api check spelling android learning natural language processing required make project trying build application check spelling vietnamese however stopped putting data software know split input data write code check spelling know android apis support recommend api use thanks lot
Lucene English tokenizer gives weird words,"<p>I am using english tokenizer to parse out tokens and I am facing a weird situation where words like really/reply gets converted to realli, repli.</p>

<p>Below is the code snippet that I am using.</p>

<pre><code>object Learning {

  def tokenize(content: String): Seq[String] = {
    val tReader = new StringReader(content)
    val analyzer = new EnglishAnalyzer()
    val tStream = analyzer.tokenStream(""contents"", tReader)
    val term = tStream.addAttribute(classOf[CharTermAttribute])
    tStream.reset()

    val result = mutable.ArrayBuffer.empty[String]
    while(tStream.incrementToken()) {
      result += term.toString
    }
    result
  }

  def main(args: Array[String]): Unit = {
    println(tokenize(""This deal looks really interesting, I will look into it and reply""))
  }

}
</code></pre>

<p>This prints out as - ArrayBuffer(deal, look, realli, interest, i, look, repli). As far as I can see, there are no words such as realli,repli in the english language.</p>

<p>Can anybody point why this is giving output in such a way?</p>
",Multilingual Language Processing & Language Identification,lucene english tokenizer give weird word using english tokenizer parse token facing weird situation word like really reply get converted realli repli code snippet using print arraybuffer deal look realli interest look repli far see word realli repli english language anybody point giving output way
Simplify a Logic Expression using NLTK,"<p>I have a doubt at using Natural Language ToolKit (NLTK). I'm trying to make an app in order to translate a Natural Language Question into it's logic representation, and query to a database.</p>

<p>The result I got after using the simplify() method under nltk.sem.logic package and got the following expression:</p>

<pre><code>exists z2.(owner(fido, z2) &amp; (z0 = z2))
</code></pre>

<p>But what I need is to simplify it as follow:</p>

<pre><code>owner(fido, z0)
</code></pre>

<p>Is there another method that could reduce the sentence as I want?</p>
",Multilingual Language Processing & Language Identification,simplify logic expression using nltk doubt using natural language toolkit nltk trying make app order translate natural language question logic representation query database result got using simplify method nltk sem logic package got following expression need simplify follow another method could reduce sentence want
NLP. Split sentence into parts,"<p>Good example of what I want to do is lingvolive.com. When you type your sentence it shows to you translation of each part of sentence.</p>

<p><strong>Screenshot of lingvolive</strong>
<img src=""https://i.sstatic.net/D7aUG.png"" alt=""abc""></p>

<p>There is a sentence: <code>""Never give up!""</code> and I want to split in into parts <code>[""never"", ""give up""]</code>. Or <code>""Consider the larger picture beyond narrowly focused goal""</code> into <code>[""Consider"", ""the"", ""larger picture"", ""beyond"", ""narrowly"", ""focused"" ,""goal""]</code></p>

<p>For this task I have big labeled database parsed from englishcentral.com. (<a href=""http://www.englishcentral.com/api/bridge/content/dialog/26882?complete=true&amp;siteLanguage=en"" rel=""nofollow"">Example</a>) There for every word/phrase specified its part of speech.</p>

<p>It's no matter it's a phrasal verb or idiom, I just need to extract groups of words from sentence.</p>

<p>Any ideas how to do that?</p>
",Multilingual Language Processing & Language Identification,nlp split sentence part good example want lingvolive com type sentence show translation part sentence screenshot lingvolive sentence want split part task big labeled database parsed englishcentral com example every word phrase specified part speech matter phrasal verb idiom need extract group word sentence idea
How to prepare Google Natural Language Proscessing output (json) for Big Query,"<p>I'm trying to query the output of a Natural Language Processing (NLP) call in Big Query (BQ) but I'm struggling to get the output in the right format for BQ.</p>

<p>I understand that BQ takes json files (as newline delimited) - but just not sure that (a) the output of NLP is json newline delimited and (b) if my schema is correct. </p>

<p>Here's the json output I'm working with:</p>

<pre><code>{
  ""entities"": [
    {
      ""name"": ""Rowling"",
      ""type"": ""PERSON"",
      ""metadata"": {
        ""wikipedia_url"": ""http://en.wikipedia.org/wiki/J._K._Rowling""
      },
      ""salience"": 0.65751493,
      ""mentions"": [
        {
          ""text"": {
            ""content"": ""   J."",
            ""beginOffset"": -1
          }
        },
        {
          ""text"": {
            ""content"": ""K. Rowl"",
            ""beginOffset"": -1
          }
        }
      ]
    },
    {
      ""name"": ""LONDON"",
      ""type"": ""LOCATION"",
      ""metadata"": {
        ""wikipedia_url"": ""http://en.wikipedia.org/wiki/London""
      },
      ""salience"": 0.14284456,
      ""mentions"": [
        {
          ""text"": {
            ""content"": ""\ufeffLON"",
            ""beginOffset"": -1
          }
        }
      ]
    },
    {
      ""name"": ""Harry Potter"",
      ""type"": ""WORK_OF_ART"",
      ""metadata"": {
        ""wikipedia_url"": ""http://en.wikipedia.org/wiki/Harry_Potter""
      },
      ""salience"": 0.0726779,
      ""mentions"": [
        {
          ""text"": {
            ""content"": ""th Harry Pot"",
            ""beginOffset"": -1
          }
        },
        {
          ""text"": {
            ""content"": ""‚ÄòHarry Pot"",
            ""beginOffset"": -1
          }
        }
      ]
    },
    {
      ""name"": ""Deathly Hallows"",
      ""type"": ""WORK_OF_ART"",
      ""metadata"": {
        ""wikipedia_url"": ""http://en.wikipedia.org/wiki/Harry_Potter_and_the_Deathly_Hallows""
      },
      ""salience"": 0.022565609,
      ""mentions"": [
        {
          ""text"": {
            ""content"": ""he Deathly Hall"",
            ""beginOffset"": -1
          }
        }
      ]
    }
  ],
  ""language"": ""en""
}
</code></pre>

<p>Is there a way to send the output directly to big query via the command line in Google Cloud shell?</p>

<p>Any information would be greatly appreciated!</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,prepare google natural language proscessing output json big query trying query output natural language processing nlp call big query bq struggling get output right format bq understand bq take json file newline delimited sure output nlp json newline delimited b schema correct json output working way send output directly big query via command line google cloud shell information would greatly appreciated thanks
Compute probability of sentence with out of vocabulary words,"<p>I trained Ngram language models (unigram and bigram) on a corpus of English and I'm trying to compute the probabilities of sentences from a disjoint corpus.</p>

<p>For example, the training corpus consists of the 3 sentences:</p>

<p><strong>1: I, am, Sam</strong></p>

<p><strong>2: Sam, I, am</strong></p>

<p><strong>3: I, do, not, like, green, eggs, and, ham</strong></p>

<p>N = 14 (length of the corpus)</p>

<p>For unigram, I end up with probabilities:</p>

<p>Pr(""i"") = #(""i"") / N = 3/14, Pr(""am"") = 2/14, Pr(""like"") = 1/14, and so forth...</p>

<p>For bigram, I end up with probabilities:</p>

<p>Pr(""am""|""i"") = 2/3, Pr(""do""|""i"") = 1/3, and so forth...</p>

<p>Now, I'm trying to compute the probability of the following sentence where not all ngrams (uni or bi) appear in the training corpus:</p>

<p><strong>I, ate, a, burrito</strong></p>

<p>For unigram, I need the following probability estimates:</p>

<p>Pr(""i""), Pr(""ate""), Pr(""a""), and Pr(""burrito"")</p>

<p>and for bigram, I need the following probabilities estimates:</p>

<p>Pr(""ate""|""i""), Pr(""a""|""ate""), Pr(""burrito""|""a"")</p>

<p>Apparently not all unigrams (""ate"", ""burrito"") and bigrams (like (""i"", ""ate"")) appear in the training corpus.</p>

<p>I understand that you can do smoothing (like add-one smoothing) to deal with these cases:</p>

<p>For example, the vocabulary of the training corpus is</p>

<p><strong>i, am, sam, do, not, like, green, eggs, and, ham</strong></p>

<p>and you can expand the vocabulary by including new words from the new sentence:</p>

<p><strong>ate, a, burrito</strong></p>

<p>So the size of the expanded vocabulary would be V = 13</p>

<p>So for unigram, the original probability estimates Pr(w_i) = #(w_i)/N would be turned into (#(w_i) + 1) / (N + V)</p>

<p>So Pr(""i"") = 4/27, Pr(""am"") = 3/27, Pr(""sam"") = 3/27, Pr(""do"") = 2/27, Pr(""not"") = 2/27, Pr(""like"") = 2/27, Pr(""green"") = 2/27, Pr(""eggs"") = 2/27, Pr(""and"") = 2/27, Pr(""ham"") = 2/27</p>

<p>And for the 3 new words:
Pr(""ate"") = 1/27, Pr(""a"") = 1/27, Pr(""burrito"") = 1/27</p>

<p>And the these probabilities would still sum to 1.0</p>

<p>Though this can handle the cases where some ngrams were not in the original training set, you would have to know the set of ""new"" words when you estimate the probabilities using (#(w_i) + 1) / (N + V) (V = sum of vocabulary of the original training set (10), and the test corpus (3)). I think this is equivalent to assuming the all new unigram or bigram in the test corpus occur only once, no matter how many times they actually occur.</p>

<p>My question is this the way out-of-vocabulary tokens are typically handled when computing the probability of a sentence? </p>

<p>The NLTK module nltk.module.NGramModel seem be have been removed due to bugs <a href=""https://stackoverflow.com/questions/26443084/is-there-an-alternate-for-the-now-removed-module-nltk-model-ngrammodel"">nltk ngram model</a>, so I have to implement on my own. Another question: is there python modules other than NLTK that implements Ngram training and computing probability of a sentence ? </p>

<p>Thanks in advance!</p>
",Multilingual Language Processing & Language Identification,compute probability sentence vocabulary word trained ngram language model unigram bigram corpus english trying compute probability sentence disjoint corpus example training corpus consists sentence sam sam like green egg ham n length corpus unigram end probability pr n pr pr like forth bigram end probability pr pr forth trying compute probability following sentence ngrams uni bi appear training corpus ate burrito unigram need following probability estimate pr pr ate pr pr burrito bigram need following probability estimate pr ate pr ate pr burrito apparently unigrams ate burrito bigram like ate appear training corpus understand smoothing like add one smoothing deal case example vocabulary training corpus sam like green egg ham expand vocabulary including new word new sentence ate burrito size expanded vocabulary would v unigram original probability estimate pr w w n would turned w n v pr pr pr sam pr pr pr like pr green pr egg pr pr ham new word pr ate pr pr burrito probability would still sum though handle case ngrams original training set would know set new word estimate probability using w n v v sum vocabulary original training set test corpus think equivalent assuming new unigram bigram test corpus occur matter many time actually occur question way vocabulary token typically handled computing probability sentence nltk module nltk module ngrammodel seem removed due bug href ngram model implement another question python module nltk implement ngram training computing probability sentence thanks advance
How can I POS tag German texts?,"<p>I've been doing some natural language processing work. </p>

<p>For English POS tagging, it's rather simple because I only need to use built-in nltk functions. I want to process German texts similarly.</p>

<p>Since nltk doesn't have a built-in function for German, I've tried using Stanford POSTagger:</p>

<pre><code>from nltk.tag.stanford import StanfordPOSTagger
import os
import nltk
java_path = ""C:/Program Files/Java/jdk1.8.0_71/bin/java.exe""
os.environ['JAVAHOME'] = java_path
sentence = ""Man k√∂nnte Kl√∂ckner vorhalten, sich an ihre eigenen Appelle nicht zu halten. Doch niemand in der Union wagte das. Nicht einmal die von ihr attackierten Briefschreiber. Kl√∂ckner genie√üt im Moment Narrenfreiheit.""
tokens = nltk.word_tokenize(sentence, 'german')
german_postagger1 = StanfordPOSTagger(r'E:/python/nlptest/models/german-hgc.tagger', r'E:/python/nlptest/stanford-postagger.jar')
gp1 = german_postagger1.tag(tokens)
</code></pre>

<p>It takes almost 7 seconds to finish processing, which is unbearable for me.</p>

<p>I also tried the module <a href=""http://www.clips.ua.ac.be/pages/pattern-de"" rel=""nofollow"">Pattern</a>, but it doesn't support Python 3 and I'm using Python 3.4.</p>

<p>Is there an alternative and faster way to POS tag German sentences?</p>
",Multilingual Language Processing & Language Identification,po tag german text natural language processing work english po tagging rather simple need use built nltk function want process german text similarly since nltk built function german tried using stanford postagger take almost second finish processing unbearable also tried module pattern support python using python alternative faster way po tag german sentence
Method to text analytics varchar(4000),"<p>In SQL Azure we have 20 million rows of English text stored in varchar(4000) describing why customers calling us for maintenance. Some examples are:</p>

<ol>
<li>My windows is broken by a cat whose eyes are green.</li>
<li>The toilet is blocked and the kitchen is now full of bugs! This is worst nightmare i ever had!!!!</li>
<li>Please repair the bed. </li>
</ol>

<p>The main purpose to be achieved is that we could identify most frequent words in the column, i.e. we know that Windows appears 1 million time in the database and bed repair 5 times only , therefore we could plan for stocks, hiring and training. Data are in Azure SQL. Could you recommend the best tool we should use in this scenario?</p>
",Multilingual Language Processing & Language Identification,method text analytics varchar sql azure million row english text stored varchar describing customer calling u maintenance example window broken cat whose eye green toilet blocked kitchen full bug worst nightmare ever please repair bed main purpose achieved could identify frequent word column e know window appears million time database bed repair time therefore could plan stock hiring training data azure sql could recommend best tool use scenario
Methods of calculating text string similarity?,"<p>Let‚Äôs say I have an array of strings and I need to sort them into clusters. I am currently doing the analysis using n-grams, e.g.:</p>

<p>Cluster 1:</p>

<ul>
<li>Pipe fixing

<ul>
<li>Pipe fixing in Las Vegas</li>
<li>Movies about Pipe fixing</li>
</ul></li>
</ul>

<p>Cluster 2:</p>

<ul>
<li>Classical music

<ul>
<li>Why classical music is great</li>
<li>What is classical music</li>
</ul></li>
</ul>

<p>etc.</p>

<p>Let‚Äôs say within this array I have these two strings of text (among others):</p>

<ul>
<li>Japanese students</li>
<li>Students from Japan</li>
</ul>

<p>Now, the N-gram method will obviously not put these two strings together, as they do not share the same tokenized structure. I tried using Damerau-Levenshtein distance calculation and TF/IDF, but both grab too much outer noise. Which other techniques can I use to understand that these two strings belong within a single cluster?</p>
",Multilingual Language Processing & Language Identification,method calculating text string similarity let say array string need sort cluster currently analysis using n gram e g cluster pipe fixing pipe fixing la vega movie pipe fixing cluster classical music classical music great classical music etc let say within array two string text among others japanese student student japan n gram method obviously put two string together share tokenized structure tried using damerau levenshtein distance calculation tf idf grab much outer noise technique use understand two string belong within single cluster
Search Open Multilingual Wordnet with JWI,"<p>Does JWI support the Open Multilingual Wordnet or (even better) the Extended Open Multilingual Wordnet? Both of those data sets look great, but they extremely clunky as is. It looks like I would have to parse and aggregate a dozen (or hundred in the case of EOMW) indices and merge the same number of dump files. Has anyone done this using JWI? Is there an alternative implementation that more easily supports either multilingual wordnets? They show a demo in SQL -- is a dump available anywhere?</p>
",Multilingual Language Processing & Language Identification,search open multilingual wordnet jwi doe jwi support open multilingual wordnet even better extended open multilingual wordnet data set look great extremely clunky look like would parse aggregate dozen hundred case eomw index merge number dump file ha anyone done using jwi alternative implementation easily support either multilingual wordnet show demo sql dump available anywhere
count words in all the classes of a file,"<p>I have some files in Persian. All of them contain a lot of sentences, then a ""tab"", then a Persian word, again a ""tab"" and then an English word. The English words show each sentence class. I have to count the number of each word in Persian sentences in all of the classes. For example, how many times the word ""ÿØÿßŸÜÿ¥⁄ØÿßŸá"" occurs in ""passion"" class and how many times it occurs in ""salty"" class. (some files have more than 2 classes). The code I wrote counts the words just once in the file. How can I change it that it returns the words count, as I described above? (hint: I JUST need the count of words in the sentences NOT the Persian and English words after ""tab""). </p>

<p><a href=""https://i.sstatic.net/OF3RF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OF3RF.png"" alt=""enter image description here""></a></p>

<pre><code>from collections import Counter

corpus = []
with open(""T.txt"", encoding='utf-8') as f:
    for line in f:
        t = line.strip().split(""\t"")
        corpus.append (t)
        for row in corpus:
            wordcount = Counter(row[0].split())
        print (wordcount)
</code></pre>

<p><a href=""https://www.dropbox.com/s/r88hglemg7aot0w/F.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/r88hglemg7aot0w/F.txt?dl=0</a></p>

<p><a href=""https://i.sstatic.net/huk7J.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/huk7J.png"" alt=""enter image description here""></a></p>

<p>The result is as above picture. But the thing I want should be like below for all of the words:
passion {""ÿØÿßŸÜÿ¥⁄ØÿßŸá"": 1 , ...}
salty {""ÿØÿßŸÜÿ¥⁄ØÿßŸá"": 0, .....}</p>
",Multilingual Language Processing & Language Identification,count word class file file persian contain lot sentence tab persian word tab english word english word show sentence class count number word persian sentence class example many time word occurs passion class many time occurs salty class file class code wrote count word file change return word count described hint need count word sentence persian english word tab result picture thing want like word passion salty
french lemmatization Core NLP,"<p>I'm trying to use Stanford CoreNLP for French texts.
I have two questions: </p>

<ol>
<li>I want to know if french lemmatization is available with Core NLP?</li>
<li>In some cases the output dependencies do not make sense for example for the sentence <code>""Le chat mange la souris""</code>  (the cat is eating the mouse) there is a problem in the token <code>""mange""</code> which is typed as adj and not verb, for that it's not considered as the root of sentence.
But when I use the plurial <code>""Les chats mangent la souris""</code> it's correct.</li>
</ol>

<p>Any help would be appreciated!</p>
",Multilingual Language Processing & Language Identification,french lemmatization core nlp trying use stanford corenlp french text two question want know french lemmatization available core nlp case output dependency make sense example sentence cat eating mouse problem token typed adj verb considered root sentence use plurial correct help would appreciated
Natural language processing - get data about animals from text,"<p>I'm looking for some advice about natural language processing.
I wanna do some research but i'm not sure what i'm researching. Sounds awkward but..
Imagine i have a text about an animal. It contains sentences like </p>

<blockquote>
  <p>""Dogs live at the northpole. They are about <code>1-3m</code> long.""</p>
</blockquote>

<p>And stuff like that. Not only about dogs but a bunch of texts describing animals in words.
Now i'm looking for something that analyses the text and recognizes ""<code>keywords</code>"" like <code>""live""</code> or <code>""long""</code> and then kinda collects the data and provides something like a data sheet for the animal like you get when you buy a new printer.</p>

<p>So i'm not looking for a tool or something like that (but wouldn't mind) i need more like some advice about keywords i could do some research on. pretty tough starting with a search about nlp.
thanks in advance!!</p>
",Multilingual Language Processing & Language Identification,natural language processing get data animal text looking advice natural language processing wan na research sure researching sound awkward imagine text animal contains sentence like dog live northpole long stuff like dog bunch text describing animal word looking something analysis text recognizes like kinda collect data provides something like data sheet animal like get buy new printer looking tool something like mind need like advice keywords could research pretty tough starting search nlp thanks advance
count words of different classes separately in a text file,"<p>I have some text files in Persian. each file contains a lot of sentences, each in a new line. And in front of each sentence there is a tab, then a word, then a tab and then an English word. These English words in some files are 2, in some are 3, in some are 5 and in some other, are more or less. Actually, they show the sentences classes. I have to count each class's total words separately (just count the words of sentences not the words after them). For that I have to change the file to a list, so that I can achieve the sentences. Now the problem is that, how should I write the code that it returns each class's total words separately. Below is the sample sentences. </p>

<p><a href=""https://i.sstatic.net/lMLsP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lMLsP.png"" alt=""enter image description here""></a></p>

<pre><code>corpus = []
def CountWords (file):
    with open (file, encoding = ""utf-8"") as f1:
         for line in f1:
             t = line.strip().split(""\t"")
             corpus.append(t)
             for row in corpus:
                 if row[2] != row[2]:
</code></pre>

<p>Now I don't know how to continue. I appreciate a lot if someone can help. (I have no background in programming).</p>
",Multilingual Language Processing & Language Identification,count word different class separately text file text file persian file contains lot sentence new line front sentence tab word tab english word english word file le actually show sentence class count class total word separately count word sentence word change file list achieve sentence problem write code return class total word separately sample sentence know continue appreciate lot someone help background programming
Better Database Design for a Hierarchical Structure?,"<p>I've created a bilingual dictionary app<sup>1</sup>, and it's currently very simple, but we're going to be starting to develop the entries more fully and I'm trying to figure out the best database structure for it. Previous dictionary projects I've worked on have used xml (since dictionary entries are largely hierarchical), but I need to do it using a database.<sup>2</sup></p>

<p>This is what a typical, medium-complexity entry would look like (simplified a bit):
<hr />
<b>dar</b><br>
<em>/dƒÅr/</em></p>

<ul>
<li>noun

<ol>
<li>house, dwelling, abode<br>
<em>ar-rƒÅjl dkhul ad-dƒÅr</em>, ""The man entered the house.""</li>
<li>home<br>
<em>rja∆∑na lid-dƒÅr</em>, ""We returned home.""</li>
</ol></li>
<li>verb

<ol>
<li>to turn<br>
<em>d≈´r li-yamƒ´n</em>, ""Turn right.""</li>
<li>to turn around/about</li>
</ol></li>
</ul>

<hr />

<p>As you can see, one word can have multiple parts of speech, so ""part of speech"" can't simply be an attribute of Entry, it has to be related to the senses. Each pos can have multiple senses (numbered), and of course each sense could have multiple near-synonymous translations. Senses may also have example sentences (possibly more than one), but not always. Thinking of how the entry parts relate to each other, I came up with the following structure, using five tables:  </p>

<pre><code>Entry
-id
-headword
-pronunciation
-...

PartOfSpeech
-id
-entry (ForeignKey)
-pos

Sense
-id
-sense_number
-part_of_speech (ForeignKey)
-...

Translation
-id
-tr
-sense (ForeignKey)
-...

Example
-id
-ex
-ex_tr
-sense (ForeignKey)
-...
</code></pre>

<p>Or, in other words:</p>

<pre><code>                                  _ Translation
Entry -- PartOfSpeech -- Sense --|
                                  - Example
</code></pre>

<p>This seems simple and makes sense to me, but I'm wondering if it will be too complicated in the execution. For instance, to display a selection of entries, I would need to write several nested <code>for</code> loops (<code>for e in entries ‚Üí for p in pos ‚Üí for s in senses ‚Üí for tr in translations</code>) ‚Äî and all with reverse lookups! </p>

<p>And I don't think I could even edit a whole entry in the Django admin (unless it lets you somehow do an Inline of an Inline of an Inline). I'm going to build an editor interface anyway, but it's nice to be able to check things on the admin site when you want to.</p>

<p>Is there a better way to do this? I feel like there must be something clever that I'm missing.</p>

<p>Thanks,
Karen</p>

<p><hr/>
<sup>1</sup> If you're curious: <a href=""http://tunisiandictionary.org"" rel=""nofollow"">tunisiandictionary.org</a>. In its simple, current form it only has two tables (Entry, Sense), with the translations just comma-delineated in a single  field. Which is bad.<br>
<sup>2</sup> For two reasons: 1) because it's a web app I've written with Python/Django, and 2) because I hate xml.</p>
",Multilingual Language Processing & Language Identification,better database design hierarchical structure created bilingual dictionary app currently simple going starting develop entry fully trying figure best database structure previous dictionary project worked used xml since dictionary entry largely hierarchical need using database typical medium complexity entry would look like simplified bit dar r noun house dwelling abode ar r jl dkhul ad r man entered house home rja na lid r returned home verb turn r li yam n turn right turn around see one word multiple part speech part speech simply attribute entry ha related sens po multiple sens numbered course sense could multiple near synonymous translation sens may also example sentence possibly one always thinking entry part relate came following structure using five table word seems simple make sense wondering complicated execution instance display selection entry would need write several nested loop reverse lookup think could even edit whole entry django admin unless let somehow inline inline inline going build editor interface anyway nice able check thing admin site want better way feel like must something clever missing thanks karen curious tunisiandictionary org simple current form ha two table entry sense translation comma delineated single field bad two reason web app written python django hate xml
Which sub-topic of Natural Language Processing will help me do this?,"<p>What I am trying to do is identify the context of the query a user might input. So if the user enters ""High Proteins"", I want to be able to understand that what he means by that is ""protein > certain_threshold"".</p>

<p>Example 2: User input : ""Calories less than 250""
I should be able to understand that what the user means by this is calories &lt; 250</p>

<p>If I am able to do this, I will be able to construct my queries accordingly. Which sub-topic of NLP will help me do this. Any leads woul be greatly appreciated.</p>
",Multilingual Language Processing & Language Identification,sub topic natural language processing help trying identify context query user might input user enters high protein want able understand mean protein certain threshold example user input calorie le able understand user mean calorie able able construct query accordingly sub topic nlp help lead woul greatly appreciated
German corenlp model defaulting to english models,"<p>I use the following command to serve a  corenlp server for German language models which are downloaded as jar in the classpath , but it does not output german tags or parse but loads only english models:</p>

<pre><code> java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer   -props ./german.prop
</code></pre>

<p>german.prop contents:</p>

<pre><code>annotators = tokenize, ssplit, pos, depparse, parse

tokenize.language = de

pos.model = edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger

ner.model = edu/stanford/nlp/models/ner/german.hgc_175m_600.crf.ser.gz
ner.applyNumericClassifiers = false
ner.useSUTime = false

parse.model = edu/stanford/nlp/models/lexparser/germanFactored.ser.gz
depparse.model = edu/stanford/nlp/models/parser/nndep/UD_German.gz
</code></pre>

<p>client command:</p>

<pre><code>wget --post-data ' Meine Mutter ist aus Wuppertal' 'localhost:9000/?properties""=""{""tokenize.whitespace"":""true"",""annotators"":""tokenize, ssplit, pos, depparse, parse"",""outputFormat"":""text"",""tokenize.language"" :""de"" ,
 ""pos.model"":"" edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger"",
""depparse.model"" : ""edu/stanford/nlp/models/parser/nndep/UD_German.gz"",
""parse.model"" : ""edu/stanford/nlp/models/lexparser/germanFactored.ser.gz""

 }' -O -
</code></pre>

<p>I get following incorrect output:</p>

<pre><code> {""dep"":""dep"",""governor"":4,""governorGloss"":""aus"",""dependent"":5,""dependentGloss"":""Wuppertal""}],""openie"":[{""subject"":""Wuppertal"",""subjectSpan"":[4,5],""relation"":""is ist aus of"",""relationSpan"":[2,4],""object"":""Meine Mutter"",""objectSpan"":[0,2]}],""tokens"":[{""index"":1,""word"":""Meine"",""originalText"":""Meine"",""lemma"":""Meine"",""characterOffsetBegin"":1,""characterOffsetEnd"":6,""pos"":""NNP"",""ner"":""PERSON"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":2,""word"":""Mutter"",""originalText"":""Mutter"",""lemma"":""Mutter"",""characterOffsetBegin"":7,""characterOffsetEnd"":13,""pos"":""NNP"",""ner"":""PERSON"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":3,""word"":""ist"",""originalText"":""ist"",""lemma"":""ist"",""characterOffsetBegin"":14,""characterOffsetEnd"":17,""pos"":""NN"",""ner"":""O"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":4,""word"":""aus"",""originalText"":""aus"",""lemma"":""aus"",""characterOffsetBegin"":18,""characterOffsetEnd"":21,""pos"":""NN"",""ner"":""O"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":5,""word"":""Wuppertal"",""originalText"":""Wuppertal"",""lemma"":""Wuppertal"",""characterOffsetBegin"":22,""characterOffsetEnd"":31,""pos"":""NNP"",""ner"":""LOCATI100%[==========================================================================&gt;] 2,
</code></pre>

<p>in the server log I see it loads english models eventhough it lists german models on startup:</p>

<pre><code>pos.model=edu/stanford/nlp/models/pos-tagger/ge...
parse.model=edu/stanford/nlp/models/lexparser/ger...
tokenize.language=de
depparse.model=edu/stanford/nlp/models/parser/nndep/...
annotators=tokenize, ssplit, pos, depparse, parse
Starting server on port 9000 with timeout of 5000 milliseconds.
StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[/203.:61563] API call w/annotators tokenize,ssplit,pos,depparse
Die Katze liegt auf der Matte.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.5 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...
PreComputed 100000, Elapsed Time: 1.396 (s)
</code></pre>

<p>The following question for same error in french models also points to the same problem but even after following , it does not resolve the problem for the server case, I am able to get the correct output without using the server and just using the <code>edu.stanford.nlp.pipeline.StanfordCoreNLP command</code> , it is the server command <code>edu.stanford.nlp.pipeline.StanfordCoreNLPServer</code> which defaults to english:
<a href=""https://stackoverflow.com/questions/36223002/french-dependency-parsing-using-corenlp#"">French dependency parsing using CoreNLP</a></p>
",Multilingual Language Processing & Language Identification,german corenlp model defaulting english model use following command serve corenlp server german language model downloaded jar classpath doe output german tag parse load english model german prop content client command get following incorrect output server log see load english model eventhough list german model startup following question error french model also point problem even following doe resolve problem server case able get correct output without using server using server command default english href dependency parsing using corenlp
Natural language processing library in java,"<p>For my small POC, I am looking for any open source library using which i can make use of natural language processing, preferably in java. Basically i am planning to have a application which takes input from user in human language and  return the results by filtering through supplied documents or web. Any leads will be appreciated. </p>
",Multilingual Language Processing & Language Identification,natural language processing library java small poc looking open source library using make use natural language processing preferably java basically planning application take input user human language return result filtering supplied document web lead appreciated
Cross-Lingual Word Sense Disambiguation,"<p>I am a beginner in computer programming and I am completing an essay on Parallel Corpora in Word Sense Disambiguation. 
Basically,  I intend to show that substituting a sense for a word translation simplifies the process of identifying the meaning of ambiguous words. I have already word-aligned my parallel corpus (EUROPARL English-Spanish) with GIZA++, but I don't know what to do with the output files. My intention is to build a classifier to calculate the probability of a translation word given the contextual features of the tokens which surround the ambiguous word in the source text.
So, my question is: how do you extract instances of an ambiguous word from a parallel corpus WITH its aligned translation?</p>

<p>I have tried various scripts on Python, but these are run on the assumption that 1) the English and Spanish texts are in separate corpora and 2) the English and Spanish sentences share the same indexes, which obviously does not work. 
e.g.</p>

<pre><code>def ambigu_word2(document, document2):
    words = ['letter']
    for sentences in document:
        tokens = word_tokenize(sentences)
        for item in tokens:
            x = w_lemma.lemmatize(item)
            for w in words:
                if w == x in sentences:
                    print (sentences, document2[document.index(sentences)])
print (ambigu_word2(raw1, raw2))
</code></pre>

<p>I would be really grateful if you could provide any guidance on this matter.</p>
",Multilingual Language Processing & Language Identification,cross lingual word sense disambiguation beginner computer programming completing essay parallel corpus word sense disambiguation basically intend show substituting sense word translation simplifies process identifying meaning ambiguous word already word aligned parallel corpus europarl english spanish giza know output file intention build classifier calculate probability translation word given contextual feature token surround ambiguous word source text question extract instance ambiguous word parallel corpus aligned translation tried various script python run assumption english spanish text separate corpus english spanish sentence share index obviously doe work e g would really grateful could provide guidance matter
NLP extract dictionary words from sentence,"<p>I have used Standford NLP for identify POS of a given sentence.</p>

<p>I need to extract only proper words ( words defined in dictionary) from a sentence.</p>

<p>For eg: If the sentence is "" <em>I went to New York in Flight 6AWDR</em> ""</p>

<p>When I use standford NLP , both the words ""<em>flight</em>"" and ""<em>6AWDR</em>"" are marked as ""NNP""  by the PartOfSpeechAnnotation.class.</p>

<p>How can I extract proper english words alone in a sentence ?</p>
",Multilingual Language Processing & Language Identification,nlp extract dictionary word sentence used standford nlp identify po given sentence need extract proper word word defined dictionary sentence eg sentence went new york flight awdr use standford nlp word flight awdr marked nnp partofspeechannotation class extract proper english word alone sentence
calculate the totall number of uniqe words of the first column of a list,"<p>I have a file that consists of many Persian sentences. each line contains a sentence, then a ""tab"", then a word, again a ""tab"" and then an English word. I have to know just the number of unique words of the sentences (the words after tabs should not be in calculation). For that I changed the file to a list, so I have a list that contains a lot of lines and each line contains three indices; the sentence, a Persian word, an English word. Now I can achieve the sentences. The problem is that, the code I wrote returns the number of unique words of each line separately. For example if the file has 100 lines it returns 100 numbers, each in a new line. But I want the summation of all the numbers and have just one number which shows the total number of unique words. How can I change the code?</p>

<pre><code>from hazm import*

def WordsProbs (file):
    with open (file, encoding = ""utf-8"") as f1:
        normalizer = Normalizer()
        for line in f1:
        tmp = line.strip().split(""\t"")
        tmp[0] = normalizer.normalize(tmp[0])
        corpus.append(tmp)
   for row in corpus:
       UniqueWords = len(set(row[0].split()))
       print (UniqueWords)
</code></pre>

<p>The sample data:</p>

<p>ÿ®ÿßÿØ  ÿ®ÿßÿ±ÿ¥ ÿ®ÿ±ŸÅ Ÿàÿ≤ÿ¥ ÿ®ÿßÿØÿå ⁄©ŸàŸÑÿß⁄© €åÿÆÿ®ŸÜÿØÿßŸÜ ÿ≥ÿ∑ÿ≠  wind</p>
",Multilingual Language Processing & Language Identification,calculate totall number uniqe word first column list file consists many persian sentence line contains sentence tab word tab english word know number unique word sentence word tab calculation changed file list list contains lot line line contains three index sentence persian word english word achieve sentence problem code wrote return number unique word line separately example file ha line return number new line want summation number one number show total number unique word change code sample data wind
Differentiate between sentences that have the same meaning but use different word combinations,"<p>I am trying to learn Natural Language Processing and an stuck with an open ended question. How do I club together sentences that mean the same. There  can be a finite set of sentences that have the same meaning. What kind of algorithms do I use to club them?</p>

<p>For example: Consider the following sentences:</p>

<pre><code>There is a man. There is a lion. The lion will chase the man on seeing him. If the lion catches the man he dies.

There is a man and a lion. If the lion catches the man he dies. The lion will chase the man if he sees him.

You have a lion that chases men on seeing them. There is one man. If the lion catches the man he dies.
</code></pre>

<p>Basically what all these sentences say is this:</p>

<pre><code> 1 Lion. 1 Man. Lions chase men. If lion catches men the man dies.
</code></pre>

<p>I am unable to zero in on one category of Machine Learning or Deep Learning algorithm that would help me achieve something similar. Please guide me in the right direction or point me to some algorithms that are good enough to achieve this.</p>

<p>Another important factor is having a scale-able solution. There could be lots of such sentences out there. What happens then?</p>

<p>One possible solutions is:
Use the parts of speech and the relations between words in a sentence as features for some Machine Leaning algo. But will this be practical in a large set of sentences? Do we need to consider more things?</p>
",Multilingual Language Processing & Language Identification,differentiate sentence meaning use different word combination trying learn natural language processing stuck open ended question club together sentence mean finite set sentence meaning kind algorithm use club example consider following sentence basically sentence say unable zero one category machine learning deep learning algorithm would help achieve something similar please guide right direction point algorithm good enough achieve another important factor scale able solution could lot sentence happens one possible solution use part speech relation word sentence feature machine leaning algo practical large set sentence need consider thing
Works LibShortText with other languages too?,"<p>LibShortText is an open source tool for short-text classification and analysis. 
<a href=""http://www.csie.ntu.edu.tw/~cjlin/libshorttext/"" rel=""nofollow"">http://www.csie.ntu.edu.tw/~cjlin/libshorttext/</a></p>

<p>I have tried to figure out if it also works with other languages than english (e.g. german)? But I didn't find a hint.</p>

<p>Who knows the answer? Thank you in advance.</p>
",Multilingual Language Processing & Language Identification,work libshorttext language libshorttext open source tool short text classification analysis tried figure also work language english e g german find hint know answer thank advance
Is there a standard diagnostic for handling unknown words in FCG?,"<p>I have an FCG grammar for English and I'm parsing some text with out-of-vocabulary words. At this moment, I write my own customized diagnostics and repairs. Is there any standard way of treating unknown words in the latest FCG release?</p>
",Multilingual Language Processing & Language Identification,standard diagnostic handling unknown word fcg fcg grammar english parsing text vocabulary word moment write customized diagnostics repair standard way treating unknown word latest fcg release
How to check whether a phrase &quot;functions&quot; as noun in a sentence,"<p>In addition to noun and noun phrase, there are some other constructs in English that can also function as noun. Gerundive, for example, can be used as noun: you need good habits such as ""being polite"". </p>

<p>In an app I'm developing, I need to find all the components functioning as noun. I tried various chunking tools (NLTK, etc.) but they all seem to only recognize noun and noun phrase and not anything else. </p>

<p>These clunkers also don't recognize complements as part of NP, for example, ""the fact that she's alive"" will not be a single chunk even though they together act as noun in this sentence.</p>

<p>Is there any tool that can do trick like this? </p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,check whether phrase function noun sentence addition noun noun phrase construct english also function noun gerundive example used noun need good habit polite app developing need find component functioning noun tried various chunking tool nltk etc seem recognize noun noun phrase anything else clunkers also recognize complement part np example fact alive single chunk even though together act noun sentence tool trick like thanks
How to plot Dataset,"<p>Problem --- Clustering of english sentences.</p>

<p>Dataset --- Consists of English sentences.</p>

<p>Requirement --- For my interest I want to plot my data-set in a 2D or 3D graph before doing clustering. </p>

<p>What have i done so far --- I am working in python and have successfully created a sample-feature vector by using scikit CountVectorizer, where samples are english sentences and feature are set of words from these sentences.</p>

<p><code>text=[""machine learning"", ""Natural Language Processing"", ""Python and Vectors""]
vec = CountVectorizer(tokenizer=MyTokenizer(), analyzer = 'word')
fit_vec = vec.fit_transform(lines)
print fit_vec.toarray()</code></p>

<blockquote>
  <p>[[0 0 1 1 0 0 0 0]</p>
  
  <p>[0 1 0 0 1 1 0 0]</p>
  
  <p>[1 0 0 0 0 0 1 1]]</p>
  
  <p>[u'and', u'language', u'learning', u'machine', u'natural', u'processing', u'python, u'vectors']</p>
</blockquote>

<p>I want to use samples and these vectors into a 2-D graph where y-axis is these vectors and x axis are samples. The problem comes in expressing these vector in a single value along y-axis. I want to choose some point of reference with which I can calculate distance/angle of every vector and use these values along y-axis.</p>

<p>I may be wrong in my observations since i am very little in knowledge. That's why i have came here for all clarifications. </p>
",Multilingual Language Processing & Language Identification,plot dataset problem clustering english sentence dataset consists english sentence requirement interest want plot data set graph clustering done far working python successfully created sample feature vector using scikit countvectorizer sample english sentence feature set word sentence u u language u learning u machine u natural u processing u python u vector want use sample vector graph axis vector x axis sample problem come expressing vector single value along axis want choose point reference calculate distance angle every vector use value along axis may wrong observation since little knowledge came clarification
stanford Core NLP: Splitting sentences from text,"<p>I am new to stanford Core NLP. I would like to use it for splitting sentences from text in English, German,French. Which class does this work?Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,stanford core nlp splitting sentence text new stanford core nlp would like use splitting sentence text english german french class doe work thanks advance
Check whether a string is alphabetical for languages other than english,"<p>I am working with text data with a mix of several languages. Now trying to test whether a token/string is alphabetical, which means is potentially a word. 
Is there some built in function like <code>'somestring'.isAlpha()</code> to test whether a string is alphabetical for other languages (Portuguese and Spanish)? I tried <code>'√≥'.isalpha()</code>, which returns <code>False</code>.</p>

<p>What I thought of now is to get the Unicode table. Find the starting and ending letter and test whether a letter is in the range of alphabets. </p>
",Multilingual Language Processing & Language Identification,check whether string alphabetical language english working text data mix several language trying test whether token string alphabetical mean potentially word built function like test whether string alphabetical language portuguese spanish tried return thought get unicode table find starting ending letter test whether letter range alphabet
nltk built-in (or easily obtainable) probabilistic parsing models?,"<p>So I want to analyze the structure of fairly arbitrary English sentences with nltk. There seem to be lots of classes for doing this (eg PCFGs, <code>ProbabilisticProjectiveDependencyParser</code>s), but all require data to train on? Does NLTK come with data that can be used to train such parsers for arbitrary English (ie, I don't need exotic words, but basic English sentences should work).</p>

<p><a href=""http://www.nltk.org/_modules/nltk/parse/projectivedependencyparser.html#projective_prob_parse_demo"" rel=""nofollow"">The demo for the PPDP</a>, for instance, seems to use a data set for Dutch. Further, this data sentence seems incomplete. It doesn't seem to be able to parse sentences with 'Ik' ('I' in Dutch according to google translate).</p>
",Multilingual Language Processing & Language Identification,nltk built easily obtainable probabilistic parsing model want analyze structure fairly arbitrary english sentence nltk seem lot class eg pcfgs require data train doe nltk come data used train parser arbitrary english ie need exotic word basic english sentence work demo ppdp instance seems use data set dutch data sentence seems incomplete seem able parse sentence ik dutch according google translate
How can I enter data using non English (Bangla) language into this database table?,"<p>How can I enter data using non English (Bangla) language into this database table ?
<a href=""https://i.sstatic.net/YePmc.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/YePmc.png"" alt=""enter image description here""></a></p>
",Multilingual Language Processing & Language Identification,enter data using non english bangla language database table enter data using non english bangla language database table
"Phonetic translation from Latin (English, German) to Arabic","<p>I read a few papers about machine translation but did not understand them well.</p>

<p>The language models (in Google translate) use phonetics and machine learning as best as I can tell.</p>

<p>My question then becomes is it possible to convert an Arabic word that is phonetically spelled in English to translate the users intended Arabic word?</p>

<p>For instance the word 'Hadith' is an English phonetic of the Arabic word 'ÿ≠ÿØŸäÿ´'.  Can I programmatically go from 'Hadith' to Arabic? </p>
",Multilingual Language Processing & Language Identification,phonetic translation latin english german arabic read paper machine translation understand well language model google translate use phonetics machine learning best tell question becomes possible convert arabic word phonetically spelled english translate user intended arabic word instance word hadith english phonetic arabic word programmatically go hadith arabic
how to extract nouns and substantives from a phrase?,"<p>I would like to extract nouns, substantives and adjectives from a given text phrase. Is there a java library (open source) that does that? Does anyone know how to do that?</p>

<p>Basically, I was thinking in creating separated dictionaries for these categories (nouns, substantives, adjectives) and then parse the phrase, separate words in tokens and compare against these dictionaries but having something (lib) that already does that for me would be great. More perfect if it supports Brazilian Portuguese language!</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,extract noun substantive phrase would like extract noun substantive adjective given text phrase java library open source doe doe anyone know basically wa thinking creating separated dictionary category noun substantive adjective parse phrase separate word token compare dictionary something lib already doe would great perfect support brazilian portuguese language thanks
How can I use a trained GloVe/word2vec model to extract keywords from articles?,"<p>I have trained a GloVe with ~5M <strong>spanish</strong> articles. I know how to load this GloVe in gensim and use it as if it was a word2vec model.
Now I am facing  the problem of topic modelling and keywords extraction from news articles (also in spanish) so I was wondering how could I use the trained model to do so.</p>

<p>How could I do it?</p>
",Multilingual Language Processing & Language Identification,use trained glove word vec model extract keywords article trained glove spanish article know load glove gensim use wa word vec model facing problem topic modelling keywords extraction news article also spanish wa wondering could use trained model could
Junit: Unit Testing for big data natural language processing,"<p>My problem is a tad vague but my question is specific.</p>

<p><strong>QUESTION</strong></p>

<p>Can anyone point me to well-informed documentation on how to unit test big data natural language processing systems?</p>

<p><strong>CONTEXT</strong></p>

<p>Big data processing systems are often multi-threaded and are also highly forgiving of Exception states. Very often Exceptions are caught and not thrown in order to allow for ""messy"" data.</p>

<p>However, in unit testing such systems, levels of stringency should be higher than in production conditions. This level of stringency allows developers to at least be aware of what kind of conditions throw exceptions.</p>

<p>The problem is that such systems absorb the exceptions, so junit tests won't pick them up. I am looking for a parsimonious design for unit testing in such circumstances. Don't want to re-invent the wheel. Hence looking for relevant documentation.</p>
",Multilingual Language Processing & Language Identification,junit unit testing big data natural language processing problem tad vague question specific question anyone point well informed documentation unit test big data natural language processing system context big data processing system often multi threaded also highly forgiving exception state often exception caught thrown order allow messy data however unit testing system level stringency higher production condition level stringency allows developer least aware kind condition throw exception problem system absorb exception junit test pick looking parsimonious design unit testing circumstance want invent wheel hence looking relevant documentation
Toolkits to design a TTS (Text-to-speech) system for a custom language?,"<p>I'd like to create a TTS system for a native american language (wayuunaiki).
The language is written in latin (western) alphabet.
I also have information about the phonetics (the rules to convert each word into IPA symbols).</p>

<p>I'm planning to create a database of voice recordings from the native people. Then I want to somehow train that data, using the IPA equivalency information to generate a more accurate speech model.</p>

<p>I'm totally new to Natural Language Processing, so my question is.. which tools can I use to perform what I'm planning?</p>

<p>I've heard that HTK ans CMU Sphinx are quite good in speech recognition. No idea about speech generation. Also heard about Festival, but i read it only uses predefined most known languages: English, Spanish, and so.</p>

<p>Excuse my typing faults. I'm still learning English. Thanks in advance!</p>
",Multilingual Language Processing & Language Identification,toolkits design tt text speech system custom language like create tt system native american language wayuunaiki language written latin western alphabet also information phonetics rule convert word ipa symbol planning create database voice recording native people want somehow train data using ipa equivalency information generate accurate speech model totally new natural language processing question tool use perform planning heard htk cmu sphinx quite good speech recognition idea speech generation also heard festival read us predefined known language english spanish excuse typing fault still learning english thanks advance
Building Jarvis like application for local languages,"<p>The jarvis application that is currently developed, is in English. I want to customize it to use local language. How to develop this kind of app for local languages? what kind of programming languages I must know to proceed to the development? I have tested the english version of the jarvis, it works well for me. How to attach the c# with HTK for the purpose of the development?</p>
",Multilingual Language Processing & Language Identification,building jarvis like application local language jarvis application currently developed english want customize use local language develop kind app local language kind programming language must know proceed development tested english version jarvis work well attach c htk purpose development
Twitter dataset filtering for only English language text using Python,"<p>Is there a way to filter already processed dataset for only English language text using Python? Maybe some NLTK features or something like that. The data was extracted from Twitter, and it's format is the following:</p>

<pre><code>&lt;tweetid&gt;, &lt;username&gt;, &lt;userid&gt; &amp;8888 &lt;tweet text&gt;
</code></pre>

<p>Stream filtering is not appropriate, since I have the initial data only in the format showed above.
Any help will be appreciated, thanks.</p>
",Multilingual Language Processing & Language Identification,twitter dataset filtering english language text using python way filter already processed dataset english language text using python maybe nltk feature something like data wa extracted twitter format following stream filtering appropriate since initial data format showed help appreciated thanks
Google Like Search Mechanism,"<p>I want to do search mechanism similar to google using NLP(Natural Language Processing) in java. The algorithm should be able to give auto-suggestions , spellcheck , get the meaning out of the sentence and display top relevant results.
For Example , if I typed ""laptop"" relevant results to be shown [""laptop bags"",""laptop deals"",""laptop prices"",""laptop services"" ,""laptop tablet""]</p>

<p>Is it possible to achieve with NLP and Semantics? It would be appreciable if you post any reference links or ideas to achieve.</p>
",Multilingual Language Processing & Language Identification,google like search mechanism want search mechanism similar google using nlp natural language processing java algorithm able give auto suggestion spellcheck get meaning sentence display top relevant result example typed laptop relevant result shown laptop bag laptop deal laptop price laptop service laptop tablet possible achieve nlp semantics would appreciable post reference link idea achieve
How to do semantic keyword search with nlp,"<p>I want to do SEMANTIC keyword search on list of topics with NLP(Natural Language Processing ). It would be very appreciable if you post any reference links or ideas.</p>
",Multilingual Language Processing & Language Identification,semantic keyword search nlp want semantic keyword search list topic nlp natural language processing would appreciable post reference link idea
Is there any CFG available (with pos tags -part of speech tags) to validate the grammar of sentences in english?,"<p>It may not be 100 % accurate but still is there any written and tested CFG.
is it available with nltk data?</p>
",Multilingual Language Processing & Language Identification,cfg available po tag part speech tag validate grammar sentence english may accurate still written tested cfg available nltk data
which prolog implementation will be helpful in my case,"<p>I was going through Prolog. I want to use it for natural language processing. I came across this paper for <a href=""http://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/"" rel=""nofollow""><strong>natural language processing with Prolog in the IBM Watson system</strong></a>. As stated in the paper I want to try it out in some what similar way. 
Now I was wondering which of the Prolog implementation to use. I came across all of these <a href=""http://en.wikipedia.org/wiki/Comparison_of_Prolog_implementations"" rel=""nofollow"">Comparison onto Prolog onto wiki which is stated in this link</a>. So which one of these implementations can be used for the purpose of 
NLP using onto Ubunutu. Also the one which will easily integrate with python and good in speed. Has anyone ever worked any of these implementations. Is SWI-Prolog good?</p>

<p>Help is appreciated. Thankz :)</p>
",Multilingual Language Processing & Language Identification,prolog implementation helpful case wa going prolog want use natural language processing came across paper natural language processing prolog ibm watson system stated paper want try similar way wa wondering prolog implementation use came across comparison onto prolog onto wiki stated link one implementation used purpose nlp using onto ubunutu also one easily integrate python good speed ha anyone ever worked implementation swi prolog good help appreciated thankz
when tokenize arabic text with python I get strange result?,"<p>I'v been working with NLTK for a research to tokenize Arabic text and analyze it. The problem is when I do this code:</p>

<pre><code>bsm = 'ÿ®ÿ≥ŸÖ ÿßŸÑŸÑŸá ÿßŸÑÿ±ÿ≠ŸÖŸÜ ÿßŸÑÿ±Ÿäÿ≠ŸÖ'
wordsBsm = nltk.tokenize.wordpunct_tokenize(anas)
print "" "".join(wordsBsm)
</code></pre>

<p>I get this our put:</p>

<pre><code>ÔøΩ ÔøΩ ÿ≥ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ
</code></pre>

<p>I don't know how to solve this problem!</p>
",Multilingual Language Processing & Language Identification,tokenize arabic text python get strange result v working nltk research tokenize arabic text analyze problem code get put know solve problem
Caching of the data of a big file in memory in java,"<p>Hi I am working on Spelling Corrector project of Natural Language processing and I am supposed read data from a file whose size is <strike>6.2 MB</strike> 1 GB. While it is working fine, the problem that I am facing is that every time I run the java program I have to load the data in to the memory and it is taking same amount of time every time it is run. </p>

<p>Is there any way this data can cached in to the memory in java?Can any one suggest me some work around of it? </p>

<p>Basically what I want to know is that What is procedure of storing content of a large file in memory so that I dont have to read it again? lets say file is of GB. </p>
",Multilingual Language Processing & Language Identification,caching data big file memory java hi working spelling corrector project natural language processing supposed read data file whose size mb gb working fine problem facing every time run java program load data memory taking amount time every time run way data cached memory java one suggest work around basically want know procedure storing content large file memory dont read let say file gb
How to convert from Stanford Universal Dependencies to Phrase Grammar?,"<p>In my application I am using Stanford CoreNLP for parsing english text into a graph data structure (Universal Dependencies).</p>

<p>After some modifications of the graph I need to generate a natural language output for which I am using SimpleNLG: <a href=""https://github.com/simplenlg/simplenlg"" rel=""nofollow"">https://github.com/simplenlg/simplenlg</a></p>

<p>However SimpleNLG is using Phrase Grammar.</p>

<p>Therefore in order to successfully use SimpleNLG for natural language generation I need to convert from Universal Dependencies into Phrase Grammar.</p>

<p>What is the easiest way of achieving this?</p>

<p>So far I have only come across this article on this topic:
<a href=""http://delivery.acm.org/10.1145/1080000/1072147/p14-xia.pdf?ip=86.52.161.138&amp;id=1072147&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=642131329&amp;CFTOKEN=21335001&amp;__acm__=1468166339_844b802736ce07dab89064efb7f8ede9"" rel=""nofollow"">http://delivery.acm.org/10.1145/1080000/1072147/p14-xia.pdf?ip=86.52.161.138&amp;id=1072147&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=642131329&amp;CFTOKEN=21335001&amp;<strong>acm</strong>=1468166339_844b802736ce07dab89064efb7f8ede9</a></p>

<p>I am hoping that someone might have some more practical code examples to share on this issue?</p>
",Multilingual Language Processing & Language Identification,convert stanford universal dependency phrase grammar application using stanford corenlp parsing english text graph data structure universal dependency modification graph need generate natural language output using simplenlg however simplenlg using phrase grammar therefore order successfully use simplenlg natural language generation need convert universal dependency phrase grammar easiest way achieving far come across article topic hoping someone might practical code example share issue
Best parser algorithm for lexical structure transfer?,"<p>As part of a bigger project I want to implement a machine translator from language <em>A</em> to language <em>B</em>. Since there are not available tools that automatically do machine translation over this set of languages, and the available corpus of language B is quite small, <strong>I am trying to do the following</strong>:</p>

<p><strong>1.</strong> Given a sentence in language <em>A</em>, use a tool to get its set of language <em>A</em> PoS (Part-of-speech) tags.</p>

<p><strong>2.</strong> The tool I am using for PoS tagging (Freeling) does not return a parse tree, so I thought on building my own parse tree from the set of tags.</p>

<p><strong>3.</strong> After the parse tree is completed, traverse it by levels (starting on the root) and reorder its elements according to the grammar rules of language <em>B</em>.</p>

<p><a href=""https://en.wikipedia.org/wiki/Apertium#/media/File:Pipeline_of_Apertium_System.svg"" rel=""nofollow noreferrer"">Graphical explanation</a></p>

<p><a href=""https://i.sstatic.net/IgG3h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IgG3h.png"" alt=""enter image description here""></a></p>

<p>After doing some research I found out about Earley parsing (whose ability of parsing any language caught my attention because grammar on language <em>B</em> might change overtime, so I cannot guarantee that it will always meet any specific criterion).
<strong>However, given that my ultimate goal is doing structure transfer I am not sure if using a bottom-up parser and trying to reorder the elements as I match them with the rules would give me a better performance, or if I am on the wrong path and my solution is wrong altogether</strong>.</p>
",Multilingual Language Processing & Language Identification,best parser algorithm lexical structure transfer part bigger project want implement machine translator language language b since available tool automatically machine translation set language available corpus language b quite small trying following given sentence language use tool get set language po part speech tag tool using po tagging freeling doe return parse tree thought building parse tree set tag parse tree completed traverse level starting root reorder element according grammar rule language b graphical explanation research found earley parsing whose ability parsing language caught attention grammar language b might change overtime guarantee always meet specific criterion however given ultimate goal structure transfer sure using bottom parser trying reorder element match rule would give better performance wrong path solution wrong altogether
Information about the Translation Engines,"<p>I am interested in statistical machine translation. Can anyone suggest where can I find more information about state-of-the-art implementations like Google Translate, Microsoft Translate?</p>

<p>I would like to know about the following stuff:</p>

<p>1) <strong>The size of training data for different languages.</strong></p>

<p>2) <strong>The quality of the translations for different languages.</strong></p>

<p>and any other interesting point about engine implementation.</p>
",Multilingual Language Processing & Language Identification,information translation engine interested statistical machine translation anyone suggest find information state art implementation like google translate microsoft translate would like know following stuff size training data different language quality translation different language interesting point engine implementation
Translation API with candidates,"<p>I am looking for a translation API that outputs all the candidates and not just single ""best"" candidate.</p>

<p>All statistical machine translation systems at the last stage score the list of translation candidates and choice the best candidate. I wonder if there is a system like Google translate or Microsoft translate that returns the list of all possible candidates so that I will be able to score them by myself.</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,translation api candidate looking translation api output candidate single best candidate statistical machine translation system last stage score list translation candidate choice best candidate wonder system like google translate microsoft translate return list possible candidate able score thanks
what is the format of word alignments in machine translation?,"<p>I am reading <a href=""http://www.aclweb.org/anthology/P05-1032"" rel=""nofollow"">this</a> paper and having a difficulty understanding the way word alignments are represented. To be precise, right below section <code>4.1</code>, the authors say the format of the alignment is <code>(i,j)</code> where <code>i</code> ranges within the source sentence length and <code>j</code> ranges within the target sentence range. This means that each alignment is a pair of two numbers, which given that sentences are typically not longer than 40-100 words, values for <code>i</code>, and <code>j</code> can be stored using <code>short</code> type. So, I expect to see that the amount of space required to store these alignments be <code>2 x sizeof(short) x number of word alignments</code>. But if you go to the next page where, right above section <code>4.2</code>, they say the space is <code>sizeof(short) x number of word alignments</code>. WHY? Am I confusing stuff?</p>
",Multilingual Language Processing & Language Identification,format word alignment machine translation reading paper difficulty understanding way word alignment represented precise right section author say format alignment range within source sentence length range within target sentence range mean alignment pair two number given sentence typically longer word value stored using type expect see amount space required store alignment go next page right section say space confusing stuff
Are there any off-the-shelf solutions for lexical analysis in Haskell that allow for a run-time dynamic lexicon?,"<p>I'm working on a small Haskell project that needs to be able to lex a very small subset of strictly formed English in to tokens for semantic parsing.  It's a very na√Øve natural language interface to a system with many different end effectors than can be issued commands.  I'm currently using Alex for this, but Alex relies on its lexicon to be statically compiled.  The nature of the system is such that the number and even type of end effectors in the world can increase as well as decrease after compilation, and so I need to be able to add or remove viable tokens from the lexicon at runtime.</p>

<p>I've tried looking around for dynamic lexing solutions, and the closest I could get was <a href=""http://www.nondot.org/sabre/Projects/HaskellLexer/"">this</a> Dynamic Lexer Engine that doesn't look to have been updated since 2000.</p>

<p>I've been considering some techniques like using a less-high level approach (Attoparsec, perhaps), or even wiring up a recompilation hook for Alex  and separating the lexer from the rest of the application.</p>

<p>Are there any well-known solutions for this sort of lexical analysis?  I intend on working through <a href=""http://nlpwp.org/book/index.xhtml"">Natural Language Processing for the Working Programmer</a> eventually so I can take a less simplified approach, but currently a basically lexer is what I need.</p>
",Multilingual Language Processing & Language Identification,shelf solution lexical analysis haskell allow run time dynamic lexicon working small haskell project need able lex small subset strictly formed english token semantic parsing na natural language interface system many different end effector issued command currently using alex alex relies lexicon statically compiled nature system number even type end effector world increase well decrease compilation need able add remove viable token lexicon runtime tried looking around dynamic lexing solution closest could get wa eventually take le simplified approach currently basically lexer need
Compute Pairwise Cosine Similarity using scikit-learn,"<p>I am new to this, so it would be helpful if someone could point me in right direction/help me with some tutorial.
Given a sentence and a list of other sentences (English):</p>

<pre><code>s = ""This concept of distance is not restricted to two dimensions.""
list_s = [""It is not difficult to imagine the figure above translated into three dimensions."", ""We can persuade ourselves that the measure of distance extends to an arbitrary number of dimensions;""]
</code></pre>

<p>I want to compute pairwise cosine similarity between each sentence in the list and sentence s, then find the max value.</p>

<p>What i've got so far:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)
bow_matrix = tfidf.fit_transform([s, ' '.join(list_s)])
</code></pre>

<h1>1. What's next?</h1>

<h1>2. Should we transform the whole corpus or just 2 sentences when compute pairwise cosine similarity?</h1>

<h1>3. How to apply removing stopwords and stemming for this?</h1>

<h1>Thanks!</h1>
",Multilingual Language Processing & Language Identification,compute pairwise cosine similarity using scikit learn new would helpful someone could point right direction help tutorial given sentence list sentence english want compute pairwise cosine similarity sentence list sentence find max value got far next transform whole corpus sentence compute pairwise cosine similarity apply removing stopwords stemming thanks
R: Natural Language Processing on Support Vector Machine-TermDocumentMatrix,"<p>I have started working on a project which requires Natural Language Processing and building a model on Support Vector Machine (SVM) in R. </p>

<p>I‚Äôd like to generate a Term Document Matrix with all the tokens. </p>

<p>Example:</p>

<pre><code>testset &lt;- c(""From month 2 the AST and total bilirubine were not measured."", ""16:OTHER - COMMENT REQUIRED IN COMMENT COLUMN;07/02/2004/GENOTYPING;SF- genotyping consent not offered until T4."",  ""M6 is 13 days out of the visit window"")
word_ann &lt;- Maxent_Word_Token_Annotator()
sent_ann &lt;- Maxent_Sent_Token_Annotator()
test_annotations &lt;- annotate(testset, list(sent_ann, word_ann))
test_doc &lt;- AnnotatedPlainTextDocument(testset, test_annotations)
sents(test_doc)

[[1]]
 [1] ""From""       ""month""      ""2""          ""the""        ""AST""        ""and""        ""total""     
 [8] ""bilirubine"" ""were""       ""not""        ""measured""   "".""         

[[2]]
 [1] ""16:OTHER""                         ""-""                               
 [3] ""COMMENT""                          ""REQUIRED""                        
 [5] ""IN""                               ""COMMENT""                         
 [7] ""COLUMN;07/02/2004/GENOTYPING;SF-"" ""genotyping""                      
 [9] ""consent""                          ""not""                             
[11] ""offered""                          ""until""                           
[13] ""T4""                               "".""                               

[[3]]
[1] ""M6""     ""is""     ""13""     ""days""   ""out""    ""of""     ""the""    ""visit""  ""window"" 
</code></pre>

<p>And then I generated a TDM:</p>

<pre><code>tdm &lt;- TermDocumentMatrix(as.VCorpus(list(test_doc)))
inspect(tdm)
&lt;&lt;TermDocumentMatrix (terms: 22, documents: 1)&gt;&gt;
Non-/sparse entries: 22/0
Sparsity           : 0%
Maximal term length: 32
Weighting          : term frequency (tf)

                                  Docs
Terms                              NULL
  16:other                            1
  and                                 1
  ast                                 1
  bilirubine                          1
  column;07/02/2004/genotyping;sf-    1
  comment                             2
  consent                             1
  days                                1
  from                                1
  genotyping                          1
  measured                            1
  month                               1
  not                                 2
  offered                             1
  out                                 1
  required                            1
  the                                 2
  total                               1
  until                               1
  visit                               1
  were                                1
  window                              1
</code></pre>

<p>I actually have three documents in the dataset: 
""From month 2 the AST and total bilirubine were not  measured."", 
""16:OTHER - COMMENT REQUIRED IN COMMENT COLUMN;07/02/2004/GENOTYPING;SF- genotyping consent not offered until  T4."",<br>
""M6 is 13 days out of the visit window"" so it should have shown 3 columns of documents.
But I only have one column shown here.</p>

<p>Could anyone please give me some advice on this?</p>

<pre><code>sessionInfo()
    R version 3.3.0 (2016-05-03)
    Platform: x86_64-w64-mingw32/x64 (64-bit)
    Running under: Windows &gt;= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] tm_0.6-2       openxlsx_3.0.0 magrittr_1.5   RWeka_0.4-28   openNLP_0.2-6  NLP_0.1-9     
[7] rJava_0.9-8   
</code></pre>
",Multilingual Language Processing & Language Identification,r natural language processing support vector machine termdocumentmatrix started working project requires natural language processing building model support vector machine svm r like generate term document matrix token example generated tdm actually three document dataset month ast total bilirubine measured comment required comment column genotyping sf genotyping consent offered day visit window shown column document one column shown could anyone please give advice
Is Text.concordance() in nltk available for pyspark as distributed method,"<p>I'm working on Natural Language Processing using NLTK on spark. where tried to implement <strong>3.1   Accessing Text from the Web and from Disk</strong> part from <a href=""http://www.nltk.org/book/ch03.html"" rel=""nofollow"">http://www.nltk.org/book/ch03.html</a>. For this I followed How to do Natural Language Processing (<a href=""https://docs.continuum.io/anaconda-cluster/howto/spark-nltk"" rel=""nofollow"">https://docs.continuum.io/anaconda-cluster/howto/spark-nltk</a> ). I tried to implement <strong>text.concordance('gene')</strong>  method but ended with result [None, None, None, None, None]. Here is my complete code. Any help will be greatly appreciated.</p>

<pre><code>from pyspark import SparkConf
from pyspark import SparkContext


conf = SparkConf()
conf.setMaster('yarn-client')
conf.setAppName('spark-nltk')
sc = SparkContext(conf=conf)

data = sc.textFile('/user/test/2554.txt')

def word_tokenize(x):
    import nltk
    return nltk.word_tokenize(x)

def pos_tag(x):
    import nltk
    return nltk.pos_tag([x])


words = data.flatMap(word_tokenize)
print words.take(10)


from nltk.text import Text 
text = words.map(lambda x : Text(x).concordance('gene'))
print text.take(5)



pos_word = words.map(pos_tag)
print pos_word.take(5)
</code></pre>
",Multilingual Language Processing & Language Identification,text concordance nltk available pyspark distributed method working natural language processing using nltk spark tried implement accessing text web disk part followed natural language processing tried implement text concordance gene method ended result none none none none none complete code help greatly appreciated
How to resolve English sentence verbs semantically,"<p>I am trying to transform English statements into SQL queries.<br> 
e.g. How many products were created last year?</p>

<p>This should get transformed to <br><code>select count(*) from products
 where manufacturing date between 1/1/2015 and 31/12/2015</code></p>

<p>I am not able to understand how to map the verb ""created"" to ""manufacturing date"" attribute in my table. I am using Stanford core nlp suite to parse my statement. I am also using wordnet taxonomies with JWI framework. </p>

<p>I have tried to map the verbs to the attributes by defining simple rules. But it is not a very generic approach, since I can not know all the verbs in advance. Is there any better way to achieve this?</p>

<p>I would appreciate any help in this regard.</p>
",Multilingual Language Processing & Language Identification,resolve english sentence verb semantically trying transform english statement sql query e g many product created last year get transformed able understand map verb created manufacturing date attribute table using stanford core nlp suite parse statement also using wordnet taxonomy jwi framework tried map verb attribute defining simple rule generic approach since know verb advance better way achieve would appreciate help regard
How to extract lines numbers that match a regular expression in a text file,"<p>I'm doing a project on statistical machine translation in which I need to extract line numbers from a POS-tagged text file that match a regular expression (any non-separated phrasal verb with the particle 'out'), and write the line numbers to a file (in python).</p>

<p>I have this regular expression: '\w*_VB.?\sout_RP' and my POS-tagged text file: 'Corpus.txt'.
I would like to get an output file with the line numbers that match the above-mentioned regular expression, and the output file should just have one line number per line (no empty lines), e.g.:</p>

<p>2</p>

<p>5</p>

<p>44</p>

<p>So far all I have in my script is the following:</p>

<pre><code>OutputLineNumbers = open('OutputLineNumbers', 'w')
with open('Corpus.txt', 'r') as textfile:
    phrase='\w*_VB.?\sout_RP'
    for phrase in textfile: 

OutputLineNumbers.close()
</code></pre>

<p>Any idea how to solve this problem?</p>

<p>In advance, thanks for your help!</p>
",Multilingual Language Processing & Language Identification,extract line number match regular expression text file project statistical machine translation need extract line number po tagged text file match regular expression non separated phrasal verb particle write line number file python regular expression w vb sout rp po tagged text file corpus txt would like get output file line number match mentioned regular expression output file one line number per line empty line e g far script following idea solve problem advance thanks help
Natural Language Processing - Truecaser classifier,"<p>Please suggest a good machine learning classifier for truecasing of dataset.
Also, Is it possible to specify out own rules/features for truecasing in such a classifier? Thanks for all your suggestions.</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,natural language processing truecaser classifier please suggest good machine learning classifier truecasing dataset also possible specify rule feature truecasing classifier thanks suggestion thanks
English translation of the STTS tagset,"<p>The most common part-of-speech tagset for German is the <a href=""http://www.ims.uni-stuttgart.de/projekte/corplex/TagSets/stts-table.html"" rel=""nofollow noreferrer"">STTS tagset</a>. I need an English translation of the explanations for each tag. Not being a linguist I don't feel comfortable (let alone qualified) for translating this myself.</p>

<p>Google turned up nothing, so any help is appreciated.</p>
",Multilingual Language Processing & Language Identification,english translation stts tagset common part speech tagset german stts tagset need english translation explanation tag linguist feel comfortable let alone qualified translating google turned nothing help appreciated
"Removing special characters from .txt file give `LRB` , `LSB` , `RSB`, `LRB` , `RRB` etc. in Java","<p>I have a .txt file. Actually I got it from reading URL and converting HTML file. My .txt file contain so many special characters. I want to keep only English words. I used,</p>

<pre><code>`String result = listOfWords.replaceAll(""[^a-zA-Z]+"","" "");`
</code></pre>

<p>But, output replace some special characters as  <code>LRB</code> , <code>LSB</code> , <code>RSB</code> ,<code>LRB</code> ,  <code>RRB</code> like words.</p>

<p>Eg:</p>

<pre><code>Eleanor (2008), Mathematics
</code></pre>

<p><code>recent years. (TOP500 2006)^ Agatha C. Hughes (2000). Systems, Experts, and Computers. MIT Press. p. 161. ISBN 978-0-262-08285-3. The experience of SAGE helped make possible the first truly large-scale commercial real-time network: the SABRE computerized airline reservations system</code>  </p>

<p>After replacing it gave result as,</p>

<pre><code> Eleanor  LRB     RRB    Mathematics
</code></pre>

<p><code>recent years    LRB  TOP     RRB    Agatha C  Hughes  LRB     RRB    Systems   Experts   and Computers   MIT Press   p        ISBN       The experience of SAGE helped make possible the first truly large scale commercial real time network   the SABRE computerized airline reservations system</code></p>

<p>How to solveenter code here this problem?</p>
",Multilingual Language Processing & Language Identification,removing special character txt file give etc java txt file actually got reading url converting html file txt file contain many special character want keep english word used output replace special character like word eg replacing gave result solveenter code problem
WordNet does not have a definition for the word &#39;for&#39;,"<p>I'm using <code>WordNet 3</code> to validate English words, but it does not have a definition for the word <code>for</code>.</p>

<p><strong>Code snippet:</strong></p>

<pre><code>System.setProperty(""wordnet.database.dir"", modelFolder); // path to WordNet

NounSynset nounSynset;
NounSynset[] hyponyms;

WordNetDatabase database = WordNetDatabase.getFileInstance();
Synset[] synsets = database.getSynsets(word); // no results

boolean isFound = synsets.length &gt; 0;
</code></pre>
",Multilingual Language Processing & Language Identification,wordnet doe definition word using validate english word doe definition word code snippet
Word embeddings over user/customer reviews corpus,"<p>Most of the embeddings, publicly available, that I know are done over news articles, which use a different language/words as the one used in user/customer reviews. </p>

<p>Although such embeddings can be used in NLP tasks concerning reviews
and user generated content, I think the difference in language has an important role, and as such I would rather use embeddings trained over user generated content, such as product reviews.</p>

<p>I'm looking for a corpus of reviews or comments in English -- although in German and Dutch would also be useful -- to generate embeddings, or alternatively embeddings already trained over such a corpus.</p>
",Multilingual Language Processing & Language Identification,word embeddings user customer review corpus embeddings publicly available know done news article use different language word one used user customer review although embeddings used nlp task concerning review user generated content think difference language ha important role would rather use embeddings trained user generated content product review looking corpus review comment english although german dutch would also useful generate embeddings alternatively embeddings already trained corpus
"How can I open, read and write Non-english text file (i.e Gujarati) in python 3.5?","<p>I am newbie to the python programming, so I do not know how basic this question is. I Want to process a Gujarati text file in python 3.5. When I tried to execute this block of code, it gave me an error. How can i fix this error?</p>

<pre><code>import tkinter.filedialog
import fileinput
import tkinter

filename1 = tkinter.filedialog.askopenfile()


my_file = open(filename1, ""r"", encoding= ""utf-16"")

content = my_file.read()

print(content)
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last):
File ""D:\PhD\python workspace\guj.py"", line 8, in &lt;module&gt;
    my_file = open(filename1, ""r"", encoding= ""utf-16"")
TypeError: invalid file: &lt;_io.TextIOWrapper name='D:/PhD/python    workspace/text files/Gujarati.txt' mode='r' encoding='cp1252'&gt;
</code></pre>
",Multilingual Language Processing & Language Identification,open read write non english text file e gujarati python newbie python programming know basic question want process gujarati text file python tried execute block code gave error fix error error
reading Chinese csvfile in python,"<p>I'm trying to read a csv file with the following code but it still can't print Chinese</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import pandas as pd 

df = pd.read_csv ('weibo_status.csv') status = df[1:]
#print (df.head)
</code></pre>

<p>I think this might be the problem that the characters are in a data frame, because the following code works fine in my system  (python 2.7, editor: pycharm)</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import jieba

seg_list = jieba.cut(""ÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶"", cut_all=True) print(""Full Mode: "" + ""/ "".join(seg_list))
</code></pre>

<p>weibo_status.csv</p>

<pre><code>userid     status
1          Êàë‰ªäÂ§©ÂêÉÈ•≠‰∫Ü
2          ÂêÉ‰∫ÜÊ∞¥Êûú
3          ‰ªäÂ§©ÊÑüÂÜí‰∫Ü
</code></pre>
",Multilingual Language Processing & Language Identification,reading chinese csvfile python trying read csv file following code still print chinese think might problem character data frame following code work fine system python editor pycharm weibo status csv
Tutorials For Natural Language Processing,"<p>I recently attended a class on <a href=""https://class.coursera.org/nlp/lecture"" rel=""noreferrer"">coursera</a> about ""Natural Language Processing"" and I learnt a lot about parsing, IR and other interesting aspects like Q&amp;A etc. though I grasped the concepts well but I did not actually get any practical knowledge of it. Can anyone suggest me good online tutorials or books for Natural Language Processing?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,tutorial natural language processing recently attended class coursera natural language processing learnt lot parsing ir interesting aspect like q etc though grasped concept well actually get practical knowledge anyone suggest good online tutorial book natural language processing thanks
Efficient clustering algorithm for nearly uniformly distributed data,"<p>I'm playing around with natural language processing and I'm trying to cluster news article titles. I turned the titles into vectors, but they are nearly uniformly distributed. There are small clusters of 2-3 news articles, but most news articles should be in their own clusters.</p>

<p>I tried using k-means, but articles rarely change clusters because the data is fairly uniform. The initial random clusters end up being the final clusters.</p>

<p>I tried agglomerative clustering and it works great for a small data set (a few hundred articles). However, it takes a long time because it's at least O(n^2).</p>

<p>Is there any efficient algorithm to cluster nearly uniformly distributed data?</p>

<p>For example, if my data is a set of real numbers, it could look like this:</p>

<p>1 2 3 4 4.1 5 6</p>

<p>In this case, the clusters should be: (1), (2), (3), (4, 4.1), (5), (6).
Is there a better way to do this than agglomerative clustering?</p>
",Multilingual Language Processing & Language Identification,efficient clustering algorithm nearly uniformly distributed data playing around natural language processing trying cluster news article title turned title vector nearly uniformly distributed small cluster news article news article cluster tried using k mean article rarely change cluster data fairly uniform initial random cluster end final cluster tried agglomerative clustering work great small data set hundred article however take long time least n efficient algorithm cluster nearly uniformly distributed data example data set real number could look like case cluster better way agglomerative clustering
I&#39;m making an NLP app but models are too big,"<p>I'm making an Android app that does some Natural Language Processing. The app itself works fine except the models OpenNLP give are big so the apk is too large for distribution. What should I do to make the apk smaller? I was thinking about using a server to do the NLP but I have no clue how to go about it</p>
",Multilingual Language Processing & Language Identification,making nlp app model big making android app doe natural language processing app work fine except model opennlp give big apk large distribution make apk smaller wa thinking using server nlp clue go
Arabic Wordnet for Synonyms in Python?,"<p>I am using python 2.7, I am trying to get synonyms of Arabic words using Arabic WordNet</p>

<p>I downloaded both needed files:</p>

<p>AWNDatabaseManagement.py
upc_db.xml</p>

<p>Its working fine when I enter the word itself not using a variable</p>

<p>outputs In Backwalter form but I managed to translate them to arabic:</p>

<p>But the problem is that I want to loop on a set of words, but I get the following error</p>
",Multilingual Language Processing & Language Identification,arabic wordnet synonym python using python trying get synonym arabic word using arabic wordnet downloaded needed file awndatabasemanagement py upc db xml working fine enter word using variable output backwalter form managed translate arabic problem want loop set word get following error
Arabic Stemming on XML file in python?,"<p>I have an Arabic XML below is a small sample. I want to stem all the XML except those in <code>&lt;en&gt;</code> tags, and I want the words to change in the original XML file.</p>

<pre><code>&lt;?xml version='1.0' encoding='UTF-8' ?&gt;
&lt;TEXT&gt;
&lt;PHRASE&gt;
&lt;PSEUDO-V&gt;ÿßŸÜ&lt;/PSEUDO-V&gt;
&lt;N&gt;Ÿàÿ≤Ÿäÿ±&lt;/N&gt;
&lt;N&gt;ÿßŸÑÿÆÿßÿ±ÿ¨Ÿäÿ©&lt;/N&gt;
&lt;en x='PERS'&gt;ŸÅÿ±ÿßŸÜŸÉ ŸÅÿßŸÑÿ™ÿ± ÿ¥ÿ™ÿßŸäŸÜŸÖÿßŸäÿ±&lt;/en&gt;
&lt;V y='0'&gt;ÿ≥Ÿäÿ™Ÿàÿ¨Ÿá&lt;/V&gt;
&lt;N&gt;ÿßŸÑÿ≥ÿ®ÿ™&lt;/N&gt;
&lt;PREP&gt;ÿ•ŸÑŸâ&lt;/PREP&gt;
&lt;en x='LOC'&gt;ÿßŸÑÿ¥ÿ±ŸÇ ÿßŸÑÿ£Ÿàÿ≥ÿ∑&lt;/en&gt;
&lt;/PHRASE&gt;
&lt;PHRASE&gt;
&lt;V&gt;ÿπŸÑŸÖ&lt;/V&gt;
&lt;N&gt;ÿßŸÑÿ£ŸáŸÑ&lt;/N&gt;
&lt;PREP&gt;ÿ®&lt;/PREP&gt;
&lt;N y='1'&gt;ŸÖÿ∫ÿßÿØÿ±ÿ™&lt;/N&gt;
&lt;en x='PERS'&gt;ÿßŸÑÿ®ÿßÿ®ÿß&lt;/en&gt;
&lt;PREP y='1'&gt;ÿ•ŸÑŸâ&lt;/PREP&gt;
&lt;en x='LOC'&gt;ÿßŸÑŸÖÿØŸäŸÜÿ© ŸÖŸÉÿ©&lt;/en&gt;
&lt;/PHRASE&gt;
&lt;PHRASE&gt; 
&lt;/TEXT&gt;
</code></pre>

<p>I tried the following but for some reason it didn't work.
<strong>note:</strong> X attribute in <code>&lt;en&gt;</code> tag is either: LOC-PERS-DATE-ORG</p>

<pre><code>import re
import xml.etree.ElementTree as ET
from nltk.stem.isri import ISRIStemmer

tree2 = ET.parse('TrainBaseEnglishcopy.xml')
root2 = tree2.getroot()


for phrase in root2.findall('./PHRASE'):
    ens = {en.get('x'): en.text for en in phrase.findall('en')}
    if not ('ORG' in ens and 'PERS' in ens and 'LOC' in ens and 'DATE' in ens):
      phrase=st.stem(phrase)
</code></pre>

<p>I got the error: </p>

<pre><code>Traceback (most recent call last): 
File ""20Dec.py"", line 475, 
     in &lt;module&gt; phrase=st.stem(phrase) 
File /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/n‚Äå‚Äãltk/stem/isri.py"", line 153, 
     in stem token = self.norm(token, 1) # remove diacritics which representing Arabic short vowels 
File /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/n‚Äå‚Äãltk/stem/isri.py"", line 186, 
     in norm word = self.re_short_vowels.sub('', word) TypeError: expected string or bytes-like object ‚Äì
</code></pre>

<p>Note: Stemming on its own is working fine
for example</p>

<pre><code>w = 'ŸäÿπŸÖŸÑ'
print (st.stem(w))
</code></pre>

<p>works fine.</p>

<p>---Update--
<strong>I got to work like that, but I will have to repeat for every tag, but it didn't change the text in the original XML file, any ideas ?</strong></p>

<pre><code>for phrase in root2.findall('./PHRASE/N'):
    ens = {en.get('x'): en.text for en in phrase.findall('en')}
    if not ('ORG' in ens and 'PERS' in ens and 'LOC' in ens and 'DATE' in ens):
      phrase.text=st.stem(phrase.text)
      print(phrase.text)
</code></pre>
",Multilingual Language Processing & Language Identification,arabic stemming xml file python arabic xml small sample want stem xml except tag want word change original xml file tried following reason work note x attribute tag either loc pers date org got error note stemming working fine example work fine update got work like repeat every tag change text original xml file idea
"What are the libraries in R to tokenise any language text(e.g. : Chinese, Japanese, Arabic, etc)","<p>I have to Tokenize a text to words. But I don't know the language of text. I could be any language. So I have to build a Tokenizer which will detect text language and tokenize it. If Tokenizer is not able to tokenize then I will return some flag like ""<strong>not able to tokenize</strong>"".</p>

<p>Please help me to tokenize non-space languages if its possible.</p>
",Multilingual Language Processing & Language Identification,library r tokenise language text e g chinese japanese arabic etc tokenize text word know language text could language build tokenizer detect text language tokenize tokenizer able tokenize return flag like able tokenize please help tokenize non space language possible
Natural Language Proccessing detect similarity between phrases,"<p>I have been working on a natural language proccessing algorithm for an app of mine. Currently, all is well. However I am struggling to compare like phrases. For example, how can I detect that the phrases ""right now"" and ""current"" are relevant to each other? My algorithm is being written in python and am working a lot with the popular NLTK library. Is there any good way to accomplish this?</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,natural language proccessing detect similarity phrase working natural language proccessing algorithm app mine currently well however struggling compare like phrase example detect phrase right current relevant algorithm written python working lot popular nltk library good way accomplish thanks
Natural Language Proccessing detect similarity between phrases,"<p>I have been working on a natural language proccessing algorithm for an app of mine. Currently, all is well. However I am struggling to compare like phrases. For example, how can I detect that the phrases ""right now"" and ""current"" are relevant to each other? My algorithm is being written in python and am working a lot with the popular NLTK library. Is there any good way to accomplish this?</p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,natural language proccessing detect similarity phrase working natural language proccessing algorithm app mine currently well however struggling compare like phrase example detect phrase right current relevant algorithm written python working lot popular nltk library good way accomplish thanks
Get grammatical conjugation with GermaNLTK,"<p>What I can see in the <a href=""https://docs.google.com/document/d/1rdn0hOnJNcOBWEZgipdDfSyjJdnv_sinuAUSDSpiQns/edit?hl=en#"" rel=""nofollow"">documentation of GermaNLTK</a> (an integration of NLTK and <a href=""http://www.sfs.uni-tuebingen.de/GermaNet/"" rel=""nofollow"">GermaNet</a>) is the possibility to lemmatize German words correctly.</p>

<pre><code>&gt;&gt;&gt; gwl.lemmatize('geht')
'gehen'
</code></pre>

<p>or</p>

<pre><code>&gt;&gt;&gt; gwl.lemmatize('kann')
'k√∂nnen'
</code></pre>

<p>It's good to know the infinitive, but I want more. What I actually want is to get the information about grammatical conjugation. For example something like this:</p>

<pre><code>&gt;&gt;&gt; gwl.grammatical_conjugation('geht')
{'gehen':'3. Person Singular'}
</code></pre>

<p>or </p>

<pre><code>&gt;&gt;&gt; gwl.grammatical_conjugation('kann')
{'k√∂nnen': ['1. Person Singular', '3. Person Singular']}
</code></pre>

<p>How would you get the information about the grammatical conjugation?</p>
",Multilingual Language Processing & Language Identification,get grammatical conjugation germanltk see documentation germanltk integration nltk germanet possibility lemmatize german word correctly good know infinitive want actually want get information grammatical conjugation example something like would get information grammatical conjugation
Autogenerate summaries based on pre-existing topics?,"<p>I'd like to do the following given a document:</p>

<ul>
<li>create a summary using pre-existing topics </li>
</ul>

<p>In the first scenario, documents are neatly organized in a uniform way. 
For example, most Wikipedia movie articles have the following subtopics (ex: <a href=""https://en.wikipedia.org/wiki/Between_Us_(2012_film)"" rel=""nofollow"">https://en.wikipedia.org/wiki/Between_Us_(2012_film)</a>)</p>

<ul>
<li>Plot</li>
<li>Cast</li>
<li>Reception</li>
<li>other optional topics</li>
</ul>

<p>In the second scenario, documents contain the same info as above; however,  documents do NOT have clean organization. Documents may use the same or similar language but organized differently. </p>

<p>In both cases, given the subtopics, I'd like to extract this info from a document.</p>

<p>Are there any machine learning/natural language processing strategies/algorithms that I can use? Combination of algorithms is fine. Algorithms that mostly work are also fine.</p>

<p>Update: It looks like what I want is <em>Information Extraction</em>.</p>
",Multilingual Language Processing & Language Identification,autogenerate summary based pre existing topic like following given document create summary using pre existing topic first scenario document neatly organized uniform way example wikipedia movie article following subtopics ex plot cast reception optional topic second scenario document contain info however document clean organization document may use similar language organized differently case given subtopics like extract info document machine learning natural language processing strategy algorithm use combination algorithm fine algorithm mostly work also fine update look like want information extraction
Get a word&#39;s synonyms,"<p>Is there an API where for a given English word I can get a set of its synonyms?
If not synonyms, then at least words used in similar contexts.</p>

<p>My goal is to construct sentences with similar structure.</p>

<p>For instance, from this:</p>

<pre><code>Jason found 49 seashells and 48 starfish
</code></pre>

<p>I would like to obtain this:</p>

<pre><code>Joan grew 29 carrots and 14 watermelons
</code></pre>
",Multilingual Language Processing & Language Identification,get word synonym api given english word get set synonym synonym least word used similar context goal construct sentence similar structure instance would like obtain
How can Machine Learning approaches be applied to Natural Language Processing?,"<p>I am trying to do a paper about the Machine learning been applied in NLP. Can you guys please suggest me applications that have already used the Machine learning with the NLP?</p>
",Multilingual Language Processing & Language Identification,machine learning approach applied natural language processing trying paper machine learning applied nlp guy please suggest application already used machine learning nlp
Spark MLlib LDA: the possible reasons behind generating always very similar LDA topics?,"<p>I am applying the <a href=""https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala"" rel=""nofollow"">MLlib LDA example</a> on various corpora downloaded from <a href=""http://corpora2.informatik.uni-leipzig.de/"" rel=""nofollow"">enter link description here</a>
I am filtering out the stopwords, and also excluding the very frequent terms and the very rare terms.
The problem is that I am always having topics that have very similar profile.</p>

<p>Here is an example of the topics I am getting, by running the algorithm on a corpora of 300K english sentences from Wikipedia (eng_wikipedia_2010_300K-sentences), knowing that I am having similar behaviour when using other corpora:</p>

<pre><code>TOPIC 0
dai 0.0020492776129338083
call    0.0019627409470977355
citi    0.0019496273507300062
three   0.0019172201890256511
gener   0.0018325842193426059
plai    0.0018287121439402873
peopl   0.001786839660855886
well    0.0017792000702589461
system  0.0017410979899730565
area    0.001721711978388363
power   0.0016906026954800833
forc    0.0016646631729486227
number  0.0016343386030518979
1   0.0016238591786476033
team    0.0016112030952801443
second  0.0015692071709961662
develop 0.0015670177558504078
group   0.0015378927495689552
unit    0.001535180513974118
nation  0.001520548489788889

TOPIC 1
dai 0.002027230927747474
call    0.0019861147606781222
citi    0.0019793753441068825
three   0.0019315799215582723
gener   0.0018482143436741026
plai    0.0018088629290540156
peopl   0.0017929339168126625
well    0.0017549252518608278
system  0.0016936542725510587
power   0.0016792684719108006
area    0.0016604962232717288
forc    0.0016575624332970456
1   0.0016344588453542676
number  0.0016147026427518426
team    0.0015914797457267642
develop 0.001580085843019015
unit    0.0015659585445574969
nation  0.0015412334667742672
second  0.0015292625574896467
group   0.0015111594105132022

TOPIC 2
dai 0.002028407701986021
call    0.001987655848237808
citi    0.0019737160296217846
three   0.0019183385421321895
plai    0.0018470661666555599
gener   0.0018431319454591765
peopl   0.0017947273975068192
well    0.00174922095206974
area    0.0017256327188664123
system  0.0016995971624202812
forc    0.001690002995539528
power   0.0016779250581379353
1   0.0016214669556130525
team    0.0016134935452659781
number  0.00161273946842774
develop 0.0015712560226793318
unit    0.0015385515465297065
second  0.001537016434433013
nation  0.001529578699246495
group   0.0015259003261706866

TOPIC 3
dai 0.0020271063080981745
call    0.001973996689805456
citi    0.0019709486233839084
three   0.0019445106630149387
gener   0.0018677792917783514
plai    0.0018485914586526906
peopl   0.0018082458859327093
well    0.0017955363877379456
area    0.0017455386898734308
system  0.0017118889300776724
power   0.0017085249825238942
forc    0.0016416026632813164
1   0.001625823945554925
team    0.0015984923365964885
number  0.001584888932954503
develop 0.0015753517064182336
unit    0.0015587234313666533
second  0.0015545107852806973
nation  0.001551230039407881
form    0.0015004750009120491

TOPIC 4
dai 0.0020367505428973216
citi    0.0019778590305849857
call    0.0019772546555550576
three   0.001909390366412786
peopl   0.001822249318126459
gener   0.0018136257455996375
plai    0.0018128359158538045
well    0.0017692106359278286
system  0.0017220797688845334
area    0.0017158874212548339
power   0.0016752592665713634
forc    0.0016481228833262157
1   0.0016364343814157618
develop 0.0016172188646470641
team    0.0016018835612051036
number  0.0015991873726231036
group   0.0015593423279207062
second  0.0015532604092917898
unit    0.001549525336335323
2   0.0015220460130066676

TOPIC 5
dai 0.0020635883517150367
call    0.0019664003159491844
citi    0.001961190935833301
three   0.001945998746077669
plai    0.0018498883070569758
peopl   0.0018146602342867515
gener   0.0018135991027718233
well    0.0017837359414291816
area    0.0017440315427199456
system  0.0016954828503859868
power   0.001684533695977363
forc    0.001669704443002364
number  0.00161528564937031
1   0.001615272821378791
team    0.0016121988960501902
unit    0.0015895009183487473
develop 0.001577936587739003
group   0.0015555325586313624
nation  0.0015404874848355308
second  0.0015394146696500102

TOPIC 6
dai 0.0020136284206896792
call    0.001992567179072041
citi    0.0019601308797825385
three   0.0019185595159400765
plai    0.0018409472012516875
gener   0.001829303983728153
peopl   0.0017780620849170163
well    0.001771180582253062
system  0.0017377818879564248
area    0.0016871361621009276
power   0.0016862650658960986
forc    0.00167141172198367
1   0.001629498191900329
number  0.0015977527836457993
develop 0.0015960475085336815
team    0.001571055963470908
unit    0.0015559866004530513
group   0.0015445653607137958
second  0.0015346412996486915
2   0.001533194322154979

TOPIC 7
dai 0.0020097600649219504
citi    0.001996121452902739
call    0.001976365831615543
three   0.0019444233325152307
gener   0.0018347697960641011
plai    0.0018294437097569366
peopl   0.001809068711352435
well    0.0017851474017785431
system  0.0017266117477556496
power   0.001696861186965475
area    0.0016963032173278431
forc    0.0016424242914518095
team    0.0016341651077031543
number  0.0016257268377783236
1   0.0016221579346215153
develop 0.0015930555191603342
unit    0.0015895942206181324
group   0.0015703868353222673
second  0.001515454552733173
2   0.0015143190174102155

TOPIC 8
dai 0.002044683052793855
call    0.001992448963405555
citi    0.00195425798896221
three   0.0018970773269210957
plai    0.001853887836159108
gener   0.0018252502592182695
peopl   0.0018160312050590462
well    0.0017935933754513543
system  0.0017479534729456555
area    0.0017288815955179666
power   0.0017029539375086361
forc    0.0016706673237865313
1   0.0016681586343593317
number  0.0016501255143390717
team    0.0015894156993455188
develop 0.0015724268907364824
unit    0.0015371351757786232
second  0.0015247527824288484
nation  0.0015235190916716697
group   0.0015194534324480095

TOPIC 9
dai 0.0020620160901430877
citi    0.001987856719658478
call    0.001973103036828604
three   0.001924295805136688
peopl   0.0018232321289066767
plai    0.0018172215529843724
gener   0.0018125979152302458
well    0.0018056742813131674
system  0.001725860669839185
area    0.0017232894719674296
power   0.001697643253119442
1   0.001640662972775316
forc    0.0016394197000681693
number  0.0015927389128238725
unit    0.0015785177165666606
team    0.0015751611459412492
develop 0.0015670613914512046
nation  0.0015287394547847542
2   0.0015262474392790497
group   0.0015196717933709822

TOPIC 10
dai 0.0020203137546454856
citi    0.001985814822156114
call    0.001974265937728284
three   0.001934180185122672
gener   0.0018803136198652043
plai    0.0018164056544889878
peopl   0.0018083393449413536
well    0.0017804569091358126
power   0.0017051544274740097
area    0.0016959804754901494
system  0.0016918620528211653
1   0.0016435864049172597
forc    0.0016413861291761263
number  0.001638383798987439
develop 0.0016053710214565596
team    0.0015754232749060797
unit    0.001543834810440448
group   0.0015352472722856185
nation  0.0015350540825884074
2   0.001500158078774582
</code></pre>
",Multilingual Language Processing & Language Identification,spark mllib lda possible reason behind generating always similar lda topic applying mllib lda example various corpus downloaded enter link description filtering stopwords also excluding frequent term rare term problem always topic similar profile example topic getting running algorithm corpus k english sentence wikipedia eng wikipedia k sentence knowing similar behaviour using corpus
Hibernate search Lucene accent insensitive search,"<p>I'm working on a J2E application with an Hibernate Search / Lucene int√©gration. I index Documents (and other entities) and want to make accent insensitive search on it (content and classes' fields). </p>

<p>I'm using the FrenchAnalyzer which is seems to be Case insensitive (that's good), but i'm surprised to see he doesn't do the same with accent... </p>

<p>After some search and documentation reading, I understand I have to implement my own analyzer, based on the French one and include a filter (the ASCIIFoldingFilter seems to be the one I need).</p>

<p>If I'm right, this means this customAnalyzer have to be applied on the Class index and on the queries. The result will be an index without accents.</p>

<p>Is that the good way todo ? Is there no param or conf attribute for the FrenchAnalyzer to ignore accents ? </p>

<p>thanks</p>
",Multilingual Language Processing & Language Identification,hibernate search lucene accent insensitive search working j e application hibernate search lucene int gration index document entity want make accent insensitive search content class field using frenchanalyzer seems case insensitive good surprised see accent search documentation reading understand implement analyzer based french one include filter asciifoldingfilter seems one need right mean customanalyzer applied class index query result index without accent good way todo param conf attribute frenchanalyzer ignore accent thanks
Correctly folding ASCII characters in Elasticsearch,"<p>I'm looking into supporting folding of non standard ASCII characters like <a href=""https://www.elastic.co/guide/en/elasticsearch/guide/1.x/asciifolding-token-filter.html"" rel=""nofollow"">this guide</a> suggests.</p>

<pre><code>PUT /my_index
{
  ""settings"": {
    ""analysis"": {
      ""analyzer"": {
        ""folding"": {
          ""tokenizer"": ""standard"",
          ""filter"":  [ ""lowercase"", ""asciifolding"" ]
        }
      }
    }
  }
}
</code></pre>

<p>Strangely enough, I'm not able to replicate the sample in the first snippet of code.</p>

<p>When I execute</p>

<pre><code>GET /my_index/_analyze?analyzer=folding&amp;text=My ≈ìsophagus caused a d√©b√¢cle
</code></pre>

<p>the following tokens are returned:</p>

<pre><code>sophagus, caused, a, d, b, cle
</code></pre>

<p>What I want to achieve is:</p>

<p>Variations of the spelling of words like ""√©cole"" (e.g. ecole, √®cole) should be treated as the same word.</p>

<p>Right now, if I execute</p>

<pre><code>GET /my_index/_analyze?analyzer=folding&amp;text=√©cole ecole
</code></pre>

<p>I get the tokens <code>cole, ecole</code></p>

<p>These are the settings I currently use for the text analysis of the documents </p>

<pre><code>    ""analysis"": {
  ""filter"": {
    ""french_stop"": {
      ""type"": ""stop"",
        ""stopwords"": ""_french_""
    },
      ""french_elision"": {
        ""type"": ""elision"",
          ""articles"": [
            ""l"",
            ""m"",
            ""t"",
            ""qu"",
            ""n"",
            ""s"",
            ""j"",
            ""d"",
            ""c"",
            ""jusqu"",
            ""quoiqu"",
            ""lorsqu"",
            ""puisqu""
          ]
      },
        ""french_stemmer"": {
          ""type"": ""stemmer"",
            ""language"": ""light_french""
        }
  },
    ""analyzer"": {
      ""index_French"": {
        ""filter"": [
          ""french_elision"",
          ""lowercase"",
          ""french_stop"",
          ""french_stemmer""
        ],
          ""char_filter"": [
            ""html_strip""
          ],
            ""type"": ""custom"",
              ""tokenizer"": ""standard""
      },
        ""sort_analyzer"": {
          ""type"": ""custom"",
            ""filter"": [
              ""lowercase""
            ],
              ""tokenizer"": ""keyword""
        }
    }
}
</code></pre>

<p>My idea was to change the filters of the index_French analyzer so that the list is the following:</p>

<pre><code>""filter"": [""french_elision"",""lowercase"",""asciifolding"",""french_stop"",""french_stemmer""]
</code></pre>

<p>Thanks for your help.</p>
",Multilingual Language Processing & Language Identification,correctly folding ascii character elasticsearch looking supporting folding non standard ascii character like guide suggests strangely enough able replicate sample first snippet code execute following token returned want achieve variation spelling word like cole e g ecole cole treated word right execute get token setting currently use text analysis document idea wa change filter index french analyzer list following thanks help
Character classes used in ffi-aspell,"<p>I am trying to use the <a href=""https://rubygems.org/gems/ffi-aspell/versions/1.1.0"" rel=""nofollow"">ffi-aspell</a> gem to spell check a text. In order to do that, it seems that I have to extract the words by myself. I am trying to do that by applying <code>String#scan</code> to the text with a regex, but it does not seem straightforward.</p>

<p>What is the easiest way to define the class of characters that may appear in an ffi-aspell dictionary of some language? I want to make it available not only for English, so things like <code>/[a-zA-Z']/</code> for the character (or <code>/[a-zA-Z']+/</code> the word) does not work. <code>/[[:word:]]/</code> seems to capture characters that are not in the dictionary, such as numerals, and further does not match the apostrophe (single quote), which is frequently used in a word. Is there some documentation that defines the character set used in an ffi-aspell dictionary?</p>
",Multilingual Language Processing & Language Identification,character class used ffi aspell trying use ffi aspell gem spell check text order seems extract word trying applying text regex doe seem straightforward easiest way define class character may appear ffi aspell dictionary language want make available english thing like character word doe work seems capture character dictionary numeral doe match apostrophe single quote frequently used word documentation defines character set used ffi aspell dictionary
"Must use *unicode* string as text to tag, while tagging with TreeTagger?","<p>From <a href=""http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"" rel=""nofollow"">TreeTagger's website</a> I created a directory and downloaded the specified files. Then <a href=""https://pypi.python.org/pypi/treetaggerwrapper/2.0.6"" rel=""nofollow"">treetaggerwrapper</a>, thus from the <a href=""https://treetaggerwrapper.readthedocs.org/en/latest/"" rel=""nofollow"">documentation</a> I tried to test and try how to tag some text as follows:</p>

<pre><code>In [40]:

import treetaggerwrapper

tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')

tags = tagger.TagText(""This is a very short text to tag."")

print tags
</code></pre>

<p>Then I got the following warnings:</p>

<pre><code>WARNING:TreeTagger:Abbreviation file not found: english-abbreviations
WARNING:TreeTagger:Processing without abbreviations file.
ERROR:TreeTagger:Must use *unicode* string as text to tag, not &lt;type 'str'&gt;.

---------------------------------------------------------------------------
TreeTaggerError                           Traceback (most recent call last)
&lt;ipython-input-40-37b912126580&gt; in &lt;module&gt;()
      1 import treetaggerwrapper
      2 tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')
----&gt; 3 tags = tagger.TagText(""This is a very short text to tag."")
      4 print tags

/usr/local/lib/python2.7/site-packages/treetaggerwrapper.pyc in TagText(self, text, numlines, tagonly, prepronly, tagblanks, notagurl, notagemail, notagip, notagdns, encoding, errors)
   1236         return self.tag_text(text, numlines=numlines, tagonly=tagonly,
   1237                  prepronly=prepronly, tagblanks=tagblanks, notagurl=notagurl,
-&gt; 1238                  notagemail=notagemail, notagip=notagip, notagdns=notagdns)
   1239 
   1240     # --------------------------------------------------------------------------

/usr/local/lib/python2.7/site-packages/treetaggerwrapper.pyc in tag_text(self, text, numlines, tagonly, prepronly, tagblanks, notagurl, notagemail, notagip, notagdns, nosgmlsplit)
   1302             # Raise exception now, with an explicit message.
   1303             logger.error(""Must use *unicode* string as text to tag, not %s."", type(text))
-&gt; 1304             raise TreeTaggerError(""Must use *unicode* string as text to tag."")
   1305 
   1306         if isinstance(text, six.text_type):

TreeTaggerError: Must use *unicode* string as text to tag.
</code></pre>

<p>Where do I download the abbreviation file for english and spanish languages?, and how can I install correctly treetaggerwrapper?.</p>
",Multilingual Language Processing & Language Identification,must use unicode string text tag tagging treetagger treetagger website created directory downloaded specified file treetaggerwrapper thus documentation tried test try tag text follows got following warning download abbreviation file english spanish language install correctly treetaggerwrapper
OpenNLP: foreign names does not get recognized,"<p>I just started using openNLP to recognize names. I am using the model (en-ner-person.bin) that comes with open NLP.  I noticed that while it recognizes us, uk, and european names, it fails to recognize Indian or Japanese names.  My questions are (1) is there already models available that I can use to recognize foreign names (2) If not, then I believe I will need to generate new models.  In that case, is there a copora available that I can use?</p>
",Multilingual Language Processing & Language Identification,opennlp foreign name doe get recognized started using opennlp recognize name using model en ner person bin come open nlp noticed recognizes u uk european name fails recognize indian japanese name question already model available use recognize foreign name believe need generate new model case copora available use
How to use target label as feature in CRF++?,"<p>I'm trying to build a Chinese word segmentator as <a href=""http://www.aclweb.org/anthology/Y09-2034"" rel=""nofollow"">this paper</a>. If I understand it correctly, they use a 2-tag segmentation approach with CRF++. My question is, how to make the tag transition in that paper (e.g.T(-1)C(0)T(0)) as a feature template in CRF++? For example,for such training data:</p>

<p>ÂÖ± B</p>

<p>Âêå M</p>

<p>Ââµ B</p>

<p>ÈÄ† M</p>

<p>Áæé B &lt;- Current word</p>

<p>Â•Ω M</p>

<p>ÁöÑ B</p>

<p>Êñ∞ B</p>

<p>‰∏ñ B</p>

<p>Á¥Ä M</p>

<p>Is it possible to have feature T(-1)C(0)T(0) -> M/Áæé/B in CRF++? I've tried add feature tempalte such as U01:%x[-1,1]/%x[0,0]/%x[0,1] but that failed. I am also confused about that since the B/I tag is the tag we want to tag in the testing data(e.g. raw Chinese sentences), why it's possible to use the tag as feature in the paper? Or I misunderstood anything?</p>
",Multilingual Language Processing & Language Identification,use target label feature crf trying build chinese word segmentator paper understand correctly use tag segmentation approach crf question make tag transition paper e g c feature template crf example training data b b b current word b b b possible feature c b crf tried add feature tempalte u x x x failed also confused since b tag tag want tag testing data e g raw chinese sentence possible use tag feature paper misunderstood anything
Multi-Threaded NLP with Spacy pipe,"<p>I'm trying to apply Spacy NLP (Natural Language Processing) pipline to a big text file like Wikipedia Dump. Here is my code based on Spacy's <a href=""https://spacy.io/docs"" rel=""noreferrer"">documentation</a> example:</p>

<pre><code>from spacy.en import English

input = open(""big_file.txt"")
big_text= input.read()
input.close()

nlp= English()    

out = nlp.pipe([unicode(big_text, errors='ignore')], n_threads=-1)
doc = out.next() 
</code></pre>

<p>Spacy applies all nlp operations like POS tagging, Lemmatizing and etc all at once. It is like a pipeline for NLP that takes care of everything you need in one step. Applying pipe method tho is supposed to make the process a lot faster by multithreading the expensive parts of the pipeline. But I don't see big improvement in speed and my CPU usage is around 25% (only one of 4 cores working). I also tried to read the file in multiple chuncks and increase the batch of input texts:</p>

<pre><code>out = nlp.pipe([part1, part2, ..., part4], n_threads=-1)
</code></pre>

<p>but still the same performance. Is there anyway to speed up the process? I suspect that OpenMP feature should be enabled compiling Spacy to utilize multi-threading feature. But there is no instructions on how to do it on Windows. </p>
",Multilingual Language Processing & Language Identification,multi threaded nlp spacy pipe trying apply spacy nlp natural language processing pipline big text file like wikipedia dump code based spacy documentation example spacy applies nlp operation like po tagging lemmatizing etc like pipeline nlp take care everything need one step applying pipe method tho supposed make process lot faster multithreading expensive part pipeline see big improvement speed cpu usage around one core working also tried read file multiple chuncks increase batch input text still performance anyway speed process suspect openmp feature enabled compiling spacy utilize multi threading feature instruction window
How to replace english abbreviated form to their dictionary form,"<p>I'm working on a system to analyze texts in english: I use stanford-core nlp to make sentences from whole documents and to make tokens from sentences. I also use the maxent tagger to get tokens pos tags. 
Now, considering that I use this corpus to build a supervised classifier, it would be good if I could replace any word like 're, 's, havin, sayin', etc. to its standard form(are, is, having, saying). I've been searching for some english dictionary file, but I don't know how to use it. There are so many distinct cases to consider that I don't think it's an easy task to realize: is there some similar work or whole project that I could use? </p>
",Multilingual Language Processing & Language Identification,replace english abbreviated form dictionary form working system analyze text english use stanford core nlp make sentence whole document make token sentence also use maxent tagger get token po tag considering use corpus build supervised classifier would good could replace word like havin sayin etc standard form saying searching english dictionary file know use many distinct case consider think easy task realize similar work whole project could use
Performance Issues With Natural Language Processing in MATLAB,"<p>For a class, I'm processing raw text documents (our examples include novels that can be downloaded from the Gutenberg project) and turning them into a dictionary data structure. For each word, I need to know which paragraph(s) it occurs in, and how many times in each paragraph.</p>

<p>The procedure can be divided roughly as</p>

<ol>
<li>Break document into words, removing whitespace, commas, periods, etc.</li>
<li>For each document, iterate over the words. Look up in dictionary. If the word exists, update its entry. If it doesn't exist, create a new entry.</li>
</ol>

<p>I'm doing this MATLAB because the rest of my work is in MATLAB and I didn't want to have to deal with another language/environment. It turns out MATLAB has some pretty good string processing functions. However, I'm disturbed by how slow my code is running. The first part I mentioned above is not a problem; I use a <code>parfor</code> loop and it goes pretty fast. The second part is where my problem is.</p>

<p>My initial attempt at creating this dictionary was to use structs, a MATLAB built-in data type. The idea was to create a struct called <code>dictionary</code>, whose field names are the actual words, e.g., <code>dictionary.the</code> or <code>dictionary.gnome</code> for the words 'the' and 'gnome.' This worked provided the word was a valid field name (it couldn't be a contraction, for instance). But it ran slow, so I sought a different solution. My next attempt was to use a <code>containers.Map</code>, which is MATLAB's equivalent of a hash map or dictionary object. (One of my coworkers informed me that MATLAB is somewhat inefficient at looking up field names of structs, whereas the hash table has O(1) lookup time.) However, making this substitution actually <em>slowed down</em> my performance!</p>

<p>At this point, I feel I've some pretty substantial attempts at optimizing my code and am starting to wonder if MATLAB is really a sensible choice here. I'm basically trying to figure out whether the slowness is a product of MATLAB or of me being a terrible programmer (normally MATLAB is quite fast when it comes to linear algebra, arrays, and matrices). Rather than have someone read over my code, I'd greatly appreciate whatever feedback the community has to offer on either of the following:</p>

<ul>
<li><p>Does anyone do this type of thing (i.e., language processing) in MATLAB or a similar language, such as Python? If so, I'd like to try to benchmark some of my timings. (I've got a MacBook, 2.8 GHz processor, and I'm currently getting about 10-20K words per second.)</p></li>
<li><p>Is it possible I'd get better results if I switched to a compiled language like Java or C++? Ballpark estimate, what kind of improvement?</p></li>
</ul>
",Multilingual Language Processing & Language Identification,performance issue natural language processing matlab class processing raw text document example include novel downloaded gutenberg project turning dictionary data structure word need know paragraph occurs many time paragraph procedure divided roughly break document word removing whitespace comma period etc document iterate word look dictionary word exists update entry exist create new entry matlab rest work matlab want deal another language environment turn matlab ha pretty good string processing function however disturbed slow code running first part mentioned problem use loop go pretty fast second part problem initial attempt creating dictionary wa use structs matlab built data type idea wa create struct called whose field name actual word e g word gnome worked provided word wa valid field name contraction instance ran slow sought different solution next attempt wa use matlab equivalent hash map dictionary object one coworkers informed matlab somewhat inefficient looking field name structs whereas hash table ha lookup time however making substitution actually slowed performance point feel pretty substantial attempt optimizing code starting wonder matlab really sensible choice basically trying figure whether slowness product matlab terrible programmer normally matlab quite fast come linear algebra array matrix rather someone read code greatly appreciate whatever feedback community ha offer either following doe anyone type thing e language processing matlab similar language python like try benchmark timing got macbook ghz processor currently getting k word per second possible get better result switched compiled language like java c ballpark estimate kind improvement
NLTK - Where is the organization names list stored in source file,"<p>When i open the file i see different files which are full of java functions. But how nltk compare tokenized words with its database to check if the word is ogranization name or not ? Where is that organization name list ?</p>

<p>Sorry for bad english.</p>

<p><a href=""http://www.nltk.org/_modules/nltk/tag/stanford.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/tag/stanford.html</a></p>
",Multilingual Language Processing & Language Identification,nltk organization name list stored source file open file see different file full java function nltk compare tokenized word database check word ogranization name organization name list sorry bad english
NLP Methodology in Python to find 3-Character Common Combinations,"<p>I have a real-world meets math world problem.  I'm creating cupcake molds that are 3x4 cavity to a mold.  Each mold cost $1,000 to make. </p>

<p>I have a set of characters A-Z (26) and I need to find an efficient combination of 3 letters.</p>

<p>ie., 
Option 1) Create 26 molds where each mold has a single character
Option 2) Create 13 molds where each mold has two characters
(i.e., 3x2 for A, 3x2 for B in an AB-mold)
Option 3) Create n molds where each mold has three characters</p>

<p>ie., based on tokenization of 3-letter combinations in the english language, is there a solution for common 3-letter pairing that each letter is at least represented once and the total mold count is &lt;13 (option 2)</p>

<p><a href=""http://www.rollingr.net/wordpress/2007/02/02/common-letter-sequence/"" rel=""nofollow"">http://www.rollingr.net/wordpress/2007/02/02/common-letter-sequence/</a></p>

<p>For example, for Option 3) I would imagine there would be a single mold for 'ING' but perhaps also one for 'ION'</p>

<p>Is there a minimum number of trigraphs that would cover the entire alphabet?</p>
",Multilingual Language Processing & Language Identification,nlp methodology python find character common combination real world meet math world problem creating cupcake mold x cavity mold mold cost make set character z need find efficient combination letter ie option create mold mold ha single character option create mold mold ha two character e x x b ab mold option create n mold mold ha three character ie based tokenization letter combination english language solution common letter pairing letter least represented total mold count option example option would imagine would single mold ing perhaps also one ion minimum number trigraphs would cover entire alphabet
accumulating results for each row in a text while looping,"<p>Why I'm having accumulating results for each row in this example:</p>

<p>my text example contains two rows:</p>

<p>he</p>

<p>the</p>

<p>I need the result for each row in a my text as a separate result </p>

<pre><code>for row in sample_file:
    for word in row.split():
        for c in word:
            product = product*(float(char_value_en.get(c, 0)))
final_prob_es = prob_es*tweet_prob_es
print 'Spanish Prob'+' '+'='+' '+str(final_prob_es)
print row
</code></pre>

<p>my output is:</p>

<p>Spanish Prob = 0.698565816073</p>

<p>he</p>

<p>Spanish Prob = 0.836227448039</p>

<p>the</p>

<p>Spanish Prob = 0.957639196166</p>

<p>he</p>

<p>Spanish Prob = 1.14635466369</p>

<p>the</p>
",Multilingual Language Processing & Language Identification,accumulating result row text looping accumulating result row example text example contains two row need result row text separate result output spanish prob spanish prob spanish prob spanish prob
assign numeric values from a text to characters in a string using python 2,"<p>if I have a text file contains all english alphabets with some corresponding value like the following:</p>

<p>A   0.00733659550399</p>

<p>B   0.00454138879023</p>

<p>C   0.00279849519224</p>

<p>D   0.00312734304092</p>

<p>.</p>

<p>.</p>

<p>.</p>

<p>I want to assign these numeric values to each line I'm reading from another txt file. </p>

<pre><code>L = open(os.path.join(dir, file), ""r"").read()
            line = L.rstrip()
            tokens = line.split()

            for word in tokens:
                for char in word:
</code></pre>

<p>find</p>
",Multilingual Language Processing & Language Identification,assign numeric value text character string using python text file contains english alphabet corresponding value like following b c want assign numeric value line reading another txt file find
"Preserving the order/occurrence of an adjective,noun label-id with a regular expression?","<p>Im new with python could anybody help me on how can to create a regular expresion given a list of strings like this:</p>

<pre><code>  test_string =  ""pero pero CC 
    tan tan RG
    antigua antiguo AQ0FS0
    que que CS 
    seg√∫n seg√∫n SPS00 
    mi mi DP1CSS 
    madre madre NCFS000""
</code></pre>

<p>how to return a tuple like this where NCFS00 denotes female noun and AQ0FS0 denotes female adjective (in spanish), the morphological order is important, since they have to match as the following example:</p>

<pre><code>&gt; ([madre, NCFS00],[antigua, AQ0FS0]) 
</code></pre>

<p>I would like to return the word with it¬¥s associated tag given <code>test_string</code> if and only if they have the adjective, noun morphological occurrence (e.g. <code>[Noun, NCFS00],[Adjective, AQ0FS0]</code>) this is what i done:</p>

<pre><code># -- coding: utf-8 --
import re


#str = ""pero pero CC "" \
   ""tan tan RG "" \
   ""antigua antiguo AQ0FS0"" \
    ""que que CS "" \
    ""seg√∫n seg√∫n SPS00 "" \
    ""mi mi DP1CSS "" \
    ""madre madre NCFS000""

tupla1 = re.findall(r'(\w+)\s\w+\s(AQ0FS0)', str)
print tupla1

tupla2 = re.findall(r'(\w+)\s\w+\s(NCFS00)',str)
print tupla2
</code></pre>

<p>the output is the following:</p>

<blockquote>
  <p>[('antigua', 'AQ0FS0')] [('madre', 'NCFS00')]</p>
</blockquote>

<p>The problem with this output is that if i pass it a long <code>test_string</code> i need to preserve the ""order"" or ""occurrence"" of the tags (i.e. i only can print a tuple if and only if they have the following order: AQ0FS0 and NCFS000 in oder words: female adjective, female noun). </p>

<p>For example, if i pass a string like this:</p>

<pre><code> str = ''' Hola hola I 1
compis compis NCMS000 0.500006
! ! Fat 1

No no RN 0.998045
sab√≠a saber VMII3S0 0.592869
como como CS 0.999289
se se P00CN000 0.465639
pon√≠a poner VMII3S0 0.65
una uno DI0FS0 0.951575
lavadora lavadora NCFS000 0.414738
hasta hasta SPS00 0.957698
que que PR0CN000 0.562517
conoc√≠ conocer VMIS1S0 1
esta este DD0FS0 0.986779
y y CC 0.999962
es ser VSIP3S0 1
que que CS 0.437483
es ser VSIP3S0 1
muy muy RG 1
sencilla sencillo AQ0FS0 1
de de SPS00 0.999984
utilizar utilizar VMN0000 1
! ! Fat 1

Todo todo DI0MS0 0.560961
un uno DI0MS0 0.987295
gustazo gustazo NCMS000 1
cuando cuando CS 0.985595
estamos estar VAIP1P0 1
aprendiendo aprender VMG0000 1
para para SPS00 0.999103
emancipar emancipar VMN0000 1
nos nos PP1CP000 1
, , Fc 1
que que CS 0.437483
si si CS 0.99954
nos nos PP1CP000 0.935743
ponen poner VMIP3P0 1
facilidad facilidad NCFS000 1
con con SPS00 1
las el DA0FP0 0.970954
tareas tarea NCFP000 1
de de SPS00 0.999984
la el DA0FS0 0.972269
casa casa NCFS000 0.979058
pues pues CS 0.998047
mejor mejor AQ0CS0 0.873665
que que PR0CN000 0.562517
mejor mejor AQ0CS0 0.873665
. . Fp 1

Antes_de antes_de SPS00 1
esta este PD0FS000 0.0132212
ten√≠amos tener VMII1P0 1
otra otro DI0FS0 0.803899
de de SPS00 0.999984
la el DA0FS0 0.972269
marca marca NCFS000 0.972603
Otsein otsein NP00000 1
, , Fc 1
de de SPS00 0.999984
estas este DD0FP0 0.97043
que que PR0CN000 0.562517
van ir VMIP3P0 1
incluidas incluir VMP00PF 1
en en SPS00 1
el el DA0MS0 1
mobiliario mobiliario NCMS000 0.476077
y y CC 0.999962
adem√°s adem√°s RG 1
era ser VSII1S0 0.491262
de de SPS00 0.999984
carga carga NCFS000 0.952569
superior superior AQ0CS0 0.992424
, , Fc 1
pero pero CC 0.999764
tan tan RG 1
antigua antiguo AQ0FS0 0.953488
que que CS 0.437483
seg√∫n seg√∫n SPS00 0.995943
mi mi DP1CSS 0.999101
madre madre NCFS000 1
, , Fc 1
nadie nadie PI0CS000 1
la lo PP3FSA00 0.0277039
pod√≠a poder VMII3S0 0.63125
tocar tocar VMN0000 1
porque porque CS 1
solo solo RG 0.0472103
la lo PP3FSA00 0.0277039
entend√≠a entender VMII3S0 0.65
ella √©l PP3FS000 1
. . Fp 1

Esta este PD0FS000 0.0132212
es ser VSIP3S0 1
de de SPS00 0.999984
la el DA0FS0 0.972269
marca marca NCFS000 0.972603
Aeg aeg NP00000 1
y y CC 0.999962
dentro_de dentro_de SPS00 1
este este DD0MS0 0.960092
tipo tipo NCMS000 1
de de SPS00 0.999984
lavadoras lavadora NCFP000 0.411969
de de SPS00 0.999984
esta este DD0FS0 0.986779
marca marca NCFS000 0.972603
las lo PP3FPA00 0.0289466
hab√≠a haber VAII1S0 0.353863
m√°s m√°s RG 1
caras caro AQ0FP0 0.417273
o o CC 0.999769
m√°s m√°s RG 1
baratas barato AQ0FP0 0.3573
y y CC 0.999962
est√° estar VAIP3S0 0.999201
digamos decir VMSP1P0 0.785925
que que CS 0.437483
est√° estar VAIP3S0 0.999201
en en SPS00 1
el el DA0MS0 1
punto punto NCMS000 1
medio medio AQ0MS0 0.314286
. . Fp 1

Es ser VSIP3S0 1
de de SPS00 0.999984
color color NCMS000 1
blanco blanco AQ0MS0 0.598684
y y CC 0.999962
tiene tener VMIP3S0 1
carga carga NCFS000 0.952569
frontal frontal AQ0CS0 0.657209
, , Fc 1
con con SPS00 1
una uno DI0FS0 0.951575
capacidad capacidad NCFS000 1
de de SPS00 0.999984
6kg 6kg Z 1
. . Fp 1

En en SPS00 1
casa casa NCFS000 0.979058
a_pesar_de a_pesar_de SPS00 1
ser ser VSN0000 0.940705
cuatro 4 Z 1
, , Fc 1
se se P00CN000 0.465639
ponen poner VMIP3P0 1
lavadoras lavadora NCFP000 0.411969
casi casi RG 1
todos todo DI0MP0 0.624221
o o CC 0.999769
todos todo DI0MP0 0.624221
los el DA0MP0 0.976481
d√≠as d√≠a NCMP000 1
. . Fp 1

En en SPS00 1
su su DP3CS0 1
parte parte NCFS000 0.499183
de de SPS00 0.999984
arriba arriba RG 0.986014
encontramos encontrar VMIP1P0 0.65
la el DA0FS0 0.972269
"" "" Fe 1
; ; Fx 1
zona zona NCFS000 1
de de SPS00 0.999984
mandos mando NCMP000 1
"" "" Fe 1
; ; Fx 1
, , Fc 1
donde donde PR000000 0.967437
se se P00CN000 0.465639
puede poder VMIP3S0 0.999117
echar echar VMN0000 1
el el DA0MS0 1
detergente detergente NCMS000 0.49273
, , Fc 1
aunque aunque CC 1
en en SPS00 1
nuestro nuestro DP1MSP 0.94402
caso caso NCMS000 0.99812
lo lo PP3CNA00 0.271177
a a SPS00 1
el el DA0MS0 1
ser ser VSN0000 0.940705
gel gel NCMS000 1
lo lo PP3CNA00 0.271177
ponemos poner VMIP1P0 1
directamente directamente RG 1
junto_con junto_con SPS00 1
la el DA0FS0 0.972269
ropa ropa NCFS000 1
. . Fp 1

Luego luego RG 0.996689
tiene tener VMIP3S0 1
la el DA0FS0 0.972269
rueda rueda NCFS000 0.72093
para para SPS00 0.999103
elegir elegir VMN0000 1
el el DA0MS0 1
programa programa NCMS000 0.953488
y y CC 0.999962
los el DA0MP0 0.976481
intermitentes intermitente NCMP000 0.342773
que que PR0CN000 0.562517
indican indicar VMIP3P0 1
en en SPS00 1
que que CS 0.437483
paso paso NCMS000 0.905738
de de SPS00 1
el el DA0MS0 1
programa programa NCMS000 0.953488
estaba estar VAII1S0 0.5
. . Fp 1

Como como CS 0.999289
todas todo PI0FP000 0.0490506
tiene tener VMIP3S0 1
programas programa NCMP000 0.97619
m√°s m√°s RG 1
cortos corto AQ0MP0 1
y y CC 0.999962
m√°s m√°s RG 1
largos largo AQ0MP0 0.97619
, , Fc 1
incluso incluso RG 0.996383
un uno DI0MS0 0.987295
programa programa NCMS000 0.953488
que que PR0CN000 0.562517
seria seriar VMIP3S0 0.151546
como como CS 0.999289
lavar lavar VMN0000 1
a a SPS00 0.996023
mano mano NCFS000 0.992095
y y CC 0.999962
otro otro DI0MS0 0.612994
ideal ideal NCMS000 0.5
para para SPS00 0.999103
estores estor NCMP000 1
, , Fc 1
que que PR0CN000 0.562517
salen salir VMIP3P0 0.972603
casi casi RG 1
secos seco AQ0MP0 1
y y CC 0.999962
planchaditos planchar VMP00PM 0.691767
para para SPS00 0.999103
colgar colgar VMN0000 1
y y CC 0.999962
ya ya RG 0.999395
est√° estar VAIP3S0 0.999201
. . Fp 1

Es ser VSIP3S0 1
muy muy RG 1
f√°cil f√°cil AQ0CS0 1
de de SPS00 0.999984
aprender aprender VMN0000 1
la lo PP3FSA00 1
y y CC 0.999962
adem√°s adem√°s RG 1
tiene tener VMIP3S0 1
indicador indicador NCMS000 0.64273
por por SPS00 1
sonido sonido NCMS000 1
de de SPS00 0.999984
cuando cuando CS 0.985595
acaba acabar VMIP3S0 0.992958
, , Fc 1
lista listar VMIP3S0 0.220088
para para SPS00 0.999103
abrir abrir VMN0000 1
y y CC 0.999962
tender tender VMN0000 1
. . Fp 1

Por por SPS00 1
decir decir VMN0000 0.997512
algo algo PI0CS000 0.900246
malo malo AQ0MS0 0.657087
de de SPS00 0.999984
ella √©l PP3FS000 1
, , Fc 1
ser√≠a ser VSIC1S0 0.5
que que CS 0.437483
cuando cuando CS 0.985595
centrifuga centrifugar VMIP3S0 0.994859
, , Fc 1
algo algo PI0CS000 0.900246
que que PR0CN000 0.562517
hace hacer VMIP3S0 1
muy muy RG 1
bien bien RG 0.902728
, , Fc 1
pues pues CS 0.998047
vibra vibrar VMIP3S0 0.994856
un uno DI0MS0 0.987295
poco poco RG 0.542435
y y CC 0.999962
se se P00CN000 0.465639
nota notar VMIP3S0 0.419995
el el DA0MS0 1
ruido ruido NCMS000 1
jeje jeje NCMS000 0.833445
, , Fc 1
pero pero CC 0.999764
no no RN 0.998134
se se P00CN000 0.465639
mueve mover VMIP3S0 0.994868
de de SPS00 0.999984
su su DP3CS0 1
sitio sitio NCMS000 0.980769
! ! Fat 1
! ! Fat 1

Saludillos saludillos NP00000 0.411768
! ! Fat 1

'''
</code></pre>

<p>I have the following output: </p>

<pre><code>[('sencilla', 'AQ0FS0'), ('antigua', 'AQ0FS0')]
[('lavadora', 'NCFS00'), ('facilidad', 'NCFS00'), ('casa', 'NCFS00'), ('marca', 'NCFS00'), ('carga', 'NCFS00'), ('madre', 'NCFS00'), ('marca', 'NCFS00'), ('marca', 'NCFS00'), ('carga', 'NCFS00'), ('capacidad', 'NCFS00'), ('casa', 'NCFS00'), ('parte', 'NCFS00'), ('zona', 'NCFS00'), ('ropa', 'NCFS00'), ('rueda', 'NCFS00'), ('mano', 'NCFS00')]
</code></pre>

<p>Which don¬¥t preserve the previous  Noun, adjective order.</p>
",Multilingual Language Processing & Language Identification,preserving order occurrence adjective noun label id regular expression im new python could anybody help create regular expresion given list string like return tuple like ncfs denotes female noun aq f denotes female adjective spanish morphological order important since match following example would like return word associated tag given adjective noun morphological occurrence e g done output following antigua aq f madre ncfs problem output pas long need preserve order occurrence tag e print tuple following order aq f ncfs oder word female adjective female noun example pas string like following output preserve previous noun adjective order
How to determine if a string is English sentence or code?,"<p>Consider the following two strings, the first one is code, the second one is English sentence (phrase to be precise). How can I detect that the first one is code and the second is not. </p>

<pre><code>1. for (int i = 0; i &lt; b.size(); i++) {
2. do something in English (not necessary to be a sentence).
</code></pre>

<p>I'm thinking about counting special characters (such as ""="", "";"", ""++"", etc ), and set if to some threshold.  Are there any better ways to do this? Any Java libraries? </p>

<p>Note that the code may not parsable, because it is not a complete method/statement/expression.</p>

<p>My assumption is that English sentences are pretty regular, it most likely contains only "","", ""."", ""_"", ""("", "")"", etc. They do not contains something like this: <code>write(""the whole lot of text"");</code></p>
",Multilingual Language Processing & Language Identification,determine string english sentence code consider following two string first one code second one english sentence phrase precise detect first one code second thinking counting special character etc set threshold better way java library note code may parsable complete method statement expression assumption english sentence pretty regular likely contains etc contains something like
Can IBM-Watson infer from the context of sentences?,"<p>I mean, for example, when we say some things to describe a scene, Can Watson understand how that scene is or look like? Cyc cop tried to design common sense for AI, like understanding and inferring from sentences from knowledge base systems, but Watson uses statistical methods for Natural Language Processing (NLP). Cyc Corp believes, with statistical analysis cannot make AI to infer and we need underlying knowledge to infer.</p>
",Multilingual Language Processing & Language Identification,ibm watson infer context sentence mean example say thing describe scene watson understand scene look like cyc cop tried design common sense ai like understanding inferring sentence knowledge base system watson us statistical method natural language processing nlp cyc corp belief statistical analysis make ai infer need underlying knowledge infer
Semi Natural language Search using Apache Solr,"<p>I did some analysis on Apache Solr and its pretty good to search data from various sources. 
The problem I am facing is how do I standardize my search grammar and translate search text into Solr query.</p>

<p>I have three types of file/database table to search from - namely Customer, Industry and Unit. The first keyword in the search box should be any of the three. After that, the user can define a fix set of criteria: </p>

<pre><code>Metrics : 0 or many (ex, exposure, income, revenue, loan_amt etc)
Dimension : 0 or many (Geography, region, etc)
</code></pre>

<p>Example:</p>

<pre><code>customer - Returns all customer data from customer core
customer income from Asia - Returns all customer income details who belongs to Asia 
customer income revenue from Asia - Returns all customer income and revenue details who belongs to Asia 
</code></pre>

<p>How can I translate the above natural language search text to solr query?
Can I fix my grammar of text in Solr like 
first keyword should be customer/industry/unit,
second key-value would be one or more region/geography
and then metric values. </p>

<p>I am not looking for google like search but a limited search where the user knows what to search. </p>
",Multilingual Language Processing & Language Identification,semi natural language search using apache solr analysis apache solr pretty good search data various source problem facing standardize search grammar translate search text solr query three type file database table search namely customer industry unit first keyword search box three user define fix set criterion example translate natural language search text solr query fix grammar text solr like first keyword customer industry unit second key value would one region geography metric value looking google like search limited search user know search
The NLP Tagger called SENNA,"<p>I have a question about the nlp tagger called SENNA, that is developed by Collbert and his colleagues based on their paper: Natural Language Processing (almost) from Scratch.</p>

<p>Does SENNA (it's code which available at this address: <a href=""http://ronan.collobert.com/senna/download.html"" rel=""nofollow"">http://ronan.collobert.com/senna/download.html</a>) contain any code for training the neural network? </p>

<p>Or it just uses information that is obtained by training the network (it is trained beforehand and its code is not in SENNA)?</p>
",Multilingual Language Processing & Language Identification,nlp tagger called senna question nlp tagger called senna developed collbert colleague based paper natural language processing almost scratch doe senna code available address contain code training neural network us information obtained training network trained beforehand code senna
"Language detection for pinyin, translit etc?","<p>Real-world user-generated text in non-Latin alphabet languages is often not in canonical form but in <a href=""https://en.wikipedia.org/wiki/Translit"" rel=""nofollow noreferrer"">translit</a>, <a href=""https://en.wikipedia.org/wiki/Romanization_of_Bulgarian#Informal_writing"" rel=""nofollow noreferrer"">shlyokavitsa</a>, <a href=""https://en.wikipedia.org/wiki/Arabic_chat_alphabet"" rel=""nofollow noreferrer"">arabizi</a>, pinyin and so on.  Language detection software is starting to handle it <a href=""https://translate.google.com/#auto/en/vot%20tak%20dolzhna%20byt&#39;%20umnaya%20sistema"" rel=""nofollow noreferrer"">smartly</a>, but usually it <a href=""https://translate.google.com/#auto/en/Chi%20ashxatum%2C%20apsos"" rel=""nofollow noreferrer"">doesn't work</a>, even though it's technically fairly trivial to incorporate it.</p>

<p><a href=""https://i.sstatic.net/ROWOC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ROWOC.png"" alt=""enter image description here""></a></p>

<p>Is there a language detection system that is handling these informal Latinisations well?  (Ideally a Python lib, but any language or service would be interesting.)</p>

<p>The Yandex, Microsoft and top Python lang id libs, like <a href=""https://github.com/saffsd/langid.py"" rel=""nofollow noreferrer"">langid</a>, have nothing on this front.  Two that halfway work are known to me, both from Google:<br>
- <a href=""https://github.com/CLD2Owners/cld2"" rel=""nofollow noreferrer"">CLD</a>, which is <a href=""https://developer.chrome.com/extensions/i18n#method-detectLanguage"" rel=""nofollow noreferrer"">part of Chrome</a><br>
- <a href=""https://cloud.google.com/translate/v2/using_rest#detect-language"" rel=""nofollow noreferrer"">the Google Translate API</a><br>
Besides only recognising translit for a few top languages, they are not ideal for a variety of reasons (accuracy, performance, price...)</p>

<p>This is a major issue for major languages like Hindi, Persian, Chinese, Arabic and Russian, and for all the other languages not written in the Latin alphabet but commonly Latinised (Romanised) online.</p>
",Multilingual Language Processing & Language Identification,language detection pinyin translit etc real world user generated text non latin alphabet language often canonical form translit shlyokavitsa arabizi pinyin language detection software starting handle smartly usually work even though technically fairly trivial incorporate language detection system handling informal latinisations well ideally python lib language service would interesting yandex microsoft top python lang id libs like langid nothing front two halfway work known google cld part chrome google translate api besides recognising translit top language ideal variety reason accuracy performance price major issue major language like hindi persian chinese arabic russian language written latin alphabet commonly latinised romanised online
How to compile java file which calls MeCab - Japanese part-of-speech &amp; morphological analyzer?,"<p>I'm trying to use MeCab (http://mecab.sourceforge.net/#download) to do the word segmentation of Japanese sentences as well to tag every word by part of speech. I installed MeCab by following these instructions <a href=""http://mecab.sourceforge.net/#install-unix"" rel=""nofollow"">http://mecab.sourceforge.net/#install-unix</a>. Since I don't want to write shell scripts to process 150,000 sentences (as my Mac OS X Terminal have problems showing Japanese characters), I'm using existing binding for Java: <a href=""http://sourceforge.net/projects/mecab/files/mecab-java/0.98pre3/"" rel=""nofollow"">http://sourceforge.net/projects/mecab/files/mecab-java/0.98pre3/</a>. At this point I'm trying to compile and run the given test.java file:</p>

<pre><code>import org.chasen.mecab.MeCab;
import org.chasen.mecab.Tagger;
import org.chasen.mecab.Node;

public class test {
  static {
    try {
       System.loadLibrary(""MeCab"");
    } catch (UnsatisfiedLinkError e) {
       System.err.println(""Cannot load the example native code.\nMake sure your LD_LIBRARY_PATH contains \'.\'\n"" + e);
       System.exit(1);
    }
  }

  public static void main(String[] argv) {
     System.out.println(MeCab.VERSION);
     Tagger tagger = new Tagger();
     String str = ""Â§™ÈÉé„ÅØ‰∫åÈÉé„Å´„Åì„ÅÆÊú¨„ÇíÊ∏°„Åó„Åü„ÄÇ"";
     System.out.println(tagger.parse(str));
     Node node = tagger.parseToNode(str);
     for (;node != null; node = node.getNext()) {
    System.out.println(node.getSurface() + ""\t"" + node.getFeature());
     }
     System.out.println (""EOS\n"");
  }
}
</code></pre>

<p>Here's the README:</p>

<pre><code>1. Build UTF-8 dictionary

2. How to use?

  See test.java as sample program.

  % java -classpath MeCab.jar test -d ../dic
</code></pre>

<p>I compile: javac test.java. Then I run: java -classpath MeCab.jar test -d ../dic. The result is the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: //
Caused by: java.lang.ClassNotFoundException: ..
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
</code></pre>

<p>I don't really understand the hierarchy of this mecab-java-0.98pre3 directory, so don't see how to actually compile and run this test.java. Any ideas, guys? Thanks! </p>
",Multilingual Language Processing & Language Identification,compile java file call mecab japanese part speech morphological analyzer trying use mecab word segmentation japanese sentence well tag every word part speech installed mecab following instruction since want write shell script process sentence mac x terminal problem showing japanese character using existing binding java point trying compile run given test java file readme compile javac test java run java classpath mecab jar test dic result following error really understand hierarchy mecab java pre directory see actually compile run test java idea guy thanks
How to Stem Shakespere/KJV Using nltk.stem.snowball,"<p>I want to stem early modern English text:</p>

<pre><code>sb.stem(""loveth"")
&gt;&gt;&gt; ""lov""
</code></pre>

<p>Apparently, all I need to do is <a href=""http://snowball.tartarus.org/texts/earlyenglish.html"" rel=""nofollow"">a small tweak</a> to the Snowball Stemmer:</p>

<blockquote>
  <p>And to put the endings into the English stemmer, the list</p>
  
  <blockquote>
    <p>ed   edly   ing   ingly</p>
  </blockquote>
  
  <p>of Step 1b should be extended to</p>
  
  <blockquote>
    <p>ed   edly   ing   ingly   est   eth</p>
  </blockquote>
  
  <p>As far as the Snowball scripts are concerned, the endings  'est' 'eth'  must be added against ending  'ing'. </p>
</blockquote>

<p>Great, so I just have to change the variables. Perhaps add a special rule to deal with ""thee""/""thou""/""you"" and ""shalt""/""shall"". The <a href=""http://www.nltk.org/api/nltk.stem.html"" rel=""nofollow"">NLTK documentation</a> show the variables as:</p>

<blockquote>
  <p><strong>class</strong> nltk.stem.snowball.EnglishStemmer(ignore_stopwords=False)</p>
  
  <p><strong>Bases:</strong> nltk.stem.snowball._StandardStemmer</p>
  
  <p><strong>The English Snowball stemmer.</strong></p>
  
  <p><strong>Variables:</strong>    </p>
  
  <blockquote>
    <p>__vowels ‚Äì The English vowels.</p>
    
    <p>__double_consonants ‚Äì The English double consonants.</p>
    
    <p>__li_ending ‚Äì Letters that may directly appear before a word final ‚Äòli‚Äô.</p>
    
    <p>__step0_suffixes ‚Äì Suffixes to be deleted in step 0 of the algorithm.</p>
    
    <p>__step1a_suffixes ‚Äì Suffixes to be deleted in step 1a of the algorithm.</p>
    
    <p><strong>__step1b_suffixes ‚Äì Suffixes to be deleted in step 1b of the algorithm. (Here we go)</strong></p>
    
    <p>__step2_suffixes ‚Äì Suffixes to be deleted in step 2 of the algorithm.</p>
    
    <p>__step3_suffixes ‚Äì Suffixes to be deleted in step 3 of the algorithm.</p>
    
    <p>__step4_suffixes ‚Äì Suffixes to be deleted in step 4 of the algorithm.</p>
    
    <p>__step5_suffixes ‚Äì Suffixes to be deleted in step 5 of the algorithm.</p>
    
    <p><strong>__special_words ‚Äì A dictionary containing words which have to be stemmed specially. (I can stick my ""thee""/""thou"" and ""shalt"" issues here)</strong></p>
  </blockquote>
</blockquote>

<p>Now, dumb question. How do I change the variable? Everywhere I've looked for the variables, I keep getting ""object has no attribute""...</p>
",Multilingual Language Processing & Language Identification,stem shakespere kjv using nltk stem snowball want stem early modern english text apparently need small tweak snowball stemmer put ending english stemmer list ed edly ing ingly step b extended ed edly ing ingly est eth far snowball script concerned ending est eth must added ending ing great change variable perhaps add special rule deal thee thou shalt shall nltk documentation show variable class nltk stem snowball englishstemmer ignore stopwords false base nltk stem snowball standardstemmer english snowball stemmer variable vowel english vowel double consonant english double consonant li ending letter may directly appear word final li step suffix suffix deleted step algorithm step suffix suffix deleted step algorithm step b suffix suffix deleted step b algorithm go step suffix suffix deleted step algorithm step suffix suffix deleted step algorithm step suffix suffix deleted step algorithm step suffix suffix deleted step algorithm special word dictionary containing word stemmed specially stick thee thou shalt issue dumb question change variable everywhere looked variable keep getting object ha attribute
Creating Customer Self Service Using NLP APIS?,"<p>i want to create a customer self service Using NLP (Natural Language Processing). 
simply the user will enter search sentence like ""How do I pay the weekly fee?"" and the Self Service will answer the user with the most relevant FAQs  and their answers . </p>

<ol>
<li>i will use <a href=""http://www.alchemyapi.com/products/demo/alchemylanguage"" rel=""nofollow"">Alchemyapi</a> (NLP APIs) to analyze the user search sentence.</li>
<li>search in the DB (Tagged Answers will be saved there) using the keywords &amp; Entities returned by the NLP APIs.</li>
<li>show results to user.</li>
<li>update the answers tags according to the user feedback (ranks ) . </li>
</ol>

<p>this is the logic i will go through to build a simple self service can any one suggest efficient way to do that or an exist APIs do that for me .</p>

<p>thanks in advance  </p>
",Multilingual Language Processing & Language Identification,creating customer self service using nlp apis want create customer self service using nlp natural language processing simply user enter search sentence like pay weekly fee self service answer user relevant faq answer use alchemyapi nlp apis analyze user search sentence search db tagged answer saved using keywords entity returned nlp apis show result user update answer tag according user feedback rank logic go build simple self service one suggest efficient way exist apis thanks advance
Discover Domain-Specific Attributes in Text,"<p>I'm trying to search some Natural Language Processing technique to find special attributes in Text, like entrance, amount of parcels, color of cloth or other types using some supervised-learning or semi-supervised learning (I've tried Stanford Bootstrapped Entity Learning / SPIED).</p>

<p>I don't know exactly how can I search for that in other places. I've tried using Hidden Markov Chains and CRF Classifier for that purposes, but I didn't get the expected output.</p>

<p>Example:</p>

<p>""I want buy a new <strong>black</strong> car in <strong>30 parcels</strong> with <strong>entrance of $1000</strong>""</p>

<p>How can I do that?</p>
",Multilingual Language Processing & Language Identification,discover domain specific attribute text trying search natural language processing technique find special attribute text like entrance amount parcel color cloth type using supervised learning semi supervised learning tried stanford bootstrapped entity learning spied know exactly search place tried using hidden markov chain crf classifier purpose get expected output example want buy new black car parcel entrance
NLP sentence analysis,"<p>I was solving problem 7 of chapter 8 from Natural Language Processing with Python.I know how to make grammar and draw tree. But I am stuck at sentence compression using NLP. 
The problem is given below: </p>

<p><strong>Analyze the A.A. Milne sentence about Piglet, by underlining all of the sentences it contains then replacing these with S (e.g. the first sentence becomes S when:lx` S). Draw a tree structure for this ""compressed"" sentence. What are the main syntactic constructions used for building such a long sentence?</strong></p>

<p>I am not able to understand what does it mean in below sentence.
""by underlining all of the sentences it contains then replacing these with S (e.g. the first sentence becomes S when: lx` S).""</p>

<p>The text which needs to be analyzed is as below :</p>

<p><strong>[You can imagine Piglet's joy when at last the ship came in sight of him.] In after-years he liked to think that he had been in Very Great Danger during the Terrible Flood, but the only danger he had really been in was the last half-hour of his imprisonment, when Owl, who had just flown up, sat on a branch of his tree to comfort him, and told him a very long story about an aunt who had once laid a seagull's egg by mistake, and the story went on and on, rather like this sentence, until Piglet who was listening out of his window without much hope, went to sleep quietly and naturally, slipping slowly out of the window towards the water until he was only hanging on by his toes, at which moment, luckily, a sudden loud squawk from Owl, which was really part of the story, being what his aunt said, woke the Piglet up and just gave him time to jerk himself back into safety and say, ""How interesting, and did she?"" when ‚Äî well, you can imagine his joy when at last he saw the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear) coming over the sea to rescue him...</strong></p>

<p>Any help would be great.</p>
",Multilingual Language Processing & Language Identification,nlp sentence analysis wa solving problem chapter natural language processing python know make grammar draw tree stuck sentence compression using nlp problem given analyze milne sentence piglet underlining sentence contains replacing e g first sentence becomes lx text need analyzed imagine piglet joy last ship came sight year liked think great danger terrible flood danger really wa last half hour imprisonment owl flown sat branch tree told long story aunt seagull egg mistake story went rather like sentence piglet wa listening window without much hope went sleep quietly naturally slipping slowly window towards water wa hanging toe moment luckily sudden loud squawk owl wa really part story aunt said woke piglet gave time jerk back safety say interesting well imagine joy last saw good ship brain pooh captain c robin st mate p bear coming sea rescue help would great
Is there any corpora for automatic text summarization?,"<p>I'd like to work on different task related to text summarization:</p>

<ul>
<li>topic/keyword extraction</li>
<li>sentence extraction</li>
</ul>

<p>What english corporus exist to help in this task?</p>
",Multilingual Language Processing & Language Identification,corpus automatic text summarization like work different task related text summarization topic keyword extraction sentence extraction english corporus exist help task
Tagging references/citations in text,"<p>I need to find a way to tag references to publications in text. We've been doing this via regex but it won't work these new patterns.</p>
<p>Some examples (language is german):</p>
<blockquote>
<p>Herzog (August 2012), Einkommensteuerskriptum Band 1, S 8</p>
<p>Achatz/Bieber in Achatz/Kirchmayr, K√∂rperschaftsteuergesetz (2011)</p>
<p>Heinrich in Quantschnigg/Renner/Schellmann/St√∂ger, Die K√∂rperschaftsteuer (2013) ¬ß 7 Rz 32</p>
<p>Raab/Renner in Quantschnigg/Renner/Schellmann/St√∂ger/Vock, Die K√∂rperschaftsteuer, 24. Lfg., ¬ß 8 Tz 292,293</p>
<p>Quantschnigg/Renner/Schellmann/St√∂ger/Vock (Hrsg), KStG<sup>23</sup> (2013) ¬ß 13 Rz 67</p>
</blockquote>
<p>So it mostly starts out with author names and the Title of the publication but then it becomes pretty diverse. It might not look as bad in the examples but I could give a bunch more that again look differently.</p>
<p>So I thought this might be a task for machine learning. However having very little experience in that field i find it hard to find the right technique.</p>
<p>I found POS tagging but that doesn't seem to be the way to go here.
I also stumbled upton CRF but there is little material on it that would get a beginner like myself started.</p>
<p>I've done some classification and regression in sklearn but that's about it.</p>
<p>Could anyone point me in the right direction ?</p>
",Multilingual Language Processing & Language Identification,tagging reference citation text need find way tag reference publication text via regex work new pattern example language german herzog august einkommensteuerskriptum band achatz bieber achatz kirchmayr k rperschaftsteuergesetz heinrich quantschnigg renner schellmann st ger die k rperschaftsteuer rz raab renner quantschnigg renner schellmann st ger vock die k rperschaftsteuer lfg tz quantschnigg renner schellmann st ger vock hrsg kstg rz mostly start author name title publication becomes pretty diverse might look bad example could give bunch look differently thought might task machine learning however little experience field find hard find right technique found po tagging seem way go also stumbled upton crf little material would get beginner like started done classification regression sklearn could anyone point right direction
Finding the semantics of webpage content,"<p>I need to find the semantics of text in webpages.
i.e finding out if a webpage content is a poem or a conversation or an essay, etc
I could sense that this could be done using machine learning and natural language processing. It will be helpful if someone could provide more information about the techniques that could be followed as well as reference to some research paper.</p>
",Multilingual Language Processing & Language Identification,finding semantics webpage content need find semantics text webpage e finding webpage content poem conversation essay etc could sense could done using machine learning natural language processing helpful someone could provide information technique could followed well reference research paper
Language detection in R with the textcat package : how to restrict to a few languages?,"<p>I need to detect the language of many short texts, using R.
I am using the textcat package, which find which among many (say 30) European
languages is the one of each text. However, I know my texts are either French or English (or, more generally, a small subset of the langages handled by textcat). </p>

<p>How could add this knowledge when calling textcat functions ?</p>

<p>Thanks,</p>
",Multilingual Language Processing & Language Identification,language detection r textcat package restrict language need detect language many short text using r using textcat package find among many say european language one text however know text either french english generally small subset langages handled textcat could add knowledge calling textcat function thanks
"How to separate, tag and transpose different pieces of text in XML with Python?","<p>Disclaimer: I am still a novice when it comes to coding.</p>

<p>I am editing a lexicon using Python. Now say I have the following mark-up:</p>

<p><code>&lt;ab n=""7"" ana=""lg11"" lang=""grk""&gt;&lt;anchor id=""lg11_7""/&gt;&lt;link type=""gnoo"" targets=""aGNO_25_t"" n=""g25_t_126_18"" id=""SuperfluousIdlgo_03_0004_30""&gt;&lt;hi rend=""b""&gt;25&lt;/hi&gt;,126,18&lt;/link&gt; (h√§ufig verneint:) nicht erlauben, nicht gestatten: Œø·ΩêŒ∫ ·ºê·æ∑ Œ∫Œ±œÑ·æΩ Œ±·ΩêœÑ·ø∂ŒΩ Œ¥œÖŒΩŒ±œÉœÑŒµ·ΩªŒµŒπŒΩ Œº·Ω≥œáœÅŒπ œëŒ±ŒΩ·Ω±œÑŒøœÖ œÑ·Ω¥ŒΩ ŒΩ·ΩπœÉŒøŒΩ&lt;/ab&gt;</code></p>

<p>I need to separate out the German text from the Greek, and put it in its own <code>&lt;ab&gt;</code> tag. Then I need to have the German text in front of the locus and the Greek text. How do I accomplish this? I have searched on Regex and ElementTree but I haven't found anything yet. </p>
",Multilingual Language Processing & Language Identification,separate tag transpose different piece text xml python disclaimer still novice come coding editing lexicon using python say following mark need separate german text greek put tag need german text front locus greek text accomplish searched regex elementtree found anything yet
Can an author&#39;s unique &quot;literary style&quot; be used to identify him/her as the author of a text?,"<p>Let's imagine, I have two English language texts written by the same person.
Is it possible to apply some Markov chain algorithm to analyse each: create some kind of fingerprint based on statistical data, and compare fingerprints gotten from different texts?
Let's say, we have a library with 100 texts. Some person wrote text number 1 and some other as well, and we need to guess which one by analyzing his/her writing style.
Is there any known algorithm doing it? Can be Markov chains applied here?</p>
",Multilingual Language Processing & Language Identification,author unique literary style used identify author text let imagine two english language text written person possible apply markov chain algorithm analyse create kind fingerprint based statistical data compare fingerprint gotten different text let say library text person wrote text number well need guess one analyzing writing style known algorithm markov chain applied
Detect if text in English with python,"<p>Well, i knew this question being asked multiple of times but i still couldn't fix it with the ""available"" solution. Hope to got any further ideas or concepts of how to detect my sentences is english in python. The available solution:</p>

<ul>
<li>Language Detector (in ruby not in python :/)</li>
<li>Google Translate API v2 (No longer free, have to pay 20 bucks a month while i'm doing this project for academic purposes. Courtesy limit: 0 characters/day ) </li>
<li>Language identification for python (source code not found, link at below. <a href=""http://cogscicoder.blogspot.com/2009/03/automatic-language-identification-using.html"" rel=""nofollow noreferrer"">automatic-language-identification</a>)</li>
<li><a href=""http://pythonhosted.org/pyenchant/"" rel=""nofollow noreferrer"">Enchant</a> (it's not for python 2.7? I'm new to python, any guide? I bet this would be the one i need)</li>
<li>Wordnet from NLTK (i got no idea why ""wordnet.synsets"" is missing and only ""wordnet.Synset"" is available. the sample code in solution is not working for me as well T_T, probably versioning issue again?)</li>
<li>Store english words into list and compare if the word exist (yea, it's kinda bad approach while the sentences are from twitter and.. you knew that :P)</li>
</ul>

<p><strong>WORKING SOLUTION</strong></p>

<p>Finally after a series of trying, the following is the working solution (alternative to the above list)</p>

<ul>
<li>Wiktionary API (Using Urllib2, and simplejson to parse it. then find if the key is -1 means the word doesn't exist. else it's english. of course, for use in twitter have to preprocess your word into no special character like @#,?!. For how to find the key would referencing here. <a href=""https://stackoverflow.com/questions/15261465/simplejson-and-random-key-value"">Simplejson and random key value</a>)</li>
<li>Answer from Dogukan Tufekci (Ticked)(Weakness: Let say if the sentence shorter than 20 characters long have to install PyEnchant or it will return UNKNOWN. While PyEnchant is not supporting Python 2.7, means couldn't install and not working to less than 20 character sentence)</li>
</ul>

<p><strong><em>References</em></strong></p>

<ul>
<li><a href=""https://stackoverflow.com/questions/4605062/detecting-whether-or-not-text-is-english-in-bulk"">Detecting whether or not text is English (in bulk)</a></li>
<li><a href=""https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python"">How to check if a word is an English word with Python?</a></li>
<li><a href=""https://stackoverflow.com/questions/2770547/is-there-an-api-available-to-retrieve-raw-wiktionary-data"">How to retrieve Wiktionary word content?</a></li>
</ul>
",Multilingual Language Processing & Language Identification,detect text english python well knew question asked multiple time still fix available solution hope got idea concept detect sentence english python available solution language detector ruby python google translate api v longer free pay buck month project academic purpose courtesy limit character day language identification python source code found link automatic language identification enchant python new python guide bet would one need wordnet nltk got idea wordnet synset missing wordnet synset available sample code solution working well probably versioning issue store english word list compare word exist yea kinda bad approach sentence twitter knew p working solution finally series trying following working solution alternative list wiktionary api using urllib simplejson parse find key mean word exist else english course use twitter preprocess word special character like find key would referencing
Does Gensim handle multi-word terms when processing Wikipedia corpus?,"<p>I was reading the <a href=""https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""nofollow"">Experiments on the English Wikipedia</a> tutorial and noticed that many of the topics generated by LSA and LDA contained multi-word terms that had clearly been concatenated e.g. <em>northamerica</em>, <em>hockeyarchives</em></p>

<p>Could someone indicate where this takes place. I have looked at <em>gensim.scripts.make_wiki</em>, <em>gensim.corpora.wikicorpus</em> and <em>genesis.utils</em>.</p>
",Multilingual Language Processing & Language Identification,doe gensim handle multi word term processing wikipedia corpus wa reading experiment english wikipedia tutorial noticed many topic generated lsa lda contained multi word term clearly concatenated e g northamerica hockeyarchives could someone indicate take place looked gensim script make wiki gensim corpus wikicorpus genesis utils
WordNet in Portuguese,"<p>I'm doing a java project where I have to find synonyms of some words. I know WordNet is widely spread, however it is not available in 'portuguese' (the language I'm currently using). Is there something similar to WordNet for the portuguese language?</p>

<p>Thank You.</p>
",Multilingual Language Processing & Language Identification,wordnet portuguese java project find synonym word know wordnet widely spread however available portuguese language currently using something similar wordnet portuguese language thank
"Detect contrast, comparison or example in an english sentence","<p>Is there any tool or technique available in NLP / machine learning so that I can easily detect presence of contrast, comparison or example in an English sentence? For example, lets consider the following sentences:</p>

<blockquote>
  <ol>
  <li><p>Students often <strong>cram</strong> at the last minute; <strong>however</strong>, spaced study proves more effective.</p></li>
  <li><p>Many <strong>atolls</strong> can be found in the Pacific Ocean; 
  <strong>similarly</strong>, other <strong>coral islands</strong> are found in the Caribbean Sea.</p></li>
  <li><p><strong>Unguents</strong>, <strong>such as</strong> first aid cream, Cortaid, Vick-Salve, and Deep heat, are found in most homes.</p></li>
  </ol>
</blockquote>

<p>Sentence 1 shows contrast to the word ""cram"" with ""spaced study"" with the use of the word ""however"".</p>

<p>Sentence 2 is showing comparison - ""atolls"" followed by ""similarly"", then ""coral islands"".</p>

<p>Sentence 3 is giving examples of ""Unguents"" followed by ""such as"" and the related example words.</p>

<p>I need to detect presence of such contrast, comparison and example in a sentence for the specified keyword. In our ""case"", ""cram"", ""atolls"" and unguents are the keywords.</p>
",Multilingual Language Processing & Language Identification,detect contrast comparison example english sentence tool technique available nlp machine learning easily detect presence contrast comparison example english sentence example let consider following sentence student often cram last minute however spaced study prof effective many atoll found pacific ocean similarly coral island found caribbean sea unguent first aid cream cortaid vick salve deep heat found home sentence show contrast word cram spaced study use word however sentence showing comparison atoll followed similarly coral island sentence giving example unguent followed related example word need detect presence contrast comparison example sentence specified keyword case cram atoll unguent keywords
Why nltk.align.bleu_score.bleu gives an error?,"<p>I found zero-value when I calculate BLEU score for Chinese sentences.</p>

<p>The candidate sentence is <code>c</code> and two references are <code>r1</code> and <code>r2</code></p>

<pre><code>c=[u'\u9274\u4e8e', u'\u7f8e\u56fd', u'\u96c6', u'\u7ecf\u6d4e', u'\u4e0e', u'\u8d38\u6613', u'\u6700\u5927', u'\u56fd\u4e8e', u'\u4e00\u8eab', u'\uff0c', u'\u4e0a\u8ff0', u'\u56e0\u7d20', u'\u76f4\u63a5', u'\u5f71\u54cd', u'\u7740', u'\u4e16\u754c', u'\u8d38\u6613', u'\u3002']

r1 = [u'\u8fd9\u4e9b', u'\u76f4\u63a5', u'\u5f71\u54cd', u'\u5168\u7403', u'\u8d38\u6613', u'\u548c', u'\u7f8e\u56fd', u'\u662f', u'\u4e16\u754c', u'\u4e0a', u'\u6700\u5927', u'\u7684', u'\u5355\u4e00', u'\u7684', u'\u7ecf\u6d4e', u'\u548c', u'\u8d38\u6613\u5546', u'\u3002']

r2=[u'\u8fd9\u4e9b', u'\u76f4\u63a5', u'\u5f71\u54cd', u'\u5168\u7403', u'\u8d38\u6613', u'\uff0c', u'\u56e0\u4e3a', u'\u7f8e\u56fd', u'\u662f', u'\u4e16\u754c', u'\u4e0a', u'\u6700\u5927', u'\u7684', u'\u5355\u4e00', u'\u7684', u'\u7ecf\u6d4e\u4f53', u'\u548c', u'\u8d38\u6613\u5546', u'\u3002']
</code></pre>

<p>The code isÔºö </p>

<pre><code>weights = [0.1, 0.8, 0.05, 0.05]
print nltk.align.bleu_score.bleu(c, [r1, r2], weights)
</code></pre>

<p>But I got a result <code>0</code>. When I step into the <code>bleu</code> process, I found that </p>

<pre><code>try:
    s = math.fsum(w * math.log(p_n) for w, p_n in zip(weights, p_ns))
except ValueError:
    # some p_ns is 0
    return 0
</code></pre>

<p>The above program goes to <code>except ValueError</code>. However, I don't know why this returns an error. If I try other sentences, I can get a non-zero value. </p>
",Multilingual Language Processing & Language Identification,nltk align bleu score bleu give error found zero value calculate bleu score chinese sentence candidate sentence two reference code got result step process found program go however know return error try sentence get non zero value
MySQL Structure for natural language processing,"<p>I want to create a question / answer MySQL DB in which questions and their matching answers are stored. The user should be able to enter questions in natural language based on which the most fitting stored question is searched and the matching answer is displayed. Since I don't want to make my life more difficult than needed I would like to get some input on how to best structure the DB for such an application. Thanks for your reply in advance.</p>
",Multilingual Language Processing & Language Identification,mysql structure natural language processing want create question answer mysql db question matching answer stored user able enter question natural language based fitting stored question searched matching answer displayed since want make life difficult needed would like get input best structure db application thanks reply advance
Autocomplete with Natural language,"<p>My project needs some natural language processing. I'm completely new to the field.</p>

<p>what I'm trying to achieve is that when the User enter the description of the product I look for in my database which description is nearest and suggest that the category, product group and sub-group (the tree of the product).</p>

<p>For this titles 250 extracts products for each subgroup.</p>

<p>What is the specific term in NLP for doing this? I tried googling for a while, but had no luck since I don't know the term. Any good tutorials to start with? Are there any good libraries in doing this specific task?</p>

<p>Thank you.</p>
",Multilingual Language Processing & Language Identification,autocomplete natural language project need natural language processing completely new field trying achieve user enter description product look database description nearest suggest category product group sub group tree product title extract product subgroup specific term nlp tried googling luck since know term good tutorial start good library specific task thank
How does sentences and text are represented in NLP?,"<p>Sentences are just sequences of words. These sequences can have a lot of ambiguities. One of the main goals of natural languages processing is to represent sentences as something that has more structure and less ambiguities. </p>

<p>So, my question is: What are the ways to represent sentences? I assume that there are many alternative approaches to that. What are the difference between them? Do they have their advantages and disadvantages?</p>
",Multilingual Language Processing & Language Identification,doe sentence text represented nlp sentence sequence word sequence lot ambiguity one main goal natural language processing represent sentence something ha structure le ambiguity question way represent sentence assume many alternative approach difference advantage disadvantage
Linking SFST with python?,"<p>I've used SFST language for my project on Natural Language Processing. 
How do I link or import SFST to my python code?</p>

<p>some link I referred:</p>

<p><a href=""http://home.gna.org/pysfst/tests/intro.html"" rel=""nofollow"">http://home.gna.org/pysfst/tests/intro.html</a>
<a href=""ftp://ftp.ims.uni-stuttgart.de/pub/corpora/SFST/SFST-Manual.pdf"" rel=""nofollow"">ftp://ftp.ims.uni-stuttgart.de/pub/corpora/SFST/SFST-Manual.pdf</a></p>

<p>commands from the first link isn't working and sfst is not being setup.</p>
",Multilingual Language Processing & Language Identification,linking sfst python used sfst language project natural language processing link import sfst python code link referred ftp ftp ims uni stuttgart de pub corpus sfst sfst manual pdf command first link working sfst setup
Is there a way to use french in Stanford CoreNLP sentiment analysis?,"<p>I am aware that only the English model is available for sentiment analysis but I found <code>edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz</code> in stanford-parser-3.5.2-models.jar. I'm actually looking at <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP</a>  Is it possible to use this model instead of englishPCFG.sez.gz with CoreNLP and if so, how ? </p>
",Multilingual Language Processing & Language Identification,way use french stanford corenlp sentiment analysis aware english model available sentiment analysis found stanford parser model jar actually looking possible use model instead englishpcfg sez gz corenlp
Mallet Natural Language Processing Mallet,"<p>I am trying to learn MALLET developed by UMASS Amhrest. I am pretty new to this and hence this may be a silly question.</p>

<p>I just ran a sample example given on their website using following command.</p>

<pre><code>bin/mallet import-dir --input sample-data/web/* --output web.mallet
</code></pre>

<p>Now, I have the web.mallet output file and I dont know how to open it.
I am using Linux and gedit cant read this file.</p>

<p>How can I open the output file and see its contents?</p>

<p>Thank You.</p>
",Multilingual Language Processing & Language Identification,mallet natural language processing mallet trying learn mallet developed uma amhrest pretty new hence may silly question ran sample example given website using following command web mallet output file dont know open using linux gedit cant read file open output file see content thank
Faster way to search strings in big file with python,"<p>I have two text files to be processed. Here is my situation:</p>

<ul>
<li>These two files are extremely big, one is 1.21 GB and another is 1.1 GB. Each of them contain about over 30 million lines of Chinese strings.</li>
<li>Each string in each file is unique.</li>
<li>I don't have to modify these files, once they're loaded, they will not change.</li>
</ul>

<p>The thing is, one of these file is corrupted. Let's call it N5. N5 should have each line of string looks like this: 'a5 b5 c5 d5 e5\tf5'</p>

<p>Instead, it is: 'a5b5 c5 d5 e5\tf5'</p>

<p>I am trying to recovery it from another file, let's call it N4, it looks like this: 'a4 b4 c4 d4\tf4'</p>

<p>What I am trying to do is using N4 to separate a5b5 in N5, which might have three results:</p>

<ol>
<li>'a4 b4 c4 d4' equals 'a5 b5 c5 d5'</li>
<li>'a4 b4 c4 d4' equals 'b5 c5 d5 e5'</li>
<li>There is no match for N4 in N5.</li>
</ol>

<p>In situation 1 and 2, I can get the answer. However, in 3, it takes about 140 seconds to complete the search in N4.</p>

<p>I am now using list to store N4 and N5, and down below is my code for comparing them.</p>

<pre><code># test data
N4 = ['a1 b1 c1 e1\t3', 'a2 b2 c2 e2\t2', 'c3 e3 f3 g3\t3']
N5 = ['a1b1 c1 e1 f1\t2', 'a2b c2 e2 f2\t1', 'b3c3 e3 f3 g3\t3']

# result stroage
list_result = []
list_result_no_none = []

counter_none = 0

list_len = len(N4)

for each_item in N5:
    counter_list_len = 0
    list_str_2 = str(each_item).split(' ')
    list_str_2_2 = str(list_str_2[3]).split('\t')
    str_list_str_2_0 = str(list_str_2[0])
    for each_item in N4:
        list_str_1 = str(each_item).split(' ')
        list_str_1_2 = str(list_str_1[3]).split('\t')

        # if n4 y == n5
        if (str(list_str_1[0])+str(list_str_1[1]) == str(list_str_2[0]) and \
           (str(list_str_1[2]) == str(list_str_2[1]) and \
           (str(list_str_1_2[0]) == str(list_str_2[2])) and \
           (str(list_str_1_2[1]) &gt;= str(list_str_2_2[1])))) :

            list_result.append(list_str_1[0] +' '+ list_str_1[1] +' '+ list_str_1[2] +' '+ list_str_1_2[0] +' '+ list_str_2[3])
            list_result_no_none.append(list_str_1[0] +' '+ list_str_1[1] +' '+ list_str_1[2] +' '+ list_str_1_2[0] +' '+ list_str_2[3])
        break

        # if x n4 == n5
        elif ((str(list_str_1[0]) in (str(list_str_2[0]))) and \
            (str(list_str_1[1]) == str(list_str_2[1])) and \
            (str(list_str_1[2]) == str(list_str_2[2])) and \
            (str(list_str_1_2[0]) == str(list_str_2_2[0]) and \
            (str(list_str_1_2[1]) &gt;= str(list_str_2_2[1])))):

            list_result.append(str_list_str_2_0[0:(str(list_str_2[0]).find(str(list_str_1[0])))]\
            +' '+ str_list_str_2_0[(str(list_str_2[0]).find(str(list_str_1[0]))):len(list_str_2[0])]\
            +' '+ list_str_1[1] +' '+ list_str_1[2] +' '+ list_str_2[3])
        list_result_no_none.append(str_list_str_2_0[0:(str(list_str_2[0]).find(str(list_str_1[0])))]\
            +' '+ str_list_str_2_0[(str(list_str_2[0]).find(str(list_str_1[0]))):len(list_str_2[0])]\
            +' '+ list_str_1[1] +' '+ list_str_1[2] +' '+ list_str_2[3])
        break

        # not found
        else:
            counter_list_len += 1
            if counter_list_len == list_len:
                list_result.append('none' +' '+ list_str_2[0] +' '+ list_str_2[1] +' '+ list_str_2[2] +' '+ list_str_2[3])
                counter_none += 1


print(list_result)
print(list_result_no_none)
print(""Percentage of not found: %.2f"" % ((100*(counter_none/len(N5)))) + '%')
</code></pre>

<p>It works on small scale, however, it takes ages of run on real file.</p>

<p>I am new to python, and have little experience in other programming languages. So if my question looks stupid for you, I am sorry. Also, I am not a native speaker, so apologies for my poor English.</p>
",Multilingual Language Processing & Language Identification,faster way search string big file python two text file processed situation two file extremely big one gb another gb contain million line chinese string string file unique modify file loaded change thing one file corrupted let call n n line string look like b c e tf instead b c e tf trying recovery another file let call n look like b c tf trying using n separate b n might three result b c equal b c b c equal b c e match n n situation get answer however take second complete search n using list store n n code comparing work small scale however take age run real file new python little experience programming language question look stupid sorry also native speaker apology poor english
How to retrieve translation from TextBlob translate,"<p>I'm using TextBlob to translate a sentence, and for the life of me I can't figure out how to retrieve the translated text as a string. For example</p>

<pre><code>from textblob import TextBlob
trans = TextBlob('Hello World')
foo = trans.translate(to='la')
</code></pre>

<p>How can I assign the translated text as a variable? Looking through the docs doesn't help. </p>
",Multilingual Language Processing & Language Identification,retrieve translation textblob translate using textblob translate sentence life figure retrieve translated text string example assign translated text variable looking doc help
Stanford NNDep parser: java.lang.ArrayIndexOutOfBoundsException,"<p>After training a model, i‚Äôm trying to parse the test treebank. Unfortunately, this error keeps popping up:</p>

<pre><code>Loading depparse model file: nndep.model.txt.gz ...
###################
#Transitions: 77
#Labels: 38
ROOTLABEL: root
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 25
        at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:663)
        at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:637)
        at edu.stanford.nlp.parser.nndep.DependencyParser.initialize(DependencyParser.java:1151)
        at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:589)
        at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:493)
        at edu.stanford.nlp.parser.nndep.DependencyParser.main(DependencyParser.java:1245)
</code></pre>

<p>If the pre-trained english model, which ships with the NLP package, is used, that error does not appear. Therefore, there is maybe something wrong with the trained model? There were no errors during training, however. 500 iterations were done (default 20000 takes over 15 hours on my 2,33 GHz Core 2 Duo CPU @ 4 Gb RAM ‚Äì is such an amount of time normal, by the way?)„ÄÄTrain, dev and test sets are <a href=""http://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548"" rel=""nofollow"">UD 1.2</a>; word embeddings used are <a href=""http://github.com/wolet/sprml13-word-embeddings"" rel=""nofollow"">these</a>. Seems that this error happens when non-english treebank is used for training (tried swedish and polish UD; <code>-tlp</code> option is not set, using <code>UniversalEnglish</code>).</p>
",Multilingual Language Processing & Language Identification,stanford nndep parser java lang arrayindexoutofboundsexception training model trying parse test treebank unfortunately error keep popping pre trained english model ship nlp package used error doe appear therefore maybe something wrong trained model error training however iteration done default take hour ghz core duo cpu gb ram amount time normal way train dev test set ud word embeddings used seems error happens non english treebank used training tried swedish polish ud option set using
Wordnet Synset Offset? How to compare words,"<p>I am using the Chinese Wordnet from Academic Sinica. It is a translation of Wordnet 1.6. Unfortunately it is not freely available, and has to be purchased, and the manual basically says refer to Wordnet's manual. What I am trying to figure out is how to compare the similarity between two words. I imagine it is done with the WordNetSynsetOffset but I could not find anything on the Wordnet website or documentation on how to use this to compare two words. As for the actual algorithms I suppose this is a good start <a href=""http://marimba.d.umn.edu/similarity/measures.html"" rel=""nofollow"">http://marimba.d.umn.edu/similarity/measures.html</a></p>

<pre><code>&lt;Record Conut=""65""&gt;  
    &lt;EnglishLemma&gt;exercise&lt;/EnglishLemma&gt;  
    &lt;POS&gt;Noun&lt;/POS&gt;  
    &lt;WordNetSynsetOffset Version=""1.6""&gt;00469856&lt;/WordNetSynsetOffset&gt;  
    &lt;EnglishFrequancyRank&gt;ÈÄöÁî®Ë©ûÂΩô&lt;/EnglishFrequancyRank&gt;  
    &lt;ChineseTransList&gt;  
        &lt;ChineseTrans&gt;  
            &lt;ChineseLemma&gt;‰æãÈ°å&lt;/ChineseLemma&gt;  
            &lt;ChineseFrequancyRank&gt;ÈÄöÁî®Ë©ûÂΩô&lt;/ChineseFrequancyRank&gt;  
        &lt;/ChineseTrans&gt;  
    &lt;/ChineseTransList&gt;  
&lt;/Record&gt;  
</code></pre>
",Multilingual Language Processing & Language Identification,wordnet synset offset compare word using chinese wordnet academic sinica translation wordnet unfortunately freely available ha purchased manual basically say refer wordnet manual trying figure compare similarity two word imagine done wordnetsynsetoffset could find anything wordnet website documentation use compare two word actual algorithm suppose good start
OpenNLP yielding undesired result,"<p>I am using OpenNLP to process queries like ""doctor working in Los Angeles"" and ""female living in Hollywood and working in Santa Monica"". For English understanding human these sentences are very obvious that the subjects are ""doctor"" and ""female"". However when I use opennlp it tagged the sentence as</p>

<pre><code>female_JJ living_NN in_IN hollywood_NN
</code></pre>

<p>[ female living ] [ in ] [ hollywood ]</p>

<p>Here's another sentence ""person living in santa monica and working in malibu and playing football"" was processed to be </p>

<pre><code>person_NN living_VBG in_IN santa_NN monica_NN and_CC working_VBG in_IN malibu_NN and_CC playing_NN football_NN
 [ person ] [ living ] [ in ] [ santa monica ] and [ working ] [ in ] [ malibu and playing football ]
</code></pre>

<p>Why does OpenNLP's POS tagger tagged them wrongly? These sentences have simplest grammatical structures. If the most advanced NLP technologies still fails to parse these sentences does it mean that NLP is far from being practical currently?</p>
",Multilingual Language Processing & Language Identification,opennlp yielding undesired result using opennlp process query like doctor working los angeles female living hollywood working santa monica english understanding human sentence obvious subject doctor female however use opennlp tagged sentence female living hollywood another sentence person living santa monica working malibu playing football wa processed doe opennlp po tagger tagged wrongly sentence simplest grammatical structure advanced nlp technology still fails parse sentence doe mean nlp far practical currently
Steps to generate parse tree from CYK Algorithm (Natural Language Processing),"<p>I am currently working on a project involving NLP. I have implemented a CKY identifier as given in Jurafsky and Martin (algorithm on page 450). The table so produced actually stores the nonterminals in the table (instead of the usual boolean values). However, the only issue I am getting is to retrieve the parse tree.</p>

<p>Here is an illustration of what my CKY identifier does: </p>

<p>This is my grammar</p>

<pre><code>          S -&gt; NP VP 
          S -&gt; VP 
          NP -&gt; MODAL PRON | DET NP | NOUN VF | NOUN | DET NOUN | DET FILENAME
          MODAL -&gt; 'MD'
          PRON -&gt; 'PPSS' | 'PPO'
          VP -&gt; VERB NP
          VP -&gt; VERB VP
          VP -&gt; ADVERB VP
          VP -&gt; VF
          VERB -&gt; 'VB' | 'VBN'
          NOUN -&gt; 'NN' | 'NP'
          VF -&gt; VERB FILENAME
          FILENAME -&gt; 'NN' | 'NP'
          ADVERB -&gt; 'RB'
          DET -&gt; 'AT'
</code></pre>

<p>And this is the algorithm:</p>

<pre><code>for j  from i to LENGTH(words) do
    table[j-1,j] = A where A -&gt; POS(word[j])
    for i from j-2 downto 0
        for k from i+1 to j-1
            table[i,j] = Union(table[i,j], A such that A-&gt;BC)
            where B is in table[i,k] and C is in table[k,j]
</code></pre>

<p>And this is what my parsing table looks like after being filled:</p>

<p><a href=""https://i.sstatic.net/3zIwQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3zIwQ.jpg"" alt=""CKY Table filled as per algorithm mentioned""></a></p>

<p>Now that I know that since S resides in [0,5], the string has been parsed, and that for k = 1 (as per the algorithm given in Martin and Jurafsky), we have S -> table[0][2] table[2][5]
    i.e. S -> NP VP</p>

<p>The only issue I am getting is that I have been able to retrieve the rules used, but then they are in a jumbled format, i.e. not on the basis of their appearance in parse tree. Can someone suggest an algorithm to retrieve the correct parse tree?</p>

<p>Thankyou.</p>
",Multilingual Language Processing & Language Identification,step generate parse tree cyk algorithm natural language processing currently working project involving nlp implemented cky identifier given jurafsky martin algorithm page table produced actually store nonterminals table instead usual boolean value however issue getting retrieve parse tree illustration cky identifier doe grammar algorithm parsing table look like filled know since resides string ha parsed k per algorithm given martin jurafsky table table e np vp issue getting able retrieve rule used jumbled format e basis appearance parse tree someone suggest algorithm retrieve correct parse tree thankyou
"How to improve check spelling performance, using Apache Spark","<p>I'm using Apache Spark for NLP (natural language processing) with LDA.
And before run LDA Model, I have a module calling ""CorrectEmoticon""
I have a dictionary for this purpose, and this file look like:</p>

<pre><code>:) \t smile
;) \t blink
...
</code></pre>

<p>After that, to apply this dictionary into my module I try:</p>

<pre><code>public static String correctSentence(String sentence) {
   String rplace = sentence.replaceAll(""[,.]"", """");
   String[] split = rplace.split("" "");
   StringBuilder sb = new StringBuilder();
   for (String inputStr : split) {
       sb.append(correctWord(inputStr));
       sb.append("" "");
   }
   return sb.toString();
}

private static String correctWord(String word) {
    word = getDefination(word);
    return word;
}
public static String getDefination(String word) {
    List&lt;String&gt; foundList = dict.lookup(word.trim().toLowerCase());
    if (foundList != null &amp;&amp; !foundList.isEmpty()){
        return foundList.get(0);
    } 
    return word;
}
</code></pre>

<p>And variable dict is:</p>

<pre><code>private static JavaPairRDD&lt;String, String&gt; dict;
</code></pre>

<p>""dict"" is contain value of emoticon and emoticon's meaning.</p>

<p>But, if I use this algorithm, it run very slow. 
So, can you help me to correct this algorithm to improve performance..
Thank you very much.</p>
",Multilingual Language Processing & Language Identification,improve check spelling performance using apache spark using apache spark nlp natural language processing lda run lda model module calling correctemoticon dictionary purpose file look like apply dictionary module try variable dict dict contain value emoticon emoticon meaning use algorithm run slow help correct algorithm improve performance thank much
What is a regular expression for parsing out persian individual sentences?,"<p>I am looking for a good .NET regular expression that I can use for parsing out individual sentences from a Persian body of text.<br>
So I thought of using a pattern to do splitting like if a <code>.</code> <code>ÿü</code> <code>!</code>  <code>ÿõ</code> follows a space and than do the split.<br>
in Persian language there is no Capital letter and the question mark is <code>ÿü</code> and its a right to left language. 
look at this example it has 4 sentences:</p>

<blockquote>
  <p>ÿ±ÿßŸá€å ⁄©Ÿá ÿ®ÿ±ÿß€åÿ™ ŸáŸÖŸàÿßÿ± ŸÜ€åÿ≥ÿ™ Ÿà ÿ™Ÿà ÿßÿ≤ ŸÖ€åÿßŸÜ ÿ™ŸÖÿßŸÖ ÿØÿ∫ÿØÿ∫Ÿá‚ÄåŸáÿß€å ÿ±€åÿ≤ Ÿà ÿØÿ±ÿ¥ÿ™ ÿß€åŸÜ ÿ¥Ÿáÿ± ÿßŸÜÿ™ÿÆÿßÿ® ŸÖ€å‚Äå⁄©ŸÜ€å ⁄©Ÿá €å⁄© ÿ±Ÿàÿ≤ÿ™ ÿ±ÿß ÿ®ÿß ŸÖÿ±ÿØ⁄ØÿßŸÜ ŸáŸÖÿ±ÿßŸá ÿ¥Ÿà€å. ÿ®ÿ±Ÿà€å ÿ®Ÿá ÿØŸÇ€åŸÇŸá 91ÿõ ÿ®Ÿá‚Äåÿ¨ÿß€å€å ⁄©Ÿá ÿ®ÿß ÿ™ŸÖÿßŸÖ ÿ¥ŸÜ€åÿØŸá‚ÄåŸáÿß€åÿ™ ŸÅÿ±ŸÇ ÿØÿßÿ±ÿØ. ÿ®ÿ±Ÿà€å ÿ®Ÿá ŸÖÿ±ÿ≤ ÿ®€åŸÜ ÿØŸÜ€åÿß Ÿà ÿ¢ÿÆÿ±ÿ™ÿå ÿ®Ÿá‚Äåÿ¨ÿß€å€å ⁄©Ÿá ÿπÿ±Ÿàÿ¨€åÿßŸÜ ÿ±ÿßÿ™ÿ∑Ÿá€åÿ± ŸÖ€å‚Äå⁄©ŸÜŸÜÿØÿå ÿ¢ÿ±ÿß€åÿ¥ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ ÿ®ÿ±ÿß€å ŸàÿµÿßŸÑ €åÿßÿ±ÿõ  </p>
</blockquote>

<p>i found this question useful <a href=""https://stackoverflow.com/questions/1936388/what-is-a-regular-expression-for-parsing-out-individual-sentences"">What is a regular expression for parsing out individual sentences?</a> and here is a regular expression for English  </p>

<pre><code>    Regex rx = new Regex(@""(\S.+?[.!?])(?=\s+|$)"");
foreach (Match match in rx.Matches(str)) {
    int i = match.Index;
    Console.WriteLine(match.Value);
}
</code></pre>
",Multilingual Language Processing & Language Identification,regular expression parsing persian individual sentence looking good net regular expression use parsing individual sentence persian body text thought using pattern splitting like follows space split persian language capital letter question mark right left language look example ha sentence found question useful href regular expression parsing individual sentence regular expression english
Set of rules for textual analysis - Natural language processing,"<p>Does there exist a guide with a set of rules for <em>textual analysis</em> / <em>natural language processing</em>?</p>

<p>Do you have some specific developed package (e.g. in Python) for textual sentiment analysis?</p>

<p>Here is the application I am faced with:</p>

<p>Let's say <strong>I have two dictionaries, A and B. A contains ""negative"" words, and B contains ""positive"" words.</strong> What I can do is <em>count</em> the negative and the positive number of words.</p>

<p>This created some issues, such as the following: let's suppose that ""<code>exceptionally</code>"" is a <code>positive</code> word, and ""<code>serious</code>"" is a <code>negative</code> word.</p>

<p>If I have the two words following each other, I have <code>""exceptionally serious""</code>. In such a case, the two words cancel each other, which means I have 1 negative and 1 positive word. This is not true, because in reality it is a <strong>double negative</strong>.</p>

<p>So, my question is, is there a <em>set of rules</em> I can apply so that I improve my code, or is there some <em>software</em> that already takes into account such mechanisms, and applies <em>textual sentiment analysis</em>? Is there some implementation which I can feed the dictionaries and provide me with textual sentiment after it applies a <em>set of rules such as double negatives</em>?</p>
",Multilingual Language Processing & Language Identification,set rule textual analysis natural language processing doe exist guide set rule textual analysis natural language processing specific developed package e g python textual sentiment analysis application faced let say two dictionary b contains negative word b contains positive word count negative positive number word created issue following let suppose word word two word following case two word cancel mean negative positive word true reality double negative question set rule apply improve code software already take account mechanism applies textual sentiment analysis implementation feed dictionary provide textual sentiment applies set rule double negative
What the format requirement or language requirement of the corpus using by LDA-Ruby?,"<p>I am using LDA-ruby to perform topic modelling of Chinese tweets content. However, when use txt file as my own TextCorpus the result is quite strange. First time, the result topics were all English letters which occurred in the content; then I stripped out all letters of the input files for the corpus, and the result still make no sense. So does the lda-ruby only work with English corpus or any special requirements needed for building my own corpus? 
(btw. new to ruby/nlp)</p>

<p>Thanks in advance if any solution of how to build corpus (format/language).</p>
",Multilingual Language Processing & Language Identification,format requirement language requirement corpus using lda ruby using lda ruby perform topic modelling chinese tweet content however use txt file textcorpus result quite strange first time result topic english letter occurred content stripped letter input file corpus result still make sense doe lda ruby work english corpus special requirement needed building corpus btw new ruby nlp thanks advance solution build corpus format language
"Is there a downloadable corpus (dictionary/ lexicon) for informal, playful words such as &#39;gonna&#39;, &#39;LOL&#39;, &#39;wanna&#39; in English?","<p>Please suggest me a downloadable English corpus that contains informal, playful words such as 'gonna', 'LOL' and 'wanna'</p>
",Multilingual Language Processing & Language Identification,downloadable corpus dictionary lexicon informal playful word gon na lol wan na english please suggest downloadable english corpus contains informal playful word gon na lol wan na
How do I parse arabic sentence using Stanford core nlp,"<p>I have pretty enough experience with English using Stanford core nlp package. </p>

<p>I want to for non-english language like Arabic, how can I parse sentence using this package? Any example with command?</p>
",Multilingual Language Processing & Language Identification,parse arabic sentence using stanford core nlp pretty enough experience english using stanford core nlp package want non english language like arabic parse sentence using package example command
How to recognize alternative expression using natural language processing,"<p>I'm trying to use opennlp to understand alternative expression of user query. For example, ""Kate Winslet movie"" and ""movie by Kate Winslet"" are of the same meaning. I want opennlp to generate the same machine query for the two user inputs. What is the best approach to the different expressions of same meaning?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,recognize alternative expression using natural language processing trying use opennlp understand alternative expression user query example kate winslet movie movie kate winslet meaning want opennlp generate machine query two user input best approach different expression meaning thanks
Morphological realisation for the Spanish language,"<p>Does anyone know a Morphological realisation Tool (preferably a Java one).
I am working on a project and I need to realise the correct verb ""to be"" providing if it is for male/female - singular/plural - first person/third person and regarding such inputs generate the correct verb ""to be"".
SimpleNLG is the ideal software that contains a Morphological realisation but it is only for English and French.For example : if the features are male first person singular the result will be ""I"", if the features are plural third person males the result will be ""they"".</p>
",Multilingual Language Processing & Language Identification,morphological realisation spanish language doe anyone know morphological realisation tool preferably java one working project need realise correct verb providing male female singular plural first person third person regarding input generate correct verb simplenlg ideal software contains morphological realisation english french example feature male first person singular result feature plural third person male result
Differences between lexical features and orthographic features in NLP?,"<p>Features are used for model training and testing. What are the differences between lexical features and orthographic features in Natural Language Processing? Examples preferred. </p>
",Multilingual Language Processing & Language Identification,difference lexical feature orthographic feature nlp feature used model training testing difference lexical feature orthographic feature natural language processing example preferred
Applied NLP: how to score a document against a lexicon of multi-word terms?,"<p>This is probably a fairly basic NLP question but I have the following task at hand: I have a collection of text documents that I need to score against an (English) lexicon of terms that could be 1-, 2-, 3- etc <code>N</code>-word long. <code>N</code> is bounded by some ""reasonable"" number but the distribution of various terms in the dictionary for various values of <code>n = 1, ..., N</code> might be fairly uniform. This lexicon can, for example, contain a list of devices of certain type and I want to see if a given document is likely about any of these devices. So I would want to score a document high(er) if it has one or more occurrences of any of the lexicon entries. </p>

<p>What is a standard NLP technique to do the scoring while accounting for various forms of the words that may appear in the lexicon? What sort of preprocessing would be required for both the input documents and the lexicon to be able to perform the scoring? What sort of open-source tools exist for both the preprocessing and the scoring?</p>
",Multilingual Language Processing & Language Identification,applied nlp score document lexicon multi word term probably fairly basic nlp question following task hand collection text document need score english lexicon term could etc word long bounded reasonable number distribution various term dictionary various value might fairly uniform lexicon example contain list device certain type want see given document likely device would want score document high er ha one occurrence lexicon entry standard nlp technique scoring accounting various form word may appear lexicon sort preprocessing would required input document lexicon able perform scoring sort open source tool exist preprocessing scoring
How to edit .csv in python to proceed NLP,"<p>Hello i am not very familiar with programming and found Stackoverflow while researching my task. I want to do natural language processing on a .csv file that looks like this and has about 15.000 rows</p>

<pre><code>    ID | Title        | Body
    ----------------------------------------
    1  | Who is Jack? | Jack is a teacher... 
    2  | Who is Sam?  | Sam is a dog.... 
    3  | Who is Sarah?| Sarah is a doctor...
    4  | Who is Amy?  | Amy is a wrestler... 
</code></pre>

<p>I want to read the .csv file and do some basic NLP operations and write the results back in a new or in the same file. After some research python and nltk seams to be the technologies i need. (i hope thats right). After tokenizing i want my .csv file to look like this</p>

<pre><code>    ID | Title                 | Body
    -----------------------------------------------------------
    1  | ""Who"" ""is"" ""Jack"" ""?"" | ""Jack"" ""is"" ""a"" ""teacher""... 
    2  | ""Who"" ""is"" ""Sam"" ""?""  | ""Sam"" ""is"" ""a"" ""dog"".... 
    3  | ""Who"" ""is"" ""Sarah"" ""?""| ""Sarah"" ""is"" ""a"" ""doctor""...
    4  | ""Who"" ""is"" ""Amy"" ""?""  | ""Amy"" ""is"" ""a"" ""wrestler""... 
</code></pre>

<p>What i have achieved after a day of research and putting pieces together looks like this</p>

<pre><code>    ID | Title                 | Body
    ----------------------------------------------------------
    1  | ""Who"" ""is"" ""Jack"" ""?"" | ""Jack"" ""is"" ""a"" ""teacher""... 
    2  | ""Who"" ""is"" ""Sam"" ""?""  | ""Jack"" ""is"" ""a"" ""teacher""...
    3  | ""Who"" ""is"" ""Sarah"" ""?""| ""Jack"" ""is"" ""a"" ""teacher""...
    4  | ""Who"" ""is"" ""Amy"" ""?""  | ""Jack"" ""is"" ""a"" ""teacher""... 
</code></pre>

<p>My first idea was to read a specific cell in the .csv ,do an operation and write it back to the same cell. And than somehow do that automatically on all rows. Obviously i managed to read a cell and tokenize it. But i could not manage to write it back in that specific cell. And i am far away from ""do that automatically to all rows"". I would appreciate some help if possible.</p>

<p>My code:</p>

<pre><code>    import csv
    from nltk.tokenize import word_tokenize 

    ############Read CSV File######################
    ########## ID , Title, Body#################### 

    line_number = 1 #line to read (need some kind of loop here)
    column_number = 2 # column to read (need some kind of loop here)
    with open('test10in.csv', 'rb') as f:
        reader = csv.reader(f)
        reader = list(reader)
        text = reader[line_number][column_number] 


        stringtext = ''.join(text) #tokenizing just work on strings 
        tokenizedtext = (word_tokenize(stringtext))
        print(tokenizedtext)

    #############Write back in same cell in new CSV File######

    with open('test11out.csv', 'wb') as g:
        writer = csv.writer(g)
        for row in reader:
            row[2] = tokenizedtext
            writer.writerow(row)
</code></pre>

<p>I hope i asked the question correctly and someone can help me out. </p>
",Multilingual Language Processing & Language Identification,edit csv python proceed nlp hello familiar programming found stackoverflow researching task want natural language processing csv file look like ha row want read csv file basic nlp operation write result back new file research python nltk seam technology need hope thats right tokenizing want csv file look like achieved day research putting piece together look like first idea wa read specific cell csv operation write back cell somehow automatically row obviously managed read cell tokenize could manage write back specific cell far away automatically row would appreciate help possible code hope asked question correctly someone help
NLTK data out of date - Python 3.4,"<p>I'm trying to install NLTK for Python 3.4. The actual NLTK module appears to have installed fine. I then ran</p>

<pre><code>import nltk

nltk.download()
</code></pre>

<p>and chose to download everything. However, after it was done, the window simply says 'out of date'. I tried refreshing and downloading, yet it stays 'out of date' as shown here:<a href=""https://i.sstatic.net/Bo1fo.png"" rel=""noreferrer"">NLTK Window 1</a></p>

<p>I looked online and tried various fixes, but I haven't found any that helped my case yet.</p>

<p>I also tried to manually find the missing parts, which turned out to be 'Open Multilingual Wordnet' and 'Wordnet'. Here's how I found which parts were missing: <a href=""https://i.sstatic.net/IO7oC.png"" rel=""noreferrer"">Open Multilingual Wordnet</a>.</p>

<p>What should I do? Should I uninstall and reinstall NLTK? I haven't really found a way to delete the packages (except for manually deleting it).</p>

<p>EDIT: Regarding Solution 2 and Solution 3:
For more clarification on the Solution 2 issue:</p>

<p>If something has sucessfully downloaded, this is the output:</p>

<pre><code>&gt;&gt;&gt; nltk.download('subjectivity')
[nltk_data] Downloading package subjectivity to
[nltk_data]     C:\Users\Shane\AppData\Roaming\nltk_data...
[nltk_data]   Package subjectivity is already up-to-date!
True
</code></pre>

<p>However, for 'wordnet' and 'omw', this is what happens when I redownload:</p>

<pre><code>&gt;&gt;&gt; nltk.download('omw')
[nltk_data] Downloading package omw to
[nltk_data]     C:\Users\Shane\AppData\Roaming\nltk_data...
[nltk_data]   Unzipping corpora\omw.zip.
True
</code></pre>
",Multilingual Language Processing & Language Identification,nltk data date python trying install nltk python actual nltk module appears installed fine ran chose download everything however wa done window simply say date tried refreshing downloading yet stay date shown nltk window looked online tried various fix found helped case yet also tried manually find missing part turned open multilingual wordnet wordnet found part missing open multilingual wordnet uninstall reinstall nltk really found way delete package except manually deleting edit regarding solution solution clarification solution issue something ha sucessfully downloaded output however wordnet omw happens redownload
How to use lexicon dictionary in c#,"<p>i am working on sentiment analysis in c#, I have done preprocessing, and the next part is lexicon based analysis, for which I have found English Lexicon of about 6800 word <a href=""https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"" rel=""nofollow"">lexicon by Professor Bing Liu </a> which contains two text files, one for positive and other is for negative. </p>

<p>I was thinking that I have to just find the each word(sentiment word) from these files that either the particular word is positive or negative. but the problem is, these files contains words without any space , without any format (means individual word can not be recognized from file ). </p>

<p>So how can I find the word in file? Or is there any other way through which I can work easily with this?</p>
",Multilingual Language Processing & Language Identification,use lexicon dictionary c working sentiment analysis c done preprocessing next part lexicon based analysis found english lexicon word lexicon professor bing liu contains two text file one positive negative wa thinking find word sentiment word file either particular word positive negative problem file contains word without space without format mean individual word recognized file find word file way work easily
How to deal with different sizes of sentences when giving them as input to a Neural Network?,"<p>I am giving a sentence as input to a tree structured Neural Network, where the leaf nodes will be the word vectors of the words in the sentence. </p>

<p>That tree will be a <a href=""https://en.wikipedia.org/wiki/Branching_(linguistics)"" rel=""nofollow noreferrer"">binarized constituency</a>(see the binary vs n-ary branching section) parse tree. </p>

<p>I am trying to develop a semantic representation of the sentence. </p>

<p><strong>The problem is</strong>, that since each sentence will have a different parse tree and different lengths, each sentence will have a different neural network. Due to this ever changing structure of neural network, I can't train it.</p>

<p>But this paper develops a tree structured Neural network using constituency and dependency based parse trees-</p>

<p>1) <a href=""http://arxiv.org/pdf/1503.00075v3.pdf"" rel=""nofollow noreferrer"">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</a> by Kai Sheng Tai, Richard Socher, and Christopher Manning</p>

<p>And this paper uses CNNs to extract a semantic representation.</p>

<p>2) <a href=""http://nal.co/papers/KalchbrennerBlunsom_EMNLP13"" rel=""nofollow noreferrer"">Recurrent Continuous Translation Models</a> by Nal Kalchbrenner,and  Phil Blunsom. This picture gives a rough idea.<img src=""https://i.sstatic.net/qvcI4.png"" alt=""CSM""></p>

<p><strong>Possible Solution</strong>-<br>
I can map the sentence vector to a fixed number of neurons and then use those neurons to create a tree structure. For example if there is the sentence length is 10 and the max sentence length is 20, I can create a fixed dimensionality layer of 20 neurons and then (in this particular case) the first word to the first 2 neurons, 2nd word to the 3rd and 4th neuron and so on.
Dynamic mapping can be done based on the sentence length.</p>

<p>The weight matrix of the sentence layer to the fixed dimensionality layer will be fixed(the weights should be kept 1). No biases.</p>

<p>But I think there will be some problems in this representation- for example- If this sentence ""I had a lovely icecream and a pastry for dessert."" is mapped to the fixed dimensionality layer it will become ""I I had had a a lovely lovely icecream icecream and and a a pastry pastry for for dessert dessert.."". So that means that shorter sentences will have a more profound effect on the neural network compared to the longer sentences. This biasness towards shorter sentences should also create duplicate words in the output(of a sequence generator). Could someone correct me if I am wrong?</p>

<p>I would welcome more solutions, especially ones that do not remove the relationships that words have between them in a sentence.</p>

<p>I will be implementing this using theano and python. I am considering a layer based approach and using theano.scan to iterate over the layers to finally form the sentence representation.</p>
",Multilingual Language Processing & Language Identification,deal different size sentence giving input neural network giving sentence input tree structured neural network leaf node word vector word sentence tree binarized constituency see binary v n ary branching section parse tree trying develop semantic representation sentence problem since sentence different parse tree different length sentence different neural network due ever changing structure neural network train paper develops tree structured neural network using constituency dependency based parse tree improved semantic representation tree structured long short term memory network kai sheng tai richard socher christopher manning paper us cnns extract semantic representation recurrent continuous translation model nal kalchbrenner phil blunsom picture give rough idea possible solution map sentence vector fixed number neuron use neuron create tree structure example sentence length max sentence length create fixed dimensionality layer neuron particular case first word first neuron nd word rd th neuron dynamic mapping done based sentence length weight matrix sentence layer fixed dimensionality layer fixed weight kept bias think problem representation example sentence lovely icecream pastry dessert mapped fixed dimensionality layer become lovely lovely icecream icecream pastry pastry dessert dessert mean shorter sentence profound effect neural network compared longer sentence biasness towards shorter sentence also create duplicate word output sequence generator could someone correct wrong would welcome solution especially one remove relationship word sentence implementing using theano python considering layer based approach using theano scan iterate layer finally form sentence representation
Obtain Phrasal Verb by using English Grammatical Relation - Dependency Parser,"<p>I have following code which correctly identifies phrasal verb but yielding seems tricky. I want to take the phrasal verb part(compound:prt(come, down) only. I have read Stanford's dependency manual but found no help from there. I have spent two much time figuring out where I am doing mistake.</p>

<pre><code>import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.parser.lexparser.*;
import edu.stanford.nlp.process.*;
import java.util.Collection;
import java.util.*;
import java.io.*;

class dependencyParser {
   public static void main(String []args) throws Exception {
      String text = ""Come down from the tree."";
      TreebankLanguagePack tlp = new PennTreebankLanguagePack();
      GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
      LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
      //lp.setOptionFlags(new String[]{""-maxLength"", ""500"", ""-retainTmpSubcategories""});
      TokenizerFactory tokenizerFactory = PTBTokenizer.factory( new CoreLabelTokenFactory(), """");
      List wordList = tokenizerFactory.getTokenizer(new StringReader(text)).tokenize();
      Tree tree = lp.apply(wordList);



System.out.println(tree);

  GrammaticalStructure gs = gsf.newGrammaticalStructure(tree);
  Collection&lt;TypedDependency&gt; typedDependencies = gs.typedDependenciesCollapsed();
 // System.out.println(tdl);
 // Main.writeImage(tree,tdl, ""image.png"",3);
  System.out.println();

  for(TypedDependency td : gs.typedDependencies()) {
  if(td.reln().equals(EnglishGrammaticalRelations.PHRASAL_VERB_PARTICLE)) {
              System.out.println( td);
  }
  }

 }
}
</code></pre>
",Multilingual Language Processing & Language Identification,obtain phrasal verb using english grammatical relation dependency parser following code correctly identifies phrasal verb yielding seems tricky want take phrasal verb part compound prt come read stanford dependency manual found help spent two much time figuring mistake
How to use Stanford LexParser for Chinese text?,"<p>I can't seem to get the correct input encoding for <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">Stanford NLP's LexParser</a>. </p>

<p><strong>How do I use the Stanford LexParser for Chinese text?</strong></p>

<p>I've done the following to download the tool:</p>

<pre><code>$ wget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip
$ unzip stanford-parser-full-2015-04-20.zip 
$ cd stanford-parser-full-2015-04-20/
</code></pre>

<p>And my input text is in <code>UTF-8</code>:</p>

<pre><code>$ echo ""Â∫îÊúâÂ∞ΩÊúâ ÁöÑ ‰∏∞ÂØå ÈÄâÊã© ÂÆö Â∞Ü ‰∏∫ ÊÇ® ÁöÑ ÊóÖÁ®ã Â¢ûÊ∑ª Êó†Êï∞ ÁöÑ ËµèÂøÉ ‰πê‰∫ã „ÄÇ"" &gt; input.txt

$ echo ""Â∫îÊúâÂ∞ΩÊúâ#VV ÁöÑ#DEC ‰∏∞ÂØå#JJ ÈÄâÊã©#NN ÂÆö#VV Â∞Ü#AD ‰∏∫#P ÊÇ®#PN ÁöÑ#DEG ÊóÖÁ®ã#NN Â¢ûÊ∑ª#VV Êó†Êï∞#CD ÁöÑ#DEG ËµèÂøÉ#NN ‰πê‰∫ã#NN  „ÄÇ#PUNCT"" &gt; pos-input.txt
</code></pre>

<p>According to the <code>README.txt</code>, the parser was trained on:</p>

<blockquote>
  <p>Chinese 
  There are Chinese grammars trained just on mainland material
  from Xinhua and more mixed material from the LDC Chinese Treebank. The
  default input encoding is GB18030.</p>
</blockquote>

<p>So I've tried with the <code>UTF-8</code> file first:</p>

<pre><code>$ bash lexparser-lang.sh Chinese 80 edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz parsed input.txt
Loading parser from serialized file edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz ...  done [1.0 sec].
Parsing file: input.txt
Parsing [sent. 1 len. 16]: Â∫îÊúâÂ∞ΩÊúâ ÁöÑ1ÔøΩ7 ‰∏∞ÂØå ÈÄâÊã© ÂÆÑ1ÔøΩ7 Â∞Ñ1ÔøΩ7 ‰∏Ñ1ÔøΩ7 ÊÇÑ1ÔøΩ7 ÁöÑ1ÔøΩ7 ÊóÖÁ®ã Â¢ûÊ∑ª Êó†Êï∞ ÁöÑ1ÔøΩ7 ËµèÂøÉ ‰πê‰∫ã „ÄÑ1ÔøΩ7
Parsed file: input.txt [1 sentences].
Parsed 16 words in 1 sentences (21.00 wds/sec; 1.31 sents/sec).
</code></pre>

<p>It didn't seem to work. The parser produced this file, <code>input.txt.parsed.80.stp</code></p>

<p>[out]:</p>

<pre><code>$ cat input.txt.parsed.80.stp 
(FRAG (NR Â∫îÊúâÂ∞ΩÊúâ) (NR ÁöÑ1ÔøΩ7) (NT ‰∏∞ÂØå) (NT ÈÄâÊã©) (NN ÂÆÑ1ÔøΩ7) (NN Â∞Ñ1ÔøΩ7) (NN ‰∏Ñ1ÔøΩ7) (NN ÊÇÑ1ÔøΩ7) (NR ÁöÑ1ÔøΩ7) (NT ÊóÖÁ®ã) (NT Â¢ûÊ∑ª) (NN Êó†Êï∞) (NN ÁöÑ1ÔøΩ7) (NR ËµèÂøÉ) (NR ‰πê‰∫ã) (VV „ÄÑ1ÔøΩ7))
</code></pre>

<p>Then i'ved tried to encode the sentence into GB18030:</p>

<pre><code>$ bash lexparser-lang.sh Chinese 80 edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz parsed input-gb18030.txt
Loading parser from serialized file edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz ...  done [1.0 sec].
Parsing file: input-gb18030.txt
Parsing [sent. 1 len. 16]: ”¶ÔøΩ–æÔøΩÔøΩÔøΩ ÔøΩÔøΩ ÔøΩ·∏ª —°ÔøΩÔøΩ ÔøΩÔøΩ ÔøΩÔøΩ Œ™ ÔøΩÔøΩ ÔøΩÔøΩ ÔøΩ√≥ÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩ
Parsed file: input-gb18030.txt [1 sentences].
Parsed 16 words in 1 sentences (19.90 wds/sec; 1.24 sents/sec).
alvas@ubi:~/stanford-parser-full-2015-04-20$ cat input-gb18030.txt.parsed.80.stp 
(IP
  (NP
    (CP
      (IP
        (VP (VV ”¶ÔøΩ–æÔøΩÔøΩÔøΩ)))
      (DEC ÔøΩÔøΩ))
    (ADJP (JJ ÔøΩ·∏ª))
    (NP (NN —°ÔøΩÔøΩ)))
  (VP (VV ÔøΩÔøΩ)
    (VP
      (ADVP (AD ÔøΩÔøΩ))
      (PP (P Œ™)
        (NP
          (DNP
            (NP (PN ÔøΩÔøΩ))
            (DEG ÔøΩÔøΩ))
          (NP (NN ÔøΩ√≥ÔøΩ))))
      (VP (VV ÔøΩÔøΩÔøΩÔøΩ)
        (NP
          (DNP
            (ADJP (JJ ÔøΩÔøΩÔøΩÔøΩ))
            (DEG ÔøΩÔøΩ))
          (NP (NN ÔøΩÔøΩÔøΩÔøΩ) (NN ÔøΩÔøΩÔøΩÔøΩ))))))
  (PU ÔøΩÔøΩ))
</code></pre>

<p>It seems like it's working but <strong>how do I convert the file back into UTF8?</strong></p>

<p>I've tried this but it didn't work:</p>

<pre><code>$ cat input-gb18030.txt.parsed.80.stp | python -c ""print raw_input().decode('GB18030').encode('utf8')""
(IP
</code></pre>

<p>Here's some concluding question:</p>

<ul>
<li><strong>How do I convert between GB18030 to UTF8 and UTF8 to GB18030?</strong></li>
<li><strong>How do I use the Stanford LexParser for Chinese UTF8 text?</strong></li>
</ul>
",Multilingual Language Processing & Language Identification,use stanford lexparser chinese text seem get correct input encoding stanford nlp lexparser use stanford lexparser chinese text done following download tool input text according parser wa trained chinese chinese grammar trained mainland material xinhua mixed material ldc chinese treebank default input encoding gb tried file first seem work parser produced file ved tried encode sentence gb seems like working convert file back utf tried work concluding question convert gb utf utf gb use stanford lexparser chinese utf text
Is it possible to achieve something similar to word2vec using a graphdb?,"<p>Otherwise said replace eigen vectors with pattern matching and graph traversal and emulate dimension reduction?</p>

<p>I mean that given a semantic graph of english words compute something similar to: </p>

<p><code>king - man = queen</code></p>

<p>Which means that I can subtract from a graph a subgraph and score the resulting subgraph given a metric.</p>

<p>I don't expect that this will be a single neo4j or gremlin query. I'm interested in the underlying mechanic involved in reasoning at the same time globaly and localy over a graph database.</p>
",Multilingual Language Processing & Language Identification,possible achieve something similar word vec using graphdb otherwise said replace eigen vector pattern matching graph traversal emulate dimension reduction mean given semantic graph english word compute something similar mean subtract graph subgraph score resulting subgraph given metric expect single neo j gremlin query interested underlying mechanic involved reasoning time globaly localy graph database
How to reverse use a nlp parser to generate sentences,"<p>I am trying to develop a Sinhala (My native language) to English translator. Still I am thinking for an approach.</p>

<p><strong>If I however parse a sentence of my language, then can I use that for generating english sentence with the help of stanford parser or any other parser. Or is there any other method you can recommend.</strong></p>

<p>And I am thinking of a bottom up parser for my language, but still have no idea how to implement. Any suggestions for steps I can follow.</p>

<p>Thanks Mathee</p>
",Multilingual Language Processing & Language Identification,reverse use nlp parser generate sentence trying develop sinhala native language english translator still thinking approach however parse sentence language use generating english sentence help stanford parser parser method recommend thinking bottom parser language still idea implement suggestion step follow thanks mathee
How does pyaramorph (Arabic morphological analyzer) work?,"<p>I have downloaded pyaramorph (Arabic morphological analyzer) on python , but I do not know how to use it. Could someone explain to me how can I get it to work?</p>

<p>If I have articles and I want to use this morphological tools to know how many (root) words I found it?</p>
",Multilingual Language Processing & Language Identification,doe pyaramorph arabic morphological analyzer work downloaded pyaramorph arabic morphological analyzer python know use could someone explain get work article want use morphological tool know many root word found
Looking for Java spell checker library,"<p>I am looking for an open source Java spell checking library which has dictionaries for at least the following languages: French, German, Spanish, and Czech. Any suggestion?</p>
",Multilingual Language Processing & Language Identification,looking java spell checker library looking open source java spell checking library ha dictionary least following language french german spanish czech suggestion
Create a user assistant using NLP,"<p>I am following a course titled Natural Language Processing on Coursera, and while the course is informative, I wonder if the contents given cater to what am I looking for.<br /><br />Basically I want to implement a textual version of Cortana, or Siri for now as a project, i.e. where the user can enter commands for the computer in natural language and they will be processed and translated into appropriate OS commands. My question is<br />
<li>What is generally sequence of steps for the above applications, after processing the speech? Do they tag the text and then parse it, or do they have any other approach?
<br /><br />
<li>Under which application of NLP does it fall? Can someone cite me some good resources for same? My only doubt is that what I follow now, shall that serve any important part towards my goal or not?</p>
",Multilingual Language Processing & Language Identification,create user assistant using nlp following course titled natural language processing coursera course informative wonder content given cater looking basically want implement textual version cortana siri project e user enter command computer natural language processed translated appropriate command question generally sequence step application processing speech tag text parse approach application nlp doe fall someone cite good resource doubt follow shall serve important part towards goal
Assign a short text to one of two categories according to previous assignments (votes),"<p>There is a stream of short texts. Each one has the size of a tweet, or let us just assume they are all tweets.</p>

<p>The user can vote on any tweet. So, each tweet has one of the following three states:</p>

<p>relevant (positive vote)</p>

<p>default (neutral i.e. no vote)</p>

<p>irrelevant (negative vote)</p>

<p>Whenever a new set of tweets come, they will be displayed in a specific order. This order is determined by the votes of the user on all previous tweets. The aim is to assign a score to each new tweet. This score is calculated based on the word similarity or match between the text of this tweet and all the previous tweets voted by the user. In other words, the tweet with the highest score is going to be the one which contains the maximum number of words voted previously positive and the minimum of words voted previously as negative. Also, the new tweets having a high score will trigger a notification to the user as they are considered very relevant.</p>

<p>One last thing, a minimum of semantic consideration (natural language processing) would be great. </p>

<p>I have read about <strong>Term Frequency‚ÄìInverse Document Frequency</strong> and come up with this very simple and basic solution:</p>

<p>Reminder: a high weight in tf‚Äìidf is reached by a high word frequency and a low total frequency of the word in the whole collection.</p>

<p>If the user votes positive on a Tweet, all the words of this tweet will receive a positive point (same thing for the negative case). This means that we will have a large set of words where each word has the total number of positive points and negative points.    </p>

<p>If (Tweet score > 0) then this tweet will trigger a notification.</p>

<p>Tweet score = sum of all individual words‚Äô scores of this tweet</p>

<p>word score = word frequency * inverse total frequency</p>

<p>word frequency in all previous votes = ( total <strong>positive</strong> votes for this word - total <strong>negative</strong> votes for this word) / <strong>total</strong> votes for this word</p>

<p>Inverse total frequency = log ( total votes of <strong>all</strong> words / total votes for <strong>this</strong> word)</p>

<p>Is this method enough? I am open to any better methods and any ready API or algorithm.</p>
",Multilingual Language Processing & Language Identification,assign short text one two category according previous assignment vote stream short text one ha size tweet let u assume tweet user vote tweet tweet ha one following three state relevant positive vote default neutral e vote irrelevant negative vote whenever new set tweet come displayed specific order order determined vote user previous tweet aim assign score new tweet score calculated based word similarity match text tweet previous tweet voted user word tweet highest score going one contains maximum number word voted previously positive minimum word voted previously negative also new tweet high score trigger notification user considered relevant one last thing minimum semantic consideration natural language processing would great read term frequency inverse document frequency come simple basic solution reminder high weight tf idf reached high word frequency low total frequency word whole collection user vote positive tweet word tweet receive positive point thing negative case mean large set word word ha total number positive point negative point tweet score tweet trigger notification tweet score sum individual word score tweet word score word frequency inverse total frequency word frequency previous vote total positive vote word total negative vote word total vote word inverse total frequency log total vote word total vote word method enough open better method ready api algorithm
Confusing appearance of unicode literals in NLTK stock texts,"<p>I've been starting to work through the 1st edition of <em>Natural Language Processing With Python</em> (2009), and have run into some odd behavior where my output doesn't match what the book is saying I should see.</p>

<p>For instance, consider the following example from the book (page 8):</p>

<p><a href=""https://i.sstatic.net/WMWi3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WMWi3.png"" alt=""enter image description here""></a> </p>

<p>Whenever I attempt to do the same thing, however, the output has every word as a unicode literal:</p>

<pre><code>&gt;&gt;&gt; sorted(set(text3))
[u'!', u""'"", u'(', u')', u',', u',)', u'.', u'.)', u':', u';', u';)', u'?',
 u'?)', u'A', u'Abel', u'Abelmizraim', u'Abidah', u'Abide', u'Abimael', 
u'Abimelech', u'Abr', u'Abrah', u'Abraham', u'Abram', u'Accad', u'Achbor', u'Adah'...]
</code></pre>

<p>What's going on here?</p>

<p>Using Python 2.7.10 on Mac OS X 10.10.5 with NLTK version 3.0.4. </p>
",Multilingual Language Processing & Language Identification,confusing appearance unicode literal nltk stock text starting work st edition natural language processing python run odd behavior output match book saying see instance consider following example book page whenever attempt thing however output ha every word unicode literal going using python mac x nltk version
Identifying topics of blog posts by taxonomy / natural language / elasticsearch aggregations,"<p>I want to do some analysis on a set of blog posts, specifically to determine what topics the posts are about. We can assume the blog posts will be in English.</p>

<p>The data is stored in an elasticsearch system, so perhaps aggregations would be useful?</p>

<p>I am looking for guidance on how I could go about doing this. For instance, if I were to explore utilizing a taxonomy set, what would be a good way to do the mapping? </p>

<p>If taxonomies are the way to go, is there any good standard taxonomy to make use of or is it necessary to build out own?</p>

<p>Thank you in advance!</p>
",Multilingual Language Processing & Language Identification,identifying topic blog post taxonomy natural language elasticsearch aggregation want analysis set blog post specifically determine topic post assume blog post english data stored elasticsearch system perhaps aggregation would useful looking guidance could go instance explore utilizing taxonomy set would good way mapping taxonomy way go good standard taxonomy make use necessary build thank advance
Calculate distance from densest part of cosine similarity 2d distribution,"<p>Forgive me in advance if my terminology sounds a bit vague, but I am trying to explain my problem in plain English.</p>

<p>Let's say I have 10 sets of documents and for each set I have calculated the cosine similarity matrix based on the term frequency matrix of the set.</p>

<p>In R we can simulate my list of cosine similarity matrices like this</p>

<pre><code>cosine_simil_mat &lt;- list()
for (i in 1:10) {
  cosine_simil_mat[[i]] &lt;-
    matrix(rnorm(n=100, mean=0.89, sd=.2),ncol=10)
}
</code></pre>

<p>Now, for each matrix, I can visualise the distance separating each document on a plane with this</p>

<pre><code>mds &lt;- cmdscale(1-cosine_simil_mat[[1]], eig=TRUE, k=2)
x &lt;- mds$points[,1]
y &lt;- mds$points[,2]
</code></pre>

<p>(note the <code>1-</code> part, since make more sense to visualise the dissimilarities (or distance) not the similarities)</p>

<p>I can plot my <code>mds</code> with </p>

<pre><code>mds_df &lt;-
  data.frame(x,y, type=c(""alpha"", c(rep(""betas"",dim(cosine_simil_mat[[i]])[1]-1)) ) )

require(ggplot2)
ggplot(mds_df, aes(x,y)) +
  geom_point(aes(colour = type)) + geom_density2d() + theme_bw()
</code></pre>

<p>which plots</p>

<p><a href=""https://i.sstatic.net/pL8Br.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pL8Br.png"" alt=""enter image description here""></a></p>

<p>Now, what I want to do is to understand how my document of interest (<code>alpha</code>), which is always the first row/column of the cosine similarity matrix, behaves in the different sets. Specifically, I want to measure the distance of my document <code>alpha</code> from the densest part of each plot, in order to understand whether the document <code>alpha</code> is at the core of the sets, measured in terms of relative term frequencies, or at the periphery and if its position change in the different sets. </p>

<p>Does any statistic capture this distance from the densest part of the plot? Does it make any sense?</p>
",Multilingual Language Processing & Language Identification,calculate distance densest part cosine similarity distribution forgive advance terminology sound bit vague trying explain problem plain english let say set document set calculated cosine similarity matrix based term frequency matrix set r simulate list cosine similarity matrix like matrix visualise distance separating document plane note part since make sense visualise dissimilarity distance similarity plot plot want understand document interest always first row column cosine similarity matrix behaves different set specifically want measure distance document densest part plot order understand whether document core set measured term relative term frequency periphery position change different set doe statistic capture distance densest part plot doe make sense
Replace single quotes with double with exclusion of some elements,"<p>I want to replace all single quotes in the string with double with the exception of occurrences such as ""n't"", ""'ll"", ""'m"" etc.</p>

<pre><code>input=""the stackoverflow don\'t said, \'hey what\'""
output=""the stackoverflow don\'t said, \""hey what\""""
</code></pre>

<p>Code 1:(@<a href=""https://stackoverflow.com/users/918959/antti-haapala"">https://stackoverflow.com/users/918959/antti-haapala</a>)</p>

<pre><code>def convert_regex(text): 
     return re.sub(r""(?&lt;!\w)'(?!\w)|(?&lt;!\w)'(?=\w)|(?&lt;=\w)'(?!\w)"", '""', text)
</code></pre>

<p>There are 3 cases: ' is NOT preceded and is NOT followed by a alphanumeric character; or is not preceded, but followed by an alphanumeric character; or is preceded and not followed by an alphanumeric character.</p>

<p>Issue: That doesn't work on words that end in an apostrophe, i.e. 
most possessive plurals, and it also doesn't work on informal 
abbreviations that start with an apostrophe.</p>

<p>Code 2:(@<a href=""https://stackoverflow.com/users/953482/kevin"">https://stackoverflow.com/users/953482/kevin</a>)</p>

<pre><code>def convert_text_func(s):
    c = ""_"" #placeholder character. Must NOT appear in the string.
    assert c not in s
    protected = {word: word.replace(""'"", c) for word in [""don't"", ""it'll"", ""I'm""]}
    for k,v in protected.iteritems():
        s = s.replace(k,v)
    s = s.replace(""'"", '""')
    for k,v in protected.iteritems():
        s = s.replace(v,k)
    return s
</code></pre>

<p>Too large set of words to specify, as how can one specify persons' etc.
Please help.</p>

<p><strong>Edit 1:</strong>
 I am using @anubhava's brillant answer. I am facing this issue. Sometimes, there language translations which the approach fail. 
Code=</p>

<pre><code>text=re.sub(r""(?&lt;!s)'(?!(?:t|ll|e?m|s|d|ve|re|clock)\b)"", '""', text)
</code></pre>

<p>Problem:</p>

<p>In text, 'Kumbh melas' melas is a Hindi to English translation not plural possessive nouns.</p>

<pre><code>Input=""Similar to the 'Kumbh melas', celebrated by the banks of the holy rivers of India,""
Output=Similar to the ""Kumbh melas', celebrated by the banks of the holy rivers of India,
Expected Output=Similar to the ""Kumbh melas"", celebrated by the banks of the holy rivers of India,
</code></pre>

<p>I am looking maybe to add a condition that somehow fixes it. Human-level intervention is the last option.</p>

<p><strong>Edit 2:</strong>
Naive and long approach to fix:</p>

<pre><code>def replace_translations(text):
    d = enchant.Dict(""en_US"")
    words=tokenize_words(text)
    punctuations=[x for x in string.punctuation]
    for i,word in enumerate(words):
        print i,word
        if(i!=len(words) and word not in punctuations and d.check(word)==False and words[i+1]==""'""):
            text=text.replace(words[i]+words[i+1],words[i]+""\"""")
    return text
</code></pre>

<p>Are there any corner cases I am missing or are there any better approaches?</p>
",Multilingual Language Processing & Language Identification,replace single quote double exclusion element want replace single quote string double exception occurrence n etc code large set word specify one specify person etc please help edit using anubhava brillant answer facing issue sometimes language translation approach fail code problem text kumbh melas melas hindi english translation plural possessive noun looking maybe add condition somehow fix human level intervention last option edit naive long approach fix corner case missing better approach
detect words with the same root,"<p>I am currently developing an automated <strong>Index-Generator</strong> for pdf-files in Java. The concept is pretty simple (right now): I iterate over every word in the pdf, compare with an ignore list (something like the 10000 most common words in that language) and then add it to a <code>com.google.common.collect.HashMultimap</code> with the word as String and a HashSet of pages, the word occurs on.</p>

<p>This is working pretty fine, but still I am getting words in all different <strong>declination/conjugation</strong> forms as separate items in the index. I was thinking of just comparing a relative sub-string of those words, but for instance in the German language (which the program is intended for) with all its irregularities, there is very less benefit of this approach. </p>

<p>Any other ideas, libraries, regex's, whatsoever?
Thanks in advance</p>
",Multilingual Language Processing & Language Identification,detect word root currently developing automated index generator pdf file java concept pretty simple right iterate every word pdf compare ignore list something like common word language add word string hashset page word occurs working pretty fine still getting word different declination conjugation form separate item index wa thinking comparing relative sub string word instance german language program intended irregularity le benefit approach idea library regex whatsoever thanks advance
Regular Expression Doesn&#39;t Work Properly With Turkish Characters,"<p>I write a regex that should extracts following patterns;</p>

<ul>
<li>""√ß√ß√ßoookkk ggg√º√º√ºzzzeeelll"" (it means vvveeerrryyy gggoooddd with turkish characters ""√ß"" and ""√º"")</li>
<li>""ccccoookkk ggguuuzzzeeelll"" (it means the same but with english characters ""c"" and ""u"")</li>
</ul>

<p>here is the regular expressions i'm trying;</p>

<ul>
<li><code>""\b[√ßc]+o+k+\sg+[√ºu]+z+e+l+\b""</code> : this works in english but not in turkish characters</li>
<li><code>""√ßok""</code>: finds ""√ßok"" but when i try <code>""√ß+o+k+""</code> doesn't work for ""√ß√ß√ßoookkk"", it finds ""√ßoookkk""</li>
<li><code>""g√ºzel""</code>: finds ""g√ºzel"" but when i try <code>""g+√º+z+e+l+""</code> doesn't work for ""ggg√º√º√ºzzzeeelll""</li>
<li><code>""\b(c+o+k+)|(√ß+o+k+)\s(g+u+z+e+l)|(g+√º+z+e+l+)\b""</code>: doesn't work properly</li>
<li><code>""[√ßc]ok\sg[u√º]zel""</code>: I also tried this to get ""√ßok g√ºzel"" pattern but doesn't work neither.</li>
</ul>

<p>I thing the problem might be using regex operators with turkish characters. I don't know how can i solve this. </p>

<p>I am using <a href=""http://www.myregextester.com"" rel=""nofollow"">http://www.myregextester.com</a> to check if my regular expressions are correct.</p>

<p>I am using Php programming language to get a specific pattern from searched tweets via Twitter Rest Api.</p>

<p>Thanks,</p>
",Multilingual Language Processing & Language Identification,regular expression work properly turkish character write regex extract following pattern oookkk ggg zzzeeelll mean vvveeerrryyy gggoooddd turkish character ccccoookkk ggguuuzzzeeelll mean english character c u regular expression trying work english turkish character find ok try work oookkk find oookkk find g zel try work ggg zzzeeelll work properly also tried get ok g zel pattern work neither thing problem might using regex operator turkish character know solve using check regular expression correct using php programming language get specific pattern searched tweet via twitter rest api thanks
How can I use Natural Language Processing to check if a paragraph contains predefined topics?,"<p>We have a system that allows users to answer a question as free text and we want to check whether their answer contains any of our predefined topics. These topics will be defined prior to answers being submitted.</p>

<p>We tried to use a method similar to spam detection, but this is only good for determining whether something is true/false, incorrect/correct. We need the response to say which of the predefined topics a piece of text contains. Is there an algorithm that would solve this problem?</p>
",Multilingual Language Processing & Language Identification,use natural language processing check paragraph contains predefined topic system allows user answer question free text want check whether answer contains predefined topic topic defined prior answer submitted tried use method similar spam detection good determining whether something true false incorrect correct need response say predefined topic piece text contains algorithm would solve problem
What are the methods except Bag Of Words (TF-IDF) for converting textual features into numerical features?,"<p>I have been working on Natural Language Processing these days. My aim is to classify different words in a multi-lingual sentence written in Roman Script based on some criteria. Thus, I need a classifier for it. Unquestionably, there are many. But since my features aren't numerical but textual, and most of the classifiers like Support Vector Machine (SVM) input numerical features, I looked for some methodology to convert my textual features into numerical one. Though the concept of Bag Of Words with the use of Term Frequency and Inverse Document Frequency (TF-IDF) is a generic approach for this purpose, one of my textual feature, namely local context, is of fixed length and i want to know if it is possible to convert it into numerical feature without using TF-IDF. Local context feature refers to considering previous two and next two words (which comprise the context of a particular word). Therefore, I am looking for any other methodology which could prove to be better in this case. I found similar query at Cross Validated <a href=""https://stats.stackexchange.com/questions/144327/convert-categorical-data-into-numerical-data"">here</a>, but that is for document clustering and i want to classify individual words into different classes. I also found one unanswered similar <a href=""https://www.quora.com/What-are-different-techniques-for-converting-qualitative-features-into-numerical-features-for-machine-learning-algorithms"" rel=""nofollow noreferrer"">question</a> on quora.</p>

<p>To serve my purpose, I want either the textual feature to be converted into numerical one or a classifier that can take textual features as input. Is there any one who could help me...</p>
",Multilingual Language Processing & Language Identification,method except bag word tf idf converting textual feature numerical feature working natural language processing day aim classify different word multi lingual sentence written roman script based criterion thus need classifier unquestionably many since feature numerical textual classifier like support vector machine svm input numerical feature looked methodology convert textual feature numerical one though concept bag word use term frequency inverse document frequency tf idf generic approach purpose one textual feature namely local context fixed length want know possible convert numerical feature without using tf idf local context feature refers considering previous two next two word comprise context particular word therefore looking methodology could prove better case found similar query cross validated question quora serve purpose want either textual feature converted numerical one classifier take textual feature input one could help
Natural Language Processing - Converting unstructured bibliography to structured metadata,"<p>Currently working on a natural language processing project in which I need to convert unstructured bibliography section (which is at the end of research article) to structured metadata like ""Year"", ""Author"", ""Journal"",  ""Volume ID"", ""Page Number"", ""Title"", etc.</p>

<hr>

<p>For example: Input  </p>

<pre class=""lang-none prettyprint-override""><code>McCallum, A.; Nigam, K.; and Ungar, L. H. (2000). Efficient clustering of high-dimensional data sets with application to reference matching. In Knowledge Discovery and Data Mining, 169‚Äì178
</code></pre>

<hr>

<p>Expected output: </p>

<pre class=""lang-none prettyprint-override""><code>&lt;Author&gt; McCallum, A.&lt;/Author&gt; &lt;Author&gt;Nigam, K.&lt;/Author&gt; &lt;Author&gt;Ungar, L. H.&lt;/Author&gt;
&lt;Year&gt; 2000 &lt;/Year&gt;
&lt;Title&gt;Efficient clustering of high-dimensional data sets with application to reference matching &lt;Title&gt; and so on
</code></pre>

<p>Tool used: <a href=""http://www.chokkan.org/software/crfsuite/"">CRFsuite</a> </p>

<hr>

<p>Data-set: This contains 12000 references</p>

<ol>
<li>Contains Journal title,  </li>
<li>Contains article title's words, </li>
<li>Contains location names,</li>
</ol>

<hr>

<p>Each word in given line considered as token and for each token I derive following features</p>

<ol>
<li>BOR at the start of line, </li>
<li>EOR for end </li>
<li>digitFeature : if token is digit </li>
<li>Year: if token is in year format like 19** and 20** </li>
<li>available in current data-set,</li>
</ol>

<hr>

<p>From above tool and data-set I got only 63.7% accuracy. Accuracy is very less for ""Title"" and good for ""Year"" and ""Volume"".</p>

<p>Questions:</p>

<ol>
<li>Can I draw any additional features?</li>
<li>Can I use any other tool?</li>
</ol>
",Multilingual Language Processing & Language Identification,natural language processing converting unstructured bibliography structured metadata currently working natural language processing project need convert unstructured bibliography section end research article structured metadata like year author journal volume id page number title etc example input expected output tool used question draw additional feature use tool
Lexical richness as Shannon&#39;s Entropy; Python,"<p>The Entropy formula for lexical richness is </p>

<p><a href=""https://i.sstatic.net/kTJoW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kTJoW.png"" alt=""enter image description here""></a></p>

<p>The probability p-ith is calculated by dividing V-ith by N, where N is the total number of tokens in the text and V-ith is the number of times a specific type occurs (at least that's my understanding). </p>

<p>So, if i have a string <code>the, the, the, a, a, over, love, one, tree</code>
there are 9 <code>tokens</code>, but only 6 <code>types</code>.</p>

<p><code>V-'theth'</code> (from what I understand) would be <code>3</code> and therefore <code>p-'theth'</code> would be calculated as <code>3/9 = 0.33</code>. <code>V-'ath'</code> would then be <code>0.22</code>, and so on. <code>H</code> in this instance would be <code>-100*((0.33*log0.33 + 0.22*log0.22 + 0.11*log0.11 + 0.11*log0.11 + 0.11*log0.11+ 0.11*log0.11)/log9)</code></p>

<p>Although I can get the length of a string (tokens) in Python:</p>

<pre><code> string = ['the', 'the', 'the', 'a', 'a', 'over', 'love', 'one', 'tree']
 len(string)
 9
</code></pre>

<p>And the number of types:</p>

<pre><code>len(set(string))
6
</code></pre>

<p>I am not entirely sure how can I calculate this formula in Python. 
Thanks.</p>

<p>Source: Dale, Moisl, and Somers (p.551). ""Handbook of Natural Language Processing"" (2000). <a href=""https://books.google.at/books?id=VoOLvxyX0BUC&amp;pg=PA551&amp;lpg=PA551&amp;dq=entropy+vocabulary+richness&amp;source=bl&amp;ots=wucWFF1Rn_&amp;sig=Hms1qwhXlcOaPEXI84eDqxsTEdo&amp;hl=en&amp;sa=X&amp;ved=0CC8Q6AEwAmoVChMIjvvQnvPVxwIVhJ5yCh35ZAb_#v=onepage&amp;q&amp;f=false"" rel=""nofollow noreferrer"">https://books.google.at/books?id=VoOLvxyX0BUC&amp;pg=PA551&amp;lpg=PA551&amp;dq=entropy+vocabulary+richness&amp;source=bl&amp;ots=wucWFF1Rn_&amp;sig=Hms1qwhXlcOaPEXI84eDqxsTEdo&amp;hl=en&amp;sa=X&amp;ved=0CC8Q6AEwAmoVChMIjvvQnvPVxwIVhJ5yCh35ZAb_#v=onepage&amp;q&amp;f=false</a></p>
",Multilingual Language Processing & Language Identification,lexical richness shannon entropy python entropy formula lexical richness probability p ith calculated dividing v ith n n total number token text v ith number time specific type occurs least understanding string understand would therefore would calculated would instance would although get length string token python number type entirely sure calculate formula python thanks source dale moisl somers p handbook natural language processing
API for Natural Language Processing in Android,"<p>I am trying to make an Android application similar to the one on <a href=""http://cleverbot.com/"" rel=""noreferrer"">this website</a>. </p>

<p>The thing is that I am pretty new to the field of Natural Language Processing. I do not wish to achieve much, just provide the user some interaction with the application, give him a feeling that he is indeed chatting with someone.</p>

<p>Basically, I would just capture the text entered by the user and send it over to the API and display the result retrieved from the API.
I came across <a href=""http://opennlp.apache.org/"" rel=""noreferrer"">http://opennlp.apache.org/</a> and <a href=""http://gate.ac.uk/"" rel=""noreferrer"">http://gate.ac.uk/</a> but have NO idea how to implement them in my Android application.</p>

<p><strong>ANY</strong> ideas/ suggestions/ help would indeed be great.</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,api natural language processing android trying make android application similar one website thing pretty new field natural language processing wish achieve much provide user interaction application give feeling indeed chatting someone basically would capture text entered user send api display result retrieved api came across idea implement android application idea suggestion help would indeed great thanks
Morphology software for English,"<p>In my application I need to use a piece of software able to: a) convert the words to their basic forms and b) find if they are 'nouns', 'verbs' etc.</p>

<p>I found a list of software able to do that job.</p>

<p><a href=""http://aclweb.org/aclwiki/index.php?title=Morphology_software_for_English"" rel=""nofollow"">http://aclweb.org/aclwiki/index.php?title=Morphology_software_for_English</a></p>

<p>Does anyone have any experience with any of these? Which one do you recommend?</p>
",Multilingual Language Processing & Language Identification,morphology software english application need use piece software able convert word basic form b find noun verb etc found list software able job doe anyone experience one recommend
Getting rare translations from Google Translate request,"<p>For my Bachelors Degree, I am working on a statistical machine translator. Currently, I need a dictionary of English words with all their possible translations. For example:</p>

<pre><code>[EN] Food -&gt; [RO] Mancare
</code></pre>

<p>or</p>

<pre><code>[EN] Food -&gt; [RO] Hrana
</code></pre>

<p>To build the dictionary I need, I adapted a python script (original <a href=""https://github.com/mouuff/Google-Translate-API/blob/master/python/GoogleTranslate.py"" rel=""nofollow"">here</a>) that takes every word from a list of English words and sends it to google translate, then writes the result in a file. However, I only receive one result per word, the most common translation.</p>

<p>How can I receive the uncommon and rare translations? The Google translate website lists other possible translations, but they do not appear in my result. The <a href=""https://translate.google.com/m?hl=ro&amp;sl=en&amp;q=food"" rel=""nofollow"">link used in the script</a> is different from the regular Google Translate. I tried sending the request to the regular Google Translate link, but the return page doesn't contain the information I need.</p>

<p>Alternatively, do you know an already existing English - Romanian dictionary with multiple possible translations per word?</p>

<p>Thank you!</p>

<p>This is my Google Translate script:</p>

<pre><code>    import urllib2

def translate(word, sourceLanguage, targetLanguage):
    agents = {'User-Agent':""Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30)""}
    before_trans = 'class=""t0""&gt;'
    link = ""http://translate.google.com/m?hl=%s&amp;sl=%s&amp;q=%s"" % (targetLanguage, sourceLanguage, word)
    request = urllib2.Request(link, headers=agents)
    page = urllib2.urlopen(request).read()
    result = page[page.find(before_trans)+len(before_trans):]
    result = result.split(""&lt;"")[0]
    return (result,page)

def main():
    englishWords = open('englishWords.txt', 'r')
    romanianWords = open('romanianWords.txt', 'w')
    studyHTML = open('studyHTML.html', 'w')
    studyTXT = open('studyTXT.txt', 'w')
    for englishWord in englishWords:
        translation = translate(englishWord, 'en', 'ro')
        romanianWord = translation[0]
        page = translation[1]
        if romanianWord != ""d&gt;"":
            romanianWords.write(romanianWord)
            romanianWords.write(""\n"")
            studyHTML.write(page)
            studyTXT.write(page)


if __name__ == '__main__':
    main()
</code></pre>

<p>Example of what I'm trying to get:</p>

<pre><code>Food: Mancare, Hrana, Aliment
Car: Masina, Autovehicul
Cat: Pisica, Felina
</code></pre>
",Multilingual Language Processing & Language Identification,getting rare translation google translate request bachelor degree working statistical machine translator currently need dictionary english word possible translation example build dictionary need adapted python script original take every word list english word sends google translate writes result file however receive one result per word common translation receive uncommon rare translation google translate website list possible translation appear result link used script different regular google translate tried sending request regular google translate link return page contain information need alternatively know already existing english romanian dictionary multiple possible translation per word thank google translate script example trying get
How to train an Italian language model in OpenNLP on Hadoop?,"<p>I would like to implement a Natural language processing algorithm on Hadoop for Italian language</p>

<p>I have 2 questions; </p>

<ol>
<li><strong>how I can find a stemming algorithm for italian</strong> ?</li>
<li><strong>how to integrate in hadoop</strong>?</li>
</ol>

<p>here is my code</p>

<pre><code>String pathSent=...tagged sentences...;
String pathChunk=....chunked train path....;
File fileSent=new File(pathSent);
File fileChunk=new File(pathChunk);
InputStream inSent=null;
InputStream inChunk=null;

inSent = new FileInputStream(fileSent);
inChunk = new FileInputStream(fileChunk);
POSModel posModel=POSTaggerME.train(""it"", new WordTagSampleStream((
new InputStreamReader(inSent))), ModelType.MAXENT, null, null, 3, 3);

ObjectStream stringStream =new PlainTextByLineStream(new InputStreamReader(inChunk));
ObjectStream chunkStream = new ChunkSampleStream(stringStream);
ChunkerModel chunkModel=ChunkerME.train(""it"",chunkStream ,1, 1);
this.tagger= new POSTaggerME(posModel);
this.chunker=new ChunkerME(chunkModel);


inSent.close();
inChunk.close();
</code></pre>
",Multilingual Language Processing & Language Identification,train italian language model opennlp hadoop would like implement natural language processing algorithm hadoop italian language question find stemming algorithm italian integrate hadoop code
How to efficiently filter a string against a long list of words in Python/Django?,"<p>Stackoverflow implemented its ""Related Questions"" feature by taking the title of the current question being asked and removing from it the 10,000 most common English words according to Google. The remaining words are then submitted as a fulltext search to find related questions.</p>

<p>I want to do something similar in my Django site. What is the best way to filter a string (the question title in this case) against a long list of words in Python? Any libraries that would enable me to do that efficiently?</p>
",Multilingual Language Processing & Language Identification,efficiently filter string long list word python django stackoverflow implemented related question feature taking title current question asked removing common english word according google remaining word submitted fulltext search find related question want something similar django site best way filter string question title case long list word python library would enable efficiently
PHP Array before element and after element,"<p>I have a string and I change it into an array like this:</p>

<pre><code>$a = ""adik melempar kail ke tengah danau""
</code></pre>

<p>Array :</p>

<pre><code>[0] =&gt; adik 
[1] =&gt; melempar
[2] =&gt; kail 
[3] =&gt; ke 
[4] =&gt; tengah 
[5] =&gt; danau
</code></pre>

<p>I have marked <code>array[1] =&gt; melempar</code> as verb (kata kerja).</p>

<p>How do I mark the next array (after <code>array[1]</code>) as object and the previous array (before <code>array[1]</code>) as subject?</p>

<p>An english example might be:</p>

<pre><code>$a = ""brother throw hook to the lake""
</code></pre>

<p>Array :</p>

<pre><code>[0] =&gt; brother 
[1] =&gt; throw
[2] =&gt; hook
[3] =&gt; to 
[4] =&gt; the 
[5] =&gt; lake
</code></pre>
",Multilingual Language Processing & Language Identification,php array element element string change array like array marked verb kata kerja mark next array object previous array subject english example might array
Variations in spelling of first name,"<p>As part of a contact management system I have a large database of names. People frequently edit this and as a result we run into issues of the same person existing in different forms (John Smith and Jonathan Smith). I looked into word similarity but it's easy to think of name variations which are not similar at all (Richard vs Dick). I was wondering if there was a list of common English first name variations that I could use to detect and correct such errors.</p>
",Multilingual Language Processing & Language Identification,variation spelling first name part contact management system large database name people frequently edit result run issue person existing different form john smith jonathan smith looked word similarity easy think name variation similar richard v dick wa wondering wa list common english first name variation could use detect correct error
Annotating a corpus for SMS Text Normalization,"<p>I want to build an SMS Text Normalizer using Supervised Learning Techniques. SMS Text Normalization is the task of converting SMS lingo into correct English.</p>

<p>eg) 'wts up? r u hme?' will become 'What's up? Are you home?'.</p>

<p>Ideally, I would like a readily available corpus with SMS Text and the subsequent English text as the training data. However, I couldn't find any such publicly available data set online. (SMS Text corpora are available, but not the corresponding text in grammatically correct English) People who have worked on similar problems before seem to have manually annotated the text. </p>

<ol>
<li><p>Which would be the quickest way to annotate this text? Possibly, one can scrape data for each word token from standard SMS conversion sites/ urban dictionary to get the equivalent English words. But this would work correctly only for standard SMS Text and only marginally reduce the manual work.</p></li>
<li><p>Partition the corpus and ask individuals to annotate it manually, but this will be very slow especially for large amount of text.</p></li>
</ol>
",Multilingual Language Processing & Language Identification,annotating corpus sm text normalization want build sm text normalizer using supervised learning technique sm text normalization task converting sm correct english eg wts r u hme become home ideally would like readily available corpus sm text subsequent english text training data however find publicly available data set online sm text corpus available corresponding text grammatically correct english people worked similar problem seem manually annotated text would quickest way annotate text possibly one scrape data word token standard sm conversion site urban dictionary get equivalent english word would work correctly standard sm text marginally reduce manual work partition corpus ask individual annotate manually slow especially large amount text
"Need help Piping Python twitter script to NLP Bash script (Sed, grep etc....)","<p><strong>Hello, I'm very new to programming and I've started only a few weeks ago. It would be greatly appreciated if I could get some help. Thanks in advance !</strong></p>

<ol>
<li><p>My python script (stream_tweets.py) streams 200 - 300 tweets from twitter per minute. The script is located in '/home/computer/Twitter/examples/stream_tweets.py'</p></li>
<li><p>I have an NLP (Natural Language Processing) bash script that analyzes sentences and prints it out on bash. The NLP script (corenlp.sh) is located in '/home/computer/Standford/corenlp.sh'</p></li>
<li><p>If I create a new bash script, how do I pipe the tweets into the NLP? How would this script look like?</p></li>
<li><p>My python tweet script (stream_tweets.py) needs to output the text in utf-8 format, how do I change the script to do so.</p></li>
<li><p>The NLP takes a while to load, If tweets are storming into the NLP which hasn't loaded yet, will it affect my script? If so what can I do and how to do it?</p></li>
<li><p><strong>Take a look at the stream_twitter.py script</strong></p>

<pre><code> from TwitterAPI import TwitterAPI
 TRACK_TERM = ‚Äòkeyword1,keyword2,keyword3'
 CONSUMER_KEY = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
 CONSUMER_SECRET = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
 ACCESS_TOKEN_KEY = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
 ACCESS_TOKEN_SECRET = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
 api = TwitterAPI(CONSUMER_KEY,
                  CONSUMER_SECRET,
                  ACCESS_TOKEN_KEY,
                  ACCESS_TOKEN_SECRET)
 r = api.request('statuses/filter', {'track': TRACK_TERM})
 for item in r:
     print(item['text'] if 'text' in item else item)
</code></pre></li>
<li><p><strong>Take a look at the NLP script</strong> </p>

<pre><code> OS=`uname`
 # Macs (BSD) don't support readlink -e
 if [ ""$OS"" == ""Darwin"" ]; then
     scriptdir=`dirname $0`
 else
     scriptpath=$(readlink -e ""$0"") || scriptpath=$0
     scriptdir=$(dirname ""$scriptpath"")
 fi
 echo java -mx3g -cp \""$scriptdir/*\""            edu.stanford.nlp.pipeline.StanfordCoreNLP $*
 java -mx3g -cp ""$scriptdir/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP $*
</code></pre></li>
</ol>
",Multilingual Language Processing & Language Identification,need help piping python twitter script nlp bash script sed grep etc hello new programming started week ago would greatly appreciated could get help thanks advance python script stream tweet py stream tweet twitter per minute script located home computer twitter example stream tweet py nlp natural language processing bash script analyzes sentence print bash nlp script corenlp sh located home computer standford corenlp sh create new bash script pipe tweet nlp would script look like python tweet script stream tweet py need output text utf format change script nlp take load tweet storming nlp loaded yet affect script take look stream twitter py script take look nlp script
UIMA/dkpro: Get type of conjunction,"<p>I am using UIMA in combination with UIMAfit and dkpro and the StanfordParser to parse english sentences.</p>

<p>I can build dependency trees without any problem. For ""and""/""or"" conjunctions, I get a Annotation with class <code>CONJ</code>, which is a subclass of <code>Dependency</code>. As of now, I did not find out, how to check if the found conjunction is an ""AND"" or an ""OR"" conjunction.</p>

<p>Does anybody know how to solve this? I saw examples where the dependencies ""conj_and"" and ""conj_or"" are displayed, but I dont see where they come from.
<a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow"">http://nlp.stanford.edu/software/dependencies_manual.pdf</a></p>

<p>Thanks in advance</p>

<p>Some code for visualization:</p>

<pre><code>// CONJ a;
// StringBuilder sb
Token dependent = a.getDependent();
Token governor = a.getGovernor();

sb.append(""Dependent: "");
sb.append(dependent);
sb.append("", "");

sb.append(""Governor: "");
sb.append(governor);

// How to check type conj_and/conj_or?
</code></pre>
",Multilingual Language Processing & Language Identification,uima dkpro get type conjunction using uima combination uimafit dkpro stanfordparser parse english sentence build dependency tree without problem conjunction get annotation class subclass find check found conjunction conjunction doe anybody know solve saw example dependency conj conj displayed dont see come thanks advance code visualization
Regex to extract the words from other languages,"<p>I know that I can extract the English letters and numbers by using the <code>A-Za-z0-9</code> regex.</p>

<p>How can I extract the words from other languages such as Arabic and only allow the letters and numbers in their script and nothing else?</p>

<p>One way I have used is to filter out everything I don't want from the text and then I am left with the just words but this approach takes a lot of CPU time and is not efficient on large-scale applications.</p>

<p>Now I was wondering what other methods there were in use or someone knows that can be used to analyse the text of other languages.</p>

<p>How can be words extracted from languages such as Chinese, Japanese, etc which do not even use spaces between words? One approach I took to differentiate between words is to see the styles and line breaks as a method to realise that they must be different works but this approach can be unreliable sometimes when people don't use a lot of line breaks or formatting to separate different words.</p>

<p>So, to sum up, how can other languages can be analysed using regex? </p>
",Multilingual Language Processing & Language Identification,regex extract word language know extract english letter number using regex extract word language arabic allow letter number script nothing else one way used filter everything want text left word approach take lot cpu time efficient large scale application wa wondering method use someone know used analyse text language word extracted language chinese japanese etc even use space word one approach took differentiate word see style line break method realise must different work approach unreliable sometimes people use lot line break formatting separate different word sum language analysed using regex
Stanford CoreNLP train model from text file like englishPCFG.ser.gz,"<p>I am new to Stanford CoreNLP, Initially I have worked with Moses project.
So far I have worked with the demo file ParserDemo2 and everything worked fine using englishPCFG.caseless.ser.gz model.
I need to create my own model, from the text English monolingual corpus which I have.</p>

<p>So far I have searched and found that I need to create a TreeBank and use method trainFromTreebank in LexicalizedParser class.</p>

<p>I am really confused how to do this.</p>

<p>Can you provide some information or point me to the documentation on how to do so?</p>
",Multilingual Language Processing & Language Identification,stanford corenlp train model text file like englishpcfg ser gz new stanford corenlp initially worked moses project far worked demo file parserdemo everything worked fine using englishpcfg caseless ser gz model need create model text english monolingual corpus far searched found need create treebank use method trainfromtreebank lexicalizedparser class really confused provide information point documentation
bad tokenization in stanford postagger,"<p>I'm trying to use the Stanford POS tagger to tag some French text. To do that, I use the following command:</p>

<blockquote>
  <p>cat file.txt | java -mx10000m -cp 'stanford-postagger.jar:'
  edu.stanford.nlp.tagger.maxent.MaxentTagger -model
  models/french.tagger -sentenceDelimiter newline > output.txt</p>
</blockquote>

<p>(There is one sentence per line.)</p>

<p>But I noticed that the tags were pretty bad, and that the real issue actually comes from the French tokenization itself. I think that the tokenization is done by an English tokenizer.</p>

<p>So I tried to only tokenize the text in French by doing this:</p>

<blockquote>
  <p>cat file.txt | java -mx10000m -cp 'stanford-postagger.jar:'
  edu.stanford.nlp.international.french.process.FrenchTokenizer
  -sentenceDelimiter newline > tokenized.txt</p>
</blockquote>

<p>And there the French tokens are good.</p>

<p>How can I tell the tagger to use the French model for tagging, but also the French tokenizer at the same time?</p>
",Multilingual Language Processing & Language Identification,bad tokenization stanford postagger trying use stanford po tagger tag french text use following command cat file txt java mx cp stanford postagger jar edu stanford nlp tagger maxent maxenttagger model model french tagger sentencedelimiter newline output txt one sentence per line noticed tag pretty bad real issue actually come french tokenization think tokenization done english tokenizer tried tokenize text french cat file txt java mx cp stanford postagger jar edu stanford nlp international french process frenchtokenizer sentencedelimiter newline tokenized txt french token good tell tagger use french model tagging also french tokenizer time
Algorithm/Library for merged english words analysis,"<p>I'm currently having a lot of data (subtitles for films) which contains ""compound words"" in english. </p>

<p>There is no certain pattern, but in some places words are merged. E.g.: </p>

<pre><code>   You got to keep going. They're always looking **forsomething** under the     seats.
</code></pre>

<p>I don't have specific constraints on language/platform, however there is really lots and lots of data and I won't be able to check results properly. So I need kind of highly-precise solution here. </p>

<p>So what are basically the ways to attack this kind of data? </p>
",Multilingual Language Processing & Language Identification,algorithm library merged english word analysis currently lot data subtitle film contains compound word english certain pattern place word merged e g specific constraint language platform however really lot lot data able check result properly need kind highly precise solution basically way attack kind data
Nullpointer exception during loading dependency parser,"<p>I'm using stanford-corenlp-3.5.2 and the german parsing model germanFactored.ser.gz. The code works fine until I try to do dependency parsing. While initialilizing the <code>SemanticGraph</code> I retrieve a <code>NullPointerException</code>.</p>

<p>Loading: CollapsedCCProcessedDependenciesAnnotation.class (the first line throws the exception)</p>

<pre><code>SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
System.out.println(dependencies);    
dependencies.prettyPrint();
</code></pre>

<p>Whole code:</p>

<pre><code>public static void main(String[] args) {
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse"");
    props.put(""parse.model"", ""edu/stanford/nlp/models/lexparser/germanFactored.ser.gz"");
    props.put(""ner.model"", ""edu/stanford/nlp/models/ner/german.dewac_175m_600.crf.ser.gz"");
    props.put(""pos.model"", ""edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger"");
    props.put(""parse.flags"", """");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    String text = ""Sehr gute Kenntnisse in Englisch und Deutsch sowie Programmierkenntnisse (C#, Java, C++)."";                                                                                                                                              // text
    Annotation document = new Annotation(text);
    pipeline.annotate(document);

    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for (CoreMap sentence : sentences) {
        for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
            String word = token.get(TextAnnotation.class);
            String pos = token.get(PartOfSpeechAnnotation.class);
            String ne = token.get(NamedEntityTagAnnotation.class);

            System.out.println(String.format(""%s - %s - %s"", word, pos, ne));
        }

        Tree tree = sentence.get(TreeAnnotation.class);
        System.out.println(tree.toString());

        try {
            SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
            System.out.println(dependencies);

            dependencies.prettyPrint();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
</code></pre>
",Multilingual Language Processing & Language Identification,nullpointer exception loading dependency parser using stanford corenlp german parsing model germanfactored ser gz code work fine try dependency parsing initialilizing retrieve loading collapsedccprocesseddependenciesannotation class first line throw exception whole code
Convert natural language into logical formula,"<p>I tried for days to write a NLTK grammar to convert simple French sentences into logical formulas. My problem can be similar with English sentences. My goal is that this grammar accepts several orders (home automation) and converts them into logical formulas. Some examples of orders:</p>

<p>Turn on the light:</p>

<pre><code>exists x.(turn_on(x) &amp; light(x))
</code></pre>

<p>Turn on the green light:</p>

<pre><code>exists x.(turn_on(x) &amp; light(x) &amp; green(x))
</code></pre>

<p>Turn on the light of the kitchen</p>

<pre><code>exists x.(turn_on(x) &amp; light(x) &amp; exists y.(kitchen(y) &amp; in(x, y)))
</code></pre>

<p>In these examples, the word <em>turn_on</em> is not really a logical predicate. It will be used in the next step of my program (when it will convert this formula into another representation).</p>

<p>However, I have many difficulties to write the rule about possession relationship. I would like that the rule accepts an ""infinite"" recursion like:</p>

<ul>
<li>turn on the light of the kitchen (the light belongs to the kitchen in my database)</li>
<li>turn on the light of the kitchen of the house (the kitchen belongs to the house in my database)</li>
<li>turn on the light of the kitchen of the house of [...] (etc.)</li>
</ul>

<p>I succeeded to convert the first sentence but not the others. Here my grammar (I translate French to English for a better understanding):</p>

<pre><code>% start SV

SV[SEM=&lt;?v(?sn)&gt;] -&gt; V[SEM=?v] SN[SEM=?sn]

SN[SEM=&lt;?ap(?sn1, ?sn2)&gt;] -&gt; SN[SEM=?sn1] AP[SEM=?ap] SN[SEM=?sn2]
SN[SEM=&lt;?ad(?n)&gt;]         -&gt; AD[SEM=?ad] N[SEM=?n]
SN[SEM=?n]                -&gt; N[SEM=?n]

N[SEM=&lt;?adj(?n)&gt;] -&gt; ADJ[SEM=?adj] N[SEM=?n]

V[SEM=&lt;\P.P(\x.turn_on(x))&gt;]  -&gt; 'turn' 'on'

N[SEM=&lt;\x.light(x)&gt;]    -&gt; 'light'
N[SEM=&lt;\x.kitchen(x)&gt;]  -&gt; 'kitchen'
N[SEM=&lt;\x.house(x)&gt;]    -&gt; 'house'

ADJ[SEM=&lt;\P x.(P(x) &amp; big(x))&gt;]   -&gt; 'big'
ADJ[SEM=&lt;\P x.(P(x) &amp; green(x))&gt;] -&gt; 'green'

AD[SEM=&lt;\P Q.exists x.(P(x) &amp; Q(x))&gt;]         -&gt; 'the'
AP[SEM=&lt;\P Q R.Q(\x.P(\y.(in(y,x) &amp; R(y))))&gt;] -&gt; 'of'
</code></pre>

<p>With this grammar and the order ""turn on the light of the kitchen"", I get:</p>

<pre><code>exists x.(kitchen(x) &amp; exists z1.(light(z1) &amp; in(z1,x) &amp; turn_on(z1)))
</code></pre>

<p>But, for the order ""turn on the light of the kitchen of the house"":</p>

<pre><code>exists x.(house(x) &amp; exists z5.(kitchen(z5) &amp; exists z2.(light(z2) &amp; in(z2,z5) &amp; in(z2,x) &amp; turn_on(z2))))
</code></pre>

<p>To be more readable, the same formula without the ""exists"":</p>

<pre><code>(house(x4) &amp; kitchen(x6) &amp; light(x7) &amp; in(x7,x6) &amp; in(x7,x4) &amp; turn_on(x7))
</code></pre>

<p>There is a problem with the ""in"" predicates. Indeed, I want that the light is in the kitchen and that the kitchen is in the house. However, in this case, the light is in the kitchen and in the house (yes, it's true, but I don't want that =/). Here's what I would like:</p>

<pre><code>(house(x4) &amp; kitchen(x6) &amp; light(x7) &amp; in(x7,x6) &amp; in(x6,x4) &amp; turn_on(x7))
                                   the difference -----^
</code></pre>

<p>I tried several methods but none of them worked... Can you help me please? I don't know if it's possible with my grammar. My knowledge on logic and lambda calcul are limited, I only just beginning to get interested in these topics.</p>

<p><strong>EDIT:</strong>
Here is the python code that I use for my tests:</p>

<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import nltk

def exec(parser, query):
    try:
        trees = list(parser.parse(query.split()))
    except ValueError:
        print('Invalid query')
        return
    if len(trees) == 0:
        print('Invalid query')
        return
    print('query: %s' % query)
    print('results:')
    for t in trees:
        sem = t.label()['SEM']
        print('\t%s' % sem)
    print('')

if __name__ == '__main__':
    parser = nltk.load_parser('./en_grammar.fcfg')
    exec(parser, 'turn on the light')
    exec(parser, 'turn on the light of the kitchen')
    exec(parser, 'turn on the light of the kitchen of the house')
</code></pre>

<p>Thanks a lot and sorry for my English.</p>
",Multilingual Language Processing & Language Identification,convert natural language logical formula tried day write nltk grammar convert simple french sentence logical formula problem similar english sentence goal grammar accepts several order home automation convert logical formula example order turn light turn green light turn light kitchen example word turn really logical predicate used next step program convert formula another representation however many difficulty write rule possession relationship would like rule accepts infinite recursion like turn light kitchen light belongs kitchen database turn light kitchen house kitchen belongs house database turn light kitchen house etc succeeded convert first sentence others grammar translate french english better understanding grammar order turn light kitchen get order turn light kitchen house readable formula without exists problem predicate indeed want light kitchen kitchen house however case light kitchen house yes true want would like tried several method none worked help please know possible grammar knowledge logic lambda calcul limited beginning get interested topic edit python code use test thanks lot sorry english
Semantic-based sentiment classification using NLTK,"<p>I am trying to build a feedback sentiment analyser based on a semantic approach. As an example: ""I had safe journey"" ‚Äì assume this is feedback for a driver, provided by a passenger. I need to extract the following information from this sentence:</p>

<pre><code>""I had safe journey"" -&gt; 
 SUBJECT= ""driving""
 SENTIMENT= ""positive""
</code></pre>

<p>I am using NLTK and I referred NLTK Book. I already followed the ""Extracting Information from Text"" section. That section talks about Chunking using a regular-expression based grammar. But I am confused, there are a number of different sentence patterns for the English language and I don't know how to cover all the possibilities.</p>

<p>Actually, I am new to NLP. I also found ""Analyzing Sentence Structure"" and ""Analyzing the Meaning of Sentences"" in the NLTK book. Is chunking enough to achieve  my goal and what am I supposed to do?</p>
",Multilingual Language Processing & Language Identification,semantic based sentiment classification using nltk trying build feedback sentiment analyser based semantic approach example safe journey assume feedback driver provided passenger need extract following information sentence using nltk referred nltk book already followed extracting information text section section talk chunking using regular expression based grammar confused number different sentence pattern english language know cover possibility actually new nlp also found analyzing sentence structure analyzing meaning sentence nltk book chunking enough achieve goal supposed
Tokenizing in french using nltk,"<p>I am trying to tokenize french words but when i tokenize french words the words which contain  ""^"" symbol returns \xe .The following is the code that i implemented
.</p>

<pre><code>import nltk
from nltk.tokenize import WhitespaceTokenizer
from nltk.tokenize import SpaceTokenizer
from nltk.tokenize import RegexpTokenizer
data = ""Vous √™tes au volant d'une voiture et vous roulez √† vitesse""
#wst = WhitespaceTokenizer()
#tokenizer = RegexpTokenizer('\s+', gaps=True)
token=WhitespaceTokenizer().tokenize(data)
print token
</code></pre>

<p>Output i got </p>

<pre><code>['Vous', '\xeates', 'au', 'volant', ""d'une"", 'voiture', 'et', 'vous', 'roulez', '\xe0', 'vitesse']
</code></pre>

<p>Desired output</p>

<pre><code>['Vous', '√™tes', 'au', 'volant', ""d'une"", 'voiture', 'et', 'vous', 'roulez', '√†', 'vitesse']
</code></pre>
",Multilingual Language Processing & Language Identification,tokenizing french using nltk trying tokenize french word tokenize french word word contain symbol return xe following code implemented output got desired output
MaltParser 1.5 PatternSyntaxException,"<p>I need to work with MaltParser 1.5 version for parsing russian sentences.
So, when I'm trying to type in cmd next command:</p>

<pre><code>java -jar malt.jar 
</code></pre>

<p>I'm getting next exception:</p>

<pre><code>C:\malt1.5&gt;java -jar malt.jar
Exception in thread ""main"" java.lang.ExceptionInInitializerError
        at org.maltparser.MaltConsoleEngine.&lt;init&gt;(MaltConsoleEngine.java:29)
        at org.maltparser.Malt.main(Malt.java:17)
Caused by: java.util.regex.PatternSyntaxException: Unclosed character class near index 17
^.*malt[^\]*\.jar$
                 ^
        at java.util.regex.Pattern.error(Unknown Source)
        at java.util.regex.Pattern.clazz(Unknown Source)
        at java.util.regex.Pattern.sequence(Unknown Source)
        at java.util.regex.Pattern.expr(Unknown Source)
        at java.util.regex.Pattern.compile(Unknown Source)
        at java.util.regex.Pattern.&lt;init&gt;(Unknown Source)
        at java.util.regex.Pattern.compile(Unknown Source)
        at org.maltparser.core.helper.SystemInfo.&lt;init&gt;(SystemInfo.java:28)
        at org.maltparser.core.helper.SystemInfo.&lt;clinit&gt;(SystemInfo.java:21)
        ... 2 more
</code></pre>

<p>I've tried to run this command whith JRE 1.5 and 1.8.45 - same result.</p>
",Multilingual Language Processing & Language Identification,maltparser patternsyntaxexception need work maltparser version parsing russian sentence trying type cmd next command getting next exception tried run command whith jre result
Text Tagging in Natural Language Processing,"<p>I have the following project where I need to tag news items with company names to which these news items are relevant to (company names are mentioned in the news items and in many cases, in the headline of the news item). </p>

<p>For example: I have about 2000 news items (in XML format) tagged with company names and their relevance level (High/Low) to the story [this was done manually]. For each news item, I have the following fields: </p>

<p><strong>story_ID, Headline; story_Text; company_name; relevance_level(H/L)</strong></p>

<p>with the last two fields are put in manually.</p>

<p>I need to automate this tagging procedure i,e I need to tag an incoming news items with company names and their relevance with High(H)/Low(L).</p>

<p>Note: </p>

<ol>
<li><p>some of the news items are not relevant to any company and so these are not tagged.</p></li>
<li><p>some of the news items are relevant to multiple companies and so these are tagged with multiple company names and their corresponding relevance level.</p></li>
</ol>

<p>I am wondering what machine learning algorithms we can use. I am very new to Natural Language Processing. So I am not able to get a handle on how to go about solving the problem. I understand I need to use Multi-label/multi-class classification but I have never had to use multi-label classification.</p>

<p>Any help would be greatly appreciated. </p>

<p>Thank you.</p>
",Multilingual Language Processing & Language Identification,text tagging natural language processing following project need tag news item company name news item relevant company name mentioned news item many case headline news item example news item xml format tagged company name relevance level high low story wa done manually news item following field story id headline story text company name relevance level h l last two field put manually need automate tagging procedure e need tag incoming news item company name relevance high h low l note news item relevant company tagged news item relevant multiple company tagged multiple company name corresponding relevance level wondering machine learning algorithm use new natural language processing able get handle go solving problem understand need use multi label multi class classification never use multi label classification help would greatly appreciated thank
Algorithm for Determining Word Type using WordNet Database,"<p>I'm working on a project which requires scanning through paragraphs of natural text in English and detecting what type of word they are. The application works with AJAX, PHP, and MySQL.</p>

<p>My application doesn't need to be 100% accurate and simply tries to find the best content that matches text input. To do this I've used an SQL version of the WordNet database which allows me to search for words and their types as so using the <code>dict</code> view.</p>

<pre><code>SELECT lemma, pos FROM dict WHERE lemma = 'fool' ORDER BY lemma;
</code></pre>

<p>The above is an example of what the database sees but my PHP actually creates dynamic bound parameters based on the text from the AJAX calls and in reality, will contain many keywords.</p>

<p>This will return an array of records with each word searched for and their type.</p>

<p>My problem however is that most words can be multiple types, for example, with the fool example, it brings back three as a noun, and four as a verb. The minute differences aren't needed for me but I would like to know if the word is a noun or a verb in it's usage.</p>

<p>This problem persists across most words which means I cannot accurately detect different types of words because it could be any of the uses.</p>

<p>I am wondering if anybody could point me in the right direction of an algorithm or what I may be able to do in order to give at the very least a best guess at what the word type is.</p>

<p>The ones most important to get right are adjectives and nouns.</p>
",Multilingual Language Processing & Language Identification,algorithm determining word type using wordnet database working project requires scanning paragraph natural text english detecting type word application work ajax php mysql application need accurate simply try find best content match text input used sql version wordnet database allows search word type using view example database see php actually creates dynamic bound parameter based text ajax call reality contain many keywords return array record word searched type problem however word multiple type example fool example brings back three noun four verb minute difference needed would like know word noun verb usage problem persists across word mean accurately detect different type word could us wondering anybody could point right direction algorithm may able order give least best guess word type one important get right adjective noun
How to get html page language in node.js?,"<p>How do I get which language html page is written in with nodejs? I don't care about 100% accuracy, just something simple to tell if it's English, Russian, German etc.</p>
",Multilingual Language Processing & Language Identification,get html page language node j get language html page written nodejs care accuracy something simple tell english russian german etc
Are there any Lucene stemmers that handle Shakespearean English?,"<p>I'm trying to index some old documents for searching -- 16th, 17th, 18th century.</p>

<p>Modern stemmers don't seem to handle the antiquated word endings: worketh, liveth, walketh.</p>

<p>Are there stemmers that specialize in the English from the time of Shakespeare and the King James Bible? I'm currently using <code>solr.PorterStemFilterFactory</code>.</p>
",Multilingual Language Processing & Language Identification,lucene stemmer handle shakespearean english trying index old document searching th th th century modern stemmer seem handle antiquated word ending worketh liveth walketh stemmer specialize english time shakespeare king james bible currently using
Debugging Neural Network for (Natural Language) Tagging,"<p>I've been coding a Neural Network for recognizing and tagging parts of speech in English (written in Java). The code itself has no 'errors' or apparent flaws. Nevertheless, it is not learning -- the more I train it does not change its ability to predict the testing data. The following is information about what I've done, please ask me to update this post if I left something important out.</p>

<p>I wrote the neural network and tested it on several different problems to make sure that the network itself worked. I trained it to learn how to double numbers, XOR, cube numbers, and learn the sin function to a decent accuracy. So, I'm fairly confident that the actual algorithm is working.</p>

<p>The network using using the sigmoid activation function. The learning rate is .3, Momentum is .6. The weights are initialized to rng.nextFloat() -.5) * 4</p>

<p>I then got the Brown Corpus data-set and simplified the tagset to 'universal' with NLTK. I used NLTK for generating and saving all the corpus and dictionary data. I cut the last 15,000 sentences out of the corpus for testing purposes. I used the rest of the corpus (about 40,000 sentences of tagged words) for training. </p>

<p>The neural network layout is as follows: There is an input neuron for each Tag. Output Layer: There is one output neuron for each tag. The network is taking inputs for 3 words: first: the word coming before the word we want to tag, second: the word that needs to be tagged, third: the word that follows the second word. So, total number of inputs are 3x(total number of possible tags). The input values are numbers between 0 and 1. Each of the 3 words being fed into the input layer is searched for in a dictionary (made up by the 40,000 corpus, the same corpus that is used for training). The dictionary holds the number of times that each word has been tagged in the corpus as what part of speech. </p>

<blockquote>
  <p>For instance, the word 'cover' is tagged as a noun 1 time and a verb 3
  times.</p>
</blockquote>

<p>Percentages of being tagged are computed for each part of speech that the word is associated as, and this is what is fed into the network for that particular word. So, the input neuron designated as NOUN would receive .33 and VERB would receive .66. The other input neurons that hold tags for that word receive an input of 0.0. This is done for each of the 3 words to be inputted. If a word is the first word of a sentence, the first group of tags are all 0. If a word is the last word of a sentence, the final group of input neurons that hold the tag probabilities for the following word are left as 0s.
I've been using 10 hidden nodes (I've read a number of papers and this seems to be a good place to start testing with)</p>

<p>None of the 15,000 testing sentences were used to make the 'dictionary.' So, when testing the network with this partial corpus there will be some words the network has never seen. Words that are not recognized have their suffix stripped, and their suffix is searched for in another 'dictionary.' Whatever is most probable for that word is then used as inputs for it.</p>

<p>This is my set-up, and I started trying to train the network. I've been training the network with all 40,000 sentences. 1 epoch = 1 forward and backpropagation of every word in each sentence of the 40,000 training-set. So, just doing 1 epoch takes quite a few seconds. Just by knowing the word probabilities the network did pretty well, but the more I train it, nothing happens. The numbers that follow the epochs are the number of correctly tagged words divided by the total number of words.</p>

<p>First run 50 epochs: 0.928218786</p>

<p>100 epochs:        0.933130661</p>

<p>500 epochs:       0.928614499 took around 30 minutes to train this                   </p>

<p>Tried 10 epochs:         0.928953683 </p>

<p>Using only 1 epoch had results that pretty much varied between .92 and .93</p>

<p>So, it doesn't appear to be working...</p>

<p>I then took 55 sentences from the corpus and used the same dictionary that had probabilities for all 40,000 words. For this one, I trained it in the same way I trained my XOR -- I only used those 55 sentences and I only tested the trained network weights on those 55 sentences. The network was able to learn those 55 sentences quite easily. With 120 epochs (taking a couple seconds) the network went from tagging 3768 incorrectly and 56 correctly (on the first few epochs) to tagging 3772 correctly and 52 incorrectly on the 120th epoch. </p>

<p>This is where I'm at, I've been trying to debug this for over a day now, and haven't figured anything out.</p>
",Multilingual Language Processing & Language Identification,debugging neural network natural language tagging coding neural network recognizing tagging part speech english written java code ha error apparent flaw nevertheless learning train doe change ability predict testing data following information done please ask update post left something important wrote neural network tested several different problem make sure network worked trained learn double number xor cube number learn sin function decent accuracy fairly confident actual algorithm working network using using sigmoid activation function learning rate momentum weight initialized rng nextfloat got brown corpus data set simplified tagset universal nltk used nltk generating saving corpus dictionary data cut last sentence corpus testing purpose used rest corpus sentence tagged word training neural network layout follows input neuron tag output layer one output neuron tag network taking input word first word coming word want tag second word need tagged third word follows second word total number input x total number possible tag input value number word fed input layer searched dictionary made corpus corpus used training dictionary hold number time word ha tagged corpus part speech instance word cover tagged noun time verb time percentage tagged computed part speech word associated fed network particular word input neuron designated noun would receive verb would receive input neuron hold tag word receive input done word inputted word first word sentence first group tag word last word sentence final group input neuron hold tag probability following word left using hidden node read number paper seems good place start testing none testing sentence used make dictionary testing network partial corpus word network ha never seen word recognized suffix stripped suffix searched another dictionary whatever probable word used input set started trying train network training network sentence epoch forward backpropagation every word sentence training set epoch take quite second knowing word probability network pretty well train nothing happens number follow epoch number correctly tagged word divided total number word first run epoch epoch epoch took around minute train tried epoch using epoch result pretty much varied appear working took sentence corpus used dictionary probability word one trained way trained xor used sentence tested trained network weight sentence network wa able learn sentence quite easily epoch taking couple second network went tagging incorrectly correctly first epoch tagging correctly incorrectly th epoch trying debug day figured anything
What are dictionary based features in Chinese Word Segmentation in core NLP,"<p>I see dictionary based features in Chinese Word Segmentation in Core NLP but there is no explanation on what they are or what they signify. 
I traced the code to see where it is calculated and I see it is in ""Sighan2005DocumentReaderAndWriter.java"" file in function ""addDictionaryFeatures"". I still don't understand the details of it. There are no comments in the code or in the file.
It would be great help if someone could explain what is going on in this function or point me to some place which does. </p>
",Multilingual Language Processing & Language Identification,dictionary based feature chinese word segmentation core nlp see dictionary based feature chinese word segmentation core nlp explanation signify traced code see calculated see sighan documentreaderandwriter java file function adddictionaryfeatures still understand detail comment code file would great help someone could explain going function point place doe
kernelized methods in natural language processing,"<p>I am new to NLP. I want to implement a matching approach for short sentences( e.g. questions in cQA). I want to use tree kernel function as a syntactic feature. I am wondering to know is there any implementation available in NLP tools, or elsewhere? </p>

<p>Specifically, I like a method like this paper Collins and Duffy, Convolution kernels for natural language processing.</p>

<p>Any suggestion is useful and appreciated.</p>
",Multilingual Language Processing & Language Identification,kernelized method natural language processing new nlp want implement matching approach short sentence e g question cqa want use tree kernel function syntactic feature wondering know implementation available nlp tool elsewhere specifically like method like paper collins duffy convolution kernel natural language processing suggestion useful appreciated
Is there a formal grammar for english language?,"<p>I'm browsing web searching for english language grammar, but i found only few simple examples like:</p>

<pre><code>s -&gt; np vp
np -&gt; det n
vp -&gt; v | v np
det -&gt; 'a' | 'the'
n -&gt; 'woman' | 'man'
v -&gt; 'shoots' 
</code></pre>

<p>Maybe I don't realise how big this problem is because i tought that grammar has been formalised. Can somebody provide me a source for some expanded formal english grammar?</p>
",Multilingual Language Processing & Language Identification,formal grammar english language browsing web searching english language grammar found simple example like maybe realise big problem tought grammar ha formalised somebody provide source expanded formal english grammar
Chinese sentence segmenter with Stanford coreNLP,"<p>I'm using the Stanford coreNLP system with the following command:</p>

<pre><code>java -cp stanford-corenlp-3.5.2.jar:stanford-chinese-corenlp-2015-04-20-models.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -annotators segment,ssplit -file input.txt
</code></pre>

<p>And this is working great on small chinese texts. However, I need to train a MT system which just requires me to segment my input. So I just need to use <code>-annotators segment</code>, but with this parameters the system outputs an empty file. I could run the tool using the <code>ssplit</code> annotator as well but I don't want to do that because my input is a parallel corpora that contains one sentence by line already, and the ssplit will probably not split sentences perfectly and create problems in the parallel data.</p>

<p>Is there a way to tell the system to do the segmentation only, or to tell it that the input already contains a sentence by line exactly?</p>
",Multilingual Language Processing & Language Identification,chinese sentence segmenter stanford corenlp using stanford corenlp system following command working great small chinese text however need train mt system requires segment input need use parameter system output empty file could run tool using annotator well want input parallel corpus contains one sentence line already ssplit probably split sentence perfectly create problem parallel data way tell system segmentation tell input already contains sentence line exactly
How to detect features of a product in an english sentence - nlp,"<p>I am trying to detect features(eg.: screen, processing speed) of a product(eg.: mobile, respectively) in an english sentence. For this, my approach is that in a paragraph(that talks about the product) containing multiple sentences, the words( apart from words like pronouns or sentiment words like good, bad etc, which I store in a file) that appear most frequently are the features of that product and so I rank on the basis of their frequency and their distance with the sentiment words and take teh top n of them.</p>

<p>However, it is not very effective. Can anyone suggest some other and better approach for detecting the words which are features of a product?</p>
",Multilingual Language Processing & Language Identification,detect feature product english sentence nlp trying detect feature eg screen processing speed product eg mobile respectively english sentence approach paragraph talk product containing multiple sentence word apart word like pronoun sentiment word like good bad etc store file appear frequently feature product rank basis frequency distance sentiment word take teh top n however effective anyone suggest better approach detecting word feature product
Is there any kind of statistical natural language processing library for Haskell?,"<p>Currently I'm reading Natural Language Processing for the Working Programmer (a work in progress book <a href=""http://nlpwp.org/"" rel=""noreferrer"">http://nlpwp.org/</a>) and wondering if there is a decent library for statistical natural language processing tasks.</p>
",Multilingual Language Processing & Language Identification,kind statistical natural language processing library haskell currently reading natural language processing working programmer work progress book wondering decent library statistical natural language processing task
List of all questions along with tags on StackOverflow (for NLP tasks),"<p>Since StackOverflow comes with a wealth of questions and user-contributed tags, I am looking at it as an interesting, richly annotated, text corpus for NLP (natural language processing) tasks.</p>

<p>Basically, I want to automatically <strong>predict question tags based on the questions body</strong>. I am sure this can be done to a certain extend, and there's a number of nice use cases, such as tag suggestions (e.g. to make tag usage more consistent), to name just one.</p>

<p>For this I would need a lot - or even better: - all questions along with their body text and user tags <strong>to train a tag predicter</strong> with machine learning algorithms.</p>

<p>I know there's the StackOverflow API, but the amount of data I can fetch through it seems to be very limited - for good reasons of course.</p>

<p><strong>So the question is</strong>: Is there a way to fetch/download all questions along with their user-tags from StackOverflow?</p>
",Multilingual Language Processing & Language Identification,list question along tag stackoverflow nlp task since stackoverflow come wealth question user contributed tag looking interesting richly annotated text corpus nlp natural language processing task basically want automatically predict question tag based question body sure done certain extend number nice use case tag suggestion e g make tag usage consistent name one would need lot even better question along body text user tag train tag predicter machine learning algorithm know stackoverflow api amount data fetch seems limited good reason course question way fetch download question along user tag stackoverflow
German Stemmers in RTextTools,"<p>I am trying to use the German stemmer that comes with RTextTools but the results I get are quite off the mark.</p>

<p>Say, I have the following vector:</p>

<pre><code>v &lt;- c(""gro√ü"", ""gr√∂√üer"", ""am"", ""gr√∂√üten"", ""√§hnlicher"")
</code></pre>

<p>Using</p>

<pre><code>library(RTextTools)
wordStem(v, ""german"")
</code></pre>

<p>I get</p>

<pre><code>[1] ""gro√ü""    ""gr√∂√üer""  ""am""      ""gr√∂√üten"" ""√§hnlich""
</code></pre>

<p>What am I missing??</p>
",Multilingual Language Processing & Language Identification,german stemmer rtexttools trying use german stemmer come rtexttools result get quite mark say following vector using get missing
convert my text file that contains the words and their frequencies to arff file suitable for weka,"<p>I have 4 text files used to represent economy,politics,health,and sport categories.Each file contains 400 Arabic words and the frequency of each word which used to represent each category.</p>

<p>ex: health.txt contains</p>

<p>ÿßÿµÿßÿ®ÿ© 113</p>

<p>6 ÿ∫ÿ∞ÿßÿ¶Ÿäÿ©</p>

<p>6 ÿ∑ÿ®ŸäÿπŸä
.
.</p>

<p>I used Simple CI to create arff. the output arff file is as the following:
@relation C__finaloutput</p>

<p>@attribute text string</p>

<p>@attribute @@class@@ {economy,health,politics,sport}</p>

<p>@data</p>

<p>'ÿ•ÿµÿßÿ®ÿ© 113\r\nÿ∫ÿ∞ÿßÿ¶Ÿäÿ© 6\r\nÿ∑ÿ®ŸäÿπŸä 6\r\nŸÖÿ±Ÿäÿ∂ÿß 6\r\n',health</p>

<p>.</p>

<p>.</p>

<p>problems are:1.how weka will recognize the number in arff file as the frequency of each word?</p>

<p>2.how to use SMO classifier or other classifiers like j48 which not handle string attributes?</p>
",Multilingual Language Processing & Language Identification,convert text file contains word frequency arff file suitable weka text file used represent economy politics health sport category file contains arabic word frequency word used represent category ex health txt contains used simple ci create arff output arff file following relation c finaloutput attribute text string attribute class economy health politics sport data r n r n r n r n health problem weka recognize number arff file frequency word use smo classifier classifier like j handle string attribute
NLP: Arrange words with tags into proper English sentence?,"<p>lets say I have a sentence:</p>

<pre><code>""you hello how are ?""
</code></pre>

<p>I get output of:</p>

<p><code>you_PRP hello_VBP how_WRB are_VBP</code></p>

<p>What is best way to arrange the wording into proper English sentence like: <code>Hello how are you ?</code></p>

<p>I am new to this whole natural language processing so I am unfamiliar with many terms. 
The only way I can think of on top of my head is - Using statements to determine:
<code>adverb - verb - noun</code> and then re-arrange them based on that? </p>

<p>Note: Lets assume I am trying to form proper question, so ignore determining if it's a question or a statement. </p>
",Multilingual Language Processing & Language Identification,nlp arrange word tag proper english sentence let say sentence get output best way arrange wording proper english sentence like new whole natural language processing unfamiliar many term way think top head using statement determine arrange based note let assume trying form proper question ignore determining question statement
Querying database based on natural language input,"<p>I am looking to build a system where I can ask my database questions in a natural language format and it returns the appropriate result. For example:</p>

<pre><code>Database contains winners of all Football world cups.
My Search Query: Who was the winner in 2014
Result: Germany
</code></pre>

<p>Basically I want a way of understanding that what my query essentially meant was winner = 2014 and I ask the database to return the rows where winner = 2014.</p>

<p>What is the most efficient way of doing this. This was just an example but I need a more dynamic way of doing this. I also want to know what databases support Natural Language Processing as this looks like a classic NLP problem. How should I be approaching this problem. Which databases support such NLP querying? I don't have any experience in NLP so I am really looking for leads more than anything else. Is ElasticSearch/Solr/Lucene something that I should be looking into? Would really appreciate any kind of input.</p>
",Multilingual Language Processing & Language Identification,querying database based natural language input looking build system ask database question natural language format return appropriate result example basically want way understanding query essentially meant wa winner ask database return row winner efficient way wa example need dynamic way also want know database support natural language processing look like classic nlp problem approaching problem database support nlp querying experience nlp really looking lead anything else elasticsearch solr lucene something looking would really appreciate kind input
Data structure / data model for multi-language phrasebook,"<p>We want create a multi-language phrasebook / dictionary for a specific 
area. </p>

<p>And now I'm thinking about the best data structure / data model for that.</p>

<p>Since it should be more phrasebook than dictionary we want to keep the data model / structure first simple. It should be only used for fast translation: i.e. user selects two languages, types a word and gets translation. The article and description parts are just for displaying, not for search. </p>

<p>There are some specific cases I'm thniking about:</p>

<ul>
<li>One term can be expressed with several (1..n) words in any language </li>
<li>Any term can also be translated into several (1..m) words in another language</li>
<li>In some languages the word's articel could be important to know</li>
<li>For some words description could be important (e.g. for words from dialects etc.)</li>
</ul>

<p>I'm not sure about one point: do I reinvent the wheel creating a data model by myself? But I couldn't find any solutions.</p>

<p>I've just created a json data model I'm not sure about if it good enough or not:</p>

<pre><code>[
    {
        wordgroup-id: 1,
        en: [
                {word: 'car', plural: 'cars'},
                {word: 'auto', plural: 'autos'},
                {word: 'vehicle', plural: 'vehicles'},
            ],
        de: [
                {word: 'Auto', article: 'das', description: 'Some explanation eg. when to use this word', plural: 'Autos'},
                {word: 'Fahrzeug', article: 'das', plural: 'Fahrzeuge'}
            ],
        ru: [...],
        ...
    },
    {
        wordgroup-id: 2,
        ...
    },
    ...
]
</code></pre>

<p>I also thought about some ""corner"" cases @triplee wrote about. I thought to solve them with some kind of redundance. Only the word group id and the word within a language should be unique.</p>

<p>I would be very thankfull for any feedback to the first draft of the data model.</p>
",Multilingual Language Processing & Language Identification,data structure data model multi language phrasebook want create multi language phrasebook dictionary specific area thinking best data structure data model since phrasebook dictionary want keep data model structure first simple used fast translation e user selects two language type word get translation article description part displaying search specific case thniking one term expressed several n word language term also translated several word another language language word articel could important know word description could important e g word dialect etc sure one point reinvent wheel creating data model find solution created json data model sure good enough also thought corner case triplee wrote thought solve kind redundance word group id word within language unique would thankfull feedback first draft data model
Stanford NLP: Chinese Part of Speech labels?,"<p>I am trying to find a table explaining each label in the Chinese part-of-speech tagger for the 2015.1.30 version.  I couldn't find anything on this topic.  The closest thing I could find was in the ""Morphological features help POS tagging of unknown words across language varieties"" article, but it doesn't explain what VC represent.  I would love to get an updated list.</p>
",Multilingual Language Processing & Language Identification,stanford nlp chinese part speech label trying find table explaining label chinese part speech tagger version find anything topic closest thing could find wa morphological feature help po tagging unknown word across language variety article explain vc represent would love get updated list
Way to dump the relations from Freebase?,"<p>I have ran through the Google API for Freebase, but still confusing.
Is there simple way to dump the relations from Freebase?</p>

<blockquote>
  <p>I want to dump <strong>all entity-name-pair</strong> with a specific relation (e.g. marry_with,  ...), and also want the chinese entity names.</p>
</blockquote>

<p>Should I </p>

<ul>
<li>write MQL to query all entity satisfying the condition? (but the MQL service is going to be retired recently. )</li>
<li>or dump all freebase and parse? </li>
<li>or is there other API capable of doing this?</li>
<li>or other KB (YAGO, DBpedia, wikidata) is more easier of doing this?</li>
</ul>

<p>Which way is easier to work out.
Please shed me some direction . thanks</p>
",Multilingual Language Processing & Language Identification,way dump relation freebase ran google api freebase still confusing simple way dump relation freebase want dump entity name pair specific relation e g marry also want chinese entity name write mql query entity satisfying condition mql service going retired recently dump freebase parse api capable kb yago dbpedia wikidata easier way easier work please shed direction thanks
determination of human language from text:: system structure,"<p>I'm using <a href=""http://www.winedt.org/Dict/"" rel=""nofollow"">these word lists</a>.</p>

<p>Right now I'm only thinking about German, Russian, English, and French. </p>

<p>I guess what I'm going to do is put them all as part of a hashmap, one for each language with the word as the key, and a boolean as the value. </p>

<p>When I get an input text I'll search over all the lists and whichever has the most hits will be returned as the answer. </p>

<p>Maybe I'll try to use multi-threading and search each of the dictionaries simultaneously using a different thread. </p>

<p>Is that a good solution to this problem?</p>
",Multilingual Language Processing & Language Identification,determination human language text system structure using word list right thinking german russian english french guess going put part hashmap one language word key boolean value get input text search list whichever ha hit returned answer maybe try use multi threading search dictionary simultaneously using different thread good solution problem
Mecab output - list of name types,"<p>A sample output from meecab:</p>

<pre><code>„Å´   „Éã   „Éã   „Å´   Âä©Ë©û-Ê†ºÂä©Ë©û      
</code></pre>

<p>We have <code>Âä©Ë©û</code>(particle) as the type and <code>Ê†ºÂä©Ë©û</code> (case-marking particle) as the PoS. Where can I find a list of all possible types and PoS's that mecab uses? I want to be able to map the Japanese to a translated set, without the need to translate it on the fly.</p>
",Multilingual Language Processing & Language Identification,mecab output list name type sample output meecab particle type case marking particle po find list possible type po mecab us want able map japanese translated set without need translate fly
Natural Language Processing Libraries,"<p>I'm having a hard time figuring out what library and datasets go together.</p>

<p>Toolkits / Libraries I've found:</p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">CoreNLP</a> - Java</li>
<li><a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a> - Python</li>
<li><a href=""https://opennlp.apache.org/"" rel=""nofollow"">OpenNLP</a> - Java</li>
<li><a href=""https://github.com/clir/clearnlp"" rel=""nofollow"">ClearNLP</a> - Java</li>
</ul>

<p>Out of all of these, some are missing features. For example OpenNLP didn't have a dependency parsing.</p>

<p>I need to find a library that's quick that will also do dependency parsing and part of speech tagging.</p>

<p>The next hurdle is where do we get data sets. I've found a lot of things out there, but nothing full and comprehensive.</p>

<p>Data I've found:</p>

<ul>
<li><a href=""http://www.nltk.org/nltk_data/"" rel=""nofollow"">NLTK Corpora</a></li>
<li><a href=""https://catalog.ldc.upenn.edu/LDC2012T13"" rel=""nofollow"">English Web Treebank</a> (looks to be the best but is paid)</li>
<li><a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">OpenNLP</a></li>
<li><a href=""https://www.cis.upenn.edu/~treebank/"" rel=""nofollow"">Penn Treebank</a></li>
</ul>

<p>I'm confused as to what data sets I need for what features and what's actually available publicly. From my research is seems ClearNLP will work best for but has very little data.</p>

<p>Thank you</p>
",Multilingual Language Processing & Language Identification,natural language processing library hard time figuring library datasets go together toolkits library found corenlp java nltk python opennlp java clearnlp java missing feature example opennlp dependency parsing need find library quick also dependency parsing part speech tagging next hurdle get data set found lot thing nothing full comprehensive data found nltk corpus english web treebank look best paid opennlp penn treebank confused data set need feature actually available publicly research seems clearnlp work best ha little data thank
Removing /**/ from html using python,"<p>This is the code I am trying to skip which is in Multiple Lines.
    <pre></p>

<blockquote>
  <p>/****/<br>
      if (window.bbcdotcom &amp;&amp; bbcdotcom.slot) {
      bbcdotcom.slot('mpu', [1,2,3]);
      }
      /**/
      </pre>
      I tried BeautifulSoup, My Best of Knowledge and also regex but am not able to get it to work properly. I am beginner in python &amp; trying to get some content using RSS Feed &amp; perform Natural Language Processing.</p>
</blockquote>
",Multilingual Language Processing & Language Identification,removing html using python code trying skip multiple line window bbcdotcom bbcdotcom slot bbcdotcom slot mpu tried beautifulsoup best knowledge also regex able get work properly beginner python trying get content using r feed perform natural language processing
How to find if two or more texts are correlated,"<p>Say we have two english texts T1 and T2 (disjoint no overlap ) cut out from a larger text T. Is there a way to figure out that T1 and T2 came from a single larger text?  </p>

<p>For example , newspaper layout is in multi column text. Given two column of text from any where in newspaper , I want to find out whether these two column of text came from the same story.</p>
",Multilingual Language Processing & Language Identification,find two text correlated say two english text disjoint overlap cut larger text way figure came single larger text example newspaper layout multi column text given two column text newspaper want find whether two column text came story
programmatically access IME,"<p>Is there a way to access Japanese or chinese IME either from the command line or python? I have Linux/osx/win8 boxes, so which ever system exposes the easiest accessible api is fine. </p>

<p>I'm experimenting with building a Japanese kana-kanji conversion algorithm and would like to establish a baseline using existing tools. I also have some collections of kana I would like to process.</p>

<p>Preferably I would like something along the lines of </p>

<pre><code>$ ime JP ""„Åç„Åó„ÇÉ„ÅÆ„Åç„Åó„ÇÉ„Åå„Åç„Åó„ÇÉ„Åß„Åç„Åó„ÇÉ„Åó„Åü""
Ë≤¥Á§æ„ÅÆË®òËÄÖ„ÅåÊ±ΩËªä„ÅßÂ∏∞Á§æ„Åó„Åü
</code></pre>

<p>I've looked at anthy, mozc and dbus on Linux but can't find anyway to interact with them via the terminal or scripting (such as python)</p>
",Multilingual Language Processing & Language Identification,programmatically access ime way access japanese chinese ime either command line python linux osx win box ever system expose easiest accessible api fine experimenting building japanese kana kanji conversion algorithm would like establish baseline using existing tool also collection kana would like process preferably would like something along line looked anthy mozc dbus linux find anyway interact via terminal scripting python
Convert nl string to vector or some numeric equivalent,"<p>I'm trying to convert a string to a numeric equivalent so I can train a neural-network to classify the strings. I tried the sum of the ascii values, but that just results in larger numbers vs smaller numbers.</p>

<p>For example, I could have a short string in german and it puts it into the english class because the english words that it has been trained with are short and numerically small.</p>

<p>I was looking into Google's word2vec, which seems like it should work. But I want to do this on the client-side. And I found a <a href=""https://github.com/Planeshifter/node-word2vec"" rel=""nofollow"">node.js implementation, here</a>, but that just runs the command-line tool. </p>

<p>How can I convert a string to something numeric, a vector perhaps in js?</p>
",Multilingual Language Processing & Language Identification,convert nl string vector numeric equivalent trying convert string numeric equivalent train neural network classify string tried sum ascii value result larger number v smaller number example could short string german put english class english word ha trained short numerically small wa looking google word vec seems like work want client side found node j implementation run command line tool convert string something numeric vector perhaps j
algorithms in natural language processing,"<p>Im doing a project which could detect users emotions by text. Im using my own algorithm to do this from java.Im not sure whether this algorithm is good or bad.What are the weaknesses in this algorithm?</p>

<p>Here is what im doing.</p>

<p>1) Add pos tagging to the text.</p>

<p>2)get the sentiment of the text.</p>

<p>3) Extract the adjectives from the sentence and check in the database for each adjective. If it found a word add points to the found emotion.
at last get the emotion with the highest points.</p>
",Multilingual Language Processing & Language Identification,algorithm natural language processing im project could detect user emotion text im using algorithm java im sure whether algorithm good bad weakness algorithm im add po tagging text get sentiment text extract adjective sentence check database adjective found word add point found emotion last get emotion highest point
Principle of NLP algorithm,"<p>I'm new to this really useful Q&amp;A website and I'm not really good in English, so sorry about that.</p>

<p>I was interested in a web project that I think is not hard to do and it is a simplified surfing. </p>

<p><a href=""http://us.generation-nt.com/algorithm-optimization-search-internet-news-3313271.html"" rel=""nofollow"">Algorithm description 1</a><br>
<a href=""http://www.kurzweilai.net/forums/topic/olds-16yr-summary-genetic-algorithm-for-data-mining-beats-pros"" rel=""nofollow"">Algorithm description 2</a></p>

<p>This algorithm is made ‚Äã‚Äãsure the kid is very simple because it quickly analyzes web content and find relevant information.</p>

<p>Can someone tell me how this algorithm functioning that I tried to make something similar?</p>

<p>On what principles funcionise this algorithm?</p>

<p>THANKS!</p>
",Multilingual Language Processing & Language Identification,principle nlp algorithm new really useful q website really good english sorry wa interested web project think hard simplified surfing algorithm description algorithm description algorithm made sure kid simple quickly analyzes web content find relevant information someone tell algorithm functioning tried make something similar principle funcionise algorithm thanks
Techniques for categorising natural language strings?,"<p>What is available in terms of libraries / open-source software for processing and categorising natural language? I've got a database full of strings which are user descriptions of a particular item. I'd like to categorise these words to weed out the useless and make an educated guess as to what category the item fits into (e.g Technology, Sport, Music).</p>

<p>I realise this a fairly specific request and my knowledge of natural language processing is very limited. I'm wondering what would be the best and if possible most computationally cheap way of making these sort of predictions? </p>

<p>I would prefer to do this in Ruby however, Python or Java is also acceptable.</p>
",Multilingual Language Processing & Language Identification,technique categorising natural language string available term library open source software processing categorising natural language got database full string user description particular item like categorise word weed useless make educated guess category item fit e g technology sport music realise fairly specific request knowledge natural language processing limited wondering would best possible computationally cheap way making sort prediction would prefer ruby however python java also acceptable
Natural English language words,"<p>I need the most exhaustive English word list I can find for several types of language processing operations, but I could not find anything on the internet that has good enough quality.</p>

<p>There are 1,000,000 words in the English language including foreign and/or technical words. </p>

<p>Can you please suggest such a source (or close to 500k words) that can be downloaded from the internet that is maybe a bit categorized? What input do you use for your language processing applications?</p>
",Multilingual Language Processing & Language Identification,natural english language word need exhaustive english word list find several type language processing operation could find anything internet ha good enough quality word english language including foreign technical word please suggest source close k word downloaded internet maybe bit categorized input use language processing application
Python: Goslate translation request returns &quot;503: Service Unavailable&quot;,"<p>A few months ago, I used Python's <code>goslate</code> package to translate a bunch of French text to English. When I tried to do so this morning, though, the service returned an error:</p>

<pre><code>import goslate
gs = goslate.Goslate()
print gs.translate('hello world', 'de')

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""c:\Python27\lib\site-packages\goslate.py"", line 389, in translate
    return _unwrapper_single_element(self._translate_single_text(text, target_language, source_language))
  File ""c:\Python27\lib\site-packages\goslate.py"", line 317, in _translate_single_text
    results = list(self._execute(make_task(i) for i in split_text(text)))
  File ""c:\Python27\lib\site-packages\goslate.py"", line 200, in _execute
    yield each()
  File ""c:\Python27\lib\site-packages\goslate.py"", line 315, in &lt;lambda&gt;
    return lambda: self._basic_translate(text, target_language, source_lauguage)[0]
  File ""c:\Python27\lib\site-packages\goslate.py"", line 241, in _basic_translate
    response_content = self._open_url(url)
  File ""c:\Python27\lib\site-packages\goslate.py"", line 178, in _open_url
    response = self._opener.open(request, timeout=self._TIMEOUT)
  File ""c:\Python27\lib\urllib2.py"", line 437, in open
    response = meth(req, response)
  File ""c:\Python27\lib\urllib2.py"", line 550, in http_response
    'http', request, response, code, msg, hdrs)
  File ""c:\Python27\lib\urllib2.py"", line 469, in error
    result = self._call_chain(*args)
  File ""c:\Python27\lib\urllib2.py"", line 409, in _call_chain
    result = func(*args)
  File ""c:\Python27\lib\urllib2.py"", line 656, in http_error_302
    return self.parent.open(new, timeout=req.timeout)
  File ""c:\Python27\lib\urllib2.py"", line 437, in open
    response = meth(req, response)
  File ""c:\Python27\lib\urllib2.py"", line 550, in http_response
    'http', request, response, code, msg, hdrs)
  File ""c:\Python27\lib\urllib2.py"", line 475, in error
    return self._call_chain(*args)
  File ""c:\Python27\lib\urllib2.py"", line 409, in _call_chain
    result = func(*args)
  File ""c:\Python27\lib\urllib2.py"", line 558, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 503: Service Unavailable
</code></pre>

<p>Does anyone know what happened to <code>goslate</code>? If it's gone for good, are there decent alternatives to the <code>goslate</code> package for translating French to English via an API call?</p>
",Multilingual Language Processing & Language Identification,python goslate translation request return service unavailable month ago used python package translate bunch french text english tried morning though service returned error doe anyone know happened gone good decent alternative package translating french english via api call
Find semantically similar word for natural language processing,"<p>I am working on a natural language processing project in Java. I have a requirement where I want to identify words that belong to similar semantic groups. 
e.g. : if the words such as <code>study</code> , <code>university</code>, <code>graduate</code> , <code>attend</code> are found I want them to be categorized as being related to education.
If words such as <code>golfer</code>, <code>batsman</code>, <code>athlete</code> are found, it should categorize all under a parent like sportsperson.
Is there a way I can achieve this task without using and training approach. Is there some toll like WordNet that can be used directly? Any pointer would be greatly appreciated! 
Thanx cheers!! :-) </p>
",Multilingual Language Processing & Language Identification,find semantically similar word natural language processing working natural language processing project java requirement want identify word belong similar semantic group e g word found want categorized related education word found categorize parent like sportsperson way achieve task without using training approach toll like wordnet used directly pointer would greatly appreciated thanx cheer
Natural Language Processing - visual definition,"<p>May be a stupid thought/question.</p>

<p>Is there any computer branch/system in NLP, AI, or ML where I can define basic words using images. For example I link 'point', 'line', 'angel', or triangle' etc. with relevant 2d vector images. Instead of defining their definition in English again. Because, To explain below sentence to the system</p>

<blockquote>
  <p>This is triangle.</p>
</blockquote>

<p>I need to define: 'this', 'is', 'triangle'. Now each definition will have more words. I need to define all of them too and so on. It'll never end since I'll fall in loop of definitions.</p>

<p>Specially If I need to explain 'Left', 'Right'.</p>
",Multilingual Language Processing & Language Identification,natural language processing visual definition may stupid thought question computer branch system nlp ai ml define basic word using image example link point line angel triangle etc relevant vector image instead defining definition english explain sentence system triangle need define triangle definition word need define never end since fall loop definition specially need explain left right
how can i create my own model in Stanford Pos tagger?,"<p>I want to add new tagged words( local words that is used in our region ) and create a new model. I created a .prop file from command line but how can i create a .tagger file? </p>

<p>When i tried to create such file as mentioned on Stanford website it shows an error like </p>

<blockquote>
  <p>""No model specified""</p>
</blockquote>

<p>what is the -model argument, is it the corpus? how can i add my new tagged words into that?</p>

<p>How do I train a tagger, then?</p>

<p>The <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford site</a> says that:</p>

<blockquote>
  <p>You need to start with a .props file which contains options for the
  tagger to use.  The .props files we used to create the sample taggers
  are included in the models directory; you can start from whichever one
  seems closest to the language you want to tag. </p>
  
  <p>For example, to train a new English tagger, start with the left3words
  tagger props file. To train a tagger for a western language other than
  English, you can consider the props files for the German or the French
  taggers, which are included in the full distribution. For languages
  using a different character set, you can start from the Chinese or
  Arabic props files. Or you can use the -genprops option to
  MaxentTagger, and it will write a sample properties file, with
  documentation, for you to modify. It writes it to stdout, so you'll
  want to save it to some file by redirecting output (usually with >).
  The # at the start of the line makes things a comment, so you'll want
  to delete the # before properties you wish to specify.</p>
</blockquote>
",Multilingual Language Processing & Language Identification,create model stanford po tagger want add new tagged word local word used region create new model created prop file command line create tagger file tried create file mentioned stanford website show error like model specified model argument corpus add new tagged word train tagger stanford site say need start prop file contains option tagger use prop file used create sample tagger included model directory start whichever one seems closest language want tag example train new english tagger start left word tagger prop file train tagger western language english consider prop file german french tagger included full distribution language using different character set start chinese arabic prop file use genprops option maxenttagger write sample property file documentation modify writes stdout want save file redirecting output usually start line make thing comment want delete property wish specify
Best data structure to use for translation probabilities of two sets of words in Python,"<p>I'm building up probabilities <code>p(e | f)</code> that an english word <code>e</code> is the translation of a foreign word <code>f</code>. I'm using IBM model 1.</p>

<p>What data structure can I use for this in Python? I'm a complete Python beginner, but I can program fairly proficiently. I guess I'm looking for something like a nested dictionary, or a matrix with named columns and rows. </p>
",Multilingual Language Processing & Language Identification,best data structure use translation probability two set word python building probability english word translation foreign word using ibm model data structure use python complete python beginner program fairly proficiently guess looking something like nested dictionary matrix named column row
Database which has categorized the english words into matching emotion,"<p>is there a database or api which has the categorized version of English words into the matching emotion? </p>

<p>e.g: - <a href=""http://www.psychpage.com/learning/library/assess/feelings.html"" rel=""nofollow"">http://www.psychpage.com/learning/library/assess/feelings.html</a></p>
",Multilingual Language Processing & Language Identification,database ha categorized english word matching emotion database api ha categorized version english word matching emotion e g
How to remove english text from arabic string in python?,"<p>I have an Arabic string with English text and punctuations. I need to filter Arabic text and I tried removing punctuations and English words using sting. However, I lost the spacing between Arabic words. Where am I wrong?   </p>

<pre><code>import string
exclude = set(string.punctuation)

main_text = ""Ÿàÿ≤ÿßÿ±ÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©: ŸÑÿß ÿ™ÿ™ŸàŸÅÿ± ŸÑÿØŸäŸÜÿß ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ±ÿ≥ŸÖŸäÿ© ÿπŸÜ ÿ≥ÿπŸàÿØŸäŸäŸÜ ŸÖŸàŸÇŸàŸÅŸäŸÜ ŸÅŸä ŸÑŸäÿ®Ÿäÿß http://alriyadh.com/1031499""
main_text = ''.join(ch for ch in main_text if ch not in exclude)
[output after this step=""Ÿàÿ≤ÿßÿ±ÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ© ŸÑÿß ÿ™ÿ™ŸàŸÅÿ± ŸÑÿØŸäŸÜÿß ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ±ÿ≥ŸÖŸäÿ© ÿπŸÜ ÿ≥ÿπŸàÿØŸäŸäŸÜ ŸÖŸàŸÇŸàŸÅŸäŸÜ ŸÅŸä ŸÑŸäÿ®Ÿäÿß httpalriyadhcom1031499]""
n = filter(lambda x: x not in string.printable, n)
print n
Ÿàÿ≤ÿßÿ±ÿ©ÿßŸÑÿØÿßÿÆŸÑŸäÿ©ŸÑÿßÿ™ÿ™ŸàŸÅÿ±ŸÑÿØŸäŸÜÿßŸÖÿπŸÑŸàŸÖÿßÿ™ÿ±ÿ≥ŸÖŸäÿ©ÿπŸÜÿ≥ÿπŸàÿØŸäŸäŸÜŸÖŸàŸÇŸàŸÅŸäŸÜŸÅŸäŸÑŸäÿ®Ÿäÿß
</code></pre>

<p>I am able to remove punctuations and english text but I lost the space between words. How can I retain each words?</p>
",Multilingual Language Processing & Language Identification,remove english text arabic string python arabic string english text punctuation need filter arabic text tried removing punctuation english word using sting however lost spacing arabic word wrong able remove punctuation english text lost space word retain word
How to read a paragraph in natural language processing GATE,"<p>I am using GATE tool for natural language processing.. i am using java code to read lines from the sentence and get the keywords.. what modification has to be done in creole xml to read complete paragraph..</p>
",Multilingual Language Processing & Language Identification,read paragraph natural language processing gate using gate tool natural language processing using java code read line sentence get keywords modification ha done creole xml read complete paragraph
Similarity for arrays of parts of speech,"<p>K-nearest neighbor and natural language processing: How do you test the distance between arrays of parts of speech? eg</p>

<p>('verb','adverb','noun')  and ('adjective','adverb','pronoun')?</p>

<p>A better phrased question would be how do you tell the similarity between the two in the context that they are parts of speech and not just strings?</p>
",Multilingual Language Processing & Language Identification,similarity array part speech k nearest neighbor natural language processing test distance array part speech eg verb adverb noun adjective adverb pronoun better phrased question would tell similarity two context part speech string
answer type detection in NLP,"<p>I have searched so many research papers. but I did not find a good procedure to do this. </p>

<p>how to identify the question type and answer type detection in natural language processing with out using entity recognition?</p>
",Multilingual Language Processing & Language Identification,answer type detection nlp searched many research paper find good procedure identify question type answer type detection natural language processing using entity recognition
Split String When Words Joined Without Delimiter,"<p>We have quite a lot of text (mostly written in English) which was incorrectly imported (<em>from a source we have no control over</em>). For example</p>

<ol>
<li><code>configuredincorrectly</code> - into the 2 words <code>configured</code> &amp; <code>incorrectly</code></li>
<li><code>RegardsJohn Doe</code> - into a word <code>Regards</code> and a named entity <code>John Doe</code></li>
<li><code>To: person1@example.comCC:addr2@example.co.ukBCC:person3@example.sg</code> - into 3 tuples <code>(To,person1@example.com)</code>, <code>(CC,addr2@example.co.uk)</code>, <code>(BCC,person3@example.sg)</code></li>
<li><code>problem.Possible</code> - into the 2 words <code>problem</code> &amp; <code>possible</code></li>
</ol>

<p>I acknowledge that we are trying to address multiple problems here. It is tempting to write non-scalable code such as</p>

<ol>
<li>regular expressions each time we try to solve a particular <em>dirty text scenario</em>, </li>
<li>string.replace(keyword,keywordwithSpace)</li>
</ol>

<p>Could anyone please point me towards a (<em>partial</em>) solution for problems 1 &amp; 2? </p>

<p>A solution which made use of natural language understanding would be most ideal. 
We have ~ 1000 words in our vocabulary, such as [communication, database, hardware, network, problem, rectify, solution, etc.]. Is there a way we can ""train"" a model to recognize that words like <code>hardwarefailure</code> really mean 2 separate words <code>hardware</code> &amp; <code>failure</code>.</p>

<p>Many thanks in advance!</p>
",Multilingual Language Processing & Language Identification,split string word joined without delimiter quite lot text mostly written english wa incorrectly imported source control example word word named entity tuples word acknowledge trying address multiple problem tempting write non scalable code regular expression time try solve particular dirty text scenario string replace keyword keywordwithspace could anyone please point towards partial solution problem solution made use natural language understanding would ideal word vocabulary communication database hardware network problem rectify solution etc way train model recognize word like really mean separate word many thanks advance
Elasticsearch - Nest - Missing First Character,"<p>I am using the Nest client against Elasticsearch. I am using an n-gram index analyzer. I am noticing some odd behavior - when I search for words from the beginning I am not getting any results. However, if I search from the second character on, it works perfectly. These are just normal English letters.</p>

<p>So, for instance, it will find words containing 'kitty' if I search for 'itty', 'itt', 'tty', etc. but not 'ki', 'kit', etc. It's almost like n-gram is just skipping over the first character.</p>

<p>I am not sure if this is being caused by Nest or if this is normal behavior for n-gram. My index settings look similar to those found in this post: <a href=""https://stackoverflow.com/questions/13788178/elasticsearch-using-nest-how-to-configure-analyzers-to-find-partial-words"">Elasticsearch using NEST: How to configure analyzers to find partial words?</a> except my max-gram is only 10.</p>

<p><strong>Update</strong></p>

<p>I simplified my code a little bit and verified the same behavior. </p>

<p>Here is the mapping configuration defined using Nest:</p>

<pre><code>const string index = ""myApp"";
const string type = ""account"";
const string indexAnalyzer = ""custom_ngram_analyser"";
const string searchAnalyzer = ""standard"";
const string tokenizer = ""custom_ngram_tokenizer"";
const string tokenFilter = ""custom_ngram_tokenFilter"";
...
client.CreateIndex(index, i =&gt; i
        .Analysis(ad =&gt; ad
            .Analyzers(a =&gt; a.Add(indexAnalyzer, new CustomAnalyzer() { Tokenizer = tokenizer }))
            .Tokenizers(t =&gt; t.Add(tokenizer, new NGramTokenizer() { MinGram = 1, MaxGram = 15 }))
            .TokenFilters(f =&gt; f.Add(tokenFilter, new NgramTokenFilter() { MinGram = 1, MaxGram = 15 })))
        .TypeName(account);
        .IdField(r =&gt; r.SetPath(""accountId"").SetIndex(""not_analyzed"").SetStored(true));
        .Properties(ps =&gt; ps.Number(p =&gt; p.Name(r =&gt; r.AccountId)
                                          .Index(NonStringIndexOption.not_analyzed)
                                          .Store(true));
                            .String(p =&gt; p.Name(r =&gt; r.AccountName)
                                          .Index(FieldIndexOption.analyzed)
                                          .IndexAnalyzer(indexAnalyzer)
                                          .SearchAnalyzer(searchAnalyzer)
                                          .Store(true)
                                          .TermVector(TermVectorOption.no))));
</code></pre>

<p>And this is the search where the first character is missing:</p>

<pre><code>SearchCriteria criteria = new SearchCriteria() { AccountName = ""kitty"" };

client.Search&lt;SearchAccountResult&gt;(s =&gt; s
    .Index(index)
    .Type(type)
    .Query(q =&gt; q.Bool(b =&gt; b.Must(d =&gt; d.Match(m =&gt; m.OnField(r =&gt; r.AccountName).QueryString(criteria.AccountName)))))
    .SortDescending(""_score""))
</code></pre>
",Multilingual Language Processing & Language Identification,elasticsearch nest missing first character using nest client elasticsearch using n gram index analyzer noticing odd behavior search word beginning getting result however search second character work perfectly normal english letter instance find word containing kitty search itty itt tty etc ki kit etc almost like n gram skipping first character sure caused nest normal behavior n gram index setting look similar found post href using nest configure analyzer find partial word except max gram update simplified code little bit verified behavior mapping configuration defined using nest search first character missing
How do I get a list of the most common words in various languages?,"<p>Stack Overflow implemented its ""Related Questions"" feature by taking the title of the current question being asked and removing from it the 10,000 most common English words according to Google. The remaining words are then submitted as a fulltext search to find related questions.</p>

<p>How do I get such a list of the most common English words? Or most common words in other languages? Is this something I can just get off the Google website?</p>
",Multilingual Language Processing & Language Identification,get list common word various language stack overflow implemented related question feature taking title current question asked removing common english word according google remaining word submitted fulltext search find related question get list common english word common word language something get google website
PHP: Translate in natural language a weekly calendar availability?,"<p>I have in my DB users' weekly availability stored like</p>

<pre><code>Monday Morning - yes
Monday Afternoon - yes
Monday Night - NO
Tuesday Morning - yes
Tuesday Afternoon - yes
Tuesday Night - NO
Wednesday Morning - yes
Wednesday Afternoon - yes
Wednesday Night - NO
etc.
</code></pre>

<p>basically is a matrix 7x3, and i'm trying to find a way to express it in a language like</p>

<p>Mike's available <code>""MON-WED, Morning thru afternoon""</code></p>

<p>or something similar...</p>

<p>my brain is exploding to understand the best way to tackle this.</p>

<p>I'm using PHP, but can be JS or whatever...</p>
",Multilingual Language Processing & Language Identification,php translate natural language weekly calendar availability db user weekly availability stored like basically matrix x trying find way express language like mike available something similar brain exploding understand best way tackle using php j whatever
Counting the number of Lemmas in the Given Corpus,"<p>I am a newbie in the Natural Language Processing.</p>

<p>Currently I am working on one Corpus and trying to implement bigram model using the add-one smoothing..</p>

<p>According to the Add-one Smoothing method  </p>

<p><img src=""https://i.sstatic.net/HhQUg.jpg"" alt=""Add-one smoothing formula""></p>

<p>Here V is the vocabulary of the Corpus i.e. Total number of word types in the Corpus.
But while counting the word types I am considering the words in same stem as single word. i.e. cats and cat will be the same while counting.</p>

<p>I want to know if there is any method or algorithm that can help me with finding the lemmas in the Corpus.</p>
",Multilingual Language Processing & Language Identification,counting number lemma given corpus newbie natural language processing currently working one corpus trying implement bigram model using add one smoothing according add one smoothing method v vocabulary corpus e total number word type corpus counting word type considering word stem single word e cat cat counting want know method algorithm help finding lemma corpus
English dictionary in readable format (text or xml),"<p>I am hoping to find a <strong>downloadable (free or paid) English dictionary</strong> preferably from Oxford, Cambridge, Webster in text or XML format to do some NLP.</p>

<p>I hope that each entry has </p>

<ul>
<li>a full part of speech,</li>
<li>pronunciation, </li>
<li>morphology in case of verb and noun</li>
<li>multiple sense/definition entries</li>
</ul>

<p>such as in the following page <a href=""http://www.merriam-webster.com/dictionary/side"" rel=""nofollow"">http://www.merriam-webster.com/dictionary/side</a>.</p>

<p>The actual text of the definition is not important. What I need most is the part of speech, pronunciation, morphology, order of definition entries.</p>

<p><strong>Also wondering</strong>: what does the Stanford NLP toolkit use as lexical resources when it does POS tagging?</p>

<p>Thank you.</p>
",Multilingual Language Processing & Language Identification,english dictionary readable format text xml hoping find downloadable free paid english dictionary preferably oxford cambridge webster text xml format nlp hope entry ha full part speech pronunciation morphology case verb noun multiple sense definition entry following page actual text definition important need part speech pronunciation morphology order definition entry also wondering doe stanford nlp toolkit use lexical resource doe po tagging thank
Corpus/data set of English words with syllabic stress information?,"<p>I know this is a long shot, but does anyone know of a dataset of English words that has stress information by syllable?  Something as simple as the following would be fantastic:</p>

<pre><code>AARD vark
A ble
a BOUT
ac COUNT
AC id
ad DIC tion
ad VERT ise ment
...
</code></pre>
",Multilingual Language Processing & Language Identification,corpus data set english word syllabic stress information know long shot doe anyone know dataset english word ha stress information syllable something simple following would fantastic
Is there anyway in python to count syllables without the use of a dictionary?,"<p>CMUdict works for the english language, but what if I want to count the syllables of content in another language? </p>
",Multilingual Language Processing & Language Identification,anyway python count syllable without use dictionary cmudict work english language want count syllable content another language
Identify an english word as a thing or product?,"<p>Write a program with the following objective -
be able to identify whether a word/phrase represents a thing/product. For example -
1) ""A <strong>glove</strong> comprising at least an index finger receptacle, a middle finger receptacle.."" &lt;-Be able to identify glove as a thing/product. 
2) ""In a window <strong>regulator</strong>, especially for automobiles, in which the window is connected to a drive..."" &lt;- be able to identify regulator as a thing.
Doing this tells me that the text is talking about a thing/product. as a contrast, the following text talks about a process instead of a thing/product -> ""An extrusion coating <strong>process</strong> for the production of flexible packaging films of nylon coated substrates consisting of the steps of...""</p>

<p>I have millions of such texts; hence, manually doing it is not feasible. So far, with the help of using NLTK + Python, I have been able to identify some specific cases which use very similar keywords. But I have not been able to do the same with the kinds mentioned in the examples above. Any help will be appreciated! </p>
",Multilingual Language Processing & Language Identification,identify english word thing product write program following objective able identify whether word phrase represents thing product example glove comprising least index finger receptacle middle finger receptacle able identify glove thing product window regulator especially automobile window connected drive able identify regulator thing tell text talking thing product contrast following text talk process instead thing product extrusion coating process production flexible packaging film nylon coated substrate consisting step million text hence manually feasible far help using nltk python able identify specific case use similar keywords able kind mentioned example help appreciated
Java: Convert an English verb to a particular tense,"<p>Is there a way to convert an English verb to a particular tense with java?</p>

<p>for example:</p>

<pre><code>convertToPast(""go""); //will give went
convertToCtn(""go""); //will give going
</code></pre>

<p>I have used Stanford nlp and WordNet with JAWS.</p>
",Multilingual Language Processing & Language Identification,java convert english verb particular tense way convert english verb particular tense java example used stanford nlp wordnet jaw
using NLP to complete sentences,"<p>I am working on sign language translator android application where the signs are inputted then the application will translate it to english sentence.
one problem that I am facing is that sometimes due to some error in capturing the signs correctly, the sentence formed as a result becomes ill formatted, lose some words or lose it meaning.</p>

<p>For Example:</p>

<blockquote>
  <p>My Name is Mike </p>
</blockquote>

<p>Becomes--</p>

<blockquote>
  <p>My Name is Mke</p>
</blockquote>

<p>or somthing like</p>

<blockquote>
  <p>I am heading to school</p>
</blockquote>

<p>becomes--</p>

<blockquote>
  <p>I am to school</p>
</blockquote>

<p>I want something to check every sentence that is entered and provide suggestion in case the sentence is not formed correctly.</p>

<p>I tried using Stanford CoreNLP but I couldn't find a straightforward solution to my problem.</p>
",Multilingual Language Processing & Language Identification,using nlp complete sentence working sign language translator android application sign inputted application translate english sentence one problem facing sometimes due error capturing sign correctly sentence formed result becomes ill formatted lose word lose meaning example name mike becomes name mke somthing like heading school becomes school want something check every sentence entered provide suggestion case sentence formed correctly tried using stanford corenlp find straightforward solution problem
How can I split text on punctuation but not on emails or other expressions?,"<p>I want split text on punctuation but not on emails - consider that unicode must be used since not all people speaks English.</p>

<pre><code>import re

example = 'My email is John@gmail.com. My name is John. Her email is Anna@gmail.com'
print re.split('[.]\s*', example, re.UNICODE)
# gives ['My email is John@gmail', 'com', 'My name is John', 'Her email is Anna@gmail', 'com']
# required ['My email is John@gmail.com', 'My name is John', 'Her email is Anna@gmail.com']
</code></pre>

<p>How to separate it correctly - I know regex but have not idea how to solve - I think that <a href=""https://docs.python.org/2/library/re.html"" rel=""nofollow"">look behind</a> will not work since number of characters is not fixed.</p>

<p>I can write concurrent matching first email that separator and consider that email always wins separator.</p>

<hr>

<p>Consider that <strong>humans are imperfect</strong> and it is natural language so example can be - we should help with their simple mistakes but not all:</p>

<pre><code>'My email is john@www.mysite.pl.I am teenager.'
'My email is john@www.mysite.pl. I am teenager.'
</code></pre>

<p><a href=""http://data.iana.org/TLD/tlds-alpha-by-domain.txt"" rel=""nofollow"">Top level domains ends</a> can be learned and save in some dictionary like '.com|.pl|...'.</p>
",Multilingual Language Processing & Language Identification,split text punctuation email expression want split text punctuation email consider unicode must used since people speaks english separate correctly know regex idea solve think look behind work since number character fixed write concurrent matching first email separator consider email always win separator consider human imperfect natural language example help simple mistake top level domain end learned save dictionary like com pl
"Text mining for financial news, hand-produced dictionary and framework?","<p>Go straight into the point, for a single piece of financial news, there is no clear border between good or bad news. However, with the shallow understanding, we can know that a news sounds like good news.</p>

<p>Example: MSFT quote rises recently due to.....</p>

<p>Notice that <strong>rises</strong> indicates a rise in quote, so this is good news for those who own MSFT stocks.</p>

<p>Based on this idea, I have a dictionary that each word indicates (or sounds like ) a good news or bad news.</p>

<p>The news is considered as <strong>text</strong>, and the solution is basically based on the dictionary to go through the news information.</p>

<p><strong>Question</strong>:</p>

<ol>
<li>As I can see this is a very very simple of <em>text mining</em>, isn't it?</li>
<li>What type of work/task for this in the <strong>Natural Language Processing</strong>?</li>
<li>Is there any .NET library or framework that support this (Text mining)?</li>
</ol>
",Multilingual Language Processing & Language Identification,text mining financial news hand produced dictionary framework go straight point single piece financial news clear border good bad news however shallow understanding know news sound like good news example msft quote rise recently due notice rise indicates rise quote good news msft stock based idea dictionary word indicates sound like good news bad news news considered text solution basically based dictionary go news information question see simple text mining type work task natural language processing net library framework support text mining
Categorize social events,"<p>I am having name and description of event and i want to find out about the categories of the event(for example is it entertainment event, politic event or something else).
 I was searching on the web and i looked at some natural language processing  techniques such as Latent Dirichlet Allocation but i can not see a way to use it in my situation. 
Is it a good idea to try to categorize by having predefined keywords for each category, and then to query the text and decide by the amount of keywords from each category?
Can someone give me a clue about my problem ? Many thanks</p>
",Multilingual Language Processing & Language Identification,categorize social event name description event want find category event example entertainment event politic event something else wa searching web looked natural language processing technique latent dirichlet allocation see way use situation good idea try categorize predefined keywords category query text decide amount keywords category someone give clue problem many thanks
Looking for a good semantic parser for the Russian language,"<p>Does anyone know of a semantic parser for the Russian language? I've attempted to configure the link-parser available from <a href=""http://www.abisource.com/projects/link-grammar/"" rel=""nofollow noreferrer"">link-grammar site</a> but to no avail.</p>

<p>I'm hoping for a system that can run on the Mac and generate either a prolog or lisp-like representation of the parse tree (but XML output is fine as well).</p>
",Multilingual Language Processing & Language Identification,looking good semantic parser russian language doe anyone know semantic parser russian language attempted configure link parser available link grammar site avail hoping system run mac generate either prolog lisp like representation parse tree xml output fine well
A good word splitter,"<p>I have a set of short strings (average length &lt; 12). 
The strings are mostly sequence of English words (names, dict words etc). 
However there is no delimiter between the words. I want to split each string into individual words. I tried google but didn't find anything. </p>

<p>Is there any standard way to do that? Also where can I get dictionary which also includes name of person, along with other English words.</p>

<p>Please note: The strings might not adhere to grammatical rules of English.</p>

<p>Examples of Strings are given below: <br>
dontdisturb <br>
ilovejane   <br>
iamagoodperson <br></p>
",Multilingual Language Processing & Language Identification,good word splitter set short string average length string mostly sequence english word name dict word etc however delimiter word want split string individual word tried google find anything standard way also get dictionary also includes name person along english word please note string might adhere grammatical rule english example string given dontdisturb ilovejane iamagoodperson
Creating custom plugin for chinese tokenization,"<p>I'm working towards properly integrating the stanford segmenter within SOLR for chinese tokenization. </p>

<p>This plugin involves loading other jar files and model files. I've got it working in a crude manner by hardcoding the complete path for the files. </p>

<p>I'm looking for methods to create the plugin where the paths need not be hardcoded and also to have the plugin in conformance with the SOLR plugin architecture. Please let me know if there are any recommended sites or tutorials for this.</p>

<p>I've added my code below :</p>

<p>public class ChineseTokenizerFactory extends TokenizerFactory {</p>

<pre><code>/** Creates a new WhitespaceTokenizerFactory */
public ChineseTokenizerFactory(Map&lt;String,String&gt; args) {
    super(args);
    assureMatchVersion();
    if (!args.isEmpty()) {
        throw new IllegalArgumentException(""Unknown parameters: "" + args);
    }
}

@Override
public ChineseTokenizer create(AttributeFactory factory, Reader input) {
    Reader processedStringReader = new ProcessedStringReader(input);
    return new ChineseTokenizer(luceneMatchVersion, factory, processedStringReader);
}
</code></pre>

<p>}</p>

<p>public class ProcessedStringReader extends java.io.Reader {</p>

<pre><code>private static final int BUFFER_SIZE = 1024 * 8;
//private static TextProcess m_textProcess = null;
private static final String basedir = ""/home/praveen/PDS_Meetup/solr-4.9.0/custom_plugins/"";
static Properties props = null;
static CRFClassifier&lt;CoreLabel&gt; segmenter = null;
private char[] m_inputData = null;
private int m_offset = 0;
private int m_length = 0;

public ProcessedStringReader(Reader input){
    char[] arr = new char[BUFFER_SIZE];
    StringBuffer buf = new StringBuffer();
    int numChars;

    if(segmenter == null)
    {
        segmenter = new CRFClassifier&lt;CoreLabel&gt;(getProperties());
        segmenter.loadClassifierNoExceptions(basedir + ""ctb.gz"", getProperties());
    }

    try {
        while ((numChars = input.read(arr, 0, arr.length)) &gt; 0) {
            buf.append(arr, 0, numChars);
        }
    } catch (IOException e) {
        e.printStackTrace();
    }

    m_inputData = processText(buf.toString()).toCharArray();
    m_offset = 0;
    m_length = m_inputData.length;
}

@Override
public int read(char[] cbuf, int off, int len) throws IOException {
    int charNumber = 0;
    for(int i = m_offset + off;i&lt;m_length &amp;&amp; charNumber&lt; len; i++){
        cbuf[charNumber] = m_inputData[i];
        m_offset ++;
        charNumber++;
    }
    if(charNumber == 0){
        return -1;
    }
    return charNumber;
}
@Override
public void close() throws IOException {
    m_inputData = null;
    m_offset = 0;
    m_length = 0;
}
public String processText(String inputText)
{
    List&lt;String&gt; segmented = segmenter.segmentString(inputText);
    String output = """";
    if(segmented.size() &gt; 0)
    {
        output = segmented.get(0);
        for(int i=1;i&lt;segmented.size();i++)
        {
            output = output + "" "" +segmented.get(i);
        }
    }
    System.out.println(output);
    return output;
}
static Properties getProperties()
{
    if (props == null) {
        props = new Properties();
        props.setProperty(""sighanCorporaDict"", basedir);
        // props.setProperty(""NormalizationTable"", ""data/norm.simp.utf8"");
        // props.setProperty(""normTableEncoding"", ""UTF-8"");
        // below is needed because CTBSegDocumentIteratorFactory accesses it
        props.setProperty(""serDictionary"",basedir+""dict-chris6.ser.gz"");
        props.setProperty(""inputEncoding"", ""UTF-8"");
        props.setProperty(""sighanPostProcessing"", ""true"");
    }
    return props;
}
</code></pre>

<p>}</p>

<p>public final class ChineseTokenizer extends CharTokenizer {</p>

<pre><code>public ChineseTokenizer(Version matchVersion, Reader in) {
    super(matchVersion, in);
}
public ChineseTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
    super(matchVersion, factory, in);
}

/** Collects only characters which do not satisfy
 * {@link Character#isWhitespace(int)}.*/
@Override
protected boolean isTokenChar(int c) {
    return !Character.isWhitespace(c);
}
</code></pre>

<p>}</p>
",Multilingual Language Processing & Language Identification,creating custom plugin chinese tokenization working towards properly integrating stanford segmenter within solr chinese tokenization plugin involves loading jar file model file got working crude manner hardcoding complete path file looking method create plugin path need hardcoded also plugin conformance solr plugin architecture please let know recommended site tutorial added code public class chinesetokenizerfactory extends tokenizerfactory public class processedstringreader extends java io reader public final class chinesetokenizer extends chartokenizer
Unknown symbol in nltk pos tagging for Arabic,"<p>I have used nltk to tokenize some arabic text </p>

<p>However, i ended up with some results like </p>

<p>(u'an arabic character/word', '``')
or 
(u'an arabic character/word', ':')</p>

<p>However, they do not provide the `` or : in the documentation. </p>

<p>hence i would like to find out what is this </p>

<pre><code>from nltk.toeknize.punkt import PunktWordTokenizer 

z = ""ÿ£ŸÜÿß ÿ™ÿ≥ŸÑŸÇ ÿ¥ÿ¨ÿ±ÿ©""
tkn = PunkWordTokenizer
sen = tkn.tokenize(z)
tokens = nltk.pos_tag(sent)

print tokens
</code></pre>
",Multilingual Language Processing & Language Identification,unknown symbol nltk po tagging arabic used nltk tokenize arabic text however ended result like u arabic character word u arabic character word however provide documentation hence would like find
How does generative language model work in the natural language processing?,"<p>AFAIK, the generative language model is nothing but <strong>a probability distribution for some vocabulary</strong>. I am wondering how to use this probability distribution to generate a stream of words, i.e. language? </p>

<p>If I always pick the word with biggest probability, it will always be the same word because the distribution is fixed.</p>

<p>I am not sure if I understand it correctly. Could anyone provide a concrete operational example?</p>
",Multilingual Language Processing & Language Identification,doe generative language model work natural language processing afaik generative language model nothing probability distribution vocabulary wondering use probability distribution generate stream word e language always pick word biggest probability always word distribution fixed sure understand correctly could anyone provide concrete operational example
an index of chinese characters organized by component radicals. stanford core nlp,"<p>I want to use the one described <a href=""https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/trees/international/pennchinese"" rel=""nofollow"">here</a>, part of the Stanford CoreNLP, as it looks promising but I can't understand how it works. I downloaded the entire CoreNLP but the <code>.jar</code> file mentioned in the README document, i.e. <code>chinese_map_utils.jar</code> is nowhere to be found. Do you think they're expecting me to create such a <code>.jar</code> file myself out of the component code they have listed there? That seems a bit absurd. </p>

<p>Essentially what I'm after is a system of breaking down Chinese characters into their component strokes or radicals (I know that not all the parts are called radicals, spare me the pedantics), so if you know of an alternative solution which is actionable then I'd be happy to hear about it. </p>
",Multilingual Language Processing & Language Identification,index chinese character organized component radical stanford core nlp want use one described part stanford corenlp look promising understand work downloaded entire corenlp file mentioned readme document e nowhere found think expecting create file component code listed seems bit absurd essentially system breaking chinese character component stroke radical know part called radical spare pedantics know alternative solution actionable happy hear
Accessible English Dictionary with words and and proper names,"<p>I need a way to check whether a word does exist in english dictionary. THis word can be also a proper name (city's name, vip's name, location's name.. whatever), considering that also proper names will be lower cased.</p>

<p>What is the best database or dictionary I can access via Python without restrictions for doing this?</p>
",Multilingual Language Processing & Language Identification,accessible english dictionary word proper name need way check whether word doe exist english dictionary word also proper name city name vip name location name whatever considering also proper name lower cased best database dictionary access via python without restriction
How do I list out all English terms in a sentence that indicate an animal?,"<p>For example, in the sentence ""<em>The two horses had just lain down when a brood of ducklings, which had lost their mother, filed into the barn, cheeping feebly and wandering from side to side to find some place where they would not be trodden on.</em>"", there are two animals: horse and duck. </p>

<p>I was looking for vocabulary lists for animal names but was unable to get anything that's complete enough. The <a href=""http://wordnet.princeton.edu/"" rel=""nofollow"">WordNet</a> database looks promising but may be overkill and not broad enough either.</p>
",Multilingual Language Processing & Language Identification,list english term sentence indicate animal example sentence two horse lain brood duckling lost mother filed barn cheeping feebly wandering side side find place would trodden two animal horse duck wa looking vocabulary list animal name wa unable get anything complete enough wordnet database look promising may overkill broad enough either
Stanford tokenizer run from command line pipe output to file,"<p>I've looked all over the place to determine if it's possible to just run the Stanford CoreNLP tokenizer component from the command line and pipe the output directly to a file (as one can do with the Stanford Chinese Word Segmenter) and I've not seen any definitive answer so I'm posing it here as a distinct question, is that possible? If so, how?</p>

<p>I've attempted to use the command specified on the <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">CoreNLP website</a>, i.e.</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt 
</code></pre>

<p>With the added modification of <code>&gt;&gt; output.txt</code> at the end, which I realize is quite simplistic, but as I mentioned above, for the segmenter that worked. </p>

<p>This will tell me how many tokens there are, as output to the terminal, but the output.txt file always comes up empty, why is that? Perhaps I can employ some alternative piping strategy?</p>

<p>Ideally what I would like are all the tokens separated onto different lines in an output file. </p>

<p>At this point I'm thinking that I'm going to have to write a small Java program to achieve task, is that the case? </p>
",Multilingual Language Processing & Language Identification,stanford tokenizer run command line pipe output file looked place determine possible run stanford corenlp tokenizer component command line pipe output directly file one stanford chinese word segmenter seen definitive answer posing distinct question possible attempted use command specified corenlp website e added modification end realize quite simplistic mentioned segmenter worked tell many token output terminal output txt file always come empty perhaps employ alternative piping strategy ideally would like token separated onto different line output file point thinking going write small java program achieve task case
"Computing Hamming weight, also called popcount in Java?","<p>I am not sure how to translate this from C++ to Java.
It is a function that computes the Hamming weight.</p>

<pre><code>/** This is popcount_3() from:
 * http://en.wikipedia.org/wiki/Hamming_weight */
unsigned int popcnt32(uint32_t n) const
{
    n -= ((n &gt;&gt; 1) &amp; 0x55555555);
    n = (n &amp; 0x33333333) + ((n &gt;&gt; 2) &amp; 0x33333333);
    return (((n + (n &gt;&gt; 4))&amp; 0xF0F0F0F)* 0x1010101) &gt;&gt; 24;
}
</code></pre>

<p>More concretely, I don't know what to use instead of uint32_t,
and if I use that type whatever it is, can I just leave the rest
code unchanged?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,computing hamming weight also called popcount java sure translate c java function computes hamming weight concretely know use instead uint use type whatever leave rest code unchanged thanks
Finding probable number of meaningful English words for given number of blank spaces,"<p>I am currently looking for a piece of Java/JavaScript code to identify probable words for given blank spaces.</p>

<p>Example : Question -----</p>

<p>Answer : The given question has five blank spaces, I want to identify how many 5 length words are there in English to fill these blanks. </p>
",Multilingual Language Processing & Language Identification,finding probable number meaningful english word given number blank space currently looking piece java javascript code identify probable word given blank space example question answer given question ha five blank space want identify many length word english fill blank
"python regular expression split can&#39;t into (&#39;ca&#39;, &quot;n&#39;t&quot;)","<p>Disclaimer: This question is based on one of the questions in the book <a href=""http://www.nltk.org/book/"" rel=""nofollow"">""Natural Language Processing with Python""</a>.</p>

<p>I wish to: </p>

<p>1) split words with a ""n't"" to separate words: such as ""can't"" to (""ca"", ""n't""), </p>

<p>2) Also match words such as ""the"" to ""the"" </p>

<p>I got the right answer, but not sure why. In particular, why does the end-of-line anchor ($) make the ""\w+"" less greedy and stops it from matching the ""n""?</p>

<pre><code>import re
words = [""can't"", 'dog']

#Not the write answer b/c \w+ matches the ""n""
[re.findall(r""(\w+)(n't)?"", w) for w in words] # [[('can', ''), ('t', '')], [('dog', '')]]

#Somehow end of line anchor stops \w+ from matching 'n'
[re.findall(r""(\w+)(n't)?$"", w) for w in words] #[[('ca', ""n't"")], [('dog', '')]]
</code></pre>

<p>Thank you.</p>
",Multilingual Language Processing & Language Identification,python regular expression split ca n disclaimer question based one question book natural language processing python wish split word n separate word ca n also match word got right answer sure particular doe end line anchor make w le greedy stop matching n thank
Porter Stemming of fried,"<p>Why does the porter stemming algorithm online at</p>

<p><a href=""http://text-processing.com/demo/stem/"" rel=""nofollow"">http://text-processing.com/demo/stem/</a></p>

<p>stem <code>fried</code> to <code>fri</code> and not <code>fry</code>?</p>

<p>I can't recall any words ending with <code>ied</code> past tense in English that have a nominative form ending with <code>i</code>.</p>

<p>Is this a bug?</p>
",Multilingual Language Processing & Language Identification,porter stemming fried doe porter stemming algorithm online stem recall word ending past tense english nominative form ending bug
stanford NER classification with additional classes,"<p>Current stanford NER gives mainly 6 classes <code>LOCATION,   TIME,  PERSON'  ORGANIZATION'  MONEY'  PERCENT'  DATE</code>
 Additionally it has been trained with English data so could not classify Indian entities.</p>

<p>Is it possible to train the classifier with additional classes so that it can also identify NE as <code>product, month, disease, device</code> etc.</p>

<p>Also it does not classify Indian entities, so support for such non-english classes too can also be added if this is possible.</p>

<p>Is it possible to retrain classifier, tagger for this additional support?</p>
",Multilingual Language Processing & Language Identification,stanford ner classification additional class current stanford ner give mainly class additionally ha trained english data could classify indian entity possible train classifier additional class also identify ne etc also doe classify indian entity support non english class also added possible possible retrain classifier tagger additional support
Parse raw text with MaltParser in Java,"<p>I found that NLKT in python does it via *raw_parse* function but I need to use Java. I found cleartk has a MaltParser wrapper but there is no documentation about it. I'm looking for a function or a project that first converts raw English text to conll file that MaltParser can use and parses it with MaltParser. Any help is appreciated. </p>
",Multilingual Language Processing & Language Identification,parse raw text maltparser java found nlkt python doe via raw parse function need use java found cleartk ha maltparser wrapper documentation looking function project first convert raw english text conll file maltparser use par maltparser help appreciated
Simplifying the French POS Tag Set with NLTK,"<p>How can one simplify the part of speech tags returned by Stanford's French POS tagger? It is fairly easy to read an English sentence into NLTK, find each word's part of speech, then use map_tag() to simplify the tag set:</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import os
from nltk.tag.stanford import POSTagger
from nltk.tokenize import word_tokenize
from nltk.tag import map_tag

#set java_home path from within script. Run os.getenv(""JAVA_HOME"") to test java_home
os.environ[""JAVA_HOME""] = ""C:\\Program Files\\Java\\jdk1.7.0_25\\bin""

english = u""the whole earth swarms with living beings, every plant, every grain and leaf, supports the life of thousands.""

path_to_english_model = ""C:\\Text\\Professional\\Digital Humanities\\Packages and Tools\\Stanford Packages\\stanford-postagger-full-2014-08-27\\stanford-postagger-full-2014-08-27\\models\\english-bidirectional-distsim.tagger""
path_to_jar = ""C:\\Text\\Professional\\Digital Humanities\\Packages and Tools\\Stanford Packages\\stanford-postagger-full-2014-08-27\\stanford-postagger-full-2014-08-27\\stanford-postagger.jar""

#define english and french taggers
english_tagger = POSTagger(path_to_english_model, path_to_jar, encoding=""utf-8"")

#each tuple in list_of_english_pos_tuples = (word, pos)
list_of_english_pos_tuples = english_tagger.tag(word_tokenize(english))

simplified_pos_tags_english = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in list_of_english_pos_tuples]

print simplified_pos_tags_english

#output = [(u'the', u'DET'), (u'whole', u'ADJ'), (u'earth', u'NOUN'), (u'swarms', u'NOUN'), (u'with', u'ADP'), (u'living', u'NOUN'), (u'beings', u'NOUN'), (u',', u'.'), (u'every', u'DET'), (u'plant', u'NOUN'), (u',', u'.'), (u'every', u'DET'), (u'grain', u'NOUN'), (u'and', u'CONJ'), (u'leaf', u'NOUN'), (u',', u'.'), (u'supports', u'VERB'), (u'the', u'DET'), (u'life', u'NOUN'), (u'of', u'ADP'), (u'thousands', u'NOUN'), (u'.', u'.')]
</code></pre>

<p>But I'm not sure how to map the French tags returned by the following code to the universal tagset:</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import os
from nltk.tag.stanford import POSTagger
from nltk.tokenize import word_tokenize
from nltk.tag import map_tag

#set java_home path from within script. Run os.getenv(""JAVA_HOME"") to test java_home
os.environ[""JAVA_HOME""] = ""C:\\Program Files\\Java\\jdk1.7.0_25\\bin""

french = u""Chaque plante, chaque graine, chaque particule de mati√®re organique contient des milliers d'atomes anim√©s.""

path_to_french_model = ""C:\\Text\\Professional\\Digital Humanities\\Packages and Tools\\Stanford Packages\\stanford-postagger-full-2014-08-27\\stanford-postagger-full-2014-08-27\\models\\french.tagger""
path_to_jar = ""C:\\Text\\Professional\\Digital Humanities\\Packages and Tools\\Stanford Packages\\stanford-postagger-full-2014-08-27\\stanford-postagger-full-2014-08-27\\stanford-postagger.jar""

french_tagger = POSTagger(path_to_french_model, path_to_jar, encoding=""utf-8"")

list_of_french_pos_tuples = french_tagger.tag(word_tokenize(french))

#up to this point all is well, but I'm not sure how to successfully create a simplified pos tagset with the French tuples
simplified_pos_tags_french = [(word, map_tag('SOME_ARGUMENT', 'universal', tag)) for word, tag in list_of_french_pos_tuples]
print simplified_pos_tags_french
</code></pre>

<p>Does anyone know how to simplify the default tag set used by the french model in the Stanford POS tagger? I would be grateful for any insights others can offer on this question.</p>
",Multilingual Language Processing & Language Identification,simplifying french po tag set nltk one simplify part speech tag returned stanford french po tagger fairly easy read english sentence nltk find word part speech use map tag simplify tag set sure map french tag returned following code universal tagset doe anyone know simplify default tag set used french model stanford po tagger would grateful insight others offer question
maximum entropy model and logistic regression,"<p>I am doing a project that has some Natural Language Processing to do. I am using <a href=""http://nlp.stanford.edu/downloads/classifier.shtml"" rel=""noreferrer"">stanford MaxEnt Classifier</a> for the purpose.But I am not sure, whether Maximum entropy model and logistic regression are one at the same or is it some special kind of logistic regression?</p>

<p>Can anyone come up with an explanation?</p>
",Multilingual Language Processing & Language Identification,maximum entropy model logistic regression project ha natural language processing using stanford maxent classifier purpose sure whether maximum entropy model logistic regression one special kind logistic regression anyone come explanation
how to divide a series of words into &quot;N&quot; chunks?,"<p>first of all forgive me for any ambiguity . i find my problem a bit hard to explain in English . 
basically what i want to do is , to divide a huge set of words to ""N"" parts . </p>

<p>for example read all the words in a file , then divide them between lets say N=10 parts .
to be more precise , i'm working on a data mining project . there are thousands of documents which i need to sort the words of . </p>

<p>say n = 2 . i know i can put a-m and n-z in a file . i need an algorithm which can do this for n > 100 .</p>

<p>PS : my program FIRST has to create the N files ( or chunks ) then read all the words and depending on how they begin , assign them to one of the chunks .</p>

<p>EXAMPLE :
input : 
N = 2
words = [....]</p>

<p>output :
[words starting with a-m] , [words starting with n-z] </p>

<p>in other words i want to divide my words Lexicographically</p>
",Multilingual Language Processing & Language Identification,divide series word n chunk first forgive ambiguity find problem bit hard explain english basically want divide huge set word n part example read word file divide let say n part precise working data mining project thousand document need sort word say n know put n z file need algorithm n p program first ha create n file chunk read word depending begin assign one chunk example input n word output word starting word starting n z word want divide word lexicographically
how to divide a series of words into &quot;N&quot; chunks?,"<p>first of all forgive me for any ambiguity . i find my problem a bit hard to explain in English . 
basically what i want to do is , to divide a huge set of words to ""N"" parts . </p>

<p>for example read all the words in a file , then divide them between lets say N=10 parts .
to be more precise , i'm working on a data mining project . there are thousands of documents which i need to sort the words of . </p>

<p>say n = 2 . i know i can put a-m and n-z in a file . i need an algorithm which can do this for n > 100 .</p>

<p>PS : my program FIRST has to create the N files ( or chunks ) then read all the words and depending on how they begin , assign them to one of the chunks .</p>

<p>EXAMPLE :
input : 
N = 2
words = [....]</p>

<p>output :
[words starting with a-m] , [words starting with n-z] </p>

<p>in other words i want to divide my words Lexicographically</p>
",Multilingual Language Processing & Language Identification,divide series word n chunk first forgive ambiguity find problem bit hard explain english basically want divide huge set word n part example read word file divide let say n part precise working data mining project thousand document need sort word say n know put n z file need algorithm n p program first ha create n file chunk read word depending begin assign one chunk example input n word output word starting word starting n z word want divide word lexicographically
How to train a Chinese segmenter model by Stanford NLP Tools,"<p>I am new to the Stanford CoreNLP Tools. Now I do not get an excellent segment result in Chinese, so I want to change the granulity of the Segmenter. I thought I could do this by training my own dict. </p>

<p>I downloaded the <a href=""http://nlp.stanford.edu/software/trainSegmenter-20080521.tar.gz"" rel=""nofollow"" title=""trainSegmenter-20080521.tar.gz""><code>trainSegmenter-20080521</code></a> file, and follow the <code>trainSegmenter-20080521/README.txt</code>.</p>

<p>This is the README.txt:</p>

<pre><code>Sat Jun 21 00:57:22 2008
Author: Pi-Chuan Chang

Here's a documentation of how to train and test the segmenter on specific split 
range of the CTB data.

The following steps assumes you have 3 files defining the ranges of train/dev/test.
They should be named as ""ctb6.train"", ""ctb6.dev"", ""ctb6.test"" respectively.
The format should be like:
      chtb_0003.fid
      chtb_0015.fid
      ...

[STEP 1] change the CTB6 path in the Makefile:
      CTB6=/afs/ir/data/linguistic-data/Chinese-Treebank/6/

[STEP 2] download and uncompress the lastest segmenter from:
      http://nlp.stanford.edu/software/stanford-chinese-segmenter-2008-05-21.tar.gz
and change this path in the Makefile to your local path:
      SEGMENTER=/tmp/stanford-chinese-segmenter-2008-05-21/

[STEP 3] simply type:
      make all
You can also split down into these sub-steps:
      make internaldict # make internal dictionaries for affixation feaetures
      make data         # make datasets
      make traintest    # train &amp; test the CRF segmenter
</code></pre>

<p>But I still have some problems:</p>

<ol>
<li><p>What is the format of the training file and what is <code>train/dev/test</code> each for?</p></li>
<li><p>What is the <code>chtb_0003.fid</code>, <code>chtb_0015.fid</code> and so on?</p></li>
<li><p>What is the <code>CTB6 path</code> in the Makefile, it seems that I shoud change the variable <code>CTB6</code> into <code>/afs/ir/data/linguistic-data/Chinese-Treebank/6/</code>. But it is aready there and it seems not a valid subpath.</p></li>
</ol>

<p>By the way, there are many properties should be set for special demands, e.g., <code>sighanPostProcessing</code> and <code>serDictionary</code>. </p>

<p>Is there somewhere I can get all of those properties and their explanation?</p>

<p>All I can do now is to read the source code, e.g., <code>edu.stanford.nlp.sequences.SeqClassifierFlags.java</code>, but I still get confused with these property flags.</p>

<p>So appreciated for anyone's help.</p>
",Multilingual Language Processing & Language Identification,train chinese segmenter model stanford nlp tool new stanford corenlp tool get excellent segment result chinese want change granulity segmenter thought could training dict downloaded file follow readme txt still problem format training file makefile seems shoud change variable aready seems valid subpath way many property set special demand e g somewhere get property explanation read source code e g still get confused property flag appreciated anyone help
"how to detect who, what, when, where from a sentence as user types","<p>Is there a good algorithm/tokenizer/regex or some other technique that can detect which part of an English language sentence is the ""who"", ""what"", ""when"" and ""where"" as the user is typing?</p>

<p>Ideally it would be something that could be implemented in either elastic search or javascript or .net.</p>

<p>Alternatively maybe something that can just detect the subject and verb of a sentence.</p>

<p>If it works for English, would it be something that can be internationalized?</p>
",Multilingual Language Processing & Language Identification,detect sentence user type good algorithm tokenizer regex technique detect part english language sentence user typing ideally would something could implemented either elastic search javascript net alternatively maybe something detect subject verb sentence work english would something internationalized
Detecting foreign words,"<p>I am writing a script to detect words from a language B in a language A. The two languages are very similar and may have instances of the same words.</p>

<p>The code is here if you are interested in what I have so far:
<a href=""https://github.com/arashsa/language-detection.git"" rel=""nofollow"">https://github.com/arashsa/language-detection.git</a></p>

<p>I will explain my method here:
I create a list of bigrams in language B, a list of bigrams in language A (small corpus in language B, large corpus in language A). Then I remove all bigrams that are common. Then I go through the text in language A, and using the bigrams I detect these in language A and store them in a file. However, this methods finds many words that are common for both languages, and it also finds strange bigrams like the name of two countries adjacent to each other, and other anomalies. </p>

<p>Do any of you have suggestions, reading material, NLP methods that I might use?</p>
",Multilingual Language Processing & Language Identification,detecting foreign word writing script detect word language b language two language similar may instance word code interested far explain method create list bigram language b list bigram language small corpus language b large corpus language remove bigram common go text language using bigram detect language store file however method find many word common language also find strange bigram like name two country adjacent anomaly suggestion reading material nlp method might use
How to run GATE/JAPE for non English language?,"<p>I would like to use JAPE/GATE to my own mother language (not English), as my documents are already tokenized and POS Tag. 
So how can I let GATE load my documents as resource for processing?</p>
",Multilingual Language Processing & Language Identification,run gate jape non english language would like use jape gate mother language english document already tokenized po tag let gate load document resource processing
How to linguistically parse English Text?,"<p>Is there a way to linguistically parse English text? I mean get something like this?</p>

<pre><code>I{I,pronoun} am{to be, verb, Present Simple} late{late, adverb}.
</code></pre>

<p>Or even better with dependencies, like:</p>

<pre><code>I -&gt; am -&gt; (what?) -&gt; late.
</code></pre>

<p>Better in <em>Java</em>, but it doesn't matter much.</p>
",Multilingual Language Processing & Language Identification,linguistically parse english text way linguistically parse english text mean get something like even better dependency like better java matter much
Why is there an extra label when classifier predicts from a test file?,"<p>In a previous <a href=""https://stackoverflow.com/questions/26602794/how-to-vectorize-bigrams-with-the-hashing-trick-in-scikit-learn/26629217?noredirect=1#comment42032677_26629217"">post</a> i asked about text classification and i would like to understand better whats happening and how is working scikit-learn, Assuming that <code>train.txt</code> is </p>

<pre><code>Po≈°to je EULEX obeƒáao da ƒáe obaviti istragu o pro≈°losedmiƒçnom izbijanju nasilja na sjeveru Kosova, taj incident predstavlja jo≈° jedan ispit kapaciteta misije da doprinese jaƒçanju vladavine prava.
De todas as prova√ß√µes que teve de suplantar ao longo da vida, qual foi a mais dif√≠cil? O in√≠cio. Qualquer come√ßo apresenta dificuldades que parecem intranspon√≠veis. Mas tive sempre a minha m√£e do meu lado. Foi ela quem me ajudou a encontrar for√ßas para enfrentar as situa√ß√µes mais decepcionantes, negativas, as que me punham mesmo furiosa.
Al parecer, Andrea Guasch pone que una relaci√≥n a distancia es muy dif√≠cil de llevar como excusa. Algo con lo que, por lo visto, Alex Lequio no est√° nada de acuerdo. ¬øO es que m√°s bien ya ha conseguido la fama que andaba buscando?
Vo v√§ƒç≈°ine golfov√Ωch rezortov ide o veƒæk√Ω komplex niekoƒæk√Ωch ihr√≠sk bl√≠zko pri sebe spojen√Ωch s hotelmi a ƒèal≈°√≠mi mo≈ænos≈•ami tr√°venia voƒæn√©ho ƒçasu ‚Äì nie v≈ædy s√∫ man≈æelky ƒçi deti nad≈°en√Ωmi golfistami, a tak potrebuj√∫ in√Ω druh vy≈æitia. Zauj√≠mav√© kombin√°cie pon√∫kaj√∫ aj rak√∫ske, ≈°vajƒçiarske ƒçi talianske Alpy, kde sa d√° v zime ly≈æova≈• a v lete hra≈• golf pod vysok√Ωmi alpsk√Ωmi konƒçiarmi.
</code></pre>

<p>and <code>test.txt</code> is </p>

<pre><code>Por ello, ha insistido en que Europa tiene que darle un toque de atenci√≥n porque Portugal esta incumpliendo la directiva del establecimiento del peaje
Estima-se que o mercado homossexual s√≥ na Cidade do M√©xico movimente cerca de oito mil milh√µes de d√≥lares, aproximadamente seis mil milh√µes de euros
</code></pre>

<p>I have this little script that can perform language identification:</p>

<pre><code># -- coding: utf-8 --
import codecs
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

train = []

#We set the classes
tags = ['bos','pt','es','slov']

word_vectorizer = CountVectorizer(analyzer='word')


#vectorize the train and the test files
trainset = word_vectorizer.fit_transform(
    codecs.open('/Users/user/Desktop/train.txt','r','utf8'))

testset = word_vectorizer.transform(
    codecs.open('/Users/user/Desktop/test.txt','r','utf8'))

# We train the algorithm:
mnb = MultinomialNB()
mnb.fit(trainset, tags)
res = mnb.predict(testset)

print res
</code></pre>

<p>And this is the output: <code>['es' 'pt' 'bos']</code>. The problem with the output is that clearly is adding one more class that actually is wrong (i.e. <code>bos</code>) to the classification output. I guess that the problem lies on how I'm tagging the <code>train</code> and the <code>test</code> files so, how can i tag both files in order to have the correct result?,  which is: <code>['es' 'pt']</code>. </p>

<p>By now i understand that both files need to be vectorized, but i dont understand what's happening here:</p>

<pre><code>mnb = MultinomialNB()
mnb.fit(trainset, tags)
res = mnb.predict(testset)
</code></pre>

<p>Could anybody explain me deeply what happened above part of the script?.</p>
",Multilingual Language Processing & Language Identification,extra label classifier predicts test file previous href asked text classification would like understand better whats happening working scikit learn assuming p little script perform language identification output problem output clearly adding one class actually wrong e classification output guess problem lie tagging file tag file order correct result understand file need vectorized dont understand happening could anybody explain deeply happened part script
Stopwords filtering not working entirely?,"<p>Good afternoon,</p>

<p>I am running a function in C# designed to remove certain ""stopwords"" from a string such as ""the, or, it"" so it will be more useful in natural language processing. However the function will for some reason not remove the first instance of the word.</p>

<p>For example</p>

<p>input:
The lion the witch and the wardrobe</p>

<p>return:
the lion witch and wardrobe    (I am not using 'and' as a stopword as it can be useful)</p>

<p>My function is below:</p>

<pre><code>    private void filterStopWords(string textToFilter)
    {
        textToFilter.ToLower();
        StringBuilder builder = new StringBuilder(textToFilter);
        for (int i = 0; i &lt; 27; i++)
        {
            if (textToFilter.Contains(stopWords[i]))
            {
                builder.Replace(stopWords[i], "" "");
            }
        }
        filterQueryBox.Text = builder.ToString();
    }
</code></pre>

<p>Stopwords[] is an array containing all my stopwords.</p>

<p>Thanks in advance for any response that may help me here!</p>
",Multilingual Language Processing & Language Identification,stopwords filtering working entirely good afternoon running function c designed remove certain stopwords string useful natural language processing however function reason remove first instance word example input lion witch wardrobe return lion witch wardrobe using stopword useful function stopwords array containing stopwords thanks advance response may help
Python way to detect language ISO code,"<p>I have millions of sentence fragments and I am trying to determine if each is in English, French, Japanese, or Germ. Is there a python program to do this?</p>

<pre><code>s1 = 'This is where lies a person'
s2 = '„Éú„Ç¶„É™„É≥„Ç∞„Éª„Éï„Ç©„Éº„Éª„Ç≥„É≠„É≥„Éê„Ç§„É≥(Â≠óÂπïÁâà)'
s3 = 'Ep. 2448 : √©pisode du 12 mars 2014 (Plus belle la vie, Saison 10, Vol. 6)

language_of_string(s1) ==&gt; EN
language_of_string(s2) ==&gt; JP
language_of_string(s3) ==&gt; FR
</code></pre>
",Multilingual Language Processing & Language Identification,python way detect language iso code million sentence fragment trying determine english french japanese germ python program
Tools for identifying near duplicate documents,"<p>I'm doing a NLP project and identifying near duplicate document is a part of that. Can anyone who has experience with this area suggest the tools (implementations like Weka) available for near duplicate detection?</p>

<p>The project is about generating a statistical report for crimes after analyzing news articles of some local English news papers. The crime articles are firstly classified. Then duplicate articles should be detected and merged. Data collection may contain about 1000 crime related articles for near duplicate detection.</p>

<p>I define near duplicates here as the articles containing the same crime incident. Sometimes different news papers may report the same incidents. Also same news paper may report news articles in different days.</p>

<p>The time taken for duplicate detection is not a problem as this is not online processing. The accuracy is very important here.</p>

<p>Thank you in advance.</p>
",Multilingual Language Processing & Language Identification,tool identifying near duplicate document nlp project identifying near duplicate document part anyone ha experience area suggest tool implementation like weka available near duplicate detection project generating statistical report crime analyzing news article local english news paper crime article firstly classified duplicate article detected merged data collection may contain crime related article near duplicate detection define near duplicate article containing crime incident sometimes different news paper may report incident also news paper may report news article different day time taken duplicate detection problem online processing accuracy important thank advance
error in applying pre-trained model for english of maltparser,"<p>I'm embarking on <strong>maltparser</strong>. I want to apply the offered </p>

<p><strong>pre-trained model</strong> <a href=""http://www.maltparser.org/mco/english_parser/engmalt.html"" rel=""nofollow"">'engmalt.linear-1.7'</a> to parsing </p>

<p><strong>english</strong> sentence on <strong>command line</strong>.</p>

<p>I download the <strong>engmalt</strong> and changed its file name from </p>

<p>'engmalt.linear-1.7.<strong>zip</strong>' to 'engmalt.linear-1.7.<strong>mco</strong>'. I </p>

<p>save the <strong>example english sentence</strong> (on engmalt's download page) </p>

<p>in 'infile.conll' and run on command line. </p>

<p>I can't get the result and there's a strange <strong>error</strong>:</p>

<blockquote>
  <p>No label symbol available for label 'FORM'.</p>
</blockquote>

<p>(By the way, if I test the '/maltparser-</p>

<p>1.7.1/examples/data/talbanken05_test.conll' of maltparser </p>

<p>package,it can surprisingly get the dependency! )</p>

<ul>
<li><p>Q1: Should I change the filename extension of 'engmalt.linear'?</p></li>
<li><p>Q2: Why I can use engmalt to parse 'talbanken05_test.conll'(not </p></li>
</ul>

<p>english sentence)?</p>

<ul>
<li>Q3:Why there's an error when I parse english sentence?What does </li>
</ul>

<p>it mean?</p>

<p>Thanks a lot !</p>
",Multilingual Language Processing & Language Identification,error applying pre trained model english maltparser embarking maltparser want apply offered pre trained model engmalt linear parsing english sentence command line download engmalt changed file name engmalt linear zip engmalt linear mco save example english sentence engmalt download page infile conll run command line get result strange error label symbol available label form way test maltparser example data talbanken test conll maltparser package surprisingly get dependency q change filename extension engmalt linear q use engmalt parse talbanken test conll english sentence q error parse english sentence doe mean thanks lot
repeats specific chars after String,"<p>I'm using a named entity classifier to detect artists in text.  Currently I want to detect patterns in the text that repeatedly appear. </p>

<pre><code>11.20.12 Dillon Francis + Clockwork / / / Trees .
11.24.12 Michael Woods / / / Lizard Lounge .
12.08.12 Flosstradamus / / / Lizard Lounge .
12.14.12 Mat Zo / / / Lizard Lounge .
12.31.12 New Years Eve with BT / / / Lizard Lounge .
</code></pre>

<p>In the text I can detect the following named entities: </p>

<pre><code>11.20.12 Dillon Francis + Clockwork / / / Trees .
[Dillon Francis]
11.24.12 Michael Woods / / / Lizard Lounge .
[Michael Woods, Lounge]
12.08.12 Flosstradamus / / / Lizard Lounge .
[Lizard Lounge]
12.14.12 Mat Zo / / / Lizard Lounge .
[Lizard Lounge]
12.31.12 New Years Eve with BT / / / Lizard Lounge .
[Lizard Lounge]
</code></pre>

<p>I want to use Regex to detect if the pattern is repeating multiple times. So when Dillon Francis is found, I see that after the named entity special chars are used except , and . 
so it's</p>

<pre><code>[named entity][special chars] [ words ] endline
</code></pre>

<p>How can I translate this to a regex? </p>

<p>What i've tried:</p>

<pre><code>([named entity])*([^\\dA-Za-z : , \\. ]) 
[^(Dillon Francis)]*[^a-zA-Z0-9] 
</code></pre>

<p>The output is a true or false.</p>
",Multilingual Language Processing & Language Identification,repeat specific char string using named entity classifier detect artist text currently want detect pattern text repeatedly appear text detect following named entity want use regex detect pattern repeating multiple time dillon francis found see named entity special char used except translate regex tried output true false
Library to extract phrasal verbs from English texts,"<p>Is there any library for auto detection of phrasal (compound) verbs in English texts? And maybe other kinds of word groups that form a special meaning?</p>
",Multilingual Language Processing & Language Identification,library extract phrasal verb english text library auto detection phrasal compound verb english text maybe kind word group form special meaning
Processing English Statements,"<p>Any recommendations for languages/libraries to convert sentence like:</p>

<blockquote>
  <p>""X bumped Y, who in turn kicked Z.""</p>
</blockquote>

<p>to</p>

<ol>
<li>X: Bumped</li>
<li>Y: Was bumped, kicked Z</li>
</ol>
",Multilingual Language Processing & Language Identification,processing english statement recommendation language library convert sentence like x bumped turn kicked z x bumped wa bumped kicked z
What are the prerequisites to learning natural language processing?,"<p>I am planning to learn natural language processing this year.</p>

<p>But when I start reading introductory books on this topic, I found that I miss a lot of points relating mainly to mathematics.</p>

<p>So I'm here searching for what I should learn before I can learn nlp, well, more smoothly?</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,prerequisite learning natural language processing planning learn natural language processing year start reading introductory book topic found miss lot point relating mainly mathematics searching learn learn nlp well smoothly thanks advance
Maltparser doesn&#39;t do anything,"<p>I'm working with maltparser, nltk for process texts. Well i have a integration between maltparser and nltk that works fine. But since every time i execute the program nltk call java VE this take a lot of time... So i think make a webservice who takes conll .txt and return conll parsed by java app. </p>

<p>Well the problem come when i test examples from maltparser sources. I pick one from just initialize model and parser a array of tokens. I just change de model to the regular english one (engmalt.linear-1.7.mco). So execute and return the sentences just like input.</p>

<p>The code is this</p>

<pre><code>public static void main(String[] args) {
    // Loading the Swedish model swemalt-mini
    ConcurrentMaltParserModel model = null;
    try {
        URL swemaltMiniModelURL = new File(""inputs/engmalt.linear-1.7.mco"").toURI().toURL();
        System.out.println(swemaltMiniModelURL.getFile());
        model = ConcurrentMaltParserService.initializeParserModel(swemaltMiniModelURL);
    } catch (Exception e) {
        e.printStackTrace();
    }

    // Creates an array of tokens, which contains the Swedish sentence 'Samtidigt f√•r du h√∂gsta sparr√§nta plus en skattefri sparpremie.'
    // in the CoNLL data format.
    String[] tokens = new String[5];
    tokens[0] = ""1\tThis\t_\tDT\tDT\t_\t0\ta\t_\t_"";
    System.out.println(tokens[0]);
    tokens[1] = ""2\tis\t_\tVBZ\tVBZ\t_\t0\ta\t_\t_"";
    System.out.println(tokens[1]);
    tokens[2] = ""3\ta\t_\tZ\tZ\t_\t0\ta\t_\t_"";
    System.out.println(tokens[2]);
    tokens[3] = ""4\ttest\t_\tNN\tNN\t_\t0\ta\t_\t_"";
    System.out.println(tokens[3]);
    tokens[4] = ""5\t.\t_\tFp\tFp\t_\t0\ta\t_\t_"";
    System.out.println(tokens[4]);
    try {
        String[] outputTokens = model.parseTokens(tokens);
        ConcurrentUtils.printTokens(outputTokens);
    } catch (Exception e) {
        e.printStackTrace();
    }
}
</code></pre>

<p>and the output is:</p>

<pre><code>/home/tomas/workspace/PruebaMalt/inputs/engmalt.linear-1.7.mco
1   This    _   DT  DT  _   0   a   _   _
2   is  _   VBZ VBZ _   0   a   _   _
3   a   _   Z   Z   _   0   a   _   _
4   test    _   NN  NN  _   0   a   _   _
5   .   _   Fp  Fp  _   0   a   _   _
1   This    _   DT  DT  _   0   a   _   _
2   is  _   VBZ VBZ _   0   a   _   _
3   a   _   Z   Z   _   0   a   _   _
4   test    _   NN  NN  _   0   a   _   _
5   .   _   Fp  Fp  _   0   a   _   _
</code></pre>

<p>I try with others models and languages and the same... Any suggestions? ty!</p>
",Multilingual Language Processing & Language Identification,maltparser anything working maltparser nltk process text well integration maltparser nltk work fine since every time execute program nltk call java take lot time think make webservice take conll txt return conll parsed java app well problem come test example maltparser source pick one initialize model parser array token change de model regular english one engmalt linear mco execute return sentence like input code output try others model language suggestion ty
How to honor/inherit user&#39;s language settings in WinForm app?,"<p><em>I have worked with globalization settings in the past but not within the .NET environment, which is the topic of this question. What I am seeing is most certainly due to knowledge I have yet to learn so I would appreciate illumination on the following.</em></p>

<hr>

<p>Setup:
My default language setting is English (en-us specifically). I added a second language (Danish) on my development system (WinXP) and then opened the language bar so I could select either at will. </p>

<hr>

<p>I selected Danish on the language bar then opened Notepad and found the language reverted to English on the language bar. I understand that the language setting is <em>per application</em>, so it seemed that Notepad set the default back to English. (I found that strange since Windows and thus Notepad is used all over the world.) Closing Notepad returned the setting on the language bar to Danish. I then launched my open custom WinForm application--which I know does not set the language--and it <em>also</em> reverted from English to Danish when opened, then back to Danish when terminated!</p>

<p><strong>Question #1A:</strong> How do I get my WinForm application upon launch to inherit the current setting of the language bar? My experiment seems to indicate that each application starts with the system default and requires the user to manually change it once the app is running--this would seem to be a major inconvenience for anyone that wants to work with more than one language!</p>

<p><strong>Question #1B:</strong> If one must, in fact, set the language manually in a multi-language scenario, how do I change my default system language (e.g. to Danish) so I can test my app's launch in another language?</p>

<hr>

<p>I added a display of the current language in my application for this next experiment. Specifically I set a <code>MouseEnter</code> handler on a label that set its tooltip to <code>CultureInfo.CurrentCulture.Name</code> so each time I mouse over I thought I should see the current language setting. Since setting the language before I launch my app did not work, I launched it then set the language to Danish. I found that some things (like typing in a TextBox) did honor this Danish setting. But mousing over the instrumented label still showed en-us!</p>

<p><strong>Question #2A:</strong> Why does <code>CultureInfo.CurrentCulture.Name</code> not reflect the change from my language bar while other parts of my app seem to recognize the change? (Trying <code>CultureInfo.CurrentUICulture.Name</code> produced the same result.) </p>

<p><strong>Question #2B:</strong> Is there an event that fires upon changes on the language bar so I could recognize within my app when the language setting changes?</p>

<hr>

<h2>2010.05.13 Update</h2>

<p>The short but sweet information provided by Eric from Microsoft (see his answer below) directly addressed only one of my four questions (#2A) but it provided just the impetus I needed to delve further and figure out the rest. For the benefit of others who may also be befuddled by this, here is what I uncovered:</p>

<p><strong>Answer #1A:</strong> An application inherits the setting of the default input language, <em>not</em> the language you specify on the language bar. Once your application is running, then changes on the language bar <em>will</em> be noticed immediately by your app.</p>

<p><strong>Answer #1B:</strong> Setting the default input language is done via Regional and Language Options control panel >> Languages tab >> Details >> Settings tab >> default input language.</p>

<p><strong>Answer #2A:</strong> Answered by Eric, the current culture is distinct from the current input language that is reflected on the language bar; typing in a text box is influenced only by the current input language.</p>

<p><strong>Answer #2B:</strong> There is no predefined event for either input language or current culture change notification. An important fact to note here is that input language changes are automatically recognized immediately while current culture changes are not. You must restart your application for a current culture change to take effect--unless you can notice the change and act upon it yourself. To that end I found an MSDN article (<a href=""http://msdn.microsoft.com/en-us/magazine/cc163824.aspx#S1"" rel=""nofollow noreferrer"">The Many Faces of the CultureInfo Class</a>) that provides just such a hook to notice the change.</p>
",Multilingual Language Processing & Language Identification,honor inherit user language setting winform app worked globalization setting past within net environment topic question seeing certainly due knowledge yet learn would appreciate illumination following setup default language setting english en u specifically added second language danish development system winxp opened language bar could select either selected danish language bar opened notepad found language reverted english language bar understand language setting per application seemed notepad set default back english found strange since window thus notepad used world closing notepad returned setting language bar danish launched open custom winform application know doe set language also reverted english danish opened back danish terminated question get winform application upon launch inherit current setting language bar experiment seems indicate application start system default requires user manually change app running would seem major inconvenience anyone want work one language question b one must fact set language manually multi language scenario change default system language e g danish test app launch another language added display current language application next experiment specifically set handler label set tooltip time mouse thought see current language setting since setting language launch app work launched set language danish found thing like typing textbox honor danish setting mousing instrumented label still showed en u question doe reflect change language bar part app seem recognize change trying produced result question b event fire upon change language bar could recognize within app language setting change update short sweet information provided eric microsoft see answer directly addressed one four question provided impetus needed delve figure rest benefit others may also befuddled uncovered answer application inherits setting default input language language specify language bar application running change language bar noticed immediately app answer b setting default input language done via regional language option control panel language tab detail setting tab default input language answer answered eric current culture distinct current input language reflected language bar typing text box influenced current input language answer b predefined event either input language current culture change notification important fact note input language change automatically recognized immediately current culture change must restart application current culture change take effect unless notice change act upon end found msdn article many face cultureinfo class provides hook notice change
Extending the training of the OpenNLP organisation model,"<p>I am new to NLP and the OpenNLP libraries and am playing around at the moment with some of the functionality in particular the library's ability to extract names of organisations. If I use a simple string such as</p>

<pre><code>""Bill worked at Microsoft Corp., JP Morgan Chase, Monsanto and General Motors and was amazed at what went on in Congress. ""
</code></pre>

<p>my code drops out:</p>

<pre><code>Detected name ""Bill"". Type person with probability of 0.9604452678787172
Detected name ""Microsoft Corp ."". Type organization with probability of 0.9976452599132802
Detected name ""JP Morgan Chase"". Type organization with probability of 0.9064399433766583
Detected name ""Monsanto"". Type organization with probability of 0.7429123227376515
Detected name ""General Motors"". Type organization with probability of 0.965472905375375
Detected name ""Congress"". Type organization with probability of 0.9940809804351413
</code></pre>

<p>The all seems fine. If, however, I switch to a more British view of the world such as</p>

<pre><code>""Mark worked at The University of London, HSBC, The Royal Bank of Scotland, Dyson and GlaxoSmithKline.""
</code></pre>

<p>I get</p>

<pre><code>Detected name ""Mark"". Type person with probability of 0.7496973664676362
Detected name ""London"". Type location with probability of 0.6625435519843291
Detected name ""Scotland"". Type location with probability of 0.9564118675997605
Detected name ""University of London"". Type organization with probability of 0.8516268558212053
Detected name ""Royal Bank"". Type organization with probability of 0.8953174632171774
</code></pre>

<p>Clearly not as successful. Is this because of the fact that the organisation finder doesn't know about English institutions or have I just been unlucky? If the former is there a way for me to take the existing model and extend it's knowledge to cover the UK institutions better?. I had a quick look for the training data for the existing organisation model but couldn't find anything.</p>
",Multilingual Language Processing & Language Identification,extending training opennlp organisation model new nlp opennlp library playing around moment functionality particular library ability extract name organisation use simple string code drop seems fine however switch british view world get clearly successful fact organisation finder know english institution unlucky former way take existing model extend knowledge cover uk institution better quick look training data existing organisation model find anything
How to use Stanford Parser to extract features with python,"<p>I have included Stanford Parser in my project.Here is part of code of it.  </p>

<pre><code>def parseToStanfordDependencies(self, sentence):

        tokens, tree = self.parse(sentence)
        standoffTokens = [standoffFromToken(sentence, token)
                          for token in tokens]
        posTags = [token.tag() for token in tree.taggedYield()]
        print "" "".join([""%s/%s"" % (word.text, tag) for word, tag in zip(standoffTokens, posTags)])
        #print tree.taggedYield().toString(False)
        result = self.package.trees.EnglishGrammaticalStructure(tree)

        returnList = []
        for dependency in result.typedDependenciesCollapsedTree():

            govStandoff = standoffTokens[dependency.gov().index() - 1]
            depStandoff = standoffTokens[dependency.dep().index() - 1]

            returnList.append((str(dependency.reln()),
                               govStandoff,
                               depStandoff))

        return Dependencies(sentence, standoffTokens, posTags, returnList)
</code></pre>

<p>I am conducting a task of text classification with linear svm. I have tried </p>

<pre><code>from stanford_parser import parser

stanford_parser = parser.Parser()
print stanford_parser.parseToStanfordDependencies(""This girl I met was your sister."")
</code></pre>

<p>However,besides POS tags, I also want to use Stanford Parser to extract other syntactic features, such as Production rules. What should I do? Is anyone willing to help me? I am a newbie in Python and Natural Language Processing.</p>
",Multilingual Language Processing & Language Identification,use stanford parser extract feature python included stanford parser project part code conducting task text classification linear svm tried however besides po tag also want use stanford parser extract syntactic feature production rule anyone willing help newbie python natural language processing
Natural Language Understanding API,"<p>I am unaware if such an API or service exists currently so this is a vague question, my apologies.</p>

<p>I have a PHP script that works with Freebase and I was wondering if I can enable it so a user can ask a question on my site which will be deconstructed using natural language processing, query the Freebase API and then return an answer.</p>

<p>Does anyone know of an already existing tool like this that works with Freebase?</p>

<p>If not, does anyone know of any great Natural Language Understanding APIs that would be able to strip down a question such as <code>""how tall is mount everest?""</code> and tell my script to query <code>""height""</code> on the mount everest article on Freebase?</p>
",Multilingual Language Processing & Language Identification,natural language understanding api unaware api service exists currently vague question apology php script work freebase wa wondering enable user ask question site deconstructed using natural language processing query freebase api return answer doe anyone know already existing tool like work freebase doe anyone know great natural language understanding apis would able strip question tell script query mount everest article freebase
Database of readings for Japanese words,"<p>Does anyone know of an off-the-shelf database that provides phonetic (kana) readings for Japanese words?</p>
",Multilingual Language Processing & Language Identification,database reading japanese word doe anyone know shelf database provides phonetic kana reading japanese word
NLP project on Comment Summarization,"<p>I am planning to do my final year project on <strong>Natural Language Processing</strong> (using NLTK) and my area of interest is <strong>Comment Summarization from Social media websites</strong> such as Facebook. For example, I am trying to do something like this:</p>

<p>Random Facebook comments in a picture :</p>

<ol>
<li>Wow! Beautiful.</li>
<li>Looking really beautiful.</li>
<li>Very pretty, Nice pic.</li>
</ol>

<p>Now, all these comments will get mapped (using a template based comment summarization technique) into something like this:</p>

<p><strong><em>3 people find this picture to be ""beautiful"".</em></strong>   </p>

<p>The ouput will consist of the word <strong><em>""beautiful""</em></strong> since it is more commonly used in the comments than the word <strong><em>""pretty""</em></strong> (and also the fact that Beautiful and pretty are synonyms).In order to accomplish this task, I am going to use approaches like tracking <strong><em>Keyword frequency</em></strong> and <strong><em>Keyword Scores</em></strong> (In this scenario,<em>""Beautiful""</em> and <em>""Pretty""</em> have a very close score). 
<strong><em>Is this the best way to do it?</em></strong></p>

<p>So far with my research, I have been able to come up with the following papers but none of the papers address this kind of comment summarization :</p>

<ul>
<li><a href=""http://www.hpl.hp.com/research/scl/papers/socialmedia/tweet_summary.pdf"" rel=""nofollow"">Automatic Summarization of Events from Social Media</a></li>
<li><a href=""http://keg.cs.tsinghua.edu.cn/jietang/publications/SIGIR11-Yang-et-al-social-context-summarization.pdf"" rel=""nofollow"">Social Context Summarization</a>
-</li>
</ul>

<p>What are the other papers in this field which address a similar issue? </p>

<p>Apart from this, I also want my summarizer to improve with every summarization task.How do I apply <strong><em>machine learning</em></strong> in this regard?</p>
",Multilingual Language Processing & Language Identification,nlp project comment summarization planning final year project natural language processing using nltk area interest comment summarization social medium website facebook example trying something like random facebook comment picture wow beautiful looking really beautiful pretty nice pic comment get mapped using template based comment summarization technique something like people find picture beautiful ouput consist word beautiful since commonly used comment word pretty also fact beautiful pretty synonym order accomplish task going use approach like tracking keyword frequency keyword score scenario beautiful pretty close score best way far research able come following paper none paper address kind comment summarization automatic summarization event social medium social context summarization paper field address similar issue apart also want summarizer improve every summarization task apply machine learning regard
How to lemmatize spanish words with Pattern?,"<p>I would like to lemmatize a bunch of opinions. As I know, nltk cannot lemmatize words in languages different from English. Researching a little, I found <a href=""http://www.clips.ua.ac.be/pattern"" rel=""nofollow"">pattern</a>, which can lemmatize words in several languages. How can I lemmatize some text with pattern?</p>

<p>This is my test corpus:</p>

<pre><code># -- coding: utf-8 --

from pattern.es import lemma #unresolved reference

opinions = [""Este es un post de juguetes de aprendizaje \
autom√°tico. En realidad, contiene no mucho \
material interesante."",
""Las bases de datos de im√°genes proporcionan \
capacidades de almacenamiento."",
""La mayor√≠a de las bases de datos de im√°genes \
im√°genes seguras de forma permanente."",
""Los datos de imagen de tienda bases de datos."",
""Imagina almacenar bases de datos de bases de \
datos de im√°genes. Almacenar datos. Bases de datos \
de im√°genes de datos de la tienda.""]

print lemma(opiniones)
</code></pre>

<p>output:</p>

<pre><code>  File ""/Users/user/PycharmProjects/Pruebas/Lemmatizacion.py"", line 18, in &lt;module&gt;
    print lemma(opiniones)
  File ""/usr/local/lib/python2.7/site-packages/pattern/text/__init__.py"", line 1591, in lemma
    if verb.lower() in self._inverse:
AttributeError: 'list' object has no attribute 'lower'
</code></pre>

<p>How can I lemmatize <code>opinions</code>?</p>
",Multilingual Language Processing & Language Identification,lemmatize spanish word pattern would like lemmatize bunch opinion know nltk lemmatize word language different english researching little found pattern lemmatize word several language lemmatize text pattern test corpus output lemmatize
How to stem a list of words in spanish with nltk?,"<p>How can i stem all spanish words in the following list with nltk snowballstemer?. This is what i tried:</p>

<pre><code># coding=utf-8


from sklearn.feature_extraction.text import CountVectorizer
import nltk.stem

vectorizer= CountVectorizer(min_df=1)

opinion = [""""""
Hola compis!
No sab√åa como se pon√åa una lavadora hasta que conoc√å
esta y es que es muy sencilla de utilizar! Todo un gustazo
cuando estamos aprendiendo para emanciparnos, que si nos
ponen facilidad con las tareas de la casa pues mejor que mejor.


Antes de esta ten√åamos otra de la marca Otsein, de estas
que van incluidas en el mobiliario y adem¬∑s era de carga superior,
pero tan antigua que segÀôn mi madre, nadie la pod√åa tocar porque
solo la entend√åa ella.
Esta es de la marca Aeg y dentro de este tipo de lavadoras de
esta marca las hab√åa m¬∑s caras o m¬∑s baratas y est¬∑ digamos que
est¬∑ en el punto medio. Es de color blanco y tiene carga frontal,
 con una capacidad de 6kg. En casa a pesar de ser cuatro,
 se ponen lavadoras casi todos o todos los d√åas.


En su parte de arriba encontramos la "";zona de mandos"";,
donde se puede echar el detergente, aunque en nuestro caso
lo al ser gel lo ponemos directamente junto con la ropa.
Luego tiene la rueda para elegir el programa y los intermitentes
que indican en que paso del programa estaba.
Como todas tiene programas m¬∑s cortos y m¬∑s largos, incluso
un programa que seria como lavar a mano y otro ideal para
estores, que salen casi secos y planchaditos para colgar y
ya est¬∑. Es muy f¬∑cil de aprenderla y adem¬∑s tiene indicador
por sonido de cuando acaba, lista para abrir y tender.
Saludillos!
""""""]

spanish_stemmer = nltk.stem.SnowballStemmer('spanish')
print ""\n these are the stems of opinion"",
opinion = [[spanish_stemmer(word) for word in sentence.split("" "")]for sentence in opinion]
</code></pre>

<p>the problem with that aproach is the following this is the output:</p>

<pre><code>Traceback (most recent call last):
 these are the stems of opinion
  File ""/Users/user/PycharmProjects/untitled/prueba stem.py"", line 47, in &lt;module&gt;
    opinion = [[spanish_stemmer(word) for word in sentence.split("" "")]for sentence in opinion]
TypeError: 'SnowballStemmer' object is not callable
</code></pre>

<p>How can i return the list of stems given the list (<code>opinion</code>)? and how to lowercase the complete opinion?</p>
",Multilingual Language Processing & Language Identification,stem list word spanish nltk stem spanish word following list nltk snowballstemer tried problem aproach following output return list stem given list lowercase complete opinion
Is there any way to convert the constitutional parsing to dependency parsing in natural language processing,"<p>I wanna know that is there any way to convert the constitutional parsing to dependency parsing in natural language processing or separate grammar has to be written for the dependency parsing.</p>

<p>Is there any analogy between constitutional parsing and dependency parsing that would lead to their interconversion.</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,way convert constitutional parsing dependency parsing natural language processing wan na know way convert constitutional parsing dependency parsing natural language processing separate grammar ha written dependency parsing analogy constitutional parsing dependency parsing would lead interconversion thanks advance
Fast keyword extraction in elasticsearch,"<p>I have large database of annotations of images stored in an elasticsearch database. I want to use this database for keyword extraction. Input is text (typically a newspaper article). My basic idea for an algorithm is to go through each term from the article and use elasticsearch to discover how frequent the term is in the image annotations. Then output terms from articles which are not frequent (in order to prefer names of people or places over common English words).</p>

<p>I don't need something very sophisticated, these keywords are used just as suggestion for user input, but I want something faster then asking N search queries (where N is number of terms in text) to elasticsearch which can be slow on large texts. Is there some robust and fast technique for keyword extraction in elasticsearch?</p>
",Multilingual Language Processing & Language Identification,fast keyword extraction elasticsearch large database annotation image stored elasticsearch database want use database keyword extraction input text typically newspaper article basic idea algorithm go term article use elasticsearch discover frequent term image annotation output term article frequent order prefer name people place common english word need something sophisticated keywords used suggestion user input want something faster asking n search query n number term text elasticsearch slow large text robust fast technique keyword extraction elasticsearch
split text with inextricably phrases,"<p>I have a List containing some inextricably words, like </p>

<pre><code>List&lt;String&gt; lookUp = new ArrayList&lt;&gt;();
lookUp.add(""New York"");
lookUp.add(""Big Apple"");
</code></pre>

<p>For a sentence I want to split it into words, but don't split the inextricably words given in my list. So an example </p>

<pre><code>String sentence = ""New York is also called Big Apple"";
</code></pre>

<p>it should return me </p>

<pre><code>[""New York"", ""is"", ""also"", ""called"", ""Big Apple""]
</code></pre>

<p>I started to write an algorithm which first splits the sentence by whitespaces and then I do a loop: For every word I check if this word and it's right neighbour occure in the lookUp-list and, if true, parse these words together. </p>

<p>1) Imagine my lookUp-list also contains inextricably phrases with more than two words, like ""George W. Bush"" -> my algorithm would only lookup ""George W."" and ""W. Bush"" and won't find it in the lookup-list, so it would split it into 3 words.</p>

<p>2) The more important question (for which you can ignore question 1): Is there already a library or even a GATE plugin (so that I don't have to reinvent the wheel)? And does this also exist for german phrases? I couldn't find one =(</p>
",Multilingual Language Processing & Language Identification,split text inextricably phrase list containing inextricably word like sentence want split word split inextricably word given list example return started write algorithm first split sentence whitespaces loop every word check word right neighbour occure lookup list true parse word together imagine lookup list also contains inextricably phrase two word like george w bush algorithm would lookup george w w bush find lookup list would split word important question ignore question already library even gate plugin reinvent wheel doe also exist german phrase find one
Open source spell checking library for Java,"<p>I am looking for free and open source spell checking libraries (could check whether spelling is correct for a given string, and any suggested corrections for a mis-spelled string), which could be easily integrated into Java program on Linux. English language spell checking is a must requirement,spell checking for other languages is a better to have requirement.</p>

<p>Any suggestions?</p>

<p>BTW: libraries for C/C++ is also ok.</p>
",Multilingual Language Processing & Language Identification,open source spell checking library java looking free open source spell checking library could check whether spelling correct given string suggested correction mi spelled string could easily integrated java program linux english language spell checking must requirement spell checking language better requirement suggestion btw library c c also ok
Multilingual spell checking with language detection,"<p>I'm working on spell checking of mixed language webpages, and haven't been able to find any existing research on the subject.</p>

<p>The aim is to automatically detect language <em>at a sentence level</em> within mixed language webpages and spell check each against their appropriate language automatically. Assume that we can ignore sentences which mix multiple languages together (e.g. ""He has a certain je ne sais quoi""), and assume webpages can't contain more than 2 or 3 languages. </p>

<p>Trivial example (Welsh + English): <a href=""http://wales.gov.uk/"">http://wales.gov.uk/</a></p>

<p>I'm currently using a mix of:</p>

<ul>
<li>Character distribution (e.g. 0600-06FF = Arabic etc)</li>
<li>n-Grams to discern languages with similar characters</li>
<li>Dictionary lookup to discern locale, i.e. en-US, en-GB </li>
</ul>

<p>I have working code but am concerned it may be naive or needlessly re-inventing a wheel. Has anyone else done this before?</p>
",Multilingual Language Processing & Language Identification,multilingual spell checking language detection working spell checking mixed language webpage able find existing research subject aim automatically detect language sentence level within mixed language webpage spell check appropriate language automatically assume ignore sentence mix multiple language together e g ha certain je ne sais quoi assume webpage contain language trivial example welsh english href p currently using mix character distribution e g ff arabic etc n gram discern language similar character dictionary lookup discern locale e en u en gb working code concerned may naive needlessly inventing wheel ha anyone else done
Morphology:Tool to get the root word and suffix for a given english word,"<p>I am trying to do morph analysis in POS tagging.</p>

<p>Is there any tool (which I can call from within a python or java script) which returns the Root form and its suffix , when we call it by passing  an English word as parameter.</p>

<h2>For example:</h2>

<p>if I give input:'liked' , I want to get output:like,ed</p>

<p>To get the root form for a given english word, I tried to use porter stemmer and snowball stemmer (inside a python script) but It does not give the valid root word always, since it just strips off the suffix.</p>

<pre><code>from nltk.stem.porter import *
porter_stemmer = PorterStemmer()
print(porter_stemmer.stem(""ladies""))
print(porter_stemmer.stem(""went""))
</code></pre>

<h2>output</h2>

<pre><code>ladi   
went
</code></pre>

<p>for example:
I gave input as 'ladies'
but it return 'ladi' as root form , which is not even an English word.</p>

<p>Sometimes stemmers just return the input word as it is.
for example,
I gave input 'went' and these stemmers return 'went' as root form instead of 'go'.</p>

<p>Please suggest which tool I can use, to get the root form and suffix.</p>
",Multilingual Language Processing & Language Identification,morphology tool get root word suffix given english word trying morph analysis po tagging tool call within python java script return root form suffix call passing english word parameter example give input liked want get output like ed get root form given english word tried use porter stemmer snowball stemmer inside python script doe give valid root word always since strip suffix output example gave input lady return ladi root form even english word sometimes stemmer return input word example gave input went stemmer return went root form instead go please suggest tool use get root form suffix
Segmentation Rules Exchange Files,"<p>I am working in sentence segmentation project and I am searching about SRX files (Segmentation Rules Exchange) for sentence splitting.
I tried to find srx (Segmentation Rules Exchange) files for sentence splitting in English, French, German, Spanish, Italian. but I failed :(</p>

<p>Is there any body can help me because I don't want to spend my time to write this files ?</p>

<p>this is an example of this file :</p>

<pre><code>&lt;languagerule languagerulename=""English""&gt;
&lt;rule break=""no""&gt;
&lt;beforebreak&gt;\b[nN]o\.\s&lt;/beforebreak&gt;
&lt;afterbreak&gt;\p{N}&lt;/afterbreak&gt;
&lt;/rule&gt;
&lt;rule break=""no""&gt;
&lt;beforebreak&gt;\b(pp|[Vv]iz|i\.?\s*e|[Vvol]|[Rr]col|maj|Lt|[Ff]ig|[Ff]igs|[Vv]iz|[Vv]ols|[Aa]pprox|[Ii]ncl|Pres|[Dd]ept|min|max|[Gg]ovt|lb|ft|c\.?\s*f|vs)\.\s&lt;/beforebreak&gt;
&lt;afterbreak&gt;[^\p{Lu}]|I&lt;/afterbreak&gt;
&lt;/rule&gt;
</code></pre>
",Multilingual Language Processing & Language Identification,segmentation rule exchange file working sentence segmentation project searching srx file segmentation rule exchange sentence splitting tried find srx segmentation rule exchange file sentence splitting english french german spanish italian failed body help want spend time write file example file
"Graph database vertex/edge inference from a text (i.e. an informal Graph &#39;schema&#39;), using Natural Language Processing (NLP) - does this exist?","<p>Caveat Emptor - I'm neither a linguist nor a Graph theorist, however, I am a [Java] developer wishing to use a Graph database for persistence and the following topic is of interest to me, and I hope to others.</p>

<p>OK, the idea is to have some application or code to:</p>

<ol>
<li>recognise the embedded relationship structures between named entities within a given piece of text</li>
<li>apply or expose these discovered relationships to usage within a Graph database structure.</li>
</ol>

<p>In such a system, the text might essentially form a basic, layman-written graph schema of sorts. To better visualise this, here is some [very], basic text:</p>

<pre><code>Andrew is married to Jane
</code></pre>

<p>Using the <a href=""http://ucrel.lancs.ac.uk/claws/trial.html"" rel=""nofollow"">online CLAWS parts-of-speech tagger (POS)</a>, I'm given the following:</p>

<pre><code>Andrew_NP0 is_VBZ married_AJ0 to_SENT Jane_NP0
</code></pre>

<p>According to '<a href=""http://www.natcorp.ox.ac.uk/docs/c5spec.html"" rel=""nofollow"">The BNC Basic (C5) Tagset</a>' @ Oxford University, NP0='Proper noun', which is a name (as you know) but these NP0-tagged entries would lend themselves to becoming graph vertice instances/nodes (the end user could be further prompted to give these entries an encompassing 'type/description'). The verb(s), 'VBZ' and adjective(s), AJ0, might highlight graph relationships.</p>

<p>Once the end user has confirmed their graph representation, they might export it to <a href=""http://en.wikipedia.org/wiki/GraphML"" rel=""nofollow"">GraphML</a>, for re-import into a graph database such as <a href=""http://thinkaurelius.github.io/titan"" rel=""nofollow"">Titan</a> or <a href=""http://www.neo4j.org"" rel=""nofollow"">Neo4j</a>.</p>

<p>So, the overall idea is to have a tool that allows a layman end user the ability to create Graph-theory-based database structures, using everyday language.</p>

<p>Does such a tool exist already?</p>

<p>Some of my observations above were influenced, in some way, by the following tools (amongst others):</p>

<p><a href=""http://www.plantuml.com"" rel=""nofollow"">http://www.plantuml.com</a> &lt;- UML diagrams defined using a simple and intuitive language
<a href=""http://www.planttext.com"" rel=""nofollow"">http://www.planttext.com</a> &lt;- See plantuml<br>
<a href=""http://www.acqualia.com/soulver"" rel=""nofollow"">http://www.acqualia.com/soulver</a> &lt;- An NLP-based calculator and currency exchange tool, using natural sentence phrases<br>
<a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a> &lt;- Stanford Log-linear Part-Of-Speech Tagger</p>
",Multilingual Language Processing & Language Identification,graph database vertex edge inference text e informal graph schema using natural language processing nlp doe exist caveat emptor neither linguist graph theorist however java developer wishing use graph database persistence following topic interest hope others ok idea application code recognise embedded relationship structure named entity within given piece text apply expose discovered relationship usage within graph database structure system text might essentially form basic layman written graph schema sort better visualise basic text using online claw part speech tagger po given following according bnc basic c tagset oxford university np proper noun name know np tagged entry would lend becoming graph vertice instance node end user could prompted give entry encompassing type description verb vbz adjective aj might highlight graph relationship end user ha confirmed graph representation might export graphml import graph database titan neo j overall idea tool allows layman end user ability create graph theory based database structure using everyday language doe tool exist already observation influenced way following tool amongst others uml diagram defined using simple intuitive language see plantuml nlp based calculator currency exchange tool using natural sentence phrase stanford log linear part speech tagger
Artificial Intelligence : How to route sentence to action,"<p>I'm a beginner in the field of artificial intelligence... I can use GATE or any other Natural Language Processing but I don't have an answer for this :</p>

<p>Do you know how to evaluate how 2 sentences can be close? even with a large data set?</p>

<p>Do you have any recommendations? I can use the number of permutation, the lengh, the number of tokens, metaphone them, etc... but I don't know what test I should use.</p>

<p>My goal is :
    - ""Hello Jarvis""
    - ""Hello Romain, how are you""</p>

<pre><code>- ""Hello arvis""
- ""Hello Romain, how are you""

- ""Hello mister Swift""
- I don't know what you are expecting, is this like ""Hello Jarvis"" ?
- Yes
- Ok, Hello Romain, How are you?

- ""Hello mister swift, how are you?""
- I don't know what are you expecting.
</code></pre>

<h1>Exemple</h1>

<p>By 1, 2, 3 or n is just an example of similarity scale.</p>

<h2>Basic</h2>

<pre><code>- ""Hello IA"" is closed to
   - ""Hello IA"" by 0
   - ""Hello AI"" by 1 

- ""Hello Jarvis"" is closed to 
   - ""Hello AI"" by 2 
   - ""Hello IA"" by 2

- ""Hello! mister Swift"" is closed to
   - ""Hello AI"" by 3
   - ""Hello IA"" by 3
   - ""Hello Jarvis"" by 2
</code></pre>

<h2>Less Basic</h2>

<pre><code>- ""Hello IA"" is (token length, token word, grammatically, syntactically) closed to
   - ""Hello IA"" by (0,0,0,0)
   - ""Hello AI"" by (0,1,0,0) 

- ""Hello Jarvis"" is closed to 
   - ""Hello AI"" by (0,2,1,1) 
   - ""Hello IA"" by (0,2,1,1)

- ""Hello! mister Swift"" is closed to
   - ""Hello AI"" by (1,2,2,2)
   - ""Hello IA"" by (1,2,2,2)
   - ""Hello Jarvis"" by (1,2,2,2)
</code></pre>
",Multilingual Language Processing & Language Identification,artificial intelligence route sentence action beginner field artificial intelligence use gate natural language processing answer know evaluate sentence close even large data set recommendation use number permutation lengh number token metaphone etc know test use goal hello jarvis hello romain exemple n example similarity scale basic le basic
Sentence segmentation and aligment in noisy text corpus,"<p>I have a parallel corpus which contains about 100,000 aligned paragraphs in Arabic and Persian.</p>

<p>My corpus is a noisy corpus which its paragraphs are incomplete translation of each other (i.e., the parts of Arabic paragraphs are not translated to Persian, and the punctuation marks are not matched, too). </p>

<p>In order to divide the paragraphs to sentences, i used the punctuation marks, but the sentence count is not matched.</p>

<p>Then, I used Microsoft Aligner to align the sentences, but the result is really erroneous. </p>

<p>How do I segment and align the sentences of corpus?</p>
",Multilingual Language Processing & Language Identification,sentence segmentation aligment noisy text corpus parallel corpus contains aligned paragraph arabic persian corpus noisy corpus paragraph incomplete translation e part arabic paragraph translated persian punctuation mark matched order divide paragraph sentence used punctuation mark sentence count matched used microsoft aligner align sentence result really erroneous segment align sentence corpus
How to separate words in a &quot;sentence&quot; with spaces?,"<h2>Background</h2>

<p>Looking to automate creating Domains in JasperServer. Domains are a ""view"" of data for creating ad hoc reports. The names of the columns must be presented to the user in a human readable fashion.</p>

<h2>Problem</h2>

<p>There are over 2,000 possible pieces of data from which the organization could theoretically want to include on a report. The data are sourced from non-human-friendly names such as:</p>

<blockquote>
  <p>payperiodmatchcode
  labordistributioncodedesc
  dependentrelationship actionendoption
  actionendoptiondesc addresstype
  addresstypedesc historytype
  psaddresstype rolename
  bankaccountstatus
  bankaccountstatusdesc bankaccounttype
  bankaccounttypedesc beneficiaryamount
  beneficiaryclass beneficiarypercent
  benefitsubclass beneficiaryclass
  beneficiaryclassdesc benefitactioncode
  benefitactioncodedesc
  benefitagecontrol
  benefitagecontroldesc
  ageconrolagelimit
  ageconrolnoticeperiod</p>
</blockquote>

<h2>Question</h2>

<p>How would you automatically change such names to:</p>

<ul>
<li>pay period match code</li>
<li>labor distribution code desc</li>
<li>dependent relationship</li>
</ul>

<h2>Ideas</h2>

<ul>
<li><p>Use Google's <a href=""http://www.google.co.uk/search?q=caseactioncode&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla%3aen-US%3aofficial&amp;client=firefox-a#sclient=psy&amp;hl=en&amp;client=firefox-a&amp;rls=org.mozilla%3aen-US%3Aofficial&amp;q=labordistributioncodedesc&amp;aq=f&amp;aqi=&amp;aql=&amp;oq=labordistributioncodedesc&amp;gs_rfai=&amp;pbx=1&amp;fp=1&amp;bav=on.2,or.r_gc.r_pw.&amp;cad=b"" rel=""nofollow"">Did you mean</a> engine, however I think it violates their TOS:</p>

<p><code>lynx -dump ¬´url¬ª | grep ""Did you mean"" | awk ...</code></p></li>
</ul>

<h2>Languages</h2>

<p>Any language is fine, but text parsers such as Perl would probably be well-suited. (The column names are English-only.)</p>

<h2>Unnecessary Prefection</h2>

<p>The goal is not 100% perfection in breaking words apart; the following outcome is acceptable:</p>

<ul>
<li>enrollmenteffectivedate -> Enrollment Effective Date</li>
<li>enrollmentenddate -> Enroll Men Tend Date</li>
<li>enrollmentrequirementset -> Enrollment Requirement Set</li>
</ul>

<p>No matter what, a human will need to double-check the results and correct many. Whittling a set of 2,000 results down to 600 edits would be a dramatic time savings. To fixate on <em>some</em> cases having multiple possibilities (e.g., therapistname) is to miss the point altogether.</p>
",Multilingual Language Processing & Language Identification,separate word sentence space background looking automate creating domain jasperserver domain view data creating ad hoc report name column must presented user human readable fashion problem possible piece data organization could theoretically want include report data sourced non human friendly name payperiodmatchcode labordistributioncodedesc dependentrelationship actionendoption actionendoptiondesc addresstype addresstypedesc historytype psaddresstype rolename bankaccountstatus bankaccountstatusdesc bankaccounttype bankaccounttypedesc beneficiaryamount beneficiaryclass beneficiarypercent benefitsubclass beneficiaryclass beneficiaryclassdesc benefitactioncode benefitactioncodedesc benefitagecontrol benefitagecontroldesc ageconrolagelimit ageconrolnoticeperiod question would automatically change name pay period match code labor distribution code desc dependent relationship idea use google mean engine however think violates tos language language fine text parser perl would probably well suited column name english unnecessary prefection goal perfection breaking word apart following outcome acceptable enrollmenteffectivedate enrollment effective date enrollmentenddate enroll men tend date enrollmentrequirementset enrollment requirement set matter human need double check result correct many whittling set result edits would dramatic time saving fixate case multiple possibility e g therapistname miss point altogether
Word frequency corpus for natural language processing,"<p>I have a open source dictionary / thesaurus and I want to find out the following about each words in the dictionary / thesaurus:</p>

<ol>
<li><p>The frequency of the word and its synonyms used in any available open corpus. I could find some open corpus like on the <a href=""http://www-nlp.stanford.edu/links/statnlp.html"" rel=""nofollow"">Stanford NLP page</a> but none for word frequency corpus. Is there any open source word frequency corpus already available? If no, I am looking for some pointers to build one.</p></li>
<li><p>Is there any algorithm / heuristic that classify words into different difficulty levels (eg. very hard, difficult, medium, easy etc) ? Although subjective, but may be the rarity/ frequency of use, ambiguity of meaning i.e. usage in different sense, difficulty of spelling, no of letters in the word etc can be used to classify them. I am looking for any open source package that I can use to find these features especially the word frequency and build a corpus that classify words with difficulty levels. </p></li>
</ol>
",Multilingual Language Processing & Language Identification,word frequency corpus natural language processing open source dictionary thesaurus want find following word dictionary thesaurus frequency word synonym used available open corpus could find open corpus like stanford nlp page none word frequency corpus open source word frequency corpus already available looking pointer build one algorithm heuristic classify word different difficulty level eg hard difficult medium easy etc although subjective may rarity frequency use ambiguity meaning e usage different sense difficulty spelling letter word etc used classify looking open source package use find feature especially word frequency build corpus classify word difficulty level
Using a python Project in java,"<p>I am using java in order to build a linguistic System for arabic language and i need to use an open source project as a black box inside my System the only problem is that the open source System is in Python so my question how could i use this System in java,
so that i could make this function</p>

<pre><code>String output=applyPythonSystem(String input);
</code></pre>

<p>And please don't say Jython (Without explaining How??) i tried to use it but i couldn't find a way to do this function  </p>
",Multilingual Language Processing & Language Identification,using python project java using java order build linguistic system arabic language need use open source project black box inside system problem open source system python question could use system java could make function please say jython without explaining tried use find way function
How to extract an English word&#39;s root and affixes?,"<p>I'm a Chinese student and I'm learning English words using a flashcard software. I think highlighting a word's root and affixes will be very useful to remember that word. So I'm looking for  an algorithm or a look-up table which could extract the roots and affixes.</p>

<p>For example:</p>

<pre><code>foo(""agriculture"") -&gt; [""agri"", ""cult"", ""ure""]
foo(""anniversary"") -&gt; [""ann"", ""vers"", ""ary""]
</code></pre>
",Multilingual Language Processing & Language Identification,extract english word root affix chinese student learning english word using flashcard software think highlighting word root affix useful remember word looking algorithm look table could extract root affix example
Objective-C and natural language processing,"<p>I would like do what many have tried/done before me. I want to search through a database of sentences and connect the most similar ones. </p>

<p>User enters: I think Donovan blatantly ripped off Bob Dylan in terms of style.</p>

<p>...search...</p>

<p>Option 1: Donovan's work is unoriginal. <br/>
Option 2: Bob Dylan sounds like he's dying <br/>
Option 3: Pizza juice ripped bag style Bob Dylan </p>

<p>Option 1 and user entry would be matched. I only have experience with C and C++, and I know that Java has some pretty powerful APIs for NLP. Does Obj-C offer anything of the sort? I want to use this is an iOS app. </p>
",Multilingual Language Processing & Language Identification,objective c natural language processing would like many tried done want search database sentence connect similar one user enters think donovan blatantly ripped bob dylan term style search option donovan work unoriginal option bob dylan sound like dying option pizza juice ripped bag style bob dylan option user entry would matched experience c c know java ha pretty powerful apis nlp doe obj c offer anything sort want use io app
What programming language is the most English-like?,"<p>I'm mainly a Python programmer, and it is often described as being ""executable pseudo-code"".  I have used a little bit of AppleScript, which seems to be the most English-like programming language I have ever seen, because almost operators can be words, and it lets you use ""the"" anywhere (for example, this stupid example I just came up with:</p>

<pre><code>set the firstnumber to 1
set the secondnumber to 2
if the firstnumber is equal to the secondnumber then 
    set the sum to 5
end if
</code></pre>

<p>is a valid AppleScript program.  Are there any programming languages that are even more English-like than these?</p>
",Multilingual Language Processing & Language Identification,programming language english like mainly python programmer often described executable pseudo code used little bit applescript seems english like programming language ever seen almost operator word let use anywhere example stupid example came valid applescript program programming language even english like
Prolog- translating English to C,"<p>We have a relatively simple assignment that I understand in theory but I think I just don't quite understand Prolog's syntax enough to get that into code. Basically, we have a list of English notations that represent operations in C. They're stored as a list when they're passed to our Prolog program. For example:</p>

<pre><code>add 4 to 3
</code></pre>

<p>is </p>

<pre><code>[add, 4, to, 3]
</code></pre>

<p>We need to write a function that takes that list an returns the equivalent. So if I called</p>

<pre><code>english2C([add,4,to,3], C).
C = 4+3
</code></pre>

<p>It would bind C to the result. So the data structure itself would be something like +(4(3)). We have a list of such English notation we have to translate, so it's a finite number. It's not like we have to account for all possibilities. There are also combinations, where they take two operations and combine them (with a comma in between)</p>

<pre><code>english2C([add,3,to,5,',',then,subtract,7], C).
C = 3+5-7
</code></pre>

<p>I'm just somewhat confused as to how to start. I know I can take the very first element of the list and that will always be an operator (+,-,*, etc etc) and then I can just recursively go through the list looking for the operands. The problem there is for things that require order of operations, like ""add 3 to 5 then multiply by 4"", which should be represented as (3+5)*4 but if you just translate it directly you get 3+5*4.</p>

<p>Oh and we have to see if we can get it to run backwards (give it a C statement (3+5) and translate back to english (add 3 to 5)). That part I don't really have an idea for at all.</p>

<p>EDIT: There's a large enough permutations of possible English notations that I can't just pattern match everything. I get the idea that what I need to do is match the first operator with it's corresponding arithmetic symbol then find the operands. For a combinational statement, that would be the first part (so I would have 3+5) and then there would be a comma followed by the next statement. By the way, the combinational statements can be as long as they want, so it's not just two statements and I'm done. </p>
",Multilingual Language Processing & Language Identification,prolog translating english c relatively simple assignment understand theory think quite understand prolog syntax enough get code basically list english notation represent operation c stored list passed prolog program example need write function take list return equivalent called would bind c result data structure would something like list english notation translate finite number like account possibility also combination take two operation combine comma somewhat confused start know take first element list always operator etc etc recursively go list looking operand problem thing require order operation like add multiply represented translate directly get oh see get run backwards give c statement translate back english add part really idea edit large enough permutation possible english notation pattern match everything get idea need match first operator corresponding arithmetic symbol find operand combinational statement would first part would would comma followed next statement way combinational statement long want two statement done
Is functional programming the next step towards natural-language programming?,"<p>This is my very first question so I am a bit nervous about it because I am not sure whether I get the meaning across well enough. Anyhow, here we go....</p>

<p>Whenever new milestones in programming have been reached it seems they always have had one goal in common: to make it easier for programmers, well, to program.</p>

<p>Machine language, opcodes/mnemonics, procedures/functions, structs, classes (OOP) etc. always helped, in their time, to plan, structure and code programs in a more natural,  understandable and better maintainable way.</p>

<p>Of course functional programming is by no means a novelty but it seems that it has experienced a sort of renaissance in recent years. I also believe that FP will get an enormous boost when Microsoft will add F# to their mainstream programming languages.</p>

<p>Returning to my original question, I believe that ultimately programming will be done in a natural language (English) with very few restrictions or rules. The compiler will be part of an AI/NLP system that extracts information from the code or should I say text and transforms it into an intermediate language which the compiler can compile.</p>

<p>So, does FP take programming closer to natural-language programming or is it rather an obstacle and mainstream OOP will lead us faster to natural-language programming?</p>

<p>This question should not be used to discuss the useability or feasability of natural-language programming because only the future will tell. </p>
",Multilingual Language Processing & Language Identification,functional programming next step towards natural language programming first question bit nervous sure whether get meaning across well enough anyhow go whenever new milestone programming reached seems always one goal common make easier programmer well program machine language opcodes mnemonic procedure function structs class oop etc always helped time plan structure code program natural understandable better maintainable way course functional programming mean novelty seems ha experienced sort renaissance recent year also believe fp get enormous boost microsoft add f mainstream programming language returning original question believe ultimately programming done natural language english restriction rule compiler part ai nlp system extract information code say text transforms intermediate language compiler compile doe fp take programming closer natural language programming rather obstacle mainstream oop lead u faster natural language programming question used discus useability feasability natural language programming future tell
Is similarity to &quot;natural language&quot; a convincing selling point for a programming language?,"<p>Look, for example at AppleScript (and there are plenty of others, some admittedly quite good) which advertise their use of the natural language metaphor. Code is apparently more readable because it can be/is intended to be constructed in English-like sentences, says they. I'm sure there are people who would like nothing better than to program using only English sentences. However, I have doubts about the viability of a language that takes that paradigm too far (excepting niche cases).</p>

<p>So, after a certain reasonable point, is natural-languaginess a benefit or a misfeature? What if the concept is carried to an extreme -- will code necessarily be more readable? Or might it be unnecessarily long, difficult to work with, and just as capable of producing hilarity on the scale of obfuscated Perl, obfuscated C, and eye-twisting Bash script logorrhea? </p>

<p>I am aware of some specialty cases like ""Inform"" that are almost pure English, but these have a niche that they're not likely to venture out from. I hear and read about how great it would be for code to read more like English sentences, but are there discussions of the possible disadvantages? If everyday language is so clear, simple, clean, lovely, concise, understandable, why did we invent mathematical notation in the first place?</p>

<p>Is it really easier to describe complex instructions <em>accurately and precisely</em> to a machine in natural language, or isn't something closer to mathematical markup a much better choice? Where should that line be drawn? And finally, are you attracted to languages that are touted as resembling English sentences? Should this whole question have just been a one liner:</p>

<pre><code>naturalLanguage &gt; computerishLanguage ? booAndHiss : cheerLoudly;
</code></pre>
",Multilingual Language Processing & Language Identification,similarity natural language convincing selling point programming language look example applescript plenty others admittedly quite good advertise use natural language metaphor code apparently readable intended constructed english like sentence say sure people would like nothing better program using english sentence however doubt viability language take paradigm far excepting niche case certain reasonable point natural languaginess benefit misfeature concept carried extreme code necessarily readable might unnecessarily long difficult work capable producing hilarity scale obfuscated perl obfuscated c eye twisting bash script logorrhea aware specialty case like inform almost pure english niche likely hear read great would code read like english sentence discussion possible disadvantage everyday language clear simple clean lovely concise understandable invent mathematical notation first place really easier describe complex instruction accurately precisely machine natural language something closer mathematical markup much better choice line drawn finally attracted language touted resembling english sentence whole question one liner
Turn English Sentences into First Order Logic,"<p>Do tools or libraries capable of translation an English sentence into First Order Logic exist?</p>

<p>Eg: 'Every thing that loves all humans is a dog.' becomes:</p>

<pre><code> ‚àÄ_(x,y)  humman(y)  ‚à©  love(x,y) ‚Üí isdog(x)
</code></pre>

<p>I think it's called a semantic interpreter, but I don't seem to find anything beyond research papers.</p>

<p>I have just started learning both Java and Natural Language Processing, and I apologize in advance if my question has been answered before (Or if it's plain stupid).</p>

<p>Thanks for your help.</p>
",Multilingual Language Processing & Language Identification,turn english sentence first order logic tool library capable translation english sentence first order logic exist eg every thing love human dog becomes think called semantic interpreter seem find anything beyond research paper started learning java natural language processing apologize advance question ha answered plain stupid thanks help
Determination of Word Classes,"<p>Is there an open source library or program which can determine the word class (noun, pronoun, verb, adverb etc.) for each word  in a given English sentence?</p>
",Multilingual Language Processing & Language Identification,determination word class open source library program determine word class noun pronoun verb adverb etc word given english sentence
convert similar sound word parts,"<p>I'm having trouble searching for the right terms here to solve the below problem; I'm sure it's a done thing, I just can't find the right terms to express the problem!</p>

<p>I'm basically trying to create a classifier that will take word comparison outputs (e.g. some outputs from Levenstein distances) and decide whether the words are sufficiently different. An important input would probably be something like a soundex comparison. The trouble I'm having is creating the training set for the algorithm (an SVM in this case). I have a long list of names and I need to mutate them a bit (based on similar sounds within the word).</p>

<p>E.g. <code>John</code> and <code>Jon</code> would be a mutation to make, and I could label this in the test set as being equivalent. <code>John</code> and <code>Johann</code> have sufficiently different sound and letter distance to be considered different.</p>

<p>So I'm kinda asking for is a way to achieve a phoneme variation generator, but need to be able to retain the English lettering structure. </p>

<p>Even simple translation might suffice, like ""f"" could (sometimes) be replaced by ""ph"". I'm doing this in Java so any tips in that direction would be great too! Thanks.</p>

<p><strong>EDIT</strong></p>

<p>This is the closest I've come across so far: <a href=""http://www.isi.edu/natural-language/people/hovy/papers/07IJCAI-spelling-variants.pdf"" rel=""nofollow"">http://www.isi.edu/natural-language/people/hovy/papers/07IJCAI-spelling-variants.pdf</a></p>
",Multilingual Language Processing & Language Identification,convert similar sound word part trouble searching right term solve problem sure done thing find right term express problem basically trying create classifier take word comparison output e g output levenstein distance decide whether word sufficiently different important input would probably something like soundex comparison trouble creating training set algorithm svm case long list name need mutate bit based similar sound within word e g would mutation make could label test set equivalent sufficiently different sound letter distance considered different kinda asking way achieve phoneme variation generator need able retain english lettering structure even simple translation might suffice like f could sometimes replaced ph java tip direction would great thanks edit closest come across far
Selecting suitable model for creating Language Identification tool,"<p>I am working on developing a tool for language identification of a given text i.e. given a sample text, identify the language (for e.g. English, Swedish, German, etc.) it is written in. </p>

<p>Now the strategy I have decided to follow (based on a few references I have gathered) are as follows -</p>

<p>a) Create a <code>character n-gram model</code> (The value of n is decided based on certain heuristics and computations)</p>

<p>b) Use a machine learning classifier(such as naive bayes) to predict the language of the given text.</p>

<p>Now, the doubt I have is - Is creating a character N-gram model necessary. As in, what disadvantage does a simple bag of words strategy have i.e. if I use all the words possible in the respective language to create a prediction model, what could be the possible cases where it would fail. </p>

<p>The reason why this doubt arose was the fact that any reference document/research paper I've come across states that language identification is a very difficult task. However, just using this strategy of using the words in the language seems to be a simple task. </p>

<p>EDIT: One reason why N-gram should be preferred is to make the model robust even if there are typos as stated <a href=""https://stackoverflow.com/questions/21656861/bytes-vs-characters-vs-words-which-granularity-for-n-grams?rq=1"">here</a>. Can anyone point out more?</p>
",Multilingual Language Processing & Language Identification,selecting suitable model creating language identification tool working developing tool language identification given text e given sample text identify language e g english swedish german etc written strategy decided follow based reference gathered follows create value n decided based certain heuristic computation b use machine learning classifier naive bayes predict language given text doubt creating character n gram model necessary disadvantage doe simple bag word strategy e use word possible respective language create prediction model could possible case would fail reason doubt arose wa fact reference document research paper come across state language identification difficult task however using strategy using word language seems simple task edit one reason n gram preferred make model robust even typo stated href anyone point p
Python NLP: TypeError: not all arguments converted during string formatting,"<p>I tried the code on ""Natural language processing with python"", but a type error occurred.</p>

<pre><code>import nltk
from nltk.corpus import brown

suffix_fdist = nltk.FreqDist()
for word in brown.words():
    word = word.lower()
    suffix_fdist.inc(word[-1:])
    suffix_fdist.inc(word[-2:])
    suffix_fdist.inc(word[-3:])
common_suffixes = suffix_fdist.items()[:100]

def pos_features(word):
    features = {}
    for suffix in common_suffixes:
        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)
    return features
pos_features('people')
</code></pre>

<p>the error is below:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/wanglan/javadevelop/TestPython/src/FirstModule.py"", line 323, in &lt;module&gt;
    pos_features('people')
  File ""/home/wanglan/javadevelop/TestPython/src/FirstModule.py"", line 321, in pos_features
    features['endswith(%s)' % suffix] = word.lower().endswith(suffix)
TypeError: not all arguments converted during string formatting
</code></pre>

<p>Does anyone could help me find out where i am wrong?</p>
",Multilingual Language Processing & Language Identification,python nlp typeerror argument converted string formatting tried code natural language processing python type error occurred error doe anyone could help find wrong
Create your own gazetteer list,"<p>I am new to Natural Language Processing and GATE.Currently I'm learning to use GATE / ANNIE  . ANNIE's default gazetteer lists are great, but obviously they don't provide lists for everything.
 I need to create a list of characters in a story book.<br>
Creating lists and adding entries  to each and every list from GATE Gazetteer Editor  (as mentioned in Gate manual 13.2.2) or using  a text editor does not seem to be practicable .So anyone knows a method to create our own gazetteer lists other than,creating/editing directly through GATE or using a text editor?</p>
",Multilingual Language Processing & Language Identification,create gazetteer list new natural language processing gate currently learning use gate annie annie default gazetteer list great obviously provide list everything need create list character story book creating list adding entry every list gate gazetteer editor mentioned gate manual using text editor doe seem practicable anyone know method create gazetteer list creating editing directly gate using text editor
Is there a Chinese Full Text Search Engine in nodejs,"<p>I plan to add a Chinese full text search engine feature into my web application. However, I failed to find any existing solution for this. There are search engine solution in nodejs, but Chinese is not supported. So my question would be:</p>

<ol>
<li><p>If there is existing solution in nodejs, that would be awesome. Meanwhile, I am open to know how to build such from scratch.</p></li>
<li><p>I'm also open to know if there is such solution in other programming language.</p></li>
</ol>

<p>As a newbie in search engine, I would like to hear more suggestions in terms of its basic structure, key components and other resources.</p>

<p>Thanks,
Sean</p>
",Multilingual Language Processing & Language Identification,chinese full text search engine nodejs plan add chinese full text search engine feature web application however failed find existing solution search engine solution nodejs chinese supported question would existing solution nodejs would awesome meanwhile open know build scratch also open know solution programming language newbie search engine would like hear suggestion term basic structure key component resource thanks sean
Is there a Chinese Full Text Search Engine in nodejs,"<p>I plan to add a Chinese full text search engine feature into my web application. However, I failed to find any existing solution for this. There are search engine solution in nodejs, but Chinese is not supported. So my question would be:</p>

<ol>
<li><p>If there is existing solution in nodejs, that would be awesome. Meanwhile, I am open to know how to build such from scratch.</p></li>
<li><p>I'm also open to know if there is such solution in other programming language.</p></li>
</ol>

<p>As a newbie in search engine, I would like to hear more suggestions in terms of its basic structure, key components and other resources.</p>

<p>Thanks,
Sean</p>
",Multilingual Language Processing & Language Identification,chinese full text search engine nodejs plan add chinese full text search engine feature web application however failed find existing solution search engine solution nodejs chinese supported question would existing solution nodejs would awesome meanwhile open know build scratch also open know solution programming language newbie search engine would like hear suggestion term basic structure key component resource thanks sean
Parameters for calculating accuracy of part of speech tagger,"<p>I'm a beginner in Natural Language Processing, and I've this basic question about calculating the accuracy of a POS Tagger (tagger is using a corpus):</p>

<p>(Don't confuse the word 'set' below with the mathematical definition of set. I'm just using it as a normal English word to convey some 'group' or 'mapping' )  </p>

<p>There are different metrics of accuracy like Precision/Recall and Confusion matrix. Both of these require the following two things as input parameter:<br>
<strong><em>1.</em></strong> <em>Predicted Result set</em> : After the POS Tagger runs on the input, we have a prediction of tags for the input words. This parameter I understand; it's basically what the tagger generated using the corpus and some statistical techniques. This set is our <em>prediction</em><br>
<strong><em>2.</em></strong> <em>Actual Result set</em> : This set represents what the <em>actual</em> tag of each word is supposed to be. This set is the <em>reality</em>.<br>
<strong>My question is about this second parameter:</strong> How is this set supposed to be ""constructed"". Am I supposed to manually construct a set that maps each input word to the <strong>correct</strong> tag?. By manually, I mean reading the corpus and then finding what the corresponding tag is for each input word.</p>

<p><strong><em>So my question basically is:</em></strong> If there's some code that calculates accuracy of a POS-Tagger, what is the accuracy calculated against? How does this code know what is the correct mapping of words to tags? And if it does know the correct mapping of words to tags then why is <em>this</em> code not being used to do the tagging itself? (I hope the reader understands my confusion here).</p>

<p>I'll give this example:<br>
<strong>Input sentence</strong> : I am a boy.<br>
<strong>Predicted Tags</strong> : I_Pronoun am_Noun a_Article boy_Verb. (simplified names for tags, and obviously the tagging has been done wrong)</p>

<p><strong>Actual tagging should be</strong> : I_Pronoun am_Verb a_Article boy_Noun<br>
<em>I</em> know what the tagging should be, <em>but</em> how does the accuracy calculator code know what the actual tagging should be? Am I supposed manually prepare a mapping of correct tags for each input sentence, and then pass it as a parameter?</p>

<p>Note that I know how the calculation for Precision/Recall works. I'm simply asking: <em>how do I tell it what's the correct tagging set</em> ?</p>
",Multilingual Language Processing & Language Identification,parameter calculating accuracy part speech tagger beginner natural language processing basic question calculating accuracy po tagger tagger using corpus confuse word set mathematical definition set using normal english word convey group mapping different metric accuracy like precision recall confusion matrix require following two thing input parameter predicted result set po tagger run input prediction tag input word parameter understand basically tagger generated using corpus statistical technique set prediction actual result set set represents actual tag word supposed set reality question second parameter set supposed constructed supposed manually construct set map input word correct tag manually mean reading corpus finding corresponding tag input word question basically code calculates accuracy po tagger accuracy calculated doe code know correct mapping word tag doe know correct mapping word tag code used tagging hope reader understands confusion give example input sentence boy predicted tag pronoun noun article boy verb simplified name tag obviously tagging ha done wrong actual tagging pronoun verb article boy noun know tagging doe accuracy calculator code know actual tagging supposed manually prepare mapping correct tag input sentence pas parameter note know calculation precision recall work simply asking tell correct tagging set
python based naive base classifer for new language,"<p>I am not trying to build a whole new naive bayes classifier. There are plenty already for example <a href=""http://scikit-learn.org/stable/modules/naive_bayes.html"" rel=""nofollow"">scitkit learn</a> has Naive Bayes implementation, NLTK has its own <a href=""http://www.nltk.org/_modules/nltk/classify/naivebayes.html"" rel=""nofollow"">NaiveBayesClassifier</a>. </p>

<p>I have 1000+ sentences for training and 300+ sentences for test set in my language (one of Indic language). All I need to do is pick up one of the classifier (Naive Bayes implemented), train it and test its accuracy. </p>

<p>The problem is texts aren't in English its in Devnagari unicode.</p>

<p>I am seeking for suggestions on which Classifier well fits to cover up the main issue I am having so far is with unicode. </p>
",Multilingual Language Processing & Language Identification,python based naive base classifer new language trying build whole new naive bayes classifier plenty already example scitkit learn ha naive bayes implementation nltk ha naivebayesclassifier sentence training sentence test set language one indic language need pick one classifier naive bayes implemented train test accuracy problem text english devnagari unicode seeking suggestion classifier well fit cover main issue far unicode
Apache stanbol content extraction not working as expected,"<p>I am trying to extract phrases using Apache Stanbol. I expect results as follows.</p>

<p>content=""I live in Paris and would like to learn natural language processing using java.""
extracted phrase should be,<br>
<strong>Paris<br>
Natural Language Processing<br>
Java<br></strong></p>

<p>But Its giving only <strong>Paris</strong>. I am using .....:8080/enhancer as endpoint.</p>

<p>my environment is ubuntu 12.10 /java</p>

<p>Any help appreciated.</p>
",Multilingual Language Processing & Language Identification,apache stanbol content extraction working expected trying extract phrase using apache stanbol expect result follows content live paris would like learn natural language processing using java extracted phrase paris natural language processing java giving paris using enhancer endpoint environment ubuntu java help appreciated
Natural Language Date Parser for JS that supports arbitrary text surrounding e.g. - &quot;Follow up next week&quot;,"<h2>PROBLEM</h2>

<p>Libraries like <a href=""http://sugarjs.com/dates"" rel=""nofollow noreferrer"">sugar.js</a> can convert natural language date strings such as:
<BR>
""next week"" but <em>cannot</em> handle strings such as: ""Blah blah blah... Follow up <strong>next week</strong>""</p>

<p>In my application, I need to process a paragraph of notes and detect action items in it.  Siri and Google Calendar are able to do this.</p>

<h2>Potential Solution</h2>

<p>Option 1: Maintain a list of ""Action Verbs"" for each language such as ""Follow Up"", ""Call back"", ""Remind me"" and then grab the natural language date portion after it and pipe it into Sugar.js to get a date back.</p>

<p>I'm not sure if every language will work in this way though... like in all languages will there be  ? or in some languages is the sentence structure be completely different... </p>

<p>Option2: I might be able to get back various supported prefixes from sugar.js locale specific grammars and by semi brute force pass in strings until I find a valid date.</p>

<h2>QUESTION</h2>

<p>Is there a library i've over looked that </p>

<ol>
<li>Works in <em>Javascript</em></li>
<li>Supports multiple languages</li>
<li>Can handle arbitrary text surrounding the date grammar.</li>
</ol>

<h2>Related Posts</h2>

<ul>
<li><a href=""https://stackoverflow.com/questions/1003326/is-there-a-natural-language-parser-for-date-times-in-javascript?rq=1"">Is there a natural language parser for date/times in javascript?</a></li>
<li>JAVA: <a href=""http://ocpsoft.org/prettytime/"" rel=""nofollow noreferrer"">http://ocpsoft.org/prettytime/</a> - based on description it'd probably work... text to date only english</li>
<li>JAVA: <a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/sutime.shtml</a> - Too complex, java based.  (<a href=""https://stackoverflow.com/questions/13367066/date-extraction-from-text"">Date Extraction from Text</a>)
*</li>
</ul>
",Multilingual Language Processing & Language Identification,natural language date parser j support arbitrary text surrounding e g follow next week problem library like sugar j convert natural language date string next week handle string blah blah blah follow next week application need process paragraph note detect action item siri google calendar able potential solution option maintain list action verb language follow call back remind grab natural language date portion pipe sugar j get date back sure every language work way though like language language sentence structure completely different option might able get back various supported prefix sugar j locale specific grammar semi brute force pas string find valid date question library looked work javascript support multiple language handle arbitrary text surrounding date grammar related post based description probably work text date english java complex java based href extraction text
Processing arabic text for transliteration,"<p>I used <a href=""http://www.ar-php.org/en_index-php-arabic.html"" rel=""nofollow noreferrer"">http://www.ar-php.org/en_index-php-arabic.html</a> library for Arabic to english and English to arabic transliteration.</p>

<p>For simple English or Arabic text copied from web it work fine. </p>

<p>But for English text which is written using <code>robert_bold</code> , <code>robert_regular_0</code> fonts, which looks like:</p>

<p><img src=""https://i.sstatic.net/ZXXaf.png"" alt=""Words""></p>

<p>When I convert it, it gives me unsupported text like : </p>

<pre><code>ÿßŸÑ ‚Äòÿü ÿ≥[
ŸÉŸäÿ±[ ‚Äô[ ÿ™
ÿ¥Ÿà ‚Äô\ ŸÜ
ÿ®Ÿá ‚Äô; ÿ≥
ÿü ŸÖ[ŸÜ
ÿ≥ ÿßŸÑ@ÿßŸÜÿßŸá
</code></pre>

<p>When I convert simple English text, it gives all supported Arabic characters.</p>

<p>I am not native Arabic country residence. </p>

<p>Any suggestion to improve my system will appreciable.</p>
",Multilingual Language Processing & Language Identification,processing arabic text transliteration used library arabic english english arabic transliteration simple english arabic text copied web work fine english text written using font look like convert give unsupported text like convert simple english text give supported arabic character native arabic country residence suggestion improve system appreciable
Arabic-English Transliteration using unsupported font,"<p>I am working on language transliteration for Ar and En text.</p>

<p>Here is the link which displays character by character replacement : <a href=""https://github.com/Shnoulle/Ar-PHP/blob/master/Arabic/data/Transliteration.xml"" rel=""nofollow noreferrer"">https://github.com/Shnoulle/Ar-PHP/blob/master/Arabic/data/Transliteration.xml</a></p>

<p>Now issue is:</p>

<p>I am dealing with font style <code>robert_bold.ttf</code> and <code>robert_regular_0.ttf</code> which has some typical characters with <code>underline</code> and <code>overline</code> as in this snap</p>

<p><img src=""https://i.sstatic.net/Gzs60.png"" alt=""enter image description here""></p>

<p>I have .ttf file so I can see this fonts on my system. But in my application or in above <code>Transliteration.xml</code> characters are considered as junk like <code>[, } [</code> etc. </p>

<p>How can I add support of this unsupported characters in <code>Transliteration.xml</code> file?</p>

<pre><code>&lt;pair&gt;
  &lt;search&gt;Ÿä&lt;/search&gt;
  &lt;replace&gt;y&lt;/replace&gt;
&lt;/pair&gt;
&lt;pair&gt;
  &lt;search&gt;Ÿâ&lt;/search&gt;
  &lt;replace&gt;a&lt;/replace&gt;
&lt;/pair&gt;
&lt;pair&gt;
  &lt;search&gt;ÿ£&lt;/search&gt;
  &lt;replace&gt;^&lt;/replace&gt; // Here is one of the character s_ (s with underscore not supported)
&lt;/pair&gt;
</code></pre>
",Multilingual Language Processing & Language Identification,arabic english transliteration using unsupported font working language transliteration ar en text link display character character replacement issue dealing font style ha typical character snap ttf file see font system application character considered junk like etc add support unsupported character file
Code example for Sentiment Analysis for Asian languages - Python NLTK,"<p>There is a demo on <code>sentiment analysis</code> with <code>NLTK</code> (python) here <a href=""http://text-processing.com/demo/sentiment/"" rel=""noreferrer"">http://text-processing.com/demo/sentiment/</a>.</p>

<p>And also the tutorials on the parts of sentiment analysis</p>

<ul>
<li><a href=""http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/"" rel=""noreferrer"">http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/</a> </li>
<li><a href=""http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/"" rel=""noreferrer"">http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/</a></li>
<li><a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""noreferrer"">http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html</a></li>
</ul>

<p><strong>Is there any full code example or working projects with python NLTK on sentiment analysis for <em>Asian languages</em>?</strong> (especially for Chinese, Japanese, Korean or Arabic, Hebrew and Persian languages)</p>
",Multilingual Language Processing & Language Identification,code example sentiment analysis asian language python nltk demo python also tutorial part sentiment analysis full code example working project python nltk sentiment analysis asian language especially chinese japanese korean arabic hebrew persian language
Adapting StanfordCoreNLP to process noisy web text?,"<p>I've been trying out the StanfordCoreNLP NER and everything manually on the website, and it seems they depend on very specific/proper English cues to detect entities, for example. When dealing with web text, though, where you could have some text like </p>

<hr>

<p>John Doe</p>

<p>Assistant Professor of Computer Science</p>

<p>Stanford University</p>

<hr>

<p>StanfordNLP seems to have some trouble (labeling the whole thing as one organization due to lack of prepositions/punctuation). Is there anything I can do to allow NER to better handle this kind of text (e.g. program some pre-processing of text)?</p>
",Multilingual Language Processing & Language Identification,adapting stanfordcorenlp process noisy web text trying stanfordcorenlp ner everything manually website seems depend specific proper english cue detect entity example dealing web text though could text like john doe assistant professor computer science stanford university stanfordnlp seems trouble labeling whole thing one organization due lack preposition punctuation anything allow ner better handle kind text e g program pre processing text
How can I do Train And Test step in Giza++?,"<p>In artificial intelligence methods we have two stages of training.
These stages are data and testing.</p>

<p>In the training stage we give a huge amount of data to a system and we normally test it with smaller volume of data. Then we evaluate the output.</p>

<p>Now the question is can this training be done through the built in functionality embedded in GIZA++ or we should write a separate application for that?</p>

<p>If we should write a separate application can anybody help me by suggesting an already written application? Or a manual? 
Note: I want to have an alignment program not a statistical machine translation</p>

<p>I would prefer to train in Giza++ so I can test with unobserved data.</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,train test step giza artificial intelligence method two stage training stage data testing training stage give huge amount data system normally test smaller volume data evaluate output question training done built functionality embedded giza write separate application write separate application anybody help suggesting already written application manual note want alignment program statistical machine translation would prefer train giza test unobserved data thanks advance
How to make a .cat file from a .txt file,"<p>I'm having trouble in converting a .txt file into a .cat file. </p>

<p>It's a dictionary (LIWC2007 italian)which separates words into categories; I have to load it on WordStat 6.1. WordStat won't see any non-.cat files when I try to load the dictionary.</p>

<p>How do I convert the file? Looking on the web wasn't helpful.</p>

<p>Thanks in advance for your time.</p>
",Multilingual Language Processing & Language Identification,make cat file txt file trouble converting txt file cat file dictionary liwc italian separate word category load wordstat wordstat see non cat file try load dictionary convert file looking web helpful thanks advance time
Find all paragraphs of text that are related to a topic,"<p>Given a set of words <code>[""college"", ""sports"", ""coding""]</code>, and a set of paragraphs of text (i.e. facebook posts), how can I see for each word the paragraphs that are related to that topic?</p>

<p>So for college, how can I find all the paragraphs of text that may be about the topic college?</p>

<p>I'm new to natural language processing, and not very advanced at regex. Clues about how to get started, what the right terms to google, etc are appreciated.</p>
",Multilingual Language Processing & Language Identification,find paragraph text related topic given set word set paragraph text e facebook post see word paragraph related topic college find paragraph text may topic college new natural language processing advanced regex clue get started right term google etc appreciated
How best to detect non-nonsensical text?,"<p>My use case is I have incoming files. Some have a descriptive file name, while others have  collection of numbers and/or letters for the file name.</p>

<p>I am interested to know, what is a suitable way to detect when the file name is not an English word?</p>

<p>Are there any text mining paradigms that can do such a task?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,best detect non nonsensical text use case incoming file descriptive file name others collection number letter file name interested know suitable way detect file name english word text mining paradigm task thanks
Sentence recognition using php based natural language processing,"<p>I know I can do this in python but I wondered if there is a way to do it in php.</p>

<p>I have split a paragraph into sentences but some of these sentences are not really sentences and I would like to 'reject' them. I guess this requires some kind of sentence recognition. I know 'Punkt' can do this in nltk but I really need to be able to do the equivalent in php. 
An example is</p>

<p>'I like to run tap water.'  which should be accepted</p>

<p>'Ewing J. R Nat Gen 133;324;pp123-456.' which should be rejected</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,sentence recognition using php based natural language processing know python wondered way php split paragraph sentence sentence really sentence would like reject guess requires kind sentence recognition know punkt nltk really need able equivalent php example like run tap water accepted ewing j r nat gen pp rejected thanks
Suggestions for development of command and control kind of Natural Language based system,"<p>Lately, I have developed a keen interest in the speech recognition and natural language processing domain and have been playing around with a few different approaches to build a system which can perform commands based on natural language instructions. </p>

<p>In my study so far, I have come across various NLP tools, but haven't been able to figure out how to utilize them for my purpose.C# is my primary language, and sadly, there is hardly anything available on the dotnet platform for NLP.</p>

<p>In addition to the learning curve, there are various problems with the regular NLP approach as well. Language ambiguity, named entity recognition, sentence boundary detection etc are a few points that add to the complexity. These issues are much more prominent in free form unconstrained language detection and parsing, but for a limited domain, the complexity should be reduced. However, I couldn't really overcome the challenge as most tools have huge static dictionary data or the training process is too complex. </p>

<p>The other major issue is about the conversational approach. Most of the tools do not handle conversational history and have no way to identify the context of the incoming instruction.</p>

<p>I was hoping that some of you guys who have either worked on a similar technology earlier would be able to help me iron out these challenges and point me towards the right direction.</p>

<p>Can you share your experience with various tools, the approaches you took, the roadblocks you faced and how you resolved them during the process. </p>

<p><strong>Update:</strong> Let me also include a brief overview of what I envision. The system would essentially be a just a command executor that understands simple english. So, if I say, ""send an email to john"", it should understand that I want to send an email and now ask me questions to get more information about what should be the subject line and the content. Additionally,  if there are more than one Johns in my address book and may be more than one email address for John, the system should be able to identify that too and ask me for further directions. </p>

<p>For the implementation, I think I need following components:</p>

<ul>
<li><strong>Speech to text</strong> converter</li>
<li><strong>NLP engine</strong> to parse the text and identify the action and the objects on which the action is to be performed.</li>
<li>An <strong>execution engine</strong> to create and co-ordinate different agents to perform the different types of actions.</li>
</ul>

<p>The challenge lies in making the system extensible to be able to support more such actionable features at a later stage with a little modification.</p>

<p>I think I am fine with Speech to text part and execution part. But the pain point is the NLP engine which can understand the natural language correctly and give me exact action and parameters for it. </p>

<p>I have played around with POS taggers. They do not help much with the compound statements, and it gets a little tricky to establish the relationship between various verbs and nouns detected in the sentence. </p>

<p>Another issue is with maintaining the context of previous actions and include it making sense of the current statement.</p>

<p>P.S.: Convert it to a wiki if you feel appropriate. Please don't flame me for asking a generic problem.</p>
",Multilingual Language Processing & Language Identification,suggestion development command control kind natural language based system lately developed keen interest speech recognition natural language processing domain playing around different approach build system perform command based natural language instruction study far come across various nlp tool able figure utilize purpose c primary language sadly hardly anything available dotnet platform nlp addition learning curve various problem regular nlp approach well language ambiguity named entity recognition sentence boundary detection etc point add complexity issue much prominent free form unconstrained language detection parsing limited domain complexity reduced however really overcome challenge tool huge static dictionary data training process complex major issue conversational approach tool handle conversational history way identify context incoming instruction wa hoping guy either worked similar technology earlier would able help iron challenge point towards right direction share experience various tool approach took roadblock faced resolved process update let also include brief overview envision system would essentially command understands simple english say send email john understand want send email ask question get information subject line content additionally one john address book may one email address john system able identify ask direction implementation think need following component speech text converter nlp engine parse text identify action object action performed execution engine create co ordinate different agent perform different type action challenge lie making system extensible able support actionable feature later stage little modification think fine speech text part execution part pain point nlp engine understand natural language correctly give exact action parameter played around po tagger help much compound statement get little tricky establish relationship various verb noun detected sentence another issue maintaining context previous action include making sense current statement p convert wiki feel appropriate please flame asking generic problem
Converting Natural language logical condition into a Java code,"<p>How do I go about translating a Natural language logical condition into its Java code counterpart?</p>

<p>Say I have this condition </p>

<pre><code>(Color Equals Blue) AND (Name Contains Smith)
</code></pre>

<p>What can be done to translate this to a Java level code, which might look like</p>

<pre><code>(Color.equals(""Blue"") &amp;&amp; (Name.contains(""Smith"")))
</code></pre>

<p>I could not come up with any definitive approach to achieve the desired outcome, so here I am asking this question. 
Also, please let me know that reason before down-voting.</p>
",Multilingual Language Processing & Language Identification,converting natural language logical condition java code go translating natural language logical condition java code counterpart say condition done translate java level code might look like could come definitive approach achieve desired outcome asking question also please let know reason voting
Android speech turn off number recognition,"<p>Can anyone tell me if there is a parameter to shutoff the nlp of the google number translation within the android speech API?</p>

<p>For example I'm referring to the spoken 'seventy five' vs '75' recognized by the speech api.  It is currently only a number.</p>

<p>I've been looking on <a href=""http://developer.android.com/reference/android/speech/RecognizerIntent.html"" rel=""nofollow"">here</a>, but it doesn't look like it.</p>
",Multilingual Language Processing & Language Identification,android speech turn number recognition anyone tell parameter shutoff nlp google number translation within android speech api example referring spoken seventy five v recognized speech api currently number looking look like
Unscrambling words in a sentence using Natural Language Generation,"<p>I have a sentence in English. Now I want to jumble the words up and input that set of words into a program which should unscramble the words according to normal rules of English grammar to output the original sentence. I can vaguely assume it would require Natural Language Generation algorithms.</p>

<p>For eg:<p></p>
Sentence: Mary has gone for a walk with her dog.<br/>
Set of words:   {has, for, a, with, her, dog, Mary, gone, walk}<br/></p>

<p>The output should be the same sentence. <br/></p>

<p>I can assume only the set of words will never be enough to generate the original sentence. But what more information must be included to revive the original sentence?
Please guide me as to where I should be starting with.</p>
",Multilingual Language Processing & Language Identification,unscrambling word sentence using natural language generation sentence english want jumble word input set word program unscramble word according normal rule english grammar output original sentence vaguely assume would require natural language generation algorithm eg sentence mary ha gone walk dog set word ha dog mary gone walk output sentence assume set word never enough generate original sentence information must included revive original sentence please guide starting
How to distinguish bigrams and merge them into one CSV file in R Studio,"<p>Alright so I am trying to have R read sentences, pull out bigrams, and merge all of these bigrams together into one csv.  Right now I have the code to pull out bigrams for one sentence:</p>

<pre><code>sentence=gsub('[[:punct:]]','', sentence)
    sentence=gsub('[[:cntrl:]]','', sentence)
    sentence=gsub('\\d+','', sentence)
    sentence=tolower(sentence)
    words&lt;- strsplit(sentence, ""\\s+"")[[1]]
    New=NULL
    for(i in 1:length(words)-1){ 
      New[i]=paste(words[i],words[i+1])     
  }
New=as.matrix(New)
colnames(New)&lt;-""Bigrams""
</code></pre>

<p>However, I want to be able to import a csv filled with different sentences and have the previous line of code pull out bigrams for each sentence and then merge them together into one csv file. I started writing a code (below) but it is not right.  I would greatly appreciate any help I can get.  Pretty new to natural language processing in R.</p>

<pre><code>library(tm)
library(plyr)
library(stringr)
data&lt;-read.csv(""file.csv"")
sentences=as.vector(data$text)

bigrams&lt;-function(sentences){

bigrams2&lt;-mlply(sentences,function(sentence){
    sentence=gsub('[[:punct:]]','', sentence)
    sentence=gsub('[[:cntrl:]]','', sentence)
    sentence=gsub('\\d+','', sentence)
    sentence=tolower(sentence)
    words&lt;- strsplit(sentence, ""\\s+"")[[1]]
    New=NULL
    for(i in 1:length(words)-1){ 
      New[i]=paste(words[i],words[i+1])     
   }
New=as.matrix(New)
colnames(New)&lt;-""Bigrams""
New
})
merge(bigrams2,all=TRUE)

} 
</code></pre>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,distinguish bigram merge one csv file r studio alright trying r read sentence pull bigram merge bigram together one csv right code pull bigram one sentence however want able import csv filled different sentence previous line code pull bigram sentence merge together one csv file started writing code right would greatly appreciate help get pretty new natural language processing r thanks
How to convert plural nouns into singular nouns,"<p>In a recent project, I am faced with the task to convert plural nouns into singular forms. I know some POS tagging algortihms and tools that can recognize plural forms of nouns and tag them as 'NNS', but I did not know any algorithm that can cenvert them into singular forms. I have tried stemming, but stemming seems too aggressive to convert the word. It gives something like this:</p>

<pre><code>parties -&gt; parti
</code></pre>

<p>But what I want is:</p>

<pre><code>fish -&gt; fish
classes -&gt; class
parties -&gt; party
goods -&gt; goods
cups -&gt; cup
</code></pre>

<p>This seems to be a difficult problem without a huge dictionary with every English word in it. Is there any mature algortihm that can make it? I am also happy to learn if there is any library that can do this especially libraries in Java. Thanks.</p>
",Multilingual Language Processing & Language Identification,convert plural noun singular noun recent project faced task convert plural noun singular form know po tagging algortihms tool recognize plural form noun tag nns know algorithm cenvert singular form tried stemming stemming seems aggressive convert word give something like want seems difficult problem without huge dictionary every english word mature algortihm make also happy learn library especially library java thanks
is there a way to install the nodebox English linguistics library through pip install?,"<p>The <a href=""http://nodebox.net/code/index.php/Linguistics"" rel=""nofollow"">NodeBox English linguistic library</a> for Python has some nice features, like conjugation, that could be very useful for a project.</p>

<p>I tried installing through pip in a particular virtualenv, but <code>pip search nodebox</code> only brings up:</p>

<pre><code>NodeBox                   - Simple application for creating 2-dimensional
                            graphics and animation using Python code
nodebox-color             - Color classes for python
NodeBox-for-OpenGL        - 2D animation with Python code
nodebox-opengl            - NodeBox for OpenGL is a free, cross-platform
                            library for generating 2D animations with Python
                            programming code.
</code></pre>

<p>Is it pip-installable (in a virtualenv) by another name maybe? Or is the only way to install to </p>

<blockquote>
  <p>Put the en library folder in the same folder as your script so NodeBox
  can find the library. You can also put it in <code>~/Library/Application
  Support/NodeBox/</code>. It takes some time to load all the data the first
  time.</p>
</blockquote>

<p>as stated on their website?</p>
",Multilingual Language Processing & Language Identification,way install nodebox english linguistics library pip install nodebox english linguistic library python ha nice feature like conjugation could useful project tried installing pip particular virtualenv brings pip installable virtualenv another name maybe way install put en library folder folder script nodebox find library also put take time load data first time stated website
How to extract a dataset from twitter?,"<p>Im planning on do my bachelors thesis on machine learning, i was wondering if there is any way to extract a big dataset of tweets in order to use them for my thesis. i know there are several datasets of tweets but i would like to extract one in spanish since there is no one in this language. I all ready try with this script <a href=""https://github.com/uwescience/datasci_course_materials/blob/master/assignment1/twitterstream.py"" rel=""nofollow"">twitterstream.py</a> but i dont get how is the process of obtaining a token and if obtain the token what kind of tweets i will obtain?, i mean do i will obtain only my friend¬¥s tweets?. How¬¥s the process of obtain the dataset?, it seems that im a little bit lost on how to aproach this task.</p>
",Multilingual Language Processing & Language Identification,extract dataset twitter im planning bachelor thesis machine learning wa wondering way extract big dataset tweet order use thesis know several datasets tweet would like extract one spanish since one language ready try script twitterstream py dont get process obtaining token obtain token kind tweet obtain mean obtain friend tweet process obtain dataset seems im little bit lost aproach task
Stanford dependency parser can not deal with some Chinese sentences,"<p>I'm parsing a set of Chinese sentences. Usually Stanford parser works well, but </p>

<ol>
<li><p>in special cases, such as 'Êü¥Ê≤πÊú∫ ÂèØÁáÉÊ∑∑ÂêàÊ∞î ÁöÑ ÂΩ¢Êàê Âíå ÁáÉÁÉß ÈÉΩ ÊòØ Áõ¥Êé• Âú® ÁáÉÁÉßÂÆ§ÂÜÖ ËøõË°å ÁöÑ „ÄÇ' and 'Âú® Êó•Â∏∏ Ë°åÈ©∂ ‰∏≠ ËÇØÂÆö ‰∏ç ÂèØËÉΩ ‰øùÊåÅ ÁáÉÊ≤πÈáè ÁöÑ Â§öÂ∞ë Ôºå ‰πòÂÆ¢ ÁöÑ ËÉñ Áò¶ Ôºå Áõ¥Êé• ÂΩ±Âìç Âà∞ ÂâçÂêéËΩ¥ ÁöÑ ÈÖçÈáç ÈóÆÈ¢ò „ÄÇ'.  They are well-formed, but NullPointerException in line'List tdl = gs .typedDependenciesCCprocessed();', which copied from Demo.java.</p></li>
<li><p>I notice that even the program runs correctly, the output of dependency parsing misses something, say '[advmod(‰º†Áªü-3, ËøôÁßç-2), amod(ËåÉÁï¥-6, ‰º†Áªü-3), nn(ËåÉÁï¥-6, Ê≤πÈó®-4), nn(ËåÉÁï¥-6, Â∫îÁî®-5), dep(ÈôêÂà∂-8, ÂèóÂà∞-7), root(ROOT-0, ÈôêÂà∂-8), dep(Á≤æÁ°ÆÊÄß-11, Áº∫‰πè-10), conj_Âπ∂(ÈôêÂà∂-8, Á≤æÁ°ÆÊÄß-11), nn(ÂΩ¢Âäø-18, Ê±ΩËΩ¶ÁîµÂ≠êÊäÄÊúØ-16), nn(ÂΩ¢Âäø-18, ÂèëÂ±ï-17), num(ÁîµÂ≠êÊ≤πÈó®-23, ‰∏Ä-21), dep(‰∏Ä-21, Áßç-22), dep(egas-25, Ôºà-24), dep(ÁîµÂ≠êÊ≤πÈó®-23, egas-25), dep(egas-25, Ôºâ-26)]', it can be seen that no '-1', '-9', '-12', '-13', '-14', '-15', '19' in the dependency parsing result. The corresponding original sentence is '‰ΩÜ ËøôÁßç ‰º†Áªü Ê≤πÈó® Â∫îÁî® ËåÉÁï¥ ÂèóÂà∞ ÈôêÂà∂ Âπ∂ Áº∫‰πè Á≤æÁ°ÆÊÄß Ôºå Âú® Êó•Êñ∞ÊúàÂºÇ ÁöÑ Ê±ΩËΩ¶ÁîµÂ≠êÊäÄÊúØ ÂèëÂ±ï ÂΩ¢Âäø ‰∏ã Ôºå ‰∏Ä Áßç ÁîµÂ≠êÊ≤πÈó® Ôºà egas Ôºâ Â∫îËøêËÄåÁîü „ÄÇ', if you need.</p></li>
</ol>

<p>How to fix them. Thanks.</p>
",Multilingual Language Processing & Language Identification,stanford dependency parser deal chinese sentence parsing set chinese sentence usually stanford parser work well special case well formed nullpointerexception line list tdl g typeddependenciesccprocessed copied demo java notice even program run correctly output dependency parsing miss something say advmod amod nn nn dep root root dep conj nn nn num dep dep egas dep egas dep egas seen dependency parsing result corresponding original sentence egas need fix thanks
Stanford POS Tagger not tagging Chinese text,"<p>I'm using Stanford POS Tagger (for the first time) and while it tags English correctly, it does not seem to recognize (Simplified) Chinese even when changing the model parameter. Have I overlooked something?</p>
<p>I've downloaded and unpacked the latest full version from here:
<a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/tagger.shtml</a></p>
<p>Then I've inputed sample text into the &quot;sample-input.txt&quot;.</p>
<blockquote>
<p>ËøôÊòØ‰∏Ä‰∏™ÊµãËØïÁöÑÂè•Â≠ê„ÄÇËøôÊòØÂè¶‰∏Ä‰∏™Âè•Â≠ê„ÄÇ</p>
</blockquote>
<p>Then I simply run</p>
<blockquote>
<p>./stanford-postagger.sh models/chinese-distsim.tagger sample-input.txt</p>
</blockquote>
<p>The expected output is to tag each of the words with a part of speech, but instead it recognizes the entire string of text as one word:</p>
<blockquote>
<p>Loading default properties from tagger models/chinese-distsim.tagger</p>
<p>Reading POS tagger model from models/chinese-distsim.tagger ... done [3.5 sec].</p>
<p>ÈÄôÊòØ‰∏ÄÂÄãÊ∏¨Ë©¶ÁöÑÂè•Â≠ê„ÄÇÈÄôÊòØÂè¶‰∏ÄÂÄãÂè•Â≠ê„ÄÇ#NR</p>
<p>Tagged 1 words at 30.30 words per second.</p>
</blockquote>
<p>I appreciate any help.</p>
",Multilingual Language Processing & Language Identification,stanford po tagger tagging chinese text using stanford po tagger first time tag english correctly doe seem recognize simplified chinese even changing model parameter overlooked something downloaded unpacked latest full version inputed sample text sample input txt simply run stanford postagger sh model chinese distsim tagger sample input txt expected output tag word part speech instead recognizes entire string text one word loading default property tagger model chinese distsim tagger reading po tagger model model chinese distsim tagger done sec nr tagged word word per second appreciate help
How to use Stanford CoreNLP with a Non-English parse model?,"<p>I'm trying to detect if a sentence is in <a href=""https://stackoverflow.com/questions/19495967/getting-additional-information-active-passive-tenses-from-a-tagger"">active or passive</a>. For that, I am using Stanford CoreNLP and watch out for the dependencies 'nsubj' (=active) or 'nsubjpass' (=passive).</p>

<p>This works perfectly for English (<a href=""http://pastebin.com/M1V1dEPd"" rel=""nofollow noreferrer"">code is here</a>, if you are interested) with the following output:</p>

<p><strong>Output:</strong></p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from lib/stanford-postagger-full-2013-06-20/models/english-left3words-distsim.tagger ... done [1,2 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1,1 sec].
reln: det
reln: nsubjpass &lt;-- yeah! All I want. Passive sentence detected!
reln: auxpass
reln: root
reln: det
reln: prep_for
</code></pre>

<p>However, I also want to use German now and change the following lines for that:</p>

<pre><code>Properties props = new Properties();
props.put(""parse.flags"", """");
props.put(""pos.model"", ""lib/stanford-postagger-full-2013-06-20/models/german-fast.tagger"");
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");
props.put(""parse.model"", ""edu/stanford/nlp/models/lexparser/germanPCFG.ser.gz""); &lt;--- not there
</code></pre>

<p>This fails, because there is no file parsing model ""germanPCFG.ser.gz"" in the jar (stanford-corenlp-3.2.0-models.jar) - only english. There are German parsing models on the web which I could include (<a href=""http://lisa.spinfo.uni-koeln.de/trac/tesla/browser/tesla/trunk/tesla.component.stanfordparser/resources/germanFactored.ser.gz"" rel=""nofollow noreferrer"">see this one, for example</a>), but then I get a massive stack trace. </p>

<pre><code>Loading parser from serialized file lib/stanford-postagger-full-2013-06-20/germanFactored.ser.gz ...
java.lang.NullPointerException
    at edu.stanford.nlp.parser.lexparser.BinaryGrammar.init(BinaryGrammar.java:224)
    at edu.stanford.nlp.parser.lexparser.BinaryGrammar.readObject(BinaryGrammar.java:211)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1848)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:172)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:607)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:401)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:158)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:144)
    at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:177)
    at edu.stanford.nlp.pipeline.ParserAnnotator.&lt;init&gt;(ParserAnnotator.java:107)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:736)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:260)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:123)
    at nlp.Tagger.parse(Tagger.java:83)
    at nlp.GUI$5.doInBackground(GUI.java:474)
    at nlp.GUI$5.doInBackground(GUI.java:468)
    at javax.swing.SwingWorker$1.call(SwingWorker.java:277)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at javax.swing.SwingWorker.run(SwingWorker.java:316)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Loading parser from text file lib/stanford-postagger-full-2013-06-20/germanFactored.ser.gz java.lang.RuntimeException: lib/stanford-postagger-full-2013-06-20/germanFactored.ser.gz: expecting BEGIN block; got ÔøΩÔøΩ
</code></pre>

<p>If I just take the english parse model (englishPCFG.ser.gz) for the German input, the German passive sentence is not detected correctly. <strong>Any advice on how to continue?</strong></p>
",Multilingual Language Processing & Language Identification,use stanford corenlp non english parse model trying detect sentence code interested following output output however also want use german change following line fails file parsing model germanpcfg ser gz jar stanford corenlp model jar english german parsing model web could include see one example get massive stack trace take english parse model englishpcfg ser gz german input german passive sentence detected correctly advice continue
Lexicon dictionary for synonym words,"<p>There are few dictionaries available for natural language processing. Like positive, negative words dictionaries etc. </p>

<p>Is there any dictionary available which contains list of synonym for all dictionary words? </p>

<p>Like for <code>nice</code> </p>

<pre><code>synonyms: enjoyable, pleasant, pleasurable, agreeable, delightful, satisfying, gratifying, acceptable, to one's liking, entertaining, amusing, diverting, marvellous, good; 
</code></pre>
",Multilingual Language Processing & Language Identification,lexicon dictionary synonym word dictionary available natural language processing like positive negative word dictionary etc dictionary available contains list synonym dictionary word like
How to get NMecab to output romaji?,"<p>I'm using <a href=""http://sourceforge.jp/projects/nmecab/"" rel=""nofollow noreferrer"">a .NET port of Mecab (called NMecab)</a> to try to parse Japanese Hiragana, Katakana, and Kanji to romaji.</p>

<p>Here's my code:</p>

<pre><code>using NMeCab;    
MeCabTagger _tagger;

public string Parse(string input)
{
    _tagger = MeCabTagger.Create();
    _tagger.OutPutFormatType = ""lattice"";
    _tagger.LatticeLevel = MeCabLatticeLevel.Two;


    var output = _tagger.Parse(input);

    return output;
}
</code></pre>

<p>When I call <code>Parse(input)</code> using the following Japanese text: ""„Å©„ÇÇ""</p>

<p>I get the output: ""„Å©„ÇÇÂä©Ë©û,Êé•Á∂öÂä©Ë©û,<em>,</em>,<em>,</em>,„Å©„ÇÇ,„Éâ„É¢,„Éâ„É¢ EOS""</p>

<p>I'm looking for the romaji of ""„Å©„ÇÇ"", which would be ""domo.""</p>

<p>I've tried to use Mecab directly as <a href=""https://stackoverflow.com/questions/6365931/trying-to-get-libmecab-dll-mecab-to-work-with-c-sharp"">discussed in this SO answer</a>, but get the same output.</p>
",Multilingual Language Processing & Language Identification,get nmecab output romaji using net port mecab called nmecab try parse japanese hiragana katakana kanji romaji code call using following japanese text get output eos looking romaji would domo tried use mecab directly href answer get output
Machine learning algos for English sentence conversion,"<p>Is there any machine learning algorithm developed to convert an english sentence and its voice ( active to passive or vice versa)?
I am thinking of developing a small tool for converting a english sentence from active to passive voice and vice versa. I want to know if there is any such tool(s) that already exist?</p>
",Multilingual Language Processing & Language Identification,machine learning algos english sentence conversion machine learning algorithm developed convert english sentence voice active passive vice versa thinking developing small tool converting english sentence active passive voice vice versa want know tool already exist
How to avoid natural for node.js splitting words with special characters,"<p>I'm using <a href=""https://github.com/NaturalNode/natural"" rel=""nofollow"">node natural</a> tokenizer feature, which splits a sentence into words. Usually it's supposed to work as</p>

<pre><code>var natural = require('natural'),
tokenizer = new natural.WordTokenizer();
console.log(tokenizer.tokenize(""your dog has't flees.""));
// Returns [ 'your', 'dog', 'has', 'n't, 'flees' ]
</code></pre>

<p>It works fine, however, when used with German or French words, it splits up the words into two, such as</p>

<pre><code>var natural = require('natural'),
tokenizer = new natural.WordTokenizer();
console.log(tokenizer.tokenize(""fu√üball""));
// Returns ['fu', 'ball']
</code></pre>

<p>Which is not correct.</p>

<p>Anyone knows how to avoid that? </p>

<p>Or maybe you know a simplier way to split sentences into words in JavaScript / Node.js?</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,avoid natural node j splitting word special character using node natural tokenizer feature split sentence word usually supposed work work fine however used german french word split word two correct anyone know avoid maybe know simplier way split sentence word javascript node j thanks
Effectively parsing plain language paragraphs,"<p>Ideally if there's an existing solution you know of please let me know, but some brief googling didnt turn up much, though I may not have been using the right keywords in my search.</p>

<p>What I'd like to do is take in a paragraph of text and return an array of keywords the program thinks that paragraph might be about.</p>

<p>I considered maybe finding all the nouns and gerunds by using a database of nouns and searching the paragraph for each one, but that seems terribly inefficient with upwards of a million nouns in English.</p>

<p>Wasn't sure what to tag this other than theory.</p>
",Multilingual Language Processing & Language Identification,effectively parsing plain language paragraph ideally existing solution know please let know brief googling didnt turn much though may using right keywords search like take paragraph text return array keywords program think paragraph might considered maybe finding noun gerund using database noun searching paragraph one seems terribly inefficient upwards million noun english sure tag theory
What happened to MsNLP and Xerox PARC‚Äôs NLP engine?,"<p>I am doing a study on NLP toolkits. I found general purpose NLP toolkits such as openNLP, NLTK etc. But I cannot do a proper study on following toolkits:</p>

<ol>
<li>Microsoft Natural Language Processing tool (MsNLP) - e-rater has used this, but there's no trace of it, even in <a href=""http://research.microsoft.com/en-us/groups/nlp/"" rel=""nofollow"">Microsoft's research site</a>.</li>
<li>Xerox PARC‚Äôs NLP engine - <code>Powerset</code>, the natural language search engine has used this.</li>
</ol>

<p>I want to know:</p>

<ol>
<li>Are there any review on it? </li>
<li>What are their current statuses?</li>
<li>Is it free? Or under which license is it?</li>
</ol>
",Multilingual Language Processing & Language Identification,happened msnlp xerox parc nlp engine study nlp toolkits found general purpose nlp toolkits opennlp nltk etc proper study following toolkits microsoft natural language processing tool msnlp e rater ha used trace even microsoft research site xerox parc nlp engine natural language search engine ha used want know review current status free license
Common substrings on the internet,"<p>Is there a way to find out the most common substrings which are not English words occuring in all the documents(more importantly html) on internet (statistically significant sample would also be good), Is it possible to get some approximations say for example ""corp"" could be  a candidate as its not a whole English word , but ""umbrella"", ""the"" cannot be candidates as they are themselves whole words in English.</p>
",Multilingual Language Processing & Language Identification,common substring internet way find common substring english word occuring document importantly html internet statistically significant sample would also good possible get approximation say example corp could candidate whole english word umbrella candidate whole word english
How to translate words in NTLK swadesh corpus regardless of case - python,"<p>I'm new to python and natural language processing, and I'm trying to learn using the nltk book. I'm doing the exercises at the end of Chapter 2, and there is a question I'm stuck on. 
""In the discussion of comparative wordlists, we created an object called translate which you could look up using words in both German and Italian in order to get corresponding words in English. What problem might arise with this approach? Can you suggest a way to avoid this problem?""</p>

<p>The book had me use the swadesh corpus to create a 'translator', as follows:</p>

<pre><code>`from nltk.corpus import swadesh
fr2en = swadesh.entries(['fr', 'en'])
de2en = swadesh.entries(['de', 'en'])
es2en = swadesh.entries(['es', 'en'])
translate = dict(fr2en)
translate.update(dict(de2en))
translate.update(dict(es2en))`
</code></pre>

<p>One problem I saw was that when you translate the German word for dog (hund) to English, it only takes the uppercase form:
<code>translate['Hund']</code> returns <code>'dog'</code>, while <code>translate['hund']</code> returns <code>KeyError: 'hund'</code></p>

<p>Is there a way to make the translator translate words regardless of case? I've been playing around with it, like doing <code>translate.update(dict(de2en.lower))</code> and what not to no avail. I feel like I'm missing something obvious. Could anyone help me?</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,translate word ntlk swadesh corpus regardless case python new python natural language processing trying learn using nltk book exercise end chapter question stuck discussion comparative wordlists created object called translate could look using word german italian order get corresponding word english problem might arise approach suggest way avoid problem book use swadesh corpus create translator follows one problem saw wa translate german word dog hund english take uppercase form return return way make translator translate word regardless case playing around like avail feel like missing something obvious could anyone help thanks
how to get initial form of an English word - using python?,"<p>I want to get initial form of natural English words, e.g.:</p>

<pre><code>'words' -&gt; 'word'
'Jhon'  -&gt; 'John'
'openning' -&gt; 'open'
</code></pre>

<p>I have tried python Stemer lib:</p>

<pre><code>st=Stemer.Stemer()
for w in ('very', 'words', 'openning'):
print st.stemWord(w),

&gt;&gt;&gt;veri word open
</code></pre>

<p>i expect 'very' but instead got 'veri'</p>

<p>then nltk.corpus.wordnet lib:</p>

<pre><code>from nltk.corpus import wordnet
wordnet.synsets( 'beans' )
[Synset('bean.n.01'),
 &gt;&gt;&gt;Synset('bean.n.02'),
 &gt;&gt;&gt;Synset('bean.n.03'),
 &gt;&gt;&gt;Synset('attic.n.03'),
 &gt;&gt;&gt;Synset('bean.v.01')]
</code></pre>

<p>it give more info but not a quick dictionary. </p>

<p>LancasterStemmer can not get 'english' as 'english':</p>

<pre><code>from nltk.stem.lancaster import LancasterStemmer
st = LancasterStemmer()
st.stem('english')
&gt;&gt;&gt;&gt;'engl'
</code></pre>

<p>enchant lib method check() and sugguest() is not suitable:</p>

<pre><code>&gt;&gt;&gt; import enchant
&gt;&gt;&gt; d = enchant.Dict(""en_US"")
&gt;&gt;&gt; d.check(""Hello"")
</code></pre>

<p>Any method to get quick original form, for a document text?</p>
",Multilingual Language Processing & Language Identification,get initial form english word using python want get initial form natural english word e g tried python stemer lib expect instead got veri nltk corpus wordnet lib give info quick dictionary lancasterstemmer get english english enchant lib method check sugguest suitable method get quick original form document text
In NLP/probability/ML notation: what does a tilde over a letter mean?,"<p>I am reading <a href=""https://dl.dropboxusercontent.com/spa/juxddtvfvlf51yv/paper/convex.pdf"" rel=""nofollow noreferrer"">this</a> paper. In section 1.1 he says:</p>

<p><img src=""https://i.sstatic.net/1hbSM.png"" alt=""enter image description here""></p>

<p>What do the tildes above the letters mean? How can I translate this sentence into ordinary English?</p>
",Multilingual Language Processing & Language Identification,nlp probability ml notation doe tilde letter mean reading paper section say tilde letter mean translate sentence ordinary english
using java for Arabic NLP,"<p>I'm working on Arabic natural language processing such as word stemming, tokenization etc.
In order to deal with words/chars, I need to write arabic letters in java. So, my question is that is it a good practice to write arabic letters in java directly without encoding?
example:
which one is better:</p>

<pre><code>if(word.startsWith(""ÿ™""){...}
</code></pre>

<p>or </p>

<pre><code>if(word.startsWith(""\u1578""){...}
</code></pre>
",Multilingual Language Processing & Language Identification,using java arabic nlp working arabic natural language processing word stemming tokenization etc order deal word char need write arabic letter java question good practice write arabic letter java directly without encoding example one better
"About subject,predicate and object in RDF","<p>This is slightly Off-topic!!!. But please answer to this question.
I have studied lots of articles and materials on net about RDF but i can't understand one thing is how programatically subject, predicate and object is dividing in a natural English line.</p>

<p>Ex: Scott Directed Runner.</p>

<p>If i give this above line, then how the above line is divided into subject,predicate and object with respect to programmatical. please answer.</p>

<p>Thx...  </p>
",Multilingual Language Processing & Language Identification,subject predicate object rdf slightly topic please answer question studied lot article material net rdf understand one thing programatically subject predicate object dividing natural english line ex scott directed runner give line line divided subject predicate object respect programmatical please answer thx
NLP grammar: fixing a to an?,"<p>Is there a way to check if an <code>a</code> should really be an <code>an</code> in a sentence?</p>

<p>For example, it should indeed fix the following:</p>

<p><code>A apple  =&gt;  An apple</code></p>

<p>But the following is correct:</p>

<p><code>A urologist</code></p>

<blockquote>
  <p>In English, the correct choice of a and an depends on the initial
  sound of a word, not on the initial letter, of the word that they
  precede. The letter a should be used before all words beginning with a
  consonant sound except silent h (an honor) and before words beginning
  with vowels that represent combined consonant and vowel sounds
  (university, unit).</p>
</blockquote>
",Multilingual Language Processing & Language Identification,nlp grammar fixing way check really sentence example indeed fix following following correct english correct choice depends initial sound word initial letter word precede letter used word beginning consonant sound except silent h honor word beginning vowel represent combined consonant vowel sound university unit
How do NLP practioners assign the lamba for a feature in a maxent classifier?,"<p>I am watching the <a href=""https://www.youtube.com/watch?v=LixC4OJcc9E"" rel=""nofollow"">coursera NLP video</a> on maxent classfiers. In the video, Christopher Manning assigns lambdas to features based on how strongly they pick out some class. For instance, Manning says (~2:00) the feature = ""proceeding word is in and word is capitalized"" (ex. in Quebec) is likely to pick out a class location and so is given a positive weight of 1.8 -- but the feature = ""word has an accent"" gets a weight of -.4 because in American english it is more likely to pick out names rather than places. Manning then says (~4:21) that ""perceptron algorithms"" and ""support vector machines"" are sometimes used to pick out the weights of features -- before he goes on to show a maxent model that normalizes the feature weights. However, in his maxent example Manning still uses the lambdas that he just picked out around minute 2 of the video. Where do these lambas come from? Does the nlp practitioner just pick them out from domain knowledge (as Manning does) and then thinker with them until the algorithm comes out with the correct values? Is there a more systematic method? Am I misunderstanding something in what is happening in this video?</p>
",Multilingual Language Processing & Language Identification,nlp practioners assign lamba feature maxent classifier watching coursera nlp video maxent classfiers video christopher manning assigns lambda feature based strongly pick class instance manning say feature proceeding word word capitalized ex quebec likely pick class location given positive weight feature word ha accent get weight american english likely pick name rather place manning say perceptron algorithm support vector machine sometimes used pick weight feature go show maxent model normalizes feature weight however maxent example manning still us lambda picked around minute video lambas come doe nlp practitioner pick domain knowledge manning doe thinker algorithm come correct value systematic method misunderstanding something happening video
Split texts into sentences fast (Java),"<p>I have a set of article descriptions where I have to split the texts into sentences. The first implementation uses the opennlp tool sentdetect which works very well, but is too slow for my purpose. 
Is there anything similar to this which performs faster and has an outcome of a similar or slightly worse quality?</p>

<p>Note: I'm working with (a huge amount of) short redactional german texts. </p>
",Multilingual Language Processing & Language Identification,split text sentence fast java set article description split text sentence first implementation us opennlp tool sentdetect work well slow purpose anything similar performs faster ha outcome similar slightly worse quality note working huge amount short redactional german text
Segment multilanguage parallel text,"<p>I have multi-language text that contains a message translated to several languages.
For example:</p>

<pre><code>English message
Russian message
Ukrainian message
</code></pre>

<p>The order is not exact.
I would like to devise some kind of supervised/unsupervised learning algorithm to do the segmentation automatically, and extract each translation in order to create a parallel corpus of data.</p>

<p>Could you suggest any papers/approaches? 
I am not able to get the proper keywords for googling.</p>
",Multilingual Language Processing & Language Identification,segment multilanguage parallel text multi language text contains message translated several language example order exact would like devise kind supervised unsupervised learning algorithm segmentation automatically extract translation order create parallel corpus data could suggest paper approach able get proper keywords googling
Methodology for identifying grocery items from a OCR read,"<p>I'm writing an Ruby application that reads text off of a grocery store receipt and allows a user to see how much they are paying per ounce and possibly serving based on ingredients.  I'm using the Tesseract gem which was pretty straight forward.  However, the line items are often wrong, sometimes comically so, as in the case of ""burly parsley"" for ""curly parsley"".  </p>

<p>I assume solving this problem is in some way a natural language processing problem but I don't have the background currently to even know what direction to go in.  My first idea is to hack the ideas of others, make a Google request and if they suggest something different, use that.  However, I'd like to read and learn how this problem might be solved correctly.</p>

<p>So how should I go about solving my burly parsley problem?</p>
",Multilingual Language Processing & Language Identification,methodology identifying grocery item ocr read writing ruby application read text grocery store receipt allows user see much paying per ounce possibly serving based ingredient using tesseract gem wa pretty straight forward however line item often wrong sometimes comically case burly parsley curly parsley assume solving problem way natural language processing problem background currently even know direction go first idea hack idea others make google request suggest something different use however like read learn problem might solved correctly go solving burly parsley problem
reverse-thesaurus api (no compound words),"<p>i am trying to find a way to do a reverse definition-look-up inside a thesaurus or dictionary. there are some API's for dictionaries and thesaurus available, but few have reverse-lookup function, the ones that do are really limited (e.g. don't consider synonyms). </p>

<p>e.g. desire + emotion + not disgust + not hate ====> results in: love</p>

<p>this is for a project in communication studies. where a limited vocabulary of less than 50 words allows for more sophisticated communication and expression (used for beings that can hold only so many words in their brains)</p>

<p>the objective is to condense a number of source words (usually 3-10) into just one new word or phrase (or a short list of candidates). the new word/phrase would have a roughly similar meaning like the 3-10 words its made of. that new target word/phrase is not part of the dictionary of source words, nor will it join the dictionary. </p>

<p>the 3-10 source word can include negated ones (e.g. NOT tall, NOT fast, ...).</p>

<p>so in a way the 50 source words are my legos, and i want to find out what i can build with them. i want to come up with as many combinations and new words as possible (no compound-words)</p>

<ul>
<li>is there a clever way to do brute force? to include synonyms? </li>
<li>or are there thesauruses that i can download and query myself?</li>
<li>should work with other languages (chinese, japanese, german, ..)</li>
</ul>

<p>i m happy for pointers in any direction.</p>

<p>--</p>

<p>here are some dictionary and thesaurus API's. most big companies offer these, but i m afraid i d overwhelm them if i just did a brute force lookup.. (or they d like me to pay money for that).</p>

<p><a href=""http://words.bighugelabs.com/api.php"" rel=""nofollow"">http://words.bighugelabs.com/api.php</a>
<a href=""https://en.wiktionary.org/w/api.php"" rel=""nofollow"">https://en.wiktionary.org/w/api.php</a></p>
",Multilingual Language Processing & Language Identification,reverse thesaurus api compound word trying find way reverse definition look inside thesaurus dictionary api dictionary thesaurus available reverse lookup function one really limited e g consider synonym e g desire emotion disgust hate result love project communication study limited vocabulary le word allows sophisticated communication expression used hold many word brain objective condense number source word usually one new word phrase short list candidate new word phrase would roughly similar meaning like word made new target word phrase part dictionary source word join dictionary source word include negated one e g tall fast way source word lego want find build want come many combination new word possible compound word clever way brute force include synonym thesaurus download query work language chinese japanese german happy pointer direction dictionary thesaurus api big company offer afraid overwhelm brute force lookup like pay money
Bug with algorithm for determining if app descriptions are written in English,"<p>We index apps on the Apple App Store but need to filter out ones with descriptions not written in English.</p>

<p>Our current algorithm fails, though. It fails, for instance, in scoring <a href=""https://itunes.apple.com/us/app/threes!/id779157948?mt=8"" rel=""nofollow"">this game</a> as not written in English when it is.</p>

<p>Here's the boolean method and the regex we use to determine if something is considered English or not:</p>

<pre><code>  NonEnglishRegex = /[^\040-\176\u2000-\u206F\u2100-\u214F\u2E00-\u2E7F\u3000-\u303F\u00AE\u2605\u272b-\u272e\s]/

  def not_english?(text)
     text.gsub(NonEnglishRegex, '').length.to_f / text.length &lt; 0.95
  end
</code></pre>

<p>Is there a better way? We're using Ruby.</p>
",Multilingual Language Processing & Language Identification,bug algorithm determining app description written english index apps apple app store need filter one description written english current algorithm fails though fails instance scoring game written english boolean method regex use determine something considered english better way using ruby
"Is there a sentence where the POS tagging is right, but the word constituent is wrong?","<p>I'm a beginner to Natural Language Processing and I'm reading about <strong>POS tagging and constituents</strong>. I came across conditions where the constituent of a sentence is right but the POS tagging is wrong.</p>

<p>I used the Stanford Parser. <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/index.jsp</a></p>

<p><strong>For example, ""Madam, I'm Adam"" produces Madam as Adverb which is not right but the constituent is right.</strong></p>

<p>I'm looking for a sentence where the POS tagging is right but the constituent is wrong.
Can the above condition be possible for any sentence ?</p>
",Multilingual Language Processing & Language Identification,sentence po tagging right word constituent wrong beginner natural language processing reading po tagging constituent came across condition constituent sentence right po tagging wrong used stanford parser example madam adam produce madam adverb right constituent right looking sentence po tagging right constituent wrong condition possible sentence
Does SMS parallel corpora in English exist publicly on internet?,"<p>Does SMS parallel corpora in English exist publicly on internet?
I got NUS corpora of SMS but I need parallel corpora - one to one mapping of messages to correct English text.</p>
",Multilingual Language Processing & Language Identification,doe sm parallel corpus english exist publicly internet doe sm parallel corpus english exist publicly internet got nu corpus sm need parallel corpus one one mapping message correct english text
What tag set is used in OpenNLP&#39;s german maxent model?,"<p>currently I am using the OpenNLP tools to PoS-tag german sentences, with the maxent model listed on their <a href=""http://opennlp.sourceforge.net/models-1.5/"">download-site</a>:</p>

<pre>
de      POS Tagger      Maxent model trained on tiger corpus.   de-pos-maxent.bin
</pre>

<p>This works very well and I got results as: </p>

<pre>
Diese, Community, bietet, Teilnehmern, der, Veranstaltungen, die, M√∂glichkeit ...
PDAT, FM, VVFIN, NN, ART, NN, ART, NN ...
</pre>

<p>With the tagged sentences I want to do some further processing where I have to know the meaning of the single tags.  Unforunately searching the <a href=""http://sourceforge.net/apps/mediawiki/opennlp/index.php?title=POS_Tagger"">OpenNLP-Wiki</a> for the tag sets isn't very helpful as it says:</p>

<pre>
TODO: Add more tag sets, also for non-english languages
</pre>

<p>Does anyone know where can I find the tag set used in the german maxent model?</p>
",Multilingual Language Processing & Language Identification,tag set used opennlp german maxent model currently using opennlp tool po tag german sentence maxent model listed todo add tag set also non english language doe anyone know find tag set used german maxent model
Natural language processing: finding semantic similarities with standard algorithms?,"<p>I want to ask if there exists some (collection of) standard algorithms (standard in the sense like the Simplex algorithm for linear optimization) which are scientifically accepted and practically ""good enough"" to sort and rate short (one to three sentences) questions with respect to their similarity. That means that I want some measure to tell that from the following three questions:</p>

<ol>
<li>""Has the weight of Paul's dog increased over the last week?"" </li>
<li>""Is the dog heavier than before?""</li>
<li>""Has Paul a dog named Lucifer?""</li>
</ol>

<p>the first two share the highest common semantic. I know that this is quite a hard problem and I think that any solution down the range towards simple comparison will also serve (failing on the given example, but oh well), my question is, what is current (open source) standard?</p>
",Multilingual Language Processing & Language Identification,natural language processing finding semantic similarity standard algorithm want ask exists collection standard algorithm standard sense like simplex algorithm linear optimization scientifically accepted practically good enough sort rate short one three sentence question respect similarity mean want measure tell following three question ha weight paul dog increased last week dog heavier ha paul dog named lucifer first two share highest common semantic know quite hard problem think solution range towards simple comparison also serve failing given example oh well question current open source standard
Word/Phoneme Corpus for an Elman SRN (English),"<p>I'm writing an Elman Simple Recurrent Network. I want to give it sequences of words, where each word is a sequence of phonemes, and I want a lot of training and test data.</p>

<p>So, what I need is a corpus of English words, together with the phonemes they're made up of, written as something like ARPAbet or SAMPA. British English would be nice but is not essential so long as I know what I'm dealing with. Any suggestions?</p>

<p>I do not currently have the time or the inclination to code something that derives the phonemes a word is comprised of from spoken or written data so please don't propose that.</p>

<p>Note: I'm aware of the <a href=""http://www.speech.cs.cmu.edu/cgi-bin/cmudict"" rel=""nofollow"">CMU Pronouncing Dictionary</a>, but it claims it's only based on the ARPABet symbol set - anyone know if there are actually any differences and if so what they are? (If there aren't any then I could just use that...)</p>

<p>EDIT: CMUPD 0.7a <a href=""https://cmusphinx.svn.sourceforge.net/svnroot/cmusphinx/trunk/cmudict/cmudict.0.7a.symbols"" rel=""nofollow"">Symbol list</a> - vowels may have lexical stress, and there are variants (of ARPABET standard symbols) indicating this.</p>
",Multilingual Language Processing & Language Identification,word phoneme corpus elman srn english writing elman simple recurrent network want give sequence word word sequence phoneme want lot training test data need corpus english word together phoneme made written something like arpabet sampa british english would nice essential long know dealing suggestion currently time code something derives phoneme word comprised spoken written data please propose note aware cmu pronouncing dictionary claim based arpabet symbol set anyone know actually difference could use edit cmupd symbol list vowel may lexical stress variant arpabet standard symbol indicating
How can I generate parse trees of English sentences on iOS?,"<p>I would like to generate constituency-based parsed trees of English sentences within an iOS application.
<a href=""http://en.wikipedia.org/wiki/Parse_tree"" rel=""nofollow"">http://en.wikipedia.org/wiki/Parse_tree</a></p>

<p>My current options appear to be:</p>

<ul>
<li>Write my own tree generation on top of POS tagging from NSLinguisticTagger.</li>
<li>Embed the python-based NLTK into my app.</li>
<li>Create or use a server based approach.</li>
</ul>

<p>Could anyone recommend one of these approaches, or perhaps suggest another?</p>
",Multilingual Language Processing & Language Identification,generate parse tree english sentence io would like generate constituency based parsed tree english sentence within io application current option appear write tree generation top po tagging nslinguistictagger embed python based nltk app create use server based approach could anyone recommend one approach perhaps suggest another
Extremely inefficient and unelegant code,"<p>I have a problem with my Latin translator which I am building for a school project. I have created a grammar for Latin/English sentences. I have created a predicate (possiblesentence(Latin,English).) which can translate a Latin sentence <em>in the right order</em>. However, as Latin sentences are in any order, I have resorted to generating a big list of permutations to find the right order of Latin to translate into English. This is my translate(Latin,English) predicate. This seems very inefficient. Is there a better way of doing this?</p>

<p>I also have another file that loads a big set of predicates from a .csv that look like this:</p>

<pre><code>noun(""femin"",""femin"",1,f,""woman"",""women"").
verb(""port"",""porta"",""portav"",""portat"", 1, carry, carrying, carried).
</code></pre>

<p>Here is my main program: </p>

<pre><code>% Nick's Latin translator

% --------------
% word generator
% --------------

% predicate used to see if any two of the stems join to the ending.

stem_ending_joiner(Stem1,Stem2,Ending,Latin) :-
    (
        append(Stem1,Ending,Latin),
        !
    ;   append(Stem2,Ending,Latin),
        !
    ).

% predicate containing all possible data about nouns including endings
% and such

word_nounx(Latin,Translation,Gender,Case,Number) :-
    noun(Stem1,Stem2,_,Gender,Sgtrans,Pltrans),
    nounending(Ending,Prefix,Number,Case),
    stem_ending_joiner(Stem1,Stem2,Ending,Latin),
    (
        Number = sg,
        Translationw = Sgtrans;
        Number = pl,
        Translationw = Pltrans
    ),
    append(Prefix,"" "",Prefixspace),
    append(Prefixspace,Translationw,Translation).

% predicate containing all possible data about verbs including endings
% and such
% arguments: Latin,Translation,Tense,Number,Mood,Voice

word_verbx(Latin,Translation,Tense,Number,Mood,Voice,Person) :-
    verb(Stem1,_,_,_,_,Ptrans,_,Pastrans),
    verbending(Ending,Tense,Prefix,Number,Mood,Voice,Person),
    stem_ending_joiner(Stem1,_,Ending,Latin),
    (
        Tense = present,
        Translationw = Ptrans;
        Tense = past,
        Translationw = Pastrans
    ),
    append(Prefix,"" "",Prefixspace),
    append(Prefixspace,Translationw,Translation).

% -------
% parsing
% -------

word_noun(Latin,Translation,Gender,Case,Number) :-
    word_nounx(Latinx,Translationx,Gender,Case,Number),
    name(Latin,Latinx),
    name(Translation,Translationx).

word_verb(Latin,Translation,Tense,Number,Mood,Voice,Person) :-
    word_verbx(Latinx,Translationx,Tense,Number,Mood,Voice,Person),
    name(Latin,Latinx),
    name(Translation, Translationx).

pnoun(Gender,Case,Number) --&gt;
   [[English,Latin]],
   {word_noun(Latin,English,Gender,Case,Number)}.
nounphrase(Gender,Case,Number) --&gt;
   pnoun(Gender,Case,Number).
pverb(Tense,Number,Mood,Voice,Person) --&gt;
   [[English,Latin]],
   {word_verb(Latin,English,Tense,Number,Mood,Voice,Person)}.
verbphrase(Tense,Number,Mood,Voice,Person,Gender,Case,Nnumber) --&gt;
   pverb(Tense,Number,Mood,Voice,Person),
   nounphrase(Gender,Case,Nnumber).
sentence --&gt;
   nounphrase(_,nom,Number),
   verbphrase(_,Number,_,_,3,_,nom,_).

% Predicates which manipulate lists of list-pairs to get the first and
% last elements of each list.

headofelements([],[]).
headofelements([H|T],[[H|_]|T1]) :-
    headofelements(T,T1).

tailofelements([],[]).
tailofelements([H|T],[[_|H]|T1]) :-
    tailofelements(T,T1).

lastofelements([],[]).
lastofelements([H|T],[X|T1]) :-
    last(X,N),
    H = N,
    lastofelements(T,T1).

% generates possible sentences with the latin and the english.

possible_sentence(X,N) :-
    phrase(sentence,Y),
    lastofelements(X,Y),
    headofelements(N,Y).

translate(Latin,English) :-
    permutation(Latin,X),
    possible_sentence(X,English).
</code></pre>

<p>Here are the endings:</p>

<pre><code>% --------------------------
% ADJECTIVE ENDINGS
% ------------------------------

adjending(""us"",sg,nom,m).

% -----------------------------------
% VERBENDINGS
% ------------------------------------

verbending(""o"",present,""I"",sg,_,_,1).
verbending(""as"",present,""you"",sg,_,_,2).
verbending(""at"",present,""He"",sg,_,_,3).
verbending(""amus"",present,""We"",pl,_,_,1).
verbending(""atis"",present,""You"",pl,_,_,2).
verbending(""ant"",present,""They"",pl,_,_,3).


% -----------------------------------
% NOUN ENDINGS
% -----------------------------------

nounending(""a"","""",sg,nom).
nounending(""am"","""",sg,acc).
nounending(""ae"",""of"",sg,gen).
nounending(""ae"",""to"",sg,dat).
nounending(""a"",""with"",sg,abl).
nounending(""ae"","""",pl,nom).
nounending(""as"","""",pl,acc).
nounending(""arum"",""of"",pl,gen).
nounending(""is"",""to"",pl,dat).
nounending(""is"",""with"",pl,abl).
</code></pre>
",Multilingual Language Processing & Language Identification,extremely inefficient unelegant code problem latin translator building school project created grammar latin english sentence created predicate possiblesentence latin english translate latin sentence right order however latin sentence order resorted generating big list permutation find right order latin translate english translate latin english predicate seems inefficient better way also another file load big set predicate csv look like main program ending
natural language query processing,"<p>I have a NLP (natural language processing application) running that gives me a tree of the parsed sentence, the questions is then how should I proceed with that.</p>

<pre><code>What is the time
\-SBAR - Suborginate clause
  |-WHNP - Wh-noun phrase
  | \-WP - Wh-pronoun
  |  \-What
  \-S - Simple declarative clause
   \-VP - Verb phrase
     |-VBZ - Verb, 3rd person singular present
     | \-is
     \-NP - Noun phrase
       |-DT - Determiner
       | \-the
       \-NN - Noun, singular or mass
         \-time
</code></pre>

<p>the application has a build in javascript interpreter, and was trying to make the phrase in to a simple function such as </p>

<pre><code>function getReply() {
   return Resource.Time();
}
</code></pre>

<p>in basic terms, what = request = create function, is would be the returned object, and the time would reference the time, now it would be easy just to make a simple parser for that but then we also have what is the time now, or do you know what time it is. I need it to be able to be further developed based on the english language as the project will grow.</p>

<p>the source is C# .Net 4.5</p>

<p>thanks in advance.</p>
",Multilingual Language Processing & Language Identification,natural language query processing nlp natural language processing application running give tree parsed sentence question proceed application ha build javascript interpreter wa trying make phrase simple function basic term request create function would returned object time would reference time would easy make simple parser also time know time need able developed based english language project grow source c net thanks advance
Open-source rule-based pattern matching / information extraction frameworks?,"<p>I'm shopping for an open-source framework for writing natural language grammar rules for pattern matching over annotations. You could think of it like regexps but matching at the token rather than character level. Such a framework should enable the match criteria to reference other attributes attached to the input tokens or spans, as well as modify such attributes in an action. </p>

<p>There are three options I know of which fit this description:</p>

<ul>
<li><a href=""http://gate.ac.uk/sale/tao/splitch8.html#chap%3ajape"" rel=""nofollow"">GATE Java Expressions over Annotations (JAPE)</a></li>
<li><a href=""http://nlp.stanford.edu/software/tokensregex.shtml#Mail"" rel=""nofollow"">Stanford CoreNLP's TokensRegex</a></li>
<li><a href=""http://uima.apache.org/"" rel=""nofollow"">UIMA</a> <a href=""http://uima.apache.org/ruta.html"" rel=""nofollow"">Ruta</a> (<a href=""http://uima.apache.org/gscl13.html#gscl.tutorial"" rel=""nofollow"">Tutorial</a>)</li>
<li><a href=""http://code.google.com/p/graph-expression/"" rel=""nofollow"">Graph Expression (GExp)</a>*</li>
</ul>

<p><strong>Are there any other options like these available at this time?</strong></p>

<p><em>Related Tools</em></p>

<ul>
<li>While I know that general parser generators like <a href=""http://www.antlr.org/"" rel=""nofollow"">Antlr</a> can also serve this purpose, I'm looking for something which are more specifically tailored for natural language processing or information extraction. </li>
<li><a href=""http://uima.apache.org/"" rel=""nofollow"">UIMA</a> includes a <a href=""http://uima.apache.org/d/uima-addons-current/RegularExpressionAnnotator/RegexAnnotatorUserGuide.html"" rel=""nofollow"">Regex Annotator</a> plugin for declaring rules in XML, but appears to operate at the character rather than high-level objects.</li>
<li>I know that this kind of task is often performed with statistical models, but for narrow, structured domains there's benefit in hand-crafting rules.</li>
</ul>

<p>* With GExp 'rules' are actually implemented in code but since there are so few options I chose to include it.</p>
",Multilingual Language Processing & Language Identification,open source rule based pattern matching information extraction framework shopping open source framework writing natural language grammar rule pattern matching annotation could think like regexps matching token rather character level framework enable match criterion reference attribute attached input token span well modify attribute action three option know fit description gate java expression annotation jape stanford corenlp tokensregex uima ruta tutorial graph expression gexp option like available time related tool know general parser generator like antlr also serve purpose looking something specifically tailored natural language processing information extraction uima includes regex annotator plugin declaring rule xml appears operate character rather high level object know kind task often performed statistical model narrow structured domain benefit hand crafting rule gexp rule actually implemented code since option chose include
Split/decompose German words in Python,"<p>I need to split/decompose German composed words in Python. An example:</p>

<pre><code>Donaudampfschiffahrtsgesellschaftskapit√§n
</code></pre>

<p>should be decomposed to:</p>

<pre><code>[Donau, Dampf, Schiff, Fahrt, Gesellschaft, Kapit√§n]
</code></pre>

<p>First I found wordaxe, but it did not work. Than I came across NLTK, but still don't understand if that is smth. I need.</p>

<p>A solution with an example would be really great!</p>
",Multilingual Language Processing & Language Identification,split decompose german word python need split decompose german composed word python example decomposed first found wordaxe work came across nltk still understand smth need solution example would really great
"NLP: Building (small) corpora, or &quot;Where to get lots of not-too-specialized English-language text files?&quot;","<p>Does anyone have a suggestion for where to find archives or collections of everyday English text for use in a small corpus?  I have been using Gutenberg Project books for a working prototype, and would like to incorporate more contemporary language.  A <a href=""https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence#126378"">recent answer</a> here pointed indirectly to a great <a href=""http://us.imdb.com/Reviews/"" rel=""nofollow noreferrer"">archive of usenet movie reviews</a>, which hadn't occurred to me, and is very good.  For this particular program technical usenet archives or programming mailing lists would tilt the results and be hard to analyze, but any kind of general blog text, or chat transcripts, or anything that may have been useful to others, would be very helpful.  Also, a partial or downloadable research corpus that isn't too marked-up, or some heuristic for finding an appropriate subset of wikipedia articles, or any other idea, is very appreciated.</p>

<p>(BTW, I am being a good citizen w/r/t downloading, using a deliberately slow script that is not demanding on servers hosting such material, in case you perceive a moral hazard in pointing me to something enormous.)</p>

<p><strong>UPDATE</strong>:  User S0rin points out that wikipedia requests no crawling and provides <a href=""http://en.wikipedia.org/wiki/Special:Export"" rel=""nofollow noreferrer"">this export tool</a> instead.  Project Gutenberg has a policy specified <a href=""http://www.gutenberg.org/wiki/Gutenberg:Information_About_Robot_Access_to_our_Pages"" rel=""nofollow noreferrer"">here</a>, bottom line, try not to crawl, but if you need to: ""Configure your robot to wait at least 2 seconds between requests.""</p>

<p><strong>UPDATE 2</strong>  The wikpedia dumps are the way to go, thanks to the answerers who pointed them out.  I ended up using the English version from here: <a href=""http://download.wikimedia.org/enwiki/20090306/"" rel=""nofollow noreferrer"">http://download.wikimedia.org/enwiki/20090306/</a> , and a Spanish dump about half the size.  They are some work to clean up, but well worth it, and they contain a lot of useful data in the links.</p>

<hr>
",Multilingual Language Processing & Language Identification,nlp building small corpus get lot specialized english language text file doe anyone suggestion find archive collection everyday english text use small corpus using gutenberg project book working prototype would like incorporate contemporary language archive usenet movie review occurred good particular program technical usenet archive programming mailing list would tilt result hard analyze kind general blog text chat transcript anything may useful others would helpful also partial downloadable research corpus marked heuristic finding appropriate subset wikipedia article idea appreciated btw good citizen w r downloading using deliberately slow script demanding server hosting material case moral hazard pointing something enormous update user rin point wikipedia request crawling provides export tool instead project gutenberg ha policy specified bottom line try crawl need configure robot wait least second request update wikpedia dump way go thanks answerer pointed ended using english version spanish dump half size work clean well worth contain lot useful data link
Artificial Intelligence and Chat Filters,"<p>Are there any chat filters that works depending on the context? I'm talking about the use of new technologies like Artificial Intelligence and Natural Language Processing to determine for example if a word was rude or not, <strong>depending on the context</strong>.</p>
",Multilingual Language Processing & Language Identification,artificial intelligence chat filter chat filter work depending context talking use new technology like artificial intelligence natural language processing determine example word wa rude depending context
English verb inflector,"<p>Does anybody know of an English verb inflector that I can use on a lexicon of verbs (in present-participle) that can give me other inflected forms of the verbs?</p>

<p>For example:</p>

<pre><code>I give it     I get
=========     ======================================
run           ran, running, runs
sing          sang, singing, sings
play          played, playing, plays
</code></pre>
",Multilingual Language Processing & Language Identification,english verb inflector doe anybody know english verb inflector use lexicon verb present participle give inflected form verb example
Is there an algorithm of identifying different forms of &quot;You&quot; in a sentence AKA How to parse an English sentence,"<p>How do I tell if ""You"" is used as a subject or an object in a sentence?</p>
",Multilingual Language Processing & Language Identification,algorithm identifying different form sentence aka parse english sentence tell used subject object sentence
Automatic case conversion of text,"<p>From a remote datasource I get text nibbles (usually no longer than like 100 chars) which are all upper case. This is mostly natural language but with interspersed acronyms and punctionation (like + and -). What I would like to do is to convert this text into a readable form, that is, make most of it lower case, except for acronyms and properly capitalize nouns and names (this is for german where many more words are capitalized than, say, in english).</p>

<p>I'd prefer a solution for Cocoa (OS X), but any other approach is welcome to. I read about NSLinguisticTagger (e.g. <a href=""https://stackoverflow.com/questions/14938867/objective-c-nslinguistictagger-new-york-vs-new-york"">in this question</a>) but it seems that tagging words highly depends on already properly captialized words.</p>
",Multilingual Language Processing & Language Identification,automatic case conversion text remote datasource get text nibble usually longer like char upper case mostly natural language interspersed acronym punctionation like would like convert text readable form make lower case except acronym properly capitalize noun name german many word capitalized say english prefer solution cocoa x approach welcome read nslinguistictagger e g href question seems tagging word highly depends already properly captialized word
Mantain user defined meta data with customised functions for tm_map,"<p>I have a function which I use to translate tokens based on a key/value dictionary.</p>

<pre><code>dictionary &lt;- c(""casa"", ""barco"", ""carro"", ""arbol"")
names(dictionary) &lt;- c(""home"", ""boat"", ""car"", ""tree"")

translate2 &lt;- function (text, dictionary) {
  text_out &lt;- character(0)
  for (i in 1:length(text)) {
    text.split &lt;- strsplit(text[i], ""\\s"")
    translation &lt;- dictionary[unlist(text.split)]
    text_out &lt;- append(text_out, paste(translation, sep="""", collapse="" ""))
  }
  PlainTextDocument(text_out, id = ID(text), author = Author(text))
}
</code></pre>

<p>This function works correctly for the meta `Author: </p>

<pre><code>library(tm)

text &lt;- ""My car is on the tree next to my home under the boat""
corpus &lt;- Corpus(VectorSource(text))
meta(corpus, ""Author"", type=""local"") &lt;- ""Kant""
meta(corpus, ""TextID"", type=""local"") &lt;- ""121212""
meta(corpus[[1]], ""Author"")
# [1] ""Kant""

corpus &lt;- tm_map(corpus, translate2, dictionary)
meta(corpus[[1]], ""Author"")
# [1] ""Kant"" 
corpus[[1]]
# NA carro NA NA NA arbol NA NA NA casa NA NA barco
</code></pre>

<p>But when I try to pass a user-defined meta like <code>TextID</code> with the slightly modified version of the function</p>

<pre><code>translate1 &lt;- function (text, dictionary) {
  text_out &lt;- character(0)
  for (i in 1:length(text)) {
    text.split &lt;- strsplit(text[i], ""\\s"")
    translation &lt;- dictionary[unlist(text.split)]
    text_out &lt;- append(text_out, paste(translation, sep="""", collapse="" ""))
  }
  PlainTextDocument(text_out, id = ID(text), author = Author(text), 
                    TextID = TextID(text))
} 
</code></pre>

<p>I get</p>

<pre><code>text &lt;- ""My car is on the tree next to my home under the boat""
corpus &lt;- Corpus(VectorSource(text))
meta(corpus, ""Author"", type=""local"") &lt;- ""Kant""
meta(corpus, ""TextID"", type=""local"") &lt;- ""121212""
meta(corpus[[1]], ""Author"")
# [1] ""Kant""
meta(corpus[[1]], ""TextID"")
# [1] ""121212""

corpus &lt;- tm_map(corpus, translate1, dictionary)
# Error in PlainTextDocument(text_out, id = ID(text), author = Author(text),  : 
#                              unused argument (TextID = TextID(text)) 
</code></pre>
",Multilingual Language Processing & Language Identification,mantain user defined meta data customised function tm map function use translate token based key value dictionary function work correctly meta author try pas user defined meta like slightly modified version function get
How does Remember the Milk&#39;s string matching work?,"<p>I'm interested in developing a similar solution to RTM's Smart Add Feature. </p>

<p>For those who don't know Remember the Milk here's how it works: Adding tasks is done by means of an input box that accepts strings and parses out different parameters like task name, due date, priority, tags, etc. The parameters are usually preceded by special symbols ( ^, #, &amp;, etc. ). RTM also accepts variations like 'Tennis on Wednesday'. </p>

<p>My basic question to you is how would you design a system that is capable of intelligently discerning different parts of a string. Will I have to look into natural language processing?</p>

<p>Thus far I'm using a simple regex expression that looks for special preceding symbols  ( ^, #, &amp;, etc. ) and then parses out the different parts of the string. This gets increasingly difficult with more and more unordered parameters. maybe that stems from my lack of regex expertise.</p>

<p>A similar problem arises when trying to convert different formats of due dates ( '27 May 2008 16:00', '27th May 2008', '16th June 16:00', 'June 16th 12:00', 'today 12:00am', etc) into datetime objects. I'm currently using Python and regular expressions. My method is to basically run through a long list of possible date and time combinations and convert the matching expression with date.strptime. I found this approach to be hard to maintain; lots of false positives, leftover strings etc. You can look at my code here: <a href=""https://gist.github.com/1233786"" rel=""nofollow"">https://gist.github.com/1233786</a>    It's not pretty, you have been warned.</p>

<p>I'd appreciate any hint into the right direction to approach this topic. Coding a dateparser was really fun but I before I hunt down all the bugs in hundreds of different use cases I thought I check if there's a more elegant design pattern.</p>

<p>P.S.: I would love some code samples to sink my teeth in. Preferably Python :)</p>
",Multilingual Language Processing & Language Identification,doe remember milk string matching work interested developing similar solution rtm smart add feature know remember milk work adding task done mean input box accepts string par different parameter like task name due date priority tag etc parameter usually preceded special symbol etc rtm also accepts variation like tennis wednesday basic question would design system capable intelligently discerning different part string look natural language processing thus far using simple regex expression look special preceding symbol etc par different part string get increasingly difficult unordered parameter maybe stem lack regex expertise similar problem arises trying convert different format due date may th may th june june th today etc datetime object currently using python regular expression method basically run long list possible date time combination convert matching expression date strptime found approach hard maintain lot false positive leftover string etc look code pretty warned appreciate hint right direction approach topic coding dateparser wa really fun hunt bug hundred different use case thought check elegant design pattern p would love code sample sink teeth preferably python
Simple Natural Language Processing Startup for Java,"<p>I am willing to start developing a project on NLP. I dont know much of the tools available. After googling for about a month. I realized that openNLP can be my solution.</p>

<p>Unfortunately i dont see any complete tutorial over using the API. All of them are lacking of some general steps. I need a tutorial from ground level. I have seen a lot of downloads over the site but dont know how to use them? do i need to train or something?.. Here is what i want to know-</p>

<p>How to install / set up a nlp system which can-</p>

<ol>
<li>parse a English sentence words</li>
<li>identify the different parts of speech</li>
</ol>
",Multilingual Language Processing & Language Identification,simple natural language processing startup java willing start developing project nlp dont know much tool available googling month realized opennlp solution unfortunately dont see complete tutorial using api lacking general step need tutorial ground level seen lot downloads site dont know use need train something want know install set nlp system parse english sentence word identify different part speech
How does GATE use ontologies for NLP?,"<p>What is the role of ontologies in natural language processing when using <code>GATE</code>?<br>
As I understand it, at a high level, an ontology allows for the modelling of a domain consisting of classes, their instances, properties of these instances and relationships between classes in the domain.   </p>

<p>However is there an advantage to creating a custom ontology when working with <code>GATE</code>?<br>
Or can processing be as effective using the only the built in processing resources provided by <code>ANNIE</code>?</p>
",Multilingual Language Processing & Language Identification,doe gate use ontology nlp role ontology natural language processing using understand high level ontology allows modelling domain consisting class instance property instance relationship class domain however advantage creating custom ontology working processing effective using built processing resource provided
Sharing state between forked worker processes in a high-performance environment,"<p>This is a follow up to my <a href=""https://stackoverflow.com/questions/20955683/python-multiprocessing-sharing-a-complex-object/"">previous question</a>. As suggested by Tim Peters, using a <code>Manager</code> may not necessarily be the best approach. Unfortunately I've got too much scaffolding code to post a <a href=""http://sscce.org"" rel=""nofollow noreferrer"">SSCCE</a>. Instead, I'll try to provide a detailed explanation of my problem. Please feel free to browse the entire codebase on <a href=""https://github.com/mbatchkarov/thesisgenerator"" rel=""nofollow noreferrer"">Github</a>, but it's a bit of a mess right now.</p>

<h2>Background</h2>

<p>I am doing research in Natural Language Processing and I'd like to do (something like) dictionary-based smoothing for document classification. The idea to train a classifier to associate words and phrases with a correct answer. For example, documents containing the word <code>socialist</code> are likely to be about politics, and those containing the phrase <code>lava temperature</code> are likely about geology. The system is trained by looking at a <strong>small number</strong> of pre-labelled examples. Because language is so varied, a classifier will never ""know about"" all possible phrases that it might encounter in production. </p>

<p>This is where the dictionary comes in. Suppose we had <a href=""https://stackoverflow.com/questions/15173225/how-to-calculate-cosine-similarity-given-2-sentence-strings-python/15173821#15173821"">a cheap and easy way</a> of getting synonyms for almost any phrase out there (I'll cite myself because it's poor taste). When the poor classifier is faced with a phrase it doesn't know about, we could look it up in said dictionary and tell the classifier ""Look, you do not know about <code>communism</code>, but it's kinda like <code>socialist</code>, and you know about that!"". If the dictionary is reasonable, the classifier will generally perform better.</p>

<h2>Pseudo code</h2>

<pre><code>data = Load training and testing documents (300MB on disk)
dictionary = Load dictionary (200MB - 2GB on disk) and place into a `dict` for fast look-ups
Repeat 25 times:
    do_work(data, dictionary)

def do_work(data, dictionary)
    X = Select a random sample of data
    Train a classifier on X
    Y = Select a random sample of data
    Using dictionary, classify all documents in Y
    Write results to disk
</code></pre>

<h2>The problem</h2>

<p>The loop above is a perfect candidate for parallelisation. I have been using a Python 2.7  <code>multiprocessing.Pool</code> (through <code>joblib.Parallel</code>, because it‚Äôs easy and provides very useful traceback if things go south). All worker processes need read-only access to the dictionary and the document collection. There is no need for the workers to communicate with one another or with the parent process- all they do is spawn, do some magic, write a file and die.</p>

<p>The dictionary needs to support fast random access. I do not know what documents the sample <code>Y</code> will contain, so I cannot easily prune the dictionary and pass just the part of it that is needed to each worker. The dictionary will be queried very often- typical hit counts per run are in the millions.
Currently my code is memory-bound as (I believe) copies of the document collection and dictionary are being made for each worker process. When parsed <code>data</code> and <code>dictionary</code> typically use up several GB of RAM. I‚Äôve tried using <code>multiprocessing.managers.BaseManager</code> to avoid copying the large objects, but that slowed the workers down. </p>

<h2>The question</h2>

<p>What other alternatives are there to speed things up? Things I have thought about include:</p>

<ul>
<li>MongoDB/CouchDB/memcached should handle concurrent access well, but I‚Äôm worried about throughput. zeromq was also suggested in a comment to my previous question, haven't had a chance to look into it.</li>
<li>in-memory <code>sqlite</code> databases and database connections cannot be shared across processes, so each worker will need its own connection to an on-disk database. This means a lot of I/O at first and high memory usage as each worker's cache grows.</li>
<li>memory mapping</li>
<li>using threads instead of processes</li>
</ul>

<p><a href=""https://stackoverflow.com/questions/659865/python-multiprocessing-sharing-a-large-read-only-object-between-processes"">This SO question</a> also suggested that many real-world problems that look like they need read-only access to a <code>dict</code> may trigger <code>fork()</code>'s copy-on-write, so it may be impossible to completely avoid making copies of large objects.</p>
",Multilingual Language Processing & Language Identification,sharing state forked worker process high performance environment follow sscce instead try provide detailed explanation problem please feel free browse entire codebase github bit mess right background research natural language processing like something like dictionary based smoothing document classification idea train classifier associate word phrase correct answer example document containing word likely politics containing phrase likely geology system trained looking small number pre labelled example language varied classifier never know possible phrase might encounter production dictionary come suppose pseudo code problem loop perfect candidate parallelisation using python easy provides useful traceback thing go south worker process need read access dictionary document collection need worker communicate one another parent process spawn magic write file die dictionary need support fast random access know document sample contain easily prune dictionary pas part needed worker dictionary queried often typical hit count per run million currently code memory bound believe copy document collection dictionary made worker process parsed typically use several gb ram tried using avoid copying large object slowed worker question alternative speed thing thing thought include mongodb couchdb memcached handle concurrent access well worried throughput zeromq wa also suggested comment previous question chance look memory database database connection shared across process worker need connection disk database mean lot first high memory usage worker cache grows memory mapping using thread instead process href question also suggested many real world problem look like need read access may trigger copy write may impossible completely avoid making copy large object
"In Latent Semantic Analysis, how do you recombine the decomposed matrices after truncating the singular values?","<p>I'm reading <strong><em><a href=""http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf"" rel=""nofollow"">Matrix decompositions and latent semantic indexing</a></em></strong> (Online edition ¬©
2009 Cambridge UP)</p>

<p>I'm trying to understand how you reduce the number of dimensions in a matrix. There's an example on page 13 which I'm trying to replicate using <a href=""http://www.numpy.org/"" rel=""nofollow"">Python's numpy</a>.</p>

<p>Let's call the original occurrence matrix ""a"" and the three <a href=""https://en.wikipedia.org/wiki/Singular_value_decomposition"" rel=""nofollow"">SVD</a> (Singular Value Decomposition) decomposed matrices ""U"", ""S"" and ""V"".</p>

<p>The trouble I'm having is that after I zero out the smaller singular values in ""S"", when I multiply together ""U"", ""S"" and ""V"" using numpy, the answer is not as it is given in the pdf. The bottom 3 rows are not all zeros. The funny thing is that when I just multiply ""S"" and ""V"" I get the right answer.</p>

<p>This is sort of surprising but multiplying ""S"" and ""V"" is actually what Manning and Schutze's book Foundations of Statistical Natural Language Processing says you have to do. But this is not what the pdf says you have to do in page 10.</p>

<p>So what's going on here?</p>
",Multilingual Language Processing & Language Identification,latent semantic analysis recombine decomposed matrix truncating singular value reading matrix decomposition latent semantic indexing online edition cambridge trying understand reduce number dimension matrix example page trying replicate using python numpy let call original occurrence matrix three svd singular value decomposition decomposed matrix u v trouble zero smaller singular value multiply together u v using numpy answer given pdf bottom row zero funny thing multiply v get right answer sort surprising multiplying v actually manning schutze book foundation statistical natural language processing say pdf say page going
Getting all words and punctiation from English text,"<p>What I want to do:</p>

<p>User loads the text. I analyse it and get all words and punctuation from it. Now I can easy render text for other users with fast translation of each word or additional info of analysed words.</p>

<p>Now I'm trying to use <a href=""https://github.com/louismullie/treat"" rel=""nofollow"">treat</a> gem(NLP for ruby) but there are many problems with it.</p>

<p>For example in sentence</p>

<p>""The world ain't all sunshine and rainbows.""</p>

<p>It divides ain't in two words ""ai"" and ""n't""</p>

<p>Can anybody suggest some libraries or gem, maybe which I can implement with jruby where I can just separate text in words and punctuations without problems.</p>

<p>Or mb I'm wrong in my ideas and there is any other ways?</p>
",Multilingual Language Processing & Language Identification,getting word punctiation english text want user load text analyse get word punctuation easy render text user fast translation word additional info analysed word trying use treat gem nlp ruby many problem example sentence world sunshine rainbow divide two word ai n anybody suggest library gem maybe implement jruby separate text word punctuation without problem mb wrong idea way
How to build short sentences with a small letter set restriction?,"<p>I'm looking for a way to write a program that creates short german sentences with a restricted letter set. The sentences can be nonsense but should grammatically be correct. The following examples only contain the letters ""aeilmnost"":</p>

<ul>
<li>""Antonia ist mit Tina im Tal.""</li>
<li>""Tamina malt mit lila Tinte Enten.""</li>
<li>""Tina nimmt alle Tomaten mit.""</li>
</ul>

<p>For this task I need a dictionary like <a href=""http://sourceforge.net/projects/germandict/files/"" rel=""nofollow noreferrer"">this one</a> (found in the answer to <a href=""https://german.stackexchange.com/q/491"">""Where can I find a parsable list of German words?""</a>). The research area for programatically create text is <a href=""http://en.wikipedia.org/wiki/Natural_language_generation"" rel=""nofollow noreferrer"">NLG - Natural Language Generation</a>. On the NLG-Wiki I found a large <a href=""http://www.nlg-wiki.org/systems/Table_of_NLG_systems"" rel=""nofollow noreferrer"">table of NLG systems</a>. I picked two from the list, which could be appropriate:</p>

<ul>
<li><a href=""https://code.google.com/p/simplenlg/"" rel=""nofollow noreferrer"">SimpleNLG</a> - a Java API, which has also an <a href=""http://www.linguistics.rub.de/~bollmann/simplenlg-ger.html"" rel=""nofollow noreferrer"">adaption for the german language</a></li>
<li><a href=""http://www.fb10.uni-bremen.de/anglistik/langpro/NLG-table/details/KOMET.htm"" rel=""nofollow noreferrer"">KOMET</a> - multilingual generation, from University Bremen</li>
</ul>

<p>Do you have worked with a NLG library and have some advice which one to use for building short sentences with a letter set restriction?
Can you recommend a paper to this topic?</p>
",Multilingual Language Processing & Language Identification,build short sentence small letter set restriction looking way write program creates short german sentence restricted letter set sentence nonsense grammatically correct following example contain letter aeilmnost antonia ist mit tina im tal tamina malt mit lila tinte enten tina nimmt alle tomaten mit task need dictionary like one found answer research area programatically create text nlg natural language generation nlg wiki found large table nlg system picked two list could appropriate simplenlg java api ha also adaption german language komet multilingual generation university bremen worked nlg library advice one use building short sentence letter set restriction recommend paper topic
SVM for text classification - tutorial on machine learning? How do I get started?,"<p>I'm looking for a really good tutorial on machine learning for text classification perhaps using Support vector machine (SVM) or other appropriate technology for large-scale supervised text classification. If there isn't a great tutorial, can anyone give me pointers to how a beginner should get started and do a good job with things like feature detection for English language Text Classification. </p>

<p>Books, articles, anything that can help beginners get started would be super helpful! </p>
",Multilingual Language Processing & Language Identification,svm text classification tutorial machine learning get started looking really good tutorial machine learning text classification perhaps using support vector machine svm appropriate technology large scale supervised text classification great tutorial anyone give pointer beginner get started good job thing like feature detection english language text classification book article anything help beginner get started would super helpful
Abbreviation detection,"<p>Under what field of study under natural language processing does abbreviation detection come? Looking for sources to learn abbreviation detection. I have considered Semantics, which basically detect synonyms. so i thought i might do multi-word semantics that would detect that ""nlp"" and ""natural language processing"" are similar. but i have found NO solution to do multi-word semantics. </p>

<p><strong>Note:</strong> I know its really easy to down vote this question, but try to understand my problem. I have struggled for months now and any help is GREATLY appreciated...</p>

<p>Thankyou</p>
",Multilingual Language Processing & Language Identification,abbreviation detection field study natural language processing doe abbreviation detection come looking source learn abbreviation detection considered semantics basically detect synonym thought might multi word semantics would detect nlp natural language processing similar found solution multi word semantics note know really easy vote question try understand problem struggled month help greatly appreciated thankyou
Natural language processing / commands (prolog),"<p>I want to create a predicate, which recognizes a word (in this case: ""save"") and starts saving the next words, until the sign/word ""end"" comes.</p>

<p>It should work like this:</p>

<pre><code>?- save.
one
two
end
true.
</code></pre>

<p>The predicate for saving:</p>

<pre><code>save(X) :- assert(listitem(X)).
</code></pre>

<p>and then I started like this:</p>

<pre><code>save :- read(save).
read:- X -&gt; save(X).
end --&gt; 'end'.
</code></pre>

<p>The problem is, that I can add as much words as I want, but if I want to stop the command with ""end"", the program fails and actually the words have not been saved.</p>

<p>What part of the predicate is wrong? I'd be very happy for some help.
Thank you in advance!</p>
",Multilingual Language Processing & Language Identification,natural language processing command prolog want create predicate recognizes word case save start saving next word sign word end come work like predicate saving started like problem add much word want want stop command end program fails actually word saved part predicate wrong happy help thank advance
NLP and english dictionary database?,"<p>Is there an open dictionary database where I can get at minimum a table of the sort:
word | part of speech ?</p>

<p>Ideally I would also like antonym and synonym links to other words.</p>
",Multilingual Language Processing & Language Identification,nlp english dictionary database open dictionary database get minimum table sort word part speech ideally would also like antonym synonym link word
"How to recognize if word has no meaning, maybe some impossible syllables?","<p>Initially, I have <strong>m</strong> arrays of <strong>n</strong> characters, where each array contains unknown (for me) character of needed word (condition: word has meaning).</p>

<p>For example, <strong>m</strong> = 4, <strong>n</strong> = 3: array0 = {'<em>t</em>', '<em>e</em>', '<em>c</em>'}, array1 = {'<em>g</em>' '<em>o</em>' '<em>a</em>'}, array2 = {'<em>w</em>' '<em>d</em>' '<em>y</em>'}, array3 = {'<em>e</em>' '<em>o</em>' '<em>s</em>'}. Each array contains only one correct letter: in array0 is first letter, in array1 - second... So, the probable secret word is '<em>code</em>': array0[2] = '<em>c</em>', array1[1] = '<em>o</em>', array2[1] = '<em>d</em>', array3[0] = '<em>e</em>'. </p>

<p>I need to find all of existing letter-combinations, i.e. exclude generated meaningless words.
Are there any rules/regularities of 'impossible' syllables/letter-combinations in English?</p>

<p>I'm attacking Vigenere's cipher. So, I know the length of key and its probable characters. I'm shuffling my arrays and getting many meaningless words. Problem is to filter them. As I get it, some conditions can help to recognize incorrect words. For example, if word length is > 4 then all vowel chars, or all consonant chars word is wrong. Some syllables, such as <em>kk</em> *hh* <em>ww</em>, in general, are impossible too. Where can I find such rules?</p>
",Multilingual Language Processing & Language Identification,recognize word ha meaning maybe impossible syllable initially array n character array contains unknown character needed word condition word ha meaning example n array e c array g array w array e array contains one correct letter array first letter array second probable word code array c array array array e need find existing letter combination e exclude generated meaningless word rule regularity impossible syllable letter combination english attacking vigenere cipher know length key probable character shuffling array getting many meaningless word problem filter get condition help recognize incorrect word example word length vowel char consonant char word wrong syllable kk hh ww general impossible find rule
"Pharse level dependency parser using java,nlp","<p>Can someone please elaborate on how to obtain "" pharse level dependency"" using the Stanfords's Natural Language Processing Lexical Parser- open source Java code?
<a href=""http://svn.apache.org/repos/asf/nutch/branches/branch-1.2/src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/RobotRulesParser.java"" rel=""nofollow"">http://svn.apache.org/repos/asf/nutch/branches/branch-1.2/src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/RobotRulesParser.java</a></p>

<p><a href=""http://docs.mongodb.org/manual/reference/sql-comparison/"" rel=""nofollow"">http://docs.mongodb.org/manual/reference/sql-comparison/</a></p>

<p>such as </p>

<p>pharse               dependency </p>

<p>The accident --------->happened</p>

<p>falling    ---------> as</p>

<p>the night ---------->falling</p>

<p>as such as many more...</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,pharse level dependency parser using java nlp someone please elaborate obtain pharse level dependency using stanford natural language processing lexical parser open source java code pharse dependency accident happened falling night falling many thanks
Automatically Generate all words given root,"<p>I am working on my research in the area of Natural Language Processing and for some result I have to automatically generate all words given the root of the word.
For example- User enter word -activate. The root word for this is- activ. 
The numbers of words that can be generated from root activ are -""activate,activated,activating
activates,activation,activator.</p>

<p>So I want a method to generate all this 6 words given the root activ.   </p>
",Multilingual Language Processing & Language Identification,automatically generate word given root working research area natural language processing result automatically generate word given root word example user enter word activate root word activ number word generated root activ activate activated activating activates activation activator want method generate word given root activ
How to do a Tree Transfer in prolog for MT,"<p>I need to find a way to Transfer a parse tree in to another with different order.
It is for machine translation project with two languages with SVO and SOV architecture.</p>

<pre><code>t1 = s(np(n(he)), vp( v(went), np(n(home))))
</code></pre>

<p>and I want it to be</p>

<pre><code>t2 = s(np(n(he)), vp( np(n(home)), v(went)))
</code></pre>

<p>according to a rule that represent t1 represent the SVO language and t2 represent the SOV language architecture.</p>

<p>And the rule set should be applicable for complex sentences with adjectives and adverbs.</p>

<pre><code>t1 = s(np(n(he)), vp( v(went), np(adj(his), n(home))))

t2 = s(np(n(he)), vp( np(adj(his), n(home)), v(went)))
</code></pre>

<p>Any comment would be useful</p>

<p>thanks Mathee</p>
",Multilingual Language Processing & Language Identification,tree transfer prolog mt need find way transfer parse tree another different order machine translation project two language svo sov architecture want according rule represent represent svo language represent sov language architecture rule set applicable complex sentence adjective adverb comment would useful thanks mathee
Designing an NLP API in Javascript,"<p>I'm working on <a href=""https://github.com/erick-fernandes/nat-js"" rel=""nofollow"">nat-js</a> (a NLP toolkit in Javascript) and I'm planning to extend it to process other languages, like English and Spanish; today it process only Portuguese.</p>

<p>With nat-js, when you need a tokenizer, you write something like this:</p>

<pre><code>var tkz = new nat.tokenizer();
</code></pre>

<p>But how could it be done to process other languages? What do you think about this:</p>

<pre><code>var nat = new natFactory('pt');
var tkz = new nat.tokenizer();
</code></pre>

<p>And there is something I've been thinking about: why load the full library if you're are needing only the tokenizer in Portuguese? Can I offer a simple way to load only the required files?</p>
",Multilingual Language Processing & Language Identification,designing nlp api javascript working nat j nlp toolkit javascript planning extend process language like english spanish today process portuguese nat j need tokenizer write something like could done process language think something thinking load full library needing tokenizer portuguese offer simple way load required file
Natural Language Processing for Abstract and Concrete Text?,"<p>I'm looking for a tool/wordlist that can identify <a href=""http://www.cse.unsw.edu.au/~billw/nlpdict.html#abstractnoun"" rel=""noreferrer"">abstract</a> and <a href=""http://www.cse.unsw.edu.au/~billw/nlpdict.html#concretenoun"" rel=""noreferrer"">concrete</a> adjectives and nouns. The closest thing I've found is Euro Wordnet, which identifies entities. <a href=""http://www.illc.uva.nl/EuroWordNet/corebcs/ewnTopOntology.html"" rel=""noreferrer"">First word entities</a> are concrete in nature. Second and third levels are abstract. However, Euro Wordnet is not free, and the wordlist is over a decade old.</p>

<p>Anyone done this?</p>
",Multilingual Language Processing & Language Identification,natural language processing abstract concrete text looking tool wordlist identify abstract concrete adjective noun closest thing found euro wordnet identifies entity first word entity concrete nature second third level abstract however euro wordnet free wordlist decade old anyone done
Natural Language to Binary Fact,"<p>I would like to take a set of English sentences and convert those to a set of relations.  e.g.</p>

<p>""A pilot flies an airplane."" would map to something like the following relation:</p>

<pre><code>flies(pilot, airplane)
</code></pre>

<p>""Bob is the father of Alice and Doug."" would map to</p>

<pre><code>father(Bob, Alice)
father(Bob, Doug)
</code></pre>

<p>I know that I have seen a python library to do something like this before, but despite all of my searching I haven't been able to find that.  I have never done NLP programing before, so I may be using terms incorrectly.  My apologies.</p>

<p>UPDATE:
This is not an effort to generate Prolog, but to generate binary (and other) facts about a universe of discourse.  These facts could then be inserted into an object-role model, and help to generate a database schema.  The ""flies"" fact above is just to illustrate, and there is no requirement around the syntax of the output.  It just has to be a regular output.</p>

<p>In linguistic terms, I guess I would like to see a verb phrase and two noun phrases to capture a binary fact.  The way those are structured is not as important as being able to discern the structure in an automated manner.</p>
",Multilingual Language Processing & Language Identification,natural language binary fact would like take set english sentence convert set relation e g pilot fly airplane would map something like following relation bob father alice doug would map know seen python library something like despite searching able find never done nlp programing may using term incorrectly apology update effort generate prolog generate binary fact universe discourse fact could inserted object role model help generate database schema fly fact illustrate requirement around syntax output ha regular output linguistic term guess would like see verb phrase two noun phrase capture binary fact way structured important able discern structure automated manner
Natural language identification of source-code comments,"<p>currently I'm researching the natural language identification (NLI) of source-code comments.</p>

<p>I'm testing and comparing the up-to-date algorithms adressing this problem.</p>

<p>Do you know any open-source projects, which have comments in different natural languages? Preferably a well-known one.</p>

<p>Thank you very much,
Timo</p>
",Multilingual Language Processing & Language Identification,natural language identification source code comment currently researching natural language identification nli source code comment testing comparing date algorithm adressing problem know open source project comment different natural language preferably well known one thank much timo
"Extracting related text given a sentence, keywords or topic","<p>Are there any known ways (above and beyond statistical analysis, but not necessarily excluding it as being part of the solution) to relate sentences or concepts to one another using Natural Language Processing. Thus far I've only worked with NLTK and Stanford-NLP to aid in my project, but I am open to alternative open source solutions.</p>

<p>As an example take the following George Orwell essay (<a href=""http://orwell.ru/library/essays/wiw/english/e_wiw"" rel=""nofollow"">http://orwell.ru/library/essays/wiw/english/e_wiw</a>). Suppose I gave the application the sentence </p>

<pre><code>""What are George Orwell's opinions on writers."" 
</code></pre>

<p>or perhaps  </p>

<pre><code>""George Orwell believes writers enjoy writing to express their creativity, to make a point and for their egos.""
</code></pre>

<p>Might yield lines from the essay like</p>

<pre><code>""The aesthetic motive is very feeble in a lot of writers, but even a pamphleteer or writer of textbooks will have pet words and phrases which appeal to him for non-utilitarian reasons; or he may feel strongly about typography, width of margins, etc.""
</code></pre>

<p>or</p>

<pre><code>""Serious writers, I should say, are on the whole more vain and self-centered than journalists, though less interested in money.""
</code></pre>

<p>I understand that this is not easy and I may not achieve much accuracy, but I was hoping for ideas on what already exists and what I could try to start off, or at least get the best results possible based on what is already known and out there.</p>
",Multilingual Language Processing & Language Identification,extracting related text given sentence keywords topic known way beyond statistical analysis necessarily excluding part solution relate sentence concept one another using natural language processing thus far worked nltk stanford nlp aid project open alternative open source solution example take following george orwell essay suppose gave application sentence perhaps might yield line essay like understand easy may achieve much accuracy wa hoping idea already exists could try start least get best result possible based already known
What are some examples of Machine Translation applications/libraries currently being developed?,"<p>I'm interested in learning more about Machine Translation.  While I have some very interesting books on the matter, I'd like to see some real world applications of MT's theories.</p>

<p>I've found a couple open source projects just by searching around:</p>

<p><a href=""http://www.apertium.org/"">Apertium</a></p>

<p><a href=""http://www.statmt.org/moses/"">Moses</a></p>

<p>So, does anyone have any other examples?  I'm looking for active projects; stuff which has not been abandoned.</p>
",Multilingual Language Processing & Language Identification,example machine translation application library currently developed interested learning machine translation interesting book matter like see real world application mt theory found couple open source project searching around doe anyone example looking active project stuff ha abandoned
Selecting the most fluent text from a set of possibilities via grammar checking (Python),"<h1>Some background</h1>

<p>I am a literature student at New College of Florida, currently working on an overly ambitious creative project. <strong>The project is geared towards the algorithmic generation of poetry</strong>. It's written in Python. My Python knowledge and Natural Language Processing knowledge come only from teaching myself things through the internet. I've been working with this stuff for about a year, so I'm not helpless, but at various points I've had trouble moving forward in this project. Currently, I am entering the final phases of development, and have hit a little roadblock.</p>

<p><strong>I need to implement some form of grammatical normalization, so that the output doesn't come out as un- conjugated/inflected caveman-speak.</strong> About a month ago some friendly folks on SO <a href=""https://stackoverflow.com/questions/8541447/some-nlp-stuff-to-do-with-grammar-tagging-stemming-and-word-sense-disambiguat"">gave me some advice on how I might solve this issue</a> by using an <strong>ngram language modeller</strong>, basically -- but I'm looking for yet other solutions, as it seems that NLTK's NgramModeler is not fit for my needs. (The possibilities of POS tagging were also mentioned, but my text may be too fragmentary and strange for an implementation of such to come easy, given my amateur-ness.)</p>

<h1>Perhaps I need something like AtD, but hopefully less complex</h1>

<p><strong>I think need something that works like <a href=""http://afterthedeadline.com/"" rel=""nofollow noreferrer"">After the Deadline</a></strong> or <a href=""http://queequeg.sourceforge.net/index-e.html"" rel=""nofollow noreferrer"">Queequeg</a>, but neither of these seem exactly right. Queequeg is probably not a good fit -- it was written in 2003 for Unix and I can't get it working on Windows for the life of me (have tried everything). But I like that all it checks for is proper verb conjugation and number agreement.</p>

<p>On the other hand, AtD is much more rigorous, offering more capabilities than I need. But I can't seem to get the <a href=""http://blog.afterthedeadline.com/2009/09/15/python-bindings-for-atd/"" rel=""nofollow noreferrer"">python bindings</a> for it working. (I get 502 errors from the AtD server, which I'm sure are easy to fix, but my application is going to be online, and I'd rather avoid depending on another server. I can't afford to run an AtD server myself, because the number of ""services"" my application is going to require of my web host is already threatening to cause problems in getting this application hosted cheaply.)</p>

<h2>Things I'd like to avoid</h2>

<p><strong>Building Ngram language models myself doesn't seem right for the task.</strong> my application throws a lot of unknown vocabulary, skewing all the results. (Unless I use a corpus that's so large that it runs way too slow for my application -- the application needs to be pretty snappy.)</p>

<p><strong>Strictly checking grammar is neither right for the task.</strong> the grammar doesn't need to be perfect, and the sentences don't have to be any more sensible than the kind of English-like jibberish that you can generate using ngrams. Even if it's jibberish, I just need to enforce verb conjugation, number agreement, and do things like remove extra articles.</p>

<p>In fact, I don't even need any kind of <em>suggestions</em> for corrections. I think all I need is for something to tally up how many errors seem to occur in each sentence in a group of possible sentences, so I can sort by their score and pick the one with the least grammatical issues.</p>

<h1>A simple solution? Scoring fluency by detecting obvious errors</h1>

<p>If a script exists that takes care of all this, I'd be overjoyed (I haven't found one yet). I can write code for what I can't find, of course; I'm looking for advice on how to optimize my approach.</p>

<p>Let's say we have a tiny bit of text already laid out:</p>

<p><code>existing_text = ""The old river""</code></p>

<p>Now let's say my script needs to figure out which inflection of the verb ""to bear"" could come next. I'm open to suggestions about this routine. <strong>But I need help mostly with step #2</strong>, rating fluency by tallying grammatical errors:</p>

<ol>
<li>Use the Verb Conjugation methods in <a href=""http://nodebox.net/code/index.php/Linguistics"" rel=""nofollow noreferrer"">NodeBox Linguistics</a> to come up with all conjugations of this verb; <code>['bear', 'bears', 'bearing', 'bore', 'borne']</code>.</li>
<li>Iterate over the possibilities, (shallowly) checking the grammar of the string resulting from <code>existing_text + "" "" + possibility</code> (""The old river bear"", ""The old river bears"", etc). Tally the error count for each construction. In this case the only construction to raise an error, seemingly, would be ""The old river bear"".</li>
<li>Wrapping up should be easy... Of the possibilities with the lowest error count, select randomly.</li>
</ol>
",Multilingual Language Processing & Language Identification,selecting fluent text set possibility via grammar checking python background literature student new college florida currently working overly ambitious creative project project towards algorithmic generation poetry written python python knowledge natural language processing knowledge come teaching thing internet working stuff year helpless various point trouble moving forward project currently entering final phase development hit little roadblock need implement form grammatical normalization output come un conjugated inflected caveman speak month ago friendly folk deadline queequeg neither seem exactly right queequeg probably good fit wa written unix get working window life tried everything like check proper verb conjugation number agreement hand atd much rigorous offering capability need seem get python binding working get error atd server sure easy fix application going online rather avoid depending another server afford run atd server number service application going require web host already threatening cause problem getting application hosted cheaply thing like avoid building ngram language model seem right task application throw lot unknown vocabulary skewing result unless use corpus large run way slow application application need pretty snappy strictly checking grammar neither right task grammar need perfect sentence sensible kind english like jibberish generate using ngrams even jibberish need enforce verb conjugation number agreement thing like remove extra article fact even need kind suggestion correction think need something tally many error seem occur sentence group possible sentence sort score pick one least grammatical issue simple solution scoring fluency detecting obvious error script exists take care overjoyed found one yet write code find course looking advice optimize approach let say tiny bit text already let say script need figure inflection verb bear could come next open suggestion routine need help mostly step rating fluency tallying grammatical error use verb conjugation method nodebox linguistics come conjugation verb iterate possibility shallowly checking grammar string resulting old river bear old river bear etc tally error count construction case construction raise error seemingly would old river bear wrapping easy possibility lowest error count select randomly
Russian Document Corpus for Search Engine,"<p>I'm working on a cross language information retrieval that takes queries in english and searches documents in Russian. To evaluate this system it would be nice to have a collection of russian documents to search through. Does anyone out there know of a collection of documents I can search or websites from which I can easily scrape together a bunch of russian documents (aside from wikipedia)?</p>

<p>Documents can be about anything though it would be nice if they were in some specific area of human knowledge (CS, architecture, engineering, art, literature analysis, whatever...)</p>
",Multilingual Language Processing & Language Identification,russian document corpus search engine working cross language information retrieval take query english search document russian evaluate system would nice collection russian document search doe anyone know collection document search website easily scrape together bunch russian document aside wikipedia document anything though would nice specific area human knowledge c architecture engineering art literature analysis whatever
replace words like this with entities it is referring to,"<p>I am new to natural language processing and I want to replace words like this with what it actually refers to:</p>

<p>An example would be: </p>

<p><strong>segment topic:</strong> My brand new dell laptop</p>

<p><strong>Segment text:</strong> I bought this laptop and its good</p>

<p>How do i</p>

<ul>
<li><p>detect that the segment is talking about a laptop </p></li>
<li><p>replace the occurrence of this laptop with My brand new dell laptop  (especially in mutli topic documents)</p></li>
</ul>
",Multilingual Language Processing & Language Identification,replace word like entity referring new natural language processing want replace word like actually refers example would segment topic brand new dell laptop segment text bought laptop good detect segment talking laptop replace occurrence laptop brand new dell laptop especially mutli topic document
nlp: alternate spelling identification,"<p>Help by editing my question title and tags is greatly appreciated!</p>

<p>Sometimes one participant in my corpus of ""conversations"" will refer to another participant using a nickname, usually an abbreviation or misspelling, but hereafter I'll just say ""nicknames"". Let's say I'm willing to manually tell my software whether or not I think various possible nicknames are in fact nicknames, but I want software to come up with a list of possible matches between the handle's that identify people, and the potential nicknames. How would I go about doing that?</p>

<p>Background on me and then my corpus: I have no experience doing natural language processing but I'm a competent data analyst with R. My data is produced by 70 teams, each forecasting the likelihood of 100 distinct events occurring some time in the future. The result that I have 70 x 100 = 7000 text files, containing the stream of forecasts participants make and the comments they include with their forecasts. I'll paste a very short snip of one of these text files below, this one had to do with whether the Malian government would enter talks with the MNLA:</p>

<blockquote>
  <p>02/12/2013 20:10: past_returns answered Yes: (50%)</p>
  
  <p>I hadn't done a lot of research when I put in my previous
  placeholder... I'm bumping up a lot due to DougL's forecast</p>
  
  <p>02/12/2013 19:31: DougL answered Yes: (60%)</p>
  
  <p>Weak President Traore wants talks if MNLA drops territorial claims.
  Mali's military may not want talks. France wants talks. MNLA sugggests
  it just needs autonomy. But in 7 weeks?</p>
  
  <p>02/12/2013 10:59: past_returns answered No: (75%)</p>
  
  <p>placeholder forecast...
  <a href=""http://www.irinnews.org/Report/97456/What-s-the-way-forward-for-Mali"" rel=""nofollow"">http://www.irinnews.org/Report/97456/What-s-the-way-forward-for-Mali</a></p>
</blockquote>

<p>My initial thoughts: Obviously I can start by providing the names I'm looking to match things up with... in the above example they would be past_returns and DougL (though there is no use of nicknames in the above). I wouldn't think it'd be that hard to get a computer to guess at minor misspellings (though I wouldn't personally know where to start). I can imagine that other tricks could be used, like assuming that a string is more likely to be a nickname if it is used much much more by one team, than by other teams. A nickname is more likely to refer to someone who spoke recently than someone who spoke long ago, or not at all on regarding this question. And they should be used in sentences in a manner similar to the way the full name/screenname is typically used in the corpus.  But I'm interested to hear about simple approaches, as well as ones that try to consider more sophisticated techniques.</p>
",Multilingual Language Processing & Language Identification,nlp alternate spelling identification help editing question title tag greatly appreciated sometimes one participant corpus conversation refer another participant using nickname usually abbreviation misspelling hereafter say nickname let say willing manually tell software whether think various possible nickname fact nickname want software come list possible match handle identify people potential nickname would go background corpus experience natural language processing competent data analyst r data produced team forecasting likelihood distinct event occurring time future result x text file containing stream forecast participant make comment include forecast paste short snip one text file one whether malian government would enter talk mnla past return answered yes done lot research put previous placeholder bumping lot due dougl forecast dougl answered yes weak president traore want talk mnla drop territorial claim mali military may want talk france want talk mnla sugggests need autonomy week past return answered placeholder forecast initial thought obviously start providing name looking match thing example would past return dougl though use nickname think hard get computer guess minor misspelling though personally know start imagine trick could used like assuming string likely nickname used much much one team team nickname likely refer someone spoke recently someone spoke long ago regarding question used sentence manner similar way full name screenname typically used corpus interested hear simple approach well one try consider sophisticated technique
Semantic search system in Java,"<p>I want to implement a semantic search system in Java. Sesame will be embedded into my system to store and manipulate rdf data directly, and I want to use Tomcat, JSP and Servlet. But I also need to do natural language processing, which I know Python is really good at. So it there any way that I can merge Python code in my Java web codes? Or is there any good tools dealing with NLP in java? 
I think I'm a little confused since I know little about NLP area.</p>

<p>Thanks in advance! </p>
",Multilingual Language Processing & Language Identification,semantic search system java want implement semantic search system java sesame embedded system store manipulate rdf data directly want use tomcat jsp servlet also need natural language processing know python really good way merge python code java web code good tool dealing nlp java think little confused since know little nlp area thanks advance
How to build a conceptual search engine?,"<p>I would like to build an internal search engine (I have a very large collection of thousands of XML files) that is able to map queries to concepts.  For example, if I search for ""big cats"", I would want highly ranked results to return documents with ""large cats"" as well.  But I may also be interested in having it return ""huge animals"", albeit at a much lower relevancy score.  </p>

<p>I'm currently reading through the Natural Language Processing in Python book, and it seems WordNet has some word mappings that might prove useful, though I'm not sure how to integrate that into a search engine.  Could I use Lucene to do this? How?</p>

<p>From further research, it seems ""latent semantic analysis"" is relevant to what I'm looking for but I'm not sure how to implement it.  </p>

<p>Any advice on how to get this done?</p>
",Multilingual Language Processing & Language Identification,build conceptual search engine would like build internal search engine large collection thousand xml file able map query concept example search big cat would want highly ranked result return document large cat well may also interested return huge animal albeit much lower relevancy score currently reading natural language processing python book seems wordnet ha word mapping might prove useful though sure integrate search engine could use lucene research seems latent semantic analysis relevant looking sure implement advice get done
Find a group of top feed backs from given set of input feedback,"<p>I am very new to the field of Natural language processing. My only experience was to use the standford nlp to get the part of speech for sentence.</p>

<p>Problem: I have to find out top 10 suggestions given in a set of product feedbacks.</p>

<p>Input: It consists of around one hundred feedbacks given as suggestions for some product.</p>

<p>I am not able to figure out where should I start the work from. On what basis should I create the top 10 suggestions?</p>
",Multilingual Language Processing & Language Identification,find group top feed back given set input feedback new field natural language processing experience wa use standford nlp get part speech sentence problem find top suggestion given set product feedback input consists around one hundred feedback given suggestion product able figure start work basis create top suggestion
"Who are the major authors in Information Extraction, Text Mining and Natural Language Processing area?","<p>This is not a code question, but about concepts. I want to know who are the main author/researches for Information Extraction, Natural Language Processing and Text Mining to read his papers/books/works.</p>
",Multilingual Language Processing & Language Identification,major author information extraction text mining natural language processing area code question concept want know main author research information extraction natural language processing text mining read paper book work
"Output results in conll format (POS-tagging, stanford pos tagger)","<p>I am trying to use Stanford POS-tagger, I want to ask if it is possible to parse (actually only pos tag would be enough) an english text and output the results in conll format. Is there such an option?</p>

<p>I am using the full 3.2.0 version of the Stanford pos tagger</p>

<p>Thanks a lot</p>
",Multilingual Language Processing & Language Identification,output result conll format po tagging stanford po tagger trying use stanford po tagger want ask possible parse actually po tag would enough english text output result conll format option using full version stanford po tagger thanks lot
Python parse words from URL string,"<p>I have a large data set of urls and I need a way to parse words from the urls eg:</p>

<pre><code>realestatesales.com -&gt; {""real"",""estate"",""sales""}
</code></pre>

<p>I would prefer to do it in python.  This seems like it should be possible with some kind of english language dictionary.  There might be some ambiguous cases, but I feel like there should be a solution out there somewhere.</p>
",Multilingual Language Processing & Language Identification,python parse word url string large data set url need way parse word url eg would prefer python seems like possible kind english language dictionary might ambiguous case feel like solution somewhere
Shorten a text and only keep important sentences,"<p>The German website nandoo.net offers the possibility to shorten a news article. If you change the percentage value with a slider, the text changes and some sentences are left out.</p>

<p>You can see that in action here:</p>

<blockquote>
  <p><a href=""http://www.nandoo.net/read/article/299925/"" rel=""nofollow noreferrer"">http://www.nandoo.net/read/article/299925/</a></p>
</blockquote>

<p>The news article is on the left side and tags are marked. The slider is on the top of the second column. The more you move the slider to the left, the shorter the text becomes.</p>

<p>How can you offer something like that? Are there any algorithms which you can use to achieve that?</p>

<p>My idea was that their algorithm counts the number of tags and nouns in a sentence. Then the sentences with fewest number of tags/nouns are left out.</p>

<p>Could that be true? Or do you have another idea?</p>

<p>I hope you can help me. Thanks in advance!</p>
",Multilingual Language Processing & Language Identification,shorten text keep important sentence german website nandoo net offer possibility shorten news article change percentage value slider text change sentence left see action news article left side tag marked slider top second column move slider left shorter text becomes offer something like algorithm use achieve idea wa algorithm count number tag noun sentence sentence fewest number tag noun left could true another idea hope help thanks advance
Analyze Text to find patterns and useful information,"<p>to provide some context: Issues in an application are logged in an excel sheet and one of the columns in that sheet contains the email communication between the user (who had raised the issue) and the resolve team member. There are bunch of other columns containing other useful information. My job is to find useful insights from this data for Business.</p>

<ol>
<li>Find out what type of issue was that? e.g. was that a training issue for the user or access issue etc. This would mean that I analyze the mail text and figure out by some means the type of issue.</li>
<li>How many email conversations have happened for one issue?</li>
<li>Is it a repeat issue?</li>
<li>There are other simple statistical problems e.g. How many issues per week etc...</li>
</ol>

<p>I read that NLP with Python can be solution to my problems. I also looked at Rapidminer for the same.</p>

<p>Now my Question is 
a. ""Am I on the right track?, Is NLP(Natural Language Processing) the solution to these problems?""
b. If yes, then how to start.. I have started reading book on NLP with Python, but that is huge, any specific areas that I should concentrate on and can start my analysis?
c. How is Rapidminer tool? Can it answer all of these questions? The data volume is not too huge (may be 100000 rows)... looks like it is quite easy to build a process in rapidminer, hence started on it...</p>

<p>Appreciate any suggestions!!!</p>
",Multilingual Language Processing & Language Identification,analyze text find pattern useful information provide context issue application logged excel sheet one column sheet contains email communication user raised issue resolve team member bunch column containing useful information job find useful insight data business find type issue wa e g wa training issue user access issue etc would mean analyze mail text figure mean type issue many email conversation happened one issue repeat issue simple statistical problem e g many issue per week etc read nlp python solution problem also looked rapidminer question right track nlp natural language processing solution problem b yes start started reading book nlp python huge specific area concentrate start analysis c rapidminer tool answer question data volume huge may row look like quite easy build process rapidminer hence started appreciate suggestion
Why are NLP processes considered language-dependent?,"<p>Why are NLP Processes considered language-dependent? </p>

<p>For example, here:
<a href=""http://www.slideshare.net/saschanarr/languageindependent-twitter-sentiment-analysis"" rel=""nofollow"">http://www.slideshare.net/saschanarr/languageindependent-twitter-sentiment-analysis</a>
on slide 6, its says that: ""Natural Language Processing methods are often designed specifically for one language"".</p>

<p>Why is it so? I would think that once the method is implemented using machine learning, the algorithm is the same and all you need different is the training set...</p>
",Multilingual Language Processing & Language Identification,nlp process considered language dependent nlp process considered language dependent example slide say natural language processing method often designed specifically one language would think method implemented using machine learning algorithm need different training set
Which words to omit?,"<p>I'm trying to find some similarity measure between two sentences. Fot that I make use of individual semantic similarities of two words. But there are lots of words in the dictionary that I make from the sentences I have. I want to eliminate some words from the sentences that I don't think conveying information about the content. First I removed the words with fewer letters but I don't think it is reasoble because it also removes some informative words.</p>

<p>Look at the parts of some sentences here</p>

<pre><code>""Despite the fact that ...""
""There's a debate such that ...""
""To sum up ...""
""Although ..., there is ...""
</code></pre>

<p>If I had a text file involving these words, I would remove them from my dictionary and let only the informative words remain there.</p>

<p>Is there any list of such words that you know for English so I can use to filter my dictionary?</p>
",Multilingual Language Processing & Language Identification,word omit trying find similarity measure two sentence fot make use individual semantic similarity two word lot word dictionary make sentence want eliminate word sentence think conveying information content first removed word fewer letter think reasoble also remove informative word look part sentence text file involving word would remove dictionary let informative word remain list word know english use filter dictionary
Converting natural language to haiku?,"<p>Is it possible to programmatically convert regular English into English haiku? Or is this something too complicated to contemplate? I have a feeling that this is a lot more involved than a <a href=""http://en.wikipedia.org/wiki/Pig_Latin"" rel=""nofollow noreferrer"">Pig Latin</a> formatter.</p>
",Multilingual Language Processing & Language Identification,converting natural language haiku possible programmatically convert regular english english haiku something complicated contemplate feeling lot involved pig latin formatter
Which language or tools to learn for natural language processing?,"<p>I am French, and am a former Certified Network Security Administrator.
I went back to university 3 years ago to achieve a Bachelor's degree in linguistics, and I am now going to enroll in a Masters Degree in Computer Science applied to Linguistics, with the objective of eventually trying to go through a Doctorate (but I'm not there yet :-) ).</p>

<p>The course will focus on speech recognition, automatic language translation, statistical analysis of texts, speech encoding and decoding, and information abstratction from textual sources.
The professors will let us use any computer language we want to use to code the algorithms and programs we will develop during the curriculum.</p>

<p>I used to develop web apps as a side gig for about 3-4 years and I am proficient in Javascript as I wrote software that used node.js at the server end and the browser at the client. I also have some familiarity with postgresql.</p>

<p>My current style of coding (if we can call that a style) is mainly procedural and I use object prototyping as my main way to create/manage objects in my code. I don't have much experience with object oriented language that use the concept of classes to manage the objects. Therefore I am pretty confident my current coding skills are definitely lacking in regards to what is required for me to write efficient code to deal with that stuff.</p>

<p>So my question is this : what would be the best computer language for me to learn in order to be effective in writing algorithms and data structure suited for the above mentionned linguistic areas?</p>

<p>Thanks in advance for your enlightened answers.</p>

<p>Sat Cit Ananda.</p>
",Multilingual Language Processing & Language Identification,language tool learn natural language processing french former certified network security administrator went back university year ago achieve bachelor degree linguistics going enroll master degree computer science applied linguistics objective eventually trying go doctorate yet course focus speech recognition automatic language translation statistical analysis text speech encoding decoding information abstratction textual source professor let u use computer language want use code algorithm program develop curriculum used develop web apps side gig year proficient javascript wrote software used node j server end browser client also familiarity postgresql current style coding call style mainly procedural use object prototyping main way create manage object code much experience object oriented language use concept class manage object therefore pretty confident current coding skill definitely lacking regard required write efficient code deal stuff question would best computer language learn order effective writing algorithm data structure suited mentionned linguistic area thanks advance enlightened answer sat cit ananda
Algorithm to combine a grammatically correct phrase from words,"<p>The problem I'm currently facing is as follows. I have a set of words, and want to construct a grammatically correct phrase/sentence out of them, if at all possible. What I have tried so far is:</p>

<ul>
<li>From the reference text corpus calculate an average position of each word in a sentence;</li>
<li>Using this, sort words in set, and separate with space.</li>
</ul>

<p>The problem with this approach is that most of the time it produces bizarre phrases that make no sense. Is there any way to accomplish this, maybe using <a href=""/questions/tagged/nlp"" class=""post-tag"" title=""show questions tagged 'nlp'"" rel=""tag"">nlp</a> techniques (assuming, I'm only working with English)?</p>
",Multilingual Language Processing & Language Identification,algorithm combine grammatically correct phrase word problem currently facing follows set word want construct grammatically correct phrase sentence possible tried far reference text corpus calculate average position word sentence using sort word set separate space problem approach time produce bizarre phrase make sense way accomplish maybe using nlp technique assuming working english
Multi-tenancy with GATE,"<p>I am using Gate to implement natural language processing module in my project. In the next step, I want the system support multiple users. Each user has different set of domain words(I put it into gazetteers). As far as I know, gate use a lot of static resource and It is very difficult to create multiple instances of gate. Do any one have any idea to implement multi-tenancy with Gate? </p>

<p>Thank you so much in advance.</p>

<p>PS: Reasons I use gate is because it has jape rule. It would be great if you can introduce other tool, which is easier to customize and also has that kind of rule based grammar.</p>
",Multilingual Language Processing & Language Identification,multi tenancy gate using gate implement natural language processing module project next step want system support multiple user user ha different set domain word put gazetteer far know gate use lot static resource difficult create multiple instance gate one idea implement multi tenancy gate thank much advance p reason use gate ha jape rule would great introduce tool easier customize also ha kind rule based grammar
How to split a paragraph into small sub-strings?,"<p>I am trying to remove English words from a string or (Paragraph). But the problem is all the words are not being removed. But when i try the same with a smaller string say of 25 words, it works perfectly.
I am trying to filter this in 3 steps:</p>

<ol>
<li>Remove Links from a String</li>
<li>Remove XML Tags.</li>
<li>Remove English words.</li>
</ol>

<p>Below is the code:</p>

<pre><code>String SWList[];
public ArrayList&lt;String&gt; tokens = new ArrayList&lt;String&gt;();
String sentenceSoFar="""";
String nextToken;
String withoutLink=null;
ArrayList&lt;String&gt;    wordscount = new ArrayList&lt;String&gt;();
boolean flag=false;
String str;
int counter;
String finalStr="""";
ArrayList&lt;String&gt; spaceCheck = new ArrayList&lt;String&gt;();

public void removeLinks(String str) {
    sentenceSoFar=null;
    String delims = "" "";
    StringTokenizer st = new StringTokenizer(str, delims);
    sentenceSoFar=null;
    while (st.hasMoreTokens()) {
        String str1 = ""http"";
        String nextToken = st.nextToken();
        //System.out.println(""LINK CHECK : "" + nextToken);

        if (!(nextToken.contains(str1))) {
            flag = false;
            if (counter == 0) {
                tokens.add(nextToken);
                sentenceSoFar += "" "" + nextToken;
                withoutLink+= "" "" + nextToken;
            } else {
                if (nextToken.contains(str1)) {
                    withoutLink = nextToken;
                    counter=1;
                }
            }
        }
    }

    //System.out.println(""sentence wala :  "" + sentenceSoFar);
    removeXmlTags(sentenceSoFar);
    sentenceSoFar=null;

}

public void removeXmlTags(String strTags) {
    //strTags=null;

    String[] stopWords = new String[] {
        ""&amp;amp;nbsp;&amp;lt;/p&amp;gt;"",
        "" &amp;amp;nbsp;&amp;lt;/p&amp;gt; "",
        "".&amp;lt;/p&amp;gt;"","" .&amp;lt;/p&amp;gt; "",
        ""1??&amp;quot;&amp;gt;&amp;lt;span&amp;gt;&amp;amp;nbsp;"",
        "" 1??&amp;quot;&amp;gt;&amp;lt;span&amp;gt;&amp;amp;nbsp; "",
        ""&amp;lt;p"","" &amp;lt;p "",
        ""  align=&amp;quot;center&amp;quot;&amp;gt; "",
        "" align=&amp;quot;center&amp;quot;&amp;gt;"",
        "";"",
        ""&amp;"",
        ""/&amp;"",
        ""&amp;lt"",
        "" &amp;lt "",
        ""_rdEdi"",
        "" _rdEdi "",
        ""br"",
        "" br "",
        ""gt"",
        "" gt "",
        ""exLink"",
        "" exLink "",
        ""link"",
        "" link "",
        ""&amp;gt"",
        "" &amp;gt "",
        ""style"",
        "" style "",
        "";/div&amp; "",
        ""class"",
        "" cestry "",
        ""-"",
        "" - "",
        ""nb"",
        "" nb "",
        "" a "",
        ""&amp;lt;p&amp;gt;"",
        ""&amp;#160;"",
        "";/b&amp;"",
        "","",
        ""/"",
        "" It "",
        "" strong "",
        "" span "",
        "" Responsibilities "",
        "" bull "",
        "" amp "",
        "" b "",
        "" d "",
        "" e "",
        "" f "",
        "" g "",
        "" h "",
        "" i "",
        "" j "",
        "" k "",
        "" l "",
        "" m "",
        "" n "",
        "" o "",
        "" p "",
        "" q "",
        "" r "",
        "" s "",
        "" t "",
        "" u "",
        "" v "",
        "" w "",
        "" x "",
    };

    {
        for (String stopword : stopWords) {
            strTags = strTags.replaceAll(""(?i)""+stopword, "" "");
        }
    }

    //System.out.println(""OUTPUT STRING WITHOUT TAGS : "" + strTags);
    englishWords(strTags);
    strTags=null;
}

public void englishWords(String strWords) {

    finalStr=null;
    String[] stopWords = new String[]{
        "" i "" , "" a "" , "" natural "" , "" and "" , "" if "" ,"" your"" ,"" about "" , "" an "" , "" are "" , "" as "" , "" at "" , "" be "" , ""  by "" ,"" was "" ,"" leadership "" ,
        "" com "" , "" for "" , "" from "" , "" how "" , "" in "" , "" is "" , "" it "" , "" not "" , "" of "" , "" on "" , "" or "" , "" that "" , "" the "" , "" this "" , "" to "" , ""lt"",""quot"",
        "" what "" , "" when "" , "" where "" , "" who "" , "" will "" , "" with "" , "" the "" , "" www "" ,"" role "" ,"" provides"" ,"" you "" ,""&amp;amp;nbsp;&amp;lt;/p&amp;gt; "" ,""align"",""temp"",""tor"",
        "" Inc."" ,""  Inc."" ,"" is "" ,"" an "" ,"" equal"" ,"" equal "" ,"" Opportunity"" ,"" Opportunity "" ,"" Employer"" ,""  Employer "" ,""  The "" ,"" company"" ,"" candidates"" ,""center"",
        "" company "" ,"" its"" ,"" affiliates"" ,""  affiliates "" ,""  recruit "" ,""  hire "" ,""  qualified "" ,"" candidates"" ,""  candidates "" ,""  today "" ,"" Facebook "" ,
        "" without "" ,""  without "" ,"" regard"" ,""  regard "" ,"" to "" ,"" race"" ,"" race "" ,"" religion"" ,"" religion "" ,"" color "" ,"" color "" , "" sex "" ,"" sexual "" ,
        "" sexual "" ,""  orientation "" ,""  orientation "" ,"" gender "" ,"" gender "" , "" identity "" ,"" identity "" ,"" age "" ,"" national "" ,"" national "" ,"" origin"" ,
        "" origin "" ,"" ancestry"" ,"" ancestry "" ,"" citizenship"" ,"" citizenship "" , "" veteran"" ,"" veteran"" , ""  or "" ,"" disability"" ,""  disability "" ,"" status"" ,
        "" status "" ,""  medical"" ,""  medical "" ,"" condition"" ,"" condition "" ,"" marital"" ,"" marital "" , "" any"" ,""  any "" , "" other"" ,""  other "" ,"" factor"" ,""  factor "" ,
        "" prohibited"" ,""  prohibited "" ,"" state "" ,""  state "" ,"" provincial"" ,""  provincial "" ,"" and "" ,"" federal"" ,""  federal "" ,"" municipal"" ,""  municipal "" ,
        "" it "" ,"" ul "" ,"" LI "" ,"" HR "" ,"" div "" ,"" it "" ,"" ul "" ,"" lt "" , "" sp "" , "" Nurse "" ,"" join "" ,"" our "" ,""  Overview "" ,""  specializes "" ,""  highly "" ,"" sampling "" ,
        "" Description "" ,"" Requirements "" ,"" Intensive "" ,"" Care "" ,"" StartDate "" ,"" ASAP "" ,"" Available "" ,"" Shifts "" ,"" Exclusive "" ,"" order "" ,"" Serving "" ,
        "" throughout "" ,""  county "" ,"" members "" ,"" range "" ,"" more "" ,""  provide "" ,"" Emergency "" ,"" currently "" ,"" customer "" ,""  unparalleled "" ,""  Spending"" , 
        "" looking "" ,"" Critical "" ,"" Facility "" ,""  boggling "" ,"" entertainment "" ,"" service "" ,"" benefits "" ,"" commitment "" ,"" outdoor "" ,"" comprehensive "" ,
        "" settings "" ,"" patient "" ,""  exhilarating "" ,""  interventions "" ,""  environments "" ,"" nurses "" ,"" needs "" ,"" travel "" ,""  primary "" ,"" see "" ,""  experience"" ,
        "" gas "" ,""  transportation "" ,""  machine "" ,""  construction "" ,"" mining "" ,"" industries "" ,"" detailed "" ,"" corrective "" ,""  action "" ,""  both "" ,"" management "" ,
        "" management "" ,""  Receiving "" ,""  Inspection "" ,"" verification "" ,"" established "" ,"" which "" ,"" material "" ,"" acceptance "" ,"" measurement "" ,
        ,"" training "" ,"" Familiar "" ,""  shipment "" ,
        "" levels "" ,""  drawings "" ,"" knowledge "" ,"" Recruiter "" ,"" Recruiter: "" ,""  long "" ,"" short "" ,"" years "" ,"" opportunities "" ,"" competition "" ,""  until "" ,"" Email "" ,"" here "" ,"" quot "" ,"" replace "" ,"" schedule "" ,"" Flexible "" ,
        "" these""  ,"" can "" ,""  manage "" ,""  multiple "" ,"" tasks "" ,"" simultaneously "" ,"" adapt "" ,"" market "" ,"" changes? "" ,"" basic "" ,"" qualifications "" ,"" only "" ,
        "" half "" ,"" story "" ,""  considering "" ,"" 7 "" ,"" eleven "" ,"" right "" ,"" choice "" ,"" should "" ,"" consider "" ,"" they "" ,"" possess "" ,""  traits "" ,"" most "" ,"" common "" ,
        "" successful "" ,"" 7 "" ,""  eleven "" ,""  franchisees "" ,"" can ""  ,""  train "" ,""  supervise "" ,"" employees? "" ,"" willing "" ,"" empower "" ,""  them "" ,"" delegate "" ,"" them? "" ,
        "" dedicated "" ,"" operations ""  ,"" excellence? "" ,"" do "" ,""  focus "" ,"" details? "" ,"" committed "" ,"" creating "" ,"" managing "" ,"" organization "" ,""  effectively "" ,
        "" recruits "" ,""  trains "" ,""  retains "" ,""  motivates "" ,"" people "" ,"" do "" ,""  have "" ,""  desire "" ,""  build "" ,"" emental "" ,"" me "" ,""  through "" ,"" execution "" ,"" ability "" ,
        "" programs "" ,""  strategies? "" ,""  do"" ,""  have "" ,""  food"" ,"" can "" , "" aur "" , "" join ""
    };

    for (String stopword : stopWords)
    {
        strWords = strWords.replaceAll(""(?i)""+stopword, "" "");
    }

    String delims = "" , = ; : ' * % $ @ 0 - _ + ( ) ."";
    StringTokenizer st = new StringTokenizer(strWords, delims);

    finalStr =null;
    while(st.hasMoreTokens()) {

        String ntoken = st.nextToken();
        //    System.out.println(""LINK CHECK : "" + ntoken);

        tokens.add(ntoken);
        finalStr += "" ""+ ntoken;
        //withoutLink+= "" "" + nextToken;

    }

    //    System.out.println(""Different  STRING : "" + finalStr);
    //    new indexing.IndexAlgo().algoOne(finalStr);
    finalStr=null;
}
</code></pre>

<p>I would really appreciate if anyone can help me with some better logic or code.</p>
",Multilingual Language Processing & Language Identification,split paragraph small sub string trying remove english word string paragraph problem word removed try smaller string say word work perfectly trying filter step remove link string remove xml tag remove english word code would really appreciate anyone help better logic code
Transcript dataset for natural language processing,"<p>I've been searching on the web, and found media such as CNN and NPR provide links to access to their transcripts. To obtain them requires writing something like a crawler which is not so convenient. The reason is that I'm trying to use some transcripts of TV show, interview, radio, movie as training data in my natural language processing projects. So I'm wondering whether there's any collection or database freely available on the web so that I can download all of them at once without writing a crawler by myself?</p>
",Multilingual Language Processing & Language Identification,transcript dataset natural language processing searching web found medium cnn npr provide link access transcript obtain requires writing something like crawler convenient reason trying use transcript tv show interview radio movie training data natural language processing project wondering whether collection database freely available web download without writing crawler
What is Zone Hashing in Natural Language Processing?,"<p>Has anyone in the NLP field heard of the term <em>Zone Hashing</em>? From what I hear, zone hashing is the process of iterating through a document and extracting sentences. An accumulation of sentences is then hashed, and the process continues for the next <em>n</em> sentences...</p>

<p>I haven't found any references to this on Google, so I'm wondering if it goes by a different name. It should be related to measuring text similarity/nearness. </p>

<p>Perhaps it refers to locality sensitive hashing? </p>
",Multilingual Language Processing & Language Identification,zone hashing natural language processing ha anyone nlp field heard term zone hashing hear zone hashing process iterating document extracting sentence accumulation sentence hashed process continues next n sentence found reference google wondering go different name related measuring text similarity nearness perhaps refers locality sensitive hashing
Is there a multilingual temporal expression tagger that can run on Hadoop?,"<p>I need to extract dates from lots of text. The more languages the better; English,Spanish, and Portuguese at a minimum. Does such a tool exist? In Java and Mavenized? Here's what I've found:</p>

<ul>
<li><a href=""http://code.google.com/p/heideltime/"" rel=""nofollow"">http://code.google.com/p/heideltime/</a> many languages and an impressive online demo, but requires some odd external dependencies that I suspect will make cluster deployment hard/impossible</li>
<li><a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/sutime.shtml</a> Well documented, but English only. Easy to train?</li>
<li><a href=""http://natty.joestelmach.com/"" rel=""nofollow"">http://natty.joestelmach.com/</a> English only</li>
<li><a href=""https://github.com/samtingleff/jchronic"" rel=""nofollow"">https://github.com/samtingleff/jchronic</a> English only </li>
<li><a href=""http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py"" rel=""nofollow"">http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py</a> English only</li>
</ul>

<p>Where else should I look?</p>
",Multilingual Language Processing & Language Identification,multilingual temporal expression tagger run hadoop need extract date lot text language better english spanish portuguese minimum doe tool exist java mavenized found many language impressive online demo requires odd external dependency suspect make cluster deployment hard impossible well documented english easy train english english english else look
iOS Library for Dates Natural Language Processing?,"<p>Is there any option to detect natural language strings like:</p>

<ul>
<li>""Every other Sunday"" </li>
<li>""Mondays at 5-6pm from 20/9 until 30/11""</li>
</ul>

<p>which will covert it to an object with date, hour, repeat rule, repeat start &amp; end, etc...</p>

<p>Is it possible to detect such things on iOS?</p>
",Multilingual Language Processing & Language Identification,io library date natural language processing option detect natural language string like every sunday monday pm covert object date hour repeat rule repeat start end etc possible detect thing io
What does discriminative reranking do in NLP tasks?,"<p>Recently,i have read about the ""discriminative reranking for natural language processing"" by Collins.
I'm confused what does the reranking actually do?
Add more global features to the rerank model? or something else?</p>
",Multilingual Language Processing & Language Identification,doe discriminative reranking nlp task recently read discriminative reranking natural language processing collins confused doe reranking actually add global feature rerank model something else
suggest in enchant.Dict(..) isn&#39;t accurate,"<p>I want to achieve two parts :</p>

<ol>
<li>I want to remove redundant letters that are existing in each string more than twice. </li>
<li>Check the best matched English word after processing first part on each string. I am using python so I used enchant.Dict(""en"") and also enchant.Dict(""en"").suggest(..)</li>
</ol>

<p>For example:</p>

<pre><code>Heelllllo should be Heello
Heeeeeeeey should be Heey
yessssssss should be yees
</code></pre>

<p>My problem  is in 2. For ""yessssssss""
I have the suggested list of words:</p>

<pre><code>['yes', 'less', 'mess', 'fess', 'jess', 'Hess', 'yeas', 'yens', 'yews', 'yes s']
</code></pre>

<p>So my question is ""yes s"" as the last word in the result isn't meaningful, so I need more accurate result, Does my approach right ?, should I consider change in part 1 ?</p>
",Multilingual Language Processing & Language Identification,suggest enchant dict accurate want achieve two part want remove redundant letter existing string twice check best matched english word processing first part string using python used enchant dict en also enchant dict en suggest example problem yes suggested list word question yes last word result meaningful need accurate result doe approach right consider change part
Searching for a Library for Detecting Comparatives and Superlatives in Natural Language,"<p>I'm analyzing natural language texts in java and am currently looking for a library that enables to detect comparatives and superlatives (e.g. ""better"", ""fastest"",...) in German language.</p>

<p>I used the POS Tagger of the Stanford NLP, which worked fine for English, but is less detailed in German (detects only adjectives, but no morphological features, as far as I tried).</p>

<p>Does anyone know such a library/model (also non-java-libraries etc.)?</p>

<p>Thanks a lot!
Henning</p>
",Multilingual Language Processing & Language Identification,searching library detecting comparative superlative natural language analyzing natural language text java currently looking library enables detect comparative superlative e g better fastest german language used po tagger stanford nlp worked fine english le detailed german detects adjective morphological feature far tried doe anyone know library model also non java library etc thanks lot henning
Extract Product Core Key Words With (Python) NLTK,"<p>After reading ""Natural Language Processing With Python"" for a day, I'm still not quite clear if NLTK can be used to extract core key words for human input product descriptions --</p>

<p>e.g., from this</p>

<pre><code>Apple iPhone 4S (Latest Model) - 16GB - White or Black (Unlocked)
ANY GSM TMOBILE, AT&amp;T, Home, Smartphone
</code></pre>

<p>to</p>

<pre><code>Apple iPhone 4S 16GB Smartphone
</code></pre>

<p>or this</p>

<pre><code>Canon EOS 5D 12.8 MP DSLR Camera with lens, vert grip &amp; lots of extras
</code></pre>

<p>to</p>

<pre><code>Canon EOS 5D 12.8 MP DSLR Camera
</code></pre>

<p>Any pointer will be much appreciated.</p>
",Multilingual Language Processing & Language Identification,extract product core key word python nltk reading natural language processing python day still quite clear nltk used extract core key word human input product description e g pointer much appreciated
Looking for language translation function for PHP,"<p>I would like to add language translation to my site.  Is there any freeware (or at least inexpensive packages for a one-time fee) that will work more-or-less like this:</p>

<pre><code>$french_text = translate ($german_text, 'german', 'french');
</code></pre>

<p>I don't expect it to support every language in the world, but to be useful it should at least support most European languages.  Also, being an English site, it's acceptable to me if it needs to use English as an intermediate language like this:</p>

<pre><code>$english_text = translate ($german_text, 'german', english');
$french_text = translate ($english_text, 'english', 'french');
</code></pre>

<p>Does anything like this exist for PHP?</p>
",Multilingual Language Processing & Language Identification,looking language translation function php would like add language translation site freeware least inexpensive package one time fee work le like expect support every language world useful least support european language also english site acceptable need use english intermediate language like doe anything like exist php
NLP: compare parsed and tagged sentences,"<p>Hello language programmers</p>

<p>I'm studying online natural language processing and so far i have some understanding of how to parse a sentence including getting it's POS tags, SRL and so.
my question is what to do with this data, or more precisely how to compare two different parsed sentences to see how similar they are.</p>

<p>for example i got this tow parsed sentences and i want to be able to compare them</p>

<p><strong>1.</strong></p>

<pre><code>&lt;sentence id=""s0"" parse_status=""success"" fom=""11.6633""&gt;
&lt;cons id=""c0"" cat=""NP"" xcat="""" head=""c1"" sem_head=""c1"" schema=""empty_spec_head""&gt;
&lt;cons id=""c1"" cat=""NX"" xcat="""" head=""c2"" sem_head=""c2"" schema=""head_mod""&gt;
&lt;cons id=""c2"" cat=""NX"" xcat="""" head=""c3"" sem_head=""c3"" schema=""head_mod""&gt;
&lt;cons id=""c3"" cat=""NX"" xcat="""" head=""t0"" sem_head=""t0""&gt;
&lt;tok id=""t0"" cat=""N"" pos=""NN"" base=""apartment"" lexentry=""[D&lt;N.3sg&gt;]"" pred=""noun_arg0""&gt;apartment&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c4"" cat=""PP"" xcat="""" head=""c5"" sem_head=""c5"" schema=""head_comp""&gt;
&lt;cons id=""c5"" cat=""PX"" xcat="""" head=""t1"" sem_head=""t1""&gt;
&lt;tok id=""t1"" cat=""P"" pos=""IN"" base=""in"" lexentry=""N[&lt;P&gt;NP.acc]"" pred=""prep_arg12"" type=""noun_mod"" arg1=""c3"" arg2=""c6""&gt;in&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c6"" cat=""NP"" xcat="""" head=""c7"" sem_head=""c7"" schema=""empty_spec_head""&gt;
&lt;cons id=""c7"" cat=""NX"" xcat="""" head=""c9"" sem_head=""c9"" schema=""mod_head""&gt;
&lt;cons id=""c8"" cat=""NP"" xcat="""" head=""t2"" sem_head=""t2""&gt;
&lt;tok id=""t2"" cat=""N"" pos=""NNP"" base=""tel"" lexentry=""[D&lt;N.3sg&gt;]-noun_adjective_rule"" pred=""noun_arg1"" type=""noun_mod"" arg1=""c9""&gt;Tel&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c9"" cat=""NX"" xcat="""" head=""t3"" sem_head=""t3""&gt;
&lt;tok id=""t3"" cat=""N"" pos=""NNP"" base=""aviv"" lexentry=""[D&lt;N.3sg&gt;]"" pred=""noun_arg0""&gt;Aviv&lt;/tok&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;cons id=""c10"" cat=""PP"" xcat="""" head=""c11"" sem_head=""c11"" schema=""head_comp""&gt;
&lt;cons id=""c11"" cat=""PX"" xcat="""" head=""t4"" sem_head=""t4""&gt;
&lt;tok id=""t4"" cat=""P"" pos=""IN"" base=""with"" lexentry=""N[&lt;P&gt;NP.acc]"" pred=""prep_arg12"" type=""noun_mod"" arg1=""c2"" arg2=""c12""&gt;with&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c12"" cat=""NP"" xcat="""" head=""c13"" sem_head=""c13"" schema=""empty_spec_head""&gt;
&lt;cons id=""c13"" cat=""NX"" xcat="""" head=""c15"" sem_head=""c15"" schema=""mod_head""&gt;
&lt;cons id=""c14"" cat=""ADJP"" xcat="""" head=""t5"" sem_head=""t5""&gt;
&lt;tok id=""t5"" cat=""ADJ"" pos=""CD"" base=""five"" lexentry=""[&lt;ADJP&gt;]N"" pred=""adj_arg1"" type=""noun_mod"" arg1=""c15""&gt;five&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c15"" cat=""NX"" xcat="""" head=""t6"" sem_head=""t6""&gt;
&lt;tok id=""t6"" cat=""N"" pos=""NNS"" base=""room"" lexentry=""[D&lt;N.3sg&gt;]-plural_noun_rule"" pred=""noun_arg0""&gt;rooms&lt;/tok&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/sentence&gt;
</code></pre>

<p><strong>2.</strong></p>

<pre><code>&lt;sentence id=""s1"" parse_status=""success"" fom=""17.4367""&gt;
&lt;cons id=""c16"" cat=""NP"" xcat="""" head=""c17"" sem_head=""c17"" schema=""empty_spec_head""&gt;
&lt;cons id=""c17"" cat=""NX"" xcat="""" head=""c18"" sem_head=""c18"" schema=""head_mod""&gt;
&lt;cons id=""c18"" cat=""NX"" xcat="""" head=""t7"" sem_head=""t7""&gt;
&lt;tok id=""t7"" cat=""N"" pos=""NN"" base=""apartment"" lexentry=""[D&lt;N.3sg&gt;]"" pred=""noun_arg0""&gt;apartment&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c19"" cat=""PP"" xcat="""" head=""c20"" sem_head=""c20"" schema=""head_comp""&gt;
&lt;cons id=""c20"" cat=""PX"" xcat="""" head=""t8"" sem_head=""t8""&gt;
&lt;tok id=""t8"" cat=""P"" pos=""IN"" base=""with"" lexentry=""N[&lt;P&gt;NP.acc]"" pred=""prep_arg12"" type=""noun_mod"" arg1=""c18"" arg2=""c21""&gt;with&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c21"" cat=""NP"" xcat="""" head=""c22"" sem_head=""c22"" schema=""empty_spec_head""&gt;
&lt;cons id=""c22"" cat=""NX"" xcat="""" head=""c23"" sem_head=""c23"" schema=""head_mod""&gt;
&lt;cons id=""c23"" cat=""NX"" xcat="""" head=""c25"" sem_head=""c25"" schema=""mod_head""&gt;
&lt;cons id=""c24"" cat=""ADJP"" xcat="""" head=""t9"" sem_head=""t9""&gt;
&lt;tok id=""t9"" cat=""ADJ"" pos=""CD"" base=""-NUMBER-"" lexentry=""[&lt;ADJP&gt;]N"" pred=""adj_arg1"" type=""noun_mod"" arg1=""c25""&gt;3&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c25"" cat=""NX"" xcat="""" head=""t10"" sem_head=""t10""&gt;
&lt;tok id=""t10"" cat=""N"" pos=""NNS"" base=""room"" lexentry=""[D&lt;N.3sg&gt;]-plural_noun_rule"" pred=""noun_arg0""&gt;rooms&lt;/tok&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;cons id=""c26"" cat=""PP"" xcat="""" head=""c27"" sem_head=""c27"" schema=""head_comp""&gt;
&lt;cons id=""c27"" cat=""PX"" xcat="""" head=""t11"" sem_head=""t11""&gt;
&lt;tok id=""t11"" cat=""P"" pos=""IN"" base=""in"" lexentry=""N[&lt;P&gt;NP.acc]"" pred=""prep_arg12"" type=""noun_mod"" arg1=""c23"" arg2=""c28""&gt;in&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c28"" cat=""NP"" xcat="""" head=""c29"" sem_head=""c29"" schema=""empty_spec_head""&gt;
&lt;cons id=""c29"" cat=""NX"" xcat="""" head=""c31"" sem_head=""c31"" schema=""mod_head""&gt;
&lt;cons id=""c30"" cat=""NP"" xcat="""" head=""t12"" sem_head=""t12""&gt;
&lt;tok id=""t12"" cat=""N"" pos=""NNP"" base=""tel"" lexentry=""[D&lt;N.3sg&gt;]-noun_adjective_rule"" pred=""noun_arg1"" type=""noun_mod"" arg1=""c31""&gt;Tel&lt;/tok&gt;
&lt;/cons&gt;
&lt;cons id=""c31"" cat=""NX"" xcat="""" head=""t13"" sem_head=""t13""&gt;
&lt;tok id=""t13"" cat=""N"" pos=""NNP"" base=""aviv"" lexentry=""[D&lt;N.3sg&gt;]"" pred=""noun_arg0""&gt;Aviv&lt;/tok&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/cons&gt;
&lt;/sentence&gt;
</code></pre>

<p>thanks</p>
",Multilingual Language Processing & Language Identification,nlp compare parsed tagged sentence hello language programmer studying online natural language processing far understanding parse sentence including getting po tag srl question data precisely compare two different parsed sentence see similar example got tow parsed sentence want able compare thanks
can somebody tell me the method for sms normalization with python,"<p>I am trying to do sms normalization with the help of python</p>

<p>suppose the request is ""btw m gng home""
It should translate it into ""by the way I am going home""</p>

<p>Can somebody tell me some logic for this which I can apply. Can I use NLP for the same purpose.If yes then how?</p>
",Multilingual Language Processing & Language Identification,somebody tell method sm normalization python trying sm normalization help python suppose request btw gng home translate way going home somebody tell logic apply use nlp purpose yes
Natural Language Processing Algorithm for mood of an email,"<p>One simple question (but I haven't quite found an obvious answer in the NLP stuff I've been reading, which I'm very new to):</p>

<p>I want to classify emails with a probability along certain dimensions of mood. Is there an NLP package out there specifically dealing with this? Is there an obvious starting point in the literature I start reading at?</p>

<p>For example, if I got a short email something like ""Hi, I'm not very impressed with your last email - you said the order amount would only be $15.95! Regards, Tom"" then it might get 8/10 for Frustration and 0/10 for Happiness.</p>

<p>The actual list of moods isn't so important, but a short list of generally positive vs generally negative moods would be useful.</p>

<p>Thanks in advance!</p>

<p>--Trindaz on Fedang #NLP</p>
",Multilingual Language Processing & Language Identification,natural language processing algorithm mood email one simple question quite found obvious answer nlp stuff reading new want classify email probability along certain dimension mood nlp package specifically dealing obvious starting point literature start reading example got short email something like hi impressed last email said order amount would regard tom might get frustration happiness actual list mood important short list generally positive v generally negative mood would useful thanks advance trindaz fedang nlp
Text Extraction in word document using User Interface Automation (UIA) framework of .Net,"<p>I am developing and Accessibility enabled Application. 
<strong>Application Concept</strong>
its a Natural Language Processing NLP applications.
When a user write something in the word document it will show the suggestions. 
For this I need to have the last 100 Characters in the Current Text Control(Text Pattern).</p>

<p>UIA(User Interface Automation) Framework provides access to the every TextPattern Control which is accessibilty Enabled. However when I try to access the current word document window text pane ( Editor) and text inside it does not provide me access. Former version of UIA is MSAA.</p>

<p>I need to access the current text editor pane(not the ribbion bar, scrools else) in the microsoft window 2007 or later document.</p>

<p>i have to access this thing through the UIA other option is OFFICE WORD Interop.
OFFICE WORD Interop does not provide me the control over the user activity.</p>

<p>kindly suggest any sample code using UIA to get text inside the word editor document</p>

<p>Scenario required:</p>

<p>Open a word document and write something in the Text area. Now I need to access that text concurrently as my intelligent app will get the current text previous to max.100 characters and minimum 2 to show suggestions.</p>

<p>Please find the screencast link below the samples I made working to give you idea.</p>

<p>Only the Text Extraction part is not working right in MS-Word.</p>

<p><a href=""http://screencast.com/t/5MBc9WN7wO"" rel=""nofollow"">Check this Video </a> I required Same functionality required from the MS word USING UIA</p>

<p>Waiting for your quick reply..</p>
",Multilingual Language Processing & Language Identification,text extraction word document using user interface automation uia framework net developing accessibility enabled application application concept natural language processing nlp application user write something word document show suggestion need last character current text control text pattern uia user interface automation framework provides access every textpattern control accessibilty enabled however try access current word document window text pane editor text inside doe provide access former version uia msaa need access current text editor pane ribbion bar scrools else microsoft window later document access thing uia option office word interop office word interop doe provide control user activity kindly suggest sample code using uia get text inside word editor document scenario required open word document write something text area need access text concurrently intelligent app get current text previous max character minimum show suggestion please find screencast link sample made working give idea text extraction part working right word check video required functionality required word using uia waiting quick reply
How to use Stanford parser to generate the English sentence from given segmented nodes,"<p>I am trying to develop a Sinhala (My native language) to English translator.
Still I am thinking for an approach.</p>

<p><strong>If I however parse a sentence of my language, then can use that for generating english sentence with the help of stanford parser. Or is there any other method you can recommend.</strong></p>

<p>And I am thinking of a bottom up parser for my language, but still have no idea how to implement. Any suggestions for steps I can follow.</p>

<p>Thanks 
Mathee</p>
",Multilingual Language Processing & Language Identification,use stanford parser generate english sentence given segmented node trying develop sinhala native language english translator still thinking approach however parse sentence language use generating english sentence help stanford parser method recommend thinking bottom parser language still idea implement suggestion step follow thanks mathee
natural language processing word association,"<p>My project needs some natural language processing. I'm completely new to the field.</p>

<p>What I am trying to get is when user enter a character, I want to get a list of English characters that can follow that specific character in order to make a legitimate word. </p>

<p>What is the specific term in NLP for doing this? I tried googling for a while, but had no luck since I don't know the term. Any good tutorials to start with? Are there any good libraries in doing this specific task?</p>

<p>Thank you.</p>
",Multilingual Language Processing & Language Identification,natural language processing word association project need natural language processing completely new field trying get user enter character want get list english character follow specific character order make legitimate word specific term nlp tried googling luck since know term good tutorial start good library specific task thank
"perl: strip html tags, manipulate text, and then return html tags to their original positions","<p>I'm using the Html::Strip module to remove all html tags from a file. I want to then manipulate the resulting text (stripped of html) and finally return the html tags to their original positions. </p>

<p>The text manipulation I'm doing requires breaking the text into arrays using split(/ /, $text). I then do some natural language processing of the resulting arrays (including adding <em>new</em> html tags to some key words). Once I'm finished processing the text, I'd like to return the original tags to their places while keeping the text manipulations I've done in the meantime intact.</p>

<p>I would be satisfied if I could simply remove all whitespace from within the original tags (since whitespace within tags is ignored by browsers). That way my NLProcessing could simply ignore words that are tags (contain '&lt;' or '>').</p>

<p>I've tried diving into the guts of Html::Strip (in an effort to modify it to my needs), but I can't understand what the following piece of code does:</p>

<pre><code>  my $stripped = $self-&gt;strip_html( $text );
  if( $self-&gt;decode_entities &amp;&amp; $_html_entities_p ) {
    $stripped = HTML::Entities::decode($stripped);
  }
</code></pre>

<p>Seems like strip_html is a sub, but I can't find that sub anywhere.</p>

<p>Anyway thanks for any and all advice.</p>

<hr>

<p>... the next day...</p>

<p>After a bit of back and forth with @amon, I have come up with a solution that I believe is sufficient for my purposes. amon pushed me in the right direction even though he recommended I not do what I've done anyway, haha. </p>

<p>It is a brutish method, but gets the job done satisfactorily. Gonna leave it here in the off chance that someone else has the same wishes as me and doesn't mind a quick and dirty solution:</p>

<pre><code>my $input = text.html;
my $stripped = $hs-&gt;parse($input);
$hs-&gt;eof;
</code></pre>

<p>so now I have two string variables. One is the html file I want to manipulate, and the other is the same file stripped of html.</p>

<pre><code>my @marks = split(/\s/, $stripped);
@marks = uniq(@marks);
</code></pre>

<p>Now I have a list of all non-HTMLtag-associated words that appear in my file.</p>

<pre><code>$input = HTML::Entities::decode($input);
$input =~ s/\&lt;/ \&lt;/g; 
$input =~ s/\&gt;/\&gt; /g; 
$input =~ s/\n/ \n /g; 
$input =~ s/\r/ \r /g; 
$input =~ s/\t/ \t /g;
</code></pre>

<p>Now I've decoded my HTML containing var and have ensured that no word is up against a ""&lt;"", or "">"" or non-space whitespace character.</p>

<pre><code>foreach my $mark(@marks) { $input =~ s/ \Q$mark\E / TAQ\+$mark\TAQ /g; }
$input =~ s/TAQ\+TAQ//g;
</code></pre>

<p>Now I've ""tagged"" each word with a ""+"" and have separated words from non-words with the TAQ delimiter. I can now split on TAQ and ignore any item that does not contain a ""+"" when performing my NLP and text manipulation. Once I'm done, I rejoin and strip all of the ""+"". Follow that with some clever encoding, remove all the extra spaces I inserted, and <em>BAM!</em> I've now got my NLProcessing completed, have manipulated the text, and still have all of my HTML in the right places. </p>

<p>There are a lot of caveats here, and I'm not going to go into all of them. Most problematic is the need to decode and then encode, coupled with the fact that HTML::Strip doesn't always strip all the javascript or invalid HTML. There are ways to work around that, but again I don't have room or time to discuss that here.</p>

<p>Thanks amon for your help, and I welcome any criticism or suggestions. I'm new to this. </p>
",Multilingual Language Processing & Language Identification,perl strip html tag manipulate text return html tag original position using html strip module remove html tag file want manipulate resulting text stripped html finally return html tag original position text manipulation requires breaking text array using split text natural language processing resulting array including adding new html tag key word finished processing text like return original tag place keeping text manipulation done meantime intact would satisfied could simply remove whitespace within original tag since whitespace within tag ignored browser way nlprocessing could simply ignore word tag contain tried diving gut html strip effort modify need understand following piece code doe seems like strip html sub find sub anywhere anyway thanks advice next day bit back forth amon come solution believe sufficient purpose amon pushed right direction even though recommended done anyway haha brutish method get job done satisfactorily gon na leave chance someone else ha wish mind quick dirty solution two string variable one html file want manipulate file stripped html list non htmltag associated word appear file decoded html containing var ensured word non space whitespace character tagged word separated word non word taq delimiter split taq ignore item doe contain performing nlp text manipulation done rejoin strip follow clever encoding remove extra space inserted bam got nlprocessing completed manipulated text still html right place lot caveat going go problematic need decode encode coupled fact html strip always strip javascript invalid html way work around room time discus thanks amon help welcome criticism suggestion new
Differentiating answer of WH-query,"<p>Is there any known way of differentiating answer of wh-questions?</p>

<p>For example, I know this sentence: Omelette is stir fried egg</p>

<p>There are 2 possible questions, which have that answer: 1. WHAT is omelette? 2. HOW is omelette?</p>

<p>Human english speaker can easily guess that the question is the first one, because ""NOUN is/are blablabla"" is likely to be the answer of ""WHAT is/are blablabla?"" question.</p>

<p>On the other hand, if the sentence is the following: Omelette is delicious</p>

<p>Human english speaker still can easily guess that the question is the second one.</p>

<p>So, how can computer infer such thing?</p>
",Multilingual Language Processing & Language Identification,differentiating answer wh query known way differentiating answer wh question example know sentence omelette stir fried egg possible question answer omelette omelette human english speaker easily guess question first one noun blablabla likely answer blablabla question hand sentence following omelette delicious human english speaker still easily guess question second one computer infer thing
Language recognition in Java,"<p>Is there any language recognition open-source for Java? Found only for c/c++.</p>

<p><strong>UPD:</strong></p>

<p>I`m talking about human text language. Example:</p>

<p>Input: My name is John.
Output: English.</p>

<p>Input: Ich heisse John.
Output: German.</p>

<p>Input: –ú–µ–Ω—è –∑–æ–≤—É—Ç –î–∂–æ–Ω.
Output: Russian.</p>
",Multilingual Language Processing & Language Identification,language recognition java language recognition open source java found c c upd talking human text language example input name john output english input ich heisse john output german input output russian
Dictionary for NLP,"<p>I am in need of an offline C# (or ANY language for that matter, I can port it over) library that will be able to take a part of speech (that I will detect) and tell me the plural form of the word or the different tenses of the word that I can then turn around and use. </p>

<p>Does such a library exist for the English language?</p>
",Multilingual Language Processing & Language Identification,dictionary nlp need offline c language matter port library able take part speech detect tell plural form word different tense word turn around use doe library exist english language
Algorithm to determine probable language of a text,"<p>I'm searching for a simple algorithm or an open source library (PHP) allowing to estimate whether a text mainly uses a specific language. I found the <a href=""https://stackoverflow.com/questions/14765632/what-is-a-relatively-simple-way-to-determine-the-probability-that-a-sentence-is"">following answer</a> relating to Python, which probably leads in the right direction. But something working out-of-the-box for PHP would be a charm.</p>

<p>Of course something like an n-gram estimator wouldn't be too hard to implement, but it requires a reference database as well. </p>

<p>The actual problem to solve is as follows. I run a WordPress blog, which currently is flooded by SPAM. The blog is in German language and virtually all trackback spam is English. My idea is to inmmediately spam all trackbacks seeming to be English. However, I cannot use marker words, because I do not want to spam typos or citations.</p>

<p><strong>My solution:</strong></p>

<p>Using the answers to this question I implemented a solution, which detects German by a simple stopword ratio. Any comment must contain at least 25% German stopwords, if it has a link. So you can still comment something like ""cool article"", which has no stopwords at all, but if you put a link, you should bother to write proper language.</p>

<p>Unfortunately the stopwords from NLTK are incorrect. The list contains words, which do not exist in German. So I used the <a href=""http://snowball.tartarus.org/algorithms/german/stop.txt"" rel=""nofollow noreferrer"">snowball</a> list. Using the <a href=""http://search.cpan.org/~dankogai/Regexp-Optimizer-0.23/lib/Regexp/Optimizer.pm"" rel=""nofollow noreferrer"">Perl regexp optimizer</a> I condensed the entire list into a single regexp and count the stopwords using preg_match_all(). The whole filter is 25 lines, a third of the Perl code to produce the regexp from the list. Let's see how it performs in the wild.</p>

<p>Thanks for your help.</p>
",Multilingual Language Processing & Language Identification,algorithm determine probable language text searching simple algorithm open source library php allowing estimate whether text mainly us specific language found unfortunately stopwords nltk incorrect list contains word exist german used snowball list using perl regexp optimizer condensed entire list single regexp count stopwords using preg match whole filter line third perl code produce regexp list let see performs wild thanks help
"Has NLP ever been used to detect if someone is a native (English, or other) speaker?","<p>Are there any code samples or papers on the subject? I have not been able to find any resources directly related to the question after a bit of research.</p>
",Multilingual Language Processing & Language Identification,ha nlp ever used detect someone native english speaker code sample paper subject able find resource directly related question bit research
How to change the word order of phrasal verbs in a POS-tagged corpus file,"<p>I have a POS-tagged parallel corpus text file in which I would like to do word reordering, so that the ""separable phrasal verb particle"" will appear next to the ""verb"" of the phrasal verb ('make up a plan' instead of 'make a plan up') . This used for preprocessing in a statistical machine translation system. Here are some example lines from the POS-tagged text file:</p>

<ol>
<li>you_PRP mean_VBP we_PRP should_MD kick_VB them_PRP out_RP ._. </li>
<li>don_VB &apos;t_NNP take_VB it_PRP off_RP until_IN I_PRP say_VBP so_RB ._.</li>
<li>please_VB help_VB the_DT man_NN out_RP ._.</li>
<li>shut_VBZ it_PRP down_RP !_.</li>
</ol>

<p>I would like to move all the particles (in the examples: out_RP, off_RP, out_RP, down_RP) right next to the closest preceding verb (i.e. the verb that in combination with the particle makes up the phrasal verb). Here's what the lines should looks like after having changed the word order:  </p>

<ol>
<li>you_PRP mean_VBP we_PRP should_MD kick_VB out_RP them_PRP ._. </li>
<li>don_VB &apos;t_NNP take_VB off_RP it_PRP until_IN I_PRP say_VBP so_RB ._.</li>
<li>please_VB help_VB out_RP the_DT man_NN ._.</li>
<li>shut_VBZ down_RP it_PRP !_.</li>
</ol>

<p>So far I've tried using python and regular expressions to sort the problem by using re.findall:</p>

<pre><code>import re 

file=open('first100k.txt').read()
matchline3='\w*_VB.?\s\w*_DT\s\w*_NN\s\w*_RP'
wordorder1=re.findall(matchline3,file)
print wordorder1
</code></pre>

<p>This will find all the phrasal verbs in word order 1(see below), but that's as far as I've got since I can't figure out how to move the particle next to the verb. Any ideas how to solve this problem properly (not necessarily by using python and regex)? I would like to be able to search for all phrasal verbs and move the particles in the following word orders:</p>

<p>(The used tags are taken from the Penn Treebank tagset (<a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow"">http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a> )(the x denotes an optional character in order to include all verb forms, and * denotes a wildcard word))</p>

<ol>
<li>*_VBx+*_DT+*_NN+*_RP</li>
<li>*_VBx+*_DT+*_NNS+*_RP</li>
<li>*_<em>VBx+*</em>_DT+*_.JJ+*_NN+*_RP</li>
<li><p>*_<em>VBx+*</em>_DT+*_.JJ+*_NNS+*_RP</p></li>
<li><p>*_VBx+*_PRP$+*_NN+*_RP</p></li>
<li>*_VBx+*_PRP$+*_NNS+*_RP</li>
<li>*_<em>VBx+*</em>_PRP$+*_.JJ+*_NN+*_RP</li>
<li><p>*_<em>VBx+*</em>_PRP$+*_.JJ+*_NNS+*_RP</p></li>
<li><p>*_VBx+*_NNP+*_RP</p></li>
<li><p>*_VBx+*_JJ+*_NNP+*_RP</p></li>
<li><p>*_VBx+*_NNPS+*_RP</p></li>
<li><p>*_VBx+*_PRP+*_RP </p></li>
</ol>

<p>In advance, thanks for your help!</p>
",Multilingual Language Processing & Language Identification,change word order phrasal verb po tagged corpus file po tagged parallel corpus text file would like word reordering separable phrasal verb particle appear next verb phrasal verb make plan instead make plan used preprocessing statistical machine translation system example line po tagged text file prp mean vbp prp md kick vb prp rp vb nnp take vb prp rp prp say vbp rb please vb help vb dt man nn rp shut vbz prp rp would like move particle example rp rp rp rp right next closest preceding verb e verb combination particle make phrasal verb line look like changed word order prp mean vbp prp md kick vb rp prp vb nnp take vb rp prp prp say vbp rb please vb help vb rp dt man nn shut vbz rp prp far tried using python regular expression sort problem using findall find phrasal verb word order see far got since figure move particle next verb idea solve problem properly necessarily using python regex would like able search phrasal verb move particle following word order used tag taken penn treebank tagset x denotes optional character order include verb form denotes wildcard word vbx dt nn rp vbx dt nns rp vbx dt jj nn rp vbx dt jj nns rp vbx prp nn rp vbx prp nns rp vbx prp jj nn rp vbx prp jj nns rp vbx nnp rp vbx jj nnp rp vbx nnps rp vbx prp rp advance thanks help
Natural Language Processing - Converting Text Features Into Feature Vectors,"<p>So I've been working on a natural language processing project in which I need to classify different styles of writing. Assuming that semantic features from texts have already been extracted for me, I plan to use Weka in Java to train SVM classifiers using these features that can be used to classify other different texts. </p>

<p>The part I'm having trouble on is that to train an SVM, the features must be converted into a feature vector. I'm not sure how you would be able to represent features such as vocabulary richness, n-grams, punctuation, number of paragraphs, and paragraph length as numbers in a vector. If somebody could point in the right direction, that would be greatly appreciated.</p>
",Multilingual Language Processing & Language Identification,natural language processing converting text feature feature vector working natural language processing project need classify different style writing assuming semantic feature text already extracted plan use weka java train svm classifier using feature used classify different text part trouble train svm feature must converted feature vector sure would able represent feature vocabulary richness n gram punctuation number paragraph paragraph length number vector somebody could point right direction would greatly appreciated
Natural Language Programming solutions in C#?,"<p>Is there any library for natural language processing in C# ? Given a question like 'what is <code>&lt;xyz&gt;</code>?' 'when is <code>&lt;xyz&gt;</code> expiring?' I would like to extract the attributes which help me search some data source and return meaningful results.</p>

<p>For E.g. 'what' in a question giving me the word 'definition' and 'when' giving me 'timeline'. I can program simply this using a limited database of word which refers to questions and map them to these categories, but will NLP offer better treatment of the problem?</p>

<p>Also, I have various data-sources say one, two ,three and I would need a long list of keywords which help me determine the data-source,</p>

<p>for eg. xyz would map to 'one' and abc to 'two'. </p>

<p><strong>How can I use natural language processing to understand which word falls under which category? Or is there a better way to do the same? Any application of FullTextSearch here?</strong></p>
",Multilingual Language Processing & Language Identification,natural language programming solution c library natural language processing c given question like expiring would like extract attribute help search data source return meaningful result e g question giving word definition giving timeline program simply using limited database word refers question map category nlp offer better treatment problem also various data source say one two three would need long list keywords help determine data source eg xyz would map one abc two use natural language processing understand word fall category better way application fulltextsearch
Predict scores for documents?,"<p>I have a set of documents and their corresponding scores, which are something very similar to the number of upvote/downvote on SO. I'd like to build a system that is able to predict the score given a document. Some machine learning approaches for regression, and natural language processing techniques for deriving useful features from the document might be helpful.</p>

<p>Is there any state-of-art method for such problem? I had done some searches on Google Scholar but failed to find a satisfying result. </p>

<p>Thanks.</p>
",Multilingual Language Processing & Language Identification,predict score document set document corresponding score something similar number upvote downvote like build system able predict score given document machine learning approach regression natural language processing technique deriving useful feature document might helpful state art method problem done search google scholar failed find satisfying result thanks
Natural Language Processing (NLP) with Java,"<p>I'm starting a project in which sentiment analysis is going to take center stage. Specifically, we'll be doing sentiment analysis of Twitter, Facebook, YouTube and other social network data. </p>

<p>I know of OpenNLP from Apache. It appears great but I think it's a little heavyweight for what I want to do in addition to it's dependence on Hadoop and the like. I haven't used it before and I may be wrong in my assessment of it. </p>

<p>I've seen elsewhere on this site about Stanford NLP. I can't seem to get a good starting point with this library; a tutorial sort of. </p>

<p>Also, I've read about Sentiment Anaysis APIs like AlchemyAPI on this site, but I want a solution I'm fully in control of. I just want a library I can bundle with my application. </p>

<p>In a nut shell I'm looking for a solution that is lightweight, and that I can set up in my local PC. Also, a pointer to a good starting point for Stanford NLP or OpenNLP will be appreciated very much.</p>

<p><strong>UPDATE:</strong></p>

<p>I've gone through the UIMA documentation and its support for components like OpenNLP components and other third party components, in addition to its inbuilt text processing capabilities makes it an attractive starting point. It's open architecture makes me feel it's ideal for what I want to achieve. Additional recommendation or advice will still be appreciated very much.</p>
",Multilingual Language Processing & Language Identification,natural language processing nlp java starting project sentiment analysis going take center stage specifically sentiment analysis twitter facebook youtube social network data know opennlp apache appears great think little heavyweight want addition dependence hadoop like used may wrong assessment seen elsewhere site stanford nlp seem get good starting point library tutorial sort also read sentiment anaysis apis like alchemyapi site want solution fully control want library bundle application nut shell looking solution lightweight set local pc also pointer good starting point stanford nlp opennlp appreciated much update gone uima documentation support component like opennlp component third party component addition inbuilt text processing capability make attractive starting point open architecture make feel ideal want achieve additional recommendation advice still appreciated much
download text of an article in google news,"<p>I'd like to do some natural language processing (nlp) mining based on news.</p>

<p>google has a smart way of extracting a useful text from news articles. that's the text it displays in google news. is there an easy way to download that text rather then copy the displayed text from google news then find that text in the original html and extract the div with that text?</p>
",Multilingual Language Processing & Language Identification,download text article google news like natural language processing nlp mining based news google ha smart way extracting useful text news article text display google news easy way download text rather copy displayed text google news find text original html extract div text
How to compute letter frequency similarity?,"<p>Given this data (relative letter frequency from both languages):</p>

<pre><code>spanish =&gt; 'e' =&gt; 13.72, 'a' =&gt; 11.72, 'o' =&gt; 8.44, 's' =&gt; 7.20, 'n' =&gt; 6.83,
english =&gt; 'e' =&gt; 12.60, 't' =&gt; 9.37, 'a' =&gt; 8.34, 'o' =&gt; 7.70, 'n' =&gt; 6.80,
</code></pre>

<p>And then computing the letter frequency for the string ""this is a test"" gives me:</p>

<pre><code>""t""=&gt;21.43, ""s""=&gt;14.29, ""i""=&gt;7.14, ""r""=&gt;7.14, ""y""=&gt;7.14, ""'""=&gt;7.14, ""h""=&gt;7.14, ""e""=&gt;7.14, ""l""=&gt;7.14
</code></pre>

<p>So, what would be a good approach for matching the given string letter frequency with a language (and try to detect the language)? I've seen (and have tested) some examples using levenshtein distance, and it seems to work fine until you add more languages.</p>

<pre><code>""this is a test"" gives (shortest distance:) [:english, 13] ...
""esto es una prueba"" gives (shortest distance:) [:spanish, 13] ...
</code></pre>
",Multilingual Language Processing & Language Identification,compute letter frequency similarity given data relative letter frequency language computing letter frequency string test give would good approach matching given string letter frequency language try detect language seen tested example using levenshtein distance seems work fine add language
Find basic words and estimate their difficulty,"<p>I'm looking for a possibly simple solution of the following problem:</p>

<p><strong>Given input of a sentence</strong> like</p>

<pre><code>""Absence makes the heart grow fonder.""
</code></pre>

<p><strong>Produce a list of <em>basic</em> words followed by their difficulty/complexity</strong></p>

<pre><code>[[""absence"", 0.5], [""make"", 0.05], [""the"", 0.01""], [""grow"", 0.1""], [""fond"", 0.5]]
</code></pre>

<p><strong>Let's assume</strong> that:</p>

<ul>
<li>all the words in the sentence are valid English words</li>
<li>popularity is an acceptable measure of difficulty/complexity</li>
<li>base word can be understood in any constructive way (see below)</li>
<li>difficulty/complexity is on scale from 0 - piece of cake to 1 - mind-boggling </li>
<li>difficulty bias is ok, better to be mistaken saying easy is though than the other way</li>
<li>working simple solution is preferred to flawless but complicated stuff</li>
<li><strong>[edit]</strong> there is no interaction with user</li>
<li><strong>[edit]</strong> we can handle any proper English input</li>
<li><strong>[edit]</strong> a word is not more difficult than it's basic form (because as smart beings we can create <em>unhappily</em> if we know <em>happy</em>), unless it creates a new word (<em>unlikely</em> is not same difficulty as <em>like</em>)</li>
</ul>

<p><strong>General ideas:</strong></p>

<p>I considered using Google searches or sites like <a href=""http://www.wordcount.org/main.php"" rel=""nofollow"">Wordcount</a> to estimate words popularity that could indicate its difficulty. However, both solutions give different results depending on the form of entered words. Google gives 316m results for <em>fond</em> but 11m for <em>fonder</em>, whereas Wordcount gives them ranks of 6k and 54k.</p>

<p>Transforming words to their basic forms is not a must but solves ambiguity problem (and makes it easy to create dictionary links), however it's not a simple task and its sense could me found arguable. Obviously <em>fond</em> should be taken instead of <em>fonder</em>, however investigating <em>believe</em> instead of <em>unbelievable</em> seems to be an overkill (<strong>[edit]</strong> it might be not the best example, but there is a moment when modifying basic word we create a new one <em>like</em> -> <em>likely</em>) and words like <em>doorkeeper</em> shouldn't be cut into two.</p>

<p>Some ideas of what should be consider basic word can be found <a href=""http://en.wikipedia.org/wiki/Basic_English#Rules"" rel=""nofollow"">here on Wikipedia</a> but maybe a simpler way of determining it would be a use of a dictionary. For instance according to dictionary.reference.com <a href=""http://dictionary.reference.com/browse/unbelievable"" rel=""nofollow"">unbelievable is a basic word</a> whereas <a href=""http://dictionary.reference.com/browse/fonder"" rel=""nofollow"">fonder comes from fond</a> but then <a href=""http://dictionary.reference.com/browse/growing"" rel=""nofollow"">grow is not the same as growing</a></p>

<p><strong>Idea of a solution:</strong></p>

<p>It seems to me that the best way to handle the problem would be using a dictionary to find <em>basic</em> words, apply some of the Wikipedia rules and then use Wordcount (maybe combined with number of Google searches) to estimate difficulty.</p>

<p>Still, there might (probably is a simpler and better) way or ready to use algorithms. I would appreciate any solution that deals with this problem and is easy to put in practice. Maybe I'm just trying to reinvent the wheel (or maybe you know my approach would work just fine and I'm wasting my time deliberating instead of coding what I have). I would, however, prefer to avoid implementing frequency analysis algorithms or preparing a corpus of texts.</p>
",Multilingual Language Processing & Language Identification,find basic word estimate difficulty looking possibly simple solution following problem given input sentence like produce list basic word followed difficulty complexity let assume word sentence valid english word popularity acceptable measure difficulty complexity base word understood constructive way see difficulty complexity scale piece cake mind boggling difficulty bias ok better mistaken saying easy though way working simple solution preferred flawless complicated stuff edit interaction user edit handle proper english input edit word difficult basic form smart create unhappily know happy unless creates new word unlikely difficulty like general idea considered using google search site like wordcount estimate word popularity could indicate difficulty however solution give different result depending form entered word google give result fond fonder whereas wordcount give rank k k transforming word basic form must solves ambiguity problem make easy create dictionary link however simple task sense could found arguable obviously fond taken instead fonder however investigating believe instead unbelievable seems overkill edit might best example moment modifying basic word create new one like likely word like doorkeeper cut two idea consider basic word found wikipedia maybe simpler way determining would use dictionary instance according dictionary reference com unbelievable basic word whereas fonder come fond grow growing idea solution seems best way handle problem would using dictionary find basic word apply wikipedia rule use wordcount maybe combined number google search estimate difficulty still might probably simpler better way ready use algorithm would appreciate solution deal problem easy put practice maybe trying reinvent wheel maybe know approach would work fine time deliberating instead coding would however prefer avoid implementing frequency analysis algorithm preparing corpus text
Find basic words and estimate their difficulty,"<p>I'm looking for a possibly simple solution of the following problem:</p>

<p><strong>Given input of a sentence</strong> like</p>

<pre><code>""Absence makes the heart grow fonder.""
</code></pre>

<p><strong>Produce a list of <em>basic</em> words followed by their difficulty/complexity</strong></p>

<pre><code>[[""absence"", 0.5], [""make"", 0.05], [""the"", 0.01""], [""grow"", 0.1""], [""fond"", 0.5]]
</code></pre>

<p><strong>Let's assume</strong> that:</p>

<ul>
<li>all the words in the sentence are valid English words</li>
<li>popularity is an acceptable measure of difficulty/complexity</li>
<li>base word can be understood in any constructive way (see below)</li>
<li>difficulty/complexity is on scale from 0 - piece of cake to 1 - mind-boggling </li>
<li>difficulty bias is ok, better to be mistaken saying easy is though than the other way</li>
<li>working simple solution is preferred to flawless but complicated stuff</li>
<li><strong>[edit]</strong> there is no interaction with user</li>
<li><strong>[edit]</strong> we can handle any proper English input</li>
<li><strong>[edit]</strong> a word is not more difficult than it's basic form (because as smart beings we can create <em>unhappily</em> if we know <em>happy</em>), unless it creates a new word (<em>unlikely</em> is not same difficulty as <em>like</em>)</li>
</ul>

<p><strong>General ideas:</strong></p>

<p>I considered using Google searches or sites like <a href=""http://www.wordcount.org/main.php"" rel=""nofollow"">Wordcount</a> to estimate words popularity that could indicate its difficulty. However, both solutions give different results depending on the form of entered words. Google gives 316m results for <em>fond</em> but 11m for <em>fonder</em>, whereas Wordcount gives them ranks of 6k and 54k.</p>

<p>Transforming words to their basic forms is not a must but solves ambiguity problem (and makes it easy to create dictionary links), however it's not a simple task and its sense could me found arguable. Obviously <em>fond</em> should be taken instead of <em>fonder</em>, however investigating <em>believe</em> instead of <em>unbelievable</em> seems to be an overkill (<strong>[edit]</strong> it might be not the best example, but there is a moment when modifying basic word we create a new one <em>like</em> -> <em>likely</em>) and words like <em>doorkeeper</em> shouldn't be cut into two.</p>

<p>Some ideas of what should be consider basic word can be found <a href=""http://en.wikipedia.org/wiki/Basic_English#Rules"" rel=""nofollow"">here on Wikipedia</a> but maybe a simpler way of determining it would be a use of a dictionary. For instance according to dictionary.reference.com <a href=""http://dictionary.reference.com/browse/unbelievable"" rel=""nofollow"">unbelievable is a basic word</a> whereas <a href=""http://dictionary.reference.com/browse/fonder"" rel=""nofollow"">fonder comes from fond</a> but then <a href=""http://dictionary.reference.com/browse/growing"" rel=""nofollow"">grow is not the same as growing</a></p>

<p><strong>Idea of a solution:</strong></p>

<p>It seems to me that the best way to handle the problem would be using a dictionary to find <em>basic</em> words, apply some of the Wikipedia rules and then use Wordcount (maybe combined with number of Google searches) to estimate difficulty.</p>

<p>Still, there might (probably is a simpler and better) way or ready to use algorithms. I would appreciate any solution that deals with this problem and is easy to put in practice. Maybe I'm just trying to reinvent the wheel (or maybe you know my approach would work just fine and I'm wasting my time deliberating instead of coding what I have). I would, however, prefer to avoid implementing frequency analysis algorithms or preparing a corpus of texts.</p>
",Multilingual Language Processing & Language Identification,find basic word estimate difficulty looking possibly simple solution following problem given input sentence like produce list basic word followed difficulty complexity let assume word sentence valid english word popularity acceptable measure difficulty complexity base word understood constructive way see difficulty complexity scale piece cake mind boggling difficulty bias ok better mistaken saying easy though way working simple solution preferred flawless complicated stuff edit interaction user edit handle proper english input edit word difficult basic form smart create unhappily know happy unless creates new word unlikely difficulty like general idea considered using google search site like wordcount estimate word popularity could indicate difficulty however solution give different result depending form entered word google give result fond fonder whereas wordcount give rank k k transforming word basic form must solves ambiguity problem make easy create dictionary link however simple task sense could found arguable obviously fond taken instead fonder however investigating believe instead unbelievable seems overkill edit might best example moment modifying basic word create new one like likely word like doorkeeper cut two idea consider basic word found wikipedia maybe simpler way determining would use dictionary instance according dictionary reference com unbelievable basic word whereas fonder come fond grow growing idea solution seems best way handle problem would using dictionary find basic word apply wikipedia rule use wordcount maybe combined number google search estimate difficulty still might probably simpler better way ready use algorithm would appreciate solution deal problem easy put practice maybe trying reinvent wheel maybe know approach would work fine time deliberating instead coding would however prefer avoid implementing frequency analysis algorithm preparing corpus text
English grammar parsing in PHP (Link Grammar),"<p>Is there any way to use the Link Grammar or AbiSource grammar checker in PHP (or C# but I'd prefer php)? I need to have a tree structure for English sentences. Any ideas? The only things I found were in C and I can't use them on a shared host.</p>
",Multilingual Language Processing & Language Identification,english grammar parsing php link grammar way use link grammar abisource grammar checker php c prefer php need tree structure english sentence idea thing found c use shared host
Resum&#233; Parsing in .Net framework using Natural Language Processing,"<p>I'm trying to compare two resum√©s. I have a criteria that if certain fields match it is a duplicate. As you may know, resum√© styles differ. How do I understand that the name field is a name field, so I can store it somewhere and compare it with the same field in another resum√©?</p>

<p>As of now I have used an Interop method and I am getting all document content in a string. From the string I am splitting on all the \t, \r and empty spaces, and getting an array. From the array, how can I get my own standard xml format like below:</p>

<p><strong>XML Format:</strong></p>

<pre><code> &lt;CANDIDATE_FULL_NAME&gt;CandidateName here&lt;/CANDIDATE_FULL_NAME&gt;
 &lt;CANDIDATE_FIRST_NAME&gt;CandidateFirstName here&lt;/CANDIDATE_FIRST_NAME&gt;
 &lt;CANDIDATE_LAST_NAME&gt;CandidateLastName here&lt;/CANDIDATE_LAST_NAME&gt;
 &lt;PRIMARY_EMAI`enter code here`L_ID&gt;name@gmail.com&lt;/PRIMARY_EMAIL_ID&gt;
 &lt;PHONE_BASIC&gt;+919720018454155&lt;/PHONE_`enter code here`BASIC&gt;
 &lt;DOB&gt;8/2/1987&lt;/DOB&gt;
 &lt;STREET1&gt;&lt;/STREET1&gt;
 &lt;STREET2&gt;&lt;/STREET2&gt;
 &lt;CITY&gt;&lt;/CITY&gt;
 &lt;REGION&gt;&lt;/REGION&gt;
 &lt;COUNTRY&gt;&lt;/COUNTRY&gt;
 &lt;PIN&gt;&lt;/PIN&gt;
</code></pre>

<p>After reviewing my questions it's obvious to me that I have no idea what I'm doing and a starting point would be much appreciated.</p>
",Multilingual Language Processing & Language Identification,resum parsing net framework using natural language processing trying compare two resum criterion certain field match duplicate may know resum style differ understand name field name field store somewhere compare field another resum used interop method getting document content string string splitting r empty space getting array array get standard xml format like xml format reviewing question obvious idea starting point would much appreciated
Arabic WordNet with not-formatted words,"<p>Is it necessary for the word input to WordNet to be formatted like ""ÿßŸÑÿ™ŸëŸèŸÅŸëŸéÿßÿ≠Ÿí"" and can't expect ""ÿßŸÑÿ™ŸÅÿßÿ≠""...
is there any library or service taking not-formatted Arabic word returning a list of all its possible synonyms.</p>
",Multilingual Language Processing & Language Identification,arabic wordnet formatted word necessary word input wordnet formatted like expect library service taking formatted arabic word returning list possible synonym
Extract only English sentences,"<p>I need to extract posts and tweets from Facebok and Twitter into our database for analysis. My problem is the system can process on the English sentences (phrases) only. So how can I remove non-English posts, tweets from my database.</p>

<p>If you do know any algorithm in NLP can do this, please tell me.</p>

<p>Thanks and regards</p>
",Multilingual Language Processing & Language Identification,extract english sentence need extract post tweet facebok twitter database analysis problem system process english sentence phrase remove non english post tweet database know algorithm nlp please tell thanks regard
Parse arbitrary text to produce dependency graph,"<p>How to create dependency graph (parse tree) for random sentences. Is there any predined grammer to parse english sentences using nltk.</p>

<p>Example:</p>

<p>I want to make a parse tree for the sentence </p>

<p>‚ÄúA large company needs a sustainable business model.‚Äù
&nbsp;&nbsp;which should look like this.</p>

<p><img src=""https://i.sstatic.net/BL7tS.jpg"" alt=""enter image description here""></p>

<p>Please suggest me how this can be done.</p>
",Multilingual Language Processing & Language Identification,parse arbitrary text produce dependency graph create dependency graph parse tree random sentence predined grammer parse english sentence using nltk example want make parse tree sentence large company need sustainable business model look like please suggest done
CYK (Cocke-Younger-Kasami) Grammar Rules,"<p>I am interested in Natural Language Parsing and have written a Brill Part of Speech Tagger, and would like to enhance it by combining it with a a POS tagger based on grammar rules. Is anyone aware of open source ruleset files for English anywhere? I am particularly interested in anything related to the CYK (Cocke-Younger-Kasami) algorithm, C# especially. Thanks. </p>
",Multilingual Language Processing & Language Identification,cyk cocke younger kasami grammar rule interested natural language parsing written brill part speech tagger would like enhance combining po tagger based grammar rule anyone aware open source ruleset file english anywhere particularly interested anything related cyk cocke younger kasami algorithm c especially thanks
postgresql phrase extraction &amp; ranking,"<p>From selected rows in a table, how can one extract and rank phrases based on how often they occur?</p>

<p>example 1: <a href=""http://developer.yahoo.com/search/content/V1/termExtraction.html"" rel=""nofollow noreferrer"">http://developer.yahoo.com/search/content/V1/termExtraction.html</a></p>

<p>example 2: <a href=""http://mirror.me/i/love"" rel=""nofollow noreferrer"">http://mirror.me/i/love</a></p>

<pre><code>INPUT:
CREATE TABLE phrases (
    id  BIGSERIAL,
phrase VARCHAR(10000)
);

INSERT INTO phrases (phrase) VALUES (‚ÄòItalian sculptors and painters of the renaissance favored the Virgin Mary for inspiration.‚Äô)
INSERT INTO phrases (phrase) VALUES (‚ÄòAndrea Bolgi was an italian sculptor‚Äô)

DESIRED OUTPUT:
phrase | weight
italian sculptor  |  5
virgin mary | 2
painters | 1
renaissance | 1
inspiration | 1
Andrea Bolgi | 1
</code></pre>

<p>To find just words, not phrases, one could use</p>

<pre><code>SELECT * FROM ts_stat('SELECT to_tsvector(''simple'', phrase) FROM phrases')
ORDER BY nentry DESC, ndoc DESC, word;
</code></pre>

<p>Some notes:</p>

<ul>
<li>phrases could contain ‚Äústop words‚Äù, e.g. ‚Äúeasy to answer‚Äù</li>
<li>ideally, english language variations and synonyms would be automatically grouped. </li>
</ul>

<p>Could pg_trgm help? (it‚Äôs ok if only 2 and 3 word phrases are found). How exactly?</p>

<p>Related questions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/1426383/what-techniques-tools-are-there-for-discovering-common-phrases-in-chunks-of-text"">What techniques/tools are there for discovering common phrases in chunks of text?</a></li>
<li><a href=""https://stackoverflow.com/questions/1928997/how-to-find-common-phrases-in-a-large-body-of-text"">How to find common phrases in a large body of text</a></li>
<li><a href=""https://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries"">How to extract common / significant phrases from a series of text entries</a></li>
</ul>
",Multilingual Language Processing & Language Identification,postgresql phrase extraction ranking selected row table one extract rank phrase based often occur example example find word phrase one could use note phrase could contain stop word e g easy answer ideally english language variation synonym would automatically grouped could pg trgm help ok word phrase found exactly related question href extract common significant phrase series text entry
Find the 10 most frequently occurring words with morphology,"<p>Please tell me how to solve this problem better.</p>

<p>I have the text of the Russian and I want to find the 10 most common words <strong>with morphology</strong>.
Maybe there is any open source libraries to solve this issue in <strong>python</strong>?</p>
",Multilingual Language Processing & Language Identification,find frequently occurring word morphology please tell solve problem better text russian want find common word morphology maybe open source library solve issue python
How to determine a word is English or any other language,"<p>I am developing a small library automation software and I need to determine a word is in <a href=""http://en.wikipedia.org/wiki/English_language"" rel=""nofollow"">English</a> or <a href=""http://en.wikipedia.org/wiki/Turkish_language"" rel=""nofollow"">Turkish</a>. An example scenario is like this:</p>

<ul>
<li>User enters a book title.</li>
<li>Determine it's Turkish or English.</li>
<li>Set the languge combobox to the respective language to help user fill the form. </li>
</ul>

<p>A friend of mine suggested me ""connect to Google Translate and use it"" which seems reasonable but an algorithm without connecting an external service or database will be more appropriate for me. (I also search the Turkish/English specific characters like √ß,≈ü,ƒ∞/w,x to decide) Therefore I am searching an algorithm to do this job maybe based on letter frequencies or something like it. Anything available in literature? Thanks, in advance. (I use php, mysql if it's important)</p>
",Multilingual Language Processing & Language Identification,determine word english language developing small library automation software need determine word english turkish example scenario like user enters book title determine turkish english set languge combobox respective language help user fill form friend mine suggested connect google translate use seems reasonable algorithm without connecting external service database appropriate also search turkish english specific character like w x decide therefore searching algorithm job maybe based letter frequency something like anything available literature thanks advance use php mysql important
Text processing to identify parts of speech,"<p>I've to write a program (in Java) to identify several parts of speech like nouns, adjectives, verbs etc. The program should also identify number (in numeric e.g. 10) and numbers written in plain English (ten, hundred etc) and much more. I'm not sure that what is the way forward. Is there any library available that can help? Can this be done only with regex? Or do I need to learn NLP? </p>

<p>Please suggest a way forward.</p>
",Multilingual Language Processing & Language Identification,text processing identify part speech write program java identify several part speech like noun adjective verb etc program also identify number numeric e g number written plain english ten hundred etc much sure way forward library available help done regex need learn nlp please suggest way forward
PHP judge a string as a human name or other text,"<p>The closest existing question I have found is <a href=""https://stackoverflow.com/questions/1261338"">this</a> or <a href=""https://stackoverflow.com/questions/3258688/"">this</a></p>

<p>I would like to write a function or class that accepts a string and then based on whatever criteria can be programmed into it will return the probability that it is a real human name. At the moment I would expect it to be heavily biased toward English or European names or English transliterations of other names. (for example, ""bob"", ""bob smith"", and ""smith"" should all return 1.0 and ""sfgoisxdzzg"" should return something like .001 or even .0000001)</p>

<p>Does anyone know if this is already done / being done? (even if in another language)
My first thought was that I'd have to do some sort of machine learning script. My problem with that is my complete ignorance of any machine learning theory.</p>

<p>So, the second part of my question is this: Is machine learning a viable option for tackling this problem? If so, what resources should I start with to learn how to do it? IF not, can you point me in the right direction?</p>
",Multilingual Language Processing & Language Identification,php judge string human name text closest existing question found would like write function class accepts string based whatever criterion programmed return probability real human name moment would expect heavily biased toward english european name english transliteration name example bob bob smith smith return sfgoisxdzzg return something like even doe anyone know already done done even another language first thought wa sort machine learning script problem complete ignorance machine learning theory second part question machine learning viable option tackling problem resource start learn point right direction
Determine POS tagging in English based on database files,"<p>I'm a little bit confused how to determine part-of-speech tagging in English. In this case, I assume that one word in English has one type, for example word ""book"" is recognized as NOUN, not as VERB. I want to recognize English sentences based on tenses. For example, ""I sent the book"" is recognized as past tense.</p>

<p>Description:</p>

<p>I have a number of database (*.txt) files: NounList.txt, verbList.txt, adjectiveList.txt, adverbList.txt, conjunctionList.txt, prepositionList.txt, articleList.txt. And if input words are available in the database, I assume that type of those words can be concluded. But, how to begin lookup in the databases? For example, ""I sent the book"": how to begin a search in the databases for every word, ""I"" as Noun, ""sent"" as verb, ""the"" as article, ""book"" as noun? Any better approach than searching every word in every database? I doubt that every databases has unique element.</p>

<p>I enclose my perspective here.</p>

<pre><code>private List&lt;string&gt; ParseInput(String allInput)
{
    List&lt;string&gt; listSentence = new List&lt;string&gt;();

    char[] delimiter = "".?!;"".ToCharArray();
    var sentences = allInput.Split(delimiter, StringSplitOptions.RemoveEmptyEntries).Select(s =&gt; s.Trim());

    foreach (var s in sentences)
        listSentence.Add(s);

        return listSentence;
}

private void tenseReviewMenu_Click(object sender, EventArgs e)
    {
        string allInput = rtbInput.Text;

        List&lt;string&gt; listWord = new List&lt;string&gt;();
        List&lt;string&gt; listSentence = new List&lt;string&gt;();

        HashSet&lt;string&gt; nounList = new HashSet&lt;string&gt;(getDBList(""nounList.txt""));
        HashSet&lt;string&gt; verbList = new HashSet&lt;string&gt;(getDBList(""verbList.txt""));
        HashSet&lt;string&gt; adjectiveList = new HashSet&lt;string&gt;(getDBList(""adjectiveList.txt""));
        HashSet&lt;string&gt; adverbList = new HashSet&lt;string&gt;(getDBList(""adverbList.txt""));

        char[] separator = new char[] { ' ', '\t', '\n', ',' etc... };         

        listSentence = ParseInput(allInput);

        foreach (string sentence in listSentence)
        {
            foreach (string word in sentence.Split(separator))
                if (word.Trim() != """")
                    listWord.Add(word);               
        }

        string testPOS = """";

        foreach (string word in listWord)
        {
            if (nounList.Contains(word.ToLowerInvariant()))
                testPOS += ""noun "";
            else if (verbList.Contains(word.ToLowerInvariant()))
                testPOS += ""verb "";
            else if (adjectiveList.Contains(word.ToLowerInvariant()))
                testPOS += ""adj "";
            else if (adverbList.Contains(word.ToLowerInvariant()))
                testPOS += ""adv "";

        }
        tbTest.Text = testPOS;
    }
</code></pre>

<p>POS tagging is my secondary explanation in my assignment. So I use a simple approach to determine POS tagging that is based on database. But, if there's a simpler approach: easy to use, easy to understand, easy to get pseudocode, easy to design... to determine POS tagging, please let me know.</p>
",Multilingual Language Processing & Language Identification,determine po tagging english based database file little bit confused determine part speech tagging english case assume one word english ha one type example word book recognized noun verb want recognize english sentence based tense example sent book recognized past tense description number database txt file nounlist txt verblist txt adjectivelist txt adverblist txt conjunctionlist txt prepositionlist txt articlelist txt input word available database assume type word concluded begin lookup database example sent book begin search database every word noun sent verb article book noun better approach searching every word every database doubt every database ha unique element enclose perspective po tagging secondary explanation assignment use simple approach determine po tagging based database simpler approach easy use easy understand easy get pseudocode easy design determine po tagging please let know
Dutch Grammar in python&#39;s NLTK,"<p>I am working on a Dutch corpus and I want to know if NLTK has dutch grammar embedded in it so I can parse my sentences? In general does NLTK only work on English? I know that it has the Alpino dutch copora, but there is no indication that the functions (like parsing using CFGs) are made for Dutch also.
Thanks</p>
",Multilingual Language Processing & Language Identification,dutch grammar python nltk working dutch corpus want know nltk ha dutch grammar embedded parse sentence general doe nltk work english know ha alpino dutch copora indication function like parsing using cfgs made dutch also thanks
sort of Natural language processing,"<p>I have a word lets say ""Racing"".</p>

<p>I want to know all its formats nouns, adjectives verbs etc. For eg. Race racer races etc.</p>

<p>Is there a python library that can help me out with this?</p>
",Multilingual Language Processing & Language Identification,sort natural language processing word let say racing want know format noun adjective verb etc eg race racer race etc python library help
How to use the pretrained MaltParser parsing models for english,"<p>I am trying to use the pretrained parsing model for English of the MaltParser by following the steps in the following page, but repeatedly getting a null pointer exception.
<a href=""http://maltparser.org/mco/english_parser/engmalt.html"" rel=""nofollow noreferrer"">http://maltparser.org/mco/english_parser/engmalt.html</a></p>

<p>I am trying this on a MaltParser version 1.4 and Java version 6 on a Windows machine. I think the model was trained on a Linux machine as some directory info in the model suggests so. I am not sure if that is causing a issue. I searched the net for help but did not find anything specific to this.</p>

<p>Kindly help me to solve this issue. Thanking you in advance.</p>
",Multilingual Language Processing & Language Identification,use pretrained maltparser parsing model english trying use pretrained parsing model english maltparser following step following page repeatedly getting null pointer exception trying maltparser version java version window machine think model wa trained linux machine directory info model suggests sure causing issue searched net help find anything specific kindly help solve issue thanking advance
"Natural language parsing for Arduino, where to start","<p>For a school project I am looking into natural language programming and thinking how the concept may be applied to Arduino.</p>

<p>Think along the lines of a piece of software that would translate sentences like <code>If the analog sensor on pin 9 reads more than 2 Volts, set the duty cycle of the the servo on pin 10 to 70%.</code> or <code>If the digital sensor on pin 4 reads high, light the onboard LED for 5 seconds.</code> into Arduino code. I'm suspecting that doing this for basic Arduino use cases should be straightforward compared to more general applications.</p>

<p>Does such a thing exist for Arduino? Does it exist for any other popular high-level language, like Python or MATLAB? Could anyone recommend resources for an absolute beginner on the topic of natural language processing (more specifically, a non-CS, non-CS-background graduate student who knows his way around Python, C#, MATLAB and, obviously, Arduino)?</p>
",Multilingual Language Processing & Language Identification,natural language parsing arduino start school project looking natural language programming thinking concept may applied arduino think along line piece software would translate sentence like arduino code suspecting basic arduino use case straightforward compared general application doe thing exist arduino doe exist popular high level language like python matlab could anyone recommend resource absolute beginner topic natural language processing specifically non c non c background graduate student know way around python c matlab obviously arduino
Natural language query understanding,"<p>I am pretty newbie in the NLP area and apologize that my question may be silly or incorrect. I look forward for any assistance that can give me the motion vector to the right direction.</p>

<p>Nowadays I am working on my thesis and there is an important part of it ‚Äî natural language query parser. Previously I had some experience with search engine algorithms. But now I want my system to ""understand"" some types of queries, and be able to convert it, roughly speaking, into database query language to perform structured search. For example for the query ""my friends who live in Russia"" the system simply should look in the table ""people"" and select who have ""country = Russia"".</p>

<p>I am clearly understand that the structured search is not as simple NLP-problem as, for example, spam filtering, but there are plenty of such systems born nowadays: Siri, Google Now, Facebook Graph Search. They are able to ""understand"" the query and not just give the ranked result list (like classic search engines do) but proper presentation for the ruested type of information. I was interested in how they work inside, but could not find enough information.</p>

<p>I would appreciate for any information, any references and books that could help me in my researching of these systems and progress in my thesis. Preferably those can be applied in practice, not Ministry of Defence closed developments :)</p>

<p>English is not my native language, sorry for mistakes, I hope you understand my issues.</p>
",Multilingual Language Processing & Language Identification,natural language query understanding pretty newbie nlp area apologize question may silly incorrect look forward assistance give motion vector right direction nowadays working thesis important part natural language query parser previously experience search engine algorithm want system understand type query able convert roughly speaking database query language perform structured search example query friend live russia system simply look table people select country russia clearly understand structured search simple nlp problem example spam filtering plenty system born nowadays siri google facebook graph search able understand query give ranked result list like classic search engine proper presentation ruested type information wa interested work inside could find enough information would appreciate information reference book could help researching system progress thesis preferably applied practice defence closed development english native language sorry mistake hope understand issue
Natural language query understanding,"<p>I am pretty newbie in the NLP area and apologize that my question may be silly or incorrect. I look forward for any assistance that can give me the motion vector to the right direction.</p>

<p>Nowadays I am working on my thesis and there is an important part of it ‚Äî natural language query parser. Previously I had some experience with search engine algorithms. But now I want my system to ""understand"" some types of queries, and be able to convert it, roughly speaking, into database query language to perform structured search. For example for the query ""my friends who live in Russia"" the system simply should look in the table ""people"" and select who have ""country = Russia"".</p>

<p>I am clearly understand that the structured search is not as simple NLP-problem as, for example, spam filtering, but there are plenty of such systems born nowadays: Siri, Google Now, Facebook Graph Search. They are able to ""understand"" the query and not just give the ranked result list (like classic search engines do) but proper presentation for the ruested type of information. I was interested in how they work inside, but could not find enough information.</p>

<p>I would appreciate for any information, any references and books that could help me in my researching of these systems and progress in my thesis. Preferably those can be applied in practice, not Ministry of Defence closed developments :)</p>

<p>English is not my native language, sorry for mistakes, I hope you understand my issues.</p>
",Multilingual Language Processing & Language Identification,natural language query understanding pretty newbie nlp area apologize question may silly incorrect look forward assistance give motion vector right direction nowadays working thesis important part natural language query parser previously experience search engine algorithm want system understand type query able convert roughly speaking database query language perform structured search example query friend live russia system simply look table people select country russia clearly understand structured search simple nlp problem example spam filtering plenty system born nowadays siri google facebook graph search able understand query give ranked result list like classic search engine proper presentation ruested type information wa interested work inside could find enough information would appreciate information reference book could help researching system progress thesis preferably applied practice defence closed development english native language sorry mistake hope understand issue
Link to list which contains frequnecy ranks of all English language words,"<p>Does any body knows link to the list which contains frequency rank of all english language words.
Some sixth months back, I found one list on 'wiki' which contains this list, but unfortunately I did not bookmark it and now I am unable to find the same link or any better link.
If anybody has this link or something better, please post it. </p>
",Multilingual Language Processing & Language Identification,link list contains frequnecy rank english language word doe body know link list contains frequency rank english language word sixth month back found one list wiki contains list unfortunately bookmark unable find link better link anybody ha link something better please post
Is there an easy way generate a probable list of words from an unspaced sentence in python?,"<p>I have some text: </p>

<pre><code> s=""Imageclassificationmethodscan beroughlydividedinto two broad families of approaches:""
</code></pre>

<p>I'd like to parse this into its individual words. I quickly looked into the enchant and nltk, but didn't see anything that looked immediately useful. If I had time to invest in this, I'd look into writing a dynamic program with enchant's ability to check if a word was english or not. I would have thought there'd be something to do this online, am I wrong? </p>
",Multilingual Language Processing & Language Identification,easy way generate probable list word unspaced sentence python text like parse individual word quickly looked enchant nltk see anything looked immediately useful time invest look writing dynamic program enchant ability check word wa english would thought something online wrong
Binary Feature Extraction,"<p>I am a beginner in feature extraction for natural language processing purposes.
I want to know how I can use a hashmap to extract features for a text. If each feature is a ""key"" in hashmap and its value is the ""value"" (all the features are binary, 0 or 1), does it mean that I need to have n hashmap (n is the number of words in the text)? Because for each word I need to extract the features.</p>

<p>Am I right?</p>

<p>Thanks in advance,
Alice</p>
",Multilingual Language Processing & Language Identification,binary feature extraction beginner feature extraction natural language processing purpose want know use hashmap extract feature text feature key hashmap value value feature binary doe mean need n hashmap n number word text word need extract feature right thanks advance alice
Does this neural network model exist?,"<p>I'm looking for a neural network model with specific characteristics. This model may not exist...</p>

<p>I need a network which doesn't use ""layers"" as traditional artificial neural networks do. Instead, I want [what I believe to be] a more biological model.</p>

<p>This model will house a large cluster of interconnected neurons, like the image below. A few neurons (at bottom of diagram) will receive input signals, and a cascade effect will cause successive, connected neurons to possibly fire depending on signal strength and connection weight. This is nothing new, but, there are no explicit layers...just more and more distant, indirect connections.</p>

<p>As you can see, I also have the network divided into sections (circles). Each circle represents a semantic domain (a linguistics concept) which is the core information surrounding a concept; essentially a semantic domain is a concept.</p>

<p>Connections between nodes within a section have higher weights than connections between nodes of different sections. So the nodes for ""car"" are more connected to one another than nodes connecting ""English"" to ""car"". Thus, when a neuron in a single section fires (is activated), it is likely that the entire (or most of) the section will also be activated.</p>

<p><strong>All in all, I need output patterns to be used as input for further output, and so on.</strong> A cascade effect is what I am after.</p>

<p>I hope this makes sense. Please ask for clarification where needed.</p>

<p>Are there any suitable models in existence that model what I've described, already?</p>

<p><img src=""https://i.sstatic.net/q8qAz.jpg"" alt=""enter image description here""></p>
",Multilingual Language Processing & Language Identification,doe neural network model exist looking neural network model specific characteristic model may exist need network use layer traditional artificial neural network instead want believe biological model model house large cluster interconnected neuron like image neuron bottom diagram receive input signal cascade effect cause successive connected neuron possibly fire depending signal strength connection weight nothing new explicit layer distant indirect connection see also network divided section circle circle represents semantic domain linguistics concept core information surrounding concept essentially semantic domain concept connection node within section higher weight connection node different section node car connected one another node connecting english car thus neuron single section fire activated likely entire section also activated need output pattern used input output cascade effect hope make sense please ask clarification needed suitable model existence model described already
Python NLTK: Count list of word and make probability with valid English words,"<p>I have a dirty document which includes invalid English words, numbers, etc.
I just want to take all valid English words and then calculate the ratio of my list of words to the total number of valid English words.</p>

<p>For example, if my document has the sentence:</p>

<pre><code>sentence= ['eishgkej he might be a good person. I might consider this.']
</code></pre>

<p>I want to count only <code>""he might be a good person. I might consider this""</code> and count <code>""might""</code>.</p>

<p>So, I got the answer 2/10.</p>

<p>I am thinking about using the below code. However, I need to change not the line <code>features[word] = 1</code> but the count of features...</p>

<pre><code> all_words = nltk.FreqDist(w.lower() for w in reader.words() if w.lower() not in english_sw)

 def document_features(document):
     document_words = set(document)
     features = {}
     for word in word_features:
         if word in document_words:
             features[word] = 1
         else:
             features[word]=0
     return features
</code></pre>
",Multilingual Language Processing & Language Identification,python nltk count list word make probability valid english word dirty document includes invalid english word number etc want take valid english word calculate ratio list word total number valid english word example document ha sentence want count count got answer thinking using code however need change line count feature
Standard C++ library for large scale data processing,"<p>Could you please let me know some of the standard library of C++ useful for processing large scale data for example Natural Language Processing with huge data set,data set of protein protein interactions etc.</p>

<p>Best,
Thetna</p>
",Multilingual Language Processing & Language Identification,standard c library large scale data processing could please let know standard library c useful processing large scale data example natural language processing huge data set data set protein protein interaction etc best thetna
Getting the root of an Arabic word,"<p>I have a Python code that take an Arabic word and get the root and also remove diacritics,
but I have a problem with the output. For example: when the input is ""ÿßŸÑÿπÿ±ÿ®ŸäŸá"" the output is:""ÿπÿ±ÿ®""
but when the input is ""ŸÉÿßÿ™ÿ®"" the output is:""ÿ®"", and when the input is ""ŸäÿÆÿßŸÅ"" the output is "" ÿÆŸÅ"".</p>

<p>This is my code:</p>

<pre><code># -*- coding=utf-8 -*-

import re
from arabic_const import *
import Tashaphyne
from Tashaphyne import *
import enum
from enum import Enum
search_type=Enum('unvoc_word','voc_word','root_word')

HARAKAT_pat = re.compile(ur""["" + u"""".join([FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SUKUN, SHADDA]) + u""]"")
HAMZAT_pat = re.compile(ur""["" + u"""".join([WAW_HAMZA, YEH_HAMZA]) + u""]"");
ALEFAT_pat = re.compile(ur""["" + u"""".join([ALEF_MADDA, ALEF_HAMZA_ABOVE, ALEF_HAMZA_BELOW, HAMZA_ABOVE, HAMZA_BELOW]) + u""]"");
LAMALEFAT_pat = re.compile(ur""["" + u"""".join([LAM_ALEF, LAM_ALEF_HAMZA_ABOVE, LAM_ALEF_HAMZA_BELOW, LAM_ALEF_MADDA_ABOVE]) + u""]"");
#--------------------------------------
def strip_tashkeel(w):
        ""strip vowel from a word and return a result word""
        return HARAKAT_pat.sub('', w)

#strip tatweel from a word and return a result word
#--------------------------------------
def strip_tatweel(w):
        ""strip tatweel from a word and return a result word""
        return re.sub(ur'[%s]' % TATWEEL,       '', w)


#--------------------------------------
def normalize_hamza(w):
        ""strip vowel from a word and return a result word""
        w = ALEFAT_pat.sub(ALEF, w)
        return HAMZAT_pat.sub(HAMZA, w)

#--------------------------------------
def normalize_lamalef(w):
        ""strip vowel from a word and return a result word""
        return LAMALEFAT_pat.sub(u'%s%s' % (LAM, ALEF), w)

#--------------------------------------
def normalize_spellerrors(w):
        ""strip vowel from a word and return a result word""
        w = re.sub(ur'[%s]' % TEH_MARBUTA,      HEH, w)
        return re.sub(ur'[%s]' % ALEF_MAKSURA,  YEH, w)


def normalize_text(word,searchtype):
        word = strip_tashkeel(word)
        word = strip_tatweel(word)
        word = normalize_lamalef(word)
        word = normalize_hamza(word)
        word = normalize_spellerrors(word)
        if searchtype==search_type.root_word.index:
           ArListem=ArabicLightStemmer();
           stem=ArListem.lightStm(word);
           word=ArListem.get_root();
        print word
        return word
#---------------------------------------------
</code></pre>

<p>and this is the test code:</p>

<pre><code>**from task import normalize_text
normalize_text(u'ŸÉÿßÿ™ÿ®',2)
</code></pre>

<p>and the output is:
ÿ®</p>
",Multilingual Language Processing & Language Identification,getting root arabic word python code take arabic word get root also remove diacritic problem output example input output input output input output code test code output
Word prediction using WordNet,"<p>Lets say I have a text in English and there is a word missing somewhere in it.
I have a list of candidate words from a dictionary with no other information. These candidate words are selected by some other, rather inaccurate, algorithm. I would like to use WordNet and the context around the missing word to assign probabilities to candidate words. </p>

<p>There is an obvious ad-hoc way that came to my mind on how to solve this. One way would be to extract ""interesting"" words surrounding the missing word, calculate semantic similarity with every candidate word according to some metric and assign probabilities to candidate words based on the average score.</p>

<p>However I was unable to find any useful research papers on this problem.</p>

<p>So, what i'm asking is if you're aware of any research (papers) about this problem, how do you find my proposal and do you have a better idea?</p>
",Multilingual Language Processing & Language Identification,word prediction using wordnet let say text english word missing somewhere list candidate word dictionary information candidate word selected rather inaccurate algorithm would like use wordnet context around missing word assign probability candidate word obvious ad hoc way came mind solve one way would extract interesting word surrounding missing word calculate semantic similarity every candidate word according metric assign probability candidate word based average score however wa unable find useful research paper problem asking aware research paper problem find proposal better idea
Fast automated spellchecking,"<p>I have a really big (~50MB) file of Spanish sentences. I want to check which of these don't contain foreign words. To achieve that, I am planning to filter out sentences that contain words that don't exist in the spellchecker dictionary. Does such a tool exist? Is it worth to play around with search trees and hash tables to create an efficient spellchecker myself?</p>
",Multilingual Language Processing & Language Identification,fast automated spellchecking really big mb file spanish sentence want check contain foreign word achieve planning filter sentence contain word exist spellchecker dictionary doe tool exist worth play around search tree hash table create efficient spellchecker
What is a relatively simple way to determine the probability that a sentence is in English?,"<p>I have a number of strings (collections of characters) that represent sentences in different languages, say:</p>

<blockquote>
  <blockquote>
    <p>Hello, my name is George.</p>
    
    <p>Das brot ist gut.</p>
    
    <p>... etc.</p>
  </blockquote>
</blockquote>

<p>I want to assign each of them scores (from 0 .. 1) indicating the likelihood that they are English sentences. Is there an accepted algorithm (or Python library) from which to do this?</p>

<p>Note: I don't care if the grammar of the English sentence is perfect.</p>
",Multilingual Language Processing & Language Identification,relatively simple way determine probability sentence english number string collection character represent sentence different language say hello name george da brot ist gut etc want assign score indicating likelihood english sentence accepted algorithm python library note care grammar english sentence perfect
How to detect the language of NSString,"<p>Is there away to detect the language text in NSString. I need to know is which language the text is so that I display it differently.</p>
",Multilingual Language Processing & Language Identification,detect language nsstring away detect language text nsstring need know language text display differently
Natural Language Processing - Similar to ngram,"<p>I'm currently working on a NLP project that is trying to differentiate between synonyms (received from Python's NLTK with WordNet) in a context.  I've looked into a good deal of NLP concepts trying to find exactly what I want, and the closest thing I've found is n-grams, but its not quite a perfect fit.</p>

<p>Suppose I am trying to find the proper definition of the verb ""box"".  ""Box"" could mean ""fight"" or ""package""; however, somewhere else in the text, the word ""ring"" or ""fighter"" appears.  As I understand it, an n-gram would be ""box fighter"" or ""box ring"", which is rather ridiculous as a phrase, and not likely to appear.  But on a concept map, the ""box"" action might be linked with a ""ring"", since they are conceptually related.</p>

<p>Is n-gram what I want? Is there another name for this? Any help on where to look for retrieving such relational data?</p>

<p>All help is appreciated.</p>
",Multilingual Language Processing & Language Identification,natural language processing similar ngram currently working nlp project trying differentiate synonym received python nltk wordnet context looked good deal nlp concept trying find exactly want closest thing found n gram quite perfect fit suppose trying find proper definition verb box box could mean fight package however somewhere else text word ring fighter appears understand n gram would box fighter box ring rather ridiculous phrase likely appear concept map box action might linked ring since conceptually related n gram want another name help look retrieving relational data help appreciated
Is there a C# utility for matching patterns in (syntactic parse) trees?,"<p>I'm working on a Natural Language Processing (NLP) project in which I use a syntactic parser to create a syntactic parse tree out of a given sentence.</p>

<p><strong><em>Example Input:</em></strong> I ran into Joe and Jill and then we went shopping<br>
<strong><em>Example Output:</em></strong> [TOP [S [S [NP [PRP I]] [VP [VBD ran] [PP [IN into] [NP [NNP Joe] [CC and] [NNP Jill]]]]] [CC and] [S [ADVP [RB then]] [NP [PRP we]] [VP [VBD went] [NP [NN shopping]]]]]]
<img src=""https://i.sstatic.net/EedCb.png"" alt=""enter image description here""></p>

<p>I'm looking for a C# utility that will let me do complex queries like:</p>

<ul>
<li>Get the first VBD related to 'Joe'</li>
<li>Get the NP closest to 'Shopping'</li>
</ul>

<p>Here's a <a href=""http://nlp.stanford.edu/software/tregex.shtml"" rel=""noreferrer"">Java utility</a> that does this, I'm looking for a C# equivalent.<br>
Any help would be much appreciated.</p>
",Multilingual Language Processing & Language Identification,c utility matching pattern syntactic parse tree working natural language processing nlp project use syntactic parser create syntactic parse tree given sentence example input ran joe jill went shopping example output top np prp vp vbd ran pp np nnp joe cc nnp jill cc advp rb np prp vp vbd went np nn shopping looking c utility let complex query like get first vbd related joe get np closest shopping java utility doe looking c equivalent help would much appreciated
Troubles with NLTK bigram finder,"<p>I have a text file labeled ""all.txt"" It contains a regular english paragraph</p>

<p>For some reason when I run this code:</p>

<pre><code>    import nltk
    from nltk.collocations import *
    bigram_measures = nltk.collocations.BigramAssocMeasures()
    trigram_measures = nltk.collocations.TrigramAssocMeasures()

    # change this to read in your data                                                                                                                                                   
    finder = BigramCollocationFinder.from_words(('all.txt'))

    # only bigrams that appear 3+ times                                                                                                                                                  
    #finder.apply_freq_filter(3)                                                                                                                                                         

    # return the 10 n-grams with the highest PMI                                                                                                                                         
    print finder.nbest(bigram_measures.pmi, 10)
</code></pre>

<p>I get the following result:</p>

<pre><code>       [('.', 't'), ('a', 'l'), ('l', '.'), ('t', 'x'), ('x', 't')]
</code></pre>

<p>What am I doing wrong, since I am only getting letters? I am looking for words not letters!</p>

<p>Here is an example of what is in ""all.txt"", so you get an idea of what is being processed:
""and it 's not just democrats who oppose this plan .  americans across the country have expressed their opposition to this plan .my democratic colleagues and i have a better plan that will strengthen the ethics rules to improve congressional accountability and to make sure that legislation is properly considered .  the republican plan fails to close a loophole that allows legislation to be considered before members have read it .""</p>
",Multilingual Language Processing & Language Identification,trouble nltk bigram finder text file labeled txt contains regular english paragraph reason run code get following result wrong since getting letter looking word letter example txt get idea processed democrat oppose plan american across country expressed opposition plan democratic colleague better plan strengthen ethic rule improve congressional accountability make sure legislation properly considered republican plan fails close loophole allows legislation considered member read
Extract wordlist from wordweb lst files,"<p><strong>Hi.. I am working on a natural language processing project (It will be on JAVA)... A paragraph of text goes in and it (will) extracts information from the paragraph and add it to its database... Like give it text of wikipedia and it will update its database...
First of all I have to identify verbs, nouns, adjectives, etc(all the facts known in advance) from the paragraph..</strong></p>

<p>It can be done using a dictionary database from which I can query if a word is Verb, Noun, Adjective or something else...</p>

<p><strong>But the problem is I can't find a dictionary database to download...
Or is there any other way I can do this... Please tell me if I am not clear...!!</strong></p>
",Multilingual Language Processing & Language Identification,extract wordlist wordweb lst file hi working natural language processing project java paragraph text go extract information paragraph add database like give text wikipedia update database first identify verb noun adjective etc fact known advance paragraph done using dictionary database query word verb noun adjective something else problem find dictionary database download way please tell clear
Extract product name from english text,"<p>I want extract the names of products being sold from English text. </p>

<p>For example:</p>

<blockquote>
  <p>""I'm selling my xbox brand new""</p>
  
  <p>""Selling rarely used 27 inch TV""</p>
</blockquote>

<p>Should give me <code>""xbox""</code> and <code>""27 inch TV""</code></p>

<p>The only thing I can think of at the moment is to hardcode in a giant list of important nouns and important adjectives: <code>['tv', 'fridge', 'xbox', 'laptop', etc]</code></p>

<p>Is there a better approach?</p>
",Multilingual Language Processing & Language Identification,extract product name english text want extract name product sold english text example selling xbox brand new selling rarely used inch tv give thing think moment hardcode giant list important noun important adjective better approach
Scala Large Text file,"<p>I'm a newbie with Scala programming.</p>

<p>I have to deal with an <strong>NLP</strong> task.</p>

<p>I'm having trouble with <strong>processing a large text file</strong> in <strong>Scala</strong>.</p>

<p>I have read the entire text of a 100+ M.B file onto memory (into a string) and have to process it (I believe processing large text files is a common task in Natural Language Processing).</p>

<p><strong>The goal is to count the number of unique substrings/words in the given string</strong> (which is the entire file).  </p>

<p>I wanted to use ""<em>distinct</em>"" method in <em>List</em> object, but <strong>converting the string into a list</strong> using ""<em>.split</em>"" method raises out of memory error (""java.lang.OutOfMemoryError: Java heap space"" Error).</p>

<p>I was wondering if I could accomplish this task without using lists using String or Regular Expression methods in Scala?</p>
",Multilingual Language Processing & Language Identification,scala large text file newbie scala programming deal nlp task trouble processing large text file scala read entire text b file onto memory string process believe processing large text file common task natural language processing goal count number unique substring word given string entire file wanted use distinct method list object converting string list using split method raise memory error java lang outofmemoryerror java heap space error wa wondering could accomplish task without using list using string regular expression method scala
detect allusions (e.g. very fuzzy matches) in language of inaugural addresses,"<p>I'm trying to develop a Python script to examine every sentence in Barack Obama's second inaugural address and find similar sentences in past inaugurals. I've developed a very crude fuzzy match, and I'm hoping to improve it.</p>

<p>I start by reducing all inaugurals to lists of stopword-free sentences. I then build a frequency index.</p>

<p>Next, I compare each sentence in Obama's 2013 address to each sentence of every other address, and evaluate the similarity like so:</p>

<pre><code>#compare two lemmatized sentences. Assumes stop words already removed. frequencies is dict of frequencies across all inaugural    
def compare(sentA, sentB, frequencies):
    intersect = [x for x in sentA if x in sentB]
    N = [frequencies[x] for x in intersect]
    #calculate sum that weights uncommon words based on frequency inaugurals
    n = sum([10.0 / (x + 1) for x in N])
    #ratio of matches to total words in both sentences. (John Adams and William Harrison both favored loooooong sentences that tend to produce matches by sheer probability.)
    c = float(len(intersect)) / (len(sentA) + len(sentB))
    return (intersect, N, n, c)
</code></pre>

<p>Last, I filter out results based on arbitrary cutoffs for n and c. </p>

<p>It works better than one might think, identifying sentences that share uncommon words in a non-negligible proportion to total words. </p>

<p>For example, it picked up these matches:</p>

<hr>

<p><strong>Obama, 2013:</strong>
For history tells us that while these truths may be self-evident, they have never been self-executing; that while freedom is a gift from God, it must be secured by His people here on Earth.</p>

<p><strong>Kennedy, 1961:</strong>
With a good conscience our only sure reward, with history the final judge of our deeds, let us go forth to lead the land we love, asking His blessing and His help, but knowing that here on earth God's work must truly be our own. </p>

<hr>

<p><strong>Obama, 2013</strong>
Through blood drawn by lash and blood drawn by sword, we learned that no union founded on the principles of liberty and equality could survive half-slave and half-free.</p>

<p><strong>Lincoln, 1861</strong>
Yet, if God wills that it continue until all the wealth piled by the bondsman's two hundred and fifty years of unrequited toil shall be sunk, and until every drop of blood drawn with the lash shall be paid by another drawn with the sword, as was said three thousand years ago, so still it must be said ""the judgments of the Lord are true and righteous altogether.</p>

<hr>

<p><strong>Obama, 2013</strong>
This generation of Americans has been tested by crises that steeled our resolve and proved our resilience</p>

<p><strong>Kennedy, 1961</strong>
Since this country was founded, each generation of Americans has been summoned to give testimony to its national loyalty.</p>

<hr>

<p>But it's very crude.</p>

<p>I don't have the chops for a major machine-learning project, but I do want to apply more theory if possible. I understand bigram searching, but I'm not sure that will work here -- it's not so much exact bigrams we're interested in as general proximity of two words that are shared between quotes. Is there a fuzzy sentence comparison that looks at probability and distribution of words without being too rigid? The nature of allusion is that it's very approximate.</p>

<p>Current effort <a href=""https://c9.io/wilson428/inaugurals"" rel=""nofollow noreferrer"">available on Cloud9IDE</a></p>

<p><strong>UPDATE, 1/24/13</strong>
Per the accepted answer, here's a simple Python function for bigram windows:</p>

<pre><code>def bigrams(tokens, blur=1):
    grams = []
    for c in range(len(tokens) - 1):
        for i in range(c + 1, min(c + blur + 1, len(tokens))):
            grams.append((tokens[c], tokens[i]))
    return grams
</code></pre>
",Multilingual Language Processing & Language Identification,detect allusion e g fuzzy match language inaugural address trying develop python script examine every sentence barack obama second inaugural address find similar sentence past inaugural developed crude fuzzy match hoping improve start reducing inaugural list stopword free sentence build frequency index next compare sentence obama address sentence every address evaluate similarity like last filter result based arbitrary cutoff n c work better one might think identifying sentence share uncommon word non proportion total word example picked match obama history tell u truth may self evident never self executing freedom gift god must secured people earth kennedy good conscience sure reward history final judge deed let u go forth lead land love asking blessing help knowing earth god work must truly obama blood drawn lash blood drawn sword learned union founded principle liberty equality could survive half slave half free lincoln yet god continue wealth piled bondsman two hundred fifty year unrequited toil shall sunk every drop blood drawn lash shall paid another drawn sword wa said three thousand year ago still must said judgment lord true righteous altogether obama generation american ha tested crisis steeled resolve proved resilience kennedy since country wa founded generation american ha summoned give testimony national loyalty crude chop major machine learning project want apply theory possible understand bigram searching sure work much exact bigram interested general proximity two word shared quote fuzzy sentence comparison look probability distribution word without rigid nature allusion approximate current effort available cloud ide update per accepted answer simple python function bigram window
Unsupervised Feature extraction of dishes by building tree structure of ingerdients with Natural Language Processing,"<p>I am building a recommendation system for dishes. Consider a user eating french fries and rates it  a 5. Then I want to give a good rating to all the ingredients that the dish is made of. In the case of french fires the linked words should be ""fried"" ""potato"" ""junk food"" ""salty"" and so on. From the word Tsatsiki I want to extract ""Cucumbers"", ""Yoghurt"" ""Garlic"". From Yoghurt I want to extract Milk product, From Cucumbers vegetable and so on.</p>

<p>What is this problem called in Natural Language Processing and is there a way to address it?</p>

<p>I have no data at all, and I am thinking of building web crawler that analyzes the web for the dish. I would like it to be as little Ad-Hoc as possible and not necessarily in English. Is there a way, maybe in within deep learning to do the thing? I would not only a dish to be linked to the ingredients but also a category: junk food, vegetarian, Italian food and so on.</p>
",Multilingual Language Processing & Language Identification,unsupervised feature extraction dish building tree structure ingerdients natural language processing building recommendation system dish consider user eating french fry rate want give good rating ingredient dish made case french fire linked word fried potato junk food salty word tsatsiki want extract cucumber yoghurt garlic yoghurt want extract milk product cucumber vegetable problem called natural language processing way address data thinking building web crawler analyzes web dish would like little ad hoc possible necessarily english way maybe within deep learning thing would dish linked ingredient also category junk food vegetarian italian food
Are there any ML/NLP works/papers on parsing/solving math word problems?,"<p>As the title says, any pointers are much appreciated.</p>

<p>I am exploring, where do we stand in terms of ML/NLP efforts, in context of solving (to begin with - parsing) Math Word Problems.</p>

<p>We have decent enough softwares in likes of Mathematica which can solve well formulated math equations.</p>

<p>But when it comes to solving math problems expressed in natural languages, I could not find anything substantial.
When I think about how to approach this, I see it as a sort of Machine Translation problem (translating from English to Math-equations), but there is hardly any 'labeled' data for that. Other approach can be semi (or un) sypervised Relation Extraction.</p>

<p>Since these are just random thoughts, I want to start with some existing work/papers in this direction. My otherwise decent googling skills, didn't help much.</p>
",Multilingual Language Processing & Language Identification,ml nlp work paper parsing solving math word problem title say pointer much appreciated exploring stand term ml nlp effort context solving begin parsing math word problem decent enough software like mathematica solve well formulated math equation come solving math problem expressed natural language could find anything substantial think approach see sort machine translation problem translating english math equation hardly labeled data approach semi un sypervised relation extraction since random thought want start existing work paper direction otherwise decent googling skill help much
"How to parse languages other than English with Stanford ParserÔºü in java, not command lines","<p>I have been trying to use Stanford Parser in my Java program to parse some sentences in Chinese. Since I am quite new at both Java and Stanford Parser, I used the 'ParseDemo.java' to practice. The code works fine with sentences in English and outputs the right result. However, when I changed the model to 'chinesePCFG.ser.gz' and tried to parse some segmented Chinese sentences, things went wrong.</p>

<p>Here's my code in Java</p>

<pre class=""lang-java prettyprint-override""><code>class ParserDemo {

  public static void main(String[] args) {
    LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz"");
    if (args.length &gt; 0) {
      demoDP(lp, args[0]);
    } else {
      demoAPI(lp);
    }
  }

  public static void demoDP(LexicalizedParser lp, String filename) {
    // This option shows loading and sentence-segment and tokenizing
    // a file using DocumentPreprocessor
    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
    // You could also create a tokenier here (as below) and pass it
    // to DocumentPreprocessor
    for (List&lt;HasWord&gt; sentence : new DocumentPreprocessor(filename)) {
      Tree parse = lp.apply(sentence);
      parse.pennPrint();
      System.out.println();

      GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
      Collection tdl = gs.typedDependenciesCCprocessed(true);
      System.out.println(tdl);
      System.out.println();
    }
  }

  public static void demoAPI(LexicalizedParser lp) {
    // This option shows parsing a list of correctly tokenized words
    String sent[] = { ""Êàë"", ""ÊòØ"", ""‰∏ÄÂêç"", ""Â≠¶Áîü"" };
    List&lt;CoreLabel&gt; rawWords = Sentence.toCoreLabelList(sent);
    Tree parse = lp.apply(rawWords);
    parse.pennPrint();
    System.out.println();

    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
    GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
    List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
    System.out.println(tdl);
    System.out.println();

    TreePrint tp = new TreePrint(""penn,typedDependenciesCollapsed"");
    tp.printTree(parse);
  }

  private ParserDemo() {} // static methods only
}
</code></pre>

<p>It's basically the same as ParserDemo.java, but when I run it I get the following result:</p>

<blockquote>
  <p>Loading parser from serialized file
  edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz ... done [2.2
  sec]. (ROOT   (IP
      (NP (PN Êàë))
      (VP (VC ÊòØ)
        (NP
          (QP (CD ‰∏ÄÂêç))
          (NP (NN Â≠¶Áîü))))))</p>
  
  <p>Exception in thread ""main"" java.lang.RuntimeException: Failed to
  invoke public
  edu.stanford.nlp.trees.EnglishGrammaticalStructure(edu.stanford.nlp.trees.Tree)
    at
  edu.stanford.nlp.trees.GrammaticalStructureFactory.newGrammaticalStructure(GrammaticalStructureFactory.java:104)
    at parserdemo.ParserDemo.demoAPI(ParserDemo.java:65)    at
  parserdemo.ParserDemo.main(ParserDemo.java:23) </p>
</blockquote>

<p>the code on line 65 is:</p>

<pre><code> GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
</code></pre>

<p>My guess is that chinesePCFG.ser.gz misses something relevant to 'edu.stanford.nlp.trees.EnglishGrammaticalStructure'. Since the parser parses Chinese correctly via commandlines, there must be something wrong with my own code. I have been searching, only to find few similar cases some of which mentioned about using the right model, but I don't really know how to modify the code to the 'right model'. Hope that someone could help me with it. I am a newbie on Java and Stanford Parser, so please be specific. Thank you! </p>
",Multilingual Language Processing & Language Identification,parse language english stanford parser java command line trying use stanford parser java program parse sentence chinese since quite new java stanford parser used parsedemo java practice code work fine sentence english output right result however changed model chinesepcfg ser gz tried parse segmented chinese sentence thing went wrong code java basically parserdemo java run get following result loading parser serialized file edu stanford nlp model lexparser chinesepcfg ser gz done sec root ip np pn vp vc np qp cd np nn exception thread main java lang runtimeexception failed invoke public edu stanford nlp tree englishgrammaticalstructure edu stanford nlp tree tree edu stanford nlp tree grammaticalstructurefactory newgrammaticalstructure grammaticalstructurefactory java parserdemo parserdemo demoapi parserdemo java parserdemo parserdemo main parserdemo java code line guess chinesepcfg ser gz miss something relevant edu stanford nlp tree englishgrammaticalstructure since parser par chinese correctly via commandlines must something wrong code searching find similar case mentioned using right model really know modify code right model hope someone could help newbie java stanford parser please specific thank
"How to parse languages other than English with Stanford ParserÔºü in java, not command lines","<p>I have been trying to use Stanford Parser in my Java program to parse some sentences in Chinese. Since I am quite new at both Java and Stanford Parser, I used the 'ParseDemo.java' to practice. The code works fine with sentences in English and outputs the right result. However, when I changed the model to 'chinesePCFG.ser.gz' and tried to parse some segmented Chinese sentences, things went wrong.</p>

<p>Here's my code in Java</p>

<pre class=""lang-java prettyprint-override""><code>class ParserDemo {

  public static void main(String[] args) {
    LexicalizedParser lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz"");
    if (args.length &gt; 0) {
      demoDP(lp, args[0]);
    } else {
      demoAPI(lp);
    }
  }

  public static void demoDP(LexicalizedParser lp, String filename) {
    // This option shows loading and sentence-segment and tokenizing
    // a file using DocumentPreprocessor
    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
    // You could also create a tokenier here (as below) and pass it
    // to DocumentPreprocessor
    for (List&lt;HasWord&gt; sentence : new DocumentPreprocessor(filename)) {
      Tree parse = lp.apply(sentence);
      parse.pennPrint();
      System.out.println();

      GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
      Collection tdl = gs.typedDependenciesCCprocessed(true);
      System.out.println(tdl);
      System.out.println();
    }
  }

  public static void demoAPI(LexicalizedParser lp) {
    // This option shows parsing a list of correctly tokenized words
    String sent[] = { ""Êàë"", ""ÊòØ"", ""‰∏ÄÂêç"", ""Â≠¶Áîü"" };
    List&lt;CoreLabel&gt; rawWords = Sentence.toCoreLabelList(sent);
    Tree parse = lp.apply(rawWords);
    parse.pennPrint();
    System.out.println();

    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
    GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
    List&lt;TypedDependency&gt; tdl = gs.typedDependenciesCCprocessed();
    System.out.println(tdl);
    System.out.println();

    TreePrint tp = new TreePrint(""penn,typedDependenciesCollapsed"");
    tp.printTree(parse);
  }

  private ParserDemo() {} // static methods only
}
</code></pre>

<p>It's basically the same as ParserDemo.java, but when I run it I get the following result:</p>

<blockquote>
  <p>Loading parser from serialized file
  edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz ... done [2.2
  sec]. (ROOT   (IP
      (NP (PN Êàë))
      (VP (VC ÊòØ)
        (NP
          (QP (CD ‰∏ÄÂêç))
          (NP (NN Â≠¶Áîü))))))</p>
  
  <p>Exception in thread ""main"" java.lang.RuntimeException: Failed to
  invoke public
  edu.stanford.nlp.trees.EnglishGrammaticalStructure(edu.stanford.nlp.trees.Tree)
    at
  edu.stanford.nlp.trees.GrammaticalStructureFactory.newGrammaticalStructure(GrammaticalStructureFactory.java:104)
    at parserdemo.ParserDemo.demoAPI(ParserDemo.java:65)    at
  parserdemo.ParserDemo.main(ParserDemo.java:23) </p>
</blockquote>

<p>the code on line 65 is:</p>

<pre><code> GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
</code></pre>

<p>My guess is that chinesePCFG.ser.gz misses something relevant to 'edu.stanford.nlp.trees.EnglishGrammaticalStructure'. Since the parser parses Chinese correctly via commandlines, there must be something wrong with my own code. I have been searching, only to find few similar cases some of which mentioned about using the right model, but I don't really know how to modify the code to the 'right model'. Hope that someone could help me with it. I am a newbie on Java and Stanford Parser, so please be specific. Thank you! </p>
",Multilingual Language Processing & Language Identification,parse language english stanford parser java command line trying use stanford parser java program parse sentence chinese since quite new java stanford parser used parsedemo java practice code work fine sentence english output right result however changed model chinesepcfg ser gz tried parse segmented chinese sentence thing went wrong code java basically parserdemo java run get following result loading parser serialized file edu stanford nlp model lexparser chinesepcfg ser gz done sec root ip np pn vp vc np qp cd np nn exception thread main java lang runtimeexception failed invoke public edu stanford nlp tree englishgrammaticalstructure edu stanford nlp tree tree edu stanford nlp tree grammaticalstructurefactory newgrammaticalstructure grammaticalstructurefactory java parserdemo parserdemo demoapi parserdemo java parserdemo parserdemo main parserdemo java code line guess chinesepcfg ser gz miss something relevant edu stanford nlp tree englishgrammaticalstructure since parser par chinese correctly via commandlines must something wrong code searching find similar case mentioned using right model really know modify code right model hope someone could help newbie java stanford parser please specific thank
Natural Language Processing in Windows 8,"<p>I'm a newbie to windows 8 programming, C# and NLP.</p>

<p>I'm looking for a library that allows me to use NLP in windows 8.</p>

<p>I found SharpNLP but it is very poorly documented with no tutorials. I've also come across the Antelope framework but this seems to have even worse documentation.</p>

<p>Is there any resource that'll help me (either tutorials or a better documented framework)?</p>
",Multilingual Language Processing & Language Identification,natural language processing window newbie window programming c nlp looking library allows use nlp window found sharpnlp poorly documented tutorial also come across antelope framework seems even worse documentation resource help either tutorial better documented framework
Natural Language Processing for Android,"<p>I was interested in what opensource NLP APIs people on stackoverflow have used and liked for working on Android? Have you found performance issues related to doing NLP on Android Devices?</p>
",Multilingual Language Processing & Language Identification,natural language processing android wa interested opensource nlp apis people stackoverflow used liked working android found performance issue related nlp android device
How to determine the (natural) language of a document?,"<p>I have a set of documents in two languages: English and German. There is no usable meta information about these documents, a program can look at the content only. Based on that, the program has to decide which of the two languages the document is written in.</p>

<p>Is there any ""standard"" algorithm for this problem that can be implemented in a few hours' time? Or alternatively, a free .NET library or toolkit that can do this? I know about <a href=""http://alias-i.com/lingpipe/"" rel=""noreferrer"">LingPipe</a>, but it is </p>

<ol>
<li>Java</li>
<li>Not free for ""semi-commercial"" usage</li>
</ol>

<p>This problem seems to be surprisingly hard. I checked out the <a href=""http://www.google.com/uds/samples/language/detect.html"" rel=""noreferrer"">Google AJAX Language API</a> (which I found by searching this site first), but it was ridiculously bad. For six web pages in German to which I pointed it only one guess was correct. The other guesses were Swedish, English, Danish and French...</p>

<p>A simple approach I came up with is to use a list of stop words. My app already uses such a list for German documents in order to analyze them with Lucene.Net. If my app scans the documents for occurrences of stop words from either language the one with more occurrences would win. A very naive approach, to be sure, but it <em>might</em> be good enough. Unfortunately I don't have the time to become an expert at natural-language processing, although it is an intriguing topic.</p>
",Multilingual Language Processing & Language Identification,determine natural language document set document two language english german usable meta information document program look content based program ha decide two language document written standard algorithm problem implemented hour time alternatively free net library toolkit know lingpipe java free semi commercial usage problem seems surprisingly hard checked google ajax language api found searching site first wa ridiculously bad six web page german pointed one guess wa correct guess swedish english danish french simple approach came use list stop word app already us list german document order analyze lucene net app scan document occurrence stop word either language one occurrence would win naive approach sure might good enough unfortunately time become expert natural language processing although intriguing topic
Are there published generative grammars for natural languages?,"<p>I have some ideas to do with natural language processing. I will need some grammars of the</p>

<pre><code>S -&gt; NP VP
</code></pre>

<p>variety in order to play with them.</p>

<p>If I try to write these rules myself it will be a tedious and error-prone business. <strong>Has anyone ever typed up and released comprehensive rule sets for English and other natural languages?</strong> Ideally written in BNF, Prolog or similar syntax.</p>

<p>My project only relates to context-free grammars, I'm not interested in statistical methods or machine learning -- I need to systematically <em>produce</em> Engligh-like and Foobarian-like sentences.</p>

<p>If you know where to find such materiel, I'd very much appreciate it.</p>
",Multilingual Language Processing & Language Identification,published generative grammar natural language idea natural language processing need grammar variety order play try write rule tedious error prone business ha anyone ever typed released comprehensive rule set english natural language ideally written bnf prolog similar syntax project relates context free grammar interested statistical method machine learning need produce engligh like foobarian like sentence know find materiel much appreciate
Singularization function for English words in bash,"<p>Any pointers to a bash function that performs singularization on English words? It does not need to be perfect.</p>
",Multilingual Language Processing & Language Identification,singularization function english word bash pointer bash function performs singularization english word doe need perfect
Converting Between British/American English,"<p>I use Vim to edit my Latex document. Depending on the conference/journal I am submitting to, the university I am studying at, etc., I might like to convert from British English to American English or vice versa. Does anybody know if there is a plug-in to do that?</p>
",Multilingual Language Processing & Language Identification,converting british american english use vim edit latex document depending conference journal submitting university studying etc might like convert british english american english vice versa doe anybody know plug
Smalltalk and tf-idf algorithm,"<p>Can anyone show a simple implementation or usage example of a tf-idf algorithm in Smalltalk for natural language processing?
I've found an implementation in a package called <a href=""http://www.squeaksource.com/NaturalSmalltalk.html"" rel=""nofollow"">NaturalSmalltalk</a>, but it seems too complicated for my needs. A simple implementation in Python is like <a href=""http://code.google.com/p/tfidf/"" rel=""nofollow"">this one</a>.</p>

<p>I've noticed there is another tf-idf in <a href=""http://scg.unibe.ch/staff/adriankuhn/hapax"" rel=""nofollow"">Hapax</a>, but it seems related to analysis of software systems vocabularies, and I didn't found examples of how to use it.</p>
",Multilingual Language Processing & Language Identification,smalltalk tf idf algorithm anyone show simple implementation usage example tf idf algorithm smalltalk natural language processing found implementation package called naturalsmalltalk seems complicated need simple implementation python like one noticed another tf idf hapax seems related analysis software system vocabulary found example use
Find different realization of a word in a sentence string - Python,"<p><em>(This question is with regards to string checking in general and not Natural Language Procesisng per se, but if you view it as an NLP problem, imagine it's not a langauge that current analyzers can analye, for simplicity sake, i'll use english strings as e.g.)</em></p>

<p>lets say there are only 6 possible form that a word can be realized in</p>

<ol>
<li>the initial letter being capitalized </li>
<li>its plural form with an ""s""</li>
<li>its plural form with an ""es""</li>
<li>capitalized + ""es""</li>
<li>capitalized + ""s""</li>
<li>the basic form without plural or capitalization</li>
</ol>

<p>let's say i want to find the index of the 1st instance any form of the word <code>coach</code> occurs in a sentence, is there a simpler way of doing these 2 methods:</p>

<p><strong>long if condition</strong></p>

<pre><code>sentence = ""this is a sentence with the Coaches""
target = ""coach""

print target.capitalize()

for j, i in enumerate(sentence.split("" "")):
  if i == target.capitalize() or i == target.capitalize()+""es"" or \
     i == target.capitalize()+""s"" or i == target+""es"" or i==target+""s"" or \
     i == target:
    print j
</code></pre>

<p><strong>iterating try-except</strong></p>

<pre><code>variations = [target, target+""es"", target+""s"", target.capitalize()+""es"",
target.capitalize()+""s"", target.capitalize()]

ind = 0
for i in variations:
  try:
    j == sentence.split("" "").index(i)
    print j
  except ValueError:
    continue
</code></pre>
",Multilingual Language Processing & Language Identification,find different realization word sentence string python question regard string checking general natural language procesisng per se view nlp problem imagine langauge current analyzer analye simplicity sake use english string e g let say possible form word realized initial letter capitalized plural form plural form e capitalized e capitalized basic form without plural capitalization let say want find index st instance form word occurs sentence simpler way method long condition iterating try except
Code Golf: Number to Words,"<p>The code golf series seem to be fairly popular.  I ran across some code that converts a number to its word representation.  Some examples would be (powers of 2 for programming fun):</p>

<ul>
<li>2 -> Two</li>
<li>1024 -> One Thousand Twenty Four</li>
<li>1048576 -> One Million Forty Eight Thousand Five Hundred Seventy Six</li>
</ul>

<p>The algorithm my co-worker came up was almost two hundred lines long.  Seems like there would be a more concise way to do it.</p>

<p>Current guidelines:</p>

<ul>
<li>Submissions in any <strong>programming</strong> language welcome (I apologize to
PhiLho for the initial lack of clarity on this one)</li>
<li>Max input of 2^64 (see following link for words, thanks mmeyers)</li>
<li><a href=""http://en.wikipedia.org/wiki/Long_and_short_scales"" rel=""nofollow noreferrer"">Short scale</a> with English output preferred, but any algorithm is welcome.  Just comment along with the programming language as to the method used.</li>
</ul>
",Multilingual Language Processing & Language Identification,code golf number word code golf series seem fairly popular ran across code convert number word representation example would power programming fun two one thousand twenty four one million forty eight thousand five hundred seventy six algorithm co worker came wa almost two hundred line long seems like would concise way current guideline submission programming language welcome apologize philho initial lack clarity one max input see following link word thanks mmeyers short scale english output preferred algorithm welcome comment along programming language method used
picking the most relevant words from a paragraph,"<p>Not sure how to phrase this question properly, but this is what I intend to achieve using the hypothetical scenario outlined below - </p>

<p>A user's email to me has just the SUBJECT and BODY, the subject being the topic of email, and the body being a description of the topic in just one paragraph of max 1000 words. Now I would like to analyse this paragraph (in the BODY) using some computer language (python, maybe), and then come up with a list of most important words from the paragraph with respect to the topic mentioned in the SUBJECT field.</p>

<p>For example, if the topic of email is say iPhone, and the body is something like ""the iPhone redefines user-interface design with super resolution and graphics. it is fully touch enabled and allows users to swipe the screen""</p>

<p>So the result I am looking for is a sort of list with the key terms from the paragraph as related to iPhone. Example - (user-interface, design, resolution, graphics, touch, swipe, screen). </p>

<p>So basically I am looking at picking the most relevant words from the paragraph. I am not sure on what I can use or how to use to achieve this result. Searching on google, I read a little about Natural Language Processing and python and classification etc. I just need a general approach on how to go about this - using what technology/language, which area I have to read on etc..</p>

<p>Thanks!</p>

<blockquote>
  <p>EDIT:::</p>
</blockquote>

<p>I have been reading up in the meantime. To be precise, I am looking at HOW TO do this, using WHAT TOOL:</p>

<p>Generate related tags from a body of text using NLP which are based on synonyms, morphological similarity, spelling errors and contextual analysis.</p>
",Multilingual Language Processing & Language Identification,picking relevant word paragraph sure phrase question properly intend achieve using hypothetical scenario outlined user email ha subject body subject topic email body description topic one paragraph max word would like analyse paragraph body using computer language python maybe come list important word paragraph respect topic mentioned subject field example topic email say iphone body something like iphone redefines user interface design super resolution graphic fully touch enabled allows user swipe screen result looking sort list key term paragraph related iphone example user interface design resolution graphic touch swipe screen basically looking picking relevant word paragraph sure use use achieve result searching google read little natural language processing python classification etc need general approach go using technology language area read etc thanks edit reading meantime precise looking using tool generate related tag body text using nlp based synonym morphological similarity spelling error contextual analysis
Any tools to programmatically convert Japanese sentence into its romaji (phonetical reading)?,"<p>Input:</p>

<blockquote>
  <p>Êó•Êú¨„ÅåÂ•Ω„Åç„Åß„Åô.</p>
</blockquote>

<p>Output:</p>

<blockquote>
  <p>Nippon ga sukidesu.</p>
</blockquote>

<p>Phonetical reading is unfortunately not available through Google Translate API.</p>
",Multilingual Language Processing & Language Identification,tool programmatically convert japanese sentence romaji phonetical reading input output nippon ga sukidesu phonetical reading unfortunately available google translate api
Why are these words considered stopwords?,"<p>I do not have a formal background in Natural Language Processing was wondering if someone from the NLP side can shed some light on this. I am playing around with the <a href=""http://www.nltk.org"" rel=""nofollow"">NLTK</a> library and I was specifically looking into the stopwords function provided by this package:</p>

<blockquote>
  <p>In [80]:
  nltk.corpus.stopwords.words('english')</p>
  
  <p>Out[80]: </p>
  
  <p>['i',  'me',  'my', 
  'myself',  'we',  'our',  'ours', 
  'ourselves',  'you',  'your', 
  'yours',  'yourself',  'yourselves', 
  'he',  'him',  'his',  'himself', 
  'she',  'her',  'hers',  'herself', 
  'it',  'its',  'itself',  'they', 
  'them',  'their',  'theirs', 
  'themselves',  'what',  'which', 
  'who',  'whom',  'this',  'that', 
  'these',  'those',  'am',  'is', 
  'are',  'was',  'were',  'be', 
  'been',  'being',  'have',  'has', 
  'had',  'having',  'do',  'does', 
  'did',  'doing',  'a',  'an',  'the', 
  'and',  'but',  'if',  'or', 
  'because',  'as',  'until',  'while', 
  'of',  'at',  'by',  'for',  'with', 
  'about',  'against',  'between', 
  'into',  'through',  'during', 
  'before',  'after',  'above', 
  'below',  'to',  'from',  'up', 
  'down',  'in',  'out',  'on',  'off', 
  'over',  'under',  'again', 
  'further',  'then',  'once',  'here', 
  'there',  'when',  'where',  'why', 
  'how',  'all',  'any',  'both', 
  'each',  'few',  'more',  'most', 
  'other',  'some',  'such',  'no', 
  'nor',  'not',  'only',  'own', 
  'same',  'so',  'than',  'too', 
  'very',  's',  't',  'can',  'will', 
  'just',  'don',  'should',  'now']</p>
</blockquote>

<p>What I don't understand is, why is the word ""not"" present? Isn't that necessary to determine the sentiment inside a sentence? For instance, a sentence like this:</p>

<blockquote>
  <p>I am not sure what the problem is.</p>
</blockquote>

<p>is totally different once the stopword <code>not</code> is removed changing the meaning of the sentence to its opposite (<code>I am sure what the problem is</code>). If that is the case, is there a set of rules that I am missing on when not to use these stopwords?</p>
",Multilingual Language Processing & Language Identification,word considered stopwords formal background natural language processing wa wondering someone nlp side shed light playing around nltk library wa specifically looking stopwords function provided package nltk corpus stopwords word english wa ha doe understand word present necessary determine sentiment inside sentence instance sentence like sure problem totally different stopword removed changing meaning sentence opposite case set rule missing use stopwords
Loop through text and extract pre-defined words and word pairs in Rails,"<p>I have a large string of text <code>description</code>, up to 500 words long. I would like to do the following:</p>

<ol>
<li>Loop through <code>description</code> and look for a large number of pre-defined words from array <code>keywords</code>, which contains single words, word pairs and word triplets.</li>
<li>Every time a match is found, add this match to a new array <code>matches</code> (unless already added earlier in the process) and remove the matched word(s) from <code>description</code>.</li>
</ol>

<p>I've had a look around for solutions, but most of them seem to either dive in at the deep end of natural language processing, which would be too complex for my current needs, or simply split the text string on spaces which means that it's then impossible to look for word pairs.</p>

<p>Would greatly appreciate any ideas as to how to do this efficiently.</p>
",Multilingual Language Processing & Language Identification,loop text extract pre defined word word pair rail large string text word long would like following loop look large number pre defined word array contains single word word pair word triplet every time match found add match new array unless already added earlier process remove matched word look around solution seem either dive deep end natural language processing would complex current need simply split text string space mean impossible look word pair would greatly appreciate idea efficiently
How do I do use non-integer string labels with SVM from scikit-learn? Python,"<p>Scikit-learn has fairly user-friendly python modules for machine learning.</p>

<p>I am trying to train an SVM tagger for Natural Language Processing (NLP) where my labels and input data are words and annotation. E.g. Part-Of-Speech tagging, rather than using double/integer data as input tuples <code>[[1,2], [2,0]]</code>, my tuples will look like this <code>[['word','NOUN'], ['young', 'adjective']]</code></p>

<p>Can anyone give an example of how i can use the SVM with string tuples? the tutorial/documentation given here are for integer/double inputs. <a href=""http://scikit-learn.org/stable/modules/svm.html"">http://scikit-learn.org/stable/modules/svm.html</a></p>
",Multilingual Language Processing & Language Identification,use non integer string label svm scikit learn python scikit learn ha fairly user friendly python module machine learning trying train svm tagger natural language processing nlp label input data word annotation e g part speech tagging rather using double integer data input tuples tuples look like anyone give example use svm string tuples tutorial documentation given integer double input href
Java library/api which converts language code to language name,"<p>Is there a Java library/api which , given an iso language code, returns the corresponding language name. For example zh-cn should return chinese, en should return english and so on.</p>
",Multilingual Language Processing & Language Identification,java library api convert language code language name java library api given iso language code return corresponding language name example zh cn return chinese en return english
Is it possible to compress text using natural language processing?,"<p>I was thinking about compressing large blocks of text using most frequent english words, but now I doubt it would be efficient, since lzw seems to be achieving just this in a better way.</p>

<p>Still, I can't shake the feeling compressing character one by one is a little ""brutal"", since one could just analyze the structure of sentences to better organize it into smaller chunks of data, and the structure is not exactly the same when decompressed, it could use classic compression methods.</p>

<p>Does ""basic"" NLP allows that ?</p>
",Multilingual Language Processing & Language Identification,possible compress text using natural language processing wa thinking compressing large block text using frequent english word doubt would efficient since lzw seems achieving better way still shake feeling compressing character one one little brutal since one could analyze structure sentence better organize smaller chunk data structure exactly decompressed could use classic compression method doe basic nlp allows
How do I split a piece of Chinese text into individual characters?,"<p>I'm working on a machine learning project, where I'm building a Naive Bayes classifier over Chinese text. I want to use n-grams of Chinese characters as features, so I need to be able to split the text into unigrams (individual characters), bigrams (sequences of two characters), and so forth. (I don't care about special tokenization and such -- I just want raw characters as n-grams.)</p>

<p>How do I do this in Scala? I tried <code>text.sliding(2)</code> to get bigrams, but this doesn't quite seem to work. (I'm guessing because Chinese characters are not a single byte like they are in English?)</p>
",Multilingual Language Processing & Language Identification,split piece chinese text individual character working machine learning project building naive bayes classifier chinese text want use n gram chinese character feature need able split text unigrams individual character bigram sequence two character forth care special tokenization want raw character n gram scala tried get bigram quite seem work guessing chinese character single byte like english
Information on Natural Language Processing,"<p>I need some explanation about NLP. Could using PHP cURL and DOM Parser to extract data from unstructured html content to form a structured content and then save the content into a database be regarded as a form of NLP ?. Any explanation would be appreciated please.</p>
",Multilingual Language Processing & Language Identification,information natural language processing need explanation nlp could using php curl dom parser extract data unstructured html content form structured content save content database regarded form nlp explanation would appreciated please
Rubygem: ruby gem to process language,"<p>I'm trying to find a ruby gem that allows me to process English.</p>

<p>I wonder if there's a gem that can detect a subject in a phrase and its predicate easily.</p>

<p>If there's not a gem, is there an easy algorithm in ruby to do that?</p>
",Multilingual Language Processing & Language Identification,rubygem ruby gem process language trying find ruby gem allows process english wonder gem detect subject phrase predicate easily gem easy algorithm ruby
Profanities in Django comments,"<p>Since Django doesn't handle filtering profanities - does anyone have any suggestions on an easy way to implement some sort of natural language processing / filtering of profanities in django?</p>
",Multilingual Language Processing & Language Identification,profanity django comment since django handle filtering profanity doe anyone suggestion easy way implement sort natural language processing filtering profanity django
Check if string is a grammatically valid sentence?,"<p>Is there a way to check if a string represents an English sentence? I am currently looking at java packages but I cannot find anything that does it yet.</p>

<p>i.e.</p>

<pre><code>the weather is good (valid)
the good is weather (invalid) 
</code></pre>

<p>Any ideas?</p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,check string grammatically valid sentence way check string represents english sentence currently looking java package find anything doe yet e idea thanks
Build a dictionary from two sentenced-aligned texts?,"<p>I have a text corpus which is already aligned at sentence level by construction - it is a list of pairs of English strings and their translation in another language. I have about 10 000 strings of 5 - 20 words each and their translations. My goal is to try to build a metric of the quality of the translation - automatically of course, because I'm dealing with languages I know nothing about :)</p>

<p>I'd like to build a dictionary from this list of translations that would give me the (most probable) translation of each word in the source English strings into the other language. I know the dictionary will be far from perfect but I'm hoping I can have something good enough to flag when a word is not consistently translated, for example, if my dictionary says ""Store"" is to be tranlated into French by ""Magasin"" then if I spot some place where ""Store"" is translated as ""Boutique"" I can suspect that something is wrong.</p>

<p>So I'd need to:</p>

<ol>
<li>build a dictionary from my corpus</li>
<li>align the words inside the string/translation pairs</li>
</ol>

<p>Do you have good references on how to do this? Known algorithms? I found many links about text alignment but they seem to be more at the sentence level than at the word level...</p>

<p>Any other suggestion on how to automatically check whether a translation is consistent would be greatly appreciated!</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,build dictionary two sentenced aligned text text corpus already aligned sentence level construction list pair english string translation another language string word translation goal try build metric quality translation automatically course dealing language know nothing like build dictionary list translation would give probable translation word source english string language know dictionary far perfect hoping something good enough flag word consistently translated example dictionary say store tranlated french magasin spot place store translated boutique suspect something wrong need build dictionary corpus align word inside string translation pair good reference known algorithm found many link text alignment seem sentence level word level suggestion automatically check whether translation consistent would greatly appreciated thanks advance
List of Natural Language Processing Tools in Regards to Sentiment Analysis - Which one do you recommend,"<p>first up sorry for my not so perfect English... I am from Germany ;) </p>

<p>So, for a research project of mine (Bachelor thesis) I need to analyze the sentiment of tweets about certain companies and brands. For this purpose I will need to script my own program / use some sort of modified open source code (no APIs' - I need to understand what is happening). </p>

<p>Below you will find a list of some of the NLP Applications I found. My Question now is which one and which approach would you recommend? And which one does not require long nights adjusting the code?</p>

<p>For example: When I screen twitter for the music player >iPod&lt; and someone writes: ""It's a terrible day but at least my iPod makes me happy"" or even harder: ""It's a terrible day but at least my iPod makes up for it"" </p>

<p>Which software is smart enough to understand that the focused is on iPod and not the weather? </p>

<p>Also which software is scalable / resource efficient (I want to analyze several tweets and don't want to spend thousands of dollars)? </p>

<p><strong>Machine learning and data mining</strong></p>

<p><em>Weka</em> - is a collection of machine learning algorithms for data mining. It is one of the most popular text classification frameworks. It contains implementations of a wide variety of algorithms including Naive Bayes and Support Vector Machines (SVM, listed under SMO) [Note: Other commonly used non-Java SVM implementations are SVM-Light, LibSVM, and SVMTorch]. A related project is Kea (Keyphrase Extraction Algorithm) an algorithm for extracting keyphrases from text documents.</p>

<p><em>Apache Lucene Mahout</em> - An incubator project to created highly scalable distributed implementations of common machine learning algorithms on top of the Hadoop map-reduce framework.</p>

<p><strong>NLP Tools</strong></p>

<p><em>LingPipe</em> - (not technically 'open-source, see below) Alias-I's Lingpipe is a suite of java tools for linguistic processing of text including entity extraction, speech tagging (pos) , clustering, classification, etc... It is one of the most mature and widely used open source NLP toolkits in industry. It is known for it's speed, stability, and scalability. One of its best features is the extensive collection of well-written tutorials to help you get started. They have a list of links to competition, both academic and industrial tools. Be sure to check out their blog. LingPipe is released under a royalty-free commercial license that includes the source code, but it's not technically 'open-source'.</p>

<p><em>OpenNLP</em> - hosts a variety of java-based NLP tools which perform sentence detection, tokenization, part-of-speech tagging, chunking and parsing, named-entity detection, and co-reference analysis using the Maxent machine learning package.</p>

<p><em>Stanford Parser and Part-of-Speech (POS) Tagger</em> - Java packages for sentence parsing and part of speech tagging from the Stanford NLP group. It has implementations of probabilistic natural language parsers, both highly optimized PCFG and lexicalized dependency parsers, and a lexicalized PCFG parser. It's has a full GNU GPL license.</p>

<p><em>OpenFST</em> - A package for manipulating weighted finite state automata. These are often used to represented a probablistic model. They are used to model text for speech recognition, OCR error correction, machine translation, and a variety of other tasks. The library was developed by contributors from Google Research and NYU. It is a C++ library that is meant to be fast and scalable.</p>

<p><em>NTLK</em> - The natural language toolkit is a tool for teaching and researching classification, clustering, speech tagging and parsing, and more. It contains a set of tutorials and data sets for experimentation. It is written by Steven Bird, from the University of Melbourne.</p>

<p><em>Opinion Finder</em> - A system that performs subjectivity analysis, automatically identifying when opinions, sentiments, speculations and other private states are present in text. Specifically, OpinionFinder aims to identify subjective sentences and to mark various aspects of the subjectivity in these sentences, including the source (holder) of the subjectivity and words that are included in phrases expressing positive or negative sentiments.</p>

<p><em>Tawlk/osae</em> - A python library for sentiment classification on social text. The end-goal is to have a simple library that ""just works"". It should have an easy barrier to entry and be thoroughly documented. We have acheived best accuracy using stopwords filtering with tweets collected on negwords.txt and poswords.txt</p>

<p><em>GATE</em> - GATE is over 15 years old and is in active use for all types of computational task involving human language. GATE excels at text analysis of all shapes and sizes. From large corporations to small startups, from ‚Ç¨multi-million research consortia to undergraduate projects, our user community is the largest and most diverse of any system of this type, and is spread across all but one of the continents1.</p>

<p><em>textir</em> - A suite of tools for text and sentiment mining. This includes the ‚Äòmnlm‚Äô function, for sparse multinomial logistic regression, ‚Äòpls‚Äô, a concise partial least squares routine, and the ‚Äòtopics‚Äô function, for efficient estimation and dimension selection in latent topic models.</p>

<p>NLP Toolsuite - The JULIE Lab here offers a comprehensive NLP tool suite for the application purposes of semantic search, information extraction and text mining. Most of our continuously expanding tool suite is based on machine learning methods and thus is domain- and language independent.</p>

<p>...</p>

<p>On a side note: Would you recommend the twitter streaming or the get API? </p>

<p>As to me, I am a fan of python and java ;)</p>

<p>Thanks a lot for your help!!!</p>
",Multilingual Language Processing & Language Identification,list natural language processing tool regard sentiment analysis one recommend first sorry perfect english germany research project mine bachelor thesis need analyze sentiment tweet certain company brand purpose need script program use sort modified open source code apis need understand happening find list nlp application found question one approach would recommend one doe require long night adjusting code example screen twitter music player ipod someone writes terrible day least ipod make happy even harder terrible day least ipod make software smart enough understand focused ipod weather also software scalable resource efficient want analyze several tweet want spend thousand dollar machine learning data mining weka collection machine learning algorithm data mining one popular text classification framework contains implementation wide variety algorithm including naive bayes support vector machine svm listed smo note commonly used non java svm implementation svm light libsvm svmtorch related project kea keyphrase extraction algorithm algorithm extracting keyphrases text document apache lucene mahout incubator project created highly scalable distributed implementation common machine learning algorithm top hadoop map reduce framework nlp tool lingpipe technically open source see alias lingpipe suite java tool linguistic processing text including entity extraction speech tagging po clustering classification etc one mature widely used open source nlp toolkits industry known speed stability scalability one best feature extensive collection well written tutorial help get started list link competition academic industrial tool sure check blog lingpipe released royalty free commercial license includes source code technically open source opennlp host variety java based nlp tool perform sentence detection tokenization part speech tagging chunking parsing named entity detection co reference analysis using maxent machine learning package stanford parser part speech po tagger java package sentence parsing part speech tagging stanford nlp group ha implementation probabilistic natural language parser highly optimized pcfg lexicalized dependency parser lexicalized pcfg parser ha full gnu gpl license openfst package manipulating weighted finite state automaton often used represented probablistic model used model text speech recognition ocr error correction machine translation variety task library wa developed contributor google research nyu c library meant fast scalable ntlk natural language toolkit tool teaching researching classification clustering speech tagging parsing contains set tutorial data set experimentation written bird university melbourne opinion finder system performs subjectivity analysis automatically identifying opinion sentiment speculation private state present text specifically opinionfinder aim identify subjective sentence mark various aspect subjectivity sentence including source holder subjectivity word included phrase expressing positive negative sentiment tawlk osae python library sentiment classification social text end goal simple library work easy barrier entry thoroughly documented acheived best accuracy using stopwords filtering tweet collected negwords txt poswords txt gate gate year old active use type computational task involving human language gate excels text analysis shape size large corporation small startup multi million research consortium undergraduate project user community largest diverse system type spread across one continent textir suite tool text sentiment mining includes mnlm function sparse multinomial logistic regression pls concise partial least square routine topic function efficient estimation dimension selection latent topic model nlp toolsuite julie lab offer comprehensive nlp tool suite application purpose semantic search information extraction text mining continuously expanding tool suite based machine learning method thus domain language independent side note would recommend twitter streaming get api fan python java thanks lot help
Filtering out meaningless phrases,"<p>I have an algorithm (that I can't change) that outputs a list of phrases. These phrases are intended to be ""topics"". However, some of them are meaningless on their own. Take this list:
<br/><br/></p>

<pre><code>is the fear
freesat
are more likely to
first sight
an hour of
sue apple
depression and
itunes
</code></pre>

<p><br/>
How can I filter out those phrases that don't make sense on their own, to leave a list like the following?
<br/><br/></p>

<pre><code>freesat
first sight
sue apple
itunes
</code></pre>

<p><br/>This will be applied to sets of phrases in many languages, but English is the priority.</p>
",Multilingual Language Processing & Language Identification,filtering meaningless phrase algorithm change output list phrase phrase intended topic however meaningless take list filter phrase make sense leave list like following applied set phrase many language english priority
How to &quot;smartly&quot; translate formulas into form of natural language?,"<p>I am recently working on a project aiming at evaluating whether an android app crashes or not. The evaluation process is:</p>

<ol>
<li>Collect the logs(which record the execution process of an app).</li>
<li>Generate formulas to predict the result (formulas is generated by GP)</li>
<li>Evaluate the logs by formulas</li>
</ol>

<p>Now I can produce formulas, but for convenience for users, I want to translate formulas into form of natural language and tell users why crash happened.(I think it looks like ""inverse natural language processing"".)</p>

<p>To explain the idea more clearly, imagine you got a formula like this:</p>

<pre><code>  155 - count(onKeyDown) &gt;= 148
</code></pre>

<p>It's obvious that if count(onKeyDown) > 7, the result of ""155 - count(onKeyDown) >= 148"" is false, so the log contains more than 7 onKeyDown event would be predicted ""Failed"".</p>

<p>I want to show users that if onKeyDown event appears more than 7 times(155-148=7), this app will crash.</p>

<p>However, the real formula is much more complicated, such as:</p>

<pre><code>(&lt; !( ( SUM( {Att[17]}, Event[5]) &lt;= MAX( {Att[7]}, Att[0] &gt;= Att[11]) OR SUM( {Att[17]}, Event[5]) &gt; MIN( {Att[12]}, 734 &gt; Att[19]) ) OR count(Event[5]) != 1 ) &gt; (&lt; count(Att[4] = Att[3]) &gt;= count(702 != Att[8]) + 348 / SUM( {Att[13]}, 641 &lt; Att[12]) mod 587 - SUM( {Att[13]}, Att[10] &lt; Att[15]) mod MAX( {Att[13]}, Event[2]) + 384 &gt; count(Event[10]) != 1))
</code></pre>

<p>I tried to implement this function by C++, but it's quite difficult, <a href=""http://homepage.ntu.edu.tw/~b98901160/question/printNatural.cpp"" rel=""nofollow"">here's the snippet of code</a> I am working right now.</p>

<p>Does anyone knows how to implement this function quickly?(maybe by some tools or research findings?)Any idea is welcomed :)</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,smartly translate formula form natural language recently working project aiming evaluating whether android app crash evaluation process collect log record execution process app generate formula predict result formula generated gp evaluate log formula produce formula convenience user want translate formula form natural language tell user crash happened think look like inverse natural language processing explain idea clearly imagine got formula like obvious count onkeydown result count onkeydown false log contains onkeydown event would predicted failed want show user onkeydown event appears time app crash however real formula much complicated tried implement function c quite difficult snippet code working right doe anyone know implement function quickly maybe tool research finding idea welcomed thanks advance
Where can I find dictionaries in multiple languages to do language detection?,"<p>Since that Microsoft language-detect API is capped, and that Google's alternative is really expensive, I'm evaluating to create my own ""api"" to detect the language of a given text string.
Do you have any idea of where I can find dictionaries for many languages (at least 10) to begin?</p>
",Multilingual Language Processing & Language Identification,find dictionary multiple language language detection since microsoft language detect api capped google alternative really expensive evaluating create api detect language given text string idea find dictionary many language least begin
Dividing string of characters to words and sentences (English only),"<p>I'm looking for a solution to following task. I take few random pages from random book in English and remove all non letter characters and convert all chars to lower case. As a result I have something like: </p>

<blockquote>
  <p>wheniwasakidiwantedtobeapilot...</p>
</blockquote>

<p>Now what I'm looking for is something that could reverse that process with quite a good accuracy. I need to find words and sentence separators. Any ideas how to approach this problem? Are there existing solutions I can base on without reinventing the wheel?</p>
",Multilingual Language Processing & Language Identification,dividing string character word sentence english looking solution following task take random page random book english remove non letter character convert char lower case result something like wheniwasakidiwantedtobeapilot looking something could reverse process quite good accuracy need find word sentence separator idea approach problem existing solution base without reinventing wheel
NLP - Morphological Dictionary for English,"<p>I need to be able to morphologically generate a word according to its lemma and a POS Tag value. For instance change a lemma verb form to its past tense, present or change a lemma noun to its plural form...
For that I need a Morphological Dictionary for English that will include a mapping from a lemma to the word with the type requested (plural, past tense etc...).
Does anyone know of a good Morphological Dictionary for English available for download?</p>

<p>Thanks a lot,
Yaron</p>
",Multilingual Language Processing & Language Identification,nlp morphological dictionary english need able morphologically generate word according lemma po tag value instance change lemma verb form past tense present change lemma noun plural form need morphological dictionary english include mapping lemma word type requested plural past tense etc doe anyone know good morphological dictionary english available download thanks lot yaron
C# Japanese morphological analyzers,"<p>I can't find any Japanese morphological analyzers for C#. Can anyone please suggest one?</p>
",Multilingual Language Processing & Language Identification,c japanese morphological analyzer find japanese morphological analyzer c anyone please suggest one
Pulling out dates from a string in a variety of formats,"<p>I'm trying to pull as many dates (YMD, YM, or even just Y) out of a very large text sample in an SQL database. All of the text in the database is in plain English and contains large numbers as well. What I'm looking to do is find as many of these dates as possible, recognize them as dates, and pull out what date they represent as well as the string that was found. Is there a way to go about this other than thinking up every possible regular expression I can to cover cases like 1/23/1985 while also covering cases like ""The 1980s"" and ""1962 marked the beginning"". Is there an example out there of a project that already took this on? Thanks!</p>
",Multilingual Language Processing & Language Identification,pulling date string variety format trying pull many date ymd ym even large text sample sql database text database plain english contains large number well looking find many date possible recognize date pull date represent well string wa found way go thinking every possible regular expression cover case like also covering case like marked beginning example project already took thanks
POS-Tagging API for Italian Language on Mac OSX,"<p>I need help Looking for POS-Tagging API that works on documents in Italian.
My preference is for open source code (possibly; ruby, jruby, macruby, java, scala).
The program that I write will run on Mac OsX and I have already explored <a href=""http://www-nlp.stanford.edu/links/statnlp.html#Taggers"" rel=""nofollow"" title=""Statistical natural language processing and corpus-based computational linguistics: An annotated list of resources"">this list</a> but there is no much for ""Italian Language""</p>

<p><a href=""https://developer.apple.com/library/mac/#releasenotes/Cocoa/Foundation.html"" rel=""nofollow"" title=""Mac OS X Developer Release Note"">As of 10.8, Cocoa NSLinguisticTagger provides parts-of-speech tags and lemmas for Spanish and Italian</a>, I could try it, but before upgrading my OSX 10.7 please let me know if you think really worth it or if I have other good options.</p>
",Multilingual Language Processing & Language Identification,po tagging api italian language mac osx need help looking po tagging api work document italian preference open source code possibly ruby jruby macruby java scala program write run mac osx already explored list much italian language cocoa nslinguistictagger provides part speech tag lemma spanish italian could try upgrading osx please let know think really worth good option
Using NLP to switch genders,"<p>Basically I am writing a Java module that is supposed to take English text and switch the genders of the pronouns.  So for example, if you give it ""She put the box on the table"" it would give you back ""He put the box on the table.""  If you gave it ""His feet hurt"" it would give you back ""Her feet hurt.""</p>

<p>This is pretty much easy, except for the word ""hers.""  Sometimes his = her, sometimes his = hers.</p>

<p>I've been looking into NLP, which I know pretty much nothing about, and I tried out OpenNLP but it's failing me (I can't use the Standford NLP because of the licensing issue).  The POS tagger and the Chunker get confused with her/hers, and so even does the parser.  So for example:</p>

<p>The box was his.</p>

<pre><code>(TOP (S (NP (DT The) (NN box)) (VP (VBD was) (NP (PRP$ his))) (. .)))
</code></pre>

<p>The box was hers.</p>

<pre><code>(TOP (S (NP (DT The) (NN box)) (VP (VBD was) (ADJP (JJ hers))) (. .)))
</code></pre>

<p>The box was his box.</p>

<pre><code>(TOP (S (NP (DT The) (NN box)) (VP (VBD was) (NP (PRP$ his) (NN box))) (. .)))
</code></pre>

<p>The box was her box.</p>

<pre><code>(TOP (S (NP (DT The) (NN box)) (VP (VBD was) (NP (PRP$ her) (NN box))) (. .)))
</code></pre>

<p>It correctly identifies ""hers"" as an adjective phrase, but when ""his"" is used in the predicate in the exact same way, it incorrectly identifies it as a possessive pronoun, as if it was modifying some noun as in the third and fourth examples..</p>

<p>Is this just an issue of training set?  Would it be possible to create my own training set that does a better job of doing this, basically a set that just has tons of his/hers sentences?</p>

<p>Bonus points if you can tell me whether there's any way to use NLP to determine the antecedent of a pronoun.  For example:</p>

<pre><code>""Wanda gave a watch to a girl named Lucy.  She loved it.""
</code></pre>

<p>My guess is this is pretty much impossible since this is sometimes even hard for humans.</p>
",Multilingual Language Processing & Language Identification,using nlp switch gender basically writing java module supposed take english text switch gender pronoun example give put box table would give back put box table gave foot hurt would give back foot hurt pretty much easy except word sometimes sometimes looking nlp know pretty much nothing tried opennlp failing use standford nlp licensing issue po tagger chunker get confused even doe parser example box wa box wa box wa box box wa box correctly identifies adjective phrase used predicate exact way incorrectly identifies possessive pronoun wa modifying noun third fourth example issue training set would possible create training set doe better job basically set ha ton sentence bonus point tell whether way use nlp determine antecedent pronoun example guess pretty much impossible since sometimes even hard human
Understanding Brown tags,"<p>I was reading about linguistics in relation to Natural Language Processing, but the Brown tags are confusing me.</p>

<p>Can you help me explaining the following tags (if you can add an example, much better)? All of them are related to <a href=""http://en.wikipedia.org/wiki/Interrogative_pronoun"" rel=""nofollow"">interrogative pronouns and interrogative determiners</a>.</p>

<ol>
<li><p>WDT (wh-determiner: what, which)</p></li>
<li><p>WP$ (possessive wh-pronoun: whose)</p></li>
<li><p>WPO (objective wh-pronoun: whom, which, that)</p></li>
<li><p>WPS (nominative wh-pronoun: who, which, that)</p></li>
</ol>

<p>For example, a determiner should be <strong>the</strong> in <strong>The dog</strong> because as it's defined, it describes the reference of a noun in a context. But what about a wh-determiner? What purpose does it serve? Asking about the reference of a noun or in which way should I interpret these forms?</p>

<p>Thanks a lot</p>
",Multilingual Language Processing & Language Identification,understanding brown tag wa reading linguistics relation natural language processing brown tag confusing help explaining following tag add example much better related interrogative pronoun interrogative determiner wdt wh determiner wp possessive wh pronoun whose wpo objective wh pronoun wps nominative wh pronoun example determiner dog defined describes reference noun context wh determiner purpose doe serve asking reference noun way interpret form thanks lot
Natural Language Processing - Word Alignment,"<p>I am looking for word alignment tools and algorithms.<br>
I am dealing with bilingual English - Hindi text, and currently working on </p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""noreferrer""><strong>DTW</strong></a> (Dynamic Time Warping) algorithm</li>
<li><strong>CLA</strong> (Competitive Linking Algorithm)</li>
<li><a href=""http://linguateca.di.uminho.pt/natools/"" rel=""noreferrer""><strong>NATools</strong></a></li>
<li><a href=""http://www.fjoch.com/GIZA++.html"" rel=""noreferrer""><strong>Giza++</strong></a></li>
</ul>

<p>Could you please suggest any other algorithm/tool which is language independent and which could achieve <strong>Statistical word alignment for parallel English Hindi Corpora and its evaluation</strong>.<br>
Some tools are best for certain languages; could you please tell me how true that is and, if so, could you please provide an example of what would be better suited  for Asian languages like Hindi.  Counter-examples of what one shouldn't I use for such languages is also welcome.</p>

<p>I have heard a bit about <a href=""http://stp.lingfil.uu.se/~joerg/uplug/"" rel=""noreferrer""><strong>Uplug word aligner</strong></a>... Could someone tell me if this tool is useful for my purpose.</p>

<p>Thank you.. :)</p>
",Multilingual Language Processing & Language Identification,natural language processing word alignment looking word alignment tool algorithm dealing bilingual english hindi text currently working dtw dynamic time warping algorithm cla competitive linking algorithm natools giza could please suggest algorithm tool language independent could achieve statistical word alignment parallel english hindi corpus evaluation tool best certain language could please tell true could please provide example would better suited asian language like hindi counter example one use language also welcome heard bit uplug word aligner could someone tell tool useful purpose thank
"can NLTK/pyNLTK work &quot;per language&quot; (i.e. non-english), and how?","<p>How can I tell NLTK to treat the text in a particular language?</p>

<p>Once in a while I write a specialized NLP routine to do POS tagging, tokenizing and etc. on a non-english (but still hindo-European) text domain.</p>

<p>This question seem to address only different corpora, not the change in code/settings:
<a href=""https://stackoverflow.com/questions/1639855/nltk-tagging-in-german"">POS tagging in German</a></p>

<p>Alternatively,are there any specialized Hebrew/Spanish/Polish NLP modules for python?</p>
",Multilingual Language Processing & Language Identification,nltk pynltk work per language e non english tell nltk treat text particular language write specialized nlp routine po tagging tokenizing etc non english still hindo european text domain question seem address different corpus change code setting href tagging german alternatively specialized hebrew spanish polish nlp module python
How to parse text too variable for formal grammars but too constrained for NLP?,"<p>I'm dealing with a corpus of text that is written informally, but generally conforms to a very standard format by convention (think something like <em>Froyo Frozen Yogurt</em>, <em>Smucker's Peanut Butter</em>) and occasionally requires recursion (<em>Froyo Frozen Yogurt with Smucker's Peanut Butter</em>). </p>

<p>With regexes, the complexity quickly grows out of hand (<em>Frozen Yogurt by Froyo</em>, <em>Froyo Frozen Yogurt with Peanut Butter by Smucker's</em>, etc).</p>

<p>I'm having trouble finding resources to help me write an EBNF for this, and NLP method are too complex (plus my ""parts of speech"" don't really correspond to normal english). Is there an intermediate approach, aimed at semi-formalized text? </p>
",Multilingual Language Processing & Language Identification,parse text variable formal grammar constrained nlp dealing corpus text written informally generally conforms standard format convention think something like froyo frozen yogurt smucker peanut butter occasionally requires recursion froyo frozen yogurt smucker peanut butter regexes complexity quickly grows hand frozen yogurt froyo froyo frozen yogurt peanut butter smucker etc trouble finding resource help write ebnf nlp method complex plus part speech really correspond normal english intermediate approach aimed semi formalized text
NLTK stem words produces odd results,"<p>After running <code>nltk.stem.porter.PorterStemmer().stem_word(word)</code> I get many words with 'ing' cut off or 'y' swapped with 'i' . e.g. 'Quality' becomes 'Qualiti' and (even stranger) 'value' becomes 'valu'?</p>

<p>As the resulting words are not actual english words, I am not sure how am I meant to use them? My best guess is that I am meant to put the stem words into another function which will give me all the derived/child words from this stem (e.g. 'valu' would return <code>['valuing','valued', 'values', ...]</code>. Is there such a function?</p>
",Multilingual Language Processing & Language Identification,nltk stem word produce odd result running get many word ing cut swapped e g quality becomes qualiti even stranger value becomes valu resulting word actual english word sure meant use best guess meant put stem word another function give derived child word stem e g valu would return function
"Determine whether a romanized name is Japanese or not, preferably in Ruby","<p>How can I determine whether a romanized name is likely, or unlikely, to be a Japanese name?</p>

<pre><code>""Yukihiro Matsumoto"".likely_to_be_japanese? # =&gt; true
""John Smith"".likely_to_be_japanese? # =&gt; false
</code></pre>

<p>Ideally, I'd also like to feed in free-form text, and detect whether the text has a name likely to be Japanese, has a name unlikely to be Japanese, or doesn't have any names in it. Preferably ignoring false positives.</p>

<pre><code>""call Koichi on (02) 5550 5555"".has_japanese_name_in_it? # =&gt; true
""call John on (02) 5550 5556"".has_non_japanese_name_in_it? # =&gt; true
""utility bill to be shared equally"".has_non_japanese_name_in_it? =&gt; false
</code></pre>

<p>Are there any libraries that can help me do this, preferably in Ruby? Or would I have to find a corpus of Japanese, and non-Japanese, names and build my own solution?</p>
",Multilingual Language Processing & Language Identification,determine whether romanized name japanese preferably ruby determine whether romanized name likely unlikely japanese name ideally also like feed free form text detect whether text ha name likely japanese ha name unlikely japanese name preferably ignoring false positive library help preferably ruby would find corpus japanese non japanese name build solution
How to tokenize italian input?,"<p>I'm trying to tokenize italian text for further processing, in Java.
Is there any tool that for tokenizing italian input?
A <a href=""http://code.google.com/p/tt4j/wiki/SimpleTokenizer"" rel=""nofollow"">SimpleTokenizer</a> works fine a certain extent, but then in case like Italian family names like ""De Marchi"" I get it as 2 tokens.</p>
",Multilingual Language Processing & Language Identification,tokenize italian input trying tokenize italian text processing java tool tokenizing italian input simpletokenizer work fine certain extent case like italian family name like de marchi get token
English Word Declension and Conjugation,"<p>I am currently working on a Natural Language Parser and I need to be able to conjugate English Verbs and Nouns.</p>

<p>I already have a list of Verb Irregulars but I am struggling to find a set of ""rules"" if you will, for conjugating regular English verbs. I know there are a few, various rules such as: ""if the word ends in 'X', then the plural form would end in 'Y'"" with the most basic being add an s to the end.</p>

<p>I am looking for the rules for finding the: Base form, Past simple, Past participle, 3rd person singular, Present participle Gerund</p>

<p>Also, I would be looking to do the same for finding the plurals and possessive forms of any given noun, along with a list of regulars. I have had no source of results or luck in my searching in this area and any help with conjugating (for lack of a better word) nouns would be very helpful.</p>

<p>[edit]
A link to a list of irregular nouns and a list of rules like: if the word ends in a consonant, then ad ""s"" (or whatever) would be awesome!!!
[/edit]</p>

<p>One more thing... for my english ver irregulars, I am using <a href=""http://www.online-languages.info/english/irregular_verbs.php?l=1"" rel=""nofollow"">this site</a></p>

<p>Sorry for what appears to be the lack of searching, trust me I have looked.</p>
",Multilingual Language Processing & Language Identification,english word declension conjugation currently working natural language parser need able conjugate english verb noun already list verb irregular struggling find set rule conjugating regular english verb know various rule word end x plural form would end basic add end looking rule finding base form past simple past participle rd person singular present participle gerund also would looking finding plural possessive form given noun along list regular source result luck searching area help conjugating lack better word noun would helpful edit link list irregular noun list rule like word end consonant ad whatever would awesome edit one thing english ver irregular using site sorry appears lack searching trust looked
extracting English verbs from a given text,"<p>I need to extract all English verbs from a given text and I was wondering how I could do it...
At first glance, my idea is to use regular expressions because all English verb tenses follow patterns but maybe there is another way to do it. What I've thought is simply:</p>

<ol>
<li>Create a pattern for every verb tense. I have to distinguish between regular verbs (http://en.wikipedia.org/wiki/English_verbs) and irregular verbs (http://www.chompchomp.com/rules/irregularrules01.htm) in some way.</li>
<li>Iterate over these patterns and split the text using them (the last word of each substring is supposed to be the verb that gives complete meaning to the sentence, which I need for other purposes -> nominalization)</li>
</ol>

<p>What do you think? I guess this isn't an efficient way to do it but I can't imagine another one.</p>

<p>Thank you in advance!</p>

<p>PS: </p>

<ol>
<li>I have two dictionaries, one for all English Verbs and the other one for all English nouns</li>
<li>The main problem of all this is that the project consists on verb nominalization (is just a uni project), so all the ""effort"" is supposed to be focused in this part, nominalization. In concrete, I follow this model: acl.ldc.upenn.edu/P/P00/P00-1037.pdf). The project consists on given a text, find all the verbs in that text and propose multiple nominalizations for each verb. So the first step (finding verbs), should be as simple as possible... but I can't use any parser, it's not allowed</li>
</ol>
",Multilingual Language Processing & Language Identification,extracting english verb given text need extract english verb given text wa wondering could first glance idea use regular expression english verb tense follow pattern maybe another way thought simply create pattern every verb tense distinguish regular verb irregular verb way iterate pattern split text using last word substring supposed verb give complete meaning sentence need purpose nominalization think guess efficient way imagine another one thank advance p two dictionary one english verb one english noun main problem project consists verb nominalization uni project effort supposed focused part nominalization concrete follow model acl ldc upenn edu p p p pdf project consists given text find verb text propose multiple nominalizations verb first step finding verb simple possible use parser allowed
"Can you programmatically detect pluralizations of English words, and derive the singular form?","<p><strong>Given some (English) word that we shall assume is a plural</strong>, is it possible to derive the singular form? I'd like to avoid lookup/dictionary tables if possible.</p>

<p>Some examples:</p>

<pre>
Examples  -> Example    a simple 's' suffix
Glitch    -> Glitches   'es' suffix, as opposed to above
Countries -> Country    'ies' suffix.
Sheep     -> Sheep      no change: possible fallback for indeterminate values
</pre>

<p>Or, <a href=""http://en.wiktionary.org/wiki/Appendix:Irregular_plurals:English"" rel=""nofollow noreferrer"">this seems to be a fairly exhaustive list.</a></p>

<p>Suggestions of libraries in language <code>x</code> are fine, as long as they are open-source (ie, so that someone can examine them to determine how to do it in language <code>y</code>)</p>
",Multilingual Language Processing & Language Identification,programmatically detect pluralization english word derive singular form given english word shall assume plural possible derive singular form like avoid lookup dictionary table possible example example example simple suffix glitch glitch e suffix opposed country country suffix sheep sheep change possible fallback indeterminate value seems fairly exhaustive list suggestion library language fine long open source ie someone examine determine language
Javascript regex retrieve variables from sentence,"<p>I'm wondering if it's possible to extract a number of variables from a predefined sentence with regex or similar.</p>

<p>e.g.</p>

<p>If this was the pattern...</p>

<pre><code>""How many * awards did * win in *?""
</code></pre>

<p>And someone typed...</p>

<pre><code>""How many gold awards did johnny win in 2008?""
</code></pre>

<p>How can I somehow return... </p>

<pre><code>[""gold"",""johnny"",""2008""]
</code></pre>

<p>I'd also like to return the fact that it matches the pattern before retrieving the variables as there will be many different patterns. Note: It will also be possible for someone to type multiple words in place of a * e.g. <em>johnny english</em> instead of just <em>johnny</em></p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,javascript regex retrieve variable sentence wondering possible extract number variable predefined sentence regex similar e g wa pattern someone typed somehow return also like return fact match pattern retrieving variable many different pattern note also possible someone type multiple word place e g johnny english instead johnny thanks
Check English grammar,"<p>I am looking for a simple C# library that does the following: Takes a string representing a single sentence, and returns a boolean saying if it's grammatically correct.</p>

<p>I have not been able to find a single, self-contained library that does this after extensive searching.</p>
",Multilingual Language Processing & Language Identification,check english grammar looking simple c library doe following take string representing single sentence return boolean saying grammatically correct able find single self contained library doe extensive searching
How to build a 4language dictionary from bilingual dictionaries,"<p>3 days ago I asked a question about building a parallel dictionary which has 3 languages: <a href=""https://stackoverflow.com/questions/11135928/removing-differencies-of-some-text-pairs"">removing differencies of some text pairs</a></p>

<p>The question was as follows:
 I have 2 pair of files by the following characteristics: pair1: (File1.txt , File2.txt) pair2: (File3.txt , File4.txt)</p>

<p>There is a line by line correspondence between each files in these pairs. say that File1.txt and File3.txt are some English words, and File2.txt and File4.txt are their Arabic and French translations respectively. In addition, File1.txt and File3.txt are very similar (and in some cases the same).</p>

<pre><code>
    File1.txt       File2.txt
    EnWord1         ArTrans1
    EnWord2         ArTrans2
    EnWord3         ArTrans3
    Enword4         ArTrans4

    File3.txt       File4.txt
    EnWord1         FrTrans1
    EnWord3         FrTrans3
    Enword4         FrTrans4
    Enword5         FrTrans5
</code></pre>

<p>What I wanted to do then, was to compare English sides of the pairs, find the common words that appear in both files (EnWord1,EnWord3, and EnWord4) and filter out their corresponding translations.
In short, I can say that using two bilingual English-Arabic and English French dictionaries, I am trying to build a 3-lingual English-Arabic-French dictionary.</p>

<p>Steve answered me and wrote a nice code to find duplicated English words and remove others and their translations: <a href=""https://stackoverflow.com/a/11141345/1265960"">The answer could be found here</a></p>

<p>But I still have a bit more complicated question:
What should I do, if I want to add another language? I mean that I have another English-Russian dictionary (Say File5.txt contains English entities, and File6.txt contains Russian entities) and I want to build a 4-language dictionary instead of a 3-language one.</p>

<p>one way is to build a 3-language dictionary using the current code, and then by rerunning it on a new language pair, build a 4-language dictionary. but I think it is not efficient enough, and it would be better solution to this problem. It also may bring in some inconsistencies in other languages.
My main challenge is checking the duplications: when have just 2 language pairs, it would be very easy to check the duplications. But what should I do if I want to check the duplications in 3 pairs?
How can I change the code to be able to extract 4language dictionary in just one pass?</p>
",Multilingual Language Processing & Language Identification,build language dictionary bilingual dictionary day ago asked question building parallel dictionary ha language still bit complicated question want add another language mean another english russian dictionary say file txt contains english entity file txt contains russian entity want build language dictionary instead language one one way build language dictionary using current code rerunning new language pair build language dictionary think efficient enough would better solution problem also may bring inconsistency language main challenge checking duplication language pair would easy check duplication want check duplication pair change code able extract language dictionary one pas
English query generation through machine translation systems,"<p>I'm working on a project to generate questions from sentences. Right now, I'm at a point where I can generate questions like:
""Angela Merkel is the chancelor of Germany."" -> ""Angela Merkel is who?""</p>

<p>Now, of course, I want the questions to look like ""Who is...?"" instead. Is there any easy way to do this that I haven't thought of yet?</p>

<p>My current idea would be to train an English(not quite question) -> English(question) translator, maybe using existing machine translation engines like moses. Is this overkill? How much data would I need? Are there corpora that address this or a similar problem? Is using a general translation engine even appropriate for this task?</p>
",Multilingual Language Processing & Language Identification,english query generation machine translation system working project generate question sentence right point generate question like angela merkel chancelor germany angela merkel course want question look like instead easy way thought yet current idea would train english quite question english question translator maybe using existing machine translation engine like moses overkill much data would need corpus address similar problem using general translation engine even appropriate task
Dictionary words for download,"<p>Can someone offer a suggestion on where to find a dictionary word list with frequency information?</p>

<p>Ideally, the source would be English words of the North American variety.</p>
",Multilingual Language Processing & Language Identification,dictionary word download someone offer suggestion find dictionary word list frequency information ideally source would english word north american variety
"Translation of Single Words, Taking into Account Context, using Computer Language Processing Tools","<p>I would like to automatically annotate texts for learners of foreign languages with translations of difficult words.</p>

<p>For instance, if the original text is:</p>

<blockquote>
  <p>El gato esta en la casa de mis vecinos</p>
</blockquote>

<p>Becomes</p>

<blockquote>
  <p>El gato esta en la casa de mis <strong>vecinos</strong> (<em>neighbours</em>)</p>
</blockquote>

<p>The first step is to identify which words are the difficult ones. This could be done by lemmatization of the words in the original text and comparing them with a list of 'easy words' (a basic vocabulary of 1500-2000 words). Those not found in this list will be designated as 'hard words.' This process seems straightforward enough using the Natural Language Tool Kit (NLTK) for Python.</p>

<p>There is some difficulty in words that must be translated as a pair, such as 'newly weds,' or phrasal verbs 'he <strong>called</strong> me <strong>up</strong>' or the German 'er <strong>ruft</strong> mich <strong>an</strong>' (anrufen). Here words can't be treated individually. For phrasal verbs and the like perhaps some understanding of grammer is needed.</p>

<p>The second step involves obtaining a correct translation of the difficult words according to context in which they appear. As I understand, this is effectively applying the first half of a statistical machine translation system like google translate. I believe this problem could solved using the Google Translate Research API, that lets you send text to be translated, and the response includes information about which word in the translation corresponds to which word in the original text. So you could feed in the whole sentence and then fish out the word you wanted from the response. You have to apply to use this API however, and they have usage limits, which would likely be a problem for my application. I would rather find another solution. I expect no solution will give 100% correct translations and they will have to be checked by hand, but this should still speed things up.</p>

<p>Thanks for your comments.</p>

<p>David</p>
",Multilingual Language Processing & Language Identification,translation single word taking account context using computer language processing tool would like automatically annotate text learner foreign language translation difficult word instance original text el gato esta en la casa de mi vecinos becomes el gato esta en la casa de mi vecinos neighbour first step identify word difficult one could done lemmatization word original text comparing list easy word basic vocabulary word found list designated hard word process seems straightforward enough using natural language tool kit nltk python difficulty word must translated pair newly wed phrasal verb called german er ruft mich anrufen word treated individually phrasal verb like perhaps understanding grammer needed second step involves obtaining correct translation difficult word according context appear understand effectively applying first half statistical machine translation system like google translate believe problem could solved using google translate research api let send text translated response includes information word translation corresponds word original text could feed whole sentence fish word wanted response apply use api however usage limit would likely problem application would rather find another solution expect solution give correct translation checked hand still speed thing thanks comment david
Resources on generating equivalent phrases (same language translation)?,"<p>I'm interested in building a program that takes some text (an article, for example) and then generates a new text with equivalent meaning, but I'm not sure how to get started on such a problem. </p>

<p>Can anyone recommend some code/books/papers/techniques that would help me tackle this?</p>
",Multilingual Language Processing & Language Identification,resource generating equivalent phrase language translation interested building program take text article example generates new text equivalent meaning sure get started problem anyone recommend code book paper technique would help tackle
Japanese language detection using java langdetect library,"<p>I have a problem with language detection for Japanese language using <a href=""http://code.google.com/p/language-detection/"" rel=""nofollow"">java library</a>: </p>

<p>Using Japanese text, I'm trying to detect it's text language, but instead of expected ""ja"" I got ""en"". Has anybody seen this problem before?</p>

<p>What is the expected output? </p>

<pre><code>[ja:0.9999952022259697]
</code></pre>

<p>What do you see instead? </p>

<pre><code>[en:0.9999952022259697]
</code></pre>

<p>Original issue description with Japanese text in attachments you can find <a href=""http://code.google.com/p/language-detection/issues/detail?id=36"" rel=""nofollow"">here</a></p>
",Multilingual Language Processing & Language Identification,japanese language detection using java langdetect library problem language detection japanese language using java library using japanese text trying detect text language instead expected ja got en ha anybody seen problem expected output see instead original issue description japanese text attachment find
Chunking English words into graphemes corresponding to distinct sounds,"<p>How to convert english input word into combinations of graphemes? Is there a library or function that does the job? </p>

<p>What I'm looking for is an algorithm/implementation that splits orthographic words into segments which map to phonemes. That is, the sequence of letters in a word should be broken in between distinct sounds.</p>

<p>To my mind, this would look something like the following:</p>

<pre><code>physically --&gt; ph-y-s-i-c-a-ll-y
psychology --&gt; ps-y-ch-o-l-o-g-y
thrush --&gt;     th-r-u-sh
bought --&gt; b-ough-t
chew --&gt; ch-ew
palm --&gt; p-al-m
</code></pre>
",Multilingual Language Processing & Language Identification,chunking english word grapheme corresponding distinct sound convert english input word combination grapheme library function doe job looking algorithm implementation split orthographic word segment map phoneme sequence letter word broken distinct sound mind would look something like following
Wiktionary API to retrieve word forms (or other free service),"<p>This is a question particularly for Russian/Ukrainian languages but may be useful for other languages too. </p>

<p>Is there a possibility to retrieve word forms as raw data? To use in mobile application for example. These forms are present on the general wiki page. For example <a href=""http://ru.wiktionary.org/wiki/be#.D0.9C.D0.BE.D1.80.D1.84.D0.BE.D0.BB.D0.BE.D0.B3.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D0.B8_.D1.81.D0.B8.D0.BD.D1.82.D0.B0.D0.BA.D1.81.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D1.81.D0.B2.D0.BE.D0.B9.D1.81.D1.82.D0.B2.D0.B0_2"" rel=""nofollow"">Forms of verb 'to be'</a>. The same you can find for nouns <a href=""http://ru.wiktionary.org/wiki/%D1%8F%D0%B1%D0%BB%D0%BE%D0%BA%D0%BE#.D0.9C.D0.BE.D1.80.D1.84.D0.BE.D0.BB.D0.BE.D0.B3.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D0.B8_.D1.81.D0.B8.D0.BD.D1.82.D0.B0.D0.BA.D1.81.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D1.81.D0.B2.D0.BE.D0.B9.D1.81.D1.82.D0.B2.D0.B0"" rel=""nofollow"">Noun forms for 'apple' in Russian</a>. </p>

<p>I need these forms with description of the form. What I mean is for example:
to be - infinitive; am - first person singular, present time; are - first person plural, present time; etc.</p>

<p>So far I have found that only wiktionary.org provides such information for Russian language. It would be nice if someone could point me to some other services/dictionaries for Russian, Ukrainian and English. </p>
",Multilingual Language Processing & Language Identification,wiktionary api retrieve word form free service question particularly russian ukrainian language may useful language possibility retrieve word form raw data use mobile application example form present general wiki page example form verb find noun noun form apple russian need form description form mean example infinitive first person singular present time first person plural present time etc far found wiktionary org provides information russian language would nice someone could point service dictionary russian ukrainian english
Is there an existing library or api I can use to separate words in character based languages?,"<p>I'm working on a little hobby Python project that involves creating dictionaries for various languages using large bodies of text written in that language. For most languages this is relatively straightforward because I can use the space delimiter between words to tokenize a paragraph into words for the dictionary, but for example, Chinese does not use a space character between words. How can I tokenize a paragraph of Chinese text into words?</p>

<p>My searching has found that this is a somewhat complex problem, so I'm wondering if there are off the shelf solutions to solve this in Python or elsewhere via an api or any other language. This must be a common problem because any search engine made for asian languages would need to overcome this issue in order to provide relevant results.</p>

<p>I tried to search around using Google, but I'm not even sure what this type of tokenizing is called, so my results aren't finding anything. Maybe just a nudge in the right direction would help.</p>
",Multilingual Language Processing & Language Identification,existing library api use separate word character based language working little hobby python project involves creating dictionary various language using large body text written language language relatively straightforward use space delimiter word tokenize paragraph word dictionary example chinese doe use space character word tokenize paragraph chinese text word searching ha found somewhat complex problem wondering shelf solution solve python elsewhere via api language must common problem search engine made asian language would need overcome issue order provide relevant result tried search around using google even sure type tokenizing called result finding anything maybe nudge right direction would help
Generate parse tree from parse description,"<p>I want to generate a parse tree(Java Object) from a parse description(Condensed Form of a syntactic parse) of an English sentence. I am using Java for the same and need to define an efficient tree too. Eg. of description :</p>

<pre><code>    (ROOT (S (NP (PRP I)) (VP (MD would) (VP (VB love) (S (VP (TO to) (VP (VB go) (PRT (RP out)) (PP (IN with) (NP (PRP you)))))))) (. .))
</code></pre>
",Multilingual Language Processing & Language Identification,generate parse tree parse description want generate parse tree java object parse description condensed form syntactic parse english sentence using java need define efficient tree eg description
What methods are used for recognizing language a text is written in?,"<p>If I have a given text (both long or short), with which methods do you usually detect which language it is written in?</p>

<p>It is clear that:</p>

<ul>
<li>You need a training corpus to train the models you use (e.g. neural networks, if used)</li>
</ul>

<p>Easiest thing coming to my mind is:</p>

<ul>
<li>Check characters used in the text (e.g. hiragana are only used in Japanese, Umlauts probably only in European languages, √ß in French, Turkish, ‚Ä¶)</li>
<li>Increase the check to two or three letter pairs to find specific combinations of a language</li>
<li>Lookup a dictionary to check which words occur in which language (probably only without stemming, as stemming depends on the language)</li>
</ul>

<p>But I guess there are better ways to go. I am not searching for existing projects (those questions have already been answered), but for methods like Hidden-Markov-Models, Neural Networks, ‚Ä¶ whatever may be used for this task.</p>
",Multilingual Language Processing & Language Identification,method used recognizing language text written given text long short method usually detect language written clear need training corpus train model use e g neural network used easiest thing coming mind check character used text e g hiragana used japanese umlaut probably european language french turkish increase check two three letter pair find specific combination language lookup dictionary check word occur language probably without stemming stemming depends language guess better way go searching existing project question already answered method like hidden markov model neural network whatever may used task
Using query expressions for Unicode strings with LINQ,"<p>I am working with LINQ and and I have a database with columns for storing local content(non-english characters). Now I want to make a query using linq as follows</p>

<pre><code>var desc = from p in db.GetDesc                            
           where  p.Category.Contains(""xxxx"".ToString())
           orderby p.Date descending
           select p;
</code></pre>

<p>Here the Category column contains unicode strings and the above query string doesn't work. How can I use natural language queries with LINQ?</p>
",Multilingual Language Processing & Language Identification,using query expression unicode string linq working linq database column storing local content non english character want make query using linq follows category column contains unicode string query string work use natural language query linq
Convert Chinese Pinyin with accents to numerical form,"<p>I'm looking to convert Pinyin where the tone marks are written with accents (e.g.: N√≠n h«éo) to Pinyin written in numerical/ASCII form (e.g.: Nin2 hao1).</p>

<p>Does anyone know of any libraries for this, preferably PHP? Or know Chinese/Pinyin well enough to comment?</p>

<p>I started writing one myself that was rather simple, but I don't speak Chinese and don't fully understand the rules of when words should be split up with a space.</p>

<p>I was able to write a translator that converts:</p>

<p><code>N√≠n h«éo. W«í sh√¨ zh≈çng gu√≥ r√©n</code> ==> <code>Nin2 hao3. Wo3 shi4 zhong1 guo2 ren2</code></p>

<p>But how do you handle words like the following - do they get split up with a space into multiple words, or do you interject the tone numbers within the word (if so, where?) :
<code>huƒÅ sh√≠jiƒÅn</code>, <code>w√®ish√©nme</code>, <code>yu√®l√°iyu√®</code>, <code>shƒìngb√¨ng</code>, etc.</p>
",Multilingual Language Processing & Language Identification,convert chinese pinyin accent numerical form looking convert pinyin tone mark written accent e g n n h pinyin written numerical ascii form e g nin hao doe anyone know library preferably php know chinese pinyin well enough comment started writing one wa rather simple speak chinese fully understand rule word split space wa able write translator convert handle word like following get split space multiple word interject tone number within word etc
English to babel-lang,"<p>Recently I was checking up on some AI possibilities and found out about <a href=""http://babelproject.com/"" rel=""nofollow"">babel-lang</a>, which seems to be an interesting concept.</p>

<p>The project appears to be abandoned but the concept is quite interesting.
It offers a babel > english translator and a good spec to do so, but no way to do the oposite.</p>

<p>I was wondering how one could go about making an english > babel translator.</p>

<p>From what I can think of, the only way is to have a large set of verbs, nouns, adjectives, etc which can then be used translate the sentence into babel.</p>

<p>Is this really the only (or best) way?</p>
",Multilingual Language Processing & Language Identification,english babel lang recently wa checking ai possibility found babel lang seems interesting concept project appears abandoned concept quite interesting offer babel english translator good spec way oposite wa wondering one could go making english babel translator think way large set verb noun adjective etc used translate sentence babel really best way
Algorithm to determine if a word could be English?,"<p>I have a list of strings that I need to check against an English dictionary.
However I don't want to start checking every piece of gibberish in the list. First, I want to check if the string could be an English word.</p>

<p>Does anyone know of an algorithm that does this or at least the rules that I need to apply to verify a word?</p>

<p>For example:</p>

<p>No spoken word can start with more than 3 consonants, and if there are are 3 initial consonants in a word, the first one must be ""s"".</p>
",Multilingual Language Processing & Language Identification,algorithm determine word could english list string need check english dictionary however want start checking every piece gibberish list first want check string could english word doe anyone know algorithm doe least rule need apply verify word example spoken word start consonant initial consonant word first one must
Generate a list of English words containing consecutive consonant sounds,"<p>Start with this:</p>

<pre><code>[G|C] * [T] *
</code></pre>

<p>Write a program that generates this:</p>

<pre><code>Cat
Cut
Cute
City &lt;-- NOTE: this one is wrong, because City has an ""ESS"" sound at the start.
Caught
...
Gate
Gotti
Gut
...
Kit
Kite
Kate
Kata
Katie
</code></pre>

<p>Another Example, This:</p>

<p>[C] * [T] * [N]</p>

<p>Should produce this:</p>

<p>Cotton
   Kitten </p>

<p>Where should I start my research as I figure out how to write a program/script that does this?</p>
",Multilingual Language Processing & Language Identification,generate list english word containing consecutive consonant sound start write program generates another example c n produce cotton kitten start research figure write program script doe
Canadian to US English,"<p>Does there exist something like Canadian to US English e-dictionary which I can use in my application?</p>
",Multilingual Language Processing & Language Identification,canadian u english doe exist something like canadian u english e dictionary use application
Algorithm for Natural-Looking Sentence in English Language,"<p>I'm building an application that does sentence checking. Do you know are there any DLLs out there that recognize sentences and their logic and organize sentences correctly? Like put words in a sentence into a correct sentence.</p>

<p>If it's not available, maybe you can suggest search terms that I can research.</p>
",Multilingual Language Processing & Language Identification,algorithm natural looking sentence english language building application doe sentence checking know dlls recognize sentence logic organize sentence correctly like put word sentence correct sentence available maybe suggest search term research
mapping English words to their singular forms,"<p>Examples:
mapping a plural noun to its singular form:  children --> child, boxes ---> box
mapping comparative and superlative of adjectives and adverbs to their basic form
mapping verb .... </p>

<p>I once found an open source resource to achieve it. As I remember, it is a datasheet?/spreadsheet?/database? of the mapping. Does anyone know the open source resource, or any alternative/better solution?
I'm coding in java.</p>
",Multilingual Language Processing & Language Identification,mapping english word singular form example mapping plural noun singular form child child box box mapping comparative superlative adjective adverb basic form mapping verb found open source resource achieve remember datasheet spreadsheet database mapping doe anyone know open source resource alternative better solution coding java
Natural language generator for dates (Java),"<p>I'm building a system that needs to provide a commentary on things in natural English. One thing that is of use is to be able to express dates in a casual format. What I'm looking for is essentially the inverse of <a href=""https://github.com/samtingleff/jchronic"" rel=""nofollow noreferrer"">Chronic</a>, <a href=""http://natty.joestelmach.com/"" rel=""nofollow noreferrer"">Natty</a>, or the task described in this question: <a href=""https://stackoverflow.com/questions/1410408/natural-language-date-and-time-parser-for-java"">Natural Language date and time parser for java</a>.</p>

<p>Is this too out-there to have been done? Should I try and roll my own simple hardwired piece for the date ranges that make sense to me? Or is there some clever way to reverse existing parsers to spit out (even garbled) sentences describing dates?</p>

<p>EDIT - To clarify, although <em>any</em> kind of output is interesting and useful, I'm particularly interested in varied/creative output generation. i.e. ""Next week"", ""seven days from now"", ""next Thursday"" and ""late next week"" all for the same date.</p>
",Multilingual Language Processing & Language Identification,natural language generator date java building system need provide commentary thing natural english one thing use able express date casual format looking essentially inverse chronic natty task described question
Natural Language Processing Toolkit for .NET,"<p>Can you give me some toolkits and libraries for natural language processing in .NET.</p>

<p>Are there tools like UIMA for .NET?</p>
",Multilingual Language Processing & Language Identification,natural language processing toolkit net give toolkits library natural language processing net tool like uima net
Where to find wordlists with gender and plural for German?,"<p>I'm trying to write a simple text mining application to try to tell a German word's gender and plural form.</p>

<p>So, first of all, I need a big wordlist for training. I've searched around but could not find any list having either gender nor plural.</p>
",Multilingual Language Processing & Language Identification,find wordlists gender plural german trying write simple text mining application try tell german word gender plural form first need big wordlist training searched around could find list either gender plural
CJK Languages Pronunciation APIs,"<p>Are there any good (preferably open) APIs or databases of pronunciation audio files for Chinese/Japanese/Korean languages? I‚Äôve been looking around, but somehow couldn‚Äôt find anything other than <a href=""http://api.forvo.com/"" rel=""nofollow"">Forvo</a> or Google Translate. Both are an overkill for me, since I only need data for those languages, and only pronunciations, no translations.</p>
",Multilingual Language Processing & Language Identification,cjk language pronunciation apis good preferably open apis database pronunciation audio file chinese japanese korean language looking around somehow find anything forvo google translate overkill since need data language pronunciation translation
Speech Recognition: detecting Japanese Kana (consonant + vowel),"<p>I would like to find some open source code (although I would settle for a closed source product)  to convert an incoming audio stream of Japanese Kana (ie consonant+vowel pairs) and print them out pretty much in real-time.</p>

<p>However, I want to use these basic sound units for my own custom purpose, so I don't want any high-level processing that tries to extract genuine Japanese words. I just want to get the raw Kana.</p>

<p>Is anyone aware of such a technology?</p>

<p>I just learned today that the Japanese ' alphabet ' is basically a 10x5 grid of <a href=""http://en.wikipedia.org/wiki/Kana"" rel=""nofollow"">Kana</a>.  10 columns ( empty + 9 consonants ) and 5 rows ( vowels )</p>

<p>and each element is called a 'Kana', and the language consists of sequences of these Kana; these are the basic building blocks.</p>

<p>This must surely have a large impact on speech recognition algorithms.</p>

<p>For Western languages, all commercial speech recognition engines I am aware of derive from <a href=""http://cmusphinx.sourceforge.net/"" rel=""nofollow"">CMUSphinx</a> which operates on a tri-gram model:  it represents each movement between three phonemes with a unique MFCC vector and figures out the most likely tri-gram sequence(s) for an utterance (from which it can deduce trivially the phonemes, and then run through its dictionary of WORD-triplets, to figure out the most likely sentence).</p>

<p>But for a language such as Japanese, I would guess that this may no longer be the most efficient algorithm.</p>

<p>Instead, it may make sense to try and catch each individual Kana,  or Kana-pair.</p>

<p>...which is going to be 2-gram or 4-gram. but not 3!</p>

<p>Is there anything out there? Or do they just use the same engines the Western world does?</p>
",Multilingual Language Processing & Language Identification,speech recognition detecting japanese kana consonant vowel would like find open source code although would settle closed source product convert incoming audio stream japanese kana ie consonant vowel pair print pretty much real time however want use basic sound unit custom purpose want high level processing try extract genuine japanese word want get raw kana anyone aware technology learned today japanese alphabet basically x grid kana column empty consonant row vowel element called kana language consists sequence kana basic building block must surely large impact speech recognition algorithm western language commercial speech recognition engine aware derive cmusphinx operates tri gram model represents movement three phoneme unique mfcc vector figure likely tri gram sequence utterance deduce trivially phoneme run dictionary word triplet figure likely sentence language japanese would guess may longer efficient algorithm instead may make sense try catch individual kana kana pair going gram gram anything use engine western world doe
Language converter in C++ (from japanese to english),"<p>Once again i need ur help, i have a file in japanese language and i want to convert that file into english using C++, since i dont think that i can use any API's of google in c++, so any general idea can prove helpful for me, Please suggest something.</p>

<p>Thanks a lot</p>

<p>Owais Masood</p>
",Multilingual Language Processing & Language Identification,language converter c japanese english need ur help file japanese language want convert file english using c since dont think use api google c general idea prove helpful please suggest something thanks lot owais masood
Compose synthetic English phrase that would contain 160 bits of recoverable information,"<p>I have 160 bits of random data.</p>

<p>Just for fun, I want to generate pseudo-English phrase to ""store"" this information in. I want to be able to recover this information from the phrase. </p>

<p><strong><em>Note:</strong> This is not a security question, I don't care if someone else will be able to recover the information or even detect that it is there or not.</em></p>

<p>Criteria for better phrases, from most important to the least: </p>

<ul>
<li>Short</li>
<li>Unique</li>
<li>Natural-looking</li>
</ul>

<p>The current approach, suggested <a href=""https://stackoverflow.com/questions/4683551/generating-a-pseudo-natural-phrase-from-a-big-integer-in-a-reversible-way/4684842#4684842"">here</a>:</p>

<p>Take three lists of 1024 nouns, verbs and adjectives each (picking most popular ones). Generate a phrase by the following pattern, reading 20 bits for each word:</p>

<pre>
Noun verb adjective verb,
Noun verb adjective verb,
Noun verb adjective verb,
Noun verb adjective verb.
</pre>

<p>Now, this seems to be a good approach, but the phrase is a bit too long and a bit too dull.</p>

<p>I have found a corpus of words <a href=""http://wordlist.sourceforge.net/"" rel=""nofollow noreferrer"">here</a> (Part of Speech Database).</p>

<p>After some ad-hoc filtering, I calculated that this corpus contains, approximately</p>

<ul>
<li>50690 usable adjectives</li>
<li>123585 nouns</li>
<li>15301 verbs</li>
<li>13010 adverbs (not included in pattern, but mentioned in answers)</li>
</ul>

<p>This allows me to use up to</p>

<ul>
<li>16 bits per adjective (actually 16.9, but I can't figure how to use fractional bits)</li>
<li>15 bits per noun</li>
<li>13 bits per verb</li>
<li>13 bits per adverb</li>
</ul>

<p>For noun-verb-adjective-verb pattern this gives 57 bits per ""sentence"" in phrase. This means that, if I'll use all words I can get from this corpus, I can generate three sentences instead of four (160 / 57 ‚âà 2.8).</p>

<pre>
Noun verb adjective verb,
Noun verb adjective verb,
Noun verb adjective verb.
</pre>

<p>Still a bit too long and dull.</p>

<p>Any hints how can I improve it?</p>

<p>What I see that I can try:</p>

<ul>
<li><p>Try to compress my data somehow before encoding. But since the data is completely random, only some phrases would be shorter (and, I guess, not by much).</p></li>
<li><p>Improve phrase pattern, so it would look better.</p></li>
<li><p>Use several patterns, using the first word in phrase to somehow indicate for future decoding which pattern was used. (For example, use the last letter or even the length of the word.) Pick pattern according to the first bytes of the data.</p></li>
</ul>

<p>...I'm not that good with English to come up with better phrase patterns. Any suggestions?</p>

<ul>
<li>Use more linguistics in the pattern. Different tenses etc. </li>
</ul>

<p>...I guess, I would need much better word corpus than I have now for that. Any hints where can I get a suitable one?</p>
",Multilingual Language Processing & Language Identification,compose synthetic english phrase would contain bit recoverable information bit random data fun want generate pseudo english phrase store information want able recover information phrase note security question care someone else able recover information even detect criterion better phrase important least short unique natural looking current approach suggested part speech database ad hoc filtering calculated corpus contains approximately usable adjective noun verb adverb included pattern mentioned answer allows use bit per adjective actually figure use fractional bit bit per noun bit per verb bit per adverb noun verb adjective verb pattern give bit per sentence phrase mean use word get corpus generate three sentence instead four noun verb adjective verb noun verb adjective verb noun verb adjective verb still bit long dull hint improve see try try compress data somehow encoding since data completely random phrase would shorter guess much improve phrase pattern would look better use several pattern using first word phrase somehow indicate future decoding pattern wa used example use last letter even length word pick pattern according first byte data good english come better phrase pattern suggestion use linguistics pattern different tense etc guess would need much better word corpus hint get suitable one
Generate a pseudo-poem that would contain 160 bits of recoverable information,"<p>I have 160 bits of random data.</p>
<p>Just for fun, I want to generate an English pseudo-poem to &quot;store&quot; this information in. I want to be able to recover this information from the poem. (&quot;Poem&quot; here is a vague term for any kind of poetry.)</p>
<p><em><strong>Note:</strong> This is not a security question, I don't care if someone else will be able to recover the information or even detect that it is there or not.</em></p>
<p>Criteria for a better poem:</p>
<ul>
<li>Better aestetics</li>
<li>Better rhyme and foot</li>
<li>Uniqueness</li>
<li>Shorter length</li>
</ul>
<p>I'd say that the acceptable poem is no longer than three stanzas of four lines each. (But the other, established forms of poetry, like sonnets are good as well.)</p>
<p>I like this idea, but, I'm afraid, that I'm completely clueless in how to do English computer-generated poetry. (I programmed that for Russian when I was young, but looks like that experience will not help me here.)</p>
<p>So, any clues?</p>
<p><em><strong>Note:</strong> I already <a href=""https://stackoverflow.com/q/4698229/6236"">asked a similar question</a>. I want to try both approaches. Note how good poem criteria are different from the good phrase in parallel question. Remember, this is &quot;just for fun&quot;.</em></p>
<p><em>Also, I have to note this: There is an <a href=""https://www.rfc-editor.org/rfc/rfc1605"" rel=""nofollow noreferrer"">RFC 1605</a> on somewhat related matters. But it do not suggest any implementation details, so it is not quite useful for me, sorry. &lt;g&gt;</em></p>
",Multilingual Language Processing & Language Identification,generate pseudo poem would contain bit recoverable information bit random data fun want generate english pseudo poem store information want able recover information poem poem vague term kind poetry note security question care someone else able recover information even detect criterion better poem better aestetics better rhyme foot uniqueness shorter length say acceptable poem longer three stanza four line established form poetry like sonnet good well like idea afraid completely clueless english computer generated poetry programmed russian wa young look like experience help clue note already rfc somewhat related matter suggest implementation detail quite useful sorry g
Natural language grammar and user-entered names,"<p>Some languages, particularly Slavic languages, change the endings of people's names according to the grammatical context. (For those of you who know grammar or studied languages that do this to words, such as German or Russian, and to help with search keywords, I'm talking about noun declension.)</p>

<p>This is probably easiest with a set of examples (in Polish, to save the whole different-alphabet problem):</p>

<ol>
<li>Dorothy saw the cat ‚Äî <em>Dorota zobaczy≈Ça kota</em></li>
<li>The cat saw Dorothy ‚Äî <em>Kot zobaczy≈Ç Dorotƒô</em></li>
<li>It is Dorothy‚Äôs cat ‚Äî <em>To jest kot Doroty</em></li>
<li>I gave the cat to Dorothy ‚Äî <em>Da≈Çam kota Dorotie</em></li>
<li>I went for a walk with Dorothy ‚Äî <em>Posz≈Çam na spacer z DorotƒÖ</em></li>
<li>‚ÄúHello, Dorothy!‚Äù ‚Äî <em>‚ÄúWitam, Doroto!‚Äù</em></li>
</ol>

<p>Now, if, in these examples, the name here were to be user-entered, that introduces a world of grammar nightmares. Importantly, if I went for Katie (<em>Kasia</em>), the <a href=""http://en.wikibooks.org/wiki/Polish/Feminine_noun_declension"" rel=""nofollow noreferrer"">examples are not directly comparable</a> ‚Äî 3 and 4 are both <em>Kasi</em>, rather than <em>*Kasy</em> and <em>*Kasie</em> ‚Äî and male names will be <a href=""http://en.wikibooks.org/wiki/Polish/Masculine_noun_declension"" rel=""nofollow noreferrer"">wholly different again</a>.</p>

<p>I'm guessing someone has dealt with this situation before, but my Google-fu appears to be weak today. I can find a lot of links about natural-language processing, but I don'think that's quite what I want. To be clear: I'm only ever gonna have one user-entered name per user and I'm gonna need to decline them into known configurations ‚Äî I'll have a localised text that will have placeholders something like <code>{name¬†nominative}</code> and <code>{name¬†dative}</code>, for the sake of argument. I really don't want to have to do lexical analysis of text to work stuff out, I'll only ever need to decline that one user-entered name.</p>

<p>Anyone have any recommendations on how to do this, or do I need to start calling round localisation agencies  ;o)</p>

<hr/>

<p>Further reading (all on Wikipedia) for the interested:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Declension"" rel=""nofollow noreferrer"">Declension</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Grammatical_case"" rel=""nofollow noreferrer"">Grammatical case</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Polish_language#Nouns_and_adjectives"" rel=""nofollow noreferrer"">Declension in Polish</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Russian_grammar#Nouns"" rel=""nofollow noreferrer"">Declension in Russian</a></li>
<li>Declension in Czech <a href=""http://en.wikipedia.org/wiki/Czech_declension#Nouns"" rel=""nofollow noreferrer"">nouns</a> and <a href=""http://en.wikipedia.org/wiki/Czech_declension#Pronouns"" rel=""nofollow noreferrer"">pronouns</a></li>
</ul>

<p>Disclaimer: I know this happens in many other languages; highlighting Slavic languages is merely because I have a project that is going to be localised into some Slavic languages.</p>
",Multilingual Language Processing & Language Identification,natural language grammar user entered name language particularly slavic language change ending people name according grammatical context know grammar studied language word german russian help search keywords talking noun declension probably easiest set example polish save whole different alphabet problem dorothy saw cat dorota zobaczy kota cat saw dorothy kot zobaczy dorot dorothy cat jest kot doroty gave cat dorothy da kota dorotie went walk dorothy posz na spacer z dorot hello dorothy witam doroto example name user entered introduces world grammar nightmare importantly went katie kasia example directly comparable kasi rather kasy kasie male name wholly different guessing someone ha dealt situation google fu appears weak today find lot link natural language processing think quite want clear ever gon na one user entered name per user gon na need decline known configuration localised text placeholder something like sake argument really want lexical analysis text work stuff ever need decline one user entered name anyone recommendation need start calling round localisation agency reading wikipedia interested declension grammatical case declension polish declension russian declension czech noun pronoun disclaimer know happens many language highlighting slavic language merely project going localised slavic language
apache openNLP chuker/POS noun detection,"<p>I am experimenting apache Open NLP for one of my project, my requirement is to detect nouns out of email contents and check with our customer data base (this DB consist of individual names, organization names etc and my search engine is Solr base).</p>

<p>For normal english nouns, default trained model works properly (for most of the cases), but 
One of the tricky requirement is, we have business organization with abbreviations like OK, LET etc and thus in few scenarios I need to consider OK, LET etc as noun.</p>

<p>As an example
1) ""sending some items to LET, please expect delay in payment""
2) ""let us go for a party""</p>

<p>In #1 I want to consider LET as noun and in #2 case LET is not noun.</p>

<p>If I can achieve this requirement, I can reduce significant amount of false positive matches in my search engine.</p>

<p>Any help is highly appreciated.</p>
",Multilingual Language Processing & Language Identification,apache opennlp chuker po noun detection experimenting apache open nlp one project requirement detect noun email content check customer data base db consist individual name organization name etc search engine solr base normal english noun default trained model work properly case one tricky requirement business organization abbreviation like ok let etc thus scenario need consider ok let etc noun example sending item let please expect delay payment let u go party want consider let noun case let noun achieve requirement reduce significant amount false positive match search engine help highly appreciated
English word database with plurals/conjugations,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/8424806/verb-conjugations-database"">Verb Conjugations Database</a>  </p>
</blockquote>



<p>I'm looking for an English word database in MySQL, or easily convertible to MySQL, that contains verb conjugations and plural/singular forms. I've looked at a couple of options: WordNet, GCIDE, etc.</p>

<p>However GCIDE does not seem to be comprehensive and WordNet does not seem to label conjugations by tense (correct me if I'm wrong).</p>

<p>My problem is similar to this one:
<a href=""https://stackoverflow.com/questions/8424806/verb-conjugations-database/10192400"">Verb Conjugations Database</a></p>

<p>But it seems like no satisfactory, free solution was shared.</p>
",Multilingual Language Processing & Language Identification,english word database plural conjugation possible duplicate seems like satisfactory free solution wa shared
Splitting string containing letters and numbers not separated by any particular delimiter in PHP,"<p>Currently I am developing a web application to fetch Twitter stream and trying to create a natural language processing by my own. </p>

<p>Since my data is from Twitter (limited by 140 characters) there are many words shortened, or on this case, <strong>omitted space</strong>.</p>

<p>For example:</p>

<pre><code>""Hi, my name is Bob. I m 19yo and 170cm tall""
</code></pre>

<p>Should be tokenized to:</p>

<pre><code>- hi
- my
- name
- bob
- i
- 19
- yo
- 170
- cm
- tall
</code></pre>

<p>Notice that <code>19</code> and <code>yo</code> in <code>19yo</code> have <strong>no space</strong> between them. I use it mostly for extracting numbers with their units. </p>

<p>Simply, what I need is a way to 'explode' each tokens that has number in it by chunk of numbers or letters <strong>without</strong> delimiter. </p>

<p><code>'123abc'</code> will be <code>['123', 'abc']</code></p>

<p><code>'abc123'</code> will be <code>['abc', '123']</code></p>

<p><code>'abc123xyz'</code> will be <code>['abc', '123', 'xyz']</code></p>

<p>and so on.</p>

<p>What is the best way to achieve it in PHP?</p>

<hr>

<p>I found something close to it, but it's C# and spesifically for day/month splitting. <a href=""https://stackoverflow.com/q/2362153/670623"">How do I split a string in C# based on letters and numbers</a></p>
",Multilingual Language Processing & Language Identification,splitting string containing letter number separated particular delimiter php currently developing web application fetch twitter stream trying create natural language processing since data twitter limited character many word shortened case omitted space example tokenized notice space use mostly extracting number unit simply need way explode token ha number chunk number letter without delimiter best way achieve php found something close c spesifically day month splitting href split string c based letter number
"Parsing words into (prefix, root, suffix) in Python","<p>I'm trying to create a simple parser for some text data. (The text is in a language that NLTK doesn't have any parsers for.) </p>

<p>Basically, I have a limited number of prefixes, which can be either one or two letters; a word can have more than one prefix. I also have a limited number of suffixes of one or two letters. Whatever's in between them should be the ""root"" of the word. Many words will have more the one possible parsing, so I want to input a word and get back a list of possible parses in the form of a tuple (prefix,root,suffix).</p>

<p>I can't figure out how to structure the code though. I pasted an example of one way I tried (using some dummy English data to make it more understandable), but it's clearly not right. For one thing it's really ugly and redundant, so I'm sure there's a better way to do it. For another, it doesn't work with words that have more than one prefix or suffix, or both prefix(es) and suffix(es).</p>

<p>Any thoughts?</p>

<pre><code>prefixes = ['de','con']
suffixes = ['er','s']

def parser(word):
    poss_parses = []
    if word[0:2] in prefixes:
        poss_parses.append((word[0:2],word[2:],''))
    if word[0:3] in prefixes:
        poss_parses.append((word[0:3],word[3:],''))
    if word[-2:-1] in prefixes:
        poss_parses.append(('',word[:-2],word[-2:-1]))
    if word[-3:-1] in prefixes:
        poss_parses.append(('',word[:-3],word[-3:-1]))
    if word[0:2] in prefixes and word[-2:-1] in suffixes and len(word[2:-2])&gt;2:
        poss_parses.append((word[0:2],word[2:-2],word[-2:-1]))
    if word[0:2] in prefixes and word[-3:-1] in suffixes and len(word[2:-3])&gt;2:
        poss_parses.append((word[0:2],word[2:-2],word[-3:-1]))
    if word[0:3] in prefixes and word[-2:-1] in suffixes and len(word[3:-2])&gt;2:
        poss_parses.append((word[0:2],word[2:-2],word[-2:-1]))
    if word[0:3] in prefixes and word[-3:-1] in suffixes and len(word[3:-3])&gt;2:
        poss_parses.append((word[0:3],word[3:-2],word[-3:-1]))
    return poss_parses



&gt;&gt;&gt; wordlist = ['construct','destructer','constructs','deconstructs']
&gt;&gt;&gt; for w in wordlist:
...   parses = parser(w)
...   print w
...   for p in parses:
...     print p
... 
construct
('con', 'struct', '')
destructer
('de', 'structer', '')
constructs
('con', 'structs', '')
deconstructs
('de', 'constructs', '')
</code></pre>
",Multilingual Language Processing & Language Identification,parsing word prefix root suffix python trying create simple parser text data text language nltk parser basically limited number prefix either one two letter word one prefix also limited number suffix one two letter whatever root word many word one possible parsing want input word get back list possible par form tuple prefix root suffix figure structure code though pasted example one way tried using dummy english data make understandable clearly right one thing really ugly redundant sure better way another work word one prefix suffix prefix e suffix e thought
Python natural language processing for named entities,"<p>I am writing a python web application for which I need to process search queries having named entities in it. For example,
If search query is:
""mac os lion""
And lets say I have to process this query with the candidates available on my database:</p>

<ul>
<li>Google Android.</li>
<li>Microsoft Windows.</li>
<li>Apple Mac OS X Lion</li>
<li>...</li>
</ul>

<p>We all know that 3rd result is the right result. But is there any way we could map user's query i.e. ""mac os lion"" to ""Apple Mac OS X Lion"" (which is the available entry on my database)
Can someone please tell me what to look for or what to do.</p>
",Multilingual Language Processing & Language Identification,python natural language processing named entity writing python web application need process search query named entity example search query mac lion let say process query candidate available database google android microsoft window apple mac x lion know rd result right result way could map user query e mac lion apple mac x lion available entry database someone please tell look
"n-gram name analysis in non-english languages (CJK, etc)","<p>I'm working on deduping a database of people. For a first pass, I'm following a basic 2-step process to avoid an O(n^2) operation over the whole database, as described <a href=""http://nike.psu.edu/publications/jcdl05.pdf"" rel=""nofollow noreferrer"">in the literature</a>. First, I ""block""- iterate over the whole dataset, and bin each record based on n-grams AND initials present in the name. Second, all the records per bin are compared using Jaro-Winkler to get a measure of the likelihood of their representing the same person.</p>

<p>My problem- the names are Unicode. Some (though not many) of these names are in CJK (Chinese-Japanese-Korean) languages. I have no idea how to <a href=""https://stackoverflow.com/questions/1738788/python-split-unicode-string-on-word-boundaries"">find word boundaries</a> for something like initials in these languages. I have no idea whether n-gram analysis is valid on names in languages where names can be 2 characters. I also don't know if string edit-distance or other similarity metrics are valid in this context.</p>

<p>Any ideas from linguist programmers or native speakers?</p>
",Multilingual Language Processing & Language Identification,n gram name analysis non english language cjk etc working deduping database people first pas following basic step process avoid n operation whole database described literature first block iterate whole dataset bin record based n gram initial present name second record per bin compared using jaro winkler get measure likelihood representing person problem name unicode though many name cjk chinese japanese korean language idea href word boundary something like initial language idea whether n gram analysis valid name language name character also know string edit distance similarity metric valid context idea linguist programmer native speaker
Filter words belonging to a broad category,"<p>I have a list of words (assume they are stored in String[] if you must). I want to filter out words that belong to a broad general category such as <em>Music</em> or <em>Sports</em>.</p>

<p>Is there a ready-made solution for this (even if it's only for a limited set of general categories)?</p>

<p>Or how would you go about doing this?</p>

<p>It is to be done in <em>Java 1.6</em> and it is an NLP (Natural Language Processing) problem. The input list of words has random words, and I want to extract from this large list, only the words that belong to a given general category (which will be a subset).</p>

<p><strong>Another way of thinking:</strong> Given a single word, I want to determine if this word belongs to a category. <em>Something like this:</em></p>

<pre><code>String word1 = ""football""; //the strings will always be single word units
String word2 = ""telephone"";
boolean b1 = belongsToCategory(Categories.SPORTS, word1); //true
boolean b2 = belongsToCategory(Categories.SPORTS, word2); //false
</code></pre>

<p>If you need more info, please ask.</p>
",Multilingual Language Processing & Language Identification,filter word belonging broad category list word assume stored string must want filter word belong broad general category music sport ready made solution even limited set general category would go done java nlp natural language processing problem input list word ha random word want extract large list word belong given general category subset another way thinking given single word want determine word belongs category something like need info please ask
Performing Stemming outputs jibberish/concatenated words,"<p>I am experimenting with the python library NLTK for Natural Language Processing.</p>

<p><strong>My Problem:</strong> I'm trying to perform stemming; reduce words to their normalised form. But its not producing correct words. Am I using the stemming class correctly? And how can I get the results I am attempting to get?</p>

<p>I want to normalise the following words:</p>

<pre><code>words = [""forgot"",""forgotten"",""there's"",""myself"",""remuneration""]
</code></pre>

<p>...into this:</p>

<pre><code>words = [""forgot"",""forgot"",""there"",""myself"",""remunerate""]
</code></pre>

<p>My code:</p>

<pre><code>from nltk import stem
words = [""forgot"",""forgotten"",""there's"",""myself"",""remuneration""]
for word in words:
    print stemmer.stem(word)

#output is:
#forgot forgotten there' myself remuner
</code></pre>
",Multilingual Language Processing & Language Identification,performing stemming output jibberish concatenated word experimenting python library nltk natural language processing problem trying perform stemming reduce word normalised form producing correct word using stemming class correctly get result attempting get want normalise following word code
Building a lemmatizer: speed optimization,"<p>I am building a lemmatizer in python. As I need it to run in realtime/process fairly large amount of data the processing speed
is of the essence. 
Data: I have all possible suffixes that are linked to all wordtypes that they can be combined with. Additionally I have lemmaforms that are linked to both their wordtype(s) and lemma(s). The program takes a word as input and outputs its lemma.
word = lemmafrom + suffix</p>

<p>For example (Note: although the example is given in English I am not building a lemmatizer for English):</p>

<p>word:  forbidding</p>

<p>lemmaform: forbidd</p>

<p>suffix: ing</p>

<p>lemma: forbid</p>

<p>My solution:</p>

<p>I have converted the data to (nested) dicts:</p>

<pre><code>suffixdict : {suffix1:[type1,type2, ... , type(n)], suffix2:[type1,type2, ... ,
type(n)]}    
lemmaformdict : {lemmaform:{type1:lemma}}
</code></pre>

<p>1) Find all possible suffixes and word types that they are linked to.
If the longest possible suffix is 3 characters long, the program tries to match 'ing', 'ng', 'n' to the keys in 
suffixdict. If the key exists it returns a value (a set of wordtypes).</p>

<p>2) For each matching suffix search the lemmaform from the dict.
If lemmaform exists it returns the wordtypes.</p>

<p>3) Finally, the program tries to intersect the wordtypes produced in steps 1) ans 2) and if the intersection is
sucessful it returns the lemma of the word.</p>

<p>My question: could there be a better solution to my problem from the prespective of speed? (Disregarding the option to keep frequent words and lemmas in the dictionary) 
Help much appriciated.</p>
",Multilingual Language Processing & Language Identification,building lemmatizer speed optimization building lemmatizer python need run realtime process fairly large amount data processing speed essence data possible suffix linked wordtypes combined additionally lemmaforms linked wordtype lemma program take word input output lemma word lemmafrom suffix example note although example given english building lemmatizer english word forbidding lemmaform forbidd suffix ing lemma forbid solution converted data nested dicts find possible suffix word type linked longest possible suffix character long program try match ing ng n key suffixdict key exists return value set wordtypes matching suffix search lemmaform dict lemmaform exists return wordtypes finally program try intersect wordtypes produced step intersection sucessful return lemma word question could better solution problem prespective speed disregarding option keep frequent word lemma dictionary help much appriciated
Word Translation by letter/vowels,"<p>I'm looking to translate words by letters/vowels.
I'll try to explain.
I have a an Arabic text with ~300,000 words, my goal is to enable users to search the text using one of 10 languages I'll define. So if some search for Stack overflow in English I'll need to break down to words as S-TA-CK O-VE-R-F-LOW (I need to break it that way to get the Arabic equivalent letters).
Is there something like that already exsiting, or I just need to start from scratch and do a linguistic research???
Thank you for your time.</p>
",Multilingual Language Processing & Language Identification,word translation letter vowel looking translate word letter vowel try explain arabic text word goal enable user search text using one language define search stack overflow english need break word ta ck r f low need break way get arabic equivalent letter something like already exsiting need start scratch linguistic research thank time
Method/Tool for Extracting Keywords from List of Sentences,"<p>I have a large list of sentences and would like to tag each of them with their own unique keywords, to help me identify which sentences are similar for grouping purposes.  </p>

<p>As an example:</p>

<pre>
The dog ran fast. - tagged as: dog
The cat is sleeping - tagged as: cat
The German Sheppard is awake. - tagged as dog
</pre>

<p>I've been looking into tools like alchemy api and openCalais for the keyword extraction, however, it seems you moreso use these to extract meaning out of a block of data, like an entire document or paragraph rather than tagging 1000s of unique but similar individual sentences.  </p>

<p>In short, ideally I'd like to:</p>

<ol>
<li>Take a sentence from a document or webpage (perhaps from a large spreadsheet or a list of tweets)</li>
<li>Place a unique identifier on it (some type of keyword)</li>
<li>Group the sentences together by keywrd</li>
</ol>
",Multilingual Language Processing & Language Identification,method tool extracting keywords list sentence large list sentence would like tag unique keywords help identify sentence similar grouping purpose example dog ran fast tagged dog cat sleeping tagged cat german sheppard awake tagged dog looking tool like alchemy api opencalais keyword extraction however seems moreso use extract meaning block data like entire document paragraph rather tagging unique similar individual sentence short ideally like take sentence document webpage perhaps large spreadsheet list tweet place unique identifier type keyword group sentence together keywrd
How to find out the entropy of the English language,"<p>How to find out the entropy of the English language by using isolated symbol probabilities of the language?</p>
",Multilingual Language Processing & Language Identification,find entropy english language find entropy english language using isolated symbol probability language
What is the meaning of &quot;isolated symbol probabilities of English&quot;,"<p>In a note I found this phrase: </p>

<blockquote>
  <p>Using isolated symbol probabilities of English language, you can find out the entropy of the language.</p>
</blockquote>

<p>What is actually meant by ""isolated symbol probabilities""? This is related to the entropy of an information source.</p>
",Multilingual Language Processing & Language Identification,meaning isolated symbol probability english note found phrase using isolated symbol probability english language find entropy language actually meant isolated symbol probability related entropy information source
approach to solve two char column scramble of a text,"<p>I have a paragraph of text scrambled by columns of two chars. The purpose of my assignment is to unscramble it:</p>

<pre><code>|de|  | f|Cl|nf|ed|au| i|ti|  |ma|ha|or|nn|ou| S|on|nd|on|
|ry|  |is|th|is| b|eo|as|  |  |f |wh| o|ic| t|, |  |he|h |
|ab|  |la|pr|od|ge|ob| m|an|  |s |is|el|ti|ng|il|d |ua|c |
|he|  |ea|of|ho| m| t|et|ha|  | t|od|ds|e |ki| c|t |ng|br|
|wo|m,|to|yo|hi|ve|u | t|ob|  |pr|d |s |us| s|ul|le|ol|e |
| t|ca| t|wi| M|d |th|""A|ma|l |he| p|at|ap|it|he|ti|le|er|
|ry|d |un|Th|"" |io|eo|n,|is|  |bl|f |pu|Co|ic| o|he|at|mm|
|hi|  |  |in|  |  | t|  |  |  |  |ye|  |ar|  |s |  |  |. |
</code></pre>

<p>My current approach to find the right order of columns is trying to recursively find each column's best position according to a word occurrence count criteria.</p>

<p>The pseudo-code of the algorithm's core I have in mind would be: </p>

<pre><code>function unscramble(scrambledMatrix,indexOfColumnIveJustMoved)
    for each column on scrambledMatrix as currentIndex=&gt;currentColumn
       if (currentIndex!=indexOfColumnIveJustMoved)
           maxRepeatedWords=0;maxIndex=0;
           for (i=0;i&lt;numberOfColumnsOfScrambledMatrix;i++)
              repWordsCount=countRepWords(moveFromToOn(currentIndex,i,scrambledMatrix))
              if (maxRepeatedWords&lt;repWordsCount)
                  maxRepeatedWords=repWordsCount;
                  maxIndex=i;
              endif
           endfor
           if (maxIndex!=currentIndex)
               return unscramble(moveFromToOn(currentIndex,maxIndex,scrambledMatrix),maxIndex); //recursive call
           endif
       endif
    endfor
    return(scrambledMatrix); //returns the unscrambled matrix;
endfunction
</code></pre>

<p>The algorithm stops when no column is moved after iterating on each one. I'm guessing it should work for any language (though I'm only interested on a solution for english) as long as the writing is based on words formed by letters and the sample is big enough.</p>

<p>Any suggestions on any other approaches or improvements? I would like to know the best solution for this problem (probably a dictionary based one looking for occurrences of common words instead? How about rebuilding the algorithm to avoid recursion, would it be much faster?).</p>
",Multilingual Language Processing & Language Identification,approach solve two char column scramble text paragraph text scrambled column two char purpose assignment unscramble current approach find right order column trying recursively find column best position according word occurrence count criterion pseudo code algorithm core mind would algorithm stop column moved iterating one guessing work language though interested solution english long writing based word formed letter sample big enough suggestion approach improvement would like know best solution problem probably dictionary based one looking occurrence common word instead rebuilding algorithm avoid recursion would much faster
Using Sentiment Analysis to Detect Contradictory Arguments?,"<p>I don't have much background in sentiment analysis or natural language processing at all, but I have been reading a bit about it in my spare time.  I would like to conduct and experiment to analyze forum threads/comments such as reddit, digg, blogs, etc.  I'm particularity interested in doing something like counting the number of for, against, and neutral comments for threads of heated religious and political debates.  Here's what I am thinking.</p>

<p>1) Find a thread that the original poster has defined a touchy political or religious topic.</p>

<p>2) For each comment categorize it as supporting the original poster or otherwise taking a contradicting or neutral stance.  </p>

<p>3) Compare various mediums with the numbers of for or against arguments to determine what platforms are good ""debate platforms"" (i.e. balanced argument counts).</p>

<p>One big problem that I'm anticipating is that heated topics will invoke strong reactions from both supporting and contradicting parties so a simple happy/sad sentiment analysis won't cut it.  I'm just sort of interested in this project for my own curiosities, so if anyone knows of similar research or utilities to conduct this experiment I'd be interested to hear more.  </p>

<p>Can someone recommend a good sentiment analysis, word dictionary, training set, etc. for this task?</p>
",Multilingual Language Processing & Language Identification,using sentiment analysis detect contradictory argument much background sentiment analysis natural language processing reading bit spare time would like conduct experiment analyze forum thread comment reddit digg blog etc particularity interested something like counting number neutral comment thread heated religious political debate thinking find thread original poster ha defined touchy political religious topic comment categorize supporting original poster otherwise taking contradicting neutral stance compare various medium number argument determine platform good debate platform e balanced argument count one big problem anticipating heated topic invoke strong reaction supporting contradicting party simple happy sad sentiment analysis cut sort interested project curiosity anyone know similar research utility conduct experiment interested hear someone recommend good sentiment analysis word dictionary training set etc task
Python vs Java for natural language processing,"<p>I have been working on java to find the similarity between two documents. I prefer finding semantic similarity , but havent made efforts to find it yet . I am using the following approach . </p>

<ol>
<li>Extract terms / tokens (I am using JAWS with wordnet to remove synonyms thus improves the similarities )</li>
<li>make a term document matrix </li>
<li>LSA </li>
<li>Cosine similarity </li>
</ol>

<p>When i was looking at few stackoverflow pages , i got quite a few links to python implementations. </p>

<p>I would like to know if python is a better language to find the text similarity and would also like to know if i can find semantic similairty between two documents in python  </p>
",Multilingual Language Processing & Language Identification,python v java natural language processing working java find similarity two document prefer finding semantic similarity havent made effort find yet using following approach extract term token using jaw wordnet remove synonym thus improves similarity make term document matrix lsa cosine similarity wa looking stackoverflow page got quite link python implementation would like know python better language find text similarity would also like know find semantic similairty two document python
Automatic negation of words,"<p>Consider the following statements</p>

<pre><code>We are not talking about a well established company in the NASDAQ
I will not initiate any trades until those clowns hammer out a deal
</code></pre>

<p>I am writing a simple Naive Bayes classifier, basically marking a training set of statements by hand (as either positive or negative sentiment) and storing the words that make up the statement accordingly.</p>

<p>Problem: if I mark both of these statements as having a negative sentiment, the words ""well"", ""established"" (statement 1) and ""any"", ""until"" (statement 2) would be indivudually marked as negatives. Whereas in another case (i.e., ""This company is performing well""), the same words (""well"" in this case) would be marked as a positive, making the sum of sentiment for ""well"" -1 + 1 = 0. I would overcome this by tagging these words as negated words, for example:</p>

<pre><code>We are talking about a not-well not-established company in the NASDAY.
I will initiate not-anymore trades not-until those clowns hammer out a deal
</code></pre>

<p>Is there a standard or best way of tagging these kinds of words (I don't even know if they are of a same group of words)? Obviously, tagging ""company"" wouldn't make sense ""not-company"" doesn't hold any sentimental value. I have (in PHP) made a function that would tag all words after the negation word (not, no, couldn't, etc) but many of them didn't make real sense afterwards (such as ""not-company"", ""not-NASDAQ"", ""not-clowns"").</p>

<p>Since English is not my mother language, I'm asking you if there's a common name for the words I have marked here and if what I want is (rudimentary) possible. I am aware that there are a lot of exceptions possible (double negations etc.) but I do not want to go into that; I believe if this would be possible, it would cover a lot of ground.</p>
",Multilingual Language Processing & Language Identification,automatic negation word consider following statement writing simple naive bayes classifier basically marking training set statement hand either positive negative sentiment storing word make statement accordingly problem mark statement negative sentiment word well established statement statement would indivudually marked negative whereas another case e company performing well word well case would marked positive making sum sentiment well would overcome tagging word negated word example standard best way tagging kind word even know group word obviously tagging company make sense company hold sentimental value php made function would tag word negation word etc many make real sense afterwards company nasdaq clown since english mother language asking common name word marked want rudimentary possible aware lot exception possible double negation etc want go believe would possible would cover lot ground
Clean and natural scripting functionality without parsing,"<p>I'm experimenting with creating a semi-natural scripting language, mostly for my own learning purposes, and for fun. The catch is that it needs to be in native C#, no parsing or lexical analysis on my part, so whatever I do needs to be able to be done through normal syntactical sugar.</p>

<p>I want it to read somewhat like a sentence would, so that it is easy to read and learn, especially for those that aren't especially fluent with programming, but I also want the full functionality of native code available to the user.</p>

<p>For example, in the perfect world it would look like a natural language (English in this case):</p>

<pre><code>When an enemy is within 10 units of player, the enemy attacks the player
</code></pre>

<p>In C#, allowing a sentence like this to actually do what the scripter intends would almost certainly require that this be a string that is run through a parser and lexical analyzer. My goal isn't that I have something this natural, and I don't want the scripter to be using strings to script. I want the scripter to have full access to C#, and have things like syntax highlighting, intellisense, debugging in IDE, etc. So what I'm trying to get it something that reads easily, but is in native C#. A couple of the major hurdles that I don't see a way to overcome is getting rid of periods <code>.</code>, commas <code>,</code>, and parentheses for empty methods <code>()</code>. For example, something like this is feasible but doesn't read very cleanly:</p>

<pre><code>// C#
When(Enemy.Condition(Conditions.isWithinDistance(Enemy, Player, 10))), Event(Attack(Enemy, Player))
</code></pre>

<p>Using a language like Scala you can actually get much closer, because periods and parentheses can be replaced by a single whitespace in many cases. For example, you could take the above statement and make it look something like this in Scala:</p>

<pre><code>// Scala
When(Enemy is WithinDistance(Player, 10)) =&gt; Then(Attack From(Enemy, Player))
</code></pre>

<p>This above code would actually compile assuming you setup your engine to handle it, in fact you might be able to coax further parentheses and commas out of this. Without the syntactical sugar in the above example it would be more like this, in Scala:</p>

<pre><code>// Scala (without syntactical sugar)
When(Enemy.is(WithinDistance(Player, 10)) =&gt; Then(Attack().From(Enemy, Player))
</code></pre>

<p>The bottom line is I want to get as close as possible to something like the first scala example using native C#. It may be that there is really nothing I can do, but I'm willing to try any tricks that may be possible to make it read more natural, and get the periods, parentheses, and commas out of there (except when they make sense even in natural language).</p>

<p>I'm not as experienced with C# as other languages, so I might not know about some syntax tricks that are available, like macros in C++. Not that macros would actually be a good solution, they would probably cause more problems then they would solve, and would be a debugging nightmare, but you get where I'm going with this, at least in C++ it would be feasible. Is what I'm wanting even possible in C#?</p>

<p>Here's an example, using LINQ and Lambda expressions you can sometimes get the same amount of work done with fewer lines, less symbols, and code the reads closer to English. For example, here's an example of three collisions that happen between pairs of objects with IDs, we want to gather all collisions with the object that has ID 5, then sort those collisions by the ""first"" ID in the pair, and then output the pairs. Here is how you would do this without LINQ and/or Lambra expressions:</p>

<pre><code>struct CollisionPair : IComparable, IComparer
{
    public int first;
    public int second;

    // Since we're sorting we'll need to write our own Comparer
    int IComparer.Compare( object one, object two )
    {
        CollisionPair pairOne = (CollisionPair)one;
        CollisionPair pairTwo = (CollisionPair)two;

        if (pairOne.first &lt; pairTwo.first)
            return -1;
        else if (pairTwo.first &lt; pairOne.first)
            return 1;
        else
            return 0;
    }

    // ...and our own compable
    int IComparable.CompareTo( object two )
    {
        CollisionPair pairTwo = (CollisionPair)two;

        if (this.first &lt; pairTwo.first)
            return -1;
        else if (pairTwo.first &lt; this.first)
            return 1;
        else
            return 0;
    }
}

static void Main( string[] args )
{           
    List&lt;CollisionPair&gt; collisions = new List&lt;CollisionPair&gt;
    {
        new CollisionPair { first = 1, second = 5 },
        new CollisionPair { first = 2, second = 3 },
        new CollisionPair { first = 5, second = 4 }
    };

    // In a script this would be all the code you needed, everything above
    // would be part of the game engine   
    List&lt;CollisionPair&gt; sortedCollisionsWithFive = new List&lt;CollisionPair&gt;();
    foreach (CollisionPair c in collisions)
    {
        if (c.first == 5 || c.second == 5)
        {
            sortedCollisionsWithFive.Add(c);
        }
    }
    sortedCollisionsWithFive.Sort();

    foreach (CollisionPair c in sortedCollisionsWithFive)
    {
        Console.WriteLine(""Collision between "" + c.first +
                          "" and "" + c.second);
    }
}
</code></pre>

<p>And now the same example with LINQ and Lambda. Notice in this example we don't have to both with making <code>CollisionPair</code> both <code>IComparable</code> and <code>IComparer</code>, and don't have to implement to the <code>Compare</code> and <code>CompareTo</code> methods:</p>

<pre><code>struct CollisionPair
{
    public int first;
    public int second;
}

static void Main( string[] args )
{           
    List&lt;CollisionPair&gt; collisions = new List&lt;CollisionPair&gt;
    {
        new CollisionPair { first = 1, second = 5 },
        new CollisionPair { first = 2, second = 3 },
        new CollisionPair { first = 5, second = 4 }
    };

    // In a script this would be all the code you needed, everything above
    // would be part of the game engine
    (from c in collisions 
    where ( c.first == 5 || c.second == 5 )
    orderby c.first select c).ForEach(c =&gt;
        Console.WriteLine(""Collision between "" + c.first +
                          "" and "" + c.second));
}
</code></pre>

<p>In the end we're left with a LINQ and Lambda expression that read closer to natural language, and are much less code for both a game engine and for the script. These kinds of changes are really what I'm looking for, but obviously LINQ and Lambda are both limited to specific syntax, not something as generic as I would like in the end.</p>
",Multilingual Language Processing & Language Identification,clean natural scripting functionality without parsing experimenting creating semi natural scripting language mostly learning purpose fun catch need native c parsing lexical analysis part whatever need able done normal syntactical sugar want read somewhat like sentence would easy read learn especially especially fluent programming also want full functionality native code available user example perfect world would look like natural language english case c allowing sentence like actually scripter intends would almost certainly require string run parser lexical analyzer goal something natural want scripter using string script want scripter full access c thing like syntax highlighting intellisense debugging ide etc trying get something read easily native c couple major hurdle see way overcome getting rid period comma parenthesis empty method example something like feasible read cleanly using language like scala actually get much closer period parenthesis replaced single whitespace many case example could take statement make look something like scala code would actually compile assuming setup engine handle fact might able coax parenthesis comma without syntactical sugar example would like scala bottom line want get close possible something like first scala example using native c may really nothing willing try trick may possible make read natural get period parenthesis comma except make sense even natural language experienced c language might know syntax trick available like macro c macro would actually good solution would probably cause problem would solve would debugging nightmare get going least c would feasible wanting even possible c example using linq lambda expression sometimes get amount work done fewer line le symbol code read closer english example example three collision happen pair object id want gather collision object ha id sort collision first id pair output pair would without linq lambra expression example linq lambda notice example making implement method end left linq lambda expression read closer natural language much le code game engine script kind change really looking obviously linq lambda limited specific syntax something generic would like end
language detector,"<p>I want a java code which reads a text inside a document and say that it is in which language (English, Spanish, ...). The format of the document is not important. I want the output to be for example : ""This document is in Spanish"". Please guide me in this way and give me a sample code for it. </p>
",Multilingual Language Processing & Language Identification,language detector want java code read text inside document say language english spanish format document important want output example document spanish please guide way give sample code
How can I vary the sentence prefix &quot;I am working on [X]&quot; such that it has correct sentence structure for all X?,"<p>I want the user to be able to enter a task and I will prefix it appropriately such that it has correct sentence structure.</p>

<p>E.g.</p>

<pre><code>I am working on [making the world a better place]
</code></pre>

<p>...sounds good.</p>

<pre><code>I am working on [discuss draft proposal]
</code></pre>

<p>...doesn't sound good. In this case it would want the program to respond with something like:</p>

<pre><code>I am discussing a draft proposal
</code></pre>

<p>Basically the way people write tasks or todos appears to be imperative (e.g. pick up milk, write essay, etc.) or simply a noun (e.g. assignment 1, client meeting, etc.). I want to convert these to <a href=""http://www.englishpage.com/verbpage/presentcontinuous.html"">Present Progressive</a> tense.</p>

<p>I am looking into the field of Natural Language Processing at the moment, but I was wondering if there was some sort of API available that would do what I need, or if someone has had experience with a similar problem.</p>
",Multilingual Language Processing & Language Identification,vary sentence prefix working x ha correct sentence structure x want user able enter task prefix appropriately ha correct sentence structure e g sound good sound good case would want program respond something like basically way people write task todos appears imperative e g pick milk write essay etc simply noun e g assignment client meeting etc want convert href progressive tense looking field natural language processing moment wa wondering wa sort api available would need someone ha experience similar problem
Try to figure out a good way to split English document into sentences in C#,"<p>Is there a good way to split English document into sentences? I mean English document frequently includes Mr. Mrs. U.S.A, etc. It is difficult to separate them out. Do we need a special natural language library to accomplish this? I suspect that we need it.</p>

<p>Thank you.</p>
",Multilingual Language Processing & Language Identification,try figure good way split english document sentence c good way split english document sentence mean english document frequently includes mr mr u etc difficult separate need special natural language library accomplish suspect need thank
Tree traversing or what?,"<p>In Python, I'm writing a Natural Language Processing module and can't work out how to code a function to do the following.
Input: a list of parts of speech (POS) derived from an inputted sentence as short strings. Some items in the list are themselves lists because that part of the program doesn't know which part of speech to choose out of two or more possibles.
e.g. a particular six word sentence results in <code>[""DET"", ""NOUN"", [""VERB"", ""NOUN""], ""CONJ"", [""ADJ"", ""ADV"", ""NOUN""], ""ADV""]</code>
i.e the first word is definitely a DET
the 2nd word is definitely a NOUN
the 3rd word could be a VERB or a NOUN
the 4th word is definitely a CONJ
the 5th word could be a ADJ, ADV or NOUN
the 6th word is definitely a ADV.</p>

<p>So INPUT = <code>[""DET"", ""NOUN"", [""VERB"", ""NOUN""], ""CONJ"", [""ADJ"", ""ADV"", ""NOUN""], ""ADV""]</code></p>

<p>I need the function to return each possible combination as a list of lists. So the return value for the above should be:</p>

<pre><code>[[""DET"", ""NOUN"", ""NOUN"", ""CONJ"", ""NOUN"", ""ADV""],
 [""DET"", ""NOUN"", ""NOUN"", ""CONJ"", ""ADV"", ""ADV""],
 [""DET"", ""NOUN"", ""NOUN"", ""CONJ"", ""ADJ"", ""ADV""],
 [""DET"", ""NOUN"", ""VERB"", ""CONJ"", ""NOUN"", ""ADV""],
 [""DET"", ""NOUN"", ""VERB"", ""CONJ"", ""ADV"", ""ADV""],
 [""DET"", ""NOUN"", ""VERB"", ""CONJ"", ""ADJ"", ""ADV""]]
</code></pre>

<p>The sentences could be from one to n words long. Each word might come back with from one to n parts of speech.</p>
",Multilingual Language Processing & Language Identification,tree traversing python writing natural language processing module work code function following input list part speech po derived inputted sentence short string item list list part program know part speech choose two possible e g particular six word sentence result e first word definitely det nd word definitely noun rd word could verb noun th word definitely conj th word could adj adv noun th word definitely adv input need function return possible combination list list return value sentence could one n word long word might come back one n part speech
Words Prediction - Get most frequent predecessor and successor,"<p>Given a word I want to get the list of most frequent predecessors and successors of the word in English language.
I have developed a code that does bigram analysis on any corpus ( I have used Enron email corpus) and can predict the most frequent next possible word but I want some other solution because
a) I want to check the working / accuracy of my prediction
b) Corpus or dataset based solutions fail for an unseen word</p>

<p>For example, given the word ""excellent"" I want to get the words that are most likely to come before excellent and after excellent</p>

<p>My question is whether any particular service or api exists for the purpose?</p>
",Multilingual Language Processing & Language Identification,word prediction get frequent predecessor successor given word want get list frequent predecessor successor word english language developed code doe bigram analysis corpus used enron email corpus predict frequent next possible word want solution want check working accuracy prediction b corpus dataset based solution fail unseen word example given word excellent want get word likely come excellent excellent question whether particular service api exists purpose
Good separators for key-value pairs in English,"<p>I want to store a number of key-value pairs in a flat file. A key can have multiple values. Keys and values are in English and may contain Unicode characters occasionally. 
What are the good separators that can be used while storing these keys and values in a file, so that the separators are not encountered as a part of any key or value. For example, '&amp;' is not a good separator as a key or value can contain an '&amp;'. </p>

<p>I need two separators, one to separate the key from values, and another to separate the values.</p>
",Multilingual Language Processing & Language Identification,good separator key value pair english want store number key value pair flat file key multiple value key value english may contain unicode character occasionally good separator used storing key value file separator encountered part key value example good separator key value contain need two separator one separate key value another separate value
Common Service Locator and implementations of IDependencyResolver,"<p>I'm building a library for <a href=""http://nlp.abodit.com"" rel=""nofollow"">conversational natural language processing</a>.  In many ways it acts much like MVC3 in that it has Controllers and Action Methods.  It also uses dependency injection in much the same way as MVC3 does when instantiating constructors for the Controller classes.  The main differences being that an English sentence replaces both the URL and the form values of HTTP; routing is based on matching sentence structure; and the parameters passed in are the meanings of words and phrases used in the English sentence.  </p>

<p>Currently it uses Autofac for Dependency Injection but I'd like to remove that dependency and allow callers to use any DI container.  </p>

<p>If I use the P&amp;P / Codeplex <a href=""http://commonservicelocator.codeplex.com/SourceControl/changeset/view/27688#332676"" rel=""nofollow"">Common Service Locator</a> project in my solution then callers would still need to provide their own implementations of <code>IServiceLocator</code> against the instance of that interface exposed by my engine.  If I use <code>IDependencyResolver</code> from MVC3 instead there are at least existing implementations of the mapping from the various DI container to that interface.</p>

<p>Should I:-</p>

<ol>
<li>use the Common Service Locator and force callers to implement the mapping classes.</li>
<li>use the MVC 3 <code>IDependencyResolver</code> interface which already has mappings to other containers.</li>
<li>accept a <code>object</code> as the dependency resolver and duck type it to get the one method I need from it so I can use the MVC3 interface without even taking a dependency on ASP.NET MVC3.</li>
<li>other?</li>
</ol>
",Multilingual Language Processing & Language Identification,common service locator implementation idependencyresolver building library conversational natural language processing many way act much like mvc ha controller action method also us dependency injection much way mvc doe instantiating constructor controller class main difference english sentence replaces url form value http routing based matching sentence structure parameter passed meaning word phrase used english sentence currently us autofac dependency injection like remove dependency allow caller use di container use p p codeplex common service locator project solution caller would still need provide implementation instance interface exposed engine use mvc instead least existing implementation mapping various di container interface use common service locator force caller implement mapping class use mvc interface already ha mapping container accept dependency resolver duck type get one method need use mvc interface without even taking dependency asp net mvc
Multitask learning,"<p>Can anybody please explain multitask learning in simple and intuitive way? May be some real
world problem would be useful.Mostly, these days i am seeing many people are using it for natural language processing tasks.</p>
",Multilingual Language Processing & Language Identification,multitask learning anybody please explain multitask learning simple intuitive way may real world problem would useful mostly day seeing many people using natural language processing task
Identifying (spoken) language in javascript,"<p>Does anyone know if there is any language detection script/library available for javascript? I like to incorporate it into nodejs but didn't found any.</p>

<p>I don't want a browser language detection but a string language detection.
'Hello World' would be detected english and 'Hallo Wereld' would be detected as dutch.</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,identifying spoken language javascript doe anyone know language detection script library available javascript like incorporate nodejs found want browser language detection string language detection hello world would detected english hallo wereld would detected dutch thanks advance
Determining the Similarity Between Items in a Database,"<p>We have a database with hundreds of millions of records of log data. We're attempting to 'group' this log data as being likely to be of the same nature as other entries in the log database. For instance:</p>

<p>Record X may contain a log entry like:</p>

<blockquote>
  <p>Change Transaction ABC123 Assigned To Server US91</p>
</blockquote>

<p>And Record Y may contain a log entry like:</p>

<blockquote>
  <p>Change Transaction XYZ789 Assigned To Server GB47</p>
</blockquote>

<p>To us humans those two log entries are easily recognizable as being likely related in some way. Now, there may be 10 million rows between Record X and Record Y. And there may be thousands of other entries that are similar to X and Y, and some that are totally different but that have other records they are similar to.</p>

<p>What I'm trying to determine is the best way to group the similar items together and say that with XX% certainty Record X and Record Y are probably of the same nature. Or perhaps a better way of saying it would be that the system would look at Record Y and say based on your content you're most like Record X as apposed to all other records.</p>

<p>I've seen some mentions of Natural Language Processing and other ways to find similarity between strings (like just brute-forcing some Levenshtein calculations) - however for us we have these two additional challenges:</p>

<ol>
<li>The content is machine generated - not human generated</li>
<li>As opposed to a search engine approach where we determine results for a given query - we're trying to classify a giant repository and group them by how alike they are to one another.</li>
</ol>

<p>Thanks for your input!</p>
",Multilingual Language Processing & Language Identification,determining similarity item database database hundred million record log data attempting group log data likely nature entry log database instance record x may contain log entry like change transaction abc assigned server u record may contain log entry like change transaction xyz assigned server gb u human two log entry easily recognizable likely related way may million row record x record may thousand entry similar x totally different record similar trying determine best way group similar item together say xx certainty record x record probably nature perhaps better way saying would system would look record say based content like record x apposed record seen mention natural language processing way find similarity string like brute forcing levenshtein calculation however u two additional challenge content machine generated human generated opposed search engine approach determine result given query trying classify giant repository group alike one another thanks input
How to detect the language type of a given text via Python?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/4545977/python-can-i-detect-unicode-string-language-code"">Python - can I detect unicode string language code?</a>  </p>
</blockquote>



<p>similar question is <a href=""https://stackoverflow.com/q/4545977/4279"">Python - can I detect unicode string language code?</a></p>

<p>I had used Google Translate API to detect language type before. The result is perfect, but it's not free now. So I have to find a substition. Any suggestion?</p>
",Multilingual Language Processing & Language Identification,detect language type given text via python possible duplicate used google translate api detect language type result perfect free find substition suggestion
Topia Term Extract - Italian Lexicon,"<p>I'm looking for a tool to extract topic keywords from text. Topia seems to be a good solutions, anyway it does not came with an italian-lexicon file.</p>

<p>Searching on the web i couldn't find a precompiled file, so I guess I need to build my own. Does anyone have suggestion on how to build it without spending a lifetime?</p>

<p>The english file is formatted like this:</p>

<pre><code>images NNS
psychiatric JJ
Hope NNP NN VB VBP
elimination NN
</code></pre>

<p>Thanks in advance for any advice.</p>
",Multilingual Language Processing & Language Identification,topia term extract italian lexicon looking tool extract topic keywords text topia seems good solution anyway doe came italian lexicon file searching web find precompiled file guess need build doe anyone suggestion build without spending lifetime english file formatted like thanks advance advice
How can I select a FAQ entry from a user&#39;s natural-language inquiry?,"<p>I am working on an app where the user submits a series of questions. These questions are freeform text, but are based on a specific product, so I have a general understanding of the context.  I have a FAQ listing, and I need to try to match the user's question to a question in the FAQ.</p>

<p>My language is Delphi.  My general thought approach is to throw out small ""garbage words"", a, an, the, is, of, by, etc...  Run a stemming program over these words to get the root words, and then try to match as many of the remaining words as possible.  </p>

<p>Is there a better approach?  I have thought about some type of natural language processing, but I am afraid that I would be looking at years of development, rather than a week or two.</p>
",Multilingual Language Processing & Language Identification,select faq entry user natural language inquiry working app user submits series question question freeform text based specific product general understanding context faq listing need try match user question question faq language delphi general thought approach throw small garbage word etc run stemming program word get root word try match many remaining word possible better approach thought type natural language processing afraid would looking year development rather week two
What are good starting points for someone interested in natural language processing?,"<h1>Question</h1>

<p>So I've recently came up with some new possible projects that would have to deal with deriving 'meaning' from text submitted and generated by users.</p>

<p><a href=""http://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow noreferrer"">Natural language processing</a> is the field that deals with these kinds of issues, and after some initial research I found the <a href=""http://opennlp.sourceforge.net/"" rel=""nofollow noreferrer"">OpenNLP Hub</a> and university collaborations like the <a href=""http://attempto.ifi.uzh.ch/site/"" rel=""nofollow noreferrer"">attempto project</a>. And stackoverflow has <a href=""https://stackoverflow.com/questions/88984/your-favorite-natural-language-parser"">this</a>.</p>

<p>If anyone could link me to some good resources, from reseach papers and introductionary texts to apis, I'd be happier than a 6 year-old kid opening his christmas presents!</p>

<h1>Update</h1>

<p>Through one of your recommendations I've found <a href=""http://www.opencyc.org/"" rel=""nofollow noreferrer"">opencyc</a> (<em>'the world's largest and most complete general knowledge base and commonsense reasoning engine'</em>). Even more amazing still, there's a project that is a distilled version of opencyc called <a href=""http://umbel.org/"" rel=""nofollow noreferrer"">UMBEL</a>. It features semantic data in rdf/owl/skos n3 syntax.</p>

<p>I've also stumbled upon <a href=""http://antlr.org/"" rel=""nofollow noreferrer"">antlr</a>, a parser generator for <em>'constructing recognizers, interpreters, compilers, and translators from grammatical descriptions'</em>.</p>

<p>And there's a question on here by me, that lists tons of <a href=""https://stackoverflow.com/questions/202092/where-can-i-find-free-and-open-data"">free and open data</a>.</p>

<p>Thanks stackoverflow community!</p>
",Multilingual Language Processing & Language Identification,good starting point someone interested natural language processing question recently came new possible project would deal deriving meaning text submitted generated user natural language processing field deal kind issue initial research found opennlp hub university like attempto project stackoverflow ha opencyc world largest complete general knowledge base commonsense reasoning engine even amazing still project distilled version opencyc called umbel feature semantic data rdf owl skos n syntax also stumbled upon antlr parser generator constructing recognizers interpreter compiler translator grammatical description question list ton href open data thanks stackoverflow community
Natural Language processing for getting qualitative info,"<p>I have a requirement for which I do not know if NL can do it. Plz advise.</p>

<p>My requirement is to scan a sentence in english language and figure out some qualitative info about it. Such as , what are the subjects , nouns in the sentence and what is said about them is a descriptive , suggestive  or does it affects positively or negatively. </p>

<p>As an example, lets say I have a Fan page in facebook , and someone posts a post on my wall. I need to know if the post says something good or bad about me and accordingly I can map it to a perception scale from , say -10 to +10. </p>

<p>Can something like this be done with with Natural language processing toolkits?
If no, then what is the way.</p>

<p>Thanks in advance
Shyam </p>
",Multilingual Language Processing & Language Identification,natural language processing getting qualitative info requirement know nl plz advise requirement scan sentence english language figure qualitative info subject noun sentence said descriptive suggestive doe affect positively negatively example let say fan page facebook someone post post wall need know post say something good bad accordingly map perception scale say something like done natural language processing toolkits way thanks advance shyam
"Is vim able to detect the natural language of a file, then load the correct dictionary?","<p>I am using several languages, and currently I am obliged to indicate to vim with which of these the spell check must be done. Is there a way to set up vim so that it automatically detects the correct one? I vaguely remember that in a previous version of vim, when the spell check was not integrated, the vimspell script made this possible.</p>

<p>It would be even better if this could apply not only to a file but also to a portion of a file, since I frequently mix several languages in a single file. Of course, I would like to avoid to load several dictionaries simultaneously.</p>
",Multilingual Language Processing & Language Identification,vim able detect natural language file load correct dictionary using several language currently obliged indicate vim spell check must done way set vim automatically detects correct one vaguely remember previous version vim spell check wa integrated vimspell script made possible would even better could apply file also portion file since frequently mix several language single file course would like avoid load several dictionary simultaneously
Decoding Permutated English Strings,"<p>A coworker was recently asked this when trying to land a (different) research job:</p>

<p>Given 10 128-character strings which have been permutated in exactly the same way, decode the strings. The original strings are English text with spaces, numbers, punctuation and other non-alpha characters removed.</p>

<p>He was given a few days to think about it before an answer was expected. How would you do this? You can use any computer resource, including character/word level language models.</p>
",Multilingual Language Processing & Language Identification,decoding permutated english string coworker wa recently asked trying land different research job given character string permutated exactly way decode string original string english text space number punctuation non alpha character removed wa given day think answer wa expected would use computer resource including character word level language model
Incrementally Trainable Entity Recognition Classifier,"<p>I'm doing some semantic-web/nlp research, and I have a set of sparse records, containing a mix of numeric and non-numeric data, representing entities labeled with various features extracted from simple English sentences.</p>

<p>e.g.</p>

<pre><code>uid|features
87w39423|speaker=432, session=43242, sentence=34, obj_called=bob,favorite_color_is=blue
4535k3l535|speaker=512, session=2384, sentence=7, obj_called=tree,isa=plant,located_on=wilson_street
23432424|speaker=997, session=8945305, sentence=32, obj_called=salty,isa=cat,eats=mice
09834502|speaker=876, session=43242, sentence=56, obj_called=the monkey,ate=the banana
928374923|speaker=876, session=43242, sentence=57, obj_called=it,was=delicious
294234234|speaker=876, session=43243, sentence=58, obj_called=the monkey,ate=the banana
sd09f8098|speaker=876, session=43243, sentence=59, obj_called=it,was=hungry
...
</code></pre>

<p>A single entity may appear more than once (but with a different UID each time), and may have overlapping features with its other occurrences. A second data set represents which of the above UIDs are definitely the same.</p>

<p>e.g.</p>

<pre><code>uid|sameas
87w39423|234k2j,234l24jlsd,dsdf9887s
4535k3l535|09d8fgdg0d9,l2jk34kl,sd9f08sf
23432424|io43po5,2l3jk42,sdf90s8df
09834502|294234234,sd09f8098
...
</code></pre>

<p>What algorithm(s) would I use to <strong>incrementally</strong> train a classifier that could take a set of features, and instantly recommend the N most similar UIDs and probability of whether or not those UIDs actually represent the <strong>same</strong> entity? Optionally, I'd also like to get a recommendation of missing features to populate and then re-classify to get a more certain matches.</p>

<p>I researched traditional approximate nearest neighbor algorithms. such as <a href=""http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN"" rel=""nofollow"">FLANN</a> and <a href=""http://pypi.python.org/pypi/scikits.ann"" rel=""nofollow"">ANN</a>, and I don't think these would be appropriate since they're not trainable (in a supervised learning sense) nor are they typically designed for sparse non-numeric input.</p>

<p>As a very naive first-attempt, I was thinking about using a naive bayesian classifier, by converting each SameAs relation into a set of training samples. So, for each entity A with B sameas relations, I would iterate over each and train the classifier like:</p>

<pre><code>classifier = Classifier()
for entity,sameas_entities in sameas_dataset:
    entity_features = get_features(entity)
    for other_entity in sameas_entities:
        other_entity_features = get_features(other_entity)
        classifier.train(cls=entity, ['left_'+f for f in entity_features] + ['right_'+f for f in other_entity_features])
        classifier.train(cls=other_entity, ['left_'+f for f in other_entity_features] + ['right_'+f for f in entity_features])
</code></pre>

<p>And then use it like:</p>

<pre><code>&gt;&gt;&gt; print classifier.findSameAs(dict(speaker=997, session=8945305, sentence=32, obj_called='salty',isa='cat',eats='mice'), n=7)
[(1.0, '23432424'),(0.999, 'io43po5', (1.0, '2l3jk42'), (1.0, 'sdf90s8df'), (0.76, 'jerwljk'), (0.34, 'rlekwj32424'), (0.08, '09843jlk')]
&gt;&gt;&gt; print classifier.findSameAs(dict(isa='cat',eats='mice'), n=7)
[(0.09, '23432424'), (0.06, 'jerwljk'), (0.03, 'rlekwj32424'), (0.001, '09843jlk')]
&gt;&gt;&gt; print classifier.findMissingFeatures(dict(isa='cat',eats='mice'), n=4)
['obj_called','has_fur','has_claws','lives_at_zoo']
</code></pre>

<p>How viable is this approach? The initial batch training would be horribly slow, at least O(N^2), but incremental training support would allow updates to happen more quickly.</p>

<p>What are better approaches?</p>
",Multilingual Language Processing & Language Identification,incrementally trainable entity recognition classifier semantic web nlp research set sparse record containing mix numeric non numeric data representing entity labeled various feature extracted simple english sentence e g single entity may appear different uid time may overlapping feature occurrence second data set represents uids definitely e g algorithm would use incrementally train classifier could take set feature instantly recommend n similar uids probability whether uids actually represent entity optionally also like get recommendation missing feature populate classify get certain match researched traditional approximate nearest neighbor algorithm flann ann think would appropriate since trainable supervised learning sense typically designed sparse non numeric input naive first attempt wa thinking using naive bayesian classifier converting sameas relation set training sample entity b sameas relation would iterate train classifier like use like viable approach initial batch training would horribly slow least n incremental training support would allow update happen quickly better approach
How do I determine if a random string sounds like English?,"<p>I have an algorithm that generates strings based on a list of input words. How do I separate only the strings that sounds like English words? ie. discard <strong>RDLO</strong> while keeping <strong>LORD</strong>.</p>

<p><strong>EDIT:</strong> To clarify, they do not need to be actual words in the dictionary. They just need to sound like English. For example <strong>KEAL</strong> would be accepted.</p>
",Multilingual Language Processing & Language Identification,determine random string sound like english algorithm generates string based list input word separate string sound like english word ie discard rdlo keeping lord edit clarify need actual word dictionary need sound like english example keal would accepted
Library to classify text on affect,"<p>Are there libraries out there for the automatic classification of text in for example English on affect? If it's quite rough it might still be good enough.</p>
",Multilingual Language Processing & Language Identification,library classify text affect library automatic classification text example english affect quite rough might still good enough
java vs C++ for natural language processing,"<p>Could you please let me know which one is better to learn amongst Java and C++ for natural language processing with respect to library,support etc.</p>

<p>Best, Thetna</p>
",Multilingual Language Processing & Language Identification,java v c natural language processing could please let know one better learn amongst java c natural language processing respect library support etc best thetna
Maximum Entropy for Natural Language Processing,"<p>Can anyone explain simply how how maximum entropy models work when used in Natural Language Processing. I need to statistically parse simple words and phrases to try to figure out the likelihood of specific words and what objects they refer to or what phrases they are contained within.</p>
",Multilingual Language Processing & Language Identification,maximum entropy natural language processing anyone explain simply maximum entropy model work used natural language processing need statistically parse simple word phrase try figure likelihood specific word object refer phrase contained within
How to parse a list of words according to a simplified grammar?,"<p>Just to clarify, this isn't homework. I've been asked for help on this and am unable to do it, so it turned into a personal quest to solve it.</p>

<p>Imagine you have a grammar for an English sentence like this:</p>

<pre><code>S =&gt; NP VP | VP
NP =&gt; N | Det N | Det Adj N
VB =&gt; V | V NP
N =&gt; i you bus cake bear
V =&gt; hug love destroy am
Det =&gt; a the
Adj =&gt; pink stylish
</code></pre>

<p>I've searched for several hours and really am out of ideas. 
I found articles talking about shallow parsing, depth-first backtracking and related things, and while I'm familiar with most of them, I still can't apply them to this problem. I tagged Lisp and Haskell because those are the languages I plan to implement this in, but I don't mind if you use other languages in your replies.</p>

<p>I'd appreciate hints, good articles and everything in general.</p>
",Multilingual Language Processing & Language Identification,parse list word according simplified grammar clarify homework asked help unable turned personal quest solve imagine grammar english sentence like searched several hour really idea found article talking shallow parsing depth first backtracking related thing familiar still apply problem tagged lisp haskell language plan implement mind use language reply appreciate hint good article everything general
Brute-Force language detection,"<p>I need an algorithm (any programming language) to test the vitality with an hill climbing algorithm for breaking a cipher for a crypto challenge. The algorithm should test how likely it is that an random-decryption (has no spaces) is an English text (also giving points for yet incomplete words!)  or just a random sequence of characters.</p>

<p>I tried it with several algorithms I developed but they were not so good.</p>

<p>My research:</p>

<p>An enigma M4 crypto project ( <a href=""http://www.bytereef.org/m4_project.html"" rel=""nofollow"">http://www.bytereef.org/m4_project.html</a> ) uses the Sinkov statistics, which I want to use, too.</p>

<p>The only thing I found was a document of ¬´quebra -pedra¬ª, a Java framework that includes the Sinkov log-weight analysis I am searching for.</p>

<p><a href=""http://www.google.com/m?client=ms-android-samsung&amp;source=android-home#q=Quebra-pedra+framework+java"" rel=""nofollow"">http://www.google.com/m?client=ms-android-samsung&amp;source=android-home#q=Quebra-pedra+framework+java</a> </p>

<p>But I have not found where to download the framework. Also I have not found any implementation or description of the Sinkov test.</p>

<p>I would be glad for any hints. Thanks.</p>
",Multilingual Language Processing & Language Identification,brute force language detection need algorithm programming language test vitality hill climbing algorithm breaking cipher crypto challenge algorithm test likely random decryption ha space english text also giving point yet incomplete word random sequence character tried several algorithm developed good research enigma crypto project us sinkov statistic want use thing found wa document quebra pedra java framework includes sinkov log weight analysis searching found download framework also found implementation description sinkov test would glad hint thanks
"NLP project, python or C++","<p>We are working on Arabic Natural Language Processing project, we have limited our choices to either write the code in Python or C++ (and Boost library). We are thinking of these points:</p>

<ul>
<li><p>Python</p>

<ul>
<li>Slower than C++ (There is ongoing work to make Python faster)</li>
<li>Better UTF8 support</li>
<li>Faster in writing tests and trying different algorithms</li>
</ul></li>
<li><p>C++</p>

<ul>
<li>Faster than Python</li>
<li>Familiar code, every programmer knows C or C-like code</li>
</ul></li>
</ul>

<p>After the project is done, it should be not very hard to port the project to another programming languages.</p>

<p>What do you think is better and suitable for the project?</p>
",Multilingual Language Processing & Language Identification,nlp project python c working arabic natural language processing project limited choice either write code python c boost library thinking point python slower c ongoing work make python faster better utf support faster writing test trying different algorithm c faster python familiar code every programmer know c c like code project done hard port project another programming language think better suitable project
searching for language recognition library in Python or web service,"<p>Is there any tool that provides language recognition functionality? </p>

<p>Like if I input a sentence of English, it will suggest that the string <em>may</em> be English.</p>

<p>I think I need one of the following stuff.</p>

<ol>
<li>Python language recognition library.</li>
<li>Web service that provides such functionality.</li>
</ol>

<p>Anyone can help?</p>
",Multilingual Language Processing & Language Identification,searching language recognition library python web service tool provides language recognition functionality like input sentence english suggest string may english think need one following stuff python language recognition library web service provides functionality anyone help
Validating Syntax of a Sentence,"<p>I'm looking for a library to simply validate the the syntax of english natural language sentences. It doesn't have to be correct all the time (and obviously some sentences will be ambiguous/ humans will disagree on validity).</p>

<p>So for example:
jim likes the blue ball 
 would be valid, whereas
jim likes likes blue ball jim
 would not be.</p>

<p>I've tried ""Syntactic parser of English sentences"" by Andrej Pancik which appears to do what I want, but unfortunately most sentences I'd consider to be ""valid"" it doesn't consider to be.</p>

<p>Is there any code out there I can use? Otherwise I'm thinking to do this myself by creating parse tree with something like ANTLR and identifying nouns with WordNet.</p>
",Multilingual Language Processing & Language Identification,validating syntax sentence looking library simply validate syntax english natural language sentence correct time obviously sentence ambiguous human disagree validity example jim like blue ball would valid whereas jim like like blue ball jim would tried syntactic parser english sentence andrej pancik appears want unfortunately sentence consider valid consider code use otherwise thinking creating parse tree something like antlr identifying noun wordnet
GUI for sentence alignment tasks,"<p>Can someone introduce me on how to write a simple web (HTML/XML) interface for a simple sentence alignment task?</p>

<p>The task is as follows:</p>

<p>The 1st line of the webpage would be the English sentence that will need to be matched to the chinese sentences below:</p>

<pre><code>000325EN    Whatever goes upon two legs is an enemy.

(checkbox)001054ZH  Âá°Èù†‰∏§Êù°ËÖøË°åËµ∞ËÄÖÁöÜ‰∏∫‰ªáÊïåÔºõ
(checkbox)001055ZH  Âá°Èù†ÂõõËÇ¢Ë°åËµ∞ËÄÖÔºåÊàñËÄÖÈïøÁøÖËÜÄËÄÖÔºåÁöÜ‰∏∫‰∫≤ÂèãÔºõ
(checkbox)001056ZH  ‰ªª‰ΩïÂä®Áâ©‰∏çÂæóÁùÄË°£Ôºõ
(checkbox)001057ZH  ‰ªª‰ΩïÂä®Áâ©‰∏çÂæóÂçßÂ∫äÔºõ
(checkbox)001058ZH  ‰ªª‰ΩïÂä®Áâ©‰∏çÂæóÈ•ÆÈÖíÔºõ
(checkbox)001059ZH  ‰ªª‰ΩïÂä®Áâ©‰∏çÂæó‰º§ÂÆ≥ÂÖ∂‰ªñÂä®Áâ©Ôºõ
(checkbox)001060ZH  ÊâÄÊúâÂä®Áâ©‰∏ÄÂæãÂπ≥Á≠â„ÄÇ
(checkbox)Nil       No matching sentence

(submit button) (clear selection button)
</code></pre>

<p>The user should be able to click 1 or more of the check boxes.
When the submit button is clicked the webpage will save a line in a appendable textfile in the format</p>

<p>SentID&lt;\TAB>@English_sentence&lt;&lt;\TAB>SentID2&lt;\TAB>=Chinese_sentence (e.g.:</p>

<pre><code>000325EN    @Whatever goes upon two legs is an enemy.   001054ZH    =Âá°Èù†‰∏§Êù°ËÖøË°åËµ∞ËÄÖÁöÜ‰∏∫‰ªáÊïåÔºõ
</code></pre>

<p>if there are more than 1 match to the English sentence, it may look like this</p>

<pre><code>000325EN    @Whatever goes upon two legs is an enemy.   001054ZH    =Âá°Èù†‰∏§Êù°ËÖøË°åËµ∞ËÄÖÁöÜ‰∏∫‰ªáÊïåÔºõ  001055ZH    =Âá°Èù†ÂõõËÇ¢Ë°åËµ∞ËÄÖÔºåÊàñËÄÖÈïøÁøÖËÜÄËÄÖÔºåÁöÜ‰∏∫‰∫≤ÂèãÔºõ
</code></pre>
",Multilingual Language Processing & Language Identification,gui sentence alignment task someone introduce write simple web html xml interface simple sentence alignment task task follows st line webpage would english sentence need matched chinese sentence user able click check box submit button clicked webpage save line appendable textfile format sentid tab english sentence tab sentid tab chinese sentence e g match english sentence may look like
NLP algorithm to &#39;fill out&#39; search terms,"<p>I'm trying to write an algorithm (which I'm assuming will rely on natural language processing techniques) to 'fill out' a list of search terms. There is probably a name for this kind of thing which I'm unaware of. What is this kind of problem called, and what kind of algorithm will give me the following behavior?</p>

<p>Input:</p>

<pre><code>    docs = [
    ""I bought a ticket to the Dolphin Watching cruise"",
    ""I enjoyed the Dolphin Watching tour"",
    ""The Miami Dolphins lost again!"",
    ""It was good going to that Miami Dolphins game""
    ], 
    search_term = ""Dolphin""
</code></pre>

<p>Output:</p>

<pre><code>[""Dolphin Watching"", ""Miami Dolphins""]
</code></pre>

<p>It should basically figure out that if ""Dolphin"" appears at all, it's virtually always either in the bigrams ""Dolphin Watching"" or ""Miami Dolphins"". Solutions in Python preferred.</p>
",Multilingual Language Processing & Language Identification,nlp algorithm fill search term trying write algorithm assuming rely natural language processing technique fill list search term probably name kind thing unaware kind problem called kind algorithm give following behavior input output basically figure dolphin appears virtually always either bigram dolphin watching miami dolphin solution python preferred
Detect Programming Language from code snippet,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/475033/detecting-programming-language-from-a-snippet"">Detecting programming language from a snippet</a>  </p>
</blockquote>



<p>Is there a way to identify the following for a text snippet?</p>

<ol>
<li>Whether it is a piece of code</li>
<li>The programming language in which it is written</li>
</ol>
",Multilingual Language Processing & Language Identification,detect programming language code snippet possible duplicate href programming language snippet way identify following text snippet whether piece code programming language written
Java: Multidimensional Scaling?,"<p>I'm doing a Natural Language Processing project where I compute a bunch of attributes of a text, giving me a vector of values for each text. I want to compare these vectors with multidimensional scaling. What Java libraries/toolkits do you recommend for doing this?</p>
",Multilingual Language Processing & Language Identification,java multidimensional scaling natural language processing project compute bunch attribute text giving vector value text want compare vector multidimensional scaling java library toolkits recommend
Looking for a way to optimize this algorithm for parsing a very large string,"<p>The following class parses through a very large string (an entire novel of text) and breaks it into consecutive 4-character strings that are stored as a Tuple. Then each tuple can be assigned a probability based on a calculation. I am using this as part of a monte carlo/ genetic algorithm to train the program to recognize a language based on syntax only (just the character transitions). </p>

<p>I am wondering if there is a faster way of doing this. It takes about 400ms to look up the probability of any given 4-character tuple. The relevant method _Probablity() is at the end of the class. </p>

<p>This is a computationally intensive problem related to another post of mine: <a href=""https://stackoverflow.com/questions/7423279/algorithm-for-computing-the-plausibility-of-a-function-monte-carlo-method"">Algorithm for computing the plausibility of a function / Monte Carlo Method</a></p>

<p>Ultimately I'd like to store these values in a 4d-matrix. But given that there are 26 letters in the alphabet that would be a HUGE task. (26x26x26x26). If I take only the first 15000 characters of the novel then performance improves a ton, but my data isn't as useful. </p>

<p>Here is the method that parses the text 'source':</p>

<pre><code>    private List&lt;Tuple&lt;char, char, char, char&gt;&gt; _Parse(string src)
    {
        var _map = new List&lt;Tuple&lt;char, char, char, char&gt;&gt;(); 

        for (int i = 0; i &lt; src.Length - 3; i++)
        {
          int j = i + 1;
          int k = i + 2;
          int l = i + 3;

          _map.Add
            (new Tuple&lt;char, char, char, char&gt;(src[i], src[j], src[k], src[l])); 
        }

        return _map; 
    }
</code></pre>

<p>And here is the _Probability method:</p>

<pre><code>    private double _Probability(char x0, char x1, char x2, char x3)
    {
        var subset_x0 = map.Where(x =&gt; x.Item1 == x0);
        var subset_x0_x1_following = subset_x0.Where(x =&gt; x.Item2 == x1);
        var subset_x0_x2_following = subset_x0_x1_following.Where(x =&gt; x.Item3 == x2);
        var subset_x0_x3_following = subset_x0_x2_following.Where(x =&gt; x.Item4 == x3);

        int count_of_x0 = subset_x0.Count();
        int count_of_x1_following = subset_x0_x1_following.Count();
        int count_of_x2_following = subset_x0_x2_following.Count();
        int count_of_x3_following = subset_x0_x3_following.Count(); 

        decimal p1;
        decimal p2;
        decimal p3;

        if (count_of_x0 &lt;= 0 || count_of_x1_following &lt;= 0 || count_of_x2_following &lt;= 0 || count_of_x3_following &lt;= 0)
        {
            p1 = e;
            p2 = e;
            p3 = e;
        }
        else
        {
            p1 = (decimal)count_of_x1_following / (decimal)count_of_x0;
            p2 = (decimal)count_of_x2_following / (decimal)count_of_x1_following;
            p3 = (decimal)count_of_x3_following / (decimal)count_of_x2_following;

            p1 = (p1 * 100) + e; 
            p2 = (p2 * 100) + e;
            p3 = (p3 * 100) + e; 
        }

        //more calculations omitted

        return _final; 
    }
}
</code></pre>

<p><b>EDIT</b> - I'm providing more details to clear things up,</p>

<p>1) Strictly speaking I've only worked with English so far, but its true that different alphabets will have to be considered. Currently I only want the program to recognize English, similar to whats described in this paper: <a href=""http://www-stat.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf"" rel=""nofollow noreferrer"">http://www-stat.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf</a></p>

<p>2) I am calculating the probabilities of n-tuples of characters where n &lt;= 4. For instance if I am calculating the total probability of the string ""that"", I would break it down into these independent tuples and calculate the probability of each individually first:</p>

<p>[t][h] </p>

<p>[t][h][a]</p>

<p>[t][h][a][t]</p>

<p>[t][h] is given the most weight, then [t][h][a], then [t][h][a][t]. Since I am not just looking at the 4-character tuple as a single unit, I wouldn't be able to just divide the instances of [t][h][a][t] in the text by the total no. of 4-tuples in the next. </p>

<p>The value assigned to each 4-tuple can't overfit to the text, because by chance many real English words may never appear in the text and they shouldn't get disproportionally low scores. Emphasing first-order character transitions (2-tuples) ameliorates this issue. Moving to the 3-tuple then the 4-tuple just refines the calculation. </p>

<p>I came up with a Dictionary that simply tallies the count of how often the tuple occurs in the text (similar to what Vilx suggested), rather than repeating identical tuples which is a waste of memory. That got me from about ~400ms per lookup to about ~40ms per, which is a pretty great improvement. I still have to look into some of the other suggestions, however. </p>
",Multilingual Language Processing & Language Identification,looking way optimize algorithm parsing large string following class par large string entire novel text break consecutive character string stored tuple tuple assigned probability based calculation using part monte carlo genetic algorithm train program recognize language based syntax character transition wondering faster way take look probability given character tuple relevant method probablity end class computationally intensive problem related another post mine calculating probability n tuples character n instance calculating total probability string would break independent tuples calculate probability individually first h h h h given weight h h since looking character tuple single unit able divide instance h text total tuples next value assigned tuple overfit text chance many real english word may never appear text get disproportionally low score emphasing first order character transition tuples ameliorates issue moving tuple tuple refines calculation came dictionary simply tally count often tuple occurs text similar vilx suggested rather repeating identical tuples waste memory got per lookup per pretty great improvement still look suggestion however
How to Join Arabic letters to form words,"<p>I have to read arabic letters from xml file and display them as a word </p>

<p>input :ÿ≥ ÿπ ÿß ÿØ ÿ©
output :ÿ≥ÿπÿßÿØÿ© look like that ..</p>

<p>I dont know how do that in any language , what algorithm to read, I need some start point to acomplish this task </p>

<p>I am also not sure if i have added the right tags, please free to make changes.</p>
",Multilingual Language Processing & Language Identification,join arabic letter form word read arabic letter xml file display word input output look like dont know language algorithm read need start point acomplish task also sure added right tag please free make change
"Pure statistical, or Natural Language Processing engine?","<p>What are the statistical engines that yield better results than the OpenNLP suite of tools, if any? What I'm looking for is an engine that picks keywords from texts and provides stemming on those verbs &amp; nouns, perhaps Natural Language Processing is not the way to go here. The engine should also work with different languages.</p>
",Multilingual Language Processing & Language Identification,pure statistical natural language processing engine statistical engine yield better result opennlp suite tool looking engine pick keywords text provides stemming verb noun perhaps natural language processing way go engine also work different language
Where can I find texts that describe topic-specific events? ,"<p>So, some background: I'm trying to train a ML system to answer questions about events, where both the event descriptions and questions are posed in natural language; the event descriptions are constrained to being single sentences.</p>

<p>So far the main problem with this has been locating a corpus that describes events with a limited enough vocabulary to pose similar questions across all of the events (e.g. if all of the events involved chess, I could reasonably ask 'what piece moved?' and an answer could be drawn from a decent percentage of the event description sentences).  </p>

<p>With that in mind, I'm hoping to find a text source that is tightly focused around describing events within some fairly limited topic (more along the lines of chess commentary than a chess forum, for example). </p>

<p>While I've had some luck with a corpus of <a href=""http://www.ldc.upenn.edu/Catalog/byType.jsp"" rel=""nofollow"">air-traffic controller dialogs</a>, most of sentences aren't typical English (they involve a lot of Charlie, Tango, etc.). However, if the format is as I've described then the actual topic of focus is irrelevant, so long as it has one.</p>

<p>Since I plan on building my own corpus out of this text, no tagging is necessary. </p>
",Multilingual Language Processing & Language Identification,find text describe topic specific event background trying train ml system answer question event event description question posed natural language event description constrained single sentence far main problem ha locating corpus describes event limited enough vocabulary pose similar question across event e g event involved chess could reasonably ask piece moved answer could drawn decent percentage event description sentence mind hoping find text source tightly focused around describing event within fairly limited topic along line chess commentary chess forum example luck corpus air traffic controller dialog sentence typical english involve lot charlie tango etc however format described actual topic focus irrelevant long ha one since plan building corpus text tagging necessary
Need resources for Statistical Natural Language Processing,"<p>I'm writing a program in Java that needs to parse natural language. I need this to be done using probability and statistics. Are there any resources that can easily explain Statistical Natural Language Processing techniques?</p>
",Multilingual Language Processing & Language Identification,need resource statistical natural language processing writing program java need parse natural language need done using probability statistic resource easily explain statistical natural language processing technique
Intelligent transliteration in PHP,"<p>I'm interested in writing a PHP script (I do welcome language-agnostic suggestions) that would transliterate a sentence or word written in English (phoenetically) into the script of another language. Since I'm looking at English written phoenetically (i.e. by ear): I'd have to deal with variant spellings of the same word.</p>

<p>It is assumed that no standard exists for romanization (for instance, in Chinese, you have the Simplified Wade, etc.) </p>

<p>Does anyone have any advice on where I could start? </p>

<p>EDIT: I'm doing this purely for educational purposes, and I was initially under the impression that in order to figure out the connection between variant spellings (which could be found in a corpus of IM messages, Facebook posts written in the romanized form of the language), you'd need some sort of machine learning tool. However, I'd like to know if I was on the right track, and I'd like some help in figuring out what next I should look into to get this working (for instance: which machine learning tool should I look into?). </p>
",Multilingual Language Processing & Language Identification,intelligent transliteration php interested writing php script welcome language agnostic suggestion would transliterate sentence word written english phoenetically script another language since looking english written phoenetically e ear deal variant spelling word assumed standard exists romanization instance chinese simplified wade etc doe anyone advice could start edit purely educational purpose wa initially impression order figure connection variant spelling could found corpus im message facebook post written romanized form language need sort machine learning tool however like know wa right track like help figuring next look get working instance machine learning tool look
Comparing two English strings for similarities,"<p>So here is my problem. I have two paragraphs of text and I need to see if they are similar. Not in the sense of string metrics but in meaning. The following two paragraphs are related but I need to find out if they cover the 'same' topic. Any help or direction to solving this problem would be greatly appreciated. </p>

<blockquote>
  <p>Fossil fuels are fuels formed by natural processes such as anaerobic
  decomposition of buried dead organisms. The age of the organisms and
  their resulting fossil fuels is typically millions of years, and
  sometimes exceeds 650 million years. The fossil fuels, which contain
  high percentages of carbon, include coal, petroleum, and natural gas.
  Fossil fuels range from volatile materials with low carbon:hydrogen
  ratios like methane, to liquid petroleum to nonvolatile materials
  composed of almost pure carbon, like anthracite coal. Methane can be
  found in hydrocarbon fields, alone, associated with oil, or in the
  form of methane clathrates. It is generally accepted that they formed
  from the fossilized remains of dead plants by exposure to heat and
  pressure in the Earth's crust over millions of years. This biogenic
  theory was first introduced by Georg Agricola in 1556 and later by
  Mikhail Lomonosov in the 18th century.</p>
</blockquote>

<p>Second:</p>

<blockquote>
  <p>Fossil fuel reforming is a method of producing hydrogen or other
  useful products from fossil fuels such as natural gas. This is
  achieved in a processing device called a reformer which reacts steam
  at high temperature with the fossil fuel. The steam methane reformer
  is widely used in industry to make hydrogen. There is also interest in
  the development of much smaller units based on similar technology to
  produce hydrogen as a feedstock for fuel cells. Small-scale steam
  reforming units to supply fuel cells are currently the subject of
  research and development, typically involving the reforming of
  methanol or natural gas but other fuels are also being considered such
  as propane, gasoline, autogas, diesel fuel, and ethanol.</p>
</blockquote>
",Multilingual Language Processing & Language Identification,comparing two english string similarity problem two paragraph text need see similar sense string metric meaning following two paragraph related need find cover topic help direction solving problem would greatly appreciated fossil fuel fuel formed natural process anaerobic decomposition buried dead organism age organism resulting fossil fuel typically million year sometimes exceeds million year fossil fuel contain high percentage carbon include coal petroleum natural gas fossil fuel range volatile material low carbon hydrogen ratio like methane liquid petroleum nonvolatile material composed almost pure carbon like anthracite coal methane found hydrocarbon field alone associated oil form methane clathrates generally accepted formed fossilized remains dead plant exposure heat pressure earth crust million year biogenic theory wa first introduced georg agricola later mikhail lomonosov th century second fossil fuel reforming method producing hydrogen useful product fossil fuel natural gas achieved processing device called reformer reacts steam high temperature fossil fuel steam methane reformer widely used industry make hydrogen also interest development much smaller unit based similar technology produce hydrogen feedstock fuel cell small scale steam reforming unit supply fuel cell currently subject research development typically involving reforming methanol natural gas fuel also considered propane gasoline autogas diesel fuel ethanol
Parsing Natural Language Music Citations Using Regex,"<p>I am struggling with nailing down a fairly complex regular expression to parse song titles with optional artist attribution from loosely-typed English. The user input comes from a single text field and the regex matches will be used to query a song database to get unique track IDs. I need to be able to get these matches:</p>
<ul>
<li><code>\1</code> = song title</li>
<li><code>\2</code> = artist</li>
</ul>
<p>while being fairly liberal in allowed formats.</p>
<h2>Examples</h2>
<p>The wold &quot;by&quot; should split the string into song title and artist (but only on word boundaries); as should a comma with/without trailing whitespace:</p>
<blockquote>
<p>baby one more time by britney spears</p>
<p>baby one more time, britney spears</p>
<p>baby one more time,britney spears</p>
</blockquote>
<ul>
<li><code>\1</code> = baby one more time</li>
<li><code>\2</code> = britney spears</li>
</ul>
<p>False positives like these are acceptable:</p>
<blockquote>
<p>down by the bay</p>
</blockquote>
<ul>
<li><code>\1</code> = down</li>
<li><code>\2</code> = the bay</li>
</ul>
<blockquote>
<p>whatever people say i am, that's what i'm not</p>
</blockquote>
<ul>
<li><code>\1</code> = whatever people say i am</li>
<li><code>\2</code> = that's what i'm not</li>
</ul>
<p>‚Ä¶assuming quotes can be used to mark a run of text as a song title explicitly:</p>
<blockquote>
<p>&quot;down by the bay&quot;</p>
</blockquote>
<ul>
<li><code>\1</code> = down by the bay</li>
<li><code>\2</code> not matched</li>
</ul>
<blockquote>
<p>&quot;whatever people say i am, that's what i'm not&quot; by arctic monkeys</p>
</blockquote>
<ul>
<li><code>\1</code> = whatever people say i am, that's what i'm not</li>
<li><code>\2</code> = arctic monkeys</li>
</ul>
<p>Single quotes should work too, but obviously not if they appear within the title:</p>
<blockquote>
<p>'whatever people say i am, that's what i'm not'</p>
</blockquote>
<ul>
<li><code>\1</code> = whatever people say i am, that</li>
<li><code>\2</code> = s what i'm not'</li>
</ul>
<p>Additionally, if quotes are in use, the word &quot;by&quot; or a comma are optional:</p>
<blockquote>
<p>&quot;down by the bay&quot; raffi</p>
</blockquote>
<ul>
<li><code>\1</code> = down by the bay</li>
<li><code>\2</code> = raffi</li>
</ul>
<p>However, if there are no quotes, and more than one &quot;by&quot;, then only the last &quot;by&quot; should be used as a delimiter:</p>
<blockquote>
<p>down by the bay by raffi</p>
</blockquote>
<ul>
<li><code>\1</code> = down by the bay</li>
<li><code>\2</code> = raffi</li>
</ul>
<p>Is this even possible with a single regex? Or would the more sane way be to split it up into multiple expressions? Either way, what might this look like?</p>
",Multilingual Language Processing & Language Identification,parsing natural language music citation using regex struggling nailing fairly complex regular expression parse song title optional artist attribution loosely typed english user input come single text field regex match used query song database get unique track id need able get match song title artist fairly liberal allowed format example wold split string song title artist word boundary comma without trailing whitespace baby one time britney spear baby one time britney spear baby one time britney spear baby one time britney spear false positive like acceptable bay bay whatever people say whatever people say assuming quote used mark run text song title explicitly bay bay matched whatever people say arctic monkey whatever people say arctic monkey single quote work obviously appear within title whatever people say whatever people say additionally quote use word comma optional bay raffi bay raffi however quote one last used delimiter bay raffi bay raffi even possible single regex would sane way split multiple expression either way might look like
Should a Chunker find the head of a phrase?,"<p>My application requires that I point the head of a phrase (noum or verb). I have this kind of info in my Portuguese corpus:</p>

<p>Me pron-pers *B-NP <br/>
pergunto v-fin B-VP <br/>
sempre adv *B-ADVP <br/>
quem pron-indp *B-NP <br/>
podia v-fin B-VP <br/>
ter v-inf I-VP <br/>
sido v-pcp I-VP <br/>
aquele pron-det B-NP <br/>
jovem adj I-NP <br/>
alem√£o n *I-NP <br/>
. . O <br/></p>

<p>The syntax is similar to CONLL 2000, but the * marks the head of the phrase.
My question is: should a Chunker support head? Do you know any other corpus to train a Chunker that also includes head, or it is a particularity of mine?</p>

<p>-- edit --</p>

<p>I tried training the classifier and got good results: F1 score was 0.94 without head mark and 0.93 with it. I think it is OK. The problem is that the OpenNLP chunker API does not support this mark and gets confused while creating the spans. I changed the OpenNLP code to handle it and I was wondering if it is a good patch, but since it is not common I should not send the patch.</p>
",Multilingual Language Processing & Language Identification,chunker find head phrase application requires point head phrase noum verb kind info portuguese corpus pron pers b np pergunto v fin b vp sempre adv b advp quem pron indp b np podium v fin b vp ter v inf vp sido v pcp vp aquele pron det b np jovem adj np alem n np syntax similar conll mark head phrase question chunker support head know corpus train chunker also includes head particularity mine edit tried training classifier got good result f score wa without head mark think ok problem opennlp chunker api doe support mark get confused creating span changed opennlp code handle wa wondering good patch since common send patch
"For Java, there is a tokenizator that is matches exactly what I want?","<p>I'm want to tokenize a text, but not separating only with whitespaces. </p>

<p>There some things like proper names that I want to set only one token (eg.: ""Renato Dinhani Concei√ß√£o""). Another case: percentual (""60 %"") and not split into two tokens.</p>

<p>What I want to know if there is a Tokenizator from some libray that can provide high customization? If not, I will try to write my own, if there is some interface or practices to follow.</p>

<p>Not everything need to be universal recognition. Example: I don't need to reconigze chinese alphabet.</p>

<p>My application is a college application and it is mainly directed to portuguese language. Only some things like names, places and similars will be from another languages.</p>
",Multilingual Language Processing & Language Identification,java tokenizator match exactly want want tokenize text separating whitespaces thing like proper name want set one token eg renato dinhani concei another case percentual split two token want know tokenizator libray provide high customization try write interface practice follow everything need universal recognition example need reconigze chinese alphabet application college application mainly directed portuguese language thing like name place similars another language
Least used unicode delimiter,"<p>I'm trying to tag my text with a delimiter at specific places that will be used later for parsing. I want to use a delimiter character that is least frequently used. I'm currently looking at the ""\2"" or the U+0002 character. Is that safe enough to use? What other suggestions are there? The text is unicode and will have both english and non-english characters.</p>

<p>A want to use a character that can still be ""exploded()"" by PHP.</p>

<p><strong>Edit:</strong></p>

<p>Also I want to be able to display this piece of text on screen (to the browser) and the delimiter will be ""invisible"" to the user. I can definitely use a str_replace() to get rid of visible delimiters, but if there are good invisible delimiters, then no such processing is needed.</p>
",Multilingual Language Processing & Language Identification,least used unicode delimiter trying tag text delimiter specific place used later parsing want use delimiter character least frequently used currently looking u character safe enough use suggestion text unicode english non english character want use character still exploded php edit also want able display piece text screen browser delimiter invisible user definitely use str replace get rid visible delimiters good invisible delimiters processing needed
Extracting &#39;useful&#39; information out of sentences?,"<p>I am currently trying to understand sentences of this form: </p>

<p><code>The problem was more with the set-top box than the television. Restarting the set-top box solved the problem.</code></p>

<p>I am totally new to Natural Language Processing and started using Python's NLTK package to get my hands dirty. However, I am wondering if someone could give me an overview of the high-level steps involved in achieving this.</p>

<p>What I am trying to do is to identify what the problem was so in this case, <code>set-top box</code> and whether the action that was taken resolved the problem so in this case, <code>yes</code> because restarting fixed the problem. So if all the sentences were of this form, my life would have been easier but because it is natural language, the sentences could also be of the following form:</p>

<p><code>I took a look at the car and found nothing wrong with it. However, I suspect there is something wrong with the engine</code></p>

<p>So in this case, the problem was with the <code>car</code>. The action taken did not resolve the problem because of the presence of the word <code>suspect</code>. And the potential problem could be with the <code>engine</code>.</p>

<p>I am not looking for an absolute answer as I suspect this is very complex. What I am looking for is more rather a high-level overview that will point me in the right direction. If there is an easier/alternate way to do this, that is welcome as well.</p>
",Multilingual Language Processing & Language Identification,extracting useful information sentence currently trying understand sentence form totally new natural language processing started using python nltk package get hand dirty however wondering someone could give overview high level step involved achieving trying identify problem wa case whether action wa taken resolved problem case restarting fixed problem sentence form life would easier natural language sentence could also following form case problem wa action taken resolve problem presence word potential problem could looking absolute answer suspect complex looking rather high level overview point right direction easier alternate way welcome well
Scraping English Words using Python,"<p>I would like to scrape all English words from, say, New York Times front page. I wrote something like this in Python:</p>

<pre><code>import re
from urllib import FancyURLopener

class MyOpener(FancyURLopener):
    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'            

opener = MyOpener()
url = ""http://www.nytimes.com""
h = opener.open(url)
content = h.read()
tokens = re.findall(""\s*(\w*)\s*"", content, re.UNICODE) 
print tokens
</code></pre>

<p>This works okay, but I get HTML keywords such as ""img"", ""src"" as well as English words. Is there a simple way to get only English words from Web scaping / HTML ? </p>

<p>I saw <a href=""https://stackoverflow.com/questions/5635400/scraping-with-python"">this</a> post, it only seems to talk about the mechanics of scraping, none of the tools mentioned talk about how to filter out non-language elements. I am not interested in links, formatting, etc. Just plain words. Any help would be appreciated. </p>
",Multilingual Language Processing & Language Identification,scraping english word using python would like scrape english word say new york time front page wrote something like python work okay get html keywords img src well english word simple way get english word web scaping html saw href post seems talk mechanic scraping none tool mentioned talk filter non language element interested link formatting etc plain word help would appreciated p
ARFF for natural language processing,"<p>I'm trying to take a set of reviews, and convert them into the ARFF format for use with WEKA. Unfortunately either I completely misunderstand how the format works, or I'll have to have an attribute for ALL possible words, then a presence indicator. Does anyone know a better way, or ideally have a sample ARFF file?</p>
",Multilingual Language Processing & Language Identification,arff natural language processing trying take set review convert arff format use weka unfortunately either completely misunderstand format work attribute possible word presence indicator doe anyone know better way ideally sample arff file
Word lists for a lot of articles - document-term matrix,"<p>I have nearly 150k articles in Turkish. I will use articles for natural language processing research.
I want to store words and frequency of them per article after processing articles.</p>

<p>I'm storing them in RDBS now.</p>

<p>I have 3 tables:</p>

<p>Articles -> article_id,text<br>
Words -> word_id, type, word<br>
Words-Article -> id, word_id, article_id, frequency (index for word_id, index for article_id )  </p>

<p>I will query for  </p>

<ul>
<li>ALL Words in an article   </li>
<li>one Word's frequency per article  </li>
<li>Word occurrences in all articles and in which articles</li>
</ul>

<p>I have millions of rows in words-article table. I always worked with RDBS in this project. started with mysql and using oracle now. But I don't want to use oracle and want better performance than mysql.</p>

<p>Also I have to handle this job in a machine with 4gb ram.<br>
Simply, how to store document-term matrix and make some query on it? performance is necessary. can ""key-value databases"" beat mysql at performance?  or what can beat mysql?</p>

<p>if your answer programming language depended, I'm writing code in python. But C/C++ , Java is ok. </p>
",Multilingual Language Processing & Language Identification,word list lot article document term matrix nearly k article turkish use article natural language processing research want store word frequency per article processing article storing rdbs table article article id text word word id type word word article id word id article id frequency index word id index article id query word article one word frequency per article word occurrence article article million row word article table always worked rdbs project started mysql using oracle want use oracle want better performance mysql also handle job machine gb ram simply store document term matrix make query performance necessary key value database beat mysql performance beat mysql answer programming language depended writing code python c c java ok
English lemmatizer databases?,"<p>Do you know any big enough lemmatizer database that returns correct result for following sample words:</p>

<pre><code>geese: goose
plantes: //not found
</code></pre>

<p>Wordnet's morphological analyzer is not sufficient, since it gives the following incorrect results:</p>

<pre><code>geese: //not found
plantes: plant
</code></pre>
",Multilingual Language Processing & Language Identification,english lemmatizer database know big enough lemmatizer database return correct result following sample word wordnet morphological analyzer sufficient since give following incorrect result
Wordnet in Spanish,"<p>Is there something similar to Princeton's WordNet in Spanish?
I need to find synonyms in Spanish. </p>
",Multilingual Language Processing & Language Identification,wordnet spanish something similar princeton wordnet spanish need find synonym spanish
Tagging and Analysing a Search Query,"<p>I'm developing a search engine which functions taking the semantics of data into account, unlike the usual keyword based index. I managed to develop a reasonable index for the search using <strong>metadata extraction methods and RDF</strong>, but I have <strong>difficulty in using such methods on the search query itself since the search query is very much shorter</strong> that the actual data. any idea how to perform a successful tagging of a search query, using similar methods, natural language processing, etc. ?</p>

<p>Thank You!</p>
",Multilingual Language Processing & Language Identification,tagging analysing search query developing search engine function taking semantics data account unlike usual keyword based index managed develop reasonable index search using metadata extraction method rdf difficulty using method search query since search query much shorter actual data idea perform successful tagging search query using similar method natural language processing etc thank
"Python or Java for text processing (text mining, information retrieval, natural language processing)","<p>I'm soon to start on a new project where I am going to do lots of text processing tasks like searching, categorization/classifying, clustering, and so on. </p>

<p>There's going to be a huge amount of documents that need to be processed; probably millions of documents. After the initial processing, it also has to be able to be updated daily with multiple new documents.</p>

<p>Can I use Python to do this, or is Python too slow? Is it best to use Java?</p>

<p>If possible, I would prefer Python since that's what I have been using lately. Plus, I would finish the coding part much faster. But it all depends on Python's speed. I have used Python for some small scale text processing tasks with only a couple of thousand documents, but I am not sure how well it scales up.</p>
",Multilingual Language Processing & Language Identification,python java text processing text mining information retrieval natural language processing soon start new project going lot text processing task like searching categorization classifying clustering going huge amount document need processed probably million document initial processing also ha able updated daily multiple new document use python python slow best use java possible would prefer python since using lately plus would finish coding part much faster depends python speed used python small scale text processing task couple thousand document sure well scale
Text Game Commands Parsing?,"<p>I'm interested in writing a parsing engine such as the ones in Inform, which is very robust.</p>

<p>I've tried a couple of different approach: </p>

<ul>
<li>regex. For regex, I can only match <code>{verb} {noun1 optional-noun2} {optional-preposition} {indirect-noun1 optional-indirect-noun2}</code> This doesn't work that well.</li>
<li>list of predefined verbs and their matched preposition and nouns. This approach is too complex, since there has to be A LOT of predefined data.</li>
<li>splitting into different words and run them through a dictionary maps. Again this is too complex and not practical due to the amiguity of certain words such as ""look"" or ""put (on, out)""</li>
</ul>

<p>I know about language processing though I have actually no idea how to start and some of the stuff are too complicated. (No CS background, I'm just learning on my own right now).</p>

<p>Some ideas I have:</p>

<ul>
<li>As I've said, natural language processing.</li>
<li>Design a parser that parses something similar to SQL.</li>
</ul>

<p>Is there any other ideas? How would I go about design and implementing them?</p>

<p>Note I'm using Python as my language. and in case you missed it, I'm not design a programming language, I'm just parsing user command such as ""go north"" or complex stuff, like ""put the fire out with the blanket""</p>
",Multilingual Language Processing & Language Identification,text game command parsing interested writing parsing engine one inform robust tried couple different approach regex regex match work well list predefined verb matched preposition noun approach complex since ha lot predefined data splitting different word run dictionary map complex practical due amiguity certain word look put know language processing though actually idea start stuff complicated c background learning right idea said natural language processing design parser par something similar sql idea would go design implementing note using python language case missed design programming language parsing user command go north complex stuff like put fire blanket
how do I change hungggrrrrryyyy = hungry and other such english words,"<p>The problem is simpler to understand, but I think is difficult to solve. Given a word how to form a proper english word. example:   </p>

<pre><code>hunggrrryyy to hungry  
awweeeeseom to awesome  
frusstrated to frustrated  
looooooove  to love  
</code></pre>

<p>Are there any known solutions to such problem?  </p>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,change hungggrrrrryyyy hungry english word problem simpler understand think difficult solve given word form proper english word example known solution problem thanks
Book translation data format,"<p>I'm thinking of translating a book from English to my native language. I can translate just fine, and I'm happy with <code>vim</code> as a text editor. My problem is that I'd like to somehow preserve the semantics, i.e. which parts of my translation correspond to the original.</p>

<p>I could basically create a simple XML-based markup language, that'd look something like</p>

<pre><code>&lt;book&gt;
  &lt;chapter&gt;
    &lt;paragraph&gt;
      &lt;sentence&gt;
        &lt;original&gt;This is an example sentence.&lt;/original&gt;
        &lt;translation lang=""fi""&gt;T√§m√§ on esimerkkilause.&lt;/translation&gt;
      &lt;/sentence&gt;
    &lt;/paragraph&gt;
  &lt;/chapter&gt;
&lt;/book&gt;
</code></pre>

<p>Now, that would probably have its benefits but I don't think editing that would be very fun.</p>

<p>Another possibility that I can think of would be to keep the original and translation in separate files. If I add a newline after each translation chunk and keep line numbering consistent, editing would be easy and I'd be able to programmatically match the original and translation.</p>

<pre><code>original.txt:
  This is an example sentence.
  In this format editing is easy.

translation-fi.txt:
  T√§m√§ on esimerkkilause.
  T√§ss√§ muodossa muokkaaminen on helppoa.
</code></pre>

<p>However, this doesn't seem very robust. It would be easy to mess up. Probably someone has better ideas. Thus the question:</p>

<p><strong>What would be the best data format for making a book translation with a text editor?</strong></p>

<p>EDIT: added tag <code>vim</code>, since I'd prefer to do this with vim and believe that some vim guru might have ideas.</p>

<p>EDIT2: started a bounty on this. I'm currently leaning to the second idea I describe, but I hope to get something about as easy to edit (and quite easy to implement) but more robust.</p>
",Multilingual Language Processing & Language Identification,book translation data format thinking translating book english native language translate fine happy text editor problem like somehow preserve semantics e part translation correspond original could basically create simple xml based markup language look something like would probably benefit think editing would fun another possibility think would keep original translation separate file add newline translation chunk keep line numbering consistent editing would easy able programmatically match original translation however seem robust would easy mess probably someone ha better idea thus question would best data format making book translation text editor edit added tag since prefer vim believe vim guru might idea edit started bounty currently leaning second idea describe hope get something easy edit quite easy implement robust
Simple language identification using LINQ,"<p>I'm experimenting with LINQ for the first time and decided to try basic human language identification. The input text gets tested against <code>HashSet</code>s of the most common 10,000 words in the language and receives a score.</p>

<p>My question is, is there a better approach to the LINQ query? Maybe the other form that I don't know? It works, but I'm sure that the experts here will be able to provide a much cleaner solution!</p>

<pre><code>public PolyAnalyzer() {
    Dictionaries = new Dictionary&lt;string, AbstractDictionary&gt;();
    Dictionaries.Add(""Bulgarian"", new BulgarianDictionary());
    Dictionaries.Add(""English"", new EnglishDictionary());
    Dictionaries.Add(""German"", new GermanDictionary());
    Dictionaries.Values.Select(n =&gt; new Thread(() =&gt; n.LoadDictionaryAsync())).ToList().ForEach(n =&gt; n.Start());            
}  

public string getResults(string text) {
    int total = 0;
    return string.Join("" "",
        Dictionaries.Select(n =&gt; new {
            Language = n.Key,
            Score = new Regex(@""\W+"").Split(text).AsQueryable().Select(m =&gt; n.Value.getScore(m)).Sum()
        }).
        Select(n =&gt; { total += n.Score; return n; }).
        ToList().AsQueryable(). // Force immediate evaluation
        Select(n =&gt;
        ""["" + n.Score * 100 / total + ""% "" + n.Language + ""]"").
        ToArray());
}
</code></pre>

<p>P.S. I'm aware that this is an extremely simplistic approach to language identification, I'm just interested in the LINQ side of things.</p>
",Multilingual Language Processing & Language Identification,simple language identification using linq experimenting linq first time decided try basic human language identification input text get tested common word language receives score question better approach linq query maybe form know work sure expert able provide much cleaner solution p aware extremely simplistic approach language identification interested linq side thing
What is the most accurate open-source tool for sentence splitting?,"<p>I need to split text into sentences. I'm currently playing around with OpenNLP's sentence detector tool. I've also heard of NLTK and Stanford CoreNLP tools. What is the most accurate English sentence detection tools out there? I don't need too many NLP features--only a good tool for sentence splitting/detection.</p>

<p>I've also heard about Lucene...but that may be too much. But if it has a kick-ass sentence detection module, then I'll use it.</p>
",Multilingual Language Processing & Language Identification,accurate open source tool sentence splitting need split text sentence currently playing around opennlp sentence detector tool also heard nltk stanford corenlp tool accurate english sentence detection tool need many nlp feature good tool sentence splitting detection also heard lucene may much ha kick sentence detection module use
Simple library for Natural Language Processing in C#,"<p>I am working on a project which requires simple sort of NLP. The features that I need include: </p>

<ul>
<li>Sentence splitter</li>
<li>Phrase splitter (not word splitter)</li>
<li>Phrase nature identifier (common noun, proper noun, verb etc.)</li>
</ul>

<p>I am aware of libraries like SharpNLP, NLPTK and Antelope but all of them are too big and fancy for my project. Can someone suggest a simple one that provides only the above mentioned features. If it is a bit bigger and provides other features that's ok but I guess having a 139MB large library for this simple stuff might not be a good idea, especially when this project is already going to be complex in other areas.</p>

<p>Thanks in advance for any ideas.</p>
",Multilingual Language Processing & Language Identification,simple library natural language processing c working project requires simple sort nlp feature need include sentence splitter phrase splitter word splitter phrase nature identifier common noun proper noun verb etc aware library like sharpnlp nlptk antelope big fancy project someone suggest simple one provides mentioned feature bit bigger provides feature ok guess mb large library simple stuff might good idea especially project already going complex area thanks advance idea
Possible anticrawler,"<p>For an educational NLP project I need a list of all Italian words. I thought I would write a crawler that will get the words from www.wordreference.com. I use Python with the mechanize crawler framework. but when i use the code:</p>

<pre><code> br = mechanize.Browser()
 br.open(""http://www.wordreference.com/iten/abaco"")
 html = br.response().get_data()
 print html
</code></pre>

<p>I get some page from ""yahoo.com"". is it possible this website has an anticrawler mechanism? </p>
",Multilingual Language Processing & Language Identification,possible anticrawler educational nlp project need list italian word thought would write crawler get word use python mechanize crawler framework use code get page yahoo com possible website ha anticrawler mechanism
Ruby: is there a stemmer that &quot;knows&quot; English irregular verbs?,"<p>There is a ruby stemmer <a href=""https://github.com/aurelian/ruby-stemmer"" rel=""nofollow"">https://github.com/aurelian/ruby-stemmer</a>, but it 1) does not stem English irregular verbs 2) fails to build native extensions on Windows. Is there an alternative that fixes at least one of the problems?</p>
",Multilingual Language Processing & Language Identification,ruby stemmer know english irregular verb ruby stemmer doe stem english irregular verb fails build native extension window alternative fix least one problem
Which of these projects should I choose for summer workshop on NLP?,"<p>I am planning to attend a project oriented advanced summer workshop here in India on Natural Language Processing.
Before start of the workshop, I have to make a project preference out of the following four areas about which I have limited knowledge.</p>

<blockquote>
  <p><strong>Machine Translation</strong> Develop an English-Indian language translation
  system.</p>
  
  <p><strong>Parsing</strong> Build an Indian Language (IL) Parser.</p>
  
  <p><strong>Morphological Analysis</strong> Develop and test Morphological Analyzers for
  Indian Languages.</p>
  
  <p><strong>Speech</strong> Spoken Dialog Systems, Emotion/Prosody Detection, Synthesis
  and Conversion</p>
</blockquote>

<p>I have taken a course in Artificial Intelligence where NLP was introduced and fundamental sub-topics like <em>POS tagging(Transformation Based Learning), word prediction using N-grams, Hidden Markov Models, Viterbi Algorithm, Natural Language Parsing, Context Free Grammar, CKY Algorithm</em> were covered.</p>

<p>I understand this is a slightly vague question and the choice would depend primarily on my interests, but would appreciate guidance on which area would be better in terms of the research scope, practical application, industry opportunities etc. </p>

<p>EDIT: Application of skills/experience acquired while working on the project, outside NLP would also be a factor in the decision.</p>
",Multilingual Language Processing & Language Identification,project choose summer workshop nlp planning attend project oriented advanced summer workshop india natural language processing start workshop make project preference following four area limited knowledge machine translation develop english indian language translation system parsing build indian language il parser morphological analysis develop test morphological analyzer indian language speech spoken dialog system emotion prosody detection synthesis conversion taken course artificial intelligence nlp wa introduced fundamental sub topic like po tagging transformation based learning word prediction using n gram hidden markov model viterbi algorithm natural language parsing context free grammar cky algorithm covered understand slightly vague question choice would depend primarily interest would appreciate guidance area would better term research scope practical application industry opportunity etc edit application skill experience acquired working project outside nlp would also factor decision
Culture-independent stemmer/analyzer for Lucene.NET,"<p>We're currently developing a full-text-search-enabled app and we Lucene.NET is our weapon of choice. What's expected is that an app will be used by people from different countries, so Lucene.NET has to be able to search across Russian, English and other texts equally well.</p>

<p>Are there any universal and culture-independent stemmers and analyzers to suit our needs? I understand that eventually we'd have to use culture-specific ones, but we want to get up and running with this potentially quick and dirty approach.</p>
",Multilingual Language Processing & Language Identification,culture independent stemmer analyzer lucene net currently developing full text search enabled app lucene net weapon choice expected app used people different country lucene net ha able search across russian english text equally well universal culture independent stemmer analyzer suit need understand eventually use culture specific one want get running potentially quick dirty approach
Can I digitalize a dictionary?,"<p>I've found a public domain latin&lt;->portuguese dictionary in PDF which I'd like to convert to plain text, parse and use as the database of a program. After some testing, however, I got a little skeptical. Take a look at the <a href=""https://i.sstatic.net/ynNq4.png"" rel=""nofollow"">original file</a> and at the <a href=""http://dabc50b2975da0ae.paste.se/"" rel=""nofollow"">resulting text of gocr</a>. Is there any hope that I might reach 99%+ accuracy in some method? I thought of reCaptcha's database, but I guess it is Google's property, isn't it?</p>

<p>Thanks!</p>
",Multilingual Language Processing & Language Identification,digitalize dictionary found public domain latin portuguese dictionary pdf like convert plain text parse use database program testing however got little skeptical take look original file resulting text gocr hope might reach accuracy method thought recaptcha database guess google property thanks
How to ensure user submit only english text,"<p>I am building a project involving natural language processing, since the nlp module currently only deal with english text, so I have to make sure the user submitted content (not long, only several words) is in english. Are there established ways to achieve this? Python or Javascript way preferred.</p>
",Multilingual Language Processing & Language Identification,ensure user submit english text building project involving natural language processing since nlp module currently deal english text make sure user submitted content long several word english established way achieve python javascript way preferred
natural language processing fix for combined words,"<p>I have some text that was generate by another system.  It combined some words together in what I assume was some sort of wordwrap by-product.  So something simple like 'the dog' is combine into 'thedog'. </p>

<p>I checked the ascii and unicode string to see is there wasn't some unseen character in there, but there wasn't.  A confounding problem is that this is medical text and a corpus to check against aren't that available.  So, real example is '...test to rule out SARS versus pneumonia' ends up as '... versuspneumonia.'</p>

<p>Anyone have a suggestion for finding and separating these?</p>
",Multilingual Language Processing & Language Identification,natural language processing fix combined word text wa generate another system combined word together assume wa sort wordwrap product something simple like dog combine thedog checked ascii unicode string see unseen character confounding problem medical text corpus check available real example test rule sars versus pneumonia end versuspneumonia anyone suggestion finding separating
using minipar parser output,"<p>how to use minipar parser output to extract features like subject , object ,verb, tense etc to be use for english text to ASL conversion project</p>
",Multilingual Language Processing & Language Identification,using minipar parser output use minipar parser output extract feature like subject object verb tense etc use english text asl conversion project
Natural language processing to recognise numerical data ,"<p>My requirement is to recognize and extract numerical data from a natural language sentence (English only) in response to queries. Platform is Java. For example if the user query is ""What is the height of mount Everest"" and we have a paragraph as:</p>

<blockquote>
  <p>In 1856, the Great Trigonometric Survey of British India established the first published height of Everest, then known as Peak XV, at 29,002 ft (8,840 m). In 1865, Everest was given its official English name by the Royal Geographical Society upon recommendation of Andrew Waugh, the British Surveyor General of India at the time, who named it after his predecessor in the post, and former chief, Sir George Everest.[4] Chomolungma had been in common use by Tibetans for centuries, but Waugh was unable to propose an established local name because Nepal and Tibet were closed to foreigners. (Pasted from wikipedia)</p>
</blockquote>

<p>For a user query ""Height of mount Everest"" from the paragraph I need to get 29002 ft or 8840 m as the answer. Can anyone please suggest any possible ways of doing it in Java?  Are there any open source libraries for the same?</p>
",Multilingual Language Processing & Language Identification,natural language processing recognise numerical data requirement recognize extract numerical data natural language sentence english response query platform java example user query height mount everest paragraph great trigonometric survey british india established first published height everest known peak xv ft everest wa given official english name royal geographical society upon recommendation andrew waugh british surveyor general india time named predecessor post former chief sir george everest chomolungma common use tibetan century waugh wa unable propose established local name nepal tibet closed foreigner pasted wikipedia user query height mount everest paragraph need get ft answer anyone please suggest possible way java open source library
RegexpTokenize Japanese sentences - python,"<p>I'm trying to split the japanese sentences up using RegexpTokenizer but it is returning null sets. can someone tell me why? and how to get split the japanese sentences up?</p>

<pre><code>#!/usr/bin/python  # -*- encoding: utf-8 -*-

import nltk
import os, sys, re, glob
from nltk.tokenize import RegexpTokenizer

jp_sent_tokenizer = nltk.RegexpTokenizer(u'[^ „Äå„Äç!?„ÄÇÔºéÔºâ]*[!?„ÄÇ]')

print jp_sent_tokenizer.tokenize ('„ÅÆÂêÑÂÆ£Ë®Ä„ÇíÂÆüË°å„Åó„Å¶„Åä„ÅèÂøÖË¶Å„Åå„ÅÇ„Çã„Åì„Å®„Å´Ê≥®ÊÑè„Åó„Çà„ÅÜ„ÄÇ„Åì„Çå‰ª•‰∏ã„ÅÆÁØÄ„Åß„ÅØ„ÄÅÂêÑ„Çπ„ÇØ„É™„Éó„Éà‰æã„ÅÆÂâç„Å´„Åì„Çå„Çâ„Åå„Åô„Åß„Å´ÂÆ£Ë®Ä„Åï„Çå„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÂâçÊèê„Å®„Åô„Çã„ÄÇ')
</code></pre>

<p>the output to the above code is</p>

<pre><code>[]
</code></pre>
",Multilingual Language Processing & Language Identification,regexptokenize japanese sentence python trying split japanese sentence using regexptokenizer returning null set someone tell get split japanese sentence output code
Encoding for Multilingual .py Files,"<p>I am writing a .py file that contains strings from multiple charactersets, including English, Spanish, and Russian. For example, I have something like:</p>

<pre><code>string_en = ""The quick brown fox jumped over the lazy dog.""  
string_es = ""El veloz murci√©lago hind√∫ com√≠a feliz cardillo y kiwi.""
string_ru = ""–í —á–∞—â–∞—Ö —é–≥–∞ –∂–∏–ª –±—ã —Ü–∏—Ç—Ä—É—Å? –î–∞, –Ω–æ —Ñ–∞–ª—å—à–∏–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä!""
</code></pre>

<p>I am having trouble figuring out how to encode my file to avoid generating syntax errors like the one below when my file is run:</p>

<pre><code>SyntaxError: Non-ASCII character '\xc3' in file example.py on line 128, but no encoding
declared; see http://www.python.org/peps/pep-0263.html for details
</code></pre>

<p>I've tried adding <code># -*- coding: utf-8 -*-</code> to the beginning of my file, but without any luck.  I've also tried marking my strings as unicode (i.e. <code>string_en = u'The quick brown fox jumped over the lazy dog.""</code>), again unsuccessfully.  </p>

<p>Is it possible to include characters from different Python codecs in one file, or am I attempting to do something that is not allowed?</p>
",Multilingual Language Processing & Language Identification,encoding multilingual py file writing py file contains string multiple charactersets including english spanish russian example something like trouble figuring encode file avoid generating syntax error like one file run tried adding beginning file without luck also tried marking string unicode e unsuccessfully possible include character different python codecs one file attempting something allowed
Searching Natural Language Sentence Structure,"<p>What's the best way to store and search a database of natural language sentence structure  trees?</p>

<p>Using <a href=""http://incubator.apache.org/opennlp/"" rel=""noreferrer"">OpenNLP's</a> English Treebank Parser, I can get fairly reliable sentence structure parsings for arbitrary sentences. What I'd like to do is create a tool that can extract all the doc strings from my source code, generate these trees for all sentences in the doc strings, store these trees and their associated function name in a database, and then allow a user to search the database using natural language queries.</p>

<p>So, given the sentence <code>""This uploads files to a remote machine.""</code> for the function <code>upload_files()</code>, I'd have the tree:</p>

<pre><code>(TOP
  (S
    (NP (DT This))
    (VP
      (VBZ uploads)
      (NP (NNS files))
      (PP (TO to) (NP (DT a) (JJ remote) (NN machine))))
    (. .)))
</code></pre>

<p>If someone entered the query ""How can I upload files?"", equating to the tree:</p>

<pre><code>(TOP
  (SBARQ
    (WHADVP (WRB How))
    (SQ (MD can) (NP (PRP I)) (VP (VB upload) (NP (NNS files))))
    (. ?)))
</code></pre>

<p>how would I store and query these trees in a SQL database?</p>

<p>I've written a simple proof-of-concept script that can perform this search using a mix of regular expressions and network graph parsing, but I'm not sure how I'd implement this in a  scalable way.</p>

<p>And yes, I realize my example would be trivial to retrieve using a simple keyword search. The idea I'm trying to test is how I might take advantage of grammatical structure, so I can weed-out entries with similar keywords, but a different sentence structure. For example, with the above query, I wouldn't want to retrieve the entry associated with the sentence <code>""Checks a remote machine to find a user that uploads files.""</code> which has similar keywords, but is obviously describing a completely different behavior.</p>
",Multilingual Language Processing & Language Identification,searching natural language sentence structure best way store search database natural language sentence structure tree using opennlp english treebank parser get fairly reliable sentence structure parsings arbitrary sentence like create tool extract doc string source code generate tree sentence doc string store tree associated function name database allow user search database using natural language query given sentence function tree someone entered query upload file equating tree would store query tree sql database written simple proof concept script perform search using mix regular expression network graph parsing sure implement scalable way yes realize example would trivial retrieve using simple keyword search idea trying test might take advantage grammatical structure weed entry similar keywords different sentence structure example query want retrieve entry associated sentence ha similar keywords obviously describing completely different behavior
What should i use to crawl many news articles?,"<p>I've a project of natural language processing but for that i need to crawl many web articles from some sources like Yahoo news, Google news or blogs...</p>

<p>I'm a java developper (so i'd rather use java tools). I guess i can parse each source website on my own and extract the articles with HttpClient / XPath but i'm a bit lazy :) is there a way so that i won't have to make a parser per source?</p>

<p>(I'm not only interested by new articles but articles from 2000 to now too)</p>
",Multilingual Language Processing & Language Identification,use crawl many news article project natural language processing need crawl many web article source like yahoo news google news blog java developper rather use java tool guess parse source website extract article httpclient xpath bit lazy way make parser per source interested new article article
Server-side software for translating languages?,"<p>I am searching for a server-side application (not a service, we need to host this ourselves) that can take a given string and translate it to another language. Open-source, paid, doesn't matter.</p>

<p>Can anyone provide some recommendations?</p>
",Multilingual Language Processing & Language Identification,server side software translating language searching server side application service need host take given string translate another language open source paid matter anyone provide recommendation
Natural language processing library for auto-tagging (.NET),"<p>Dose anyone know of any good libraries out there for .NET that could help pull keywords out of blocks of natural language.</p>

<p>I'm basically trying to strip out stop words and ignore tenses, plurals and generally    find words that are essentially the same. </p>

<p>Some abilities to find synonyms would be nice, especially if it includes things like business/technology/non-dictionary words. </p>
",Multilingual Language Processing & Language Identification,natural language processing library auto tagging net dose anyone know good library net could help pull keywords block natural language basically trying strip stop word ignore tense plural generally find word essentially ability find synonym would nice especially includes thing like business technology non dictionary word
Apostrophes Converted to Correct Text?,"<p>Goal:  I need to be able to convert apostrophes to properly formed words. - at least for the most common words with apostrophes.  To do this ideally I'd want a list of words and their implied conterparts (i.e. ""don't"" and ""do not"").  </p>

<p>Issue: I'm creating a search algorithm based on natural language processing, but when users create content (or search) using an apostrophe, it causes issues for us.  Mostly because if we were to simply remove the apostrophe we would have (don't -> dont) (doesn't -> doesnt), which officially is not an english word, and can't be translated by the NLP system.</p>

<p>The ideal solution is simply a one to one mapping of what these items should be converted to, but I'm unaware of such a list.</p>

<p>Please let me know if you know of one, and where I might be able to find it.</p>

<p>thx</p>
",Multilingual Language Processing & Language Identification,apostrophe converted correct text goal need able convert apostrophe properly formed word least common word apostrophe ideally want list word implied conterparts e issue creating search algorithm based natural language processing user create content search using apostrophe cause issue u mostly simply remove apostrophe would dont doesnt officially english word translated nlp system ideal solution simply one one mapping item converted unaware list please let know know one might able find thx
"Is there a range(&#39;a&#39;,&#39;z&#39;) for Non-English Alphabet","<p>PHP has a function <code>range('a','z')</code> which prints the English alphabet a, b, c, d, etc. </p>

<p>Is there a similar function for other alphabets? maybe a function that accepts the language as a parameter</p>
",Multilingual Language Processing & Language Identification,range z non english alphabet php ha function print english alphabet b c etc similar function alphabet maybe function accepts language parameter
Get the word under the mouse cursor in Windows,"<p>Greetings everyone,</p>

<p>A friend and I are discussing the possibility of a new project: A translation program that will pop up a translation whenever you hover over any word in any control, even static, non-editable ones.  I know there are many browser plugins to do this sort of thing on webpages; we're thinking about how we would do it system-wide (on Windows).</p>

<p>Of course, the key difficulty is figuring out the word the user is hovering over.  I'm aware of MSAA and Automation, but as far as I can tell, those things only allow you to get the entire contents of a control, not the specific word the mouse is over.</p>

<p>I stumbled upon this (proprietary) application that does pretty much exactly what we want to do:  <a href=""http://www.gettranslateit.com/"" rel=""nofollow"">http://www.gettranslateit.com/</a></p>

<p>Somehow they are able to get the exact word the user is hovering over in almost any application (It seems to have trouble in a few apps, notably Windows Explorer).  It even grabs text out of obviously custom-drawn controls, somehow.  At first I thought it must be using OCR.  But even when I shrink the font so far down that the text becomes a completely unreadable blob, it can still recognize words perfectly.  (And yet, it doesn't recognize anything if I change the font to Wingdings.  But maybe that's by design?)</p>

<p>Any ideas as to how it's achieving this seemingly impossible task?</p>

<p>EDIT: It doesn't work with Wingdings, but it does work with some other nonsense fonts, so I've confirmed it can't be OCR.</p>
",Multilingual Language Processing & Language Identification,get word mouse cursor window greeting everyone friend discussing possibility new project translation program pop translation whenever hover word control even static non editable one know many browser plugins sort thing webpage thinking would system wide window course key difficulty figuring word user hovering aware msaa automation far tell thing allow get entire content control specific word mouse stumbled upon proprietary application doe pretty much exactly want somehow able get exact word user hovering almost application seems trouble apps notably window explorer even grab text obviously custom drawn control somehow first thought must using ocr even shrink font far text becomes completely unreadable blob still recognize word perfectly yet recognize anything change font wingdings maybe design idea achieving seemingly impossible task edit work wingdings doe work nonsense font confirmed ocr
What are the &quot;-P&quot;s in the Berkeley Aligner&#39;s output format?,"<p>I want to use the Berkeley Aligner for some MT research I'm doing, since, apparently, it beats GIZA++ pretty handily (a 32% alignment error reduction in some reported results).  For the most part the outputs in the Berkeley Aligner ""examples"" directory look like what Moses does to GIZA++ output files (i.e., paired aligned word indices), but there are some funny looking ""-P""s after certain pairs.  I can't for the life of me find any documentation  of what these ""-P"" annotations are supposed to signify (certainly not in the Berkeley Aligner ""documentation"" directory).</p>

<p>For clarity, I'll give a little illustrative example.  Suppose you have the sentences: ""Jean pl√¢it √† Marie"" and ""Marie likes Jean"".  French is the source language and English is the target language. The words ""Jean"" (indices 0 and 2, resp.) and ""Marie"" (indices 3 and 0, resp.) are aligned in both sentences, and ""pl√¢it"" and ""√† "" (French indices 1 and 2, resp.) are aligned with ""like"" (English index 1).  In Moses-post-processed GIZA++ output, this would be denoted by a list of source-target index pairs:</p>

<pre><code>0-2 1-1 2-1 3-0
</code></pre>

<p>Berkeley Aligner produces files that pretty much resemble this, but some index pairs have a -P on them (e.g., you <em>might</em> see something like 1-1-P).</p>

<p>What the heck does this mean? Can I safely remove these -P annotations and get a GIZA++-via-Moses style alignment, or should I be doing something more (e.g., multiplying them out into a series of aligned index pairs, or what have you)?</p>
",Multilingual Language Processing & Language Identification,p berkeley aligner output format want use berkeley aligner mt research since apparently beat giza pretty handily alignment error reduction reported result part output berkeley aligner example directory look like moses doe giza output file e paired aligned word index funny looking p certain pair life find documentation p annotation supposed signify certainly berkeley aligner documentation directory clarity give little illustrative example suppose sentence jean pl marie marie like jean french source language english target language word jean index resp marie index resp aligned sentence pl french index resp aligned like english index moses post processed giza output would denoted list source target index pair berkeley aligner produce file pretty much resemble index pair p e g might see something like p heck doe mean remove p annotation get giza via moses style alignment something e g multiplying series aligned index pair
How to compare different language String values in JAVA?,"<p>In my web application I am using <strong>two</strong> different Languages namely <strong>English</strong> and <strong>Arabic</strong>.</p>

<p>I have a <strong>search box</strong> in my web application in which if we search by name or part of the name then it will retrieve the values from DB by comparing the <strong>""Hometown""</strong> of the user</p>

<p><strong>Explanation:</strong></p>

<p>Like if a user belongs to <strong>hometown ""California""</strong> and he searches a <strong>name</strong> say <strong>""Victor""</strong> then my query will first see the people who are having the <strong>same hometown ""California""</strong> and in the list of people who have <strong>""California""</strong> as hometown the <strong>""Victor""</strong> *<em>name</em>* will be searched and it <strong>retrieve</strong> the users having <strong>""California""</strong> as their <strong>hometown</strong> and ""<strong>victor</strong>"" in their name or part of the name.</p>

<p>The <strong>problem</strong> is if the hometown <strong>""California""</strong> is <strong>saved</strong> in <strong>English</strong> it will compare and retrieve the values. But <strong>""California""</strong> will be <strong>saved as ""ŸÉÿßŸÑŸäŸÅŸàÿ±ŸÜŸäÿß"" in Arabic</strong>. In this case the <strong>hometown comparison fails</strong> and it cant retrieve the values. </p>

<p>I wish that my query should find both are same hometown and retrieve the values. Is it possible?</p>

<p>What <strong>alternate</strong> I should think of for this logic for comparison. I am confused. Any suggestion please? </p>

<p><strong>EDIT:</strong>
*<em>I have an Idea such that if the <strong>hometown</strong> is got then is it possible to use Google translator or transliterator and change the hometown to another language. if it is in english then to arabic or if it is in english then to arabic and give the search results joining both. Any suggestion?</em>*</p>
",Multilingual Language Processing & Language Identification,compare different language string value java web application using two different language namely english arabic search box web application search name part name retrieve value db comparing hometown user explanation like user belongs hometown california search name say victor query first see people hometown california list people california hometown victor name searched retrieve user california hometown victor name part name problem hometown california saved english compare retrieve value california saved arabic case hometown comparison fails cant retrieve value wish query find hometown retrieve value possible alternate think logic comparison confused suggestion please edit idea hometown got possible use google translator transliterator change hometown another language english arabic english arabic give search result joining suggestion
List of uninteresting words,"<p>[Caveat] This is not directly a programing question, but it is something that comes up so often in language processing that I'm sure it's of some use to the community.</p>

<p>Does anyone have a good list of <em>uninteresting</em> (English) words that have been tested by more then a casual look? This would include all prepositions, conjunctions, etc... words that may have semantic meaning, but are often frequent in <em>every</em> sentence, regardless of the subject. I've built my own lists from time to time for personal projects but they've been ad-hoc; I continuously add words that I forgotten as they come in.</p>
",Multilingual Language Processing & Language Identification,list uninteresting word caveat directly programing question something come often language processing sure use community doe anyone good list uninteresting english word tested casual look would include preposition conjunction etc word may semantic meaning often frequent every sentence regardless subject built list time time personal project ad hoc continuously add word forgotten come
Synchronizing text and audio. Is there a NLP/speech-to-text library to do this?,"<p>I would like to synchronize a spoken recording against a known text.  Is there a speech-to-text / natural language processing library that would facilitate this?  I imagine I'd want to detect word boundaries and compute candidate matches from a dictionary.  Most of the questions I've found on SO concern written language.</p>

<p>Desired, but not required:</p>

<ul>
<li>Open Source</li>
<li>Compatible with American English out-of-the-box</li>
<li>Cross-platform</li>
<li>Thoroughly documented</li>
</ul>

<p>Edit: I realize this is a very broad, even naive, question, so thanks in advance for your guidance.</p>

<p>What I've found so far:</p>

<ul>
<li><a href=""http://www.politepix.com/openears/"" rel=""noreferrer"">OpenEars</a> (iOS Sphinx/Flite wrapper)</li>
</ul>
",Multilingual Language Processing & Language Identification,synchronizing text audio nlp speech text library would like synchronize spoken recording known text speech text natural language processing library would facilitate imagine want detect word boundary compute candidate match dictionary question found concern written language desired required open source compatible american english box cross platform thoroughly documented edit realize broad even naive question thanks advance guidance found far openears io sphinx flite wrapper
Computational Linguistics project idea using Hadoop MapReduce,"<p>I need to do a project on Computational Linguistics course. Is there any interesting ""linguistic"" problem which is data intensive enough to work on using Hadoop map reduce. Solution or algorithm should try and analyse and provide some insight in ""lingustic"" domain. however it should be applicable to large datasets so that i can use hadoop for it. I know there is a python natural language processing toolkit for hadoop.</p>
",Multilingual Language Processing & Language Identification,computational linguistics project idea using hadoop mapreduce need project computational linguistics course interesting linguistic problem data intensive enough work using hadoop map reduce solution algorithm try analyse provide insight lingustic domain however applicable large datasets use hadoop know python natural language processing toolkit hadoop
"For the iPhone, can you program for different languages?","<p>For the iPhone, is it possible to program applications to translate words from a base language to any of several languages of various users. If so, how? </p>
",Multilingual Language Processing & Language Identification,iphone program different language iphone possible program application translate word base language several language various user
Justadistraction: tokenizing English without whitespaces. Murakami SheepMan,"<p>I wondered how <strong>you</strong> would go about tokenizing strings in English (or other western languages) if whitespaces were removed?</p>

<p>The inspiration for the question is the Sheep Man character in the Murakami novel '<a href=""http://en.wikipedia.org/wiki/Dance_Dance_Dance"" rel=""noreferrer"">Dance Dance Dance</a>'</p>

<p>In the novel, the Sheep Man is translated as saying things like:</p>

<blockquote>
  <p>""likewesaid, we'lldowhatwecan. Trytoreconnectyou, towhatyouwant,"" said the Sheep Man. ""Butwecan'tdoit-alone. Yougottaworktoo.""</p>
</blockquote>

<p>So, some punctuation is kept, but not all. Enough for a human to read, but somewhat arbitrary.</p>

<p>What would be your strategy for building a parser for this? Common combinations of letters, syllable counts, conditional grammars, look-ahead/behind regexps etc.?</p>

<p>Specifically, python-wise, how would you structure a (forgiving) translation flow? Not asking for a completed answer, just more how your thought process would go about breaking the problem down.</p>

<p>I ask this in a frivolous manner, but I think it's a question that might get some interesting (nlp/crypto/frequency/social) answers.
Thanks!</p>
",Multilingual Language Processing & Language Identification,justadistraction tokenizing english without whitespaces murakami sheepman wondered would go tokenizing string english western language whitespaces removed inspiration question sheep man character murakami novel dance dance dance novel sheep man translated saying thing like likewesaid lldowhatwecan trytoreconnectyou towhatyouwant said sheep man butwecan tdoit alone yougottaworktoo punctuation kept enough human read somewhat arbitrary would strategy building parser common combination letter syllable count conditional grammar look ahead behind regexps etc specifically python wise would structure forgiving translation flow asking completed answer thought process would go breaking problem ask frivolous manner think question might get interesting nlp crypto frequency social answer thanks
How can I make this Python2.6 function work with Unicode?,"<p>I've got this function, which I modified from material in chapter 1 of the online NLTK book. It's been very useful to me but, despite reading the chapter on Unicode, I feel just as lost as before.</p>

<pre><code>def openbookreturnvocab(book):
    fileopen = open(book)
    rawness = fileopen.read()
    tokens = nltk.wordpunct_tokenize(rawness)
    nltktext = nltk.Text(tokens)
    nltkwords = [w.lower() for w in nltktext]
    nltkvocab = sorted(set(nltkwords))
    return nltkvocab
</code></pre>

<p>When I tried it the other day on Also Sprach Zarathustra, it clobbered words with an umlat over the o's and u's. I'm sure some of you will know why that happened. I'm also sure that it's quite easy to fix. I know that it just has to do with calling a function that re-encodes the tokens into unicode strings. If so, that it seems to me it might not happen inside that function definition at all, but here, where I prepare to write to file:</p>

<pre><code>def jotindex(jotted, filename, readmethod):
    filemydata = open(filename, readmethod)
    jottedf = '\n'.join(jotted)
    filemydata.write(jottedf)
    filemydata.close()
    return 0
</code></pre>

<p>I heard that what I had to do was encode the string into unicode after reading it from the file. I tried amending the function like so:</p>

<pre><code>def openbookreturnvocab(book):
    fileopen = open(book)
    rawness = fileopen.read()
    unirawness = rawness.decode('utf-8')
    tokens = nltk.wordpunct_tokenize(unirawness)
    nltktext = nltk.Text(tokens)
    nltkwords = [w.lower() for w in nltktext]
    nltkvocab = sorted(set(nltkwords))
    return nltkvocab
</code></pre>

<p>But that brought this error, when I used it on Hungarian. When I used it on German, I had no errors.</p>

<pre><code>&gt;&gt;&gt; import bookroutines
&gt;&gt;&gt; elles1 = bookroutines.openbookreturnvocab(""lk1-les1"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""bookroutines.py"", line 9, in openbookreturnvocab
    nltktext = nltk.Text(tokens)
  File ""/usr/lib/pymodules/python2.6/nltk/text.py"", line 285, in __init__
    self.name = "" "".join(map(str, tokens[:8])) + ""...""
UnicodeEncodeError: 'ascii' codec can't encode character u'\xe1' in position 4: ordinal not in range(128)
</code></pre>

<p>I fixed the function that files the data like so:</p>

<pre><code>def jotindex(jotted, filename, readmethod):
    filemydata = open(filename, readmethod)
    jottedf = u'\n'.join(jotted)
    filemydata.write(jottedf)
    filemydata.close()
    return 0
</code></pre>

<p>However, that brought this error, when I tried to file the German:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""bookroutines.py"", line 23, in jotindex
    filemydata.write(jottedf)
UnicodeEncodeError: 'ascii' codec can't encode character u'\xf6' in position 414: ordinal not in range(128)
&gt;&gt;&gt; 
</code></pre>

<p>...which is what you get when you try to write the u'\n'.join'ed data.</p>

<pre><code>&gt;&gt;&gt; jottedf = u'/n'.join(elles1)
&gt;&gt;&gt; filemydata.write(jottedf)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
UnicodeEncodeError: 'ascii' codec can't encode character u'\xf6' in position 504: ordinal not in range(128)
</code></pre>
",Multilingual Language Processing & Language Identification,make python function work unicode got function modified material chapter online nltk book useful despite reading chapter unicode feel lost tried day also sprach zarathustra clobbered word umlat u sure know happened also sure quite easy fix know ha calling function encodes token unicode string seems might happen inside function definition prepare write file heard wa encode string unicode reading file tried amending function like brought error used hungarian used german error fixed function file data like however brought error tried file german get try write u n join ed data
Parser Generator or Library that Supports Suffix Agreement,"<p>I'm working on a syntactic parser for some language. But this language requires suffix agreement highly. For example in English a verb must agree with pronoun as I,we,you-do or he,she,it,this-does etc. In this language a verb has different forms for each pronoun. I know in literature this is handled by unification method. But I couldn't find any implementation of it in Java. I also researched Stanford parser and ANTLR but I couldn‚Äôt find any evidence that they support suffix agreement. </p>

<p>So which tool or lib. would you offer me in this situation?</p>

<p>Thanks in advance.</p>
",Multilingual Language Processing & Language Identification,parser generator library support suffix agreement working syntactic parser language language requires suffix agreement highly example english verb must agree pronoun doe etc language verb ha different form pronoun know literature handled unification method find implementation java also researched stanford parser antlr find evidence support suffix agreement tool lib would offer situation thanks advance
Transforming early modern English into 20th century spelling using the NLTK,"<p>I have a list of strings that are all early modern English words ending with 'th.' These include hath, appointeth, demandeth, etc. -- they are all conjugated for the third person singular.</p>

<p>As part of a much larger project (using my computer to convert the Gutenberg etext of Gargantua and Pantagruel into something more like 20th century English, so that I'll be able to read it more easily) I want to remove the last two or three characters from all of those words and replace them with an 's,' then use a slightly modified function on the words that still weren't modernized, both included below.</p>

<p>My main problem is that I just never manage to get my typing right in Python. I find that part of the language really confusing at this point.</p>

<p>Here's the function that removes th's:</p>

<pre><code>from __future__ import division
import nltk, re, pprint

def ethrema(word):
    if word.endswith('th'):
        return word[:-2] + 's'
</code></pre>

<p>Here's the function that removes extraneous e's:</p>

<pre><code>def ethremb(word):
    if word.endswith('es'):
        return word[:-2] + 's'
</code></pre>

<p>hence the words 'abateth' and 'accuseth' would pass through ethrema but not through ethremb(ethrema), while the word 'abhorreth' would need to pass through both.</p>

<p>If anyone can think of a more efficient way to do this, I'm all ears.</p>

<p>Here's the result of my very amateurish attempt to use these functions on a tokenized list  of words that need modernizing:</p>

<pre><code>&gt;&gt;&gt; eth1 = [w.ethrema() for w in text]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'str' object has no attribute 'ethrema'
</code></pre>

<p>So, yeah, it's really an issue of typing. These are the first functions I've ever written in Python, and I have no idea how to apply them to actual objects.</p>
",Multilingual Language Processing & Language Identification,transforming early modern english th century spelling using nltk list string early modern english word ending th include hath appointeth demandeth etc conjugated third person singular part much larger project using computer convert gutenberg etext gargantua pantagruel something like th century english able read easily want remove last two three character word replace use slightly modified function word still modernized included main problem never manage get typing right python find part language really confusing point function remove th function remove extraneous e hence word abateth accuseth would pas ethrema ethremb ethrema word abhorreth would need pas anyone think efficient way ear result amateurish attempt use function tokenized list word need modernizing yeah really issue typing first function ever written python idea apply actual object
Binarization in Natural Language Processing,"<p>Binarization is the act of transforming colorful features of of an entity into vectors of numbers, most often binary vectors, to make good examples for classifier algorithms.</p>

<p>If we where to binarize the sentence ""The cat ate the dog"", we could start by assigning every word an ID (for example cat-1, ate-2, the-3, dog-4) and then simply replace the word by it's ID giving the vector &lt;3,1,2,3,4>. </p>

<p>Given these IDs we could also create a binary vector by giving each word four possible slots, and setting the slot corresponding to a specific word with to one, giving the vector &lt;0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1>. The latter method is, as far as I know, is commonly referred to as the bag-of-words-method.</p>

<p>Now for my question, what is the <i>best</i> binarization method when it comes to describe features for natural language processing in general, and transition-based <i>dependency parsing</i> (with Nivres algorithm) in particular? </p>

<p>In this context, we do not want to encode the whole sentence, but rather the current state of the parse, for example the top word on the stack en the first word in the input queue. Since order is highly relevant, this rules out the bag-of-words-method. </p>

<p>With <i>best</i>, I am referring to the method that makes the data the most intelligible for the classifier, without using up unnecessary memory. For example I don't want a word bigram to use 400 million features for 20000 unique words, if only 2% the bigrams actually exist.</p>

<p>Since the answer is also depending on the particular classifier, I am mostly interested in maximum entropy models (liblinear), support vector machines (libsvm) and perceptrons, but answers that apply to other models are also welcome.</p>
",Multilingual Language Processing & Language Identification,binarization natural language processing binarization act transforming colorful feature entity vector number often binary vector make good example classifier algorithm binarize sentence cat ate dog could start assigning every word id example cat ate dog simply replace word id giving vector given id could also create binary vector giving word four possible slot setting slot corresponding specific word one giving vector latter method far know commonly referred bag word method question best binarization method come describe feature natural language processing general transition based dependency parsing nivres algorithm particular context want encode whole sentence rather current state parse example top word stack en first word input queue since order highly relevant rule bag word method best referring method make data intelligible classifier without using unnecessary memory example want word bigram use million feature unique word bigram actually exist since answer also depending particular classifier mostly interested maximum entropy model liblinear support vector machine libsvm perceptrons answer apply model also welcome
What&#39;s needed for NLP?,"<p>assuming that I know nothing about everything and that I'm starting in programming TODAY what do you say would be necessary for me to learn in order to start working with Natural Language Processing?</p>

<p>I've been struggling with some string parsing methods but so far it is just annoying me and making me create ugly code. I'm looking for some fresh new ideas on how to create a Remember The Milk API like to parse user's input in order to provide an input form for fast data entry that are not based on fields but in simple one line phrases instead.</p>

<p><strong>EDIT</strong>: RTM is todo list system. So in order to enter a task you don't need to type in each field to fill values (task name, due date, location, etc). You can simply type in a phrase like ""Dentist appointment monday at 2PM in WhateverPlace"" and it will parse it and fill all fields for you.</p>

<p>I don't have any kind of technical constraints since it's going to be a personal project but I'm more familiar with .NET world. Actually, I'm not sure this is a matter of language but if it's necessary I'm more than willing to learn a new language to do it.</p>

<p>My project is related to personal finances so the phrases are more like ""Spent 10USD on Coffee last night with my girlfriend"" and it would fill location, amount of $$$, tags and other stuff.</p>

<p>Thanks a lot for any kind of directions that you might give me!</p>
",Multilingual Language Processing & Language Identification,needed nlp assuming know nothing everything starting programming today say would necessary learn order start working natural language processing struggling string parsing method far annoying making create ugly code looking fresh new idea create remember milk api like parse user input order provide input form fast data entry based field simple one line phrase instead edit rtm todo list system order enter task need type field fill value task name due date location etc simply type phrase like dentist appointment monday pm whateverplace parse fill field kind technical constraint since going personal project familiar net world actually sure matter language necessary willing learn new language project related personal finance phrase like spent usd coffee last night girlfriend would fill location amount tag stuff thanks lot kind direction might give
Natural Language Processing Solution in Java?,"<p>Are there any equally great packages like Python's NTLK in Java world ?</p>
",Multilingual Language Processing & Language Identification,natural language processing solution java equally great package like python ntlk java world
"Extracting a set of words with the Python/NLTK, then comparing it to a standard English dictionary","<p>I have:</p>

<pre><code>from __future__ import division
import nltk, re, pprint
f = open('/home/a/Desktop/Projects/FinnegansWake/JamesJoyce-FinnegansWake.txt')
raw = f.read()
tokens = nltk.wordpunct_tokenize(raw)
text = nltk.Text(tokens)
words = [w.lower() for w in text]

f2 = open('/home/a/Desktop/Projects/FinnegansWake/catted-several-long-Russian-novels-and-the-NYT.txt')
englishraw = f2.read()
englishtokens = nltk.wordpunct_tokenize(englishraw)
englishtext = nltk.Text(englishtokens)
englishwords = [w.lower() for w in englishwords]
</code></pre>

<p>which is straight from the NLTK manual. What I want to do next is to compare <code>vocab</code> to an exhaustive set of English words, like the OED, and extract the difference -- the set of Finnegans Wake words that have not, and probably never will, be in the OED. I'm much more of a verbal person than a math-oriented person, so I haven't figured out how to do that yet, and the manual goes into way too much detail about stuff I don't actually want to do. I'm assuming it's just one or two more lines of code, though. </p>
",Multilingual Language Processing & Language Identification,extracting set word python nltk comparing standard english dictionary straight nltk manual want next compare exhaustive set english word like oed extract difference set finnegans wake word probably never oed much verbal person math oriented person figured yet manual go way much detail stuff actually want assuming one two line code though
Natural language query processing libraries,"<p>I am looking for Natural language query processing libraries to convert plain english query to sql like statements. For ex, show the list of employees whose age is 30 should be converted to select * from employees where age = 30.</p>

<p>Can you provide pointers/references?</p>

<p>Thanks,
Mani</p>
",Multilingual Language Processing & Language Identification,natural language query processing library looking natural language query processing library convert plain english query sql like statement ex show list employee whose age converted select employee age provide pointer reference thanks mani
Theory: &quot;Lexical Encoding&quot;,"<p><strong>I am using the term ""Lexical Encoding"" for my lack of a better one.</strong></p>

<p>A Word is arguably the fundamental unit of communication as opposed to a Letter.  Unicode tries to assign a numeric value to each Letter of all known Alphabets.  What is a Letter to one language, is a Glyph to another.  Unicode 5.1 assigns more than 100,000 values to these Glyphs currently.  Out of the approximately 180,000 Words being used in Modern English, it is said that with a vocabulary of about 2,000 Words, you should be able to converse in general terms. A ""Lexical Encoding"" would encode each Word not each Letter, and encapsulate them within a Sentence.</p>

<pre><code>// An simplified example of a ""Lexical Encoding""
String sentence = ""How are you today?"";
int[] sentence = { 93, 22, 14, 330, QUERY };
</code></pre>

<p>In this example each Token in the String was encoded as an Integer. The Encoding Scheme here simply assigned an int value based on generalised statistical ranking of word usage, and assigned a constant to the question mark.</p>

<p>Ultimately, a Word has both a Spelling &amp; Meaning though.  Any ""Lexical Encoding"" would preserve the meaning and intent of the Sentence as a whole, and not be language specific.  An English sentence would be encoded into <a href=""https://stackoverflow.com/questions/170452/linguistics-lexical-encoding#174249"">""...language-neutral atomic elements of meaning ...""</a> which could then be reconstituted into any language with a structured Syntactic Form and Grammatical Structure.</p>

<p>What are other examples of ""Lexical Encoding"" techniques?</p>

<hr>

<p>If you were interested in where the word-usage statistics come from :<br>
<a href=""http://www.wordcount.org"" rel=""nofollow noreferrer"">http://www.wordcount.org</a></p>
",Multilingual Language Processing & Language Identification,theory lexical encoding using term lexical encoding lack better one word arguably fundamental unit communication opposed letter unicode try assign numeric value letter known alphabet letter one language glyph another unicode assigns value glyph currently approximately word used modern english said vocabulary word able converse general term lexical encoding would encode word letter encapsulate within sentence example token string wa encoded integer encoding scheme simply assigned int value based generalised statistical ranking word usage assigned constant question mark ultimately word ha spelling meaning though lexical encoding would preserve meaning intent sentence whole language specific english sentence would encoded could reconstituted language structured syntactic form grammatical structure example lexical encoding technique interested word usage statistic come
Building dictionary of words from large text,"<p>I have a text file containing posts in English/Italian. I would like to read the posts into a data matrix so that each row represents a post and each column a word. The cells in the matrix are the counts of how many times each word appears in the post. The dictionary should consist of all the words in the whole file or a non exhaustive English/Italian dictionary. </p>

<p>I know this is a common essential preprocessing step for NLP. And I know it's pretty trivial to code it, sill I'd like to use some NLP domain specific tool so I get stop-words trimmed etc..</p>

<p>Does anyone know of a tool\project that can perform this task?</p>

<p>Someone mentioned apache lucene, do you know if lucene index can be serialized to a data-structure similar to my needs?</p>
",Multilingual Language Processing & Language Identification,building dictionary word large text text file containing post english italian would like read post data matrix row represents post column word cell matrix count many time word appears post dictionary consist word whole file non exhaustive english italian dictionary know common essential preprocessing step nlp know pretty trivial code sill like use nlp domain specific tool get stop word trimmed etc doe anyone know tool project perform task someone mentioned apache lucene know lucene index serialized data structure similar need
Java Stanford NLP: ArrayIndexOutOfBounds after loading second lexicon,"<p>I am using the Stanford Natural Language processing toolkit. I've been trying to find spelling errors with <code>Lexicon</code>'s <code>isKnown</code> method, but it produces quite a few false positives. So I thought I'd load a second lexicon, and check that too. However, that causes a problem.</p>

<pre><code>private static LexicalizedParser lp = new LexicalizedParser(Constants.stdLexFile);
private static LexicalizedParser wsjLexParse = new LexicalizedParser(Constants.wsjLexFile);

    static {
        lp.setOptionFlags(Constants.lexOptionFlags);        
        wsjLexParse.setOptionFlags(Constants.lexOptionFlags);       
    }

public ParseTree(String input) throws IllegalArgumentException, IllegalAccessException, InvocationTargetException {
    initialInput = input;
    DocumentPreprocessor process = new DocumentPreprocessor();
    sentences = process.getSentencesFromText(new StringReader(input));

    for (List&lt;? extends HasWord&gt; sent : sentences) {
        if(lp.parse(sent)) { // line 65
            forest.add(lp.getBestParse()); //non determinism?
        }
    }

    partsOfSpeech = pos();
    runAnalysis();
}
</code></pre>

<p>The following fail trace is produced:</p>

<pre><code>java.lang.ArrayIndexOutOfBoundsException: 45547
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.initRulesWithWord(BaseLexicon.java:300)
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.isKnown(BaseLexicon.java:160)
    at edu.stanford.nlp.parser.lexparser.BaseLexicon.ruleIteratorByWord(BaseLexicon.java:212)
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.initializeChart(ExhaustivePCFGParser.java:1299)
    at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:388)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse(LexicalizedParser.java:234)
    at nth.compling.ParseTree.&lt;init&gt;(ParseTree.java:65)
    at nth.compling.ParseTreeTest.constructor(ParseTreeTest.java:33)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at org.junit.internal.runners.BeforeAndAfterRunner.invokeMethod(BeforeAndAfterRunner.java:74)
    at org.junit.internal.runners.BeforeAndAfterRunner.runBefores(BeforeAndAfterRunner.java:50)
    at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:33)
    at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
</code></pre>

<p>If I comment out this line: (and other references to wsjLexParse)</p>

<pre><code>private static LexicalizedParser wsjLexParse = new LexicalizedParser(Constants.wsjLexFile);
</code></pre>

<p>then everything works fine. What am I doing wrong here?</p>
",Multilingual Language Processing & Language Identification,java stanford nlp arrayindexoutofbounds loading second lexicon using stanford natural language processing toolkit trying find spelling error method produce quite false positive thought load second lexicon check however cause problem following fail trace produced comment line reference wsjlexparse everything work fine wrong
Splitting string on probable English word boundaries,"<p>I recently used Adobe Acrobat Pro's OCR feature to process a Japanese kanji dictionary.  The overall quality of the output is generally quite a bit better than I'd hoped, but word boundaries in the English portions of the text have often been lost.  For example, here's one line from my file:</p>

<pre><code>softening;weakening(ofthemarket)8 CHANGE [transform] oneselfINTO,takethe form of; disguise oneself
</code></pre>

<p>I could go around and insert the missing word boundaries everywhere, but this would be adding to what is already a substantial task.  I'm hoping that there might exist software which can analyze text like this, where some of the words run together, and split the text on probable word boundaries.  Is there such a package?</p>

<p>I'm using Emacs, so it'd be extra-sweet if the package in question were already an Emacs package or could be readily integrated into Emacs, so that I could simply put my cursor on a line like the above and repeatedly invoke some command that splits the line on word boundaries in decreasing order of probable correctness.</p>
",Multilingual Language Processing & Language Identification,splitting string probable english word boundary recently used adobe acrobat pro ocr feature process japanese kanji dictionary overall quality output generally quite bit better hoped word boundary english portion text often lost example one line file could go around insert missing word boundary everywhere would adding already substantial task hoping might exist software analyze text like word run together split text probable word boundary package using emacs extra sweet package question already emacs package could readily integrated emacs could simply put cursor line like repeatedly invoke command split line word boundary decreasing order probable correctness
details on the following Natural Language Processing terms?,"<pre><code>Named Entity Extraction (extract ppl, cities, organizations)
Content Tagging (extract topic tags by scanning doc)
Structured Data Extraction
Topic Categorization (taxonomy classification by scanning doc....bayesian )
Text extraction (HTML page cleaning)
</code></pre>

<p>are there libraries that i can use to do any of the above functions of NLP ?</p>

<p>dont really feel like forking out cash to AlchemyAPI</p>
",Multilingual Language Processing & Language Identification,detail following natural language processing term library use function nlp dont really feel like forking cash alchemyapi
Does knowing a Natural Language well help with Programming?,"<p>We all hear that math at least helps a little bit with programming. My question though, does English or other natural language skills help with programming? I know it has to help with technical documentation, but what about actual programming? Are certain constructs in a programming language also there in natural languages? Does knowing how to write a 20 page research paper help with writing a 20k loc programming project? </p>
",Multilingual Language Processing & Language Identification,doe knowing natural language well help programming hear math least help little bit programming question though doe english natural language skill help programming know ha help technical documentation actual programming certain construct programming language also natural language doe knowing write page research paper help writing k loc programming project
Dictionary of English Words for a J2ME app,"<p>I intend to develop a J2ME application, that should be able to read words from the English Dictionary. How do I interface to/and store a Dictionary ? Will I have to create the Dictionary myself, by inserting words, or is there a third party Dictionary available with APIs?</p>
",Multilingual Language Processing & Language Identification,dictionary english word j app intend develop j application able read word english dictionary interface store dictionary create dictionary inserting word third party dictionary available apis
Hierarchy of meaning,"<p>I am looking for a method to build a hierarchy of words. </p>

<p>Background: I am a ""amateur"" natural language processing enthusiast and right now one of the problems that I am interested in is determining the hierarchy of word semantics from a group of words.</p>

<p>For example, if I have the set which contains a ""super"" representation of others, i.e.</p>

<pre><code>[cat, dog, monkey, animal, bird, ... ]
</code></pre>

<p>I am interested to use any technique which would allow me to extract the word 'animal' which has the most meaningful and accurate representation of the other words inside this set.</p>

<p>Note: they are NOT the same in meaning. cat != dog != monkey != animal
BUT cat is a subset of animal and dog is a subset of animal.</p>

<p>I know by now a lot of you will be telling me to use wordnet. Well, I will try to but I am actually interested in doing a very domain specific area which WordNet doesn't apply because:
1) Most words are not found in Wordnet
2) All the words are in another language; translation is possible but is to limited effect.</p>

<p>another example would be:</p>

<pre><code>[ noise reduction, focal length, flash, functionality, .. ]
</code></pre>

<p>so functionality includes everything in this set. </p>

<p>I have also tried crawling wikipedia pages and applying some techniques on td-idf etc but wikipedia pages doesn't really do much either.</p>

<p>Can someone possibly enlighten me as to what direction my research should go towards? (I could use anything)</p>
",Multilingual Language Processing & Language Identification,hierarchy meaning looking method build hierarchy word background amateur natural language processing enthusiast right one problem interested determining hierarchy word semantics group word example set contains super representation others e interested use technique would allow extract word animal ha meaningful accurate representation word inside set note meaning cat dog monkey animal cat subset animal dog subset animal know lot telling use wordnet well try actually interested domain specific area wordnet apply word found wordnet word another language translation possible limited effect another example would functionality includes everything set also tried crawling wikipedia page applying technique td idf etc wikipedia page really much either someone possibly enlighten direction research go towards could use anything
How to make concept representation with the help of bag of words,"<p>Thanks for stoping to read my question :) this is very sweet place full of GREAT peoples !</p>

<p>I have a question about ""creating sentences with words"". NO NO it is not about english grammar :)</p>

<p>Let me explain, If I have bag of words like</p>

<pre><code>""person apple apple person person a eat person will apple eat hungry apple hungry""
</code></pre>

<p>and it can generate some kind of following sentence</p>

<pre><code>""hungry person eat apple""
</code></pre>

<p>I don't in which field this topic will relate. Where should I try to find an answer. I tried to search google but I only found english grammar stuff :)</p>

<p>Any body there who can tell me which algo can work in this problem? or any program </p>

<p>Thanks</p>

<p>P.S: It is not an assignment :) if it would be i would ask for source code ! I don't even know in which field I should look for :)</p>
",Multilingual Language Processing & Language Identification,make concept representation help bag word thanks stoping read question sweet place full great people question creating sentence word english grammar let explain bag word like generate kind following sentence field topic relate try find answer tried search google found english grammar stuff body tell algo work problem program thanks p assignment would would ask source code even know field look
Python - letter frequency count and translation,"<p>I am using Python 3.1, but I can downgrade if needed.</p>

<p>I have an ASCII file containing a short story written in one of the languages the alphabet of which can be represented with upper and or lower ASCII. I wish to:</p>

<p>1) Detect an encoding to the best of my abilities, get some sort of confidence metric (would vary depending on the length of the file, right?)</p>

<p>2) Automatically translate the whole thing using some free online service or a library.</p>

<p>Additional question: What if the text is written in a language where it takes 2 or more bytes to represent one letter and the byte order mark is not there to help me?</p>

<p>Finally, how do I deal with punctuation and misc characters such as space? It will occur more frequently than some letters, right? How about the fact that punctuation and characters can be sometimes mixed - there might be two representations of a comma, two representations for what looks like an ""a"", etc.?</p>

<p>Yes, I have read <a href=""http://www.joelonsoftware.com/articles/Unicode.html"" rel=""nofollow noreferrer"">the article by Joel Spolsky on Unicode</a>. Please help me with at least some of these items.</p>

<p>Thank you!</p>

<p>P.S. This is not a homework, but it is for self-educational purposes. I prefer using a letter frequency library that is open-source and readable as opposed to the one that is closed, efficient, but gets the job done well.</p>
",Multilingual Language Processing & Language Identification,python letter frequency count translation using python downgrade needed ascii file containing short story written one language alphabet represented upper lower ascii wish detect encoding best ability get sort confidence metric would vary depending length file right automatically translate whole thing using free online service library additional question text written language take byte represent one letter byte order mark help finally deal punctuation misc character space occur frequently letter right fact punctuation character sometimes mixed might two representation comma two representation look like etc yes read article joel spolsky unicode please help least item thank p homework self educational purpose prefer using letter frequency library open source readable opposed one closed efficient get job done well
English Lexicon for Search Query Correction,"<p>I'm building a spelling corrector for search engine queries by implementing the method described in ""<a href=""http://research.microsoft.com/en-us/people/silviu/emnlp04.pdf"" rel=""nofollow noreferrer"">Spelling correction as an iterative process that exploits the collective knowledge of web users</a>"".  </p>

<p>The high-level approach is as follows:  for a given query, come up with possible correction candidates (words in the query log within a certain edit distance) of each unigram and bigram, then perform a modified Viterbi search to find the most likely sequence of candidates given bigram frequencies.  Repeat this process until the sequence is of maximum probability.</p>

<p>The modification to the Viterbi search is such that if two adjacent words are both found in a trusted lexicon, at most one can be corrected.  This is especially important for avoiding correction of properly-spelled single-word queries to words of higher frequency.</p>

<p>My question is where to find such a lexicon.  It should be in English and contain proper nouns (first/last names, places, brand names, etc) likely to show up in search queries as well as common and uncommon English words.  Even a push in the right direction would be useful.</p>

<p>Also, if anyone is reading this and has any suggestions for improvement on the methodology supplied in the paper, I am open to those as well given that this is my first foray into NLP.</p>
",Multilingual Language Processing & Language Identification,english lexicon search query correction building spelling corrector search engine query implementing method described spelling correction iterative process exploit collective knowledge web user high level approach follows given query come possible correction candidate word query log within certain edit distance unigram bigram perform modified viterbi search find likely sequence candidate given bigram frequency repeat process sequence maximum probability modification viterbi search two adjacent word found trusted lexicon one corrected especially important avoiding correction properly spelled single word query word higher frequency question find lexicon english contain proper noun first last name place brand name etc likely show search query well common uncommon english word even push right direction would useful also anyone reading ha suggestion improvement methodology supplied paper open well given first foray nlp
Language related -What does Client-server application mean?,"<p>Well</p>

<p>It's not a big question, obviously.</p>

<p>But you see, an application that is using a database on the servers, and is installed on multiple clients. Is called Client/Server application.</p>

<p>And an application that is constituted by two parts: Host (or server) part, and the client part.</p>

<p>They are both called client/server apps</p>

<p>How can we distinguish them, and what's the proper name of each type.</p>

<p>P.S. I'm talking about English, you know. I want to say that this application is Client/Server(only database on server) and not a Client/Server(as real client and server).</p>
",Multilingual Language Processing & Language Identification,language related doe client server application mean well big question obviously see application using database server installed multiple client called client server application application constituted two part host server part client part called client server apps distinguish proper name type p talking english know want say application client server database server client server real client server
extract grammar features from sentence on Google App Engine,"<p>For my GAE app I need to do some natural language processing to extract the subject and object from an input sentence. </p>

<p>Apparently <a href=""https://stackoverflow.com/questions/1286301/using-the-python-nltk-2-0b5-on-the-google-app-engine"">NLTK can't be installed</a> (easily) on GAE so I am looking for another solution.
I noticed GAE comes with <a href=""http://code.google.com/appengine/docs/python/tools/libraries.html#Antlr"" rel=""nofollow noreferrer"">Antlr3</a> but from browsing their documentation it solves a different kind of grammar problem.</p>

<p>Any ideas?</p>
",Multilingual Language Processing & Language Identification,extract grammar feature sentence google app engine gae app need natural language processing extract subject object input sentence apparently antlr browsing documentation solves different kind grammar problem idea
Vista speech recognition in multiple languages,"<p>my primary language is spanish, but I use all my software in english, including windows; however I'd like to use speech recognition in spanish.</p>

<p>Do you know if there's a way to use vista's speech recognition in other language than the primary os language?</p>
",Multilingual Language Processing & Language Identification,vista speech recognition multiple language primary language spanish use software english including window however like use speech recognition spanish know way use vista speech recognition language primary language
Natural Language CFG builder Algorithm,"<p>I am working in a natural language processing project. It aims to build libraries for Arabic language. We working on a POS tagger and now I am thinking in grammar phase. Since Arabic language and many others have complicated grammar, so it is very hard to build their context free grammar (CFG). For this reason I had an idea for an algorithm to build a CFG (with probability PCFG) for any language from a tagger corpora using unsupervised learning. To explain the algorithm suppose I have these three tagged statements as an input:
1- Verb Noun
2- Verb Noun Subject
3- Verb Noun Subject adverb
The algorithm gives:
1) A--> Verb Noun
2) B-->A Subject
3) C-->B adverb.<br/>
We repeat this methodology for each statement such that we can finish with a specific PCFG. The main power of the algorithm lies beyond the fact of seeing the whole statement, so the probabilities can be conditional and they are specific. After that CKY algorithm can be applied to choose the best tree for new statements using probabilities.
Do you expect that this algorithm is good or not and does it worth to continue improving it.</p>
",Multilingual Language Processing & Language Identification,natural language cfg builder algorithm working natural language processing project aim build library arabic language working po tagger thinking grammar phase since arabic language many others complicated grammar hard build context free grammar cfg reason idea algorithm build cfg probability pcfg language tagger corpus using unsupervised learning explain algorithm suppose three tagged statement input verb noun verb noun subject verb noun subject adverb algorithm give verb noun b subject c b adverb repeat methodology statement finish specific pcfg main power algorithm lie beyond fact seeing whole statement probability conditional specific cky algorithm applied choose best tree new statement using probability expect algorithm good doe worth continue improving
Natural language processing / text structure analysis starting point,"<p>I need to parse &amp; process a big set of semi-structured text (basically, legal documents - law texts, addendums to them, treaties, judge's decisions, ...). The most fundamental thing I'm trying to do is extract information on how subparts are structured - chapters, articles, subheadings, ... plus some metadata. My question is if anyone can point me to starting points for this type of text processing, because I'm sure there has been a lot of research into this but what I find is mostly on either parsing something with a strict grammar (like code) or completely free-form text (like google tries to do on webpages). I think if I get hold of the right keywords, I would have more success in google and my journal databases. Thanks.</p>
",Multilingual Language Processing & Language Identification,natural language processing text structure analysis starting point need parse process big set semi structured text basically legal document law text addendum treaty judge decision fundamental thing trying extract information subpart structured chapter article subheading plus metadata question anyone point starting point type text processing sure ha lot research find mostly either parsing something strict grammar like code completely free form text like google try webpage think get hold right keywords would success google journal database thanks
How would you interpret these dates?,"<p>I need to interpret relative date string like:</p>

<ul>
<li>last Friday</li>
<li>this Tuesday</li>
<li>next Wednesday</li>
</ul>

<p>The ""Last Friday"" form is easy (take the most recent Friday that is not today) but what about ""this"" vs. ""next""? Could ""this Wednesday"" be yesterday on a Thursday? Could ""this"" and ""next"" Friday be the same day in some cases and a week apart in others?</p>

<hr>

<p>p.s. Given that my target audience is American, I'm primarily interested in the US English vernacular use of the term and slightly less interested in other non-US English (for instance <code>en-gb</code>) usages so if you are non-US please say where you are from.</p>

<hr>

<p>My current thinking:</p>

<ul>
<li><strong>Last X</strong>: the most recent X not including today.</li>
<li><strong>This X</strong>: the immediate next X not including today.</li>
<li><strong>Next X</strong>: the X in the next week (with the start of the week being a bit arbitrary).</li>
</ul>

<p><a href=""http://smplsite.com/NaturalDate/Default.aspx"" rel=""nofollow noreferrer"">Try it out here</a> (be sure to check <em>allow relative</em>)</p>
",Multilingual Language Processing & Language Identification,would interpret date need interpret relative date string like last friday tuesday next wednesday last friday form easy take recent friday today v next could wednesday yesterday thursday could next friday day case week apart others p given target audience american primarily interested u english vernacular use term slightly le interested non u english instance usage non u please say current thinking last x recent x including today x immediate next x including today next x x next week start week bit arbitrary try sure check allow relative
Scope ambiguity in natural language,"<p>I feel it is bit curious to understand the Natural language processing.
I have the following questions..</p>

<p><li>What is meant by Scope ambiguity in natural language?</li>
<li>How can done Statistical resolution of scope ambiguity?</li>
<li>Which is the best language can I use for the Statistical resolution?</li></p>
",Multilingual Language Processing & Language Identification,scope ambiguity natural language feel bit curious understand natural language processing following question meant scope ambiguity natural language done statistical resolution scope ambiguity best language use statistical resolution
Localizing and Globalization of WinForms applications,"<p>We've developed a WinForms application (targeting .NET 2.0 with VS2008), we've just found out that we need to localize it for use in another language (other than english) :( What are the guidelines for developing multi-lingual languages in .NET?</p>

<p>Another application borrows Paint.NET's idea of globalization (using resources) but I was wondering if there are tools out there than can automate this for us - free would be nice but commercial is OK too.</p>

<p>Any ideas?</p>

<p>How do people normally work on projects that require multi-lingual interfaces? We're talking WiNForms apps. Do you just use IsLocalized = true and let .NET handle it?</p>
",Multilingual Language Processing & Language Identification,localizing globalization winforms application developed winforms application targeting net v found need localize use another language english guideline developing multi lingual language net another application borrows paint net idea globalization using resource wa wondering tool automate u free would nice commercial ok idea people normally work project require multi lingual interface talking winforms apps use islocalized true let net handle
Very basic English grammar parser,"<p>I'm writing a very basic parser(mostly just to better understand how they work) that takes a user's input of a select few words, detects whether the sentence structure is OK or Not OK, and outputs the result. The grammar is:</p>

<p>Sentence:
Noun Verb</p>

<p>Article Sentence</p>

<p>Sentence Conjunction Sentence</p>

<p>Conjunction:
""and""
""or""
""but""</p>

<p>Noun:
""birds""
""fish""
""C++""</p>

<p>Verb:
""rules""
""fly""
""swim""</p>

<p>Article:
""the""</p>

<p>Writing the grammar was simple. It's implementing the code that is giving me some trouble. My psuedocode for it is:</p>

<pre><code>main()
get user input (string words;)
while loop (cin &gt;&gt; words)
call sentence()
end main()

sentence()
call noun()
if noun() call verb() (if verb is true return ""OK"" ???)(else ""not ok""???)
else if not noun() call article()
                if article() call sentence() (if sentence is true ""OK""???)(else ""not""?)
else if not noun() call conjunction()
                   if sentence() conjunction() sentence() - no idea how to implement
                                                             return ""OK""
else ""not ok""
</code></pre>

<p>So there is my extremely sloppy psuedo code. I have a few questions on implementing it.</p>

<ol>
<li><p>For the word functions (noun, verb, etc.) how should I go about checking if they are true? (as in checking if the user's input has birds, fish, fly, swim, etc.)</p></li>
<li><p>How should I handle the conjunction call and the output?</p></li>
<li><p>Should I handle the output from the main function or the call functions?</p></li>
<li><p>None of the above questions matter if my psuedo code is completely wrong. Is there anything wrong with the basics?</p></li>
</ol>

<p>As an added note, I'm on a Chapter 6 exercise of Programming: Practice and Principles Using C++ so I'd prefer to use language syntax that I've already learned, so anything that falls into the category of advanced programming probably isn't very helpful. (The exercise specifically says not to use tokens, so count those out.)</p>

<p>Thanks in advance</p>

<p>Last Edit: In the book's public group I asked the same question and Bjarne Stroustrup commented back saying he put the exercise solution online. He basically had the input read into the sentence function and used if statements to return true or false. However, he didn't use articles so mine was much more complex. I guess if I've learned anything from this exercise its that when dealing with a lot of user input, tokenization is key (from what I know so far.) Here is my code for now. I may go back to it later because it is still very buggy and basically only returns if the sentence is OK and can't handle things like (noun, conjunction, sentence), but for now I'm moving on.</p>

<pre><code>#include ""std_lib_facilities.h""

bool article(string words)
{
               if (words == ""the"")
               return true;
               else return false;        
}

bool verb(string words)
{
               if (words == ""rules"" || words == ""fly"" || words == ""swim"")
               return true;
               else return false;                   
}

bool noun(string words)
{
               if (words == ""birds"" || words == ""fish"" || words == ""c++"")
               return true;
               else return false;                   
}

bool conjunction(string words)
{
              if (words == ""and"" || words == ""but"" || words == ""or"")
              return true;
              else return false;                  
}

bool sentence()
{
string w1;
string w2;
string w3;
string w4;

cin &gt;&gt; w1;
if (!noun(w1) &amp;&amp; !article(w1)) return false; // grammar of IFS!

cin &gt;&gt; w2;
if (noun(w1) &amp;&amp; !verb(w2)) return false;
if (article(w1) &amp;&amp; !noun(w2)) return false;

cin &gt;&gt; w3;
if (noun(w1) &amp;&amp; verb(w2) &amp;&amp; (w3 == ""."")) return true;
if (verb(w2) &amp;&amp; !conjunction(w3)) return false;
if (noun(w2) &amp;&amp; !verb(w3)) return false;
if (conjunction(w3)) return sentence();

cin &gt;&gt; w4;
if (article(w1) &amp;&amp; noun(w2) &amp;&amp; verb(w3) &amp;&amp; (w4 == ""."")) return true;
if (!conjunction(w4)) return false;
if (conjunction(w4)) return sentence();
}


int main()
{                                   
cout &lt;&lt; ""Enter sentence. Use space then period to end.\n"";
            bool test = sentence();
            if (test)
               cout &lt;&lt; ""OK\n"";
            else
               cout &lt;&lt; ""not OK\n"";
</code></pre>

<p>keep_window_open();
    }</p>
",Multilingual Language Processing & Language Identification,basic english grammar parser writing basic parser mostly better understand work take user input select word detects whether sentence structure ok ok output result grammar sentence noun verb article sentence sentence conjunction sentence conjunction noun bird fish c verb rule fly swim article writing grammar wa simple implementing code giving trouble psuedocode extremely psuedo code question implementing word function noun verb etc go checking true checking user input ha bird fish fly swim etc handle conjunction call output handle output main function call function none question matter psuedo code completely wrong anything wrong basic added note chapter exercise programming practice principle using c prefer use language syntax already learned anything fall category advanced programming probably helpful exercise specifically say use token count thanks advance last edit book public group asked question bjarne stroustrup commented back saying put exercise solution online basically input read sentence function used statement return true false however use article mine wa much complex guess learned anything exercise dealing lot user input tokenization key know far code may go back later still buggy basically return sentence ok handle thing like noun conjunction sentence moving keep window open
Multi layer perceptron for OCR,"<p>I intend to use a multi layer perceptron network trained with backpropagation (one hidden layer, inputs served as 8x8 bit matrices containing the B/W pixels from the image). The following questions arise:</p>

<ol>
<li>which type of learning should I use: batch or on-line?</li>
<li>how could I estimate the right number of nodes in the hidden layer? I intend to process the 26 letter of english alphabet.</li>
<li>how could I stop the training process, to avoid overfitting?</li>
<li>(not quite related) is there another better NN prved to perform better than MLP? I know about MLP stucking in local minima, overfitting and so on, so is there a better (soft computing-based) approach?</li>
</ol>

<p>Thanks</p>
",Multilingual Language Processing & Language Identification,multi layer perceptron ocr intend use multi layer perceptron network trained backpropagation one hidden layer input served x bit matrix containing b w pixel image following question arise type learning use batch line could estimate right number node hidden layer intend process letter english alphabet could stop training process avoid overfitting quite related another better nn prved perform better mlp know mlp stucking local minimum overfitting better soft computing based approach thanks
Rhyme in PHP,"<p>I am having a hard time to find a way to detect if <strong>two words has the same rhyme in English</strong>. It has not to be the same syllabic ending but something closer to <strong>phonetically similarity</strong>. </p>

<p>I can not believe in 2009 the only way of doing it is using those old fashioned rhyme dictionaries. Do you know any resources (in PHP would be a plus) to help me in this painful task?</p>

<p>Thank you.</p>

<p>Your hints were all really hepful. I will take some time to investigate it. Anyway, more info about DoubleMetaPhone can be found <a href=""http://swoodbridge.com/DoubleMetaPhone/"" rel=""nofollow noreferrer"">here in a proper PHP code</a> (the other one is an extension).
There are interesting information about MethaPhone function and doublemetaphone <a href=""http://es2.php.net/metaphone"" rel=""nofollow noreferrer"">in Php.net</a>.</p>

<p>They specially alert about how slow double metaphone is compared with metaphone (something like 100 times slower).</p>
",Multilingual Language Processing & Language Identification,rhyme php hard time find way detect two word ha rhyme english ha syllabic ending something closer phonetically similarity believe way using old fashioned rhyme dictionary know resource php would plus help painful task thank hint really hepful take time investigate anyway info doublemetaphone found proper php code one extension interesting information methaphone function doublemetaphone php net specially alert slow double metaphone compared metaphone something like time slower
How to analyze simple English sentences,"<p>Is there any library that can be used for analyzing (nlp) simple english text. For example it would be perfect if it can do that;
Input: ""I am going""
Output: I, go, present continuous tense</p>
",Multilingual Language Processing & Language Identification,analyze simple english sentence library used analyzing nlp simple english text example would perfect input going output go present continuous tense
Can anyone point me at a good example of pretty printing rules to &quot;english&quot;,"<p>I've got the equivalent of an AST that a user has built using a rule engine.  But when displaying a list of the rules, I'd like to be able to ""pretty print"" each rule into something that looks nice**.  Internally when represented as a string they look like s-expressions so imagine something like:</p>

<pre><code>(and (contains ""foo"" ""foobar"") (equals 4 (plus 2 2 )))
</code></pre>

<p>Can anyone point me at a program that has done a good job of displaying rules in a readable fashion?</p>

<p>** Needs to be localizable too, but I guess we'll leave that for extra credit.</p>
",Multilingual Language Processing & Language Identification,anyone point good example pretty printing rule english got equivalent ast user ha built using rule engine displaying list rule like able pretty print rule something look nice internally represented string look like expression imagine something like anyone point program ha done good job displaying rule readable fashion need localizable guess leave extra credit
