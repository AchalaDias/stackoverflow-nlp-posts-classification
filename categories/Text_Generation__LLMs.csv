Title,Description,category,combined_text
ChatGPT Token Limit,"<p>I want ChatGPT to remember past conversations and have a consistent (stateful) conversation.</p>
<p>I have seen several code of ChatGPT prompt engineering.</p>
<p>There were two ways to design the prompt shown below (pseudo code):</p>
<ol>
<li><p><strong>Use a single input</strong> (Cheap) &lt;- Better if possible</p>
</li>
<li><p><strong>Stack all of previous history</strong> (Expensive, Token Limitation)</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>def openai_chat(prompt):
    completions = openai.Completion.create(
        engine = &quot;text-davinci-003&quot;,
        prompt = prompt,
        max_tokens = 1024,
        n = 1,
        temperature = 0.8,
    )
    response = completions.choices[0].text.strip()
    return response

# 1. Use a single input
while True:
    prompt = input(&quot;User: &quot;)
    completion = openai_chat(prompt)

# 2. Stack all of previous history (prompt + completion)
prompt = &quot;&quot;
while True:
    cur_prompt = input(&quot;User: &quot;)
    prompt += cur_prompt  # pseudo code
    completion = openai_chat(prompt)
    prompt += completion  # pseudo code
</code></pre>
<p>Is it possible to choose the first way (the cheap one) to have a consistent conversation?</p>
<p>In other words, does ChatGPT remember past history even if the prompt only has the current input?</p>
",Text Generation & LLMs,chatgpt token limit want chatgpt remember past conversation consistent stateful conversation seen several code chatgpt prompt engineering two way design prompt shown pseudo code use single input cheap better possible stack previous history expensive token limitation possible choose first way cheap one consistent conversation word doe chatgpt remember past history even prompt ha current input
Getting word-level encodings from sub-word tokens encodings,"<p>I'm looking into using a pretrained BERT ('bert-base-uncased') model to extract contextualised word-level encodings from a bunch sentences.</p>

<p>Wordpiece tokenisation breaks down some of the words in my input into subword units. Possibly a trivial question, but I was wondering what would be the most sensible way to combine output encodings for subword tokens into word-level encodings.</p>

<p>Is averaging subword encodings a reasonable way to go? If not, is there any better alternative?</p>
",Text Generation & LLMs,getting word level encoding sub word token encoding looking using pretrained bert bert base uncased model extract contextualised word level encoding bunch sentence wordpiece tokenisation break word input subword unit possibly trivial question wa wondering would sensible way combine output encoding subword token word level encoding averaging subword encoding reasonable way go better alternative
Calculate the gradient with respect to attention but also the FFN layers for a pre-trained LLMs,"<p>I would like to return the gradient with respect to specific layers and the FFN layer in the Transformer architecture of pre-trained LLMs from the hugging-face model. Is that even possible?</p>
<p>I am working with the code of this <a href=""https://github.com/kristosh/xAI/blob/main/attn_vizualizations.py"" rel=""nofollow noreferrer"">repo</a> which is the following:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-medium-4k-instruct&quot;)
model = AutoModelForCausalLM.from_pretrained(
    &quot;microsoft/Phi-3-medium-4k-instruct&quot;,  # note: check spelling if you get error
    device_map=&quot;auto&quot;,
    torch_dtype=torch.float16,            # or torch.float32 if preferred
    trust_remote_code=True
)

# Create a pipeline
generator = pipeline(
    &quot;text-generation&quot;,
    model = model,
    tokenizer = tokenizer,
    return_full_text= False,
    max_new_tokens = 100,
    do_sample = False
)

# Prepare a prompt
prompt = &quot;Whats is the co-capital of Greece according to the country's public opinion?&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
inputs = inputs.to(&quot;cuda:0&quot;)  # send inputs to cuda

# Run the model with attention outputs enabled
# Make sure to pass output_attentions=True
outputs = model(input_ids=inputs.input_ids, output_attentions=True)

# outputs.attentions is a tuple with one element per layer
# Each element is a tensor of shape (batch_size, num_heads, seq_len, seq_len)
attentions = outputs.attentions

# Generate output
output = generator(prompt)
print(output[0][&quot;generated_text&quot;])
</code></pre>
<p>How to return the gradient concerning the input or a specific attention layer (in a similar fashion with <code>grad-CAM</code> in <code>CNN</code>). Is it possible to do that in <code>transformers</code>?</p>
",Text Generation & LLMs,calculate gradient respect attention also ffn layer pre trained llm would like return gradient respect specific layer ffn layer transformer architecture pre trained llm hugging face model even possible working code repo following return gradient concerning input specific attention layer similar fashion possible
BertScore giving me high results for unrelated sentences,"<p>I've been trying out bert_score to do some metrics. When testing it, I'm finding that it's giving me some very high scores for some totally unrelated sentences.</p>
<p>For example, I tried this:</p>
<pre class=""lang-py prettyprint-override""><code>refs = [&quot;Ladies in a kitchen laughing at the camera while one grates cheese.&quot;]
cands=[&quot;sky&quot;]
P,R,F = score(cands=cands,refs=refs,lang = 'en')
print(P,R,F)
</code></pre>
<p>The above execution returns:</p>
<pre class=""lang-none prettyprint-override""><code>tensor([0.8362]) tensor([0.8028]) tensor([0.8192])
</code></pre>
<p>I'm not sure why it's giving me these, but I would assume it's not what it should give me for those two sentences.</p>
",Text Generation & LLMs,bertscore giving high result unrelated sentence trying bert score metric testing finding giving high score totally unrelated sentence example tried execution return sure giving would assume give two sentence
How many obs per class are necessary? - transfer learning w. BERT fine-tuning,"<p>I seek advice on a classification problem in industry.</p>
<p>The rows in a dataset must be classified/labeled--it lacks a target/column (labels have dot-separated levels like 'x.x.x.x.x.x.x')--during every business cycle. For each cycle, a dataset is given as input to this exercise. The dataset includes some variables, most importantly 1. an ID variable and 2. a short text description. When the dataset is correctly classified, the ID will correspond perfectly to a class. At every iteration, most of the ID---class links as identities will carry over, but some won't: There will be new labels and some labels get redefined. Basically, <em>there will be unlabeled rows at every iteration</em>.</p>
<p>The classes are assigned using variable 2, the text description (besides a few others) compared with the content of a, let's say, scoring manual accompanying the cycle's new dataset. (Well, rather, the dataset and the key come from two independent business processes, but we can ignore that here.) As a de facto scoring manual, the classes are unique and are described only once using a handful of descriptions, which are short text fields about what identifies, =, and what contrasts, !=, a class. So, <em>in the manual (available as a second dataset), each class is described only once</em>.</p>
<p>The desire is to classify the datasets ad infinitum as automatically as possible. The dataset has already been classified at time T0 but will need more new classes at T1. It is possible to supervise a learning algorithm on the first batch. <strong>The question</strong> is whether subsequent batches require labeling by experts for training the model or, ideally, whether the 'scoring manual' with one row/obs/example per label may suffice for fine-tuning? (more loosely formulated:  <a href=""https://stats.stackexchange.com/q/310947/207649"">How much data is needed for transfer learning?</a>; and in the case of training from scratch: <a href=""https://stats.stackexchange.com/questions/446667/how-many-data-points-per-class-is-neccesary-to-train-a-multi-class-deep-learning"">how many data points per class is neccesary to train a multi-class deep learning</a>)</p>
",Text Generation & LLMs,many ob per class necessary transfer learning w bert fine tuning seek advice classification problem industry row dataset must classified labeled lack target column label dot separated level like x x x x x x x every business cycle cycle dataset given input exercise dataset includes variable importantly id variable short text description dataset correctly classified id correspond perfectly class every iteration id class link identity carry new label label get redefined basically unlabeled row every iteration class assigned using variable text description besides others compared content let say scoring manual cycle new dataset well rather dataset key come two independent business process ignore de facto scoring manual class unique described using handful description short text field identifies contrast class manual available second dataset class described desire classify datasets ad infinitum automatically possible dataset ha already classified time need new class possible supervise learning algorithm first batch question whether subsequent batch require labeling expert training model ideally whether scoring manual one row ob example per label may suffice fine tuning loosely formulated
Generating an n-gram dataset based on an LLM,"<p>I want a dataset of common n-grams and their log likelihoods. Normally I would download the <a href=""https://storage.googleapis.com/books/ngrams/books/datasetsv3.html"" rel=""nofollow noreferrer"">Google Books Ngram Exports</a>, but I wonder if I can generate a better dataset using a large language model. For example, this script uses <a href=""https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_completion"" rel=""nofollow noreferrer"">llama_cpp.Llama.create_completion</a> to find likely 3-grams starting with &quot;welcome to&quot;:</p>
<pre class=""lang-py prettyprint-override""><code>from llama_cpp import Llama # pip install llama-cpp-python

llm = Llama.from_pretrained(
    repo_id=&quot;unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF&quot;,
    filename=&quot;DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf&quot;,
    logits_all=True,
)
print(
    llm.create_completion(
        &quot;welcome to&quot;,
        max_tokens=1,
        logprobs=10,
    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],
)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>{' the': np.float32(-0.18572943), ' this': np.float32(-3.444591), ' our': np.float32(-4.0559974), ' python': np.float32(-4.3010955), ' a': np.float32(-4.571982), ' bc': np.float32(-5.036485), ' module': np.float32(-5.4879394), ' week': np.float32(-5.7402453), ' all': np.float32(-6.2308974), ' thread': np.float32(-6.272795)}
</code></pre>
<p>One issue is that the prompt gets prefixed with a <a href=""https://www.linkedin.com/pulse/what-special-tokens-tokenization-farhan-naqvi-uxxsf/"" rel=""nofollow noreferrer"">BOS token</a>, so I only get n-grams that appear at the beginning of a sentence. I can fix this by passing a list of tokens instead of a string. But this leads to a problem when generating 1-grams:</p>
<pre class=""lang-py prettyprint-override""><code>print(
    llm.create_completion(
        [],
        max_tokens=1,
        logprobs=10,
    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],
)
</code></pre>
<p><code>AssertionError</code> at <a href=""https://github.com/abetlen/llama-cpp-python/blob/v0.3.7/llama_cpp/llama.py#L788"" rel=""nofollow noreferrer"">llama.py line 788</a>: <code>assert self.n_tokens &gt; 0</code></p>
<p>Apparently <a href=""https://github.com/ggerganov/llama.cpp"" rel=""nofollow noreferrer"">llama.cpp</a> is unable to generate text when no context is provided. I confirmed this by using llama.cpp directly without the Python wrapper. My question is, <strong>is this an arbitrary limitation of the library, or a fundamental limitation of the language model?</strong></p>
<p>I found a possible clue in the model's <a href=""https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/blob/6393b7559e403fd1d80bfead361586fd6f630a4d/config.json#L10"" rel=""nofollow noreferrer"">config.json</a> file:</p>
<pre class=""lang-json prettyprint-override""><code>  &quot;initializer_range&quot;: 0.02,
</code></pre>
<p>The <a href=""https://huggingface.co/docs/transformers/en/model_doc/qwen2#transformers.Qwen2Config.initializer_range"" rel=""nofollow noreferrer"">documentation</a> says that <code>initializer_range</code> is</p>
<blockquote>
<p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
</blockquote>
<p>I imagine that the model has a hidden state vector which is initialized with random values sampled from a normal distribution, and the values get updated as context is added. I wonder if it's possible to sample from the model in its initial random state, and get a list of the most common words by sampling multiple times with different random seeds.</p>
",Text Generation & LLMs,generating n gram dataset based llm want dataset common n gram log likelihood normally would download google book ngram export wonder generate better dataset using large language model example script us llama cpp llama create completion find likely gram starting welcome output one issue prompt get prefixed bos token get n gram appear beginning sentence fix passing list token instead string lead problem generating gram llama py line apparently llama cpp unable generate text context provided confirmed using llama cpp directly without python wrapper question arbitrary limitation library fundamental limitation language model found possible clue model config json file documentation say standard deviation truncated normal initializing weight matrix imagine model ha hidden state vector initialized random value sampled normal distribution value get updated context added wonder possible sample model initial random state get list common word sampling multiple time different random seed
LLM Model Lacking Confidence and Changing Answers Based on User Input,"<p>I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct.</p>
<p>For example, when asked &quot;What is the capital of France?&quot;, the model correctly responds with &quot;Paris.&quot; However, if the user replies &quot;No, it's Berlin,&quot; the model accepts this incorrect response and later provides &quot;Berlin&quot; as the capital of France when asked again.</p>
<p>I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the model’s robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated.</p>
",Text Generation & LLMs,llm model lacking confidence changing answer based user input trained large language model llm using rag method answer user query however facing issue model lack confidence answer change based user input even initial response correct example asked capital france model correctly responds paris however user reply berlin model accepts incorrect response later provides berlin capital france asked tried using different prompt template reinforce answer consistency issue persists improve model robustness prevent altering correct answer based user response suggestion guidance would greatly appreciated
llama-cpp-python not using NVIDIA GPU CUDA,"<p>I have been playing around with <a href=""https://github.com/oobabooga/text-generation-webui"" rel=""noreferrer"">oobabooga text-generation-webui </a> on my Ubuntu 20.04 with my NVIDIA GTX 1060 6GB for some weeks without problems. I have been using llama2-chat models sharing memory between my RAM and NVIDIA VRAM. I installed without much problems following the intructions on its repository.</p>
<p>So what I want now is to use the model loader <code>llama-cpp</code> with its package <code>llama-cpp-python</code> bindings to play around with it by myself. So using the same miniconda3 environment that oobabooga text-generation-webui uses I started a jupyter notebook and I could make inferences and everything is working well <em>BUT ONLY for CPU</em>.</p>
<p>A working example bellow,</p>
<pre><code>from llama_cpp import Llama

llm = Llama(model_path=&quot;/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin&quot;, 
            n_gpu_layers=32, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), 

prompt = &quot;&quot;&quot;[INST] &lt;&lt;SYS&gt;&gt;
Name the planets in the solar system? 
&lt;&lt;/SYS&gt;&gt;
[/INST] 
&quot;&quot;&quot;
output = llm(prompt, max_tokens=350, echo=True)
print(output['choices'][0]['text'].split('[/INST]')[-1])
</code></pre>
<blockquote>
<p>Of course! Here are the eight planets in our solar system, listed in order from closest to farthest from the Sun:</p>
<ol>
<li>Mercury</li>
<li>Venus</li>
<li>Earth</li>
<li>Mars</li>
<li>Jupiter</li>
<li>Saturn</li>
<li>Uranus</li>
<li>Neptune</li>
</ol>
</blockquote>
<blockquote>
<p>Note that Pluto was previously considered a planet but is now classified as a dwarf planet due to its small size and unique orbit.</p>
</blockquote>
<p>I want to make inference using GPU as well. What is wrong?
Why can't I offload to gpu like the parameter <code>n_gpu_layers=32</code> specifies and also like <code>oobabooga text-generation-webui</code> already does on the same miniconda environment whithout any problems?</p>
",Text Generation & LLMs,llama cpp python using nvidia gpu cuda playing around oobabooga text generation webui ubuntu nvidia gtx gb week without problem using llama chat model sharing memory ram nvidia vram installed without much problem following intructions repository want use model loader package binding play around using miniconda environment oobabooga text generation webui us started jupyter notebook could make inference everything working well cpu working example bellow course eight planet solar system listed order closest farthest sun mercury venus earth mar jupiter saturn uranus neptune note pluto wa previously considered planet classified dwarf planet due small size unique orbit want make inference using gpu well wrong offload gpu like parameter specifies also like already doe miniconda environment whithout problem
How to segment and transcribe an audio from a video into timestamped segments?,"<p>I want to segment a video transcript into chapters based on the content of each line of speech. The transcript would be used to generate a series of start and end timestamps for each chapter. This is similar to how YouTube now &quot;auto-chapters&quot; videos.</p>
<p>Example .srt transcript:</p>
<pre><code>...

70
00:02:53,640 --&gt; 00:02:54,760
All right, coming in at number five,

71
00:02:54,760 --&gt; 00:02:57,640
we have another habit that saves me around 15 minutes a day
...
</code></pre>
<p>I have had minimal luck doing this with ChatGPT as it finds it difficult to both segment by topic and recollect start and end timestamps accurately. I am now exploring whether there are other options for doing this.</p>
<p>I know topic modeling based on time series is possible with some python libraries. I have also read about text tiling as another option. <strong>What options are there for achieving an outcome like this?</strong></p>
<p>Note: The format above (.srt) is not necessary. It's just the idea that the input is a list of text-content with start and end timestamps.</p>
",Text Generation & LLMs,segment transcribe audio video timestamped segment want segment video transcript chapter based content line speech transcript would used generate series start end timestamps chapter similar youtube auto chapter video example srt transcript minimal luck chatgpt find difficult segment topic recollect start end timestamps accurately exploring whether option know topic modeling based time series possible python library also read text tiling another option option achieving outcome like note format srt necessary idea input list text content start end timestamps
Is it Possible to feed Embeddings generate by BERT to a LSTM based autoencoder to get the latent space?,"<p>I've just learn about how BERT produce embeddings. I might not understand it fully.</p>
<p>I was thinking of doing a project of leveraging those embeddings and feed it to an autoencoder to generate latent space for my text data.</p>
<p>I am a bit skeptical that Bert embeddings has produce like sort of relationship between words in a single vector, would it not like loss those relationship information if I feed it in an autoencoder?</p>
<p>It would be nice if someone could give an opinion regarding this.</p>
<p>I am trying to group like text with similar emotions clustered in the latent space in the same time applying contrastive learning concept in  a supervised manner.</p>
",Text Generation & LLMs,possible feed embeddings generate bert lstm based autoencoder get latent space learn bert produce embeddings might understand fully wa thinking project leveraging embeddings feed autoencoder generate latent space text data bit skeptical bert embeddings ha produce like sort relationship word single vector would like loss relationship information feed autoencoder would nice someone could give opinion regarding trying group like text similar emotion clustered latent space time applying contrastive learning concept supervised manner
How can one get the LLM model name and version using Google AI Edge SDK?,"<p>I use Google AI Edge SDK to call an on-device LLM (<a href=""https://github.com/android/ai-samples/tree/main"" rel=""nofollow noreferrer"">example app</a>). How can I get the LLM model name and version that my code uses?</p>
<p><a href=""https://developer.android.com/ai/gemini-nano"" rel=""nofollow noreferrer"">https://developer.android.com/ai/gemini-nano</a> says it uses Gemini Nano:</p>
<p><a href=""https://i.sstatic.net/fzpdHEX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fzpdHEX6.png"" alt=""enter image description here"" /></a></p>
<p>but there are at least 2 versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), and I guess there'll be more soon, if not already (different training set, compression methods, parameter count, etc.).</p>
",Text Generation & LLMs,one get llm model name version using google ai edge sdk use google ai edge sdk call device llm example app get llm model name version code us say us gemini nano least version gemini nano nano billion parameter nano billion parameter guess soon already different training set compression method parameter count etc
How do I install language model for spacy on Kaggle?,"<p>Aloha! Everybody knows how to install model at home:</p>
<blockquote>
<p><code>python -m spacy download ru_core_news_md</code></p>
</blockquote>
<p>But since python notebook on Kaggle is isolated of the global web, it does not seem possible to do so.</p>
<p>Of course, I can download ru_core_news_sm-3.7.0-py3-none-any.whl file from spacy.io website and upload it into my dataset &quot;ru-core-news-sm&quot; on Kaggle.</p>
<p>Question is, how do I import it in my Spacy? I will be very appreciative for any kind of answer!</p>
",Text Generation & LLMs,install language model spacy kaggle aloha everybody know install model home since python notebook kaggle isolated global web doe seem possible course download ru core news sm py none whl file spacy io website upload dataset ru core news sm kaggle question import spacy appreciative kind answer
LLM Question answer models for yes or no answers,"<p>imagine I have the following dataset:</p>
<pre><code>import pandas as pd

# Positive and negative sentences
positive_sentences = [
    &quot;I love this product!&quot;,
    &quot;The weather is beautiful today.&quot;,
    &quot;The team did an excellent job.&quot;,
    &quot;She is a very talented musician.&quot;
]

negative_sentences = [
    &quot;I am not satisfied with the service.&quot;,
    &quot;The food was terrible at that restaurant.&quot;,
    &quot;The movie was a complete disappointment.&quot;,
    &quot;He made a lot of mistakes in the project.&quot;
]

# Combine positive and negative sentences
sentences = positive_sentences + negative_sentences

# Create a DataFrame with a &quot;snippet&quot; column
df = pd.DataFrame({'snippet': sentences})

# Display the DataFrame
print(df)
</code></pre>
<p>I want to use a LLM model that answers the following question. Is the following sentence positive, negative or neutral?</p>
<p>This is what I have tried so far:</p>
<pre><code>## installing the libraries:
import pandas as pd
from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer
from transformers import RobertaTokenizer
from transformers import AutoTokenizer, RobertaForQuestionAnswering
import torch
from transformers import BertForQuestionAnswering, BertTokenizer

# Setting up the model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

# Create a question answering pipeline
question_answerer = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)

for index, row in df.iterrows():
    article = row[&quot;snippet&quot;]  
    prompt = f&quot;Is the following sentence positive, negative or neutral? {article}&quot;

    result = question_answerer(question=prompt, context=article)

    # Check if &quot;answer&quot; key is in the result
    if &quot;answer&quot; in result:
        main_theme = result[&quot;answer&quot;]
        print(f&quot;Article {index+1} main theme: {main_theme}&quot;)
    else:
        print(f&quot;Article {index+1} main theme not found in the result.&quot;)

</code></pre>
",Text Generation & LLMs,llm question answer model yes answer imagine following dataset want use llm model answer following question following sentence positive negative neutral tried far
Questions about training LLMs on large text datasets for text generation from scratch,"<p>I made a fully custom made GPT in Jax (with Keras 3), using Tensorflow for the data pipeline.</p>
<p>I've trained the model on the Shakespeare dataset and got good results (so no problem with the model).
Now I want to train it on the Tiny-Stories dataset which is pretty big with GPT of 15M parameters.</p>
<p>Here is the code for loading the data:</p>
<pre class=""lang-py prettyprint-override""><code>def get_dataset_lists(ds_path:str):
    dataset = open(ds_path, &quot;r&quot;, encoding=&quot;utf-8&quot;).read() # [...]
    dataset = dataset.split(&quot;&lt;|endoftext|&gt;&quot;)
    r.shuffle(dataset)
    dataset:list = spm.Encode( # llama's sentence piece encoder
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=True,
            add_eos=False
        ) # [[SOS story], ..., [SOS story]]
    print(&quot;\tNumber of stories:&quot;, len(dataset))
    return dataset

def tf_dataload(
    dataset:list,
    batch_size:int,
    maxlen:int,
    shift:int,
):
    import functools; import operator
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(&quot;\tNumber of tokens in the dataset is&quot;, num_tokens)
    unique_tok = set(dataset); print(&quot;\tNumber of unique tokens in the dataset is&quot;, len(unique_tok))
    # [SOS story ... SOS story]
    dataset = tf.data.Dataset.from_tensor_slices(dataset)
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*batch_size, reshuffle_each_iteration=reshuffle_each_iteration)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(batch_size*100)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)
    return dataset # (shape(m//B, B, maxlen) shape(m//B, B, maxlen))

def load_data(
    train_ds_path:str,
    val_ds_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
):  
    print(&quot;Training Dataset:&quot;)
    train_ds = tf_dataload(get_dataset_lists(train_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True)
    print(&quot;Validation Dataset:&quot;)
    val_ds = tf_dataload(get_dataset_lists(val_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True)
    print(f&quot;\n{train_ds}\n{val_ds}&quot;)
    datasets = {&quot;train&quot;: train_ds.repeat(), &quot;val&quot;:val_ds}
    return datasets
</code></pre>
<ul>
<li><strong>I've certain questions regarding
the value of the <code>shift</code>?</strong><br />
First I set it equal to 1, but the training was very slow, even after 100000 steps it didn't converge even though it was decreasing slowly (I think there's no problem with the learning rate as I plotted Loss Vs Lr and selected the max learning rate possible and used cosine decay with warmup)</li>
</ul>
<p><a href=""https://i.sstatic.net/MoKO4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MoKO4.png"" alt=""enter image description here"" /></a></p>
<p>So I looked into <a href=""https://github.com/karpathy/llama2.c/blob/master/tinystories.py#L217"" rel=""nofollow noreferrer"">Karpathy's llama-2 repo</a> and the shift was equal to maxlen.
So I set it equal to <code>maxlen</code> and trained it for 100000 steps but the model is learning very slowly, and didn't get a loss even close to what <a href=""https://github.com/karpathy/llama2.c#:%7E:text=15M-,1.072,-stories15M.bin"" rel=""nofollow noreferrer"">Karpathy got</a>
(I don't know what's the problem, as I've closely followed Karpathy's llama2 repo)
<strong>What is shift generally equal to when pre-training an LLM on Language Modelling?
Shouldn't it be 1, because the transformer model is not positionally invariant, and it would affect model performance if <code>shift</code> is not equal to 1? But then the number of samples will be very large...?</strong></p>
<ul>
<li>And for what number of steps to train a LLM given the number of tokens</li>
</ul>
<p>You may find the below helpful...</p>
<pre class=""lang-py prettyprint-override""><code>@dataclass
class GPTArgs:
    &quot;&quot;&quot;GPT Configuration&quot;&quot;&quot;
    d_model:int = 288
    num_layers:int = 6
    num_heads:int = 6
    max_context_length:int = 256
    vocab_size:int = VOCAB_SIZE # 32K
    output_units:int = None # equal to vocab_size if None in model init  
    assert d_model % 2 == 0
    assert d_model % num_heads == 0
    dropout_rate:float = 0.1

@dataclass
class TArgs:
    # lr scheduler
    init_lr:float = 1e-7
    max_lr:float = 6.5e-4
    min_lr:float = 0.1*max_lr # The factor is usually 0.1 or 0.0
    num_steps:int = 100_000
    warmup_steps:int = 1000 # 1000, to make training more stable instead of 2000
    decay_steps:int = num_steps

    # optimizer
    beta1:float = 0.9
    beta2:float = 0.95
    weight_decay:float = 1e-1
    clipvalue:float = 1e0
    num_grad_accumalation_steps:int = 4
    # num_tok_per_update = batch_size * maxlen * gradient_accumalation = 128 * 256 * 4 = 131_072

    # training
    checkpoint:str = 'weights/GPTstories/Epoch{epoch}.weights.h5'
    train_ds_path:str = &quot;TinyStoriesDataset/TinyStories-train.txt&quot;
    val_ds_path:str = &quot;TinyStoriesDataset/TinyStories-valid.txt&quot;
    steps_per_epoch = eval_freq = 2000
    eval_steps:int = 200
    batch_size:int = 128 
    patience:int = 10 # early stopping with restore best weights
</code></pre>
<h2>Update 1:</h2>
<p>I thought that the model wasn't getting the training samples uniformly so I modified the data pipeline and also increased the number of steps to <a href=""https://github.com/karpathy/llama2.c#:%7E:text=brief%20training%20guide"" rel=""nofollow noreferrer"">200,000</a>.
But there were no significant improvements. The training was still very slow by the end and loss was decreasing by 0.01 every epoch (of 2000 steps)... Got a loss of 1.67 on validation set</p>
<pre class=""lang-py prettyprint-override""><code>def pretokenize_and_save_dataset(dataset_path:str, num_shards:int, shard_dir:str):
    dataset = open(dataset_path, &quot;r&quot;, encoding=&quot;utf-8&quot;).read() # [...]
    dataset = dataset.split(&quot;&lt;|endoftext|&gt;&quot;)
    r.shuffle(dataset)
    dataset:list = spm.Encode(
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=True,
            add_eos=False
        ) # [[SOS story], ..., [SOS story]]
    print(&quot;Dataset:&quot;)
    print(&quot;\tNumber of stories:&quot;, len(dataset))

    # flatten
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(&quot;\tNumber of tokens in the dataset:&quot;, num_tokens)
    print(&quot;\tNumber of unique tokens in the dataset:&quot;, len(set(dataset)))
    
    dataset = np.asarray(dataset, dtype=np.uint16) # [SOS story ... SOS story]
    print(&quot;\tAvg length of story:&quot;, num_tokens/((dataset==1).sum()))

    # shard and save dataset
    sharded_datasets_list = np.array_split(dataset, num_shards) # [[SOS story...], [...], [...], ...]
    filenames = [os.path.join(shard_dir, f&quot;shard{i+1}.npy&quot;) for i in range(num_shards)]
    
    for filename, sharded_ds in zip(filenames, sharded_datasets_list):
        with open(filename, &quot;wb&quot;) as f:
            np.save(f, sharded_ds)
    return filenames

def load_data_as_tfds(
    dataset:np.ndarray,
    maxlen:int,
    shift:int,
):
    # [SOS story ... SOS story]
    dataset = tf.data.Dataset.from_tensor_slices(dataset.tolist())
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*128)
    return dataset

def batch_tfds(
        dataset:tf.data.Dataset,
        batch_size:int,
):
    dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(batch_size*1000)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.repeat().prefetch(tf.data.AUTOTUNE)
    return dataset

def load_data(
    dataset_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
    num_shards:int,
    shard_dir:str
):  
    if os.path.exists(shard_dir) and os.listdir(shard_dir):
        filenames = glob.glob(os.path.join(shard_dir, &quot;*.npy&quot;))
    else:
        os.makedirs(shard_dir)
        filenames = pretokenize_and_save_dataset(dataset_path, num_shards=num_shards, shard_dir=shard_dir)
    r.shuffle(filenames)
    to_tfds = lambda dataset: load_data_as_tfds(dataset, maxlen=maxlen, shift=shift)
    num_train_shards = round(0.9651*num_shards)
    num_val_shards = num_shards-num_train_shards

    print(&quot;Training Dataset:&quot;)
    print(f&quot;\tNumber of files taken for training: {num_train_shards}/{num_shards}&quot;)
    train_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[:num_train_shards]]
    train_ds = tf.data.Dataset.sample_from_datasets(train_datasets_lists, weights=[1/num_train_shards]*num_train_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    train_ds = batch_tfds(train_ds, batch_size=batch_size)

    print(&quot;Validation Dataset:&quot;)
    print(f&quot;\tNumber of files taken for validation: {num_val_shards}/{num_shards}&quot;)
    val_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[num_train_shards:]]
    val_ds = tf.data.Dataset.sample_from_datasets(val_datasets_lists, weights=[1/num_val_shards]*num_val_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    val_ds = batch_tfds(val_ds, batch_size=batch_size)

    print(f&quot;\n{train_ds}\n{val_ds}&quot;)
    datasets = {&quot;train&quot;: train_ds, &quot;val&quot;:val_ds}
    return datasets
</code></pre>
",Text Generation & LLMs,question training llm large text datasets text generation scratch made fully custom made gpt jax kera using tensorflow data pipeline trained model shakespeare dataset got good result problem model want train tiny story dataset pretty big gpt parameter code loading data certain question regarding value first set equal training wa slow even step converge even though wa decreasing slowly think problem learning rate plotted loss v lr selected max learning rate possible used cosine decay warmup looked karpathy llama repo shift wa equal maxlen set equal trained step model learning slowly get loss even close karpathy got know problem closely followed karpathy llama repo shift generally equal pre training llm language modelling transformer model positionally invariant would affect model performance equal number sample large number step train llm given number token may find helpful update thought model getting training sample uniformly modified data pipeline also increased number step significant improvement training wa still slow end loss wa decreasing every epoch step got loss validation set
Output probabilities of tokens generated by Llama 2 using Transformers,"<p>Given input tokens, LLMs output the tokens in their vocabulary that have the highest probability of coming after the input tokens.</p>
<p>I would like to print the probability of each token generated by the model in response to a prompt to see how confident the model is in its generated tokens. I would like to do this on Llama-2-7b-chat-hf on a simple prompt like : &quot;Could you give me 3 cities located in Europe ?&quot;.</p>
<p>In order to do this I have the following code :</p>
<pre><code>from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer
import torch
import numpy as np

device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;

model=LlamaForCausalLM.from_pretrained(&quot;Llama-2-7b-chat-hf&quot;).to(device)
tokenizer= LlamaTokenizer.from_pretrained(&quot;Llama-2-7b-chat-hf&quot;)

prompt = &quot;Could you give me 3 cities located in Europe ?&quot;

inputs = tokenizer([prompt], return_tensors=&quot;pt&quot;).to(device)

outputs=model.generate(**inputs,return_dict_in_generate=True, output_scores=True,max_new_tokens=75)

transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)

input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
generated_tokens = outputs.sequences[:,input_length:]

for tok, score in zip(generated_tokens[0], transition_scores[0]):
        # | token | token string | logits | probability
            print(f&quot;| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}&quot;)
</code></pre>
<p>As a result I receive the following :</p>
<pre><code>| token | token string | logits | probability
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 10605 | Here     | -3.0267 | 4.85%
|   526 | are      | 0.0000 | 100.00%
| 29871 |          | -0.3506 | 70.43%
| 29941 | 3        | 0.0000 | 100.00%
| 14368 | cities   | 0.0000 | 100.00%
|  5982 | located  | 0.0000 | 100.00%
|   297 | in       | 0.0000 | 100.00%
|  4092 | Europe   | 0.0000 | 100.00%
| 29901 | :        | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29896 | 1        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  3681 | Paris    | 0.0000 | 100.00%
| 29892 | ,        | 0.0000 | 100.00%
|  3444 | France   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29906 | 2        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  9184 | Rome     | -1.0413 | 35.30%
| 29892 | ,        | 0.0000 | 100.00%
| 12730 | Italy    | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29941 | 3        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  4517 | London   | 0.0000 | 100.00%
| 29892 | ,        | 0.0000 | 100.00%
|  3303 | United   | 0.0000 | 100.00%
| 12626 | Kingdom  | 0.0000 | 100.00%
|     2 | &lt;/s&gt;     | 0.0000 | 100.00%
</code></pre>
<p>As you can see, most of the words have a probability of 100% of being chosen which seems very odd to me. Llama 2 has a vocabulary of 32000 tokens surely there are other tokens that could be used at the place of those tokens, I would agree with something like 70% but 100% should be impossible. Which makes me believe something is wrong in my code.</p>
<p>Would you agree with me ? And if yes, would you know what is wrong in my code ?</p>
",Text Generation & LLMs,output probability token generated llama using transformer given input token llm output token vocabulary highest probability coming input token would like print probability token generated model response prompt see confident model generated token would like llama b chat hf simple prompt like could give city located europe order following code result receive following see word probability chosen seems odd llama ha vocabulary token surely token could used place token would agree something like impossible make believe something wrong code would agree yes would know wrong code
how can i fuse embeddings in a manner such that it increase efficiency and score?,"<p>I've been working on a problem where the goal is to supplement traditional embeddings with LLM-generated embeddings (I'm using the last_hidden_state for this purpose). So far, I've tried simply concatenating them and using a cross-attention mechanism. While concatenating the embeddings yields similar results to using traditional embeddings alone (and the score is definitely not better), the cross-attention mechanism unexpectedly degraded the performance. Are there other methods that could potentially improve the score? Code is provided below:</p>
<p>Code for Simple Concatenation:</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(self, depot_xy, node_xy_demand_tw, llm_embeddings):
        moe_loss = 0
        
        # Get traditional embeddings
        if isinstance(self.embedding_depot, MoE) or isinstance(self.embedding_node, MoE):
            embedded_depot, loss_depot = self.embedding_depot(depot_xy)
            embedded_node, loss_node = self.embedding_node(node_xy_demand_tw)
            moe_loss = moe_loss + loss_depot + loss_node
        else:
            embedded_depot = self.embedding_depot(depot_xy)
            embedded_node = self.embedding_node(node_xy_demand_tw)

        # Project LLM embeddings and normalize
        # print(320, self.llm_projection[0].weight.dtype, llm_embeddings.dtype)
        projected_llm = self.llm_projection(llm_embeddings)
        projected_llm = self.layer_norm(projected_llm)
        
        # Combine traditional embeddings with LLM embeddings
        depot_combined = embedded_depot + projected_llm[:, :1, :]  # For depot
        node_combined = embedded_node + projected_llm[:, 1:, :]   # For nodes
        
        out = torch.cat((depot_combined, node_combined), dim=1)

        for layer in self.layers:
            out, loss = layer(out)
            moe_loss = moe_loss + loss

        return out, moe_loss
</code></pre>
<p>Code for Cross attention based fusion with Cross attention class:</p>
<pre class=""lang-py prettyprint-override""><code>########################################
# CROSS ATTENTION
########################################

class CrossAttentionFusion(nn.Module):
    def __init__(self, embedding_dim, head_num, qkv_dim):
        super().__init__()
        self.head_num = head_num
        
        # Cross attention layers for traditional -&gt; LLM
        self.Wq_trad = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wk_llm = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wv_llm = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        
        # Cross attention layers for LLM -&gt; traditional
        self.Wq_llm = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wk_trad = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wv_trad = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        
        # Output projections
        self.W_out_trad = nn.Linear(head_num * qkv_dim, embedding_dim).to(dtype=torch.bfloat16)
        self.W_out_llm = nn.Linear(head_num * qkv_dim, embedding_dim).to(dtype=torch.bfloat16)
        
        # Layer norms
        self.norm_trad = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)
        self.norm_llm = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)

    def forward(self, trad_emb, llm_emb):
        # Cross attention: traditional -&gt; LLM
        # print(f&quot;trad_emb dtype: {trad_emb.dtype}, shape: {trad_emb.shape}&quot;)
        # print(f&quot;llm_emb dtype: {llm_emb.dtype}, shape: {llm_emb.shape}&quot;)
        # print(f&quot;Wq_trad type: {self.Wq_trad.weight.dtype}, shape: {self.Wq_trad.weight.shape}&quot;)
        q_trad = reshape_by_heads(self.Wq_trad(trad_emb), self.head_num)
        k_llm = reshape_by_heads(self.Wk_llm(llm_emb), self.head_num)
        v_llm = reshape_by_heads(self.Wv_llm(llm_emb), self.head_num)
        
        trad_attends_llm = multi_head_attention(q_trad, k_llm, v_llm)
        trad_fused = self.W_out_trad(trad_attends_llm)
        trad_out = self.norm_trad(trad_emb + trad_fused)
        
        # Cross attention: LLM -&gt; traditional
        q_llm = reshape_by_heads(self.Wq_llm(llm_emb), self.head_num)
        k_trad = reshape_by_heads(self.Wk_trad(trad_emb), self.head_num)
        v_trad = reshape_by_heads(self.Wv_trad(trad_emb), self.head_num)
        
        llm_attends_trad = multi_head_attention(q_llm, k_trad, v_trad)
        llm_fused = self.W_out_llm(llm_attends_trad)
        llm_out = self.norm_llm(llm_emb + llm_fused)
        
        # Combine the cross-attended features
        fused_embeddings = trad_out + llm_out
        
        return fused_embeddings


########################################
# ENCODER
########################################

class MTL_Encoder(nn.Module):
    def __init__(self, **model_params):
        super().__init__()
        self.model_params = model_params
        embedding_dim = self.model_params['embedding_dim']
        hidden_dim = self.model_params['ff_hidden_dim']
        encoder_layer_num = self.model_params['encoder_layer_num']
        head_num = self.model_params['head_num']
        qkv_dim = self.model_params['qkv_dim']
        llama_hidden_size = 4096  # Llama-2 7B hidden size
        
        # Project Llama embeddings to the model's embedding dimension with dtype torch.bfloat16
        self.llm_projection = nn.Sequential(
            nn.Linear(llama_hidden_size, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        ).to(dtype=torch.bfloat16)

        
        # Add layer normalization for better embedding fusion
        self.layer_norm = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)
        self.layer_norm_trad = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)

        if self.model_params['num_experts'] &gt; 1 and &quot;Raw&quot; in self.model_params['expert_loc']:
            self.embedding_depot = MoE(input_size=2, output_size=embedding_dim, 
                                     num_experts=self.model_params['num_experts'],
                                     k=self.model_params['topk'], T=1.0, 
                                     noisy_gating=True, 
                                     routing_level=self.model_params['routing_level'],
                                     routing_method=self.model_params['routing_method'], 
                                     moe_model=&quot;Linear&quot;)
            self.embedding_node = MoE(input_size=5, output_size=embedding_dim, 
                                    num_experts=self.model_params['num_experts'],
                                    k=self.model_params['topk'], T=1.0, 
                                    noisy_gating=True, 
                                    routing_level=self.model_params['routing_level'],
                                    routing_method=self.model_params['routing_method'], 
                                    moe_model=&quot;Linear&quot;)
        else:
            self.embedding_depot = nn.Linear(2, embedding_dim)
            self.embedding_node = nn.Linear(5, embedding_dim)

        # Cross-attention fusion module
        self.cross_attention_fusion = CrossAttentionFusion(
            embedding_dim=embedding_dim,
            head_num=head_num,
            qkv_dim=qkv_dim
        )
        
        self.layers = nn.ModuleList([EncoderLayer(i, **model_params) 
                                   for i in range(encoder_layer_num)])

    def forward(self, depot_xy, node_xy_demand_tw, llm_embeddings):
        moe_loss = 0
        
        # Get traditional embeddings
        if isinstance(self.embedding_depot, MoE) or isinstance(self.embedding_node, MoE):
            embedded_depot, loss_depot = self.embedding_depot(depot_xy)
            embedded_node, loss_node = self.embedding_node(node_xy_demand_tw)
            moe_loss = moe_loss + loss_depot + loss_node
        else:
            embedded_depot = self.embedding_depot(depot_xy)
            embedded_node = self.embedding_node(node_xy_demand_tw)
        
        # Combine depot and node embeddings
        traditional_embeddings = torch.cat((embedded_depot, embedded_node), dim=1).to(dtype=torch.bfloat16)
        
        # Project and normalize LLM embeddings
        projected_llm = self.llm_projection(llm_embeddings)
        projected_llm = self.layer_norm(projected_llm)
        
        # Normalize traditional embeddings
        traditional_embeddings = self.layer_norm_trad(traditional_embeddings)
        
        # Apply cross-attention fusion
        fused_embeddings = self.cross_attention_fusion(
            traditional_embeddings,
            projected_llm
        )
        
        # Pass through encoder layers
        out = fused_embeddings
        for layer in self.layers:
            out, loss = layer(out)
            moe_loss = moe_loss + loss

        return out, moe_loss
</code></pre>
<p>and following is how I'm getting the LLM embeddings,</p>
<pre class=""lang-py prettyprint-override""><code>                with torch.no_grad():
                    outputs = self.llama(**inputs)
                    # Use the last hidden state's [CLS] token
                    new_embeddings = outputs.hidden_states[-1][:, 0, :]
</code></pre>
<p>Am I doing something wrong? Or Do the traditional embeddings even need ones generated by LLM?</p>
",Text Generation & LLMs,fuse embeddings manner increase efficiency score working problem goal supplement traditional embeddings llm generated embeddings using last hidden state purpose far tried simply concatenating using cross attention mechanism concatenating embeddings yield similar result using traditional embeddings alone score definitely better cross attention mechanism unexpectedly degraded performance method could potentially improve score code provided code simple concatenation code cross attention based fusion cross attention class following getting llm embeddings something wrong traditional embeddings even need one generated llm
Encoder Decoder Transformer model generate a repetitive token as output in text summarization,"<p>I implemented a transformer Encoder Decoder (Bert2Bert) for text summarization task. In train phase train loss decreases but in prediction phase it generate a repetitive token as output for example [2,2,2,2,.....]. what should I do?</p>
<pre><code>class BERT2BERT(nn.Module):
    def __init__(self, encoder_model_name='HooshvareLab/bert-base-parsbert-uncased', decoder_model_name='HooshvareLab/bert-base-parsbert-uncased',tokenizer=None,vocab_size=0):
        super(BERT2BERT, self).__init__()
        self.vocab_size=vocab_size
        # Encoder: ParsBERT transformer layers
        self.encoder = BertModel.from_pretrained(encoder_model_name)

        # Decoder: ParsBERT transformer layers with modifications
        decoder_config = BertConfig.from_pretrained(decoder_model_name)
        decoder_config.is_decoder = True  # Enable cross-attention
        decoder_config.add_cross_attention = True
        decoder_config.decoder_start_token_id=tokenizer.cls_token_id
        self.decoder = BertModel(config=decoder_config)
        self.lm_head = nn.Linear(decoder_config.hidden_size, self.vocab_size)
        # Adjust weights
        #self.initialize_decoder_weights()

    # def initialize_decoder_weights(self):
    #     for name, param in self.decoder.named_parameters():
    #         if &quot;cross_attention&quot; in name:
    #             # Random initialization for cross-attention layers
    #             if param.requires_grad:
    #                 nn.init.xavier_uniform_(param.data)
    #         else:
    #             # Use pre-trained weights from ParsBERT
    #             param.data.copy_(self.encoder.state_dict().get(name, param.data))

    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):
        # Encoder: Encode the input sequence
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        encoded_sequence = encoder_outputs.last_hidden_state

        # Decoder: Decode the sequence conditioned on the encoder outputs
        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoded_sequence,
            encoder_attention_mask=attention_mask,
        )

        logits = self.lm_head(decoder_outputs.last_hidden_state)
        return logits
</code></pre>
<p>training phase:</p>
<pre><code>def train_model(model,tokenizer, input_texts, target_summaries):
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    padding_idx = tokenizer.pad_token_id
    loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)
    # Create PyTorch DataLoader
    train_data = list(zip(input_texts, target_summaries))
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    for epoch in range(EPOCH):
        model.train()
        total=len(train_loader)
        index=0
        for batch in train_loader:
            optimizer.zero_grad()
            # Tokenize and convert to tensors
            input_texts, target_texts = batch
            inputs = encode_batch(input_texts, tokenizer)
            targets = encode_batch(target_texts, tokenizer)

            input_ids = inputs['input_ids']
            attention_mask = inputs['attention_mask']
            decoder_input_ids = targets['input_ids']
            decoder_attention_mask = targets['attention_mask']

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                decoder_input_ids=decoder_input_ids,
                decoder_attention_mask=decoder_attention_mask
            )

            logits = outputs
            labels = decoder_input_ids
            logits=logits.view(-1, logits.size(-1))
            loss = loss_fn(logits, labels.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            index+=1
            print(f&quot;Epoch {epoch + 1}, Total Batches:{total} ,Batch:{index}, Loss: {loss.item()}&quot;)
            
        torch.save(model.state_dict(), &quot;model.model&quot;)
</code></pre>
<p>prediction phase:</p>
<pre><code>def evaluate_model(model, tokenizer, test_texts, reference_summaries):
    #model.load_state_dict(torch.load(MODEL_PATH))
    model.eval()
    rou = Rouge()

    predictions = []

    for text in test_texts:
        # Tokenize the input text
        inputs = tokenizer(
            text,
            return_tensors=&quot;pt&quot;,
            max_length=512,
            truncation=True,
            padding=&quot;max_length&quot;,
        )

        # Generate decoder inputs (start with &lt;sos&gt; token)
        decoder_input_ids = torch.tensor([[tokenizer.cls_token_id]])
        decoder_attention_mask = torch.ones_like(decoder_input_ids)

        output_tokens = []

        with torch.no_grad():
            for _ in range(50):  # Generate up to 50 tokens
                logits = model(
                    input_ids=inputs[&quot;input_ids&quot;],
                    attention_mask=inputs[&quot;attention_mask&quot;],
                    decoder_input_ids=decoder_input_ids,
                    decoder_attention_mask=decoder_attention_mask,
                )
                # Get the token with the highest probability
                next_token = torch.argmax(logits[:, -1, :], dim=-1)
                if (
                    next_token.item() == tokenizer.sep_token_id
                ):  # Stop if end-of-sequence token is generated
                    break
                output_tokens.append(next_token.item())

                # Update decoder inputs
                decoder_input_ids = torch.cat(
                    [decoder_input_ids, next_token.unsqueeze(0)], dim=1
                )
                decoder_attention_mask = torch.ones_like(decoder_input_ids)

        # Decode the generated tokens into text
        prediction = tokenizer.decode(output_tokens, skip_special_tokens=True)
        predictions.append(prediction)

    # Compute ROUGE scores
    results = rou.get_scores(predictions, reference_summaries, avg=True)

    return results, predictions

</code></pre>
<p>It generate a repetitive list of tokens as output like [2,2,2,2,.....]</p>
",Text Generation & LLMs,encoder decoder transformer model generate repetitive token output text summarization implemented transformer encoder decoder bert bert text summarization task train phase train loss decrease prediction phase generate repetitive token output example training phase prediction phase generate repetitive list token output like
Emotion Analysis with bhadresh-savani/bert-base-uncased-emotion,"<p>Hope I can get some help here please!
I am trying to run an emotion analysis model from Hugging Face rep. (bhadresh-savani/bert-base-uncased-emotion) and I am struggling with the model run as it's extremely slow and also the output I am trying to get is not correct (I need to append the top emotion score next to my &quot;Comment_new&quot; variable). This is in Colab.</p>
<p>By running this model I am trying to get the top emotion for each review.I have attached a print of 3 reviews from the dataset which in total contains around 4K review lines.</p>
<p>Any idea please how I can run this fast and with the right output?</p>
<p><a href=""https://i.sstatic.net/CUhkPmyr.png"" rel=""nofollow noreferrer"">data sample, top 3 rows</a></p>
<p><a href=""https://i.sstatic.net/EDqDu5KZ.png"" rel=""nofollow noreferrer"">Python Code</a></p>
",Text Generation & LLMs,emotion analysis bhadresh savani bert base uncased emotion hope get help please trying run emotion analysis model hugging face rep bhadresh savani bert base uncased emotion struggling model run extremely slow also output trying get correct need append top emotion score next comment new variable colab running model trying get top emotion review attached print review dataset total contains around k review line idea please run fast right output data sample top row python code
How to Separate Text and Code in Python Strings?,"<p>I've encountered an issue in python. I have a string that contains both a message and code, and I need to separate them and pass each to different functions. An example:</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;&quot;&quot;
Can you change sum operation to multiplication?

def process(a: int, b: int):
    print(a + b)
&quot;&quot;&quot;
</code></pre>
<p>The text and code sections can appear in any order. I've tried using regex to separate them, but it fails with other languages. Do you have any suggestions on how to handle this without involving LLMS?<br />
Thanks!</p>
<p>I've searched about how LLMS differentiate between code and text and found out they use Markdown formatting to separate them, like:</p>
<pre><code>&quot;See if anything is wrong with this code:&quot;

```python
def process(a: int, b: int):
    print(a - b)
- ```
</code></pre>
<p>and I'm afraid I cannot follow this structure because the given input or string is not categorized like this</p>
",Text Generation & LLMs,separate text code python string encountered issue python string contains message code need separate pas different function example text code section appear order tried using regex separate fails language suggestion handle without involving llm thanks searched llm differentiate code text found use markdown formatting separate like afraid follow structure given input string categorized like
Why is Keras pretrained BERT MaskedLM producing inconsistent predictions?,"<p>I am trying to use keras-nlp  with a pretrained masked BERT model to predict some tokens in a sequence. However the model produces inconsistent results. What could be wrong or am i misunderstanding something?</p>
<pre><code>import keras
import keras_nlp
import numpy as np
import tensorflow as tf

preprocessor = keras_nlp.models.BertMaskedLMPreprocessor.from_preset(
    &quot;bert_base_en&quot;
)
masked_lm = keras_nlp.models.BertMaskedLM.from_preset(
    &quot;bert_base_en&quot;,
    load_weights=True,
    preprocessor=None,
)

test_sequence = preprocessor([&quot;The capital of France is Paris.&quot;]) 
outputs = masked_lm(test_sequence[0])
predicted_ids = tf.math.argmax(outputs, axis=-1)
</code></pre>
<p>In this example the preprocessor produces the following sequence [101, 1109, 2364, 1104, 1699,  103,  103,  119,  102,0,0,...] with mask positions [5,6,0,0,0,0....]. The predicted_ids are [28059, 18994, 21690, 21690,.....] which is [b'##saur', b'##hetic', b'##lani', b'##lani',....] instead of something remotely close to 'is', 'Paris'.</p>
",Text Generation & LLMs,kera pretrained bert maskedlm producing inconsistent prediction trying use kera nlp pretrained masked bert model predict token sequence however model produce inconsistent result could wrong misunderstanding something example preprocessor produce following sequence mask position predicted id b saur b hetic b lani b lani instead something remotely close paris
Get chatGPT to respond with a single direct answer,"<p>I am querying a text using chatGPT. But I need chatGPT to respond with single direct answers, rather than long stories or irrelevant text. Any way to achieve this?</p>
<p>My code looks like:</p>
<pre><code>from langchain.document_loaders import TextLoader
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.indexes import VectorstoreIndexCreator

loader = TextLoader(&quot;path/to/extracted_text.txt&quot;)
loaded_text = loader.load()
# Save document text as vector.
index = VectorstoreIndexCreator(
            vectorstore_cls=DocArrayInMemorySearch
        ).from_loaders([loader])

# Query the text
response = index.query(&quot;At what time did john come home yesterday?&quot;)
print(&quot;Loaded text is:&quot;, loaded_text)
print(&quot;ChatGPT response is:&quot;, response)
</code></pre>
<blockquote>
<p>&gt;&gt;&gt; Loaded text is: &quot;&lt; a really long text &gt; + John came home last
night at 11:30pm + &lt; a really long text &gt;&quot;</p>
</blockquote>
<blockquote>
<p>&gt;&gt;&gt; ChatGPT response is: &quot;John came back yesterday at 11:30pm.&quot;</p>
</blockquote>
<p>The problem is that I want a concise answer <code>11:30pm</code> rather than a full sentence <code>John came home last night at 11:30pm</code>. Is there a way to achieve this without adding &quot;I need a short direct response&quot; to my query? Can I achieve a more guaranteed concise response by setting a parameter through some other means instead?</p>
",Text Generation & LLMs,get chatgpt respond single direct answer querying text using chatgpt need chatgpt respond single direct answer rather long story irrelevant text way achieve code look like loaded text really long text john came home last night pm really long text chatgpt response john came back yesterday pm problem want concise answer rather full sentence way achieve without adding need short direct response query achieve guaranteed concise response setting parameter mean instead
Generate Mock but realistic data using NLP,"<p>I want to generate realistic test data. It should support customizable Fields, structure of data by specifying field names and types. It should support a wide range of data types, including names, addresses, email addresses, electrical products, household products etc.</p>
<p>I plan to use small language model, because i don't want full sentence, just word(s).Plan to convert language model response into json. I want to implement similar to <a href=""https://www.mockaroo.com/"" rel=""nofollow noreferrer"">https://www.mockaroo.com/</a> by using AI</p>
<p>Can you refer some small language model to achieve it.</p>
",Text Generation & LLMs,generate mock realistic data using nlp want generate realistic test data support customizable field structure data specifying field name type support wide range data type including name address email address electrical product household product etc plan use small language model want full sentence word plan convert language model response json want implement similar using ai refer small language model achieve
Slow GPT4All with Python SDK,"<p>I'm trying to run some analysis on thousands of text files, and I would like to use gtp4all (In python) to provide some responses. I've been trying to use the model on a sample text file <a href=""https://home.treasury.gov/system/files/261/FSOC_20240223_Minutes.pdf"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Here's my code:</p>
<pre><code>llama_path = r&quot;C:\Users\User\AppData\Local\nomic.ai\GPT4All\Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf&quot;
llama = GPT4All(llama_path, allow_download=False, n_ctx=16384, device='kompute:NVIDIA GeForce RTX 4070 Laptop GPU', ngl=200)

prompt = &quot;&quot;&quot;

Can describe the role of each of the attendees? Give a dictionary with the job of each attendee in JSON format. 

&quot;&quot;&quot;

with open(file_path, 'r', encoding='utf-8') as file:
    content = file.read()

with llama.chat_session(system_prompt='You are a researcher analyzing board minutes. Your job is to provide scores in JSON format.&lt;|eot_id|&gt;'):
    output = llama.generate(prompt=f&quot;{prompt} The text: {content}&quot;, max_tokens=1024, temp=0)

</code></pre>
<p>Even with running this on my GPU, it still takes between 2-3 minutes to get a response, and the responses are not very good. When I see people using this online or on youtube however, it seems much faster and they get great responses. What am I doing wrong? How can I speed this up? Thanks.</p>
<p>Edit: (The response from the code above)</p>
<pre><code>To provide scores in JSON format, I'll need to analyze the text and identify key points related to each attendee's role.

Here is a dictionary with the job of each attendee:

```json
{
    &quot;Janet L. Yellen&quot;: {
        &quot;Role&quot;: &quot;Secretary of the Treasury and Chairperson of the Financial Stability Oversight Council&quot;,
        &quot;Responsibilities&quot;: [
            &quot;Leading the Financial Stability Oversight Council&quot;
        ]
    }, ...
}

Please let me know if you'd like me to add any other attendees or details!
</code></pre>
<p>How do I only get the JSON and skip the text at the front and back?</p>
",Text Generation & LLMs,slow gpt python sdk trying run analysis thousand text file would like use gtp python provide response trying use model sample text file code even running gpu still take minute get response response good see people using online youtube however seems much faster get great response wrong speed thanks edit response code get json skip text front back
What is &quot;language modeling head&quot; in BertForMaskedLM,"<p>I have recently read about BERT and want to use BertForMaskedLM for fill_mask task. I know about BERT architecture. Also, as far as I know, BertForMaskedLM is built from BERT with a language modeling head on top, but I have no idea about what <em>language modeling head</em> means here. Can anyone give me a brief explanation.</p>
",Text Generation & LLMs,language modeling head bertformaskedlm recently read bert want use bertformaskedlm fill mask task know bert architecture also far know bertformaskedlm built bert language modeling head top idea language modeling head mean anyone give brief explanation
Why doesn&#39;t permuting positional encodings in BERT affect the output as expected?,"<p>I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another <a href=""https://stackoverflow.com/questions/78902301/why-doesnt-permuting-positional-encodings-in-gpt-2-affect-the-output-as-expecte/78903454#78903454"">question</a> I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.</p>
<p><strong>What I expected to happen:</strong></p>
<ul>
<li>No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.</li>
<li>Permuting only the input IDs should return distribution B.</li>
<li>Permuting only the positional embeddings should return distribution B.</li>
<li>Permuting both the input IDs and positional embeddings should return distribution A.</li>
</ul>
<p><strong>What actually happens:</strong>
Sometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.</p>
<p><strong>My question is:</strong> Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?</p>
<p>For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in <code>masked_prediction</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import ipywidgets as widgets
from IPython.display import display
from transformers import BertForMaskedLM, AutoTokenizer
import matplotlib.pyplot as plt
import torch.nn.functional as F

# surpress renaming warnings
logging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.ERROR)
warnings.simplefilter(&quot;ignore&quot;, FutureWarning)

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

input_ids = torch.Tensor([[]])
tokens = []
permutation = []

output = widgets.Output()

def permute_columns(matrix, permutation=None):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

def update_permutation(ordered_tags):
    global permutation
    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]
    
    permutation = [tokens.index(tag) for tag in fixed_tokens]
    

def tokenize(text):
    global input_ids, tokens
    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids
    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]
    
    if len(tokens) &gt; 2:
        reorderable_tokens = tokens[1:-1]
    else:
        reorderable_tokens = []
    
    with output:
        output.clear_output(wait=True)
        tags_input.allowed_tags = reorderable_tokens
        tags_input.value = reorderable_tokens
        update_permutation(tags_input.value)

def on_tags_change(change):
    if len(change['new']) != len(tags_input.allowed_tags):
        tags_input.value = tags_input.allowed_tags  # Restore original value


def masked_prediction(input_ids, permutation, permute_input, permute_encoding):
    
    with output:
        output.clear_output(wait=True)  # Clear previous outputs
        
        if input_ids.numel() == 0:
            print(&quot;You can't use an empty sequence for prediction&quot;)
            return
        
        model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)
        
        if permute_encoding:
            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T
        if permute_input:
            input_ids = permute_columns(input_ids, permutation)
            
        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)
            
        with torch.no_grad():
            outputs = model(input_ids)
            
        logits = outputs.logits

        top_k = 5

        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]
        print(decoded_text, mask_token_indices, permutation)
        num_masks = len(mask_token_indices)
        if num_masks == 0:
            print(&quot;You need to include a [MASK] token for prediction&quot;)
            return

        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))
        
        if num_masks == 1:
            axs = [axs]

        for i, idx in enumerate(mask_token_indices):
            mask_token_logits = logits[0, idx, :]

            softmax_probs = F.softmax(mask_token_logits, dim=0)

            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)

            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]
            predicted_confidences = top_token_probs.tolist()

            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')
            axs[i].set_xlabel('Predicted Tokens')
            axs[i].set_ylabel('Confidence')
            axs[i].set_title(f'Masked Token at Position {idx.item()}')
            axs[i].set_ylim(0, 1)

        plt.show()

def on_predict_button_click(b):
    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)

text_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')
text_input.observe(lambda change: tokenize(change['new']), names='value')
tags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)

# Observe changes in tags order to update the permutation and prevent deletion
tags_input.observe(on_tags_change, names='value')
tags_input.observe(lambda change: update_permutation(change['new']), names='value')

# Create checkboxes for permute_input and permute_encoding
permute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')
permute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')

# Create a button to trigger the prediction
predict_button = widgets.Button(description=&quot;Run Prediction&quot;)
predict_button.on_click(on_predict_button_click)

# Display the widgets
display(text_input)
display(tags_input)
display(permute_input_checkbox)
display(permute_encoding_checkbox)
display(predict_button)
display(output)
</code></pre>
",Text Generation & LLMs,permuting positional encoding bert affect output expected working jupyter notebook transformer section positional encoding want demonstrate transformer relies entirely positional encoding understand order sequence previously learned another
How to update an already Fine-tuned GPT2 model?,"<p>I have trained my model using GPT-2 for my dataset. It has been trained and is giving the correct output. Now, I want to train my model on more data while retaining the previously trained model. I do not want to create a new model; I only want to use the previous one with the updated data for training.</p>
<p>I tried doing <code>overwrite=False</code> in the training, but I don't know whether it will work or not.</p>
",Text Generation & LLMs,update already fine tuned gpt model trained model using gpt dataset ha trained giving correct output want train model data retaining previously trained model want create new model want use previous one updated data training tried training know whether work
GGUF model in LM Studio returns broken answer,"<p>I try to run LLM GGUF model <a href=""https://huggingface.co/QuantFactory/T-lite-0.1-GGUF"" rel=""nofollow noreferrer"">QuantFactory/T-lite-instruct-0.1-GGUF</a> specifically its quantized version <a href=""https://huggingface.co/QuantFactory/T-lite-0.1-GGUF/blob/main/T-lite-0.1.Q2_K.gguf"" rel=""nofollow noreferrer"">T-lite-instruct-0.1.Q2_K.gguf</a> in <a href=""https://lmstudio.ai"" rel=""nofollow noreferrer"">LM Studio</a>.<br />
Sometimes it works fine. But sometimes it returns &quot;squares&quot; in answer.
<a href=""https://i.sstatic.net/Tpqi03MJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tpqi03MJ.png"" alt=""enter image description here"" /></a>
I assume that this is encoding problem but how to avoid it when using LM Studio? There is no model setting related with encoding. And I'm stuck.</p>
<pre><code>{
  &quot;name&quot;: &quot;Config for Chat ID 1710&quot;,
  &quot;load_params&quot;: {
    &quot;n_ctx&quot;: 2048,
    &quot;n_batch&quot;: 512,
    &quot;rope_freq_base&quot;: 0,
    &quot;rope_freq_scale&quot;: 0,
    &quot;n_gpu_layers&quot;: 10,
    &quot;use_mlock&quot;: true,
    &quot;main_gpu&quot;: 0,
    &quot;tensor_split&quot;: [
      0
    ],
    &quot;seed&quot;: -1,
    &quot;f16_kv&quot;: true,
    &quot;use_mmap&quot;: true,
    &quot;no_kv_offload&quot;: false,
    &quot;num_experts_used&quot;: 0
  },
  &quot;inference_params&quot;: {
    &quot;n_threads&quot;: 4,
    &quot;n_predict&quot;: -1,
    &quot;top_k&quot;: 40,
    &quot;min_p&quot;: 0.05,
    &quot;top_p&quot;: 0.95,
    &quot;temp&quot;: 0.8,
    &quot;repeat_penalty&quot;: 1.1,
    &quot;input_prefix&quot;: &quot;### Instruction:\n&quot;,
    &quot;input_suffix&quot;: &quot;\n### Response:\n&quot;,
    &quot;antiprompt&quot;: [
      &quot;### Instruction:&quot;
    ],
    &quot;pre_prompt&quot;: &quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.&quot;,
    &quot;pre_prompt_suffix&quot;: &quot;\n&quot;,
    &quot;pre_prompt_prefix&quot;: &quot;&quot;,
    &quot;seed&quot;: -1,
    &quot;tfs_z&quot;: 1,
    &quot;typical_p&quot;: 1,
    &quot;repeat_last_n&quot;: 64,
    &quot;frequency_penalty&quot;: 0,
    &quot;presence_penalty&quot;: 0,
    &quot;n_keep&quot;: 0,
    &quot;logit_bias&quot;: {},
    &quot;mirostat&quot;: 0,
    &quot;mirostat_tau&quot;: 5,
    &quot;mirostat_eta&quot;: 0.1,
    &quot;memory_f16&quot;: true,
    &quot;multiline_input&quot;: false,
    &quot;penalize_nl&quot;: true
  }
}
</code></pre>
",Text Generation & LLMs,gguf model lm studio return broken answer try run llm gguf model quantfactory lite instruct gguf specifically quantized version lite instruct q k gguf lm studio sometimes work fine sometimes return square answer assume encoding problem avoid using lm studio model setting related encoding stuck
How to Combine Semantic Search with SQL Analytical Queries?,"<p>I'm creating an LLM-agent that can provide insights from a complex database. The database includes several columns of different types (datetime, numeric, and text).</p>
<p>For simplicity, let's assume I have a table containing information about reports with three columns: <code>Date</code>, <code>Category</code>, and <code>Description</code>. Let's also assume that the number of categories is indefinite.</p>
<p>For text-related queries, semantic search has me covered. For analytical queries, such as filtering data for a time range, I could use text-to-SQL or an agent.</p>
<p>My problem arises when facing queries like <strong>&quot;How many incidents related to falling off a roof happened last year?&quot;</strong>. This requires to split the task into three sub-tasks:</p>
<ul>
<li>filter date</li>
<li>find relevant reports according to the Description AND the Category (AND <em>N</em> columns)</li>
<li>sum up the results</li>
</ul>
<p>Let me present my initial approach:</p>
<h4>Semantic Search + SQL filtering</h4>
<ul>
<li><p>Filter date</p>
</li>
<li><p>Filter Category with LIKE</p>
</li>
<li><p>Semantic Search over Description</p>
</li>
</ul>
<p>Covers more ground that using pure sql filtering, but it can still be inaccurate. My current problems are:</p>
<ul>
<li><p><strong>Problem #1</strong>: This is about something very specific: counting reports. The semantic search depends on the threshold set for the similarity/distance.</p>
</li>
<li><p><strong>Problem #2</strong>: The Category will not be fully captured with a LIKE operation.</p>
</li>
</ul>
<p>For <strong>Problem #2</strong> I could add the Category to the description and leave that for the semantic search, but it might also add some noise to the similarity computation, as we can see here:</p>
<pre class=""lang-py prettyprint-override""><code>from scipy.spatial.distance import cosine

# LangChain Ollama embeddings

eq1 = embeddings_model.embed_query(&quot;Last night I fell off my bed when I was having a nightmare and I felt really bad&quot;)
eq2 = embeddings_model.embed_query(&quot;the other day I slipped over some ice and I fell to the ground&quot;)
eq3 = embeddings_model.embed_query(&quot;I fall down all the time.&quot;)

# Cosine similarity varies with the rest of context,

# even if all three sentences talk about falling

cosine(eq1, eq2), cosine(eq1, eq3), cosine(eq2, eq3)

&gt; &gt; &gt; (0.5038163339975597, 0.6419394542874378, 0.4899502476580482)
</code></pre>
<p>For <strong>Problem #1</strong> I guess it depends on the threshold for the similarity as well, but then we would be dealing with a precision/recall scenario (counting irrelevant reports vs excluding relevant reports)</p>
<p>How to effectively combine these techniques?</p>
",Text Generation & LLMs,combine semantic search sql analytical query creating llm agent provide insight complex database database includes several column different type datetime numeric text simplicity let assume table containing information report three column let also assume number category indefinite text related query semantic search ha covered analytical query filtering data time range could use text sql agent problem arises facing query like many incident related falling roof happened last year requires split task three sub task filter date find relevant report according description category n column sum result let present initial approach semantic search sql filtering filter date filter category like semantic search description cover ground using pure sql filtering still inaccurate current problem problem something specific counting report semantic search depends threshold set similarity distance problem category fully captured like operation problem could add category description leave semantic search might also add noise similarity computation see problem guess depends threshold similarity well would dealing precision recall scenario counting irrelevant report v excluding relevant report effectively combine technique
MT-Bench evaluation of a model using pre generated model answers,"<p>I want to find MT-Bench score of an LLM (say EleutherAI/pythia-1b).I was able to run the command</p>
<blockquote>
<p>python gen_model_answer.py --model-pat EleutherAI/pythia-1b --model-id pythia-1b</p>
</blockquote>
<p>to generate answers and I could see the output in the json file &quot;data/mt_bench/model_answer/pythia-1b.jsonl&quot;.
I have downloaded pre generated model answers using the command</p>
<blockquote>
<p>python3 download_mt_bench_pregenerated.py</p>
</blockquote>
<p>How to compare &quot;pythia-1b&quot; generated answer and any pre generated answer(say llama-13b) to calculate MT-Bench score for &quot;pythia-1b&quot; model ?</p>
",Text Generation & LLMs,mt bench evaluation model using pre generated model answer want find mt bench score llm say eleutherai pythia b wa able run command python gen model answer py model pat eleutherai pythia b model id pythia b generate answer could see output json file data mt bench model answer pythia b jsonl downloaded pre generated model answer using command python download mt bench pregenerated py compare pythia b generated answer pre generated answer say llama b calculate mt bench score pythia b model
What is the difference between HuggingFace&#39;s TextGeneration and Text2TextGeneration pipelines,"<p>I'm confused about the technical difference between the two huggingface pipelines <a href=""https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.TextGenerationPipeline"" rel=""noreferrer"">TextGeneration</a> and <a href=""https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.Text2TextGenerationPipeline"" rel=""noreferrer"">Text2TextGeneration</a>.</p>
<p>In the TextGeneration it is stated that:</p>
<blockquote>
<p>Language generation pipeline using any ModelWithLMHead. This pipeline
predicts the words that will follow a specified text prompt.</p>
</blockquote>
<p>But isn't any language model doing that? &quot;predicting the next words&quot;? So how is this pipeline different than Text2TextGeneration? Isn't Text2TextGeneration going to predict the next probable words?</p>
<p>I also tried some models using &quot;Text2TextGeneration&quot; pipeline, and despite of HuggingFace's warning &quot;The model is not supported for text2text-generation&quot; it actually worked and generated some outputs.</p>
<p>If someone can explain the technical difference it will be appreciated.</p>
",Text Generation & LLMs,difference huggingface textgeneration text textgeneration pipeline confused technical difference two huggingface pipeline textgeneration text textgeneration textgeneration stated language generation pipeline using modelwithlmhead pipeline predicts word follow specified text prompt language model predicting next word pipeline different text textgeneration text textgeneration going predict next probable word also tried model using text textgeneration pipeline despite huggingface warning model supported text text generation actually worked generated output someone explain technical difference appreciated
Generate representative boolean queries from source text?,"<p>Within conventional information retrieval systems, we use a boolean query to generate a result set that matches that query.</p>
<p>What does one call the 'task', where, given a known set of similar documents, determine the boolean query that best represents it?</p>
<p>The main issue I encounter right now is I don't know what this problem is called, so I am finding it difficult to discover existing literature and research on the topic.</p>
",Text Generation & LLMs,generate representative boolean query source text within conventional information retrieval system use boolean query generate result set match query doe one call task given known set similar document determine boolean query best represents main issue encounter right know problem called finding difficult discover existing literature research topic
Fine-tuned LLaMA-2-Chat-HF Model Generates Same Responses as Pre-trained Model and Suitability for Retrieval-based Task,"<p>I am working on building a chatbot for substance abuse support. My approach involves two main steps:</p>
<ul>
<li>Fine-tuning the LLaMA-2-Chat-HF model: I have fine-tuned the LLaMA-2-Chat-HF model using a dataset of mental health conversations. The dataset was transformed into an instruction template format before fine-tuning.</li>
<li>Retrieval-based system: I am using a retrieval-based system to fetch information from a vector database that contains a textbook on substance abuse support (theory and practice).</li>
</ul>
<p>After fine-tuning, I am using the fine-tuned model to generate reponses using the context/information retrieved from the vector database. The process involves passing both the context and the user query into a prompt template, and then generating answers using an LLM chain. However, I am encountering the following issues:</p>
<p>Both the fine-tuned model and the pre-trained model are generating the exact same responses when queried, despite using the context from the vector database.</p>
<p>My Questions:
Why might the fine-tuned LLaMA-2-Chat-HF model be generating the same responses as the pre-trained model? What could be causing this issue?
Is the fine-tuned LLaMA-2-Chat-HF model suitable for a retrieval-based task? If not, what adjustments or different approaches should I consider?</p>
<p>Any insights, suggestions, or alternative approaches would be greatly appreciated.</p>
<p>Code for retrieval using the fine tuned model:</p>
<pre><code>DB_FAISS_PATH = 'vectorstores/db_faiss'

custom_prompt_template = &quot;&quot;&quot;Use the following pieces of information to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:{context}
Question:{question}

Only return the helpful answer below and nothing else.
Helpful answer:
&quot;&quot;&quot;

def set_custom_prompt():
     &quot;&quot;&quot;
     Prompt template for QA retrieval for each vector store
     &quot;&quot;&quot;
     prompt = PromptTemplate(template=custom_prompt_template, input_variables=['context', 'question'])
     return prompt

def create_llm(model, tokenizer):

     text_generation_pipeline = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer)

     llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
     return llm

def retrieval_qa_chain(llm, prompt, db):
     qa_chain = RetrievalQA.from_chain_type(
         llm=llm,
         chain_type='stuff',
         retriever=db.as_retriever(search_kwargs={'k': 2}),
         return_source_documents=True,
         chain_type_kwargs={'prompt': prompt}
     )
     return qa_chain

def qa_bot(model, tokenizer):
     embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
     db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
     llm = create_llm(model, tokenizer)
     qa_prompt = set_custom_prompt()
     qa = retrieval_qa_chain(llm, qa_prompt, db)
     return qa

def final_result(query, model, tokenizer):
     qa = qa_bot(model, tokenizer)

     qa_result = qa({'query': query})

     response = qa_result['result']

     helpful_answer = response.split('Helpful answer:')[-1].strip()

     return helpful_answer.strip()
</code></pre>
",Text Generation & LLMs,fine tuned llama chat hf model generates response pre trained model suitability retrieval based task working building chatbot substance abuse support approach involves two main step fine tuning llama chat hf model fine tuned llama chat hf model using dataset mental health conversation dataset wa transformed instruction template format fine tuning retrieval based system using retrieval based system fetch information vector database contains textbook substance abuse support theory practice fine tuning using fine tuned model generate reponses using context information retrieved vector database process involves passing context user query prompt template generating answer using llm chain however encountering following issue fine tuned model pre trained model generating exact response queried despite using context vector database question might fine tuned llama chat hf model generating response pre trained model could causing issue fine tuned llama chat hf model suitable retrieval based task adjustment different approach consider insight suggestion alternative approach would greatly appreciated code retrieval using fine tuned model
Extracting structured data from user query,"<p>I want to extract structured data from the query provided by the user.</p>
<p>For example,
user query: I need data for females above the age of 3</p>
<p>output : {
min_age: 3,
max_age: None,
sex: female
}</p>
<p>These are few of the specific parameters that I require along with others.
I have tried using llama2, llama3 and gemma but i am facing the issue of hallucination
Also if a specific value is not given I want it to return 'None'</p>
<p>Is there any way I can achieve this using LLMs from ollama.</p>
<p>I have tried extracting the values one by one but I am not able to get the desired output</p>
",Text Generation & LLMs,extracting structured data user query want extract structured data query provided user example user query need data female age output min age max age none sex female specific parameter require along others tried using llama llama gemma facing issue hallucination also specific value given want return none way achieve using llm ollama tried extracting value one one able get desired output
Why is Perplexity not reliable for open domain text generation tasks?,"<p>In the paper <a href=""https://arxiv.org/abs/1912.02164"" rel=""nofollow noreferrer"">here</a>, it says that perplexity as an automated metric is not reliable for open domain text generation tasks, but it instead uses lm-score, a model based metric to produce perplexity like values. What additional benefits does lm-score give instead of perplexity metric?</p>
",Text Generation & LLMs,perplexity reliable open domain text generation task paper say perplexity automated metric reliable open domain text generation task instead us lm score model based metric produce perplexity like value additional benefit doe lm score give instead perplexity metric
Issues with Generating Text from Fine-Tuned Mistral 7B Model on Georgian Dataset,"<p>I've fine-tuned the Mistral 7B model using a Georgian dataset with approximately 100,000 articles, including custom tokenizer fine-tuning. The fine-tuning process took about 9 hours. However, when I try to generate text, the output is not as expected; it consistently returns the input as the output, regardless of the input provided.</p>
<p>Here's the code I used for fine-tuning:</p>
<pre class=""lang-py prettyprint-override""><code>import time
import json
import torch
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

# Load dataset, preprocess, and fine-tuning details...

training_args = TrainingArguments(
    output_dir=&quot;mistral_georgian_news_finetuning&quot;,
    max_steps=3125,
    per_device_train_batch_size=32,
    learning_rate=3e-4,
    # Other arguments...
)

# Fine-tuning setup...

# Start fine-tuning
trainer.train()
</code></pre>
<p>For testing the fine-tuned model, I used the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = &quot;/path/to/fine-tuned-model&quot;
tokenizer_path = &quot;/path/to/tokenizer&quot;

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

def generate_text(prompt_text, max_length=500):
    input_ids = tokenizer(prompt_text, return_tensors=&quot;pt&quot;).input_ids
    output = model.generate(input_ids, max_length=max_length)
    return tokenizer.decode(output[0], skip_special_tokens=True)

prompt = &quot;რამდენიმე დღეში შესრულდება ...&quot;
generated_text = generate_text(prompt)
print(generated_text)
</code></pre>
<p>During testing, the model just echoes the prompt without generating new text. Below are the logs observed:</p>
<pre class=""lang-none prettyprint-override""><code>config.json: 0%| | 0.00/571 [00:00&lt;?, ?B/s]
model.safetensors.index.json: 0%| | 0.00/25.1k [00:00&lt;?, ?B/s]
Downloading shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
model-00001-of-00002.safetensors: 0%| | 0.00/9.94G [00:00&lt;?, ?B/s]
model-00002-of-00002.safetensors: 0%| | 0.00/4.54G [00:00&lt;?, ?B/s]
Loading checkpoint shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
generation_config.json: 0%| | 0.00/116 [00:00&lt;?, ?B/s]
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
</code></pre>
<p>I am unsure if the issue lies in how I am loading and testing the model or if it is related to the fine-tuning process. The model should generate text based on the input prompt, but it returns the input as the output.</p>
<p>Has anyone experienced similar issues, or can someone spot what might be wrong with my approach?</p>
",Text Generation & LLMs,issue generating text fine tuned mistral b model georgian dataset fine tuned mistral b model using georgian dataset approximately article including custom tokenizer fine tuning fine tuning process took hour however try generate text output expected consistently return input output regardless input provided code used fine tuning testing fine tuned model used following code testing model echo prompt without generating new text log observed unsure issue lie loading testing model related fine tuning process model generate text based input prompt return input output ha anyone experienced similar issue someone spot might wrong approach
Which weights change when fine-tunning a pre-trained model? (Hugging Face),"<p>I am using the AutoModelForSequenceClassification class to fine-tune a pre-trained model (which originally is based on GPT2 architecture)</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;Natooz/Maestro-REMI-bpe20k&quot;, trust_remote_code=True, torch_dtype=&quot;auto&quot;,num_labels=2)
</code></pre>
<p>And I am using the basic training loop suggested in the NLP course from hugging face:</p>
<pre><code>model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
</code></pre>
<p>My question is:</p>
<p><em><strong>1. Which weights of the model are going to change after using this training loop? Am I training all the weights of the model? Only the classification head added by the AutoModelForSequenceClassification?</strong></em></p>
<p>It wouldn't make a lot of sense to me for all weights to change, because in theory I wouldn't be &quot;transferring&quot; any knowledge from my pre-trained model to my task.</p>
<p><em><strong>2. Would it work the same if I fine-tune my pre-trained model with the trainer class instead of the training loop?</strong></em></p>
<p>Thank you so much in advance!</p>
",Text Generation & LLMs,weight change fine tunning pre trained model hugging face using automodelforsequenceclassification class fine tune pre trained model originally based gpt architecture using basic training loop suggested nlp course hugging face question weight model going change using training loop training weight model classification head added automodelforsequenceclassification make lot sense weight change theory transferring knowledge pre trained model task would work fine tune pre trained model trainer class instead training loop thank much advance
State of the art word sense disambiguation on WordNet synsets,"<p>I am trying to perform a simple task: given a corpus, identify all words that are hyponyms of a certain synset (e.g., <em>«find every mention of a &quot;plant&quot; or a &quot;bird&quot;»</em>). In order to do that accurately, I need to do word sense disambiguation on a group of synsets for every word in my corpus.</p>
<p>I am trying to do it using state-of-the-art methods as available in the open source space.</p>
<p>If using a neural method, I would need a pretrained model.</p>
<p>I have tried the greedy approach that considers every single synset for every word. This isn't great; however, I find that using traditional techniques like <code>lesk</code> as provided by <code>nltk</code> in practice is even worse, as I get way too many false negatives.</p>
<p>I see that spaCy already contains a transformer based model which comes with POS tagging out of the box, but the WordNet integration is supplied by an external package and I can't seem to find any way to do WSD on it.</p>
<p>I could certainly paraphrase the disambiguation query:</p>
<blockquote>
<p>Which of these description matches the word x in the sentence: &quot;yyy&quot;:</p>
<ol>
<li>&quot;x means aaa&quot;</li>
<li>&quot;x means bbb&quot;</li>
<li>&quot;x means ccc&quot;</li>
</ol>
</blockquote>
<p>And feed it into an LLM, so I can't see any hard limit on why there shouldn't be a more straightforward way to do this using modern deep learning techniques. Is there some available model I am unable to find?</p>
",Text Generation & LLMs,state art word sense disambiguation wordnet synset trying perform simple task given corpus identify word hyponym certain synset e g find every mention plant bird order accurately need word sense disambiguation group synset every word corpus trying using state art method available open source space using neural method would need pretrained model tried greedy approach considers every single synset every word great however find using traditional technique like provided practice even worse get way many false negative see spacy already contains transformer based model come po tagging box wordnet integration supplied external package seem find way wsd could certainly paraphrase disambiguation query description match word x sentence x mean aaa x mean bbb x mean ccc feed llm see hard limit straightforward way using modern deep learning technique available model unable find
Why replace the masked token with random token in bert?,"<p>Masking in Bert is:</p>
<ul>
<li>Take 15% of all the tokens in the sequence. These are to be used in computing the MLM loss</li>
<li>80% of the time retain the mask</li>
<li>10% of the time replace the mask with the original token</li>
<li>10% of the time replace the mask with a random token</li>
</ul>
<p>I understand why masking is needed. I also understand why the masking is replaced back to the original token. This is because otherwise, the model learns to completely ignore the word itself in deriving its contextual embeddings in the downstream tasks.</p>
<p>What is the need to replace the mask with a random word 10% of the time?</p>
",Text Generation & LLMs,replace masked token random token bert masking bert take token sequence used computing mlm loss time retain mask time replace mask original token time replace mask random token understand masking needed also understand masking replaced back original token otherwise model learns completely ignore word deriving contextual embeddings downstream task need replace mask random word time
How do I train gpt 2 from scratch?,"<p>I want to train gpt 2 from scratch but there is only fine-tuning approach based on pretrained models in articles I found.
I've used this <a href=""https://github.com/nshepperd/gpt-2"" rel=""noreferrer"">https://github.com/nshepperd/gpt-2</a> for train with existing model. Should I edit these Python scripts to train from scratch?</p>
",Text Generation & LLMs,train gpt scratch want train gpt scratch fine tuning approach based pretrained model article found used train existing model edit python script train scratch
chatbot memory problem for langchain using ConversationalRetrievalChain,"<p>i have build a chatbot on custom data using langchain, but the chatbot can not remember my previous questions, I found it difficult with the ConversationalRetrievalChain.</p>
<p>My code is as below:</p>
<pre><code>`class MyBot(ActivityHandler):
    def __init__(self, conversation_state: ConversationState):
        self.conversation_state = conversation_state
        self.session_accessor = self.conversation_state.create_property(&quot;Session&quot;)
       
            
        # Data loader
        loader = CSVLoader(file_path=&quot;data.csv&quot;, encoding=&quot;utf-8&quot;, csv_args={'delimiter': ','})
        data = loader.load()

        # OpenAI API key
        openai_api_key = os.environ.get('OPENAI_API_KEY')

        # Initialize OpenAIEmbeddings
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

        # Initialize FAISS for the vector database
        vectors = FAISS.from_documents(data, embeddings)
        

        # Initialize the ConversationalRetrievalChain
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(temperature=0.5,request_timeout=100,  model_name='gpt-4o', max_tokens= 2048, openai_api_key=openai_api_key),
            retriever=vectors.as_retriever(), combine_docs_chain_kwargs={&quot;prompt&quot;: QA_PROMPT}
        )    

def run_chain(self, chat_history, question):
        return self.chain.run({'chat_history': chat_history, 'question': question})

    
    

    async def get_response(self, user_message, turn_context: TurnContext):
    # Retrieve the session for the current user
        session = await self.session_accessor.get(turn_context, lambda: {&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You work for and M company answer any questions related to the company.&quot;}]})

    # Add user message to the session
        session[&quot;messages&quot;].append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})

        response = self.run_chain(&quot;&quot;, user_message)

        ai_response = response  # Use response directly

    # Add AI response to the session
        session[&quot;messages&quot;].append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: ai_response})

        # Save the updated session
        await self.conversation_state.save_changes(turn_context)

        return ai_response`
</code></pre>
<p>I just want my chatbot to remember my previous question like chatgpt.</p>
<p>thanks in advance!</p>
<p>integrate the memory within the chatbot</p>
",Text Generation & LLMs,chatbot memory problem langchain using conversationalretrievalchain build chatbot custom data using langchain chatbot remember previous question found difficult conversationalretrievalchain code want chatbot remember previous question like chatgpt thanks advance integrate memory within chatbot
Question about sentence generation according to specific patterns with predefined vocabulary list,"<p>I am interested in learning a language and want to generate practice sentences from a predefined vocabulary list according to patterns that I define ie noun(creature) - verb of being - emotion adjective. Chat GPT does a decent job with some of this and I can get it to output what I want some of the time, but is there a better option? Has anyone tried this before?</p>
",Text Generation & LLMs,question sentence generation according specific pattern predefined vocabulary list interested learning language want generate practice sentence predefined vocabulary list according pattern define ie noun creature verb emotion adjective chat gpt doe decent job get output want time better option ha anyone tried
No Attention returned even when output_attentions= True,"<p>I'm using a pretrained model based BERT (github link:<a href=""https://github.com/MAGICS-LAB/DNABERT_2"" rel=""nofollow noreferrer"">DNABERT-2</a>)</p>
<p>It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base.</p>
<p>I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem.
<code>output</code> is of length 2 and each element is of shape: <code>torch.Size([1, 7, 768])</code> and
<code>torch.Size([1, 768])</code>. When trying to get <code>output.attentions</code> I get <code>None</code>.</p>
<p>I'm not sure where to search and what a solution would be.</p>
<p>I'm providing my whole code:</p>
<p>Defining model, trainer, data, tokenizer:</p>
<pre><code>from copy import deepcopy

from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback
# END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List

import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset

@dataclass class ModelArguments:
    model_name_or_path: Optional[str] = field(default=&quot;facebook/opt-125m&quot;)
    use_lora: bool = field(default=False, metadata={&quot;help&quot;: &quot;whether to use LoRA&quot;})
    lora_r: int = field(default=8, metadata={&quot;help&quot;: &quot;hidden dimension for LoRA&quot;})
    lora_alpha: int = field(default=32, metadata={&quot;help&quot;: &quot;alpha for LoRA&quot;})
    lora_dropout: float = field(default=0.05, metadata={&quot;help&quot;: &quot;dropout rate for LoRA&quot;})
    lora_target_modules: str = field(default=&quot;query,value&quot;, metadata={&quot;help&quot;: &quot;where to perform LoRA&quot;})


@dataclass class DataArguments:
    data_path: str = field(default=None, metadata={&quot;help&quot;: &quot;Path to the training data.&quot;})
    kmer: int = field(default=-1, metadata={&quot;help&quot;: &quot;k-mer for input sequence. -1 means not using k-mer.&quot;})


@dataclass class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    run_name: str = field(default=&quot;run&quot;)
    optim: str = field(default=&quot;adamw_torch&quot;)
    model_max_length: int = field(default=512, metadata={&quot;help&quot;: &quot;Maximum sequence length.&quot;})
    gradient_accumulation_steps: int = field(default=1)
    per_device_train_batch_size: int = field(default=1)
    per_device_eval_batch_size: int = field(default=1)
    num_train_epochs: int = field(default=1)
    logging_steps: int = field(default=100)
    save_steps: int = field(default=100)
    fp16: bool = field(default=False)
    # START NEW
    # eval_steps: int = field(default=100)
    eval_steps: int = field(default=0.1)
    # END NEW
    evaluation_strategy: str = field(default=&quot;steps&quot;)
    warmup_steps: int = field(default=50)
    weight_decay: float = field(default=0.01)
    learning_rate: float = field(default=1e-4)
    save_total_limit: int = field(default=3)
    load_best_model_at_end: bool = field(default=True)
    output_dir: str = field(default=&quot;output&quot;)
    find_unused_parameters: bool = field(default=False)
    checkpointing: bool = field(default=False)
    dataloader_pin_memory: bool = field(default=False)
    eval_and_save_results: bool = field(default=True)
    save_model: bool = field(default=False)
    seed: int = field(default=42)


def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    &quot;&quot;&quot;Collects the state dict and dump to disk.&quot;&quot;&quot;
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa


&quot;&quot;&quot; Get the reversed complement of the original DNA sequence. &quot;&quot;&quot;


def get_alter_of_dna_sequence(sequence: str):
    MAP = {&quot;A&quot;: &quot;T&quot;, &quot;T&quot;: &quot;A&quot;, &quot;C&quot;: &quot;G&quot;, &quot;G&quot;: &quot;C&quot;}
    # return &quot;&quot;.join([MAP[c] for c in reversed(sequence)])
    return &quot;&quot;.join([MAP[c] for c in sequence])


&quot;&quot;&quot; Transform a dna sequence to k-mer string &quot;&quot;&quot;


def generate_kmer_str(sequence: str, k: int) -&gt; str:
    &quot;&quot;&quot;Generate k-mer string from DNA sequence.&quot;&quot;&quot;
    return &quot; &quot;.join([sequence[i:i + k] for i in range(len(sequence) - k + 1)])


&quot;&quot;&quot; Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved  to the same directory as the original data with the same name but with a suffix of &quot;_{k}mer&quot;. &quot;&quot;&quot;


def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -&gt; List[str]:
    &quot;&quot;&quot;Load or generate k-mer string for each DNA sequence.&quot;&quot;&quot;
    kmer_path = data_path.tokenizerreplace(&quot;.csv&quot;, f&quot;_{k}mer.json&quot;)
    if os.path.exists(kmer_path):
        logging.warning(f&quot;Loading k-mer from {kmer_path}...&quot;)
        with open(kmer_path, &quot;r&quot;) as f:
            kmer = json.load(f)
    else:
        logging.warning(f&quot;Generating k-mer...&quot;)
        kmer = [generate_kmer_str(text, k) for text in texts]
        with open(kmer_path, &quot;w&quot;) as f:
            logging.warning(f&quot;Saving k-mer to {kmer_path}...&quot;)
            json.dump(kmer, f)

    return kmer


class SupervisedDataset(Dataset):
    &quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;

    def __init__(self,
                 data_path: str,
                 tokenizer: transformers.PreTrainedTokenizer,
                 kmer: int = -1):

        super(SupervisedDataset, self).__init__()

        # load data from the disk
        with open(data_path, &quot;r&quot;) as f:
            data = list(csv.reader(f))[1:]
        if len(data[0]) == 2:
            # data is in the format of [text, label]
            logging.warning(&quot;Perform single sequence classification...&quot;)
            texts = [d[0] for d in data]
            labels = [int(d[1]) for d in data]
        # All genes sequences are concat: we don't work with the sequence-pair,
        # But we are tricking the model to think it is single sequence.
        elif len(data[0]) == 3:
            # data is in the format of [text1, text2, label]
            logging.warning(&quot;Perform sequence-pair classification...&quot;)
            texts = [[d[0], d[1]] for d in data]
            labels = [int(d[2]) for d in data]
        else:
            raise ValueError(&quot;Data format not supported.&quot;)

        if kmer != -1:
            # only write file on the first process
            if torch.distributed.get_rank() not in [0, -1]:
                torch.distributed.barrier()

            logging.warning(f&quot;Using {kmer}-mer as input...&quot;)
            texts = load_or_generate_kmer(data_path, texts, kmer)

            if torch.distributed.get_rank() == 0:
                torch.distributed.barrier()

        output = tokenizer(
            texts,
            return_tensors=&quot;pt&quot;,
            padding=&quot;longest&quot;,
            max_length=tokenizer.model_max_length,
            truncation=True,
        )

        self.input_ids = output[&quot;input_ids&quot;]
        # CHANGE
        self.input_ids[0][self.input_ids[0] == 0] = 2
        # Change to which tokens we want to attend and to which we don't
        self.attention_mask = output[&quot;attention_mask&quot;]
        self.labels = labels
        self.num_labels = len(set(labels))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]:
        return dict(input_ids=self.input_ids[i], labels=self.labels[i])


@dataclass class DataCollatorForSupervisedDataset(object):
    &quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;

    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -&gt; Dict[str, torch.Tensor]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in (&quot;input_ids&quot;, &quot;labels&quot;))
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.Tensor(labels).long()
        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )


&quot;&quot;&quot; Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. &quot;&quot;&quot;

def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):
    if logits.ndim == 3:
        # Reshape logits to 2D if needed
        logits = logits.reshape(-1, logits.shape[-1])
    predictions = np.argmax(logits, axis=-1)
    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)
    valid_predictions = predictions[valid_mask]
    valid_labels = labels[valid_mask]
    return {
        # START NEW
        &quot;sum prediction&quot;: f'{sum(valid_predictions)}/{len(valid_predictions)}',
        # END NEW
        &quot;accuracy&quot;: sklearn.metrics.accuracy_score(valid_labels, valid_predictions),
        &quot;f1&quot;: sklearn.metrics.f1_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;matthews_correlation&quot;: sklearn.metrics.matthews_corrcoef(
            valid_labels, valid_predictions
        ),
        &quot;precision&quot;: sklearn.metrics.precision_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;recall&quot;: sklearn.metrics.recall_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
    }

&quot;&quot;&quot; Compute metrics used for huggingface trainer. &quot;&quot;&quot; def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):  # Unpack logits if it's a tuple
        logits = logits[0]
    return calculate_metric_with_sklearn(logits, labels)


class CustomTrainer(transformers.Trainer):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_predictions = []
        self.epoch_labels = []
        self.epoch_loss = []

    def compute_loss(self, model, inputs, return_outputs=False):
        &quot;&quot;&quot;
        MAX: Subclassed to compute training accuracy.

        How the loss is computed by Trainer. By default, all models return the loss in
        the first element.

        Subclass and override for custom behavior.
        &quot;&quot;&quot;
        if self.label_smoother is not None and &quot;labels&quot; in inputs:
            labels = inputs.pop(&quot;labels&quot;)
        else:
            labels = None
        outputs = model(**inputs, output_attentions=True)
        # TEST
        try:
            print(f&quot;Attention: {outputs.attentions}&quot;)
        except Exception:
            print(&quot;No Attention returned&quot;)

        if &quot;labels&quot; in inputs:
            preds = outputs.logits.detach()

            # Log accuracy
            acc = (
                (preds.argmax(axis=1) == inputs[&quot;labels&quot;])
                .type(torch.float)
                .mean()
                .item()
            )
            # Uncomment it if you want to plot the batch accuracy
            # wandb.log({&quot;batch_accuracy&quot;: acc})  # Log accuracy

            # Store predictions and labels for epoch-level metrics
            self.epoch_predictions.append(preds.cpu().numpy())
            self.epoch_labels.append(inputs[&quot;labels&quot;].cpu().numpy())

        # Save past state if it exists
        if self.args.past_index &gt;= 0:
            self._past = outputs[self.args.past_index]

        if labels is not None:
            loss = self.label_smoother(outputs, labels)
        else:
            loss = outputs[&quot;loss&quot;] if isinstance(outputs, dict) else outputs[0]
            # Uncomment it if you want to plot the batch loss
            # wandb.log({&quot;batch_loss&quot;: loss})
            self.epoch_loss.append(loss.item())  # Store loss for epoch-level metrics

        return (loss, outputs) if return_outputs else loss

# Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback):

    def __init__(self, trainer) -&gt; None:
        super().__init__()
        self._trainer = trainer

    def on_epoch_end(self, args, state, control, **kwargs):
        # Aggregate predictions and labels for the entire epoch
        epoch_predictions = np.concatenate(self._trainer.epoch_predictions)
        epoch_labels = np.concatenate(self._trainer.epoch_labels)

        # Compute accuracy
        accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels)

        # Compute mean loss
        mean_loss = np.mean(self._trainer.epoch_loss)

        # Compute precision, recall, and F1-score
        precision, recall, f1, _ = precision_recall_fscore_support(
            epoch_labels, epoch_predictions.argmax(axis=1), average=&quot;weighted&quot;
        )

        # Log epoch-level metrics
        wandb.log({&quot;epoch_accuracy&quot;: accuracy, &quot;epoch_loss&quot;: mean_loss})
        wandb.log({&quot;precision&quot;: precision, &quot;recall&quot;: recall, &quot;f1&quot;: f1})

        # Clear stored predictions, labels, and loss for the next epoch
        self._trainer.epoch_predictions = []
        self._trainer.epoch_labels = []
        self._trainer.epoch_loss = []
        return None

        # TODO: use this function to gather the prediction and labels and get the metrics
#%%
</code></pre>
<p>Instantiating and training:</p>
<pre><code>from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \
    TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \
    compute_metrics


from copy import deepcopy
from transformers import TrainerCallback
# END NEW
import os
import json
import torch
import transformers

from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
)

import wandb
run = wandb.init()
assert run is wandb.run

def train(device):
    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # load tokenizer
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        padding_side=&quot;right&quot;,
        use_fast=True,
        trust_remote_code=True,
    )

    if &quot;InstaDeepAI&quot; in model_args.model_name_or_path:
        tokenizer.eos_token = tokenizer.pad_token

    # define datasets and data collator
    train_dataset = SupervisedDataset(tokenizer=tokenizer,
                                      data_path=os.path.join(data_args.data_path, &quot;train.csv&quot;),
                                      kmer=data_args.kmer)
    val_dataset = SupervisedDataset(tokenizer=tokenizer,
                                    data_path=os.path.join(data_args.data_path, &quot;dev.csv&quot;),
                                    kmer=data_args.kmer)
    test_dataset = SupervisedDataset(tokenizer=tokenizer,
                                     data_path=os.path.join(data_args.data_path, &quot;test.csv&quot;),
                                     kmer=data_args.kmer)
    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)

    # load model
    model = transformers.AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        num_labels=train_dataset.num_labels,
        trust_remote_code=True,
        output_attentions = True
    ).to(device)

    # configure LoRA
    if model_args.use_lora:
        lora_config = LoraConfig(
            r=model_args.lora_r,
            lora_alpha=model_args.lora_alpha,
            target_modules=list(model_args.lora_target_modules.split(&quot;,&quot;)),
            lora_dropout=model_args.lora_dropout,
            bias=&quot;none&quot;,
            task_type=&quot;SEQ_CLS&quot;,
            inference_mode=False,
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    trainer = CustomTrainer(model=model,
                            tokenizer=tokenizer,
                            args=training_args,
                            compute_metrics=compute_metrics,
                            train_dataset=train_dataset,
                            eval_dataset=val_dataset,
                            data_collator=data_collator
                            )

    trainer.add_callback(CustomCallback(trainer))
    trainer.train()

    # train_result = trainer.train()
    # loss = train_result[&quot;loss&quot;]
    # print(f&quot;loss issss: {loss}&quot;)
    # print(f&quot;Train reusults: {train_result}&quot;) # NEW: result: only returns metrics at the end of training

    if training_args.save_model:
        trainer.save_state()
        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)

    # get the evaluation results from trainer
    if training_args.eval_and_save_results:
        results_path = os.path.join(training_args.output_dir, &quot;results&quot;, training_args.run_name)
        results = trainer.evaluate(eval_dataset=test_dataset)
        os.makedirs(results_path, exist_ok=True)
        with open(os.path.join(results_path, &quot;eval_results.json&quot;), &quot;w&quot;) as f:
            json.dump(results, f)


if __name__ == &quot;__main__&quot;:
    # Define device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print('Using device:', device)
    # Call the train function with the device
    train(device)
</code></pre>
<p>After training, I try to run it on an example:</p>
<pre><code>model_path = './finetune/output/dnabert2'
tokenizer = AutoTokenizer.from_pretrained(model_path)
# Load the model with output_attention=True
model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True)
model_input = tokenizer(&quot;ACTGACGGGTAGTGACTG&quot;, return_tensors=&quot;pt&quot;)

with torch.inference_mode():
  output = model(**model_input, output_attentions=True)
</code></pre>
<p>My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.</p>
",Text Generation & LLMs,attention returned even output attention true using pretrained model based bert github link dnabert us automodelforsequenceclassification mosaicml mosaic bert base problem extract attention read many post show way dealing activating output attention true model none post solved problem length element shape trying get get sure search solution would providing whole code defining model trainer data tokenizer instantiating training training try run example code might test print let know anything missing thank much help
How to decide which chunking technique to use for implementing Retrieval-Augmented Generation(RAG),"<p>I want to automatically generate testcases using generative AI. For this purpose I will be using open source LLM (Llama 3, will try others as well). Since the LLM is trained only on publically available data, it needs more information regarding the application being developed (for which I wish to generate testcases) and the requirements which contain detailed information regarding the expected behaviour.</p>
<p>This additional information can be provided through RAG. The Vector Database being used is ChromaDB.</p>
<p>As of now, I want this additional information to be provided as a PDF file.
According to my researched, there are many ways of dividing this PDF file into smaller chunks:</p>
<ul>
<li>Recursive Character Splitter</li>
<li>Sentence splitter</li>
<li>Semantic splitting</li>
<li>LLM based chunking</li>
<li>Document specific splitting</li>
</ul>
<p>Please do let me know if I missed some other useful method.</p>
<p>So there are 2 questions here:</p>
<ol>
<li>How to decide which chunking method to choose?</li>
<li>How can I evaluate the performance of the chosen chunking technique?</li>
</ol>
",Text Generation & LLMs,decide chunking technique use implementing retrieval augmented generation rag want automatically generate testcases using generative ai purpose using open source llm llama try others well since llm trained publically available data need information regarding application developed wish generate testcases requirement contain detailed information regarding expected behaviour additional information provided rag vector database used chromadb want additional information provided pdf file according researched many way dividing pdf file smaller chunk recursive character splitter sentence splitter semantic splitting llm based chunking document specific splitting please let know missed useful method question decide chunking method choose evaluate performance chosen chunking technique
How to make LLMs answer in certain pattern,"<p>How can I design the LLM program so that the LLM will answer in some preset pattern?For example, if I want to use the LLM to promote some products or collect some information from the user, what adjustment should I try on the LLM program? Thx~</p>
<p>I tried modifying the prompts but it doesn't work well when the user says something deviated from the conversation pattern, like when the user does not answer LLM's question.</p>
",Text Generation & LLMs,make llm answer certain pattern design llm program llm answer preset pattern example want use llm promote product collect information user adjustment try llm program thx tried modifying prompt work well user say something deviated conversation pattern like user doe answer llm question
Optimal Learning Rate and Batch Size for LLM Training,"<p>What are the best practices for optimizing batch size and learning rate in training Large Language Models (LLMs)?</p>
<p>How should these hyperparameters be adjusted relative to each other for efficient convergence and improved performance?</p>
<p>Additionally, could you provide a concise example illustrating the interplay between batch size and learning rate adjustments in training an LLM on a text generation task?</p>
",Text Generation & LLMs,optimal learning rate batch size llm training best practice optimizing batch size learning rate training large language model llm hyperparameters adjusted relative efficient convergence improved performance additionally could provide concise example illustrating interplay batch size learning rate adjustment training llm text generation task
usage of vllm for extracting embeddings,"<p>Following is a little piece of code to extract embeddings from a certain layer of LLM:</p>
<pre class=""lang-py prettyprint-override""><code>def process_row(prompt: str, model, tokenizer, layers_to_use: list, remove_period: bool):
    &quot;&quot;&quot;
    Processes a row of data and returns the embeddings.
    &quot;&quot;&quot;
    if remove_period:
        prompt = prompt.rstrip(&quot;. &quot;)
    inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
    with torch.no_grad():
        outputs = model.generate(inputs.input_ids, output_hidden_states=True, return_dict_in_generate=True, max_new_tokens=1, min_new_tokens=1)
    embeddings = {}
    for layer in layers_to_use:
        last_hidden_state = outputs.hidden_states[0][layer][0][-1]
        embeddings[layer] = [last_hidden_state.numpy().tolist()]
    return embeddings
</code></pre>
<p>It's pretty standard way, but it's pretty slow. Is there any way to use vllm to make it faster without needing to call generate function everytime? I've tried batching, but it's slow too. Any help is appreciated!</p>
<p>One way to get last hidden state values using vllm is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from vllm import LLM, SamplingParams
from vllm.sequence import (SamplerOutput, Sequence, SequenceGroup, SequenceData, 
                           SequenceGroupMetadata, SequenceStatus)
from transformers import LlamaModel, LlamaTokenizer
from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput
from vllm.sequence import SamplerOutput, SequenceData, SequenceGroupMetadata


llm = LLM(model=path_to_llama2)


# Enable top-k sampling to reflect the accurate memory usage.
vocab_size = llm.llm_engine.workers[0].model.config.vocab_size
sampling_params = SamplingParams(top_p=0.99, top_k=vocab_size - 1)
max_num_batched_tokens = llm.llm_engine.workers[0].scheduler_config.max_num_batched_tokens
max_num_seqs = llm.llm_engine.workers[0].scheduler_config.max_num_seqs
</code></pre>
<pre class=""lang-py prettyprint-override""><code>prompt = train[0]
prompt_token_ids = llm.llm_engine.tokenizer.encode(prompt) #[2, 100, 524, 10]
seqs = []
    
group_id = 1
seq_data = SequenceData(prompt_token_ids)
seq = SequenceGroupMetadata(
    request_id=str(group_id),
    is_prompt=True,
    seq_data={group_id: seq_data},
    sampling_params=sampling_params,
    block_tables=None,
)
seqs.append(seq)
input_tokens, input_positions, input_metadata = llm.llm_engine.workers[0]._prepare_inputs(
    seqs)
prompt_len = len(seq_data.prompt_token_ids)
input_tokens = input_tokens[:prompt_len]
input_positions = input_positions[:prompt_len]
# Execute the model.
num_layers = llm.llm_engine.workers[0].model_config.get_num_layers(llm.llm_engine.workers[0].parallel_config)
tempOut = llm.llm_engine.workers[0].model.model(
    input_ids=input_tokens,
    positions=input_positions,
    kv_caches=[(None, None)] * num_layers,
    input_metadata=input_metadata,
    cache_events=None,
)
print(tempOut.size())
</code></pre>
<p>but this doesn't get me with all the hidden state embeddings (of all layers). Is there any other way to get such values in a faster manner?</p>
",Text Generation & LLMs,usage vllm extracting embeddings following little piece code extract embeddings certain layer llm pretty standard way pretty slow way use vllm make faster without needing call generate function everytime tried batching slow help appreciated one way get last hidden state value using vllm follows get hidden state embeddings layer way get value faster manner
NefTune Receiving 0 Training Loss on Transformers,"<p>I'm basically trying to fine-tune my model with Neftune. Model is based on Turkish Language. But there I'm receiving zero training lose. I've tried to another model like <a href=""https://huggingface.co/ytu-ce-cosmos/turkish-gpt2-large"" rel=""nofollow noreferrer"">Turkish-GPT2</a> there is no issue everything is okay. I think probably there is problem with model. I don't know how to handle with this issue.</p>
<p>Loading Model:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;asafaya/kanarya-750m&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;asafaya/kanarya-750m&quot;)

v3_prompt = &quot;&quot;&quot;Aşağıda, daha fazla bağlam sağlayan bir girdiyle eşleştirilmiş, bir görevi açıklayan bir talimat bulunmaktadır. İsteği uygun şekilde tamamlayan bir yanıt yazın.

### Input:
{}

### Instructions:
{}

### Response:
{}
&quot;&quot;&quot;
</code></pre>
<p>Changing format prompts:</p>
<pre><code>EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    inputs       = examples[&quot;input&quot;]
    instructions = examples['instructions']
    outputs      = examples[&quot;response&quot;]
    texts = []
    for input, instructions, output in zip(inputs, instructions, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = v3_prompt.format(input, instructions, output) + EOS_TOKEN
        texts.append(text)
    return { &quot;text&quot; : texts, }
pass

from datasets import Dataset

dataset = Dataset.from_pandas(data[:40000])
dataset = dataset.map(formatting_prompts_func, batched=True)
</code></pre>
<p>Neftune:</p>
<pre><code>from trl import SFTTrainer
from transformers import TrainingArguments

trainer_2 = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=512,
    neftune_noise_alpha=5,
    packing=False,
    args = TrainingArguments(
        per_device_train_batch_size = 1, #  batch size
        gradient_accumulation_steps = 2, #  gradient accumulation steps
        warmup_steps = 5,
        max_steps = 80,
        learning_rate = 2e-4,
        fp16 = False,
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = &quot;adamw_8bit&quot;,
        weight_decay = 0.01,
        lr_scheduler_type = &quot;linear&quot;,
        seed = 3407,
        output_dir = &quot;outputs&quot;,
    ),
)
trainer_2.train()
</code></pre>
<p>Output:</p>
<pre><code> [80/80 00:25, Epoch 0/1]
Step    Training Loss
1   0.000000
2   0.000000
3   0.000000
4   0.000000
5   0.000000
6   0.000000
7   0.000000
8   0.000000
9   0.000000
10  0.000000
11  0.000000
12  0.000000
13  0.000000
14  0.000000
15  0.000000
16  0.000000
17  0.000000
18  0.000000
19  0.000000
20  0.000000
21  0.000000
22  0.000000
23  0.000000
24  0.000000
25  0.000000
26  0.000000
27  0.000000
28  0.000000
29  0.000000
30  0.000000
31  0.000000
32  0.000000
33  0.000000
34  0.000000
35  0.000000
36  0.000000
37  0.000000
38  0.000000
39  0.000000
40  0.000000
41  0.000000
42  0.000000
43  0.000000
44  0.000000
45  0.000000
46  0.000000
47  0.000000
48  0.000000
49  0.000000
50  0.000000
51  0.000000
52  0.000000
53  0.000000
54  0.000000
55  0.000000
56  0.000000
57  0.000000
58  0.000000
59  0.000000
60  0.000000
61  0.000000
62  0.000000
63  0.000000
64  0.000000
65  0.000000
66  0.000000
67  0.000000
68  0.000000
69  0.000000
70  0.000000
71  0.000000
72  0.000000
73  0.000000
74  0.000000
75  0.000000
76  0.000000
77  0.000000
78  0.000000
79  0.000000
80  0.000000
TrainOutput(global_step=80, training_loss=0.0, metrics={'train_runtime': 25.3789, 'train_samples_per_second': 6.304, 'train_steps_per_second': 3.152, 'total_flos': 117937578909696.0, 'train_loss': 0.0, 'epoch': 0.004})
</code></pre>
",Text Generation & LLMs,neftune receiving training loss transformer basically trying fine tune model neftune model based turkish language receiving zero training lose tried another model like turkish gpt issue everything okay think probably problem model know handle issue loading model changing format prompt neftune output
DFS search sibling node in specific condition else search children node,"<p>I have to do a DFS to search a tree, the DFS will first generate a result by the value of the node. The function check_for_three_kind_of_response is a function that checks if a question is related to a sentence. The result would be three responses, -1 means the sentence is not related to the question, 0 means the sentence is related to the question but the sentence can't help to answer the question, and last, return the answer(text) if the sentence can answer the question. If the result is -1 the DFS should search the sibling node, when the result is 0, the DFS should search the children node. Also, when the result is -1 but doesn't have a sibling node, it should also search its children node. At last, if the result is text, it should just return the text.</p>
<p>This is what I have tried, however, I was stuck in the condition when the result equals -1. Also, my tree is not a binary tree, its children are listed.</p>
<pre><code>#                 Head
#                  |
#                Body
#                /  \
#      paragraph1       paragraph2
#       /   \             /  \  
# Sentence Sentence Sentence Sentence
</code></pre>
<p>This is what the tree looks like, the DFS first searches the head and returns -1 if it's not related to the question. Then, since the head doesn't have a siblings node, so it goes to its children node, which is the body. If it still returns -1, it would search its children node paragraph 1 since it didn't have a siblings node. When paragraph1 returns -1 in the function, it would jump to paragraph2. If it returns 0 it will go down to sentence. In a sentence, if the first one returns 0 it will search the next sentence while -1 will do the same thing. If all sentences return -1, the DFS should stop or otherwise go back to paragraph 2 to do the same routine again.</p>
<pre><code>class TreeNode:
    def __init__(self, value, parent = None):
        self.value = value
        self.children = []

def dfs(node, question):
    result = check_for_three_kind_of_response(question, node.value)
    # Check if the search function returns 0, -1, or text
    if result == &quot;-1&quot;:
        return &quot;-1&quot;
    elif result != &quot;0&quot;:
        return result
    for c in node.children:
        result = dfs(c, question)
        if result != &quot;-1&quot;:
            return result
    return &quot;-1&quot;
</code></pre>
",Text Generation & LLMs,dfs search sibling node specific condition else search child node dfs search tree dfs first generate result value node function check three kind response function check question related sentence result would three response mean sentence related question mean sentence related question sentence help answer question last return answer text sentence answer question result dfs search sibling node result dfs search child node also result sibling node also search child node last result text return text tried however wa stuck condition result equal also tree binary tree child listed tree look like dfs first search head return related question since head sibling node go child node body still return would search child node paragraph since sibling node paragraph return function would jump paragraph return go sentence sentence first one return search next sentence thing sentence return dfs stop otherwise go back paragraph routine
Is there any way to standardize different text that represents the same thing?,"<p>I am collecting product data from different supermarket's website. But I found that each store uses different name to represent the product.</p>
<p>For example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Product name</th>
<th>Store</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>360° Toothbrush With Tongue And Cheek Cleaner</td>
<td>NoFrills</td>
<td><a href=""https://www.nofrills.ca/360-toothbrush-with-tongue-and-cheek-cleaner/p/20306410001_EA"" rel=""nofollow noreferrer"">product link</a></td>
</tr>
<tr>
<td>MEDIUM TOOTHBRUSH, 360°</td>
<td>FoodBasics</td>
<td><a href=""https://www.foodbasics.ca/aisles/health-beauty/oral-care/toothbrushes/medium-toothbrush/p/058000006140"" rel=""nofollow noreferrer"">product link</a></td>
</tr>
</tbody>
</table></div>
<p>and I want to standardized this as &quot;360° Toothbrush&quot;.</p>
<p>My approach is first try to tokenized the product name and then compare the similarity by cosine law. But I found that this is not the right direction as I am seeking the <strong>standarized name</strong> before flushing to database (in order to categorize the &quot;same&quot; product by the store).</p>
<p>Please find my testing script below (thanks ChatGPT!)</p>
<pre><code>import torch
from transformers import BertTokenizer, BertModel

def get_sentence_vector(text):
  
    # Initialize the tokenizer and model
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    # Ensure the model is in evaluation mode
    model.eval()

    # Encode the text
    inputs = tokenizer(text, return_tensors='pt')

    # Perform a forward pass without gradient calculation
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract the last hidden state
    last_hidden_states = outputs.last_hidden_state

    # Aggregate the hidden states using mean pooling
    sentence_vector = torch.mean(last_hidden_states, dim=1)

    return sentence_vector


def cosine_similarity(vec1, vec2):
    # Calculate the cosine similarity
    cosine_sim = torch.nn.functional.cosine_similarity(vec1, vec2, dim=1)
    
    return cosine_sim.item() 


example_text = &quot;360° Toothbrush With Tongue And Cheek Cleaner&quot;
example_text_2 = &quot;MEDIUM TOOTHBRUSH, 360°&quot;
example_text_3 = &quot;Hair Expertise Hyaluron Plump Shampoo, with Hyaluronic Acid&quot;

vector = get_sentence_vector(example_text)
vector_2 = get_sentence_vector(example_text_2)
vector_3 = get_sentence_vector(example_text_3)

print(cosine_similarity(vector, vector_2)) # 0.8426903486251831
print(cosine_similarity(vector, vector_3)) # 0.7190334796905518
print(cosine_similarity(vector_2, vector_3)) # 0.622465193271637
</code></pre>
<p>The above approach indeed give me some sense, the same product gives higher similarity. But how to make use of this result to give a standardized name?</p>
",Text Generation & LLMs,way standardize different text represents thing collecting product data different supermarket website found store us different name represent product example product name store link toothbrush tongue cheek cleaner nofrills product link medium toothbrush foodbasics product link want standardized toothbrush approach first try tokenized product name compare similarity cosine law found right direction seeking standarized name flushing database order categorize product store please find testing script thanks chatgpt approach indeed give sense product give higher similarity make use result give standardized name
How to Generate Text with Specific Length Using AI API,"<p>I am attempting to generate text outputs that are exactly a certain number of characters or words long using AI API (OpenAI GPT, Claude, Gemini...), but I'm facing difficulties. Here's what I've tried so far:</p>
<p><strong>Setting Max Tokens</strong>: I've used the max_tokens parameter hoping to limit the output length, but then text is truncated.</p>
<p><strong>Explicit Prompt Requests</strong>: I've tried including explicit instructions in the prompt about the desired length. However, this approach has not produced the precise output lengths I need (ex: gives 600 words instead of 800).</p>
<p>I am looking for suggestions on how to configure the API calls or promtp to achieve exact output lengths. Is there a way to better utilize OpenAI/Claude’s parameters, or is there a method to post-process the text to fit the required length?</p>
<p>I need this because I inject the text in slides. Any alternative solution?</p>
",Text Generation & LLMs,generate text specific length using ai api attempting generate text output exactly certain number character word long using ai api openai gpt claude gemini facing difficulty tried far setting max token used max token parameter hoping limit output length text truncated explicit prompt request tried including explicit instruction prompt desired length however approach ha produced precise output length need ex give word instead looking suggestion configure api call promtp achieve exact output length way better utilize openai claude parameter method post process text fit required length need inject text slide alternative solution
Weighted input for sentence similarity,"<p>I am building a language model for sentence similarity using dot score. Currently, I am using <a href=""https://huggingface.co/thenlper/gte-large"" rel=""nofollow noreferrer"">gte-large</a>language model from hugging face.</p>
<p>I was wondering if there is a way to give a weighted input of text. For example, the input sentence
<code>Crickets-insect</code>
Is there a way to give more weightage to insect here so that when compared with huge number of words, it shows more similarity with insects than cricket the sport?</p>
",Text Generation & LLMs,weighted input sentence similarity building language model sentence similarity using dot score currently using gte largelanguage model hugging face wa wondering way give weighted input text example input sentence way give weightage insect compared huge number word show similarity insect cricket sport
How to use forward() method instead of model.generate() for T5 model,"<p>For my use case, I need to use the model.forward() instead of the model.generate() method
i.e instead of the below code</p>
<pre><code>outs = model.model.generate(input_ids=batch['source_ids'],
                                 attention_mask=batch['source_mask'],
                                 output_scores=True,
                                 max_length=model.model_arguments.max_output_seq_length)

preds_cleaned = [model.tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True) for ids in outs]
</code></pre>
<p>I need to use</p>
<pre><code>model_outputs = model.model(
            input_ids=batch[&quot;source_ids&quot;],
            attention_mask=batch[&quot;source_mask&quot;],
            labels=lm_labels.to(device),
            decoder_attention_mask=batch['target_mask']
        )
logits = model_outputs.logits
softmax_logits = m(logits)
max_logits = torch.max(softmax_logits, dim=2)

    
</code></pre>
<p>decoding these logits gives unprocessed text that has many issues like repetition of words at the end etc.
What do I need to do to get the same result as model.generate() ?</p>
",Text Generation & LLMs,use forward method instead model generate model use case need use model forward instead model generate method e instead code need use decoding logits give unprocessed text ha many issue like repetition word end etc need get result model generate
Can I appoint a Masked Language Model&#39;s outputs&#39; range?,"<p>When different kinds of models are trained with masked language modeling, the input embeddings at masked positions are replaced with a MASK token. I'm wondering if I could appoint the range of a MASK token? For example:</p>
<pre><code>1) &quot;What a [MASK] weather!&quot; 
2) &quot;What a [MASK] person he is!&quot;
3) &quot;How can you do such a [MASK] thing!&quot; 

...
</code></pre>
<p>Instead of let a pretrained model using its ability to find a suitable word in its whole vocab, I want the pretrained model to pick a word which is from a specific token set, e.g {&quot;good&quot;,&quot;great&quot;,&quot;stupid&quot;,&quot;bad&quot;}, to replace the MASK token. In another words, when facing all different kinds of input, I wish the model could replace the MASK token using the word from the specific token set. Could anyone give me some hints to do this? Thanks!</p>
",Text Generation & LLMs,appoint masked language model output range different kind model trained masked language modeling input embeddings masked position replaced mask token wondering could appoint range mask token example instead let pretrained model using ability find suitable word whole vocab want pretrained model pick word specific token set e g good great stupid bad replace mask token another word facing different kind input wish model could replace mask token using word specific token set could anyone give hint thanks
Retrieval Augmented generation vs. LLM context,"<p>I am still learning the concepts behind RAG but I was wondering,
alot if references explain RAG by saying that you will be able to increase the LLM knowledge by augmenting a new knowledge using an external retrieving system.</p>
<p>But how is that different than just add a context (even if its long as some LLMs has a huge window size) to the model prompt?</p>
",Text Generation & LLMs,retrieval augmented generation v llm context still learning concept behind rag wa wondering alot reference explain rag saying able increase llm knowledge augmenting new knowledge using external retrieving system different add context even long llm ha huge window size model prompt
How do I make sure answers are from a customized (fine-tuning) dataset?,"<p>I'm using customized text with 'Prompt' and 'Completion' to train new model.</p>
<p>Here's the tutorial I used to create customized model from my data:</p>
<p><a href=""https://beta.openai.com/docs/guides/fine-tuning/advanced-usage"" rel=""nofollow noreferrer"">beta.openai.com/docs/guides/fine-tuning/advanced-usage</a></p>
<p>However even after training the model and sending prompt text to the model, I'm still getting generic results which are not always suitable for me.</p>
<p>How I can make sure completion results for my prompts will be only from the text I used for the model and not from the generic OpenAI models?</p>
<p>Can I use some flags to eliminate results from generic models?</p>
",Text Generation & LLMs,make sure answer customized fine tuning dataset using customized text prompt completion train new model tutorial used create customized model data beta openai com doc guide fine tuning advanced usage however even training model sending prompt text model still getting generic result always suitable make sure completion result prompt text used model generic openai model use flag eliminate result generic model
Alternative to one-hot encoding for output to a model when vocabulary size is very large,"<p>I was following <a href=""https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"" rel=""nofollow noreferrer"">this blog</a>. In it he talks about how to build a language model in keras. He shows how to build a simple model in keras.</p>
<blockquote>
<p>After separating, we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.</p>
<p>This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.</p>
<p>Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.</p>
</blockquote>
<p>He uses the following:</p>
<p><code>y = to_categorical(y, num_classes=vocab_size)</code></p>
<p>In his case, the vocabulary size is manageable. I am working with vocabulary having size &gt; 100 million. I guess I should not use a one-hot encoding for the output <code>y</code> as done by him. Is there any alternative?</p>
",Text Generation & LLMs,alternative one hot encoding output model vocabulary size large wa following blog talk build language model kera show build simple model kera separating need one hot encode output word mean converting integer vector value one word vocabulary indicate specific word index word integer value model learns predict probability distribution next word ground truth learn word except actual word come next kera provides categorical used one hot encode output word input output sequence pair us following case vocabulary size manageable working vocabulary size million guess use one hot encoding output done alternative
"ChatBot langchain, set memory+retriever+map_reduce","<p>I am new on this topics.I'd like to get a chain that allows me to use a retriever ,a memory and to set the chain_type = 'map_reduce'.In addition to that I'd like to know if is it possible to use the map_reduce only in the case in which the number of token exceeds the limit.Thanks in advice.</p>
<p>I tried to use the 'ConversationalRetrievalChain.from_llm' or 'RetrievalQA.from_llm' but I'm not figuring out how to combine the different functions of the chains.</p>
",Text Generation & LLMs,chatbot langchain set memory retriever map reduce new topic like get chain allows use retriever memory set chain type map reduce addition like know possible use map reduce case number token exceeds limit thanks advice tried use conversationalretrievalchain llm retrievalqa llm figuring combine different function chain
How do non-LLM models compare to LLMs for Abstractive Summaries of HTML content?,"<p>I'm interested in utilizing an NLP model to provide short (one sentence in length) abstractive summaries of web pages, providing the model a set of commonly occurring HTML content from each web page (for example, heading tags, meta, the title, and so on).</p>
<p>In my experience, large language models could do this quite effectively. However, I'm interested in alternatives to LLMs -- how they perform, how costly in terms of computational resources they are compared to LLMs, and what degree of setup might be required for them compared to the comparatively simple prompt-and-respond format of commercial LLMs.</p>
",Text Generation & LLMs,non llm model compare llm abstractive summary html content interested utilizing nlp model provide short one sentence length abstractive summary web page providing model set commonly occurring html content web page example heading tag meta title experience large language model could quite effectively however interested alternative llm perform costly term computational resource compared llm degree setup might required compared comparatively simple prompt respond format commercial llm
Can I use LoRa and Prompt Tuning at the same time for text summarization with GPT?,"<p>LoRA is to insert and learn the rank composition matrix created by dimensionally reducing the weight matrix in the transformer. Prompt Tuning, on the other hand, typically uses a soft prompt that encodes the prompt within the model to learn, rather than a hard prompt that a person gives the task directly. Both are effective in lightening, especially prompt tuning, which is better than hard prompt use.</p>
<p>Both techniques can also be implemented using the peft module.</p>
<pre><code>from peft import get_peft_model, PeftModel, TaskType, LoraConfig, PromptTuningConfig, PromptTuningInit

for path,dirs,files in os.walk('/root/.cache/huggingface/hub/models--kakaobrain--kogpt'):
  for file in files:
    if file.endswith('tokenizer.json'):
      tokenizer_path = path
print(tokenizer_path)

prompt_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    num_virtual_tokens=10,
    prompt_tuning_init=PromptTuningInit.TEXT,
    prompt_tuning_init_text=&quot;Read the following and summarize:&quot;,
    tokenizer_name_or_path=tokenizer_path
)

lora_config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    r=8, lora_alpha=32, lora_dropout=0.1,
    target_modules = ['q_proj', 'v_proj'],
    # target_modules = r&quot;.*(q_proj|v_proj)&quot;,
)
</code></pre>
<p>However, the get_feft_model function receives only the model and one peft_config as parameters.</p>
<pre><code>peft_model = get_peft_model(base_model, prompt_config)
</code></pre>
<p>I want to use both techniques at the same time. How shall I do it?</p>
",Text Generation & LLMs,use lora prompt tuning time text summarization gpt lora insert learn rank composition matrix created dimensionally reducing weight matrix transformer prompt tuning hand typically us soft prompt encodes prompt within model learn rather hard prompt person give task directly effective lightening especially prompt tuning better hard prompt use technique also implemented using peft module however get feft model function receives model one peft config parameter want use technique time shall
Text Generation consistently results in blank characters,"<p>The following code is showing a very normal loss chart after 20 epochs, but when trying to test it with a seed text it consistently outputs blank lines (' '). Its either that I simply do not understand the process of &quot;text seed&quot; preparation or processing the predictions or something subtle is wrong that I have not been able to notice, hence my appeal to this community.</p>
<p><img src=""https://i.sstatic.net/QSWz3.jpg"" alt=""Loss Function"" /></p>
<p>Data used here is a small portion of the nietzsche.txt located here:</p>
<p><a href=""https://s3.amazonaws.com/text-datasets/nietzsche.txt"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/text-datasets/nietzsche.txt</a></p>
<p>The main issue is that the predictions always have <code>argmax() = 1</code>. The word dictionary associated with the test data has ' ' at that index, therefore the output is always series of blank characters. The data being fed to the model however does seem reasonable as shown in the <code>x sequence</code> of integers shown in the [OUTPUT] section below. You can see that after every iteration, a new (and different) character is added to the <code>pattern</code> list and the list is sliced to maintain a constant length by dropping the first character.</p>
<p>I have been trying to figure out where I am going wrong in generating non-blank texts and have not been able to identify the issue. I would be grateful for any help.</p>
<pre><code>import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical

# load ascii text and covert to lowercase
filename = &quot;/content/drive/MyDrive/Colab Notebooks/TexGen/nietzsche-short.txt&quot;
raw_text = open(filename, 'r', encoding='utf-8').read()
raw_text = raw_text.lower()

# create mapping of unique chars to integers, and a reverse mapping
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
int_to_char = dict((i, c) for i, c in enumerate(chars))

# summarize the loaded data
n_chars = len(raw_text)
n_vocab = len(chars)

# prepare the dataset of input to output pairs encoded as integers
seq_length = 100
dataX = []
dataY = []

for i in range(0, n_chars - seq_length, 1):
 seq_in = raw_text[i:i + seq_length]
 seq_out = raw_text[i + seq_length]
 dataX.append([char_to_int[char] for char in seq_in])
 dataY.append(char_to_int[seq_out])

n_patterns = len(dataX)

# reshape X to be [samples, time steps, features]
X = np.reshape(dataX, (n_patterns, seq_length, 1))

# normalize
X = X / float(n_vocab)

# one hot encode the output variable
y = to_categorical(dataY)

# define the LSTM model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(n_vocab, 50, input_length=seq_length),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),  # CNN layer
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(256, return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(256),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(y.shape[2], activation='softmax')
], name=&quot;LSTM_Model&quot;)

model.compile(loss='categorical_crossentropy', optimizer='adam')

history = model.fit(X, y,
          epochs=25,
          batch_size=128
)

# Test the model with a seed
start = np.random.randint(0, len(dataX)-1)
pattern = dataX[start] # dataX is a list of list 100 characters each
print(&quot;Seed:&quot;)
print(&quot;\&quot;&quot;, ''.join([int_to_char[value] for value in pattern]), &quot;\&quot;&quot;)

# seed:
# &quot; ay upon words, a deception on the part of grammar, or an
# audacious generalization of very restricted &quot;

# pattern[:10]
# [13, 37, 1, 33, 28, 27, 26, 1, 35, 27]

# generate characters
for i in range(5):
  x = np.reshape(pattern, (1, len(pattern), 1))
  x = x / float(n_vocab)
  print(&quot;\n==================&quot;)
  print(&quot;x[:10] : &quot;, x[:, :10, :])
  prediction = model.predict(x, verbose=0)
  index = np.argmax(prediction)
  result = int_to_char[index]
  print(&quot;index = &quot;, index)
  print(&quot;result = &quot;, result)
  # seq_in = [int_to_char[value] for value in pattern]
  #sys.stdout.write(result)
  pattern.append(index)
  pattern = pattern[1:len(pattern)]
print(&quot;\nDone.&quot;)


[output]

# predictions:

array([[0.01481258, 0.1349412 , 0.00109681, 0.00254168, 0.00037268,
        0.00085235, 0.00087828, 0.01556777, 0.00960505, 0.00228236,
        0.00174035, 0.00123438, 0.00138978, 0.07334321, 0.01076234,
        0.01881236, 0.03297085, 0.0944486 , 0.0203624 , 0.01628518,
        0.04297792, 0.05854145, 0.00125222, 0.00374453, 0.02957868,
        0.0199816 , 0.05518206, 0.05479056, 0.02143795, 0.000657  ,
        0.04608261, 0.06542768, 0.08481915, 0.02293939, 0.00776505,
        0.01505006, 0.00033998, 0.01451185, 0.00062015]], dtype=float32)

==================
x[:10] :  [[[0.33333333]
  [0.94871795]
  [0.02564103]
  [0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]]]
index =  1
result =   

==================
x[:10] :  [[[0.94871795]
  [0.02564103]
  [0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]]]
index =  1
result =   

==================
x[:10] :  [[[0.02564103]
  [0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]
  [0.41025641]]]
index =  1
result =   

==================
x[:10] :  [[[0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]
  [0.41025641]
  [0.79487179]]]
index =  1
result =   

==================
x[:10] :  [[[0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]
  [0.41025641]
  [0.79487179]
  [0.17948718]]]
index =  1
result =   

Done.
[/output]
</code></pre>
<pre><code>model.summary()

Model: &quot;LSTM_Model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 100, 50)           1950      
                                                                 
 conv1d (Conv1D)             (None, 96, 128)           32128     
                                                                 
 max_pooling1d (MaxPooling1  (None, 24, 128)           0         
 D)                                                              
                                                                 
 lstm (LSTM)                 (None, 24, 256)           394240    
                                                                 
 dropout (Dropout)           (None, 24, 256)           0         
                                                                 
 lstm_1 (LSTM)               (None, 256)               525312    
                                                                 
 dropout_1 (Dropout)         (None, 256)               0         
                                                                 
 dense (Dense)               (None, 39)                10023     
                                                                 
=================================================================
Total params: 963653 (3.68 MB)
Trainable params: 963653 (3.68 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________


  [1]: https://i.sstatic.net/QSWz3.jpg
  [2]: https://s3.amazonaws.com/text-datasets/nietzsche.txt
</code></pre>
",Text Generation & LLMs,text generation consistently result blank character following code showing normal loss chart epoch trying test seed text consistently output blank line either simply understand process text seed preparation processing prediction something subtle wrong able notice hence community data used small portion nietzsche txt located main issue prediction always word dictionary associated test data ha index therefore output always series blank character data fed model however doe seem reasonable shown integer shown output section see every iteration new different character added list list sliced maintain constant length dropping first character trying figure going wrong generating non blank text able identify issue would grateful help
How to i get word embeddings for out of vocabulary words using a transformer model?,"<p>When i tried to get word embeddings of a sentence using bio_clinical bert, for a sentence of 8 words i am getting 11 token ids(+start and end) because &quot;embeddings&quot; is an out of vocabulary word/token, that is being split into <code>em</code>, <code>bed</code> ,<code>ding</code>, <code>s</code>.</p>
<p>I would like to know if there is any aggregation strategies available that make sense apart from doing a mean of these vectors.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
# download and load model
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

sentences = ['This framework generates embeddings for each input sentence']


#Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')


#Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

print(encoded_input['input_ids'].shape)
</code></pre>
<p>Output:
<code>torch.Size([1, 13])</code></p>
<pre class=""lang-py prettyprint-override""><code>for token in encoded_input['input_ids'][0]:
      print(tokenizer.decode([token]))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]
this
framework
generates
em
##bed
##ding
##s
for
each
input
sentence
[SEP]
</code></pre>
",Text Generation & LLMs,get word embeddings vocabulary word using transformer model tried get word embeddings sentence using bio clinical bert sentence word getting token id start end embeddings vocabulary word token split would like know aggregation strategy available make sense apart mean vector output output
Finetuing BERT with masking and having multiple correct labels,"<p>I aim to fine-tune a BERT model for a specific task involving simple arithmetic operations like &quot;5 + 3 = 8&quot; or &quot;7 plus 2 equals 9&quot;. My dataset comprises thousands of examples where one operand, operator, or result is masked. For instance:</p>
<ul>
<li>Masked: &quot;1 added to [MASK] equals 7&quot;, Label: &quot;1 added to 6 is equal
to 7&quot;</li>
<li>Masked: &quot;6 plus 5 [MASK] 11&quot;, Label: &quot;6 plus 5 gives 11&quot;</li>
</ul>
<p>The challenge lies in ensuring that multiple correct labels are accepted for a masked sample during training. For instance, if the model predicts &quot;equals&quot; instead of the masked token in the second sample, it should be considered correct.</p>
",Text Generation & LLMs,finetuing bert masking multiple correct label aim fine tune bert model specific task involving simple arithmetic operation like plus equal dataset comprises thousand example one operand operator result masked instance masked added mask equal label added equal masked plus mask label plus give challenge lie ensuring multiple correct label accepted masked sample training instance model predicts equal instead masked token second sample considered correct
How are tools passed to llm? (final prompt sent to openai chat model),"<p>When using the openai chat api, we send an array of messages and tools. How are they assembled into a single prompt?</p>
<pre><code>System:
Tools:
User:
Assistant:
User:
</code></pre>
<p>I tried setting <code>langchain.debug = True</code>, but I don't see the tools being passed to the llm.</p>
<pre class=""lang-py prettyprint-override""><code>from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_openai import ChatOpenAI

import langchain
langchain.debug = True

prompt = hub.pull(&quot;hwchase17/openai-tools-agent&quot;)
tools = [TavilySearchResults(max_results=1)]

llm = ChatOpenAI(model=&quot;gpt-3.5-turbo-1106&quot;, temperature=0)
agent = create_openai_tools_agent(llm, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools)

agent_executor.invoke({&quot;input&quot;: &quot;what is LangChain?&quot;})
</code></pre>
<pre><code>[chain/start] [1:chain:AgentExecutor] Entering Chain run with input:
{
  &quot;input&quot;: &quot;what is LangChain?&quot;
}
[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 3:chain:RunnableAssign&lt;agent_scratchpad&gt;] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 3:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 4:chain:RunnableParallel&lt;agent_scratchpad&gt;] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 3:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 4:chain:RunnableParallel&lt;agent_scratchpad&gt; &gt; 5:chain:RunnableLambda] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 3:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 4:chain:RunnableParallel&lt;agent_scratchpad&gt; &gt; 5:chain:RunnableLambda] [1ms] Exiting Chain run with output:
{
  &quot;output&quot;: []
}
[chain/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 3:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 4:chain:RunnableParallel&lt;agent_scratchpad&gt;] [2ms] Exiting Chain run with output:
{
  &quot;agent_scratchpad&quot;: []
}
[chain/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 3:chain:RunnableAssign&lt;agent_scratchpad&gt;] [3ms] Exiting Chain run with output:
{
  &quot;input&quot;: &quot;what is LangChain?&quot;,
  &quot;intermediate_steps&quot;: [],
  &quot;agent_scratchpad&quot;: []
}
[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 6:prompt:ChatPromptTemplate] Entering Prompt run with input:
{
  &quot;input&quot;: &quot;what is LangChain?&quot;,
  &quot;intermediate_steps&quot;: [],
  &quot;agent_scratchpad&quot;: []
}
[chain/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 6:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:
{
  &quot;lc&quot;: 1,
  &quot;type&quot;: &quot;constructor&quot;,
  &quot;id&quot;: [
    &quot;langchain&quot;,
    &quot;prompts&quot;,
    &quot;chat&quot;,
    &quot;ChatPromptValue&quot;
  ],
  &quot;kwargs&quot;: {
    &quot;messages&quot;: [
      {
        &quot;lc&quot;: 1,
        &quot;type&quot;: &quot;constructor&quot;,
        &quot;id&quot;: [
          &quot;langchain&quot;,
          &quot;schema&quot;,
          &quot;messages&quot;,
          &quot;SystemMessage&quot;
        ],
        &quot;kwargs&quot;: {
          &quot;content&quot;: &quot;You are a helpful assistant&quot;,
          &quot;additional_kwargs&quot;: {}
        }
      },
      {
        &quot;lc&quot;: 1,
        &quot;type&quot;: &quot;constructor&quot;,
        &quot;id&quot;: [
          &quot;langchain&quot;,
          &quot;schema&quot;,
          &quot;messages&quot;,
          &quot;HumanMessage&quot;
        ],
        &quot;kwargs&quot;: {
          &quot;content&quot;: &quot;what is LangChain?&quot;,
          &quot;additional_kwargs&quot;: {}
        }
      }
    ]
  }
}
[llm/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 7:llm:ChatOpenAI] Entering LLM run with input:
{
  &quot;prompts&quot;: [
    &quot;System: You are a helpful assistant\nHuman: what is LangChain?&quot;
  ]
}
[llm/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 7:llm:ChatOpenAI] [1.51s] Exiting LLM run with output:
{
  &quot;generations&quot;: [
    [
      {
        &quot;text&quot;: &quot;&quot;,
        &quot;generation_info&quot;: {
          &quot;finish_reason&quot;: &quot;tool_calls&quot;
        },
        &quot;type&quot;: &quot;ChatGenerationChunk&quot;,
        &quot;message&quot;: {
          &quot;lc&quot;: 1,
          &quot;type&quot;: &quot;constructor&quot;,
          &quot;id&quot;: [
            &quot;langchain&quot;,
            &quot;schema&quot;,
            &quot;messages&quot;,
            &quot;AIMessageChunk&quot;
          ],
          &quot;kwargs&quot;: {
            &quot;content&quot;: &quot;&quot;,
            &quot;example&quot;: false,
            &quot;additional_kwargs&quot;: {
              &quot;tool_calls&quot;: [
                {
                  &quot;index&quot;: 0,
                  &quot;id&quot;: &quot;call_PVNGpWSVfy921HhAQpaq54sY&quot;,
                  &quot;function&quot;: {
                    &quot;arguments&quot;: &quot;{\&quot;query\&quot;:\&quot;LangChain\&quot;}&quot;,
                    &quot;name&quot;: &quot;tavily_search_results_json&quot;
                  },
                  &quot;type&quot;: &quot;function&quot;
                }
              ]
            }
          }
        }
      }
    ]
  ],
  &quot;llm_output&quot;: null,
  &quot;run&quot;: null
}
[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 8:parser:OpenAIToolsAgentOutputParser] Entering Parser run with input:
[inputs]
[chain/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 8:parser:OpenAIToolsAgentOutputParser] [1ms] Exiting Parser run with output:
{
  &quot;output&quot;: [
    {
      &quot;lc&quot;: 1,
      &quot;type&quot;: &quot;constructor&quot;,
      &quot;id&quot;: [
        &quot;langchain&quot;,
        &quot;schema&quot;,
        &quot;agent&quot;,
        &quot;OpenAIToolAgentAction&quot;
      ],
      &quot;kwargs&quot;: {
        &quot;tool&quot;: &quot;tavily_search_results_json&quot;,
        &quot;tool_input&quot;: {
          &quot;query&quot;: &quot;LangChain&quot;
        },
        &quot;log&quot;: &quot;\nInvoking: `tavily_search_results_json` with `{'query': 'LangChain'}`\n\n\n&quot;,
        &quot;message_log&quot;: [
          {
            &quot;lc&quot;: 1,
            &quot;type&quot;: &quot;constructor&quot;,
            &quot;id&quot;: [
              &quot;langchain&quot;,
              &quot;schema&quot;,
              &quot;messages&quot;,
              &quot;AIMessageChunk&quot;
            ],
            &quot;kwargs&quot;: {
              &quot;content&quot;: &quot;&quot;,
              &quot;example&quot;: false,
              &quot;additional_kwargs&quot;: {
                &quot;tool_calls&quot;: [
                  {
                    &quot;index&quot;: 0,
                    &quot;id&quot;: &quot;call_PVNGpWSVfy921HhAQpaq54sY&quot;,
                    &quot;function&quot;: {
                      &quot;arguments&quot;: &quot;{\&quot;query\&quot;:\&quot;LangChain\&quot;}&quot;,
                      &quot;name&quot;: &quot;tavily_search_results_json&quot;
                    },
                    &quot;type&quot;: &quot;function&quot;
                  }
                ]
              }
            }
          }
        ],
        &quot;tool_call_id&quot;: &quot;call_PVNGpWSVfy921HhAQpaq54sY&quot;
      }
    }
  ]
}
[chain/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence] [1.53s] Exiting Chain run with output:
[outputs]
[tool/start] [1:chain:AgentExecutor &gt; 9:tool:tavily_search_results_json] Entering Tool run with input:
&quot;{'query': 'LangChain'}&quot;
[tool/end] [1:chain:AgentExecutor &gt; 9:tool:tavily_search_results_json] [1.06s] Exiting Tool run with output:
&quot;[{'url': 'https://github.com/langchain-ai/langchain', 'content': 'About\n⚡ Building applications with LLMs through composability ⚡\nResources\nLicense\nCode of conduct\nSecurity policy\nStars\nWatchers\nForks\nReleases\n291\nPackages\n0\nUsed by 39k\nContributors\n1,848\nLanguages\nFooter\nFooter navigation Latest commit\nGit stats\nFiles\nREADME.md\n🦜️🔗 LangChain\n⚡ Building applications with LLMs through composability ⚡\nLooking for the JS/TS library? ⚡ Building applications with LLMs through composability ⚡\nLicense\nlangchain-ai/langchain\nName already in use\nUse Git or checkout with SVN using the web URL.\n 📖 Documentation\nPlease see here for full documentation, which includes:\n💁 Contributing\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\n What can you build with LangChain?\n❓ Retrieval augmented generation\n💬 Analyzing structured data\n🤖 Chatbots\nAnd much more!'}]&quot;
[chain/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 11:chain:RunnableAssign&lt;agent_scratchpad&gt;] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 11:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 12:chain:RunnableParallel&lt;agent_scratchpad&gt;] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 11:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 12:chain:RunnableParallel&lt;agent_scratchpad&gt; &gt; 13:chain:RunnableLambda] Entering Chain run with input:
{
  &quot;input&quot;: &quot;&quot;
}
[chain/end] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 11:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 12:chain:RunnableParallel&lt;agent_scratchpad&gt; &gt; 13:chain:RunnableLambda] [1ms] Exiting Chain run with output:
[outputs]
[chain/end] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 11:chain:RunnableAssign&lt;agent_scratchpad&gt; &gt; 12:chain:RunnableParallel&lt;agent_scratchpad&gt;] [2ms] Exiting Chain run with output:
[outputs]
[chain/end] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 11:chain:RunnableAssign&lt;agent_scratchpad&gt;] [3ms] Exiting Chain run with output:
[outputs]
[chain/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 14:prompt:ChatPromptTemplate] Entering Prompt run with input:
[inputs]
[chain/end] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 14:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:
{
  &quot;lc&quot;: 1,
  &quot;type&quot;: &quot;constructor&quot;,
  &quot;id&quot;: [
    &quot;langchain&quot;,
    &quot;prompts&quot;,
    &quot;chat&quot;,
    &quot;ChatPromptValue&quot;
  ],
  &quot;kwargs&quot;: {
    &quot;messages&quot;: [
      {
        &quot;lc&quot;: 1,
        &quot;type&quot;: &quot;constructor&quot;,
        &quot;id&quot;: [
          &quot;langchain&quot;,
          &quot;schema&quot;,
          &quot;messages&quot;,
          &quot;SystemMessage&quot;
        ],
        &quot;kwargs&quot;: {
          &quot;content&quot;: &quot;You are a helpful assistant&quot;,
          &quot;additional_kwargs&quot;: {}
        }
      },
      {
        &quot;lc&quot;: 1,
        &quot;type&quot;: &quot;constructor&quot;,
        &quot;id&quot;: [
          &quot;langchain&quot;,
          &quot;schema&quot;,
          &quot;messages&quot;,
          &quot;HumanMessage&quot;
        ],
        &quot;kwargs&quot;: {
          &quot;content&quot;: &quot;what is LangChain?&quot;,
          &quot;additional_kwargs&quot;: {}
        }
      },
      {
        &quot;lc&quot;: 1,
        &quot;type&quot;: &quot;constructor&quot;,
        &quot;id&quot;: [
          &quot;langchain&quot;,
          &quot;schema&quot;,
          &quot;messages&quot;,
          &quot;AIMessageChunk&quot;
        ],
        &quot;kwargs&quot;: {
          &quot;content&quot;: &quot;&quot;,
          &quot;example&quot;: false,
          &quot;additional_kwargs&quot;: {
            &quot;tool_calls&quot;: [
              {
                &quot;index&quot;: 0,
                &quot;id&quot;: &quot;call_PVNGpWSVfy921HhAQpaq54sY&quot;,
                &quot;function&quot;: {
                  &quot;arguments&quot;: &quot;{\&quot;query\&quot;:\&quot;LangChain\&quot;}&quot;,
                  &quot;name&quot;: &quot;tavily_search_results_json&quot;
                },
                &quot;type&quot;: &quot;function&quot;
              }
            ]
          }
        }
      },
      {
        &quot;lc&quot;: 1,
        &quot;type&quot;: &quot;constructor&quot;,
        &quot;id&quot;: [
          &quot;langchain&quot;,
          &quot;schema&quot;,
          &quot;messages&quot;,
          &quot;ToolMessage&quot;
        ],
        &quot;kwargs&quot;: {
          &quot;content&quot;: &quot;[{\&quot;url\&quot;: \&quot;https://github.com/langchain-ai/langchain\&quot;, \&quot;content\&quot;: \&quot;About\\n⚡ Building applications with LLMs through composability ⚡\\nResources\\nLicense\\nCode of conduct\\nSecurity policy\\nStars\\nWatchers\\nForks\\nReleases\\n291\\nPackages\\n0\\nUsed by 39k\\nContributors\\n1,848\\nLanguages\\nFooter\\nFooter navigation Latest commit\\nGit stats\\nFiles\\nREADME.md\\n🦜️🔗 LangChain\\n⚡ Building applications with LLMs through composability ⚡\\nLooking for the JS/TS library? ⚡ Building applications with LLMs through composability ⚡\\nLicense\\nlangchain-ai/langchain\\nName already in use\\nUse Git or checkout with SVN using the web URL.\\n 📖 Documentation\\nPlease see here for full documentation, which includes:\\n💁 Contributing\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n What can you build with LangChain?\\n❓ Retrieval augmented generation\\n💬 Analyzing structured data\\n🤖 Chatbots\\nAnd much more!\&quot;}]&quot;,
          &quot;tool_call_id&quot;: &quot;call_PVNGpWSVfy921HhAQpaq54sY&quot;,
          &quot;additional_kwargs&quot;: {
            &quot;name&quot;: &quot;tavily_search_results_json&quot;
          }
        }
      }
    ]
  }
}
[llm/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 15:llm:ChatOpenAI] Entering LLM run with input:
{
  &quot;prompts&quot;: [
    &quot;System: You are a helpful assistant\nHuman: what is LangChain?\nAI: \nTool: [{\&quot;url\&quot;: \&quot;https://github.com/langchain-ai/langchain\&quot;, \&quot;content\&quot;: \&quot;About\\n⚡ Building applications with LLMs through composability ⚡\\nResources\\nLicense\\nCode of conduct\\nSecurity policy\\nStars\\nWatchers\\nForks\\nReleases\\n291\\nPackages\\n0\\nUsed by 39k\\nContributors\\n1,848\\nLanguages\\nFooter\\nFooter navigation Latest commit\\nGit stats\\nFiles\\nREADME.md\\n🦜️🔗 LangChain\\n⚡ Building applications with LLMs through composability ⚡\\nLooking for the JS/TS library? ⚡ Building applications with LLMs through composability ⚡\\nLicense\\nlangchain-ai/langchain\\nName already in use\\nUse Git or checkout with SVN using the web URL.\\n 📖 Documentation\\nPlease see here for full documentation, which includes:\\n💁 Contributing\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n What can you build with LangChain?\\n❓ Retrieval augmented generation\\n💬 Analyzing structured data\\n🤖 Chatbots\\nAnd much more!\&quot;}]&quot;
  ]
}
[llm/end] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 15:llm:ChatOpenAI] [23.21s] Exiting LLM run with output:
{
  &quot;generations&quot;: [
    [
      {
        &quot;text&quot;: &quot;LangChain is a platform for building applications with LLMs (Large Language Models) through composability. It allows for the creation of applications that utilize LLMs for tasks such as retrieval augmented generation, analyzing structured data, and building chatbots. LangChain is an open-source project with extensive documentation and is open to contributions from the community. You can find more information on LangChain at the following link: [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)&quot;,
        &quot;generation_info&quot;: {
          &quot;finish_reason&quot;: &quot;stop&quot;
        },
        &quot;type&quot;: &quot;ChatGenerationChunk&quot;,
        &quot;message&quot;: {
          &quot;lc&quot;: 1,
          &quot;type&quot;: &quot;constructor&quot;,
          &quot;id&quot;: [
            &quot;langchain&quot;,
            &quot;schema&quot;,
            &quot;messages&quot;,
            &quot;AIMessageChunk&quot;
          ],
          &quot;kwargs&quot;: {
            &quot;content&quot;: &quot;LangChain is a platform for building applications with LLMs (Large Language Models) through composability. It allows for the creation of applications that utilize LLMs for tasks such as retrieval augmented generation, analyzing structured data, and building chatbots. LangChain is an open-source project with extensive documentation and is open to contributions from the community. You can find more information on LangChain at the following link: [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)&quot;,
            &quot;example&quot;: false,
            &quot;additional_kwargs&quot;: {}
          }
        }
      }
    ]
  ],
  &quot;llm_output&quot;: null,
  &quot;run&quot;: null
}
[chain/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 16:parser:OpenAIToolsAgentOutputParser] Entering Parser run with input:
[inputs]
[chain/end] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 16:parser:OpenAIToolsAgentOutputParser] [1ms] Exiting Parser run with output:
{
  &quot;lc&quot;: 1,
  &quot;type&quot;: &quot;constructor&quot;,
  &quot;id&quot;: [
    &quot;langchain&quot;,
    &quot;schema&quot;,
    &quot;agent&quot;,
    &quot;AgentFinish&quot;
  ],
  &quot;kwargs&quot;: {
    &quot;return_values&quot;: {
      &quot;output&quot;: &quot;LangChain is a platform for building applications with LLMs (Large Language Models) through composability. It allows for the creation of applications that utilize LLMs for tasks such as retrieval augmented generation, analyzing structured data, and building chatbots. LangChain is an open-source project with extensive documentation and is open to contributions from the community. You can find more information on LangChain at the following link: [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)&quot;
    },
    &quot;log&quot;: &quot;LangChain is a platform for building applications with LLMs (Large Language Models) through composability. It allows for the creation of applications that utilize LLMs for tasks such as retrieval augmented generation, analyzing structured data, and building chatbots. LangChain is an open-source project with extensive documentation and is open to contributions from the community. You can find more information on LangChain at the following link: [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)&quot;
  }
}
[chain/end] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence] [23.22s] Exiting Chain run with output:
[outputs]
[chain/end] [1:chain:AgentExecutor] [26.22s] Exiting Chain run with output:
{
  &quot;output&quot;: &quot;LangChain is a platform for building applications with LLMs (Large Language Models) through composability. It allows for the creation of applications that utilize LLMs for tasks such as retrieval augmented generation, analyzing structured data, and building chatbots. LangChain is an open-source project with extensive documentation and is open to contributions from the community. You can find more information on LangChain at the following link: [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)&quot;
}
</code></pre>
<pre><code>{'input': 'what is LangChain?',
 'output': 'LangChain is a platform for building applications with LLMs (Large Language Models) through composability. It allows for the creation of applications that utilize LLMs for tasks such as retrieval augmented generation, analyzing structured data, and building chatbots. LangChain is an open-source project with extensive documentation and is open to contributions from the community. You can find more information on LangChain at the following link: [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)'}
</code></pre>
<p>The important parts are when it says &quot;Entering LLM run with input&quot;.</p>
<pre><code>[llm/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 7:llm:ChatOpenAI] Entering LLM run with input:
{
  &quot;prompts&quot;: [
    &quot;System: You are a helpful assistant\nHuman: what is LangChain?&quot;
  ]
}
[llm/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 7:llm:ChatOpenAI] [1.51s] Exiting LLM run with output:
{
  &quot;generations&quot;: [
    [
      {
        &quot;text&quot;: &quot;&quot;,
        &quot;generation_info&quot;: {
          &quot;finish_reason&quot;: &quot;tool_calls&quot;
        },
        &quot;type&quot;: &quot;ChatGenerationChunk&quot;,
        &quot;message&quot;: {
          &quot;lc&quot;: 1,
          &quot;type&quot;: &quot;constructor&quot;,
          &quot;id&quot;: [
            &quot;langchain&quot;,
            &quot;schema&quot;,
            &quot;messages&quot;,
            &quot;AIMessageChunk&quot;
          ],
          &quot;kwargs&quot;: {
            &quot;content&quot;: &quot;&quot;,
            &quot;example&quot;: false,
            &quot;additional_kwargs&quot;: {
              &quot;tool_calls&quot;: [
                {
                  &quot;index&quot;: 0,
                  &quot;id&quot;: &quot;call_PVNGpWSVfy921HhAQpaq54sY&quot;,
                  &quot;function&quot;: {
                    &quot;arguments&quot;: &quot;{\&quot;query\&quot;:\&quot;LangChain\&quot;}&quot;,
                    &quot;name&quot;: &quot;tavily_search_results_json&quot;
                  },
                  &quot;type&quot;: &quot;function&quot;
                }
              ]
            }
          }
        }
      }
    ]
  ],
  &quot;llm_output&quot;: null,
  &quot;run&quot;: null
}
</code></pre>
<p>I only see &quot;System:&quot; and &quot;Human:&quot; in the input, yet a tool is being called in the output.</p>
<pre><code>[llm/start] [1:chain:AgentExecutor &gt; 10:chain:RunnableSequence &gt; 15:llm:ChatOpenAI] Entering LLM run with input:
{
  &quot;prompts&quot;: [
    &quot;System: You are a helpful assistant\nHuman: what is LangChain?\nAI: \nTool: [{\&quot;url\&quot;: \&quot;https://github.com/langchain-ai/langchain\&quot;, \&quot;content\&quot;: \&quot;About\\n⚡ Building applications with LLMs through composability ⚡\\nResources\\nLicense\\nCode of conduct\\nSecurity policy\\nStars\\nWatchers\\nForks\\nReleases\\n291\\nPackages\\n0\\nUsed by 39k\\nContributors\\n1,848\\nLanguages\\nFooter\\nFooter navigation Latest commit\\nGit stats\\nFiles\\nREADME.md\\n🦜️🔗 LangChain\\n⚡ Building applications with LLMs through composability ⚡\\nLooking for the JS/TS library? ⚡ Building applications with LLMs through composability ⚡\\nLicense\\nlangchain-ai/langchain\\nName already in use\\nUse Git or checkout with SVN using the web URL.\\n 📖 Documentation\\nPlease see here for full documentation, which includes:\\n💁 Contributing\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n What can you build with LangChain?\\n❓ Retrieval augmented generation\\n💬 Analyzing structured data\\n🤖 Chatbots\\nAnd much more!\&quot;}]&quot;
  ]
}
</code></pre>
<p>Also, since the openai api works with an array of messages, is it a guarantee that they use the terms &quot;System:&quot;, &quot;Human:&quot;, &quot;AI:&quot; and &quot;Tool:&quot;?</p>
",Text Generation & LLMs,tool passed llm final prompt sent openai chat model using openai chat api send array message tool assembled single prompt tried setting see tool passed llm important part say entering llm run input see system human input yet tool called output also since openai api work array message guarantee use term system human ai tool
What and how LLM is used for ranking organization job title?,"<p>Suppose there's a context like this</p>
<p>context = <code>Andy is a vice manager of finance department. \n Rio is a general manager finance deparment. \n Jason is a general manager finance deparment.</code></p>
<p>question = <code>who is the leader of finance department ?</code></p>
<p>what task is this called ?
what model is used ?
how does the model know which title is higher ?
how does the model handle two same data ? (e.g., Rio and Jason)</p>
<p>thanks</p>
",Text Generation & LLMs,llm used ranking organization job title suppose context like context question task called model used doe model know title higher doe model handle two data e g rio jason thanks
Implementation (and working) differences between AutoModelForCausalLMWithValueHead vs AutoModelForCausalLM?,"<blockquote>
<p>Before any of you mark it as a &quot;Community Specific&quot; or something else, just <a href=""https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm"">look at this question</a> which you people so proudly have marked as <strong>Part of NLP Collective</strong>.</p>
</blockquote>
<p>I know what is <code>AutoModelForCausalLM</code>. The thing I'm asking is that in the <a href=""https://huggingface.co/docs/trl/v0.7.1/lora_tuning_peft"" rel=""nofollow noreferrer""><code>peft</code> LoRA Fine tuning tutorial</a>, the autors have used <code>AutoModelForCausalLMWithValueHead</code> while you pick any code or notebook on Fine-tuning of any LLM with <code>PEFT</code> style, you'll find <code>AutoModelForCausalLM</code> being used.</p>
<p>I went to lean on the <a href=""https://huggingface.co/docs/trl/models"" rel=""nofollow noreferrer"">official documentation of <code>AutoModelForCausalLMWithValueHead</code></a> and found:</p>
<blockquote>
<p>An autoregressive model with a value head in addition to the language model head</p>
</blockquote>
<p>What I want to ask is that <strong>How, where and more importantly, <em>WHY</em> this extra <code>ValueHead</code> is used</strong></p>
<p>In case you don't know the answer, you try to upvote the question rather than trying to close it, please. Thank you :)</p>
",Text Generation & LLMs,implementation working difference automodelforcausallmwithvaluehead v automodelforcausallm mark community specific something else lora fine tuning tutorial autors used pick code notebook fine tuning llm style find used went lean official documentation found autoregressive model value head addition language model head want ask importantly extra used case know answer try upvote question rather trying close please thank
Nan output after masked TransforrmerDecoder,"<p>What is wrong with my mask, or why it doesn't work?
I try to do predict mask token based only on the first token and masked token.
For this i created special create_casual_mask method that creates this mask for multihead attention. When i run it, nan tensor is returned. Pad mask and mask of masked tokens are not intersected, i full my attention mask as it is described in torch documentations for torch.nn.TransformerDecoder. Also output of attention shouldn't be empty because there are some False values in attention mask.
So why it doesn't work?</p>
<pre><code>import torch
from torch import nn

torch.manual_seed(0)


class LookOnFirstDecoder(nn.Module):
    def __init__(self, depth, d_model, nhead, d_ff,
                 dropout, activation,
                sent_length, n_tokens, pad_idx
    ):
        super().__init__()
        &quot;&quot;&quot;
        :param sent_length: max length of sentence
        :param n_tokens: number of tokens to use including mask and padding tokens
        :param pad_idx: index of padding to don't compute the gradient
        &quot;&quot;&quot;
        self.d_model = d_model
        self.nhead = nhead
        self.n_tokens = n_tokens
        self.emb = nn.Embedding(
            num_embeddings=n_tokens,
            embedding_dim=d_model,
            padding_idx=pad_idx
        )

        self.pos_embed = nn.Parameter(
            torch.zeros(1, sent_length, d_model),
            requires_grad=True
        )
        torch.nn.init.normal_(self.pos_embed, std=.02)

        self.transformer = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=d_ff,
                dropout=dropout,
                activation=activation,
                batch_first=True,
                norm_first=True,
            ),
            num_layers=depth,
        )

        self.fin_lin = nn.Linear(d_model, n_tokens)

    def create_causal_mask(self, mask):
        &quot;&quot;&quot;
            The purpose is to create mask that allows all not first tokens
        look only on the first token and itself
        :param mask: (B, L)
        :return: (B * nhead, L)
        &quot;&quot;&quot;

        mask[:, 0] = True  # to depend on first token
        b, l = mask.shape
        batch_causal_mask = ~torch.tril(mask.unsqueeze(-1) * mask.unsqueeze(-2))  # (B, L, L)
        # batch_causal_mask = torch.tril(torch.ones((b, l, l))).to(&quot;cuda&quot;) == 0

        # batch_causal_mask = torch.where(batch_causal_mask, 0, float('-inf'))
        print(f&quot;Batch causal mask: \n{batch_causal_mask}&quot;)

        causal_mask = (
            batch_causal_mask.
            unsqueeze(1).  # (B, 1, L, L)
            expand(b, self.nhead, l, l).  # (B, nhead, L, L)
            reshape(b * self.nhead, l, l)  # (B * nhead, L, L)
        )

        return causal_mask

    def forward(self, tgt, memory, is_masked_mask, is_pad_mask):
        &quot;&quot;&quot;
        :param tgt: (B, L)
        :param memory: (B, L1, D)
        :param is_masked_mask: (B, L) - True - mask token, False - not
        :param is_pad_mask: (B, L), True - pad token, False - not
        :return: tensor of shape (B, n_tokens)
        &quot;&quot;&quot;
        b, l = tgt.shape
        tgt_tokens = self.emb(tgt) + self.pos_embed[:, :l].expand(b, l, self.d_model)

        tgt_tokens = self.transformer(
            tgt_tokens,
            memory,
            tgt_mask=self.create_causal_mask(is_masked_mask.clone()),
            tgt_is_causal=True,
            tgt_key_padding_mask=is_pad_mask
        )  # (B, L, D)

        fin_tokens = self.fin_lin(tgt_tokens[is_masked_mask])
        return fin_tokens


# my vocabulary
n_tokens = 10  # pad_idx - 9, mask_idx - 8
pad_idx = n_tokens - 1
mask_idx = n_tokens - 2

d_model = 4
nhead = 2
b, l = 3, 8

model = LookOnFirstDecoder(
    depth=2,
    d_model=4,
    nhead=2,
    d_ff=8,
    dropout=0.1,
    activation=&quot;gelu&quot;,
    sent_length=l,
    n_tokens=n_tokens,
    pad_idx=pad_idx
)

memory = torch.randn(b, l, d_model)

# so i create some random tokens, without padding and mask
in_tokens = torch.randint(0, mask_idx - 1, (b, l))

# mask and paddings add manually
in_tokens[0, 6:] = pad_idx
in_tokens[0, 5] = mask_idx

in_tokens[1, 7:] = pad_idx
in_tokens[1, 4] = mask_idx

in_tokens[2, 5:] = pad_idx
in_tokens[2, 0] = mask_idx

is_masked_mask = in_tokens == mask_idx
is_pad_mask = in_tokens == pad_idx

pred = model(in_tokens, memory, is_masked_mask, in_tokens == pad_idx)

print(f&quot;In tokens: \n{in_tokens}&quot;)
print(f&quot;Pad mask: \n{is_pad_mask}&quot;)
print(f&quot;Masked mask: \n{is_masked_mask}&quot;)
print(f&quot;Pred: \n{pred}&quot;)

</code></pre>
<p>these is my requirements.txt</p>
<pre><code>torch == 2.1.1
torchvision == 0.16.1
xformers
albumentations==1.3.1

numpy == 1.26.2
scipy == 1.11.4
scikit-learn == 1.3.2
pandas == 2.1.4
matplotlib == 3.8.2
seaborn == 0.13.0
</code></pre>
<p>That is my result after execution:</p>
<pre><code>Batch causal mask: 
tensor([[[False,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True, False,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True]],

        [[False,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True, False,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True]],

        [[False,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True]]])
In tokens: 
tensor([[2, 1, 4, 0, 3, 8, 9, 9],
        [6, 4, 0, 6, 8, 0, 5, 9],
        [8, 2, 5, 2, 6, 9, 9, 9]])
Pad mask: 
tensor([[False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False,  True,  True,  True]])
Masked mask: 
tensor([[False, False, False, False, False,  True, False, False],
        [False, False, False, False,  True, False, False, False],
        [ True, False, False, False, False, False, False, False]])
Pred: 
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
       grad_fn=&lt;AddmmBackward0&gt;)
</code></pre>
",Text Generation & LLMs,nan output masked transforrmerdecoder wrong mask work try predict mask token based first token masked token created special create casual mask method creates mask multihead attention run nan tensor returned pad mask mask masked token intersected full attention mask described torch documentation torch nn transformerdecoder also output attention empty false value attention mask work requirement txt result execution
Training a new dataset on bert,"<p>I have a amazon review dataset, where I want to predict the star rating based on the review</p>
<p>I know I can use a pretrained bert model as shown <a href=""https://github.com/nicknochnack/BERTSentiment/blob/main/Sentiment.ipynb"" rel=""nofollow noreferrer"">here</a></p>
<p>But I want to train the bert model on my own dataset. Is that whats being done <a href=""https://medium.com/analytics-vidhya/fine-tuning-bert-for-amazon-food-reviews-32e474de0e51"" rel=""nofollow noreferrer"">here</a>? And can I apply this type of 'fine tuning' on a pretrained model with any dataset to get more accurate results or do I have to do something else to train the model from scratch</p>
<p>And if I do want to train a model from scratch, where would I start</p>
",Text Generation & LLMs,training new dataset bert amazon review dataset want predict star rating based review know use pretrained bert model shown want train bert model dataset whats done apply type fine tuning pretrained model dataset get accurate result something else train model scratch want train model scratch would start
Can you train a BERT model from scratch with task specific architecture?,"<p>BERT pre-training of the base-model is done by a language modeling approach, where we mask certain percent of tokens in a sentence, and we make the model learn those missing mask. Then, I think in order to do downstream tasks, we add a newly initialized layer and we fine-tune the model.</p>
<p>However, suppose we have a gigantic dataset for sentence classification. Theoretically, can we initialize the BERT base architecture from scratch, train both the additional downstream task specific layer + the base model weights form scratch with this sentence classification dataset only, and still achieve a good result?</p>
",Text Generation & LLMs,train bert model scratch task specific architecture bert pre training base model done language modeling approach mask certain percent token sentence make model learn missing mask think order downstream task add newly initialized layer fine tune model however suppose gigantic dataset sentence classification theoretically initialize bert base architecture scratch train additional downstream task specific layer base model weight form scratch sentence classification dataset still achieve good result
Getting PermissionError while saving BERT model checkpoint,"<p>I am finetuning a BERT model for classification task,
Following code is used for the training the model.</p>
<pre><code>from transformers import Trainer, TrainingArguments, AutoConfig
from transformers import Trainer

batch_size = 8
logging_steps = len(emotions_encoded['train']) // batch_size
print(len(emotions_encoded['train']))
print(logging_steps)

model_name = f&quot;BERT-emotion-classification&quot;

# Create a configuration object
config = AutoConfig.from_pretrained(model_ckpt, output_hidden_states=True)

# Save the configuration to a JSON file
config.to_json_file(f&quot;{model_name}/config.json&quot;)

training_args = TrainingArguments(output_dir=model_name,
                                        num_train_epochs=2,
                                        learning_rate=2e-5,
                                        per_device_train_batch_size=batch_size,
                                        per_device_eval_batch_size=batch_size,
                                        weight_decay=0.01,
                                        evaluation_strategy=&quot;epoch&quot;,
                                        disable_tqdm=False,
                                        logging_steps=logging_steps,
                                        push_to_hub=True,
                                        log_level=&quot;error&quot;)


trainer = Trainer(model=model, args=training_args,
                                compute_metrics=compute_metrics,
                                train_dataset=emotions_encoded[&quot;train&quot;],
                                eval_dataset=emotions_encoded[&quot;validation&quot;],
                                tokenizer=tokenizer)
trainer.train();

# Save the model using Trainer's save_model method
trainer.save_model(f&quot;./{model_name}&quot;)
</code></pre>
<p>Train runs fine till the first checkpoint is created and then PermissionError is thrown.</p>
<pre><code>{
    &quot;name&quot;: &quot;PermissionError&quot;,
    &quot;message&quot;: &quot;[Errno 13] Permission denied: 'BERT-emotion-classification\\\\checkpoint-500'&quot;,
    &quot;stack&quot;: &quot;---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
Cell In[34], line 7
      1 from transformers import Trainer
      2 trainer = Trainer(model=model, args=training_args,
      3                                 compute_metrics=compute_metrics,
      4                                 train_dataset=emotions_encoded[\&quot;train\&quot;],
      5                                 eval_dataset=emotions_encoded[\&quot;validation\&quot;],
      6                                 tokenizer=tokenizer)
----&gt; 7 trainer.train();
      9 # Save the model using Trainer's save_model method
     10 trainer.save_model(f\&quot;./{model_name}\&quot;)

PermissionError: [Errno 13] Permission denied: 'BERT-emotion-classification\\\\checkpoint-500'&quot;
}
</code></pre>
<p>I checked the folder has the all the permission.</p>
",Text Generation & LLMs,getting permissionerror saving bert model checkpoint finetuning bert model classification task following code used training model train run fine till first checkpoint created permissionerror thrown checked folder ha permission
LLM model short in producing a successful query to satisfy the intent:,"<p>I've compiled a small list of examples where the current LLM falls short in producing a successful query to satisfy the intent:</p>
<p>`intent: Is there a pattern in the query parameters that corresponds to higher latencies?
model: openai/gpt-3.5-turbo
regex_clean_query: '</p>
<pre><code>df_filtered = df[df[''QueryParams''].notnull()]


df_filtered[''Latency_ms''] = df_filtered[''Latency_ms''].astype(float)

df_filtered[''QueryParams''] = df_filtered[''QueryParams''].apply(eval)


df_grouped = df_filtered.groupby(''QueryParams'')[''Latency_ms''].mean().reset_index()


df_sorted = df_grouped.sort_values(''Latency_ms'', ascending=False)
</code></pre>
<p>intent: Can we identify any trends in service usage over the given time period?
model: openai/gpt-3.5-turbo
regex_clean_query: '# Filter the data frame based on the given time period</p>
<pre><code>start_date = ''2021-01-01''

end_date = ''2021-12-31''

df_filtered = df[(df[''Timestamp''] &gt;= start_date) &amp; (df[''Timestamp''] &lt;= end_date)]


service_usage = df_filtered.groupby(''ServiceName'').size().reset_index(name=''UsageCount'')


service_usage = service_usage.sort_values(by=''UsageCount'', ascending=False)
</code></pre>
<p>intent: Which endpoint has the most varied latency times?
model: openai/gpt-3.5-turbo
regex_clean_query: '# Filter the dataframe to include only the required columns</p>
<pre><code>filtered_df = df[[''Endpoint'', ''Latency_ms'']]


grouped_df = filtered_df.groupby(''Endpoint'')[''Latency_ms''].nunique()


endpoint_with_most_varied_latency = grouped_df.idxmax()
</code></pre>
<p>`</p>
",Text Generation & LLMs,llm model short producing successful query satisfy intent compiled small list example current llm fall short producing successful query satisfy intent
Truncate texts in the middle for Bert,"<p>I am learning about Bert, which only deals with texts with fewer than 512 tokens, and came across this <a href=""https://stackoverflow.com/a/59778726"">answer</a> which says that truncating text in the middle (as opposed to at the start or at the end) may work well for Bert. I wonder whether there is any library to do that type of truncation because as far as I understand, one word can consist of multiple Bert token so I cannot simply get the middle 512 words. Thanks in advance</p>
",Text Generation & LLMs,truncate text middle bert learning bert deal text fewer token came across href say truncating text middle opposed start end may work well bert wonder whether library type truncation far understand one word consist multiple bert token simply get middle word thanks advance p
NLP - make summarization from each subtitle,"<p>I am very new to NLP. I'm trying to build simple text summarization model where it takes 1-2 important sentence from each subtitle in an article. For example, in the image I want take 1 sentence from &quot;Urge to clear throat&quot;, &quot;Viruses&quot;, and &quot;Smoking&quot; sections, and then combine them to make a summarization of the article. Can someone recommend me the best way to do that? Thank you in advance.</p>
<p><a href=""https://i.sstatic.net/ArU9g.png"" rel=""nofollow noreferrer"">Article example</a></p>
<p>So far I only know how to make text summarization using BERT, but it didn't summarize from each sub-title.</p>
",Text Generation & LLMs,nlp make summarization subtitle new nlp trying build simple text summarization model take important sentence subtitle article example image want take sentence urge clear throat virus smoking section combine make summarization article someone recommend best way thank advance article example far know make text summarization using bert summarize sub title
What is sent to the llm when using a chat model?,"<p>I am confused by how multiple messages are combined and sent to a large language model such as <code>ChatOpenAI</code>.</p>
<pre><code>from langchain_core.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages([
    (&quot;system&quot;, &quot;You are a helpful AI bot. Your name is {name}.&quot;),
    (&quot;human&quot;, &quot;Hello, how are you doing?&quot;),
    (&quot;ai&quot;, &quot;I'm doing well, thanks!&quot;),
    (&quot;human&quot;, &quot;{user_input}&quot;),
])

messages = template.format_messages(
    name=&quot;Bob&quot;,
    user_input=&quot;What is your name?&quot;
)

messages
</code></pre>
<pre><code>[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),
 HumanMessage(content='Hello, how are you doing?'),
 AIMessage(content=&quot;I'm doing well, thanks!&quot;),
 HumanMessage(content='What is your name?')]
</code></pre>
<p>Is it generating text that looks like this:</p>
<pre><code>System:
Human:
Assistant:
Human:
...
</code></pre>
<p>How can I print the final text sent to the llm?</p>
",Text Generation & LLMs,sent llm using chat model confused multiple message combined sent large language model generating text look like print final text sent llm
Fine-tuning a BERT model without answers,"<p>I come here after I have been googling during many hours for to know if it's was possible to make a fine-tune <code>BERT</code> Question Answering with only question and context ? I am beginner with BERT model so I don't know the deep mechanism that it have even after many search. Thanks you for your answer</p>
",Text Generation & LLMs,fine tuning bert model without answer come googling many hour know wa possible make fine tune question answering question context beginner bert model know deep mechanism even many search thanks answer
How to locally load a finetuned LLAMA2 model that currently saved to my local disk in safe tensor format,"<p>I am new to working with LLMs. I have a finetuned LLAMA2 model in safe tensor format saved to my local disk. I want to load my model locally by running a python notebook.</p>
<p>I have tried googling, and I find many examples on loading Stable diffusion models but noting much on loading LLAMA models.</p>
",Text Generation & LLMs,locally load finetuned llama model currently saved local disk safe tensor format new working llm finetuned llama model safe tensor format saved local disk want load model locally running python notebook tried googling find many example loading stable diffusion model noting much loading llama model
Latent Dirichlet allocation (LDA) with a large number of short texts and a small number of long texts,"<p>I am trying to generate comparable topics from a large number of short texts and a relatively small number of long texts. Currently, I mix them into one corpus to run LDA. I found that most topics are driven by short texts given that the number of short texts is much larger than the number of long documents. I have 2 questions:
(1) Can anyone help explain this issue (i.e., the dominance of topics from short texts) theoretically?
(2) Is there a better way to deal with long texts and short texts at the same time? I am thinking of using short texts as a training sample and long texts as the testing sample. Is it theoretically correct?</p>
",Text Generation & LLMs,latent dirichlet allocation lda large number short text small number long text trying generate comparable topic large number short text relatively small number long text currently mix one corpus run lda found topic driven short text given number short text much larger number long document question anyone help explain issue e dominance topic short text theoretically better way deal long text short text time thinking using short text training sample long text testing sample theoretically correct
How to install a language model,"<p>I am exploring using NLP for some machine learning projects. I normally code all of my projects using python through Anaconda using either Jupyter notebooks or PyCharm as my IDE. </p>

<p>I would like to start using spacy and am planning on attending a workshop on it in the near future. Two recommendations were made that I do first. Install spacy and install the <code>en_core_web_lg</code> language model. I completed the first step, just by searching for the spacy package in Anaconda environments (the conventional way) and installed it. However, as far as installing the language model, I am less familiar with how to do this to get this on my computer since it is not a traditional package.</p>

<p>The spacy installation website cites here: <a href=""https://spacy.io/models/en#en_core_web_lg"" rel=""noreferrer"">https://spacy.io/models/en#en_core_web_lg</a> that this language model can be installed by using:</p>

<pre><code>INSTALLATION

$ python -m spacy download en_core_web_lg
</code></pre>

<p>I am assuming that this is a command through terminal? I am not very experienced using terminal but tried typing in the above command in one of the command lines and pressed enter and nothing happened. Is this the correct way to install this model? How should I install it? Also, for pedagogical purposes, what exactly is happening when we install the model? It exists on our computer and then can be utilized for NLP in say a Jupyter notebook if called. </p>

<p>Sorry if these questions seem fairly basic, I am still trying to learn these new techniques. Any help, references, or advice would be greatly appreciated.</p>

<p>Thanks.</p>
",Text Generation & LLMs,install language model exploring using nlp machine learning project normally code project using python anaconda using either jupyter notebook pycharm ide would like start using spacy planning attending workshop near future two recommendation made first install spacy install language model completed first step searching spacy package anaconda environment conventional way installed however far installing language model le familiar get computer since traditional package spacy installation website cite language model installed using assuming command terminal experienced using terminal tried typing command one command line pressed enter nothing happened correct way install model install also pedagogical purpose exactly happening install model exists computer utilized nlp say jupyter notebook called sorry question seem fairly basic still trying learn new technique help reference advice would greatly appreciated thanks
How bert is a bidirectional?,"<p>Bert encoder takes the input and goes for the multi-head attention model. But how do they maintain sequence? Since current words don't take sequence of previous words. Besides, why is it bidirectional? Does it maintain forward and backward sequence like LSTM?</p>
",Text Generation & LLMs,bert bidirectional bert encoder take input go multi head attention model maintain sequence since current word take sequence previous word besides bidirectional doe maintain forward backward sequence like lstm
LLM slow inference even on A100 GPU,"<p>I am planning to deploy a fine-tuned version of Open-Orca-Platypus-2. It takes around 13.5GB on the GPU. I tried using g4dn.12xlarge in AWS which has 4 GPUs, but the inference still takes around 40 seconds. I also tried it on A100 GPU provided by Colab, but still the same.</p>
<p>What am I doing wrong? Do I still need more computational power or is anything wrong with my code?</p>
<pre><code>
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    import os

    os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,1,2,3&quot;
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map=&quot;auto&quot;
    )

    # Set the model to evaluation mode
    model.eval()

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(&quot;Open-Orca/OpenOrca-Platypus2-13B&quot;, trust_remote_code=True)

    def ask_bot(question):
        with torch.no_grad():
            # Tokenize input question
            input_ids = tokenizer.encode(question, return_tensors=&quot;pt&quot;).cuda()

            # Generate output
            output = model.module.generate(
                input_ids,
                max_length=200,
                num_return_sequences=1,
                do_sample=True,
                top_k=50
            )

        # Decode and extract the response
        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        response = generated_text.split(&quot;-&gt;:&quot;)[-1]
        return response
</code></pre>
",Text Generation & LLMs,llm slow inference even gpu planning deploy fine tuned version open orca platypus take around gb gpu tried using g dn xlarge aws ha gpus inference still take around second also tried gpu provided colab still wrong still need computational power anything wrong code
Bert topic clasiffying over a quarter of documents in outlier topic -1,"<p>I am running Bert topic with default options</p>
<pre><code>import pandas as pd
from sentence_transformers import SentenceTransformer
import time
import pickle
from bertopic import BERTopic

llm_mod =  &quot;all-MiniLM-L6-v2&quot;
model = SentenceTransformer(llm_mod)

embeddings = model.encode(skills_augmented, show_progress_bar=True)
bertopic_model = BERTopic(verbose=True)
</code></pre>
<p>I have a dataset of 40,000 documents that are only one short sentence. 13,573 of the documents get placed in the -1 topic (below distribution across top 5 topics).</p>
<pre><code>-1      13573
 0       1593
 1       1043
 2        628
 3        627
</code></pre>
<p>From the documentation: The -1 refers to all outliers and should typically be ignored. Is there a parameter I can use to get less documents in -1? Perhaps get a more even distribution across topics? Would running kmeans be better?</p>
",Text Generation & LLMs,bert topic clasiffying quarter document outlier topic running bert topic default option dataset document one short sentence document get placed topic distribution across top topic documentation refers outlier typically ignored parameter use get le document perhaps get even distribution across topic would running kmeans better
Understanding output_attentions,"<p>I have a question about <code>output_attentions</code>, and I need to make a heatmap about the attention of the final layer of the BERT model. But I do not know if the <code>output_attentions[0]</code> is the first or last layer. I tried to check the documentation, but I did not find it.</p>
",Text Generation & LLMs,understanding output attention question need make heatmap attention final layer bert model know first last layer tried check documentation find
BERT neglects dependency between the masked positions,"<p>Reading <a href=""https://arxiv.org/pdf/1906.08237.pdf"" rel=""nofollow noreferrer"">XLNET</a> paper, It was mentioned that:</p>
<blockquote>
<p>However, relying on corrupting the input with masks, BERT neglects
dependency between the masked positions and suffers from a
pretrain-finetune discrepancy.</p>
</blockquote>
<p>we all know that transformers are using positional encoding to relate tokens in the sequences and referring to the BERT <a href=""https://github.com/huggingface/transformers/blob/514de24abfd4416aeba6a6455ad5920f57f3567d/src/transformers/models/bert/modeling_bert.py#L1304"" rel=""nofollow noreferrer"">code</a>    BertModel   is using   BertEmbeddings   which used absolute positional encoding layer. My question is Why Bert after using positional encoding  neglects dependency between the masked positions? The explanation in the paper is not clear to me.</p>
<p>It was explained in the   Background subsection page-2 from the paper , but nothing mentioned about positional encoding between tokens in BERT.</p>
<p>If the positional encoding is not the only cosideration for tokens dependency so what are the other considerations?</p>
",Text Generation & LLMs,bert neglect dependency masked position reading xlnet paper wa mentioned however relying corrupting input mask bert neglect dependency masked position suffers pretrain finetune discrepancy know transformer using positional encoding relate token sequence referring bert code bertmodel using bertembeddings used absolute positional encoding layer question bert using positional encoding neglect dependency masked position explanation paper clear wa explained background subsection page paper nothing mentioned positional encoding token bert positional encoding cosideration token dependency consideration
How to train ChatGPT on custom data efficiently?,"<p>I am working with a dataset (<strong>csv</strong> format) and creating a custom trained chatbot using the ChatGPT API in Python. Approximately there are 1000 observations and 12 variables. I was able to train the model, however when using asking questions, the chatbot does not give the required results. For example when I ask &quot;What is the average age of the employees?&quot; the result that I get is 15.5, which is incorrect (should be around 40). An other example, &quot;How many males are there in the dataset?&quot;, the output is 60, however there are 340 males in the dataset.</p>
<p>I am quite sure that this has to do something with preprocessing the data, but I could not work my way around it. My other is to convert it to <strong>json</strong> format, from which the model would be able learn more accurately.</p>
<p>Has anyone else met with this issue? Did anyone else met with this issue? How did you manage to solve it?</p>
",Text Generation & LLMs,train chatgpt custom data efficiently working dataset csv format creating custom trained chatbot using chatgpt api python approximately observation variable wa able train model however using asking question chatbot doe give required result example ask average age employee result get incorrect around example many male dataset output however male dataset quite sure ha something preprocessing data could work way around convert json format model would able learn accurately ha anyone else met issue anyone else met issue manage solve
How do I make a paraphrase generation using BERT/ GPT-2,"<p>I am trying hard to understand how to make a paraphrase generation using BERT/GPT-2. I cannot understand how do I make it. Could you please provide me with any resources where I will be able to make a paraphrase generation model?
<strong>&quot;The input would be a sentence and the output would be a paraphrase of the sentence&quot;</strong></p>
",Text Generation & LLMs,make paraphrase generation using bert gpt trying hard understand make paraphrase generation using bert gpt understand make could please provide resource able make paraphrase generation model input would sentence output would paraphrase sentence
Summarizing n-grams efficiently in Python on big data,"<p>I have a very large dataset of roughly 6 million records, it does look like this snippet:</p>
<pre><code>data = pd.DataFrame({
    'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],
    'TEXT': [
        &quot;Mouthwatering BBQ ribs cheese, and coleslaw.&quot;,
        &quot;Delicious pizza with pepperoni and extra cheese.&quot;,
        &quot;Spicy Thai curry with cheese and jasmine rice.&quot;,
        &quot;Tiramisu dessert topped with cocoa powder.&quot;,
        &quot;Sushi rolls with fresh fish and soy sauce.&quot;,
        &quot;Freshly baked chocolate chip cookies.&quot;,
        &quot;Homemade lasagna with layers of cheese and pasta.&quot;,
        &quot;Gourmet burgers with all the toppings and extra cheese.&quot;,
        &quot;Crispy fried chicken with mashed potatoes and extra cheese.&quot;,
        &quot;Creamy tomato soup with a grilled cheese sandwich.&quot;
    ],
    'DATE': [
        '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02',
        '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02'
    ]
})
</code></pre>
<p>I want to generate bigrams and trigrams from the column 'TEXT.' I'm interested in two types of ngrams for both trigrams and bigrams: those that start with 'extra' and those that don't start with 'extra.' Once we have those, I want to summarize (count the unique ID frequency) of those ngrams by unique 'DATE.' This means that if an ngram appears in an ID more than once, I will count it only once because I want to know in how many different 'IDs' it ultimately appeared.</p>
<p>I'm very new to Python. I come from the R world, in which there is a library called quanteda that uses C programming and parallel computing. Searching for those ngrams looks something like this:</p>
<pre><code>corpus_food %&gt;%
  tokens(remove_punct = TRUE) %&gt;% 
  tokens_ngrams(n = 2) %&gt;% 
  tokens_select(pattern = &quot;^extra&quot;, valuetype = &quot;regex&quot;) %&gt;%
  dfm() %&gt;%
  dfm_group(groups = lubridate::date(DATE)) %&gt;%
  textstat_frequency()
</code></pre>
<p>yielding my desired results:</p>
<pre><code>       feature frequency rank docfreq group
1 extra_cheese         3    1       2   all
</code></pre>
<p>My desired result would look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ngram</th>
<th>nunique</th>
<th>group</th>
</tr>
</thead>
<tbody>
<tr>
<td>cheese and</td>
<td>3</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>and extra</td>
<td>2</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>extra cheese</td>
<td>2</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>and extra cheese</td>
<td>2</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>mouthwatering bbq</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>bbq ribs</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>ribs cheese</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>and coleslaw</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>mouthwatering bbq ribs</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>bbq ribs cheese</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>ribs cheese and</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>cheese and coleslaw</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>delicious pizza</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pizza with</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>with pepperoni</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pepperoni and</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>delicious pizza with</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pizza with pepperoni</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>with pepperoni and</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pepperoni and extra</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>spicy thai</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>thai curry</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
</tbody>
</table>
</div>
<p>I am in no way comparing the two languages, Python and R. They are amazing, but at the moment, I'm interested in a very straightforward and fast method to achieve my results in Python. I am open to hearing of a way to achieve what I'm looking for in a faster and more efficient way in Python. I'm new to Python.</p>
<p>So far I have found a way to create the bigrams and trigrams but I have no idea as to how perform the selection of those that start with &quot;extra&quot; and those who don't and this very process of creating the ngrams is taking over an hour so I will take all advice on how to reduce the time.</p>
<p>Work around:</p>
<pre><code>import nltk
from nltk import bigrams
from nltk.util import trigrams 
from nltk.tokenize import word_tokenize

data['bigrams'] = data['TEXT'].apply(lambda x: list(bigrams(word_tokenize(x))))
data['trigrams'] = data['TEXT'].apply(lambda x: list(trigrams(word_tokenize(x))))
</code></pre>
<p>Reading through some posts, some people suggest on using the gensim lib. Would that be a good direction?</p>
",Text Generation & LLMs,summarizing n gram efficiently python big data large dataset roughly million record doe look like snippet want generate bigram trigram column text interested two type ngrams trigram bigram start extra start extra want summarize count unique id frequency ngrams unique date mean ngram appears id count want know many different id ultimately appeared new python come r world library called quanteda us c programming parallel computing searching ngrams look something like yielding desired result desired result would look like ngram nunique group cheese extra extra cheese extra cheese mouthwatering bbq bbq rib rib cheese coleslaw mouthwatering bbq rib bbq rib cheese rib cheese cheese coleslaw delicious pizza pizza pepperoni pepperoni delicious pizza pizza pepperoni pepperoni pepperoni extra spicy thai thai curry way comparing two language python r amazing moment interested straightforward fast method achieve result python open hearing way achieve looking faster efficient way python new python far found way create bigram trigram idea perform selection start extra process creating ngrams taking hour take advice reduce time work around reading post people suggest using gensim lib would good direction
How can I get RoBERTa word embeddings?,"<p>Given a sentence of the type 'Roberta is a heavily optimized version of BERT.', I need to get the embeddings for each of the words in this sentence with RoBERTa. I have tried to look at the sample codes online, failing to find a definite answer. </p>

<p>My take is the following:</p>

<pre><code>tokens = roberta.encode(headline)
all_layers = roberta.extract_features(tokens, return_all_hiddens=True)
embedding = all_layers[0]
n = embedding.size()[1] - 1
embedding = embedding[:,1:n,:]
</code></pre>

<p>where <code>embedding[:,1:n,:]</code> is used to extract only the embeddings for the words in the sentence, without the start and end tokens. </p>

<p>Is it correct?</p>
",Text Generation & LLMs,get roberta word embeddings given sentence type roberta heavily optimized version bert need get embeddings word sentence roberta tried look sample code online failing find definite answer take following used extract embeddings word sentence without start end token correct
Codegen causal ML inference too short compared to results in model card API,"<p>I'm experimenting with the Salesforce/codegen-350M-mono model.</p>
<p>When I generate text using a prompt, the result on my machine is much shorter than what I get inputting the same prompt in the API window of the model card.  On my machine it just adds 'return'</p>
<pre><code>import datasets
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = &quot;Salesforce/codegen-350M-mono&quot;
device = &quot;cpu&quot;

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)

text = &quot;&quot;&quot;def round_to_multiple(num, mult):
    \&quot;&quot;&quot;Rounds to nearest multiple of another number.\&quot;&quot;&quot;
    &quot;&quot;&quot;
input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids

generated_ids = model.generate(input_ids)
print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))
</code></pre>
<p>Here is the output</p>
<pre><code>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
def round_to_multiple(num, mult):
    &quot;&quot;&quot;Rounds to nearest multiple of another number.&quot;&quot;&quot;
    return
</code></pre>
<p>When I enter the same prompt in the <a href=""https://huggingface.co/Salesforce/codegen-350M-mono?text=def%20round_to_multiple%28num%2C%20mult%29%3A%0A%20%20%20%20%5C%22%22%22Rounds%20to%20nearest%20multiple%20of%20another%20number.%5C%22%22%22"" rel=""nofollow noreferrer"">model card</a> code-generation window, I get the following:</p>
<pre><code>def round_to_multiple(num, mult):
    \&quot;&quot;&quot;Rounds to nearest multiple of another number.\&quot;&quot;&quot;
    if (num % mult == 0):
        return int(num)
    else:
        return
</code></pre>
<p>How would I get the result to be the same (or similar but longer) as on the web API?</p>
",Text Generation & LLMs,codegen causal ml inference short compared result model card api experimenting salesforce codegen mono model generate text using prompt result machine much shorter get inputting prompt api window model card machine add return output enter prompt model card code generation window get following would get result similar longer web api
I am trying to make a product which will reformat the answer using the question and Sql_answer as data,"<p>Question:&quot;How many employees are there in Department X&quot;</p>
<p>Sql_answer:[('2'),]</p>
<p>I want the model generate the answer in this format:</p>
<p>Expected answer:&quot;There are 2 employees in Department X.&quot;</p>
<p>On using the web client h2oai gpt(h2oai/h2ogpt-4096-llama2-13b-chat) I am getting the correct response however the same prompt is not working with the downloaded transformers (h2oai/h2ogpt-4096-llama2-13b-chat) for the same model. It worked for first 3-4 times then it started generating random text generation.</p>
<p>I tried using t5, gpt2 but I was not getting the expected answer.</p>
<p>Can someone suggest a method or model for this?</p>
",Text Generation & LLMs,trying make product reformat answer using question sql answer data question many employee department x sql answer want model generate answer format expected answer employee department x using web client h oai gpt h oai h ogpt llama b chat getting correct response however prompt working downloaded transformer h oai h ogpt llama b chat model worked first time started generating random text generation tried using gpt wa getting expected answer someone suggest method model
Why can we set LLM&#39;s input and output to be the same when fine tuning on text generation task?,"<p>I'm trying to fine tune my GPT-2 model for song lyrics text generation, and I have a couple of song lyrics on hand. However, I'm confused about how to fine tune GPT-2 model that doesn't have standard input and outputs format. The reason is that I want my fine tuned GPT-2 to generate anything in a lyric style, and I don't know what should be the expected output given the song lyrics datasets on hand.</p>
<p>After searching related articles online, I found a really confusing solution on <a href=""https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"" rel=""nofollow noreferrer"">How to Fine-Tune GPT-2 for Text Generation</a> using the following training method:</p>
<pre><code>outputs = model(input_tensor, labels=input_tensor)
loss = outputs[0]
loss.backward()
</code></pre>
<p>From my understanding, the first parameter is the input text of the model, while the second parameter <code>labels</code> is usually the expected output of the model. If we just set it to be the same, are we actually training a repeater that always repeats the input text? If so, how could we expect that our fine tuned model can speak everything in a lyric style?</p>
<p>(Additional Question during my trial and error): Intuitively, I thought I should split each song into 2 halfs. Then I use first half as inputs to my GPT-2 model, and set the expected output to be the second half. But after some experiments I found my fine-tuned GPT-2 kept repeating words like &quot;the&quot; in down-stream tasks. I'm curious about the reason why I failed here.</p>
",Text Generation & LLMs,set llm input output fine tuning text generation task trying fine tune gpt model song lyric text generation couple song lyric hand however confused fine tune gpt model standard input output format reason want fine tuned gpt generate anything lyric style know expected output given song lyric datasets hand searching related article online found really confusing solution fine tune gpt text generation using following training method understanding first parameter input text model second parameter usually expected output model set actually training repeater always repeat input text could expect fine tuned model speak everything lyric style additional question trial error intuitively thought split song half use first half input gpt model set expected output second half experiment found fine tuned gpt kept repeating word like stream task curious reason failed
How to Set Safety Parameters for Text Generation Model in Google Cloud Vertex AI?,"<p>I am working on a research project where I need to summarize news articles using the Google Palm2 Text Generation Model. I have encountered an issue with certain news articles in my dataset where I'm getting empty responses along with safety attributes that block the output. Here is the code I'm using:</p>
<pre><code>from vertexai.language_models import TextGenerationModel
parameters = {  # default values
    'max_output_tokens': 256,
    'temperature': 0.0,
    'top_p': 1.0,
    'top_k': 40,
}
prompt = &quot;...&quot;
model = TextGenerationModel.from_pretrained('text-bison@001')
response = model.predict(
    prompt,
    **parameters,
)
</code></pre>
<p>The following is an example prediction:</p>
<pre><code>Prediction(predictions=[{'content': '', 'citationMetadata': None, 'safetyAttributes': {'blocked': True, 'errors': [253.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None)
</code></pre>
<p>The issue seems to be related to safety parameters preventing the model from generating a summary for certain news articles. I've been trying to find documentation on how to configure these safety parameters using the Python API, but I could not locate the relevant information.</p>
<p>Could someone please provide guidance on how to set the safety parameters for the TextGenerationModel? Any help or pointers to documentation would be greatly appreciated. Thank you!</p>
",Text Generation & LLMs,set safety parameter text generation model google cloud vertex ai working research project need summarize news article using google palm text generation model encountered issue certain news article dataset getting empty response along safety attribute block output code using following example prediction issue seems related safety parameter preventing model generating summary certain news article trying find documentation configure safety parameter using python api could locate relevant information could someone please provide guidance set safety parameter textgenerationmodel help pointer documentation would greatly appreciated thank
LLM with Vector Database: Prompt to list all stored documents,"<p>Let's say I have a vector document database of recipes (not too many, 10 maximum). I want to query an LLM (like GPT) with a prompt like &quot;Which recipes do you have?&quot;, i.e. a query to list all documents. How can I achieve this behavior with LangChain (or a similar tool)?</p>
<p>One direction of thought I have so far: Create a short summary or title with GPT every time a new recipe is stored as metadata and when such a query comes in return all the summaries.</p>
",Text Generation & LLMs,llm vector database prompt list stored document let say vector document database recipe many maximum want query llm like gpt prompt like recipe e query list document achieve behavior langchain similar tool one direction thought far create short summary title gpt every time new recipe stored metadata query come return summary
How to perform unsupervised fine tuning of a Code LLM with custom code base for a task like code summarization?,"<p>How to prepare code data to fine tune a code LLM in an unsupervised way or is it even possible?</p>
<p>For example:
Task: Code summarization with custom code base (with no summaries)
Let's assume that this code base is unique, and a pre-trained model is giving unsatisfactory results. Now to fine tune there are three options,How to prepare code data to fine tune a code LLM in an unsupervised way or is it even possible?</p>
<p>For example:
Task: Code summarization with custom code base (with no summaries)
Let's assume that this code base is unique, and a pre-trained model is giving unsatisfactory results. Now to fine tune there are three options,</p>
<ol>
<li><p>Manually prepare summaries for a portion of the code and fine tune</p>
</li>
<li><p>Find a similar code base which has the labels (docstring) and fine tune</p>
</li>
<li><p>Mask some portions of the code randomly and give as input and output will be the masked portions</p>
</li>
<li><p>Manually prepare summaries for a portion of the code and fine tune</p>
</li>
<li><p>Find a similar code base which has the labels (docstring) and fine tune</p>
</li>
<li><p>Mask some portions of the code randomly and give as input and output will be the masked portions</p>
</li>
</ol>
<p>Options 1 and 2 don't seem feasible for a production environment.</p>
<p>The reasoning behind option 3 is that with no availability labels, the model will learn the patterns in the code base and provide a better summarization with its pre-trained knowledge.</p>
<p>I tried the option 3 with [CodeT5+ fine tuning] (<a href=""https://github.com/salesforce/CodeT5/blob/main/CodeT5%2B/tune_codet5p_seq2seq.py"" rel=""nofollow noreferrer"">https://github.com/salesforce/CodeT5/blob/main/CodeT5%2B/tune_codet5p_seq2seq.py</a>). The format of input and output was as follows
Input:
`class Label:
def <strong>init</strong>(self, text, font):
self._text = text
self._font = font</p>
<pre><code>def get_text(self):
    |&lt;mask&gt;|

def set_text(self, value):
    self._text = value`
</code></pre>
<p>Output:
<code>return self._text</code></p>
",Text Generation & LLMs,perform unsupervised fine tuning code llm custom code base task like code summarization prepare code data fine tune code llm unsupervised way even possible example task code summarization custom code base summary let assume code base unique pre trained model giving unsatisfactory result fine tune three option prepare code data fine tune code llm unsupervised way even possible example task code summarization custom code base summary let assume code base unique pre trained model giving unsatisfactory result fine tune three option manually prepare summary portion code fine tune find similar code base ha label docstring fine tune mask portion code randomly give input output masked portion manually prepare summary portion code fine tune find similar code base ha label docstring fine tune mask portion code randomly give input output masked portion option seem feasible production environment reasoning behind option availability label model learn pattern code base provide better summarization pre trained knowledge tried option codet fine tuning format input output wa follows input class label def init self text font self text text self font font output
Hugging Face model deployment,"<p>My question is related to how one deploys the Hugging Face model. I recently downloaded the Falcon 7B Instruct model and ran it in my Colab. However, when I am trying to load the model and want it to generate text, it takes about 40 seconds to give me an output. I was just wondering how we deploy these models in production then so that it gives us output with low latency. I am new to MLOps so I just want to explore. Also, what will be the charges of deploying that model? What if many users are simultaneously using this model? How will I handle that? Will greatly appreciate the response.</p>
<p>The code I am using is from the <a href=""https://huggingface.co/tiiuae/falcon-7b-instruct"" rel=""nofollow noreferrer"">https://huggingface.co/tiiuae/falcon-7b-instruct</a>.</p>
<p>Also, I am saving the model weights locally in a Google Drive.</p>
",Text Generation & LLMs,hugging face model deployment question related one deploys hugging face model recently downloaded falcon b instruct model ran colab however trying load model want generate text take second give output wa wondering deploy model production give u output low latency new mlops want explore also charge deploying model many user simultaneously using model handle greatly appreciate response code using also saving model weight locally google drive
Most similar words for ChatGPT Word Embeddings,"<p>I am extracting the word embeddings corresponding to a list of words from ChatGPT API. I was wondering if there is a way similar to Gensim most_similar method to extract the n words that are most similar to my desired terms in the entire model.</p>
",Text Generation & LLMs,similar word chatgpt word embeddings extracting word embeddings corresponding list word chatgpt api wa wondering way similar gensim similar method extract n word similar desired term entire model
"How do I send a pdf file to a jupyter notebook, process it in the notebook and then display the result in a webapp","<p>I have a jupyter notebook that can take a pdf file, run an llm model and then return the summary of the pdf.</p>
<p>I'm think of a way to have a webapp where a user can upload their pdf, the pdf gets sent to the notebook and the summary gets displayed to the user.</p>
<p>I know how to build the user interface (using nextjs) but I'm not sure how to build the process. Do I hot the jupyter notebook somewhere? How do I send the pdf from my nextjs app to the notebook&gt; I'm pretty confused.</p>
<p>I would really appreciate suggestions please.</p>
",Text Generation & LLMs,send pdf file jupyter notebook process notebook display result webapp jupyter notebook take pdf file run llm model return summary pdf think way webapp user upload pdf pdf get sent notebook summary get displayed user know build user interface using nextjs sure build process hot jupyter notebook somewhere send pdf nextjs app notebook pretty confused would really appreciate suggestion please
Can BERT or LLM be used for sentence - word recommendation?,"<p>I'm junior data analyst.</p>
<p>I'm looking for method for Sentence-&gt; word recommendation.
For example, if I input 'the little mermaid' and book's introduction(sentence), the model can put out 'swim suit' or 'fish doll'.</p>
<p>My knowledge about NLP is beginner level, I even didn't know about BERT.
Can I make that kinds of model through BERT or other LLM?</p>
<p>and I don't have any idea of what keyword should I search for.
I ask for your advice.</p>
<p>thank you.</p>
",Text Generation & LLMs,bert llm used sentence word recommendation junior data analyst looking method sentence word recommendation example input little mermaid book introduction sentence model put swim suit fish doll knowledge nlp beginner level even know bert make kind model bert llm idea keyword search ask advice thank
Is it possible to train gpt2 with our own data to generate text?,"<p>I want to use gpt2 to generate text. I tried to train the model on my own dataset but at the end it generate one word only. I couldn't find a solution to this issue. Is there a way to solve this?<a href=""https://i.sstatic.net/WAfSq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WAfSq.png"" alt=""enter image description here"" /></a></p>
",Text Generation & LLMs,possible train gpt data generate text want use gpt generate text tried train model dataset end generate one word find solution issue way solve
Generating embeddings from a custom hierarchical attention network model,"<p>I took the following model from here
<a href=""https://www.kaggle.com/code/hsankesara/news-classification-using-han/notebook"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/hsankesara/news-classification-using-han/notebook</a>
and now I want to generate the hierarchical attention embeddings of my dataset.</p>
<pre><code>word_input = Input(shape=(max_senten_len,), dtype='int32')
word_sequences = BertEmbeddingLayer()(word_input)
word_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)
word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)
word_att = AttentionWithContext()(word_dense)
wordEncoder = Model(word_input, word_att)[text](https://www.kaggle.com/code/hsankesara/news-classification-using-han/notebook)

#The sentence model that runs first inputing the data 
sent_input = Input(shape=(max_senten_num, max_senten_len), dtype='int32')
sent_encoder = TimeDistributed(wordEncoder)(sent_input) # this calls the wordencoder that encodes the word embeddings and runs it through the first attention layer
sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)
sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)
sent_att = AttentionWithContext()(sent_dense)
drpot = Dropout(0.5)(sent_att)
preds = Dense(7, activation='softmax')(drpot)
model = Model(sent_input, preds)
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])
</code></pre>
<p>The attention with context layer is given below</p>
<pre><code>def dot_product(x, kernel):

    if K.backend() == 'tensorflow':
        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)
    else:
        return K.dot(x, kernel)

class AttentionWithContext(Layer):
    &quot;&quot;&quot;
    Attention operation, with a context/query vector, for temporal data.
    Supports Masking.
    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]
    &quot;Hierarchical Attention Networks for Document Classification&quot;
    by using a context vector to assist the attention
    # Input shape
        3D tensor with shape: `(samples, steps, features)`.
    # Output shape
        2D tensor with shape: `(samples, features)`.
    How to use:
    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.
    The dimensions are inferred based on the output shape of the RNN.
    Note: The layer has been tested with Keras 2.0.6
    Example:
        model.add(LSTM(64, return_sequences=True))
        model.add(AttentionWithContext())
        # next add a Dense layer (for classification/regression) or whatever...
    &quot;&quot;&quot;

    def __init__(self,
                 W_regularizer=None, u_regularizer=None, b_regularizer=None,
                 W_constraint=None, u_constraint=None, b_constraint=None,
                 bias=True, **kwargs):

        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.u_regularizer = regularizers.get(u_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.u_constraint = constraints.get(u_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        super(AttentionWithContext, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight(shape = (input_shape[-1], input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        if self.bias:
            self.b = self.add_weight(shape = (input_shape[-1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)

        self.u = self.add_weight(shape = (input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_u'.format(self.name),
                                 regularizer=self.u_regularizer,
                                 constraint=self.u_constraint)

        super(AttentionWithContext, self).build(input_shape)


    def call(self, x, mask=None):
        uit = dot_product(x, self.W)

        if self.bias:
            uit += self.b

        uit = K.tanh(uit)
        ait = dot_product(uit, self.u)

        a = K.exp(ait)

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]


</code></pre>
<p>I have tried training the model and after training running an intermediate layer model</p>
<pre><code>from tensorflow import keras

new_input = Input(shape=(max_senten_num, max_senten_len), dtype='int32')
with tpu_strategy.scope():
  intermediate_layer_model = keras.Model(inputs=model.input, outputs=model.layers[4].output)
  intermediate_output = intermediate_layer_model.predict(data)
</code></pre>
<p>I am trying to use this model to genrate hierachical attention embeddings to use as feature in another model but I can't seem to understand how do I generate these embddings. At first, I thought that the prediction of the sent_att layer would be the embeddings that I need, similar to how BERT embeddings are generated (the output of pretrained BERT model on our data), but my senior lead told me that they are different. This is where I am stuck. How else are these embeddings genrated?</p>
",Text Generation & LLMs,generating embeddings custom hierarchical attention network model took following model want generate hierarchical attention embeddings dataset attention context layer given tried training model training running intermediate layer model trying use model genrate hierachical attention embeddings use feature another model seem understand generate embddings first thought prediction sent att layer would embeddings need similar bert embeddings generated output pretrained bert model data senior lead told different stuck else embeddings genrated
How can I use LangChain Callbacks to log the model calls and answers into a variable,"<p>I'm using <a href=""https://python.langchain.com/en/latest/index.html"" rel=""nofollow noreferrer"">LangChain</a> to build a NL application. I want the interactions with the LLM to be recorded in a variable I can use for logging and debugging purposes. I have created a very simple chain:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Any, Dict

from langchain import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import OpenAI

llm = OpenAI()
prompt = PromptTemplate.from_template(&quot;1 + {number} = &quot;)
handler = MyCustomHandler()

chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])
chain.run(number=2)
</code></pre>
<p>To record what's going on, I have created a custom <a href=""https://python.langchain.com/en/latest/modules/callbacks/getting_started.html#creating-a-custom-handler"" rel=""nofollow noreferrer"">CallbackHandler</a>:</p>
<pre class=""lang-py prettyprint-override""><code>class MyCustomHandler(BaseCallbackHandler):
    def on_text(self, text: str, **kwargs: Any) -&gt; Any:
        print(f&quot;Text: {text}&quot;)
        self.log = text
  
    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -&gt; Any:
        &quot;&quot;&quot;Run when chain starts running.&quot;&quot;&quot;
        print(&quot;Chain started running&quot;)
</code></pre>
<p>This works more or less as expected, but it has some side effects that I cannot figure out where they are coming from. The output is:</p>
<p><a href=""https://i.sstatic.net/EgrL2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EgrL2.png"" alt=""enter image description here"" /></a></p>
<p>And the <code>handler.log</code> variable contains:</p>
<p><code>'Prompt after formatting:\n\x1b[32;1m\x1b[1;3m1 + 2 = \x1b[0m'</code></p>
<p>Where are the &quot;Prompt after formatting&quot; and the ANSI codes setting the text as green coming from? Can I get rid of them?</p>
<p>Overall, is there a better way I'm missing to use the callback system to log the application? This seems to be poorly documented.</p>
",Text Generation & LLMs,use langchain callback log model call answer variable using langchain build nl application want interaction llm recorded variable use logging debugging purpose created simple chain record going created custom callbackhandler work le expected ha side effect figure coming output variable contains prompt formatting ansi code setting text green coming get rid overall better way missing use callback system log application seems poorly documented
Clustering topics and naming the cluster in Python,"<p>I have millions of topics in my data. These topics are one to 12 words. For instance 'Cancer Biology and Genetics' could be one topic and 'Regenerative medicines' could be another. I want to create clusters of similar topics and name them. I tried BERT+K-Means to cluster these topics and it works fine. I don't know much about NLP and want to use the best approach to this. I also do not have a way to name these clusters in a way that it makes sense and represents the cluster. Please advise</p>
",Text Generation & LLMs,clustering topic naming cluster python million topic data topic one word instance cancer biology genetics could one topic regenerative medicine could another want create cluster similar topic name tried bert k mean cluster topic work fine know much nlp want use best approach also way name cluster way make sense represents cluster please advise
Why do we add |V| in the denominator in the Add-One smoothing for n-gram language models?,"<p>In NLP when we use Laplace(Add-one) smoothing technique we assume that the every word is seen one more time than the actual count and the formula is like this</p>
<p><a href=""https://i.sstatic.net/1tyTn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1tyTn.png"" alt=""formula for add one smoothing"" /></a></p>
<p>where V is the size of the vocabulary. My question is why do we add V when we are only considering the count of the previous word.</p>
<p>I only have a rough idea that every word is incremented by one so we have to normalize it by V time but I still don't understand it properly. As I said we are only considering the count of previous word right so why don't just add 1 to it.</p>
<p>I also saw that if we add V then the addition of all bigrams will be 1 which is what it should be. But is there any other explanation of why V?</p>
",Text Generation & LLMs,add v denominator add one smoothing n gram language model nlp use laplace add one smoothing technique assume every word seen one time actual count formula like v size vocabulary question add v considering count previous word rough idea every word incremented one normalize v time still understand properly said considering count previous word right add also saw add v addition bigram explanation v
Understanding repository gpt transformer,"<p>For my project I need to understand and being able to execute <a href=""https://github.com/atcbosselut/comet-commonsense"" rel=""nofollow noreferrer"">this</a> github repository about commonsense generation using the GPT transformer language model. It is quite extensive and I don't have enough programming experience to make sense of it all. Is there anyone who is good with these subjects who can guide me through it/help me?</p>
<p>Or, is there another spot where I can post this question?</p>
",Text Generation & LLMs,understanding repository gpt transformer project need understand able execute github repository commonsense generation using gpt transformer language model quite extensive enough programming experience make sense anyone good subject guide help another spot post question
How is the GPT&#39;s masked-self-attention is utilized on fine-tuning/inference,"<p>At training time, as far as I understand from the &quot;Attention is all you need&quot; paper, the way that masked-self-attention is used in the decoder is by feeding the output sequence multiple times, each time removing the mask from the next token.</p>
<p>Q1. At inference time, the expected output sequence length is not known. How do you decide on how many masked tokens to add? Do you always fill the max-length of your input with masked tokens and stop when an end of sequence symbol is predicted?</p>
<p>Q2. The GPT inference objective task is a little different. A &quot;query&quot; vector is injected to the model (for example [text1;text2] and [text2;text1] in the similarity task). How is the masking used in this scenario? I would expect that the whole sequence will be injected in only one step with no masking, however this contradicts the masked-self-attention methodology.</p>
",Text Generation & LLMs,gpt masked self attention utilized fine tuning inference training time far understand attention need paper way masked self attention used decoder feeding output sequence multiple time time removing mask next token q inference time expected output sequence length known decide many masked token add always fill max length input masked token stop end sequence symbol predicted q gpt inference objective task little different query vector injected model example text text text text similarity task masking used scenario would expect whole sequence injected one step masking however contradicts masked self attention methodology
Is there a way to generate a completely new text column for a pandas dataframe?,"<p>I have a pandas dataframe that contains multiple features such as age, gender , many symptoms with the values 0 or 1 indicating if the patient has that particular symptom or not and the target being his final diagnosis. I want to add a new column being the patient describing his symptoms based on the other features i have. Fo eg, if the features fever and cough have the values 1 i need the description to be &quot;I have been experiencing a fever and severe coughs&quot;. Is there a way to do this without having to do it manually ? Because I have over 50 000 rows in my dataset.
Any help or suggestion would be much appreciated ! Thank you !</p>
<p>I tried using transformers's gpt-2 but it keeps giving me an error so another way would be much appreciated.</p>
",Text Generation & LLMs,way generate completely new text column panda dataframe panda dataframe contains multiple feature age gender many symptom value indicating patient ha particular symptom target final diagnosis want add new column patient describing symptom based feature fo eg feature fever cough value need description experiencing fever severe cough way without manually row dataset help suggestion would much appreciated thank tried using transformer gpt keep giving error another way would much appreciated
LLM&#39;s answering out of context ( trained on user data),"<p>I have trained <code>LLM</code> on my PDF file now I am asking questions related to same, but if a question is being asked out of the context I want the answer as &quot; <em>I don't know</em> &quot; or &quot; <em>out of context</em> &quot;</p>
<p>Right now it is answering even out of context</p>
<p>I have used following embeddings:</p>
<ol>
<li>sentence-transformers/all-mpnet-base-v2</li>
<li>hkunlp/instructor-xl</li>
</ol>
<p>and tried with following LLMs:</p>
<ol>
<li>lmsys/fastchat-t5-3b-v1.0</li>
<li>falcon-7b-instruct</li>
</ol>
<p>Here is the</p>
<p>Prompt template :</p>
<pre><code>                context: {context}
                question: {question}
                answer: 
                &quot;&quot;&quot;
                QUESTION_T5_PROMPT = PromptTemplate(
                    template=question_t5_template, input_variables=[&quot;context&quot;, &quot;question&quot;]
                )
            qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT
            qa.combine_documents_chain.verbose = True
            qa.return_source_documents = True
</code></pre>
<p>Function calling the query :</p>
<pre><code>    def answer_query(self,question:str) -&gt;str:
        &quot;&quot;&quot;
        Answer the question
        &quot;&quot;&quot;

        answer_dict = self.qa({&quot;query&quot;:question,})
        print(answer_dict)
        answer = answer_dict[&quot;result&quot;]
</code></pre>
<h3><a href=""https://replit.com/join/lxaofshjga-kvmukilan"" rel=""nofollow noreferrer"">full code here</a></h3>
<p>I was trying to upload my data and ask questions based on it but it is answering out of context questions also</p>
<p>Expected outcome: the question you asked is beyond the training data</p>
",Text Generation & LLMs,llm answering context trained user data trained pdf file asking question related question asked context want answer know context right answering even context used following embeddings sentence transformer mpnet base v hkunlp instructor xl tried following llm lmsys fastchat b v falcon b instruct prompt template function calling query full code wa trying upload data ask question based answering context question also expected outcome question asked beyond training data
How can I generate a report based on given data using openai model like gpt-3.5-turbo?,"<p>Now I want to generate a report from financial data using openai model.
I have already developed Q&amp;A system based on custom knowledge base using openai and pinecone.
It was done with text-davincii-003.
But to make a report from that answer, I think gpt-3.5-turbo is good.
Please help me if you are familiar with these field.
Thanks.</p>
<p>I have tried with openai.ChatCompletion.create() method.
But it doesn't work well.</p>
",Text Generation & LLMs,generate report based given data using openai model like gpt turbo want generate report financial data using openai model already developed q system based custom knowledge base using openai pinecone wa done text davincii make report answer think gpt turbo good please help familiar field thanks tried openai chatcompletion create method work well
How does placing the output (word) labels on the initial transitions of the words in an FST lead to effective composition?,"<p>I am going through hbka.pdf (WFST paper). <a href=""https://cs.nyu.edu/%7Emohri/pub/hbka.pdf"" rel=""nofollow noreferrer"">https://cs.nyu.edu/~mohri/pub/hbka.pdf</a></p>
<p><a href=""https://i.sstatic.net/6XHtH.jpg"" rel=""nofollow noreferrer"">A WFST figure for reference</a></p>
<p>Here the input label i, the output label o, and weight w of a transition are marked on the corresponding directed arc by i: o/w.</p>
<p>It does not make sense as to how a transducer can output the entire word at the initial transition itself. If the entire word was outputted at the final transition, it is sensible to me.</p>
<p>Later I saw the following in Page 19,</p>
<p>&quot;In order for this transducer to efficiently compose with G, the output (word) labels must be placed on the initial transitions of the words; other locations would lead to delays in the composition matching, which could consume significant time and space.&quot;</p>
<p>ChatGPT answers that &quot;placing the output labels on the initial transitions of the words in the Word bigram transducer enables more efficient composition with another transducer by <strong>optimizing the matching and combination of transitions</strong>.&quot;</p>
<p>But how exactly does it happen?</p>
<p>&quot;Placing the output labels on the initial transitions ensures that the word transitions in the Word bigram transducer align directly with the transitions in the other transducer.&quot;</p>
<p>But still, the entire word which the Finite state transducer has to figure out using phones as input symbols like d,ey,dx,ax, how can it be the output of the initial transition?</p>
",Text Generation & LLMs,doe placing output word label initial transition word fst lead effective composition going hbka pdf wfst paper wfst figure reference input label output label weight w transition marked corresponding directed arc w doe make sense transducer output entire word initial transition entire word wa outputted final transition sensible later saw following page order transducer efficiently compose g output word label must placed initial transition word location would lead delay composition matching could consume significant time space chatgpt answer placing output label initial transition word word bigram transducer enables efficient composition another transducer optimizing matching combination transition exactly doe happen placing output label initial transition ensures word transition word bigram transducer align directly transition transducer still entire word finite state transducer ha figure using phone input symbol like ey dx ax output initial transition
How are LLMs assigned tasks in Python code?,"<p>I'm following Nicholas' Renotte's tutorials on VSCode, LangChain, and OpenAI using Python.</p>
<p>These are the codeblocks I've seen from the aforementioned tutorials, and I don't see any other lines of code that tell the AI what to do.</p>
<pre class=""lang-py prettyprint-override""><code>title_template = PromptTemplate(
    input_variables = ['topic'],
    template='write me a youtube video title about {topic}'
)
</code></pre>
<p>So is AI given a task using this kind of conversational language? Is that really all that's needed, and wouldn't this place a lot of pressure on precise wording of the assignment?</p>
",Text Generation & LLMs,llm assigned task python code following nicholas renotte tutorial vscode langchain openai using python codeblocks seen aforementioned tutorial see line code tell ai ai given task using kind conversational language really needed place lot pressure precise wording assignment
Label Studio: Importing Txt Files as Whole Files &amp; Exporting the Result,"<p>I am trying to export the result of the file that I imported to Label Studio. This is my labeling interface :</p>
<pre><code>&lt;View&gt;
  &lt;Labels name=&quot;label&quot; toName=&quot;text&quot;&gt;
    &lt;Label value=&quot;ILAC_ADI&quot; background=&quot;red&quot;/&gt;
    &lt;Label value=&quot;ETKIN_MADDE&quot; background=&quot;darkorange&quot;/&gt;
    &lt;Label value=&quot;ENDIKASYON&quot; background=&quot;orange&quot;/&gt;
    &lt;Label value=&quot;YAN_ETKI&quot; background=&quot;green&quot;/&gt;
    &lt;Label value=&quot;URETEN_SIRKET&quot; background=&quot;green&quot;/&gt;
  &lt;Label value=&quot;YARDIMCI_MADDE&quot; background=&quot;#FFA39E&quot;/&gt;&lt;Label value=&quot;ORGAN&quot; background=&quot;#D4380D&quot;/&gt;&lt;/Labels&gt;

  &lt;Text name=&quot;text&quot; value=&quot;$text&quot; valueType=&quot;url&quot; saveTextResult=&quot;yes&quot; granularity=&quot;word&quot;/&gt;
&lt;/View&gt;
</code></pre>
<p>and this is my text, although I am only getting the file path when I export it JSON-MIN.</p>
<ol start=""2"">
<li>Let's assume that I am successfully able to get the text data, then how can I use this for BERT model? I think we can only use IOB tagged data?</li>
</ol>
<p><strong>Label Studio Annotation - Data Uploaded Successfully</strong>
<a href=""https://i.sstatic.net/Q6T6p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q6T6p.png"" alt=""Label Studio Annotation - Data Uploaded Successfully"" /></a>
<strong>Data Export Result</strong>
<a href=""https://i.sstatic.net/mUth4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mUth4.png"" alt=""enter image description here"" /></a></p>
",Text Generation & LLMs,label studio importing txt file whole file exporting result trying export result file imported label studio labeling interface text although getting file path export json min let assume successfully able get text data use bert model think use iob tagged data label studio annotation data uploaded successfully data export result
Finetuning GPT-3 on Windows?,"<p>While I have read the documentation on fine-tuning GPT-3, I do not understand how to do so. It seems that the proposed CLI commands do not work in the Windows CMD interface and I can not find any documentation on how to finetune GPT3 using a &quot;regular&quot; python script. I have tried to understand the functions defined in the package. However I can not make sense of them. Is there any information that I am missing or is it just not possible to fine-tune GPT-3 on a Windows machine?</p>
<p><a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a></p>
",Text Generation & LLMs,finetuning gpt window read documentation fine tuning gpt understand seems proposed cli command work window cmd interface find documentation finetune gpt using regular python script tried understand function defined package however make sense information missing possible fine tune gpt window machine
I am using langchain to chat with my database I want json format as output which includes fieldname as key,"<p>I am working on Natural language to query your SQL Database using LangChain powered by ChatGPT.I am using  Langchain's SQL database to chat with my database, it returns answers in the sentence I want the answer in JSON format so I have designed a prompt but sometimes it is not giving the proper format. I have a projects table in my DB that has id, <code>project_name</code>, area, price, <code>bhk</code>, and amenities fields.</p>
<pre><code>Q=&quot;project in Bandra area&quot;  
Ans=&quot;{'id': 5, 'project_name': 'Anantara Abode', 'area': 'Bandra', 'price': 4500000, 'bhk': 3, 'amenities': 'None'}&quot;  
Q=&quot;Flats under 65 lakhs&quot;  
A={1: {'project_name': 'Anantara', 'area': 'Shahibaug', 'price': 5000000, 'bhk': 3, 'amenities': 'Club house,Swimming Pool'},  
2: {'project_name': 'Vivanta', 'area': 'Shilaj', 'price': 7000000, 'bhk': 2, 'amenities': 'Club house, Swimming Pool'},  
3: {'project_name': 'Athence', 'area': 'SG highway', 'price': 5500000, 'bhk': 2, 'amenities': 'Garden, Temple'},  
4: {'project_name': 'Anantara Alpines', 'area': 'Hebatpur', 'price': 6000000, 'bhk': 3, 'amenities': 'None'},  
5: {'project_name': 'Anantara Abode', 'area': 'Bopal', 'price': 4500000, 'bhk': 3, 'amenities': 'None'}}
</code></pre>
<p>In the second answer &quot;id&quot; field name is not included as a key.</p>
<pre><code>%%capture  
!pip install -q langchain  
!pip install -q openai 
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from langchain import OpenAI , SQLDatabase , SQLDatabaseChain  
from langchain.chat_models import ChatOpenAI

from sqlalchemy import create_engine  
input_db= SQLDatabase.from_uri('sqlite:///main.db')  
llm_1=OpenAI(openai_api_key=openai_api_key,temperature=0)

db_agent=SQLDatabaseChain(llm = llm_1,database = input_db,verbose=True)

QUERY =&quot;&quot;&quot;
Given an input question, first create a syntactically correct sql query to run, then look at the results      of the query and return the answer in dictonary format and include id fieldname in all answers.
Use the following format.  
Question: &quot;Question here&quot;   
SQLQuery: &quot;SQL Query to run include id in all queries&quot;  
SQLResult: &quot;Result of the SQLQuery include id in all result&quot;  
Answer: &quot;Final answer should be in dictionary format include id fieldname in all answers&quot;  
{question}&quot;&quot;&quot;

question = QUERY.format(question=&quot;project in Bandra area&quot;)   
db_agent.run(question)
</code></pre>
",Text Generation & LLMs,using langchain chat database want json format output includes fieldname key working natural language query sql database using langchain powered chatgpt using langchain sql database chat database return answer sentence want answer json format designed prompt sometimes giving proper format project table db ha id area price amenity field second answer id field name included key
Fine tune GPT-2 Text Prediction for Conversational AI,"<p>I am experimenting with the gpt-2 model's conditional text generation to tweak it for a good chatbot. I am using <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">nsheppard's code</a> for retraining it on my custom dataset.</p>

<p>I trained my model on a custom dataset of conversations that I pulled from my facebook data. I changed the sample length to 20 as they are dialogues during interactive conditional generation.</p>

<p>The dataset looks something like this:</p>

<pre><code> How are you 
 Hi Great and you 
 Am also good 
 So you re a graphic designer  
 Yeah 
 How can you contribute to making the game In d graphics aspect 
 Can you show me some of your work if u don t mind  
 Am planning to learn making it a motion type    
 U can go through my photos 
 K 
 Can you make animations for it  
 Flash animations to be specific 
 No please only stable ones 
 Ok
</code></pre>

<p>But, after the training when i try to chat with it, it is instead completing my sentences instead of replying to them.</p>

<pre><code>User &gt;&gt;&gt; bye
======================================== SAMPLE 1 ========================================
 and  
 hi 
 are there any positions in khrzh being appointed right now 
</code></pre>

<p>I understand that the interactive_conditional_samples.py was built to complete the sentence based on the prompt, but I thought changing the dataset would work and sure it doesn't work.</p>

<p>train.py</p>

<pre><code>#!/usr/bin/env python3
# Usage:
#  PYTHONPATH=src ./train --dataset &lt;file|directory|glob&gt;

import argparse
import json
import os
import numpy as np
import tensorflow as tf
import time
import tqdm
from tensorflow.core.protobuf import rewriter_config_pb2

import model, sample, encoder
from load_dataset import load_dataset, Sampler
from accumulate import AccumulatingOptimizer
import memory_saving_gradients

CHECKPOINT_DIR = 'checkpoint'
SAMPLE_DIR = 'samples'


parser = argparse.ArgumentParser(
    description='Fine-tune GPT-2 on your custom dataset.',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

parser.add_argument('--dataset', metavar='PATH', type=str, required=True, help='Input file, directory, or glob pattern (utf-8 text, or preencoded .npz files).')
parser.add_argument('--model_name', metavar='MODEL', type=str, default='117M', help='Pretrained model name')
parser.add_argument('--combine', metavar='CHARS', type=int, default=50000, help='Concatenate input files with &lt;|endoftext|&gt; separator into chunks of this minimum size')

parser.add_argument('--batch_size', metavar='SIZE', type=int, default=1, help='Batch size')
parser.add_argument('--learning_rate', metavar='LR', type=float, default=0.00002, help='Learning rate for Adam')
parser.add_argument('--accumulate_gradients', metavar='N', type=int, default=1, help='Accumulate gradients across N minibatches.')
parser.add_argument('--memory_saving_gradients', default=False, action='store_true', help='Use gradient checkpointing to reduce vram usage.')
parser.add_argument('--only_train_transformer_layers', default=False, action='store_true', help='Restrict training to the transformer blocks.')
parser.add_argument('--optimizer', type=str, default='adam', help='Optimizer. &lt;adam|sgd&gt;.')
parser.add_argument('--noise', type=float, default=0.0, help='Add noise to input training data to regularize against typos.')

parser.add_argument('--top_k', type=int, default=40, help='K for top-k sampling.')
parser.add_argument('--top_p', type=float, default=0.0, help='P for top-p sampling. Overrides top_k if set &gt; 0.')

parser.add_argument('--restore_from', type=str, default='latest', help='Either ""latest"", ""fresh"", or a path to a checkpoint file')
parser.add_argument('--run_name', type=str, default='run1', help='Run id. Name of subdirectory in checkpoint/ and samples/')
parser.add_argument('--sample_every', metavar='N', type=int, default=100, help='Generate samples every N steps')
parser.add_argument('--sample_length', metavar='TOKENS', type=int, default=1023, help='Sample this many tokens')
parser.add_argument('--sample_num', metavar='N', type=int, default=1, help='Generate this many samples')
parser.add_argument('--save_every', metavar='N', type=int, default=1000, help='Write a checkpoint every N steps')

parser.add_argument('--val_dataset', metavar='PATH', type=str, default=None, help='Dataset for validation loss, defaults to --dataset.')
parser.add_argument('--val_batch_size', metavar='SIZE', type=int, default=2, help='Batch size for validation.')
parser.add_argument('--val_batch_count', metavar='N', type=int, default=40, help='Number of batches for validation.')
parser.add_argument('--val_every', metavar='STEPS', type=int, default=0, help='Calculate validation loss every STEPS steps.')


def maketree(path):
    try:
        os.makedirs(path)
    except:
        pass


def randomize(context, hparams, p):
    if p &gt; 0:
        mask = tf.random.uniform(shape=tf.shape(context)) &lt; p
        noise = tf.random.uniform(shape=tf.shape(context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)
        return tf.where(mask, noise, context)
    else:
        return context


def main():
    args = parser.parse_args()
    enc = encoder.get_encoder(args.model_name)
    hparams = model.default_hparams()
    with open(os.path.join('models', args.model_name, 'hparams.json')) as f:
        hparams.override_from_dict(json.load(f))

    if args.sample_length &gt; hparams.n_ctx:
        raise ValueError(
            ""Can't get samples longer than window size: %s"" % hparams.n_ctx)

    if args.model_name == '345M':
        args.memory_saving_gradients = True
        if args.optimizer == 'adam':
            args.only_train_transformer_layers = True

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF
    with tf.Session(config=config) as sess:
        context = tf.placeholder(tf.int32, [args.batch_size, None])
        context_in = randomize(context, hparams, args.noise)
        output = model.model(hparams=hparams, X=context_in)
        loss = tf.reduce_mean(
            tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=context[:, 1:], logits=output['logits'][:, :-1]))

        if args.val_every &gt; 0:
            val_context = tf.placeholder(tf.int32, [args.val_batch_size, None])
            val_output = model.model(hparams=hparams, X=val_context)
            val_loss = tf.reduce_mean(
                tf.nn.sparse_softmax_cross_entropy_with_logits(
                    labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))
            val_loss_summary = tf.summary.scalar('val_loss', val_loss)


        tf_sample = sample.sample_sequence(
            hparams=hparams,
            length=args.sample_length,
            context=context,
            batch_size=args.batch_size,
            temperature=1.0,
            top_k=args.top_k,
            top_p=args.top_p)

        all_vars = [v for v in tf.trainable_variables() if 'model' in v.name]
        train_vars = [v for v in all_vars if '/h' in v.name] if args.only_train_transformer_layers else all_vars

        if args.optimizer == 'adam':
            opt = tf.train.AdamOptimizer(learning_rate=args.learning_rate)
        elif args.optimizer == 'sgd':
            opt = tf.train.GradientDescentOptimizer(learning_rate=args.learning_rate)
        else:
            exit('Bad optimizer:', args.optimizer)

        if args.accumulate_gradients &gt; 1:
            if args.memory_saving_gradients:
                exit(""Memory saving gradients are not implemented for gradient accumulation yet."")
            opt = AccumulatingOptimizer(
                opt=opt,
                var_list=train_vars)
            opt_reset = opt.reset()
            opt_compute = opt.compute_gradients(loss)
            opt_apply = opt.apply_gradients()
            summary_loss = tf.summary.scalar('loss', opt_apply)
        else:
            if args.memory_saving_gradients:
                opt_grads = memory_saving_gradients.gradients(loss, train_vars)
            else:
                opt_grads = tf.gradients(loss, train_vars)
            opt_grads = list(zip(opt_grads, train_vars))
            opt_apply = opt.apply_gradients(opt_grads)
            summary_loss = tf.summary.scalar('loss', loss)

        summary_lr = tf.summary.scalar('learning_rate', args.learning_rate)
        summaries = tf.summary.merge([summary_lr, summary_loss])

        summary_log = tf.summary.FileWriter(
            os.path.join(CHECKPOINT_DIR, args.run_name))

        saver = tf.train.Saver(
            var_list=all_vars,
            max_to_keep=5,
            keep_checkpoint_every_n_hours=2)
        sess.run(tf.global_variables_initializer())

        if args.restore_from == 'latest':
            ckpt = tf.train.latest_checkpoint(
                os.path.join(CHECKPOINT_DIR, args.run_name))
            if ckpt is None:
                # Get fresh GPT weights if new run.
                ckpt = tf.train.latest_checkpoint(
                    os.path.join('models', args.model_name))
        elif args.restore_from == 'fresh':
            ckpt = tf.train.latest_checkpoint(
                os.path.join('models', args.model_name))
        else:
            ckpt = tf.train.latest_checkpoint(args.restore_from)
        print('Loading checkpoint', ckpt)
        saver.restore(sess, ckpt)

        print('Loading dataset...')
        chunks = load_dataset(enc, args.dataset, args.combine)
        data_sampler = Sampler(chunks)
        if args.val_every &gt; 0:
            val_chunks = load_dataset(enc, args.val_dataset, args.combine) if args.val_dataset else chunks
        print('dataset has', data_sampler.total_size, 'tokens')
        print('Training...')

        if args.val_every &gt; 0:
            # Sample from validation set once with fixed seed to make
            # it deterministic during training as well as across runs.
            val_data_sampler = Sampler(val_chunks, seed=1)
            val_batches = [[val_data_sampler.sample(1024) for _ in range(args.val_batch_size)]
                           for _ in range(args.val_batch_count)]

        counter = 1
        counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')
        if os.path.exists(counter_path):
            # Load the step number if we're resuming a run
            # Add 1 so we don't immediately try to save again
            with open(counter_path, 'r') as fp:
                counter = int(fp.read()) + 1

        def save():
            maketree(os.path.join(CHECKPOINT_DIR, args.run_name))
            print(
                'Saving',
                os.path.join(CHECKPOINT_DIR, args.run_name,
                             'model-{}').format(counter))
            saver.save(
                sess,
                os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),
                global_step=counter)
            with open(counter_path, 'w') as fp:
                fp.write(str(counter) + '\n')

        def generate_samples():
            print('Generating samples...')
            context_tokens = data_sampler.sample(1)
            all_text = []
            index = 0
            while index &lt; args.sample_num:
                out = sess.run(
                    tf_sample,
                    feed_dict={context: args.batch_size * [context_tokens]})
                for i in range(min(args.sample_num - index, args.batch_size)):
                    text = enc.decode(out[i])
                    text = '======== SAMPLE {} ========\n{}\n'.format(
                        index + 1, text)
                    all_text.append(text)
                    index += 1
            print(text)
            maketree(os.path.join(SAMPLE_DIR, args.run_name))
            with open(
                    os.path.join(SAMPLE_DIR, args.run_name,
                                 'samples-{}').format(counter), 'w') as fp:
                fp.write('\n'.join(all_text))

        def validation():
            print('Calculating validation loss...')
            losses = []
            for batch in tqdm.tqdm(val_batches):
                losses.append(sess.run(val_loss, feed_dict={val_context: batch}))
            v_val_loss = np.mean(losses)
            v_summary = sess.run(val_loss_summary, feed_dict={val_loss: v_val_loss})
            summary_log.add_summary(v_summary, counter)
            summary_log.flush()
            print(
                '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'
                .format(
                    counter=counter,
                    time=time.time() - start_time,
                    loss=v_val_loss))

        def sample_batch():
            return [data_sampler.sample(1024) for _ in range(args.batch_size)]


        avg_loss = (0.0, 0.0)
        start_time = time.time()

        try:
            while True:
                if counter % args.save_every == 0:
                    save()
                if counter % args.sample_every == 0:
                    generate_samples()
                if args.val_every &gt; 0 and (counter % args.val_every == 0 or counter == 1):
                    validation()

                if args.accumulate_gradients &gt; 1:
                    sess.run(opt_reset)
                    for _ in range(args.accumulate_gradients):
                        sess.run(
                            opt_compute, feed_dict={context: sample_batch()})
                    (v_loss, v_summary) = sess.run((opt_apply, summaries))
                else:
                    (_, v_loss, v_summary) = sess.run(
                        (opt_apply, loss, summaries),
                        feed_dict={context: sample_batch()})

                summary_log.add_summary(v_summary, counter)

                avg_loss = (avg_loss[0] * 0.99 + v_loss,
                            avg_loss[1] * 0.99 + 1.0)

                print(
                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'
                    .format(
                        counter=counter,
                        time=time.time() - start_time,
                        loss=v_loss,
                        avg=avg_loss[0] / avg_loss[1]))

                counter += 1
        except KeyboardInterrupt:
            print('interrupted')
            save()


if __name__ == '__main__':
    main()
</code></pre>

<p>sample.py</p>

<pre><code>import tensorflow as tf

import model

def top_k_logits(logits, k):
    if k == 0:
        # no truncation
        return logits

    def _top_k():
        values, _ = tf.nn.top_k(logits, k=k)
        min_values = values[:, -1, tf.newaxis]
        return tf.where(
            logits &lt; min_values,
            tf.ones_like(logits, dtype=logits.dtype) * -1e10,
            logits,
        )
    return tf.cond(
       tf.equal(k, 0),
       lambda: logits,
       lambda: _top_k(),
    )


def top_p_logits(logits, p):
    with tf.variable_scope('top_p_logits'):
        logits_sort = tf.sort(logits, direction='DESCENDING')
        probs_sort = tf.nn.softmax(logits_sort)
        probs_sums = tf.cumsum(probs_sort, axis=1, exclusive=True)
        logits_masked = tf.where(probs_sums &lt; p, logits_sort, tf.ones_like(logits_sort)*1000) # [batchsize, vocab]
        min_logits = tf.reduce_min(logits_masked, axis=1, keepdims=True) # [batchsize, 1]
        return tf.where(
            logits &lt; min_logits,
            tf.ones_like(logits, dtype=logits.dtype) * -1e10,
            logits,
        )


def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=0.0):
    if start_token is None:
        assert context is not None, 'Specify exactly one of start_token and context!'
    else:
        assert context is None, 'Specify exactly one of start_token and context!'
        context = tf.fill([batch_size, 1], start_token)

    def step(hparams, tokens, past=None):
        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)

        logits = lm_output['logits'][:, :, :hparams.n_vocab]
        presents = lm_output['present']
        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))
        return {
            'logits': logits,
            'presents': presents,
        }

    with tf.name_scope('sample_sequence'):
        # Don't feed the last context token -- leave that to the loop below
        # TODO: Would be slightly faster if we called step on the entire context,
        # rather than leaving the last token transformer calculation to the while loop.
        context_output = step(hparams, context[:, :-1])

        def body(past, prev, output):
            next_outputs = step(hparams, prev[:, tf.newaxis], past=past)
            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)
            if top_p &gt; 0.0:
                logits = top_p_logits(logits, p=top_p)
            else:
                logits = top_k_logits(logits, k=top_k)
            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)
            return [
                tf.concat([past, next_outputs['presents']], axis=-2),
                tf.squeeze(samples, axis=[1]),
                tf.concat([output, samples], axis=1),
            ]

        def cond(*args):
            return True

        _, _, tokens = tf.while_loop(
            cond=cond, body=body,
            maximum_iterations=length,
            loop_vars=[
                context_output['presents'],
                context[:, -1],
                context,
            ],
            shape_invariants=[
                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),
                tf.TensorShape([batch_size]),
                tf.TensorShape([batch_size, None]),
            ],
            back_prop=False,
        )

        return tokens
</code></pre>

<p>interactive_conditional_samples.py</p>

<pre><code>#!/usr/bin/env python3

import fire
import json
import os
import numpy as np
import tensorflow as tf

import model, sample, encoder

def interact_model(
    model_name='chatbot',
    seed=None,
    nsamples=1,
    batch_size=1,
    length=20,
    temperature=1,
    top_k=0,
    top_p=0.0
):
    """"""
    Interactively run the model
    :model_name=chatbot : String, which model to use
    :seed=None : Integer seed for random number generators, fix seed to reproduce
     results
    :nsamples=1 : Number of samples to return total
    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.
    :length=None : Number of tokens in generated text, if None (default), is
     determined by model hyperparameters
    :temperature=1 : Float value controlling randomness in boltzmann
     distribution. Lower temperature results in less random completions. As the
     temperature approaches zero, the model will become deterministic and
     repetitive. Higher temperature results in more random completions.
    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is
     considered for each step (token), resulting in deterministic completions,
     while 40 means 40 words are considered at each step. 0 (default) is a
     special setting meaning no restrictions. 40 generally is a good value.
    :top_p=0.0 : Float value controlling diversity. Implements nucleus sampling,
     overriding top_k if set to a value &gt; 0. A good setting is 0.9.
    """"""
    if batch_size is None:
        batch_size = 1
    assert nsamples % batch_size == 0

    enc = encoder.get_encoder(model_name)
    hparams = model.default_hparams()
    with open(os.path.join('models', model_name, 'hparams.json')) as f:
        hparams.override_from_dict(json.load(f))

    if length is None:
        length = hparams.n_ctx // 2
    elif length &gt; hparams.n_ctx:
        raise ValueError(""Can't get samples longer than window size: %s"" % hparams.n_ctx)

    with tf.Session(graph=tf.Graph()) as sess:
        context = tf.placeholder(tf.int32, [batch_size, None])
        np.random.seed(seed)
        tf.set_random_seed(seed)
        output = sample.sample_sequence(
            hparams=hparams, length=length,
            context=context,
            batch_size=batch_size,
            temperature=temperature, top_k=top_k, top_p=top_p
        )

        saver = tf.train.Saver()
        ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))
        saver.restore(sess, ckpt)

        while True:
            raw_text = input(""User &gt;&gt;&gt; "")
            while not raw_text:
                print('Prompt should not be empty!')
                raw_text = input(""User &gt;&gt;&gt; "")
            context_tokens = enc.encode(raw_text)
            generated = 0
            for _ in range(nsamples // batch_size):
                out = sess.run(output, feed_dict={
                    context: [context_tokens for _ in range(batch_size)]
                })[:, len(context_tokens):]
                for i in range(batch_size):
                    generated += 1
                    text = enc.decode(out[i])
                    print(""="" * 40 + "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)
                    print(text)
            print(""="" * 80)

if __name__ == '__main__':
    fire.Fire(interact_model)
</code></pre>

<p>How can I tweak the code to get it working like a chatbot? I am guessing it has something to do with the context part in sample.py, though i am unsure how is this going to work.</p>
",Text Generation & LLMs,fine tune gpt text prediction conversational ai experimenting gpt model conditional text generation tweak good chatbot using nsheppard code retraining custom dataset trained model custom dataset conversation pulled facebook data changed sample length dialogue interactive conditional generation dataset look something like training try chat instead completing sentence instead replying understand interactive conditional sample py wa built complete sentence based prompt thought changing dataset would work sure work train py sample py interactive conditional sample py tweak code get working like chatbot guessing ha something context part sample py though unsure going work
Fine-tuning an open-source LLM for a new language?,"<p>What are the most suitable open source LLMs and frameworks for fine-tuning? I intend to use this model in a quite specific domain, perhaps a physics mentor for a school. How long might it take (with 3070 Ti 11Gb) to achieve acceptable accuracy for this purpose? I assume that the process of fine-tuning a new language is the same as fine-tuning on any other data, or is it not?</p>
<p>I couldn't find any open source LLMs that support the language I need, or are even partially trained on it, which would've made fine-tuning less complex. While there've been LLMs that support languages from the same family, but I believe that this is more likely to cause issues and confusion, since it'll be harder for the model to distinguish between languages.</p>
",Text Generation & LLMs,fine tuning open source llm new language suitable open source llm framework fine tuning intend use model quite specific domain perhaps physic school long might take ti gb achieve acceptable accuracy purpose assume process fine tuning new language fine tuning data find open source llm support language need even partially trained would made fine tuning le complex llm support language family believe likely cause issue confusion since harder model distinguish language
Can you create a custom model using GPT-3 to answer questions only about a specific topic?,"<p>I'm using GPT-3 to create a chatbot that can answer questions related to a specific topic. Can GPT-3 be trained to detect questions that are irrelevant to the topic and refuse to answer them?</p>
<p>Example: Let's say I want to create a chatbot that can only answer questions about
Javascript. If it is asked to list the seven wonders of the world, it should refuse to answer.</p>
",Text Generation & LLMs,create custom model using gpt answer question specific topic using gpt create chatbot answer question related specific topic gpt trained detect question irrelevant topic refuse answer example let say want create chatbot answer question javascript asked list seven wonder world refuse answer
"How could I use AI to process a sentence, sum it up in a few words, and create a new column out of those words?","<p>I have a column in my dataset that contains text (A few sentences) explaining why a delivery was late. There are tens of thousands of entries in this column and I would like to create a supplementary column that sums these sentences up in one or two words for analysis purposes.</p>
<p>I am relatively new to world of NLP, all I know is that ChatGPT is able to execute my requests with acceptable accuracy. I just would like to know how to implement this on a large scale.</p>
<p>For Example:
&quot;Accident on the 401 highway caused shipping delays&quot; --&gt; 'Roadblock'
&quot;Unable to verify shipping address, needed to contact customer&quot; --&gt; 'Address Misinput'</p>
<p>These are just some examples, I don't need it to be this specific.</p>
<p>I've already tried code in SQL that count the amount of time each word was used and I manually sorted out the verbs, however, I want to see what AI is capable of as it can understand complex questions and generate a coherent response.</p>
",Text Generation & LLMs,could use ai process sentence sum word create new column word column dataset contains text sentence explaining delivery wa late ten thousand entry column would like create supplementary column sum sentence one two word analysis purpose relatively new world nlp know chatgpt able execute request acceptable accuracy would like know implement large scale example accident highway caused shipping delay roadblock unable verify shipping address needed contact customer address misinput example need specific already tried code sql count amount time word wa used manually sorted verb however want see ai capable understand complex question generate coherent response
"When you prompt GPT3, what happens to the input data?","<p>For example, let's say I open up the playground and type &quot;Quack&quot;. What does the model do with those 5 characters to figure out what letters or words should come next?</p>
<p>(As it happens, GPT3 filled in that prompt with &quot;Quackery&quot;, then a tirade against cell therapy. Weird).</p>
",Text Generation & LLMs,prompt gpt happens input data example let say open playground type quack doe model character figure letter word come next happens gpt filled prompt quackery tirade cell therapy weird
Replacing UI with LLMs,"<p>How can one replace the UI of an application with an LLM's chat window? The bot should be able to do everything it used to but via natural language. So the end user doesn't have to click at buttons or view options in a menu; rather, he/she should be able to tell this via simple sentences, which can trigger the usual APIs that were event (click/hover) driven. Are there any existing projects in github or a definite approach to solving this?</p>
",Text Generation & LLMs,replacing ui llm one replace ui application llm chat window bot able everything used via natural language end user click button view option menu rather able tell via simple sentence trigger usual apis event click hover driven existing project github definite approach solving
GPT2 special tokens: Ignore word(s) in input text when predicting next word,"<p>I just started using GPT2 and I have a question concerning special tokens:</p>
<p>I'd like to predict the next word given a text input, but I want to mask some words in my input chunk using a special token. I don't want GPT2 to predict the masked words, I just don't want to use them for the prediction and I want GPT2 to &quot;know&quot; that it doesn't &quot;see&quot; all the input words.</p>
<p>Here's an example:
I have &quot;the quick brown fox jumps over the lazy&quot; as an input sentence.
I want GPT2 to predict the last word (correct would be &quot;dog&quot; in this case).
I also want to mask the words &quot;the lazy&quot;, but GPT2 should &quot;know&quot; there is something at the end of the input sentence. So basically for GPT2, the input should look like this: &quot;the quick brown fox jumps over _ _&quot;, and not like this: &quot;the quick brown fox jumps over&quot;, so it knows not to predict the word after &quot;over&quot;.</p>
<p>I thought about using special tokens to replace the &quot;hidden&quot; words, but I think neither MASK nor PAD make sense in this case.</p>
<p>Does anyone have an idea how to solve this?</p>
<p>Thanks in advance for your help!</p>
",Text Generation & LLMs,gpt special token ignore word input text predicting next word started using gpt question concerning special token like predict next word given text input want mask word input chunk using special token want gpt predict masked word want use prediction want gpt know see input word example quick brown fox jump lazy input sentence want gpt predict last word correct would dog case also want mask word lazy gpt know something end input sentence basically gpt input look like quick brown fox jump like quick brown fox jump know predict word thought using special token replace hidden word think neither mask pad make sense case doe anyone idea solve thanks advance help
"Challenges when calculating perplexity: using bidirectional models, and dealing with large text size and values, are my approaches reasonable?","<p>Challenges when calculating perplexity: is my approach reasonable?</p>
<p>I am trying to find a pre-trained language model that will work best for my text. The text is pretty specific in its language and content but there's no test data avaiable or budget to generate it so I'm using perplexity as an intrinisic metric so allow me to compare different fine-tuned versions of BART.</p>
<p>I've had a good look online but couldn't find any discussion about some of the following issues:</p>
<ul>
<li>BART is a bi-directional model therefore when we talk about 'context' for  calculating perplexity, the normal view that this includes all the words in a window up to the masked token seems incorrect. I am therefore planning to use a window centred (rather than ending) on the masked token. Does that seem correct or does it ruin the metric in some way I'm not anticipating?</li>
<li>When I'm calculating perplexity for larger sliding window sizes <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""nofollow noreferrer"">as suggested by HuggingFace</a>, the probabilities that I'm multiplying together become so small that Python is rounding them to zero and therefore perplexity comes out as Infinite. I've checked and none of the probabilities themselves are zero, its just the product of them that becomes too small. I had planned to use 1024 tokens as the maximum that the model can take but will instead have ~350 as the limit. Has anyone else run into this problem and found another solution that I'm not seeing?</li>
<li>The text I am interested in is one single very long text, I've worked around that with my summarisation but I'm interested in seeing how well a model works for the text in general. It would take a lot more time than I have to calculate perplexity for sliding window across the entire text so instead my plan is to sample several shorter sections and calculate the perplexity of each and then aggregate that score up. Any advice on the best way to do that, take an average, or take all of the probabilities together and calculate perplexity across them despite them being discontinuous?</li>
</ul>
",Text Generation & LLMs,challenge calculating perplexity using bidirectional model dealing large text size value approach reasonable challenge calculating perplexity approach reasonable trying find pre trained language model work best text text pretty specific language content test data avaiable budget generate using perplexity intrinisic metric allow compare different fine tuned version bart good look online find discussion following issue bart bi directional model therefore talk context calculating perplexity normal view includes word window masked token seems incorrect therefore planning use window centred rather ending masked token doe seem correct doe ruin metric way anticipating calculating perplexity larger sliding window size suggested huggingface probability multiplying together become small python rounding zero therefore perplexity come infinite checked none probability zero product becomes small planned use token maximum model take instead limit ha anyone else run problem found another solution seeing text interested one single long text worked around summarisation interested seeing well model work text general would take lot time calculate perplexity sliding window across entire text instead plan sample several shorter section calculate perplexity aggregate score advice best way take average take probability together calculate perplexity across despite discontinuous
&quot;The model &#39;MPTForCausalLM&#39; is not supported for text-generation&quot;- The following warning is coming when trying to use MPT-7B instruct,"<p>I am using a VM of GCP(e2-highmem-4 (Efficient Instance, 4 vCPUs, 32 GB RAM)) to load the model and use it. Here is the code I have written-</p>
<pre><code>import torch
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import transformers
config = transformers.AutoConfig.from_pretrained(
  'mosaicml/mpt-7b-instruct',
  trust_remote_code=True,
)
# config.attn_config['attn_impl'] = 'flash'

model = transformers.AutoModelForCausalLM.from_pretrained(
  'mosaicml/mpt-7b-instruct',
  config=config,
  torch_dtype=torch.bfloat16,
  trust_remote_code=True,
  cache_dir=&quot;./cache&quot;
)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-neox-20b&quot;, cache_dir=&quot;./cache&quot;)
text_gen = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer)
text_gen(text_inputs=&quot;what is 2+2?&quot;)
</code></pre>
<p>Now the code is taking way too long to generate the text. Am I doing something wrong? or is there any way to make things faster?
Also, when creating the pipeline, I am getting the following warning-\</p>
<p><code>The model 'MPTForCausalLM' is not supported for text-generation</code></p>
<p>I tried generating text by using it but it was stuck for a long time.</p>
",Text Generation & LLMs,model mptforcausallm supported text generation following warning coming trying use mpt b instruct using vm gcp e highmem efficient instance vcpus gb ram load model use code written code taking way long generate text something wrong way make thing faster also creating pipeline getting following warning tried generating text using wa stuck long time
My gpt2 code generates a few correct words and then goes into a loop of generating the same sequence again and again,"<p>The following gpt2 code for sentence completion generates a few good sentences and then ends in a loop of repetitive sentences.</p>
<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer                         
import torch                                                                    
                                                                                
# Load the pre-trained model and tokenizer                                      
model_name = 'gpt2'                                                             
model = GPT2LMHeadModel.from_pretrained(model_name)                             
tokenizer = GPT2Tokenizer.from_pretrained(model_name)                           
                                                                                
# Set the model to evaluation mode                                              
model.eval()                                                                    
#                                                                               
# Input sentence                                                                
input_sentence = &quot;I want to go to the&quot;                                          
                                                                                
for i in range(200):                                                            
                                                                                
    # Tokenize the input sentence                                               
    input_tokens = tokenizer.encode(input_sentence, return_tensors='pt')        
                                                                                
    # Generate predictions                                                      
    with torch.no_grad():                                                       
        outputs = model.generate(input_tokens, max_length=len(input_tokens) + 1, num_return_sequences=1)
                                                                                
    # Decode the generated output                                               
    generated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)   
                                                                                
    print(generated_output)                                                     
                                                                                
    input_sentence = generated_output    
</code></pre>
",Text Generation & LLMs,gpt code generates correct word go loop generating sequence following gpt code sentence completion generates good sentence end loop repetitive sentence
Ontology vs. LLM for Query Expansion (or both?),"<p>I work at a recruitment agency where as a job seeker you can search for jobs and as a recruiter you can search for candidates in our candidate pool that might fit the job description. Currently both search engines are based on Elastic Search with some handling of synonyms, but we still have problems with showing all relevant search results if the search term doesn't fit the job description or CV (for example if some specific frontend framework is required for a job, a candidate with experience in a similar framework should still be shown in the results but with slightly lower relevancy.</p>
<p>Without much consideration for different approaches (because we don't have much NLP Expertise in the company and have a quite new data science department), we already experimented with building an ontology based on external ontologies and our own data (e.g. Python is used in Data Science) to find closely related terms and expand the search queries based on those relationships. While this approach seems to work somewhat, it feels kind of cumbersome, outdated and will probably need a lot of maintenance in the long run. For example using a prompt in GPT yielded very similar results in a matter of seconds, which raises the question if, for example, just using the embeddings of the search terms would already be enough to expand a users search query with additional relevant terms.</p>
<p>What approach would you suggest when dealing with the problem of query expansion? Or would a combination of both approaches make sense (e.g. using an LLM to automate building an ontology). Are ontologies regarding that use case outdated or am i just falling for the ChatGPT hype?</p>
<p>I would very much appreciate your insights!</p>
",Text Generation & LLMs,ontology v llm query expansion work recruitment agency job seeker search job recruiter search candidate candidate pool might fit job description currently search engine based elastic search handling synonym still problem showing relevant search result search term fit job description cv example specific frontend framework required job candidate experience similar framework still shown result slightly lower relevancy without much consideration different approach much nlp expertise company quite new data science department already experimented building ontology based external ontology data e g python used data science find closely related term expand search query based relationship approach seems work somewhat feel kind cumbersome outdated probably need lot maintenance long run example using prompt gpt yielded similar result matter second raise question example using embeddings search term would already enough expand user search query additional relevant term approach would suggest dealing problem query expansion would combination approach make sense e g using llm automate building ontology ontology regarding use case outdated falling chatgpt hype would much appreciate insight
"In a NLP AI model, how to create a persistent context per user?","<p>I’m developing a new <a href=""https://talers.io"" rel=""nofollow noreferrer"">word processor application</a>, and I’m interested in the capacities of AI and NLP to guess the end of a sentence a user is currently typing.</p>
<p>I would like the AI model to know about the context, i.e. the previous text that has been written (introducing characters, places, and the tone of the text).</p>
<p>It would work somehow like GitHub Copilot: the copilot is aware of the rest of the code, and can generate appropriate answers.</p>
<p>I imagine two possibilities:</p>
<ol>
<li>fine-tuning a model with the other inputs of the user</li>
<li>pass a part of the current text as context to the chat API before asking to complete an unfinished sentence.</li>
</ol>
<p>Solution <strong>#1</strong> is scalable (you can train your model with arbitrary amounts of text) and will produce very precise results, but there are two drawbacks:</p>
<ul>
<li>fine-tuning is slow (not the most annoying part),</li>
<li>if there are millions of users, it means millions of different fine-tuned models. Can any NLP engine process that as of today?</li>
</ul>
<p>Solution <strong>#2</strong> will work great for small texts, but it's suboptimal, as you will have to resend the context every time you start a new conversation with the AI. And the bigger the context, the worse is this solution.</p>
<p>How to achieve that with AI? GitHub Copilot is kind of achieving that, so I guess it's possible.</p>
",Text Generation & LLMs,nlp ai model create persistent context per user developing new word processor application interested capacity ai nlp guess end sentence user currently typing would like ai model know context e previous text ha written introducing character place tone text would work somehow like github copilot copilot aware rest code generate appropriate answer imagine two possibility fine tuning model input user pas part current text context chat api asking complete unfinished sentence solution scalable train model arbitrary amount text produce precise result two drawback fine tuning slow annoying part million user mean million different fine tuned model nlp engine process today solution work great small text suboptimal resend context every time start new conversation ai bigger context worse solution achieve ai github copilot kind achieving guess possible
Output logits from T5 model for text generation purposes,"<p>I am using the T5 model found on Hugging Face for text summarization. How can I output the logits of the T5 model directly given a text input for generation purposes (not training)?</p>
<p>I want to generate the outputs token by token so that I can calculate the entropy of each output token, respectively. It does not seem like the .generate() method will work for this.</p>
<p>I effectively want to create my own generate function but I need to obtain the logits of the model to be able to do this.</p>
",Text Generation & LLMs,output logits model text generation purpose using model found hugging face text summarization output logits model directly given text input generation purpose training want generate output token token calculate entropy output token respectively doe seem like generate method work effectively want create generate function need obtain logits model able
Identifying the word picked by hugging face pipeline fill-mask,"<p>I want to use hugging face's fill-mask pipeline to guess a masked token and then extract just the guessed token as a word.  This code should do that:</p>

<pre><code>!pip install -q transformers
model = pipeline('fill-mask')
outcome = model(""Kubernetes is a container orchestration &lt;mask&gt;"")[0]

#Prints: ""Kubernetes is a container orchestration platform"" 
print(outcome['sequence']) 

token = outcome['token'] 

#Prints: 1761
print(token)

#Prints: Ġplatform 
print(model.tokenizer.convert_ids_to_tokens(token))
</code></pre>

<p>But I am finding that it gives me back <code>""Ġplatform""</code> instead of <code>""platform""</code> - does anyone know why this is or what can be going on here?</p>
",Text Generation & LLMs,identifying word picked hugging face pipeline fill mask want use hugging face fill mask pipeline guess masked token extract guessed token word code finding give back instead doe anyone know going
Word2Vec - same context word different label,"<p>The link is the tutorial : <a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word2vec</a>
Here the sentence is &quot;The wide road shimmered in the hot sun&quot;.
I have two questions.</p>
<ol>
<li>It uses negative sampling but here, the same context word shimmered has two different labels and the target word &quot;road&quot; is also for the negative sampling.</li>
<li>I heard that skip-gram has weight on nearby words that is the context word close to the target word will have a higher weight but it does not show in this tutorial. I wonder which one is correct for skip-gram. Thanks
<a href=""https://i.sstatic.net/vChv4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vChv4.png"" alt=""enter image description here"" /></a></li>
</ol>
<p>I have tried to use Chat-Gpt and other skip-gram tutorial.</p>
",Text Generation & LLMs,word vec context word different label link tutorial sentence wide road shimmered hot sun two question us negative sampling context word shimmered ha two different label target word road also negative sampling heard skip gram ha weight nearby word context word close target word higher weight doe show tutorial wonder one correct skip gram thanks tried use chat gpt skip gram tutorial
Disable layers in GPT-2 model,"<p>I'm currently using a GPT-2 model that was trained on German texts. I would like to generate the next word in a text given a context chunk, but instead of using the whole model to predict the next word, I want each of the 12 layers to predict the next word separately, so I get 12 predictions for the next word. Put differently, I want to &quot;lesion&quot; all layers except for one, so they are not involved in the prediction of the next word at all.</p>
<p>This is my model:</p>
<pre><code># import modules
from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM
import torch

# download pre-trained German GPT-2 model &amp; tokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/german-gpt2&quot;)

# initialise the model
model = AutoModelForCausalLM.from_pretrained(&quot;dbmdz/german-gpt2&quot;, pad_token_id = tokenizer.eos_token_id)
</code></pre>
<p>And here's an example of a context chunk:</p>
<pre><code>input_text = &quot;Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig&quot; # correct next word: &quot;allein&quot;
</code></pre>
<p>I thought maybe I could set all attention weights to 0 in the layers I want to exclude, but I don't have a clue if that's correct and how to modify the weights in the model. Does anyone have an idea how to solve this &amp; could explain what I need to do? I've never used GPT2 before, so this is super confusing for me.</p>
<p>Thanks in advance for your help / any ideas!</p>
",Text Generation & LLMs,disable layer gpt model currently using gpt model wa trained german text would like generate next word text given context chunk instead using whole model predict next word want layer predict next word separately get prediction next word put differently want lesion layer except one involved prediction next word model example context chunk thought maybe could set attention weight layer want exclude clue correct modify weight model doe anyone idea solve could explain need never used gpt super confusing thanks advance help idea
How to estimate ngram probability?,"<p>I want to build a language model where I want to estimate the ngram probabilities. So, my question is: What are the best corpora and tools that we could use to estimate the ngram probabilities?.</p>

<p>thanks</p>
",Text Generation & LLMs,estimate ngram probability want build language model want estimate ngram probability question best corpus tool could use estimate ngram probability thanks
Using GPT-3 to identify relationships in a corpus,"<p>I have a corpus of 15K news articles. I would like to train a GPT model (3 or 4) to ingest these texts and then output how the locations, events, actions, participants, and things described in the texts are related to one another. So if the corpus says John Smith took part in a protest, I'd like to tell me this and what other people took part, how the protest was related to specific locations, etc. Is this possible?</p>
<p>If so can someone please point me in the right direction for learning how to do it? When I do searches all I'm finding is links about using GPT models to give extractive or abstractive summaries of individual texts. I suppose that's related but not quite the same.</p>
",Text Generation & LLMs,using gpt identify relationship corpus corpus k news article would like train gpt model ingest text output location event action participant thing described text related one another corpus say john smith took part protest like tell people took part protest wa related specific location etc possible someone please point right direction learning search finding link using gpt model give extractive abstractive summary individual text suppose related quite
BERTopic model: Should I remove names?,"<p>I am trying to create a movie recommender system, and want to use topic modelling in order to use movie descriptions. Currently, I am exploring the <a href=""https://pypi.org/project/bertopic/"" rel=""nofollow noreferrer"">BERTopic model</a></p>
<p>However, because I have movie descriptions, the output topics uses a lot of names. I want to remove the names in order to increase interpretability, but I was wondering if this will affect the performance of the BERT model.</p>
<p>Sorry, the output is in Dutch, but as you can see there are a lot of names in the topic outputs.</p>
<p><img src=""https://i.sstatic.net/xLkjr.png"" alt=""enter image description here"" /></p>
",Text Generation & LLMs,bertopic model remove name trying create movie recommender system want use topic modelling order use movie description currently exploring bertopic model however movie description output topic us lot name want remove name order increase interpretability wa wondering affect performance bert model sorry output dutch see lot name topic output
ValueError: k must be less than or equal to the number of training points,"<p>I am trying BerTopic on a cluster of sentences. I have actually employed Agglomerative clustering using Bert sentence embeddings, the result has many clusters one of them is this</p>
<pre><code>docs=[&quot;PARIS:France’s trade unions called for mass protests and strikes over pension reform that have brought much of the country to a halt to carry on next week, piling more pressure on President Emmanuel Macron.Commuters faced severe disruption getting to work on Friday, hospitals have been left understaffed and Paris City Hall said dozens of schools in the capital would stay closed, as unions dug in over Macron’s plans to streamline one of the developed world’s most generous pension systems.Transport workers went on strike on Thursday and took to the streets – joined by teachers, doctors, police, firemen and civil servants. Smoke and tear gas swirled through parts of Paris and Nantes as protests turned violent.Union leaders said public workers should maintain their industrial action until Tuesday when they urged members to flood the streets once again.“Unions will meet on Tuesday evening to decide on our next actions if by then Macron and (Prime Minister) Edouard Philippe has not reversed course and opened negotiations,” Catherine Perret of the hard-left CGT union told reporters.The strike pits Macron, a 41-year-old former investment banker who took office in 2017 on a promise of opening up France’s highly regulated economy, against powerful unions who say he is set on dismantling worker protections.“We’re going to protest for a week at least, and at the end of that week it’s the government that’s going to back down,” said 50-year-old Paris transport employee Patrick Dos Santos.The outcome depends on who blinks first – the unions who risk losing public support if the disruption goes on for too long, or the president whose two-and-a-half years in office have been rocked by waves of social unrest.Macron’s pension tsar Jean-Paul Delevoye is due to hold talks with the unions on Monday before the prime minister presents the broad outlines of the proposal to the public mid-week.Education Minister Jean-Michel Blanquer said far-reaching reform was needed to put the generous pension system on a sustainable footing. Fewer teachers went on strike on Friday, education ministry data showed.“It would be much easier for us to do nothing,” Blanquer told BFM TV. “We could see through this five-year term without enacting deep reform. But if every presidency reasons in this way, our children will not have an acceptable pension system.”Police had used tear gas in central Paris on Thursday afternoon when hooded protesters on the fringes of the trade unions’ march threw fireworks at officers, ransacked bus stops, and set fire to rubbish bins.More than 800,000 people rallied in protests countrywide on Thursday. Union leaders put the numbers higher.“There’s a noise in the streets, I hope the windows of the Elysee are open,” said Philippe Martinez, secretary-general of the CGT union, referring to the president’s office.Macron wants to simplify the unwieldy pension system, which comprises more than 40 different plans. Rail workers and mariners can, for instance, retire up to a decade earlier than the average worker.The president says the system is unfair and too costly and that the French will have to work longer, though he appears reluctant to simply raise the retirement age of 62.One alternative is to curb benefits for those who stop working before 64 and give a benefits boost to those who leave later.&quot;,
&quot;The French -- and particularly Parisians -- are face to face with what may be the largest strike in the country's history.On the heals of the Yellow Vest protests, employees of various sectors are preparing to go on indefinite strike beginning Thursday to protest pension reforms by the government of French President Emmanuel Macron.The walkout was sealed when the government announced its determination to implement pension reform despite pushback.According to France’s National Institute of Statistics and Economic Studies, Macron has further fueled the sense of anger and rebellion among French people against their presidents, with his economic policies that have given the wealthy a greater share of national income since his inauguration on May 17, 2017.He has been facing the biggest crisis since the yellow vest protests.The reform would lift the privileges granted to civil servants and gradually increasing the retirement age from 62 to 64. It is expected to adversely affect many sectors.Long list of strikersAmong the strikers will be employees of national carrier Air France, state-owned Parisian public transport operator RATP, electricity company EDF that is largely owned by the government, state-owned national railway firm SNCF, and automobile manufacturer Renault.Police, healthcare professionals, teachers, lawyers, taxi and freight drivers, postal workers, farmers, civil servants, refinery workers and students will also participate.Over half of all schools across the country will be suspended, while nearly all commuter trains and buses will halt and or work in intermittently. Air France will cancel 30% of its flights.The Yellow Vest protests started Nov. 17, 2018 in reaction to rising fuel costs and economic injustice, later spiraling into deadly anti-government riots.Protesters used yellow vests, part of the standard safety kit in French cars, to make their members more easily visible.The demonstrations left 11 dead and more than 4,000 injured including protesters and the police, according to government figures.Activists claim that 24 protesters were blinded in one eye and that five lost one of their hands.At least some 8,400 people have been arrested since the beginning of the Yellow Vest protests, and 2,000 were remanded into custody.A total of 17 protestors were arrested in Toulouse and five people -- two police and three civilians -- were injured.&quot;,
&quot;PARIS-The Eiffel Tower shut down, France’s vaunted high-speed trains stood still and several thousand people protested in Paris as unions launched open-ended, nationwide strikes Thursday over the government’s plan to overhaul the retirement system.Paris authorities barricaded the presidential palace and deployed 6,000 police as activists - many in yellow vests representing France’s year-old movement for economic justice - gathered in the capital in a mass outpouring of anger at President Emmanuel Macron and his centerpiece reform.Unions and their supporters fear that the changes to how and when workers can retire will threaten the hard-fought French way of life. Macron himself remained “calm and determined” to push it through, according to a top presidential official.The Louvre Museum warned of strike disruptions, and subway stations across Paris shut their gates. Many visitors - including the U.S. energy secretary - canceled plans to travel to one of the world’s most-visited countries amid the strike. Unprepared tourists discovered historic train stations standing empty Thursday, with about nine out of 10 of high-speed TGV trains canceled. Signs at Paris’ Orly Airport showed “canceled” notices, as the civil aviation authority announced 20% of flights were grounded.Some travelers showed support for the striking workers, but others complained about being embroiled in someone else’s fight. “I had no idea about the strike happening, and I was waiting for two hours in the airport for the train to arrive and it didn’t arrive,” said vacationer Ian Crossen, from New York. “I feel a little bit frustrated. And I’ve spent a lot of money. I’ve spent money I didn’t need to, apparently.”Vladimir Madeira, a Chilean tourist vacationing in Paris, said the strike has been “a nightmare.” He hadn’t heard about the protest until he arrived, and transport disruptions foiled his plans to travel directly to Zurich.Beneath the closed Eiffel Tower, tourists from Thailand, Canada and Spain echoed those sentiments. Bracing for possible violence along the route of the Paris march, police ordered all businesses, cafes and restaurants in the area to close. Authorities banned protests in the more sensitive neighborhoods around the Champs-Elysees avenue, presidential palace, parliament and Notre Dame Cathedral.Police carried out security checks of more than 6,000 people arriving for the protest and detained 65 even before it started. Embassies warned tourists to avoid the protest area. The mood was impassioned in the crowd massed on Boulevard Magenta in eastern Paris.Health workers showed up to decry conditions in hospitals. Students pointed to recent student suicides and demanded government action. Environmentalists emphasized that climate justice and social justice are one and the same. And young and old roundly condemned the new retirement plan, which they fear would take money out of their pockets and reduce the period of repose the French expect in the last decades of their lives.Eric Mettling, who joined the yellow vests at the start of their movement, said the general strike had brought together social movements across France in a manner unprecedented in recent memory to denounce “the social crisis.”Skirmishes broke out between police firing tear gas and protesters throwing flares at a protest in the western French city of Nantes, and thousands of red-vested union activists marched through cities from Marseille on the Mediterranean to Lille in the north.Lacking public transport, commuters used shared bikes or electric scooters despite near-freezing temperatures. Many workers in the Paris region worked from home or took a day off to stay with their children, since 78% of teachers in the capital were on strike.The big question is how long the strike will last. Transport Minister Elisabeth Borne said she expects the travel troubles to be just as bad Friday, and unions said they’ll maintain the Paris subway system strike at least through Monday. Public sector workers fear Macron’s reform will force them to work longer and shrink their pensions. Some private sector workers share their worries, while others welcome the reform.Joseph Kakou, who works an overnight security shift in western Paris, walked an hour to get home to the eastern side of town Thursday morning. “It doesn’t please us to walk. It doesn’t please us to have to strike,” Kakou told The Associated Press. “But we are obliged to, because we can’t work until 90 years old.”To Macron, the retirement reform is central to his plan to transform France so it can compete globally in the 21st century. The government argues France’s 42 retirement systems need streamlining. While Macron respects the right to strike, he “is convinced that the reform is needed, he is committed, that’s the project he presented the French in 2017” during his election campaign, the presidential official said. The official was not authorized to be publicly named.After extensive meetings with workers, the high commissioner for pensions is expected to detail reform proposals next week, and the prime minister will release the government’s plan days after that.&quot;,
&quot;Protesters mobilized across France on Thursday in a nationwide strike challenging President Emmanuel Macron’s controversial pension reform plans.The Interior Ministry said 806,000 people took part, while labor unions put the number at nearly 1.5 million.Some 250,000 people took part in the protests in Paris, where police used smoke bombs to disperse the crowd.The unlimited strike impacted all public transport systems in the country, according to local media reports.A total of 90 people have been arrested so far in Paris, police said.Some train, subway and bus services were canceled and many schools were closed while the law and order situation led to the cancelation of 20% of flights to the country.In a tweet, the Paris Police Department said it had conducted 6,476 checks. Labor unions said the strike will continue until Monday.The Gare du Nord, a station of the SNCF railway network in Paris, was almost empty in the morning, according to broadcaster France 24.Protesters, however, made their way to the Gare du Nord in the afternoon to attend the main march to Place de la Nation square.They included police, healthcare professionals, teachers, lawyers, taxi and freight drivers, postal workers, farmers, civil servants, refinery workers and students, according to the Le Monde daily.The walkout came after the government announced its determination to implement pension reform despite a nationwide outcry.According to France’s National Institute of Statistics and Economic Studies, Macron has further fueled the sense of anger and rebellion among French people against their president with his economic policies that have given wealthy people a greater share of national income since his inauguration on May 17, 2017.He has been facing the biggest crisis since the beginning of the Yellow Vest protests in October last year.Proposed reformFrance currently has 42 different pension programs for different sectors, but the government proposed to unify them into one pension scheme.France’s current program is based on the principle of solidarity between generations under which the working population finances the pensioners of that year.But due to the aging population, fewer people are paying into the current system.To fix this, the government introduced a point-based system that would compensate workers with pension points for every day they work or every euro they contribute.The reform would lift the privileges granted to civil servants and gradually increase the retirement age from 62 to 64, a move expected to adversely affect many sectors.Workers will get a full pension if they retire at the age of 64. If they retired before, they would lose 5% of their pensions for every year they retire early.They would also gain a 5% increase in their pensions for every year if they retire after the age of 64.The demonstrations and strikes have been supported by numerous labor and police unions as well as the Yellow Vests.Macron paused his overseas visits for a while to focus on a solution to the problems caused by the strikes and demonstrations.&quot;,
&quot;Paris-A strike over planned pension reforms that paralysed France on Thursday has entered its second day.Several unions, including rail and metro workers, voted to extend the strike action, meaning another day of major disruptions to key services.It comes after more than 800,000 people protested on Thursday, with violent clashes reported in a number of cities.Workers are angry about planned pension reforms that would see them retiring later or facing reduced payouts.France currently has 42 different pension schemes across its private and public sectors, with variations in retirement age and benefits. President Emmanuel Macron says his plans for a universal points-based system would be fairer, but many disagree.Rail workers voted to extend their strike through Friday, while unions at the Parisian bus and metro operator said their walkout would continue until at least Monday.Numerous rush-hour trains into Paris were cancelled on Friday and 10 out of 16 metro lines were closed, while others ran limited services, Reuters news agency reports.Traffic jams of more than 350km (217 miles) were reported on major roads in and around the capital.A number of flights have also been disrupted, while many schools are expected to remain shuttered and hospitals understaffed. Protesters sang songs against President Macron in ParisMr Macron’s government has reportedly made plans to deal with the strike action at the weekend.Some trade union leaders have vowed to strike until Mr Macron abandons his campaign promise to overhaul the retirement system.“We’re going to protest for a week at least, and at the end of that week it’s the government that’s going to back down,” 50-year-old Paris transport employee Patrick Dos Santos told Reuters.What happened on Thursday?French police gave the figure of 800,000 people taking to the streets across the country, including 65,000 in Paris. Union leaders put the numbers higher, with the CGT union saying 1.5m people turned out across France.The disruption meant popular tourist sites in Paris, including the Eiffel Tower, were closed for the day and usually busy transport hubs like the Gare du Nord were unusually quiet.&quot;,
&quot;Paris (AFP): France was on Saturday expecting its most serious nationwide strike in years to paralyse the country over the weekend, with unions warning the turmoil would last well into next week.&quot;,
&quot;PARIS: The French government on Friday expressed determination to plough ahead with far-reaching pension reforms in the face of the biggest strikes in years, which have brought public transport in much of the country to a standstill.The strikes, which began on Thursday, have seen most high-speed trains cancelled, flights affected and most of the Paris metro shut down in a major challenge to the ambitious reform agenda of President Emmanuel Macron.The turmoil is expected to continue over the weekend and through until at least Tuesday when unions have called more nationwide protests to follow mass rallies on Thursday that brought over 800,000 people onto the streets.With Macron not yet speaking publicly about the strikes and seeking for now to rise above the fray, Prime Minister Edouard Philippe insisted that the government would not abandon a plan which would require the French  “to work a bit longer.” He pledged to work with trade unions to introduce a single  “fairer”, points-based pension scheme for all, scrapping the 42 more advantageous plans currently enjoyed by train drivers, soldiers and a host of other workers in the process.The centre-right premier added that the government was  “very determined” to implement the reform, adding he did not believe the French would always accept a situation where some retire earlier, and with more money than others doing comparable jobs.But he emphasised that the changes, which he said would be unveiled on Wednesday, would be introduced  “progressively, without harshness”. “My logic will never be one of confrontation,” he said.Dozens of trains, metros and flights were cancelled, many schools were again closed or offering only daycare, and four of the country’s eight oil refineries remained blocked on Friday.Rail operator SNCF has already halted ticket sales through the weekend, with 90 percent of high-speed TGV trains again cancelled on Friday and little improvement expected over the weekend.Half of the Eurostar trains between Paris and London were dropped, and just two of three Thalys trains serving Paris, Brussels and Amsterdam were running.“I was supposed to take a train to Metz (northeast France), I reserved my ticket three days ago but it’s been cancelled and I’ve gotten no information,” Rachel Pallamidessi said at a deserted station in the city of Strasbourg.Several airlines cancelled flights as air traffic controllers walked off the job, with Air France cancelling 30 percent of domestic flights and 10 percent of nearby international routes.In Paris, nine of the capital’s 16 metro lines were shut while many others were running only during rush hours, prompting commuters to turn to bicycles, electric scooters and other alternatives or to work from home.It remains to be seen if the protests will match the magnitude of the 1995 strikes against pension overhauls when France was paralysed for three weeks from November to December, ultimately forcing the government to back down. The walkout is the latest test of Macron’s mettle after months of protests from teachers, hospital workers, police and firefighters, capping a year of social unrest triggered by the  “yellow vest” protest movement.Unions say Macron’s proposal for a single pension system would force millions of people in both the public and private sectors to work well beyond the official retirement age of 62.At least 800,000 took part in rallies around the country on Thursday, according to the interior ministry, one of the biggest demonstrations of union strength in nearly a decade.Another day of strikes and rallies has been called for Tuesday, a day after union leaders are to meet again with government officials over the pension reform.“There were lots of people on strike, now we need even more if we want to influence these decisions,” Philippe Martinez of the hard-line CGT union told LCI television.While most of the rallies were peaceful, police fired tear gas to disperse dozens of black-clad protesters smashing windows and throwing stones during the Paris march, with one construction trailer set on fire.Several dozens of people were arrested, and three journalists were injured after reportedly being hit by tear gas or stun grenades, including a Turkish journalist who was struck in the face.Published in Dawn, December 7th, 2019Copyright © 2019, DawnScribe Publishing Platform&quot;,
]
</code></pre>
<p>The code is as follows.</p>
<pre><code>from bertopic import BERTopic
# docs=[i for i in all_text if type(i)==str]
# docs=docs.T
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)
</code></pre>
<p>Any help is much appreciated
Thanks</p>
",Text Generation & LLMs,valueerror k must le equal number training point trying bertopic cluster sentence actually employed agglomerative clustering using bert sentence embeddings result ha many cluster one code follows help much appreciated thanks
How to save the gpt-2-simple model after training?,"<p>I trained the <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">gpt-2-simple</a> chat bot model but I am unable to save it. It's important for me to download the trained model from colab because otherwise I have to download the 355M model each time (see below code).</p>
<p>I tried various methods to save the trained model (like <code>gpt2.saveload.save_gpt2()</code>), but none worked and I don't have any more ideas.</p>
<p>My training code:</p>
<pre class=""lang-py prettyprint-override""><code>%tensorflow_version 2.x
!pip install gpt-2-simple

import gpt_2_simple as gpt2
import json

gpt2.download_gpt2(model_name=&quot;355M&quot;)

raw_data = '/content/drive/My Drive/data.json'

with open(raw_data, 'r') as f:
    df =json.load(f)

data = []

for x in df:
    for y in range(len(x['dialog'])-1):
        a = '[BOT] : ' + x['dialog'][y+1]['text']
        q = '[YOU] : ' + x['dialog'][y]['text']
        data.append(q)
        data.append(a)

with open('chatbot.txt', 'w') as f:
     for line in data:
        try:
            f.write(line)
            f.write('\n')
        except:
            pass

file_name = &quot;/content/chatbot.txt&quot;

sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              dataset=file_name,
              model_name='355M',
              steps=500,
              restore_from='fresh',
              run_name='run1',
              print_every=10,
              sample_every=100,
              save_every=100
              )

while True:
  ques = input(&quot;Question : &quot;)
  inp = '[YOU] : '+ques+'\n'+'[BOT] :'
  x = gpt2.generate(sess,
                length=20,
                temperature = 0.6,
                include_prefix=False,
                prefix=inp,
                nsamples=1,
                )
</code></pre>
",Text Generation & LLMs,save gpt simple model training trained gpt simple chat bot model unable save important download trained model colab otherwise download model time see code tried various method save trained model like none worked idea training code
Try to make fine-tuning of model gpt like,"<p>I try to make fine tuning on essays, my datset looks like:</p>
<pre><code>[ {
        &quot;topic&quot;: &quot;Мы были достаточно цивилизованны, чтобы построить машину, но слишком примитивны, чтобы ею пользоваться». (Карл Краус)&quot;,
        &quot;text&quot;: &quot;Высказывание Карла Крауса, австрийского писателя, о том, что «мы были достаточно цивилизованны, чтобы построить машину... }]
</code></pre>
<p>There is the code:</p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelWithLMHead, Trainer, TrainingArguments
model_name = &quot;tinkoff-ai/ruDialoGPT-medium&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelWithLMHead.from_pretrained(model_name)
import json

def prepare_data(filepath):
    with open(filepath, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        data = json.load(f)
    prompts = [example[&quot;topic&quot;] for example in data]
    texts = [example[&quot;text&quot;] for example in data]
    inputs = []
    for i in range(len(prompts)):
        inputs.append(prompts[i] + texts[i])
    return inputs
    
train_inputs = prepare_data(&quot;train.json&quot;)
test_inputs = prepare_data(&quot;test.json&quot;)
from transformers import TextDataset, DataCollatorForLanguageModeling

train_dataset = TextDataset(tokenizer=tokenizer, file_path=&quot;train.json&quot;, block_size=128)
test_dataset = TextDataset(tokenizer=tokenizer, file_path=&quot;test.json&quot;, block_size=128)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=&quot;./models&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
)

trainer.train()
</code></pre>
<p>But it takes a lot of memory, and i can't fin-tune it in Kaggle. The output has &gt;19gb.
Is it normal or how can i fix it?
[2501/7802 59:11 &lt; 26:52, 2.06 it/s, Epoch 0.86/3] every 10 minutes it takes 2-3 GB, is it normal?</p>
",Text Generation & LLMs,try make fine tuning model gpt like try make fine tuning essay datset look like code take lot memory fin tune kaggle output ha gb normal fix epoch every minute take gb normal
Changes in GPT2/GPT3 model during few shot learning,"<p>During transfer learning, we take a pre-trained network and some observation pair (input and label), and use these data to fine-tune the weight by use of backpropagation. However, during one shot/few shot learning, according to this paper- 'Language Models are Few-Shot Learners' (<a href=""https://arxiv.org/pdf/2005.14165.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2005.14165.pdf</a>), &quot;No gradient updates are performed&quot;. Then what changes happen to the models like GPT2 and GPT3 during one shot/few shot learning?</p>
",Text Generation & LLMs,change gpt gpt model shot learning transfer learning take pre trained network observation pair input label use data fine tune weight use backpropagation however one shot shot learning according paper language model shot learner gradient update performed change happen model like gpt gpt one shot shot learning
Extracting Features from BertForSequenceClassification,"<p>Hello together currently I´m trying to develop a model for contradicition detection. Using and fine-tuning a BERT Model I already got quite statisfactionary result but I think with with some other features I could get a better accuracy. I oriented myself on this <a href=""https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db"" rel=""nofollow noreferrer"">Tutorial</a>. After fine-tuning, my model looks like this:</p>
<pre><code>==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30000, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)

==== First Transformer ====

bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)
bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)

==== Output Layer ====

bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
</code></pre>
<p>My next step would be to get the [CLS] token from this model, combine it with a few hand crafted features and feed them into a different model (MLP) for classfification. Any hints how to do this?</p>
",Text Generation & LLMs,extracting feature bertforsequenceclassification hello together currently trying develop model contradicition detection using fine tuning bert model already got quite statisfactionary result think feature could get better accuracy oriented tutorial fine tuning model look like next step would get cl token model combine hand crafted feature feed different model mlp classfification hint
Is there a maximum sequence length for the output of a transformer?,"<p>There's just one thing that I can't find an answer to :
When putting the ouput back in the transformer, we compute it similarly to the inputs (with added masks), so is there also a sequence size limit ?</p>
<p>Even BERT has an input size limit of 512 tokens, so transformers are limited in how much they can take in.
So is there something to make the output length as big as wanted or is there a fixed max length ?</p>
<p>If I wasn't clear enough, does the network generate words infinitely until the &lt; end &gt; token or is there a token limit for the outputs?</p>
",Text Generation & LLMs,maximum sequence length output transformer one thing find answer putting ouput back transformer compute similarly input added mask also sequence size limit even bert ha input size limit token transformer limited much take something make output length big wanted fixed max length clear enough doe network generate word infinitely end token token limit output
Replacing GELU with ReLU in BERT inference,"<p>I have a question for what concerns BERT.
Actually it uses GELU activation function since it performs better than ReLU, but this is because of the gradient near zero. In inference, we do not really care about gradients (are we?), so would it be a stupid idea to replace GELU with ReLU at inference (even at the cost of slightly losing accuracy) for achieving higher throughput?</p>
<p>I expect it to be really similar to GELU, since the difference is really tiny (not for what concerns gradient, but that's another story)</p>
",Text Generation & LLMs,replacing gelu relu bert inference question concern bert actually us gelu activation function since performs better relu gradient near zero inference really care gradient would stupid idea replace gelu relu inference even cost slightly losing accuracy achieving higher throughput expect really similar gelu since difference really tiny concern gradient another story
Spacy similarity score for sweet M&amp;M fails,"<p>python 3 spacy seems to have a problem with sweets such as M&amp;Ms.</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_lg&quot; )

query = nlp( &quot;M&amp;M&quot; )
query2 = nlp(&quot;M&amp;M chocolate pouch&quot;)
print( &quot;Score of M&amp;M versus the full name in shop:&quot;, query2.similarity(query) )
</code></pre>
<p>The resultant score returned is <em><strong>0.0</strong></em>. And if query2 is any possible string, the result is always 0.0.</p>
<p>However, if you space separate M&amp;M to make &quot;M &amp; M&quot; then the scores are reasonable.</p>
<p>Does anyone know why it fails with a large language model on such a confectionary? And is there a solution to find the correct similarity score without synthetically adding in spaces around the ampersand?</p>
",Text Generation & LLMs,spacy similarity score sweet fails python spacy seems problem sweet resultant score returned query possible string result always however space separate make score reasonable doe anyone know fails large language model confectionary solution find correct similarity score without synthetically adding space around ampersand
Extracting probability from BERT next sentence prediction embeddings,"<p>I'm using BERT's next sentence prediction module (<code>BertForNextSentencePrediction</code>) and am trying to get the probability that sentence B follows sentence A. However, the numbers BERT outputs are almost always very close to 1 (or else very close to 0), seeming to indicate some sort of boolean logic. How can I get BERT to instead output the <em>probability</em> that B follows A?</p>
<p>My current code is as follows, and is based on some articles that were about using BERT for next-sentence-prediction probability.</p>
<pre><code>from transformers import BertTokenizer, BertForNextSentencePrediction
from torch.nn import functional as F

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')

def next_sentence_prediction(sentence_1, sentence_2):
  encoding = tokenizer.encode_plus(sentence_1, sentence_2, return_tensors='pt')
  outputs = nsp_model(**encoding)[0]
  softmax = F.softmax(outputs, dim = 1)
  print(softmax[0][0].item())
</code></pre>
<p>This does not give me the probability I'm looking for. For example, &quot;I might go to the store today.&quot; and &quot;Do you like butter on your toast?&quot; gives a score of 0.9999562501907349, when in actuality I feel like the probability for this pair of sentences should be very low. On the other end of the spectrum, &quot;I might go to the store today.&quot; and &quot;&quot;The man coughed.&quot; gives the almost negligible number of 4.5933375076856464e-05, when in actuality the probability should be low, but not non-existent.</p>
<p>Any ideas for how to extract the probability of B following A using BERT?</p>
",Text Generation & LLMs,extracting probability bert next sentence prediction embeddings using bert next sentence prediction module trying get probability sentence b follows sentence however number bert output almost always close else close seeming indicate sort boolean logic get bert instead output probability b follows current code follows based article using bert next sentence prediction probability doe give probability looking example might go store today like butter toast give score actuality feel like probability pair sentence low end spectrum might go store today man coughed give almost number e actuality probability low non existent idea extract probability b following using bert
Text generation AI models generating repeated/duplicate text/sentences. What am I doing incorrectly? Hugging face models - Meta GALACTICA,"<p>Whole day I have worked with available text generation models</p>
<p>Here you can find list of them : <a href=""https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads"" rel=""nofollow noreferrer"">https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads</a></p>
<p>I want to generate longer text outputs, however, with multiple different models, all I get is repetition.</p>
<p>What am I missing or doing incorrectly?</p>
<p>I will list several of them</p>
<p>Freshly released meta GALACTICA - <a href=""https://huggingface.co/facebook/galactica-1.3b"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/galactica-1.3b</a></p>
<p>The code example</p>
<pre><code>from transformers import AutoTokenizer, OPTForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/galactica-1.3b&quot;)
model = OPTForCausalLM.from_pretrained(&quot;facebook/galactica-1.3b&quot;, device_map=&quot;auto&quot;)

 
input_text = &quot;The benefits of deadlifting\n\n&quot;
input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).input_ids.to(&quot;cuda&quot;)

outputs = model.generate(input_ids,new_doc=False,top_p=0.7, max_length=1000)
print(tokenizer.decode(outputs[0]))
</code></pre>
<p>The generated output</p>
<p><a href=""https://i.sstatic.net/zV7qg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zV7qg.png"" alt=""enter image description here"" /></a></p>
<p>Facebook opt - <a href=""https://huggingface.co/facebook/opt-350m"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/opt-350m</a></p>
<p>The tested code</p>
<pre><code>from transformers import GPT2Tokenizer, OPTForCausalLM

model = OPTForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;)
tokenizer = GPT2Tokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)

prompt = &quot;The benefits of deadlifting can be listed as below:&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)

# Generate
generate_ids = model.generate(inputs.input_ids, max_length=800)
tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
</code></pre>
<p>The generated output</p>
<p><a href=""https://i.sstatic.net/zv2j9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zv2j9.png"" alt=""enter image description here"" /></a></p>
",Text Generation & LLMs,text generation ai model generating repeated duplicate text sentence incorrectly hugging face model meta galactica whole day worked available text generation model find list want generate longer text output however multiple different model get repetition missing incorrectly list several freshly released meta galactica code example generated output facebook opt tested code generated output
Train GPT-2 on custom data,"<p>I was looking for a way to train my own textual data using GPT-2 &amp; I have found a blog post here: <a href=""https://www.kaggle.com/code/ashiqabdulkhader/train-gpt-2-on-custom-language"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/ashiqabdulkhader/train-gpt-2-on-custom-language</a></p>
<p>Everything works fine, the model building, dataset building, but it shows very weird texts as output...</p>
<pre><code>from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

output_dir = &quot;kaggle/working/gpt2&quot;
tokenizer = GPT2Tokenizer.from_pretrained(output_dir)
model = TFGPT2LMHeadModel.from_pretrained(output_dir, pad_token_id=tokenizer.eos_token_id)

text = &quot;what is python?&quot;
input_ids = tokenizer.encode(text, return_tensors='tf')
beam_output = model.generate(
 input_ids,
 max_length=50,
 num_beams=5,
 temperature=0.7,
 no_repeat_ngram_size=2,
 num_return_sequences=5
)

print(tokenizer.decode(beam_output[0], skip_special_tokens=True))
</code></pre>
<p>My corpus data is:</p>
<pre class=""lang-none prettyprint-override""><code>Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[33]

Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a &quot;batteries included&quot; language due to its comprehensive standard library.[34][35]

Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0.[36] Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.[37]

Python consistently ranks as one of the most popular programming languages.[38][39][40][41]
</code></pre>
<p>taken from Wikipedia.</p>
<p>The output: <code>what is python 202gragra 202 202 language 202] 202astast 202abilityability 2027ely 202uralural 202ralral Rossum Rossum 202use 202 as Rossumability code Rossum] Rossumifi Rossumleas Rossumast Rossum80</code></p>
<p>It also says:</p>
<blockquote>
<p>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>I am completely new to this NLP section... what is the issue here?</p>
",Text Generation & LLMs,train gpt custom data wa looking way train textual data using gpt found blog post everything work fine model building dataset building show weird text output corpus data taken wikipedia output also say probably train model stream task able use prediction inference completely new nlp section issue
How to fine tune BERT on unlabeled data?,"<p>I want to fine tune BERT on a specific domain. I have texts of that domain in text files. How can I use these to fine tune BERT?
I am looking <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforpretraining"" rel=""noreferrer"">here</a> currently.</p>

<p>My main objective is to get sentence embeddings using BERT.</p>
",Text Generation & LLMs,fine tune bert unlabeled data want fine tune bert specific domain text domain text file use fine tune bert looking currently main objective get sentence embeddings using bert
Forcing transformer models to generate only some tokens from a vocab,"<p>I trained a language model (encoder-decoder) to generate text. I want to restrict the generation vocab of this model to a specific vocab. How can I do that?</p>
<p>I found in <code>generate</code> (<a href=""https://huggingface.co/docs/transformers/v4.22.2/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"" rel=""nofollow noreferrer"">model.generate</a>) function that I can pass a parameter called <code>force_words_ids</code> where the model will be forced to generate &quot;all&quot; the tokens in this list. I am looking for something similar, but instead, to generate some of the list's tokens.</p>
",Text Generation & LLMs,forcing transformer model generate token vocab trained language model encoder decoder generate text want restrict generation vocab model specific vocab found model generate function pas parameter called model forced generate token list looking something similar instead generate list token
what is gpt-1 optimization method?,"<p>I'm reviewing the paper &lt;Improving Language Understanding by Generative Pre-Training(2018)&gt;.
The concept is well understood, but there is a question about the optimization method used in the paper.</p>
<p><a href=""https://i.sstatic.net/CrjYL.png"" rel=""nofollow noreferrer"">SGD</a></p>
<p><a href=""https://i.sstatic.net/z0gyb.png"" rel=""nofollow noreferrer"">Adam</a></p>
<p>In the first image, it looked like SGD was used for pre-training, but the second image showed that Adam was used. I'm posting a question because I think I might have misunderstood the concept!</p>
<p>I think they used Adam, but I wonder where SGD was used for.</p>
",Text Generation & LLMs,gpt optimization method reviewing paper improving language understanding generative pre training concept well understood question optimization method used paper sgd adam first image looked like sgd wa used pre training second image showed adam wa used posting question think might misunderstood concept think used adam wonder sgd wa used
Which are best for Name Entity Recognition for Gujarati Language Text?,"<p>I am finding out the best working models for Name Entity Recognition in Gujarati Text. I know only 1 of them that is Indic Bert model of hugging face. Can anyone suggest other model which documentation or code available for Name Entity Recognition in Gujarati Language??</p>
<p>I found only IndicBERT model of Hugging Face. I want know other mode or any link where the code is available for Name Entity Recognition.</p>
",Text Generation & LLMs,best name entity recognition gujarati language text finding best working model name entity recognition gujarati text know indic bert model hugging face anyone suggest model documentation code available name entity recognition gujarati language found indicbert model hugging face want know mode link code available name entity recognition
How to access BERT&#39;s inter layer?,"<p>I want to put [batch_size, 768, text_length] tensor into
6th layer of BERT.</p>
<ol>
<li><p>How can I give input to 6th layer?</p>
</li>
<li><p>Can I take just 6~last layer of BERT then use it?</p>
</li>
</ol>
<p>Thank you.</p>
",Text Generation & LLMs,access bert inter layer want put batch size text length tensor th layer bert give input th layer take last layer bert use thank
Using EluetherAPI GPT models for NLP tasks,"<p>EluetherAPI released many GPT models based on the PILE dataset, which is equivalent to original GPT models. As they are trained on a larger dataset, we can perform multiple NLP tasks on the same model without retraining the model, with just a few prompts, or by providing some context using few-shot learning.</p>
<p>I am trying to achieve the same. But the problem is the return text is sometimes too large or too short. Here is my example code:</p>
<pre><code>generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B', device=0)
prompt= &quot;&quot;&quot;[Original]: The diplomatic spat came days after France cut the number of visas it issues for citizens of Algeria and other North African countries.
[Paraphrase]: &quot;&quot;&quot;
result = generator(prompt, do_sample=True, min_length=10, max_new_tokens=50, top_p=0.9, temperature=1)
</code></pre>
<p>the result gave me this:</p>
<pre><code>France has been forced to temporarily remove two of its citizens who are on a tourist visa from Algeria and Morocco, which have had a long and acrimonious history over the past decade.
[Original]: The two visa holders, who
</code></pre>
<p>As you can see, it gives me a result with the input text included, I removed the input text, and it works fine but at the end, it still shows the [Original]: prompt. How to remove it and give the exact same results?</p>
<p>I tried multiple times, even providing it context, but it works fine sometime and sometime not. I even tried few-shot learning with data as:</p>
<pre><code>&quot;&quot;&quot;[Original]: Algeria recalled its ambassador to Paris on Saturday and closed its airspace to French military planes a day later after the French president made comments about the northern Africa country. 
[Paraphrase]: Last Saturday, the Algerian government recalled its ambassador and stopped accepting French military airplanes in its airspace. It happened one day after the French president made comments about Algeria.
###
[Original]: President Macron was quoted as saying the former French colony was ruled by a &quot;political-military system&quot; with an official history that was based not on truth, but on hatred of France.
[Paraphrase]: Emmanuel Macron said that the former colony was lying and angry at France. He also said that the country was ruled by a &quot;political-military system&quot;.
###
[Original]: The diplomatic spat came days after France cut the number of visas it issues for citizens of Algeria and other North African countries.
[Paraphrase]: Diplomatic issues started appearing when France decided to stop granting visas to Algerian people and other North African people.
###
[Original]: After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.
[Paraphrase]:&quot;&quot;
</code></pre>
<p>I want to know if is there any way to pass the end_sequence so that it will stop generating after that, also the parameters top_p and temperature to get good results.</p>
",Text Generation & LLMs,using eluetherapi gpt model nlp task eluetherapi released many gpt model based pile dataset equivalent original gpt model trained larger dataset perform multiple nlp task model without retraining model prompt providing context using shot learning trying achieve problem return text sometimes large short example code result gave see give result input text included removed input text work fine end still show original prompt remove give exact result tried multiple time even providing context work fine sometime sometime even tried shot learning data want know way pas end sequence stop generating also parameter top p temperature get good result
BCELoss between logits and labels not working,"<p>I am using a GPT2 model that outputs <code>logits</code> (before softmax) in the shape <code>(batch_size, num_input_ids, vocab_size)</code> and I need to compare it with the labels that are of shape <code>(batch_size, num_input_ids)</code> to calculate BCELoss. How do I calculate it?</p>
<pre><code>logits = output.logits #--of shape (32, 56, 592)
logits = torch.nn.Softmax()(logits)
labels = labels #---------of shape (32, 56)

torch.nn.BCELoss()(logits, labels)
</code></pre>
<p>but the dimensions do not match, so how do I contract <code>logits</code> to <code>labels</code> shape or expand <code>labels</code> to <code>logits</code> shape?</p>
",Text Generation & LLMs,bceloss logits label working using gpt model output softmax shape need compare label shape calculate bceloss calculate dimension match contract shape expand shape
Build a model that answers question from dataset using GPT3,"<p>I am trying to build a chat bot, that given some text corpus, will answer questions when we ask something from that text. I have heard GPT3 is a beast and requires minimum training. Are there any links/ tutorial/github repo's that will help me get started with this?</p>
",Text Generation & LLMs,build model answer question dataset using gpt trying build chat bot given text corpus answer question ask something text heard gpt beast requires minimum training link tutorial github repo help get started
"OpenAI API and GPT-3, not clear how can I access or set up a learning/dev?","<p>I am reading tons of GPT-3 samples, and came cross many code samples.
None of them mentions that how and where I can run and play with the code myself... and especially not mentioning I can not.</p>
<p>So I did my research, and concluded, I can not, but I may be wrong:</p>
<ul>
<li>There is no way to run the &quot;thing&quot; on-premises on a dev machine, it is a hosted service by definition (?)</li>
<li>As of now (Oct. 11th 2020) the OpenAI API is in invite only beta (?)</li>
</ul>
<p>Did I miss something?</p>
",Text Generation & LLMs,openai api gpt clear access set learning dev reading ton gpt sample came cross many code sample none mention run play code especially mentioning research concluded may wrong way run thing premise dev machine hosted service definition oct th openai api invite beta miss something
how to make BERT predict new token,"<p><strong>my problem looks like this:</strong></p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
fill_mask_pipeline_pre = pipeline(&quot;fill-mask&quot;, model=model, tokenizer=tokenizer)
sentence_test = &quot;Olaf is the chancellor of germany. [MASK] is the chancellor of germany.&quot;
prediction = fill_mask_pipeline_pre(sentence_test)[:3]
</code></pre>
<p>--&gt; the 1st prediction is &quot;Olaf&quot;, which is the result I wanted.</p>
<p>However, if sentence_test is this:</p>
<pre><code>sentence_test = &quot;Scholz is the chancellor of germany. [MASK] is the chancellor of germany.&quot;
</code></pre>
<p>... I want the prediction to be &quot;Scholz&quot;.</p>
<p>Unfortunately this in never predicted, because BERT does not know the word Scholz.
BertTokenizer.from_pretrained('bert-base-uncased') turns Scholz into &quot;sc&quot; and  &quot;##holz&quot; .</p>
<p>**Is there a way how the new token can be predicted?
**</p>
<p>I tried adding the token to the tokenizer, but it still isn't predicted.</p>
<p>I also fine- tuned the model on text that contains the word &quot;Scholz&quot; a lot after adding the new word to the tokenizer, but it does not work either.</p>
",Text Generation & LLMs,make bert predict new token problem look like st prediction olaf result wanted however sentence test want prediction scholz unfortunately never predicted bert doe know word scholz berttokenizer pretrained bert base uncased turn scholz sc holz way new token predicted tried adding token tokenizer still predicted also fine tuned model text contains word scholz lot adding new word tokenizer doe work either
How to store Word vector Embeddings?,"<p>I am using BERT Word Embeddings for sentence classification task with 3 labels. I am using Google Colab for coding. My problem is, since I will have to execute the embedding part every time I restart the kernel, is there any way to save these word embeddings once it is generated? Because, it takes a lot of time to generate those embeddings.</p>
<p>The code I am using to generate BERT Word Embeddings is -</p>
<pre><code>[get_features(text_list[i]) for text_list[i] in text_list]
</code></pre>
<p>Here, gen_features is a function which returns word embedding for each i in my list text_list.</p>
<p>I read that converting embeddings into bumpy tensors and then using np.save can do it. But I actually don't know how to code it.</p>
",Text Generation & LLMs,store word vector embeddings using bert word embeddings sentence classification task label using google colab coding problem since execute embedding part every time restart kernel way save word embeddings generated take lot time generate embeddings code using generate bert word embeddings gen feature function return word embedding list text list read converting embeddings bumpy tensor using np save actually know code
Is there any language model that is able to generate text given both beginning and ending sequences?,"<p>Generative language models like GPT-2 can generate text given beginning sequences, and <a href=""https://arxiv.org/abs/2012.08561"" rel=""nofollow noreferrer"">cloze Transformer</a> can fill in the one-word blank between two sequence. Is there any generative language model that can generate multi-word text that connects given beginning and ending sequences as an extension to both models like GPT-2 and cloze transformer?</p>
<p>Input:</p>
<pre><code>Beginning sequence: What is Love?
Ending sequence: Love is an emotional state, not a physical one.
</code></pre>
<p>Output:</p>
<pre><code>[What is Love?] Love is a state of mind, a feeling, 
an emotion, or a mental state. It is the feeling 
of being in love with someone or something. The 
word &quot;love&quot; is derived from the Latin word for 
&quot;to love,&quot; &quot;lēgēre,&quot; which means to feel, to be 
attracted to, and to love. [Love is an emotional 
state, not a physical one.]
</code></pre>
",Text Generation & LLMs,language model able generate text given beginning ending sequence generative language model like gpt generate text given beginning sequence cloze transformer fill one word blank two sequence generative language model generate multi word text connects given beginning ending sequence extension model like gpt cloze transformer input output
How to generate a meaningful sentence from words only?,"<p>I want to generate a sentence from list of words. I have tried n-gram model but it only generates the text from already existing sentence i.e. we input a sentence and it outputs the next generated words based on the value of n. Which model will be helpful to generate a meaningful sentence from only the list of words and which dataset should be used to train the model?</p>
",Text Generation & LLMs,generate meaningful sentence word want generate sentence list word tried n gram model generates text already existing sentence e input sentence output next generated word based value n model helpful generate meaningful sentence list word dataset used train model
How to generate multiple patches of a single string in BERT model,"<p>I am using a BERT model for generating text embeddings. My strings are like <code>There is pneumonia detected in the left corner</code>. When I <code>encode()</code> and pass a batch of 20 strings and I print the model output, it returns <code>[20 256]</code>, where 20 is the batch size and 256 is the size of each output vector. It means that it generates texts in the form of vectors/tensors each with a size of 256 [1 256].</p>
<pre><code>def create_text_encoder(
   num_projection_layers, projection_dims, dropout_rate, trainable=False):
   
   # Load the BERT preprocessing module.
   preprocess = hub.KerasLayer(
   &quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&quot;, name=&quot;text_preprocessing&quot;,)
   
   # Load the pre-trained BERT model to be used as the base encoder.
   bert = hub.KerasLayer(
    &quot;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&quot;, name=&quot;bert&quot;,)
  
   # Set the trainability of the base encoder.
   bert.trainable = trainable
  
   # Receive the text as inputs.
   inputs = layers.Input(shape=(), dtype=tf.string, name=&quot;text_input&quot;)
  
   # Preprocess the text.
   bert_inputs = preprocess(inputs)
  
   # Generate embeddings for the preprocessed text using the BERT model.
   embeddings = bert(bert_inputs)[&quot;pooled_output&quot;]
   # Project the embeddings produced by the model.
   outputs = project_embeddings(
    embeddings, num_projection_layers, projection_dims, dropout_rate)
  
   # Create the text encoder model.
   return keras.Model(inputs, outputs, name=&quot;text_encoder&quot;)
</code></pre>
<p>Now I want to divide each string into 5 patches after feeding this <code>There is pneumonia detected in the left corner</code> single string to my above model. At first, the model was generating an embedding size of [1 256] for a single string, now it will generate [5 256] for a single text. Five vectors for single text each with a shape of 256.</p>
<p>Is it possible? Have someone done it before?</p>
",Text Generation & LLMs,generate multiple patch single string bert model using bert model generating text embeddings string like pas batch string print model output return batch size size output vector mean generates text form vector tensor size want divide string patch feeding single string model first model wa generating embedding size single string generate single text five vector single text shape possible someone done
Choosing a good prompt for GPT-3,"<p>I am trying to generate a quiz from a text that look like this:</p>
<pre><code>Text: &quot;Mary has little lamb and John has a cow. The lamb is one month old. It eats grass and milk, which Mary brings from the the farm.&quot;

Keywords: &quot;lamb&quot;, &quot;cow&quot;, &quot;one month old&quot;, &quot;farm.&quot;

1. What does Mary have?
A. Lamb
B. Cow
C. Dog
D. Cat

A. Lamb

2. What does John have?
A. Cow
B. Lamb
C. Dog
D. Cat

A. Cow

3. How old is Mary's lamb?
A. One month old
B. One year old
C. Two months old
D. Two years old

etc.
</code></pre>
<p>It works perfectly when I don't give keywords, with multiple prompts, with all these answer options, correct answer, everything. <strong>The problem is when I want to generate these questions such that the correct answer is the keyword.</strong> I tried all kinds of prompts, even giving examples like what I did above, but it doesn't work.</p>
",Text Generation & LLMs,choosing good prompt gpt trying generate quiz text look like work perfectly give keywords multiple prompt answer option correct answer everything problem want generate question correct answer keyword tried kind prompt even giving example like work
gpt3 fine tuning with openai not learning,"<p>For my fine tuning jsonl files, I wanted a model that could predict the gender of the speaker given a statement. For instance, the prompt: &quot;i went to buy a skirt today&quot; has completion as &quot;female&quot;.</p>
<p>I created several examples and gave it to gpt3 to finetune. I then fed the sentence &quot;i went to pick my wife up from the shops&quot; to the resulting model. I expected to get a gender as response but I got a whole story about picking up my wife from the shops.</p>
<p>It's as if gpt-3 didn't learn anything from my fine tuning at all.</p>
<p>I have a few questions:</p>
<ol>
<li><p>Is fine tuning equivalent to writing a few examples in openai playground and getting gpt-3 to guess what comes next?</p>
</li>
<li><p>After fine tuning, do you only pay for the tokens in the prompt/completion of subsequent runs? So If I spend $100 training a model on a million examples, I will then only have to pay for the individual prompt/completion of subsequent calls?</p>
</li>
<li><p>The chat bot for instance, come with a context sentence before the back and forth exchange of 2 chat participants. Something like &quot;this is a conversation between a rude man named John and a young girl named Sarah&quot;. How can i incorporate such context into fine tuning structure of {&quot;prompt&quot;:&quot;...&quot;,&quot;completion&quot;:...&quot;}?</p>
</li>
</ol>
",Text Generation & LLMs,gpt fine tuning openai learning fine tuning jsonl file wanted model could predict gender speaker given statement instance prompt went buy skirt today ha completion female created several example gave gpt finetune fed sentence went pick wife shop resulting model expected get gender response got whole story picking wife shop gpt learn anything fine tuning question fine tuning equivalent writing example openai playground getting gpt guess come next fine tuning pay token prompt completion subsequent run spend training model million example pay individual prompt completion subsequent call chat bot instance come context sentence back forth exchange chat participant something like conversation rude man named john young girl named sarah incorporate context fine tuning structure prompt completion
Converting a dataset to CoNLL format. Label remaining tokens with O,"<p>I have a manually annotated dataset that contains records in the following format:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;id&quot;: 1,
    &quot;text&quot;: &quot;At the end of each fiscal quarter, for the four consecutive fiscal quarters ending as of such fiscal quarter end, from the date of the Third Amendment and until December 30, 1996, the Company shall maintain a fixed charge coverage ratio of not less than 1.25 to 1.0.&quot;,
    &quot;label&quot;: [
        [
            209,
            230,
            &quot;COV_3&quot;
        ],
        [
            379,
            390,
            &quot;VAL_3&quot;
        ]
    ],
}
</code></pre>
<p>In the example above, <code>&quot;label&quot;</code> represents the custom entities I have in my dataset. In the example shown above, the phrase <code>fixed charge coverage</code> is located at position <code>[309, 336]</code> and is given the label <code>COV_3</code>. Likewise, the phrase <code>1.25 to 1.0</code> is located at <code>[379, 390]</code> and is given the label <code>VAL_3</code>.</p>
<p>Now, I would like to fine-tune some transformer model like BERT on this dataset, however, I realised that the dataset must be in CoNLL format. Or at least, all the tokens of each datapoint must be labelled. Is there any way I can easily label the remaining tokens with label <code>&quot;O&quot;</code> or I can transform this dataset in the CoNLL format?</p>
",Text Generation & LLMs,converting dataset conll format label remaining token manually annotated dataset contains record following format example represents custom entity dataset example shown phrase located position given label likewise phrase located given label would like fine tune transformer model like bert dataset however realised dataset must conll format least token datapoint must labelled way easily label remaining token label transform dataset conll format
"Given a subject and an object, what methods can i use to inference a possible verb?","<p>Given a subject A and an object B, for example, A is &quot;Peter&quot;, B is &quot;iPhone&quot;, Peter can be 'playing' or 'using' iPhone, the verb varies depending on the context, in this case, what kinds of method can I use to inference a possible verb?</p>
<p>I assume a model, which can be BERT or other models, learns the correlation between subjects, verbs, and objects through a given corpus, but I don't really know about NLP. I am expecting some off-the-shell models, or models that can be used through simple fine-tuning.</p>
",Text Generation & LLMs,given subject object method use inference possible verb given subject object b example peter b iphone peter playing using iphone verb varies depending context case kind method use inference possible verb assume model bert model learns correlation subject verb object given corpus really know nlp expecting shell model model used simple fine tuning
Does Bert model need text?,"<p>Does Bert models need pre-processed text (Like removing special characters, stopwords, etc.) or I can directly pass my text as it is to Bert models. (HuggigFace libraries).</p>
<p><em>note</em>: Follow up question to: <a href=""https://stackoverflow.com/questions/70067901/string-cleaning-preprocessing-for-bert"">String cleaning/preprocessing for BERT</a></p>
",Text Generation & LLMs,doe bert model need text doe bert model need pre processed text like removing special character stopwords etc directly pas text bert model huggigface library note follow question href cleaning preprocessing bert
how to convert text to word embeddings using bert&#39;s pretrained model &#39;faster&#39;?,"<p>I'm trying to get word embeddings for clinical data using microsoft/pubmedbert. I have 3.6 million text rows. Converting texts to vectors for 10k rows takes around 30 minutes. So for 3.6 million rows, it would take around - 180 hours(8days approx).</p>
<blockquote>
<p>Is there any method where I can speed up the process?</p>
</blockquote>
<p>My code -</p>
<pre><code>from transformers import AutoTokenizer
from transformers import pipeline
model_name = &quot;microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('feature-extraction',model=model_name, tokenizer=tokenizer)

def lambda_func(row):
    tokens = tokenizer(row['notetext'])
    if len(tokens['input_ids'])&gt;512:
        tokens = re.split(r'\b', row['notetext'])
        tokens= [t for t in tokens if len(t) &gt; 0 ]
        row['notetext'] = ''.join(tokens[:512])
    row['vectors'] = classifier(row['notetext'])[0][0]        
    return row

def process(progress_notes):     
    progress_notes = progress_notes.apply(lambda_func, axis=1)
    return progress_notes

progress_notes = process(progress_notes)
vectors_breadth = 768
vectors_length = len(progress_notes)
vectors_2d = np.reshape(progress_notes['vectors'].to_list(), (vectors_length, vectors_breadth))
vectors_df = pd.DataFrame(vectors_2d)
</code></pre>
<p>My progress_notes dataframe looks like -</p>
<pre><code>progress_notes = pd.DataFrame({'id':[1,2,3],'progressnotetype':['Nursing Note', 'Nursing Note', 'Administration Note'], 'notetext': ['Patient\'s skin is grossly intact with exception of skin tear to r inner elbow and r lateral lower leg','Patient with history of Afib with RVR. Patient is incontinent of bowel and bladder.','Give 2 tablet by mouth every 4 hours as needed for Mild to moderate Pain Not to exceed 3 grams in 24 hours']})
</code></pre>
<blockquote>
<p>Note - 1) I'm running the code on aws ec2 instance r5.8x large(32
CPUs) - I tried using multiprocessing but the code goes into a
deadlock because bert takes all my cpu cores.</p>
</blockquote>
",Text Generation & LLMs,convert text word embeddings using bert pretrained model faster trying get word embeddings clinical data using microsoft pubmedbert million text row converting text vector k row take around minute million row would take around hour day approx method speed process code progress note dataframe look like note running code aws ec instance r x large cpu tried using multiprocessing code go deadlock bert take cpu core
How to Answer Subjective/descriptive types of lQuestions using BERT Model?,"<p>I am trying to implement BERT Model for Question Answering tasks, but Its a little different from the existing Q&amp;A models,
The Model will be given some text(3-4 pages) and will be asked questions based on the text, and the expected  answer may be asked in short or descriptive subjective type</p>
<p>I tried to implement BERT, for this task.</p>
<p><strong>The Problems I am facing:</strong>
The input token limit for BERT is 512.
How to get the answer in long form, which can describe any instance, process, event, etc.</p>
",Text Generation & LLMs,answer subjective descriptive type lquestions using bert model trying implement bert model question answering task little different existing q model model given text page asked question based text expected answer may asked short descriptive subjective type tried implement bert task problem facing input token limit bert get answer long form describe instance process event etc
BERT fine-tuning for Conversational AI,"<p>I am trying to build a conversational AI chatbot. Since BERT is quite a popular model, I am thinking about using it. I can see that BERT has a pre-trained model for the Question Answering task. Can anyone tell me which version of the BERT model should I use to build a conversation AI? Or Can anyone direct me to the useful resources?</p>
<p>Thanks in advance!</p>
",Text Generation & LLMs,bert fine tuning conversational ai trying build conversational ai chatbot since bert quite popular model thinking using see bert ha pre trained model question answering task anyone tell version bert model use build conversation ai anyone direct useful resource thanks advance
How do I train a model to score sentences? (i.e. with BERT),"<p>I have researched a lot, but couldn't find something that I understand / fits what I want to do.
I have a database of sentences with annotated scores (these should act as my gold standard).
I want to train a model that learns how to rate sentences with this data, but I don't know how.</p>
<p>I also may want to use a pretrained BERT-model, but I've never worked with that.</p>
<p>My experience in coding isn't really that extensive and I often struggle where to start with the practical tasks that I want to do. I know the theory, but the practical part is my problem.</p>
<p>Can anyone give me a hint on where to start or knows some tutorial I can use to get started?</p>
",Text Generation & LLMs,train model score sentence e bert researched lot find something understand fit want database sentence annotated score act gold standard want train model learns rate sentence data know also may want use pretrained bert model never worked experience coding really extensive often struggle start practical task want know theory practical part problem anyone give hint start know tutorial use get started
How to get Non-contextual Word Embeddings in BERT?,"<p>I am already installed BERT, But I don't know how to get Non-contextual word embeddings.</p>
<p>For example:</p>
<pre><code>
input: 'Apple'
output: [1,2,23,2,13,...] #embedding of 'Apple'


</code></pre>
<p>How can i get these word embeddings?</p>
<p>Thank you.</p>
<p>I search some method, but no blogs have written the way.</p>
",Text Generation & LLMs,get non contextual word embeddings bert already installed bert know get non contextual word embeddings example get word embeddings thank search method blog written way
"How to download BERT model locally, without use of package?","<p>Company firewall seems to prevent me from just using</p>
<pre><code>model = AutoModel.from_pretrained(&quot;sentence-transformers/bert-base-nli-stsb-mean-tokens&quot;)
</code></pre>
<p>so I need to download this model locally and then read it into Python.
Couldn't find the direct AWS link, seems to be typically in this form: but did not work</p>
<pre><code>https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-nli-stsb-mean-tokens-pytorch_model.bin
</code></pre>
<p>Tried these similar questions/solutions <a href=""https://stackoverflow.com/questions/62317931/how-to-predownload-a-transformers-model/64280935#64280935"">here</a> but did not work, since I can't run the first line to download from pretrained in Python, I need an external solution</p>
",Text Generation & LLMs,download bert model locally without use package company firewall seems prevent using need download model locally read python find direct aws link seems typically form work tried similar question solution href work since run first line download pretrained python need external solution p
"Whitelist tokens for text generation (XLNet, GPT-2) in huggingface-transformers","<p>In the documentation on text generation (<a href=""https://huggingface.co/transformers/main_classes/model.html#generative-models"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/model.html#generative-models</a>) there is the option to put</p>
<pre><code>bad_words_ids (List[int], optional) – List of token ids that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use tokenizer.encode(bad_word, add_prefix_space=True).
</code></pre>
<p>Is there also the option to put something along the lines of &quot;allowed_words_ids&quot;? The idea would be to restrict the language of the generated texts.</p>
",Text Generation & LLMs,whitelist token text generation xlnet gpt huggingface transformer documentation text generation option put also option put something along line allowed word id idea would restrict language generated text
How to use a batch size bigger than one in Bert Sequence Classification?,"<p><a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">Hugging Face documentation</a> describes how to do a sequence classification using a Bert model.</p>
<p>Code I am using for a CSV dataset:</p>
<pre><code>import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

def get_embeddings(model_name,tokenizer,name,inp):
    tokenizer = tokenizer.from_pretrained(name)
    model = model_name.from_pretrained(name)
    input_ids = tf.constant(tokenizer.encode(inp))[None,:]  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[0]
    cls_token=last_hidden_states[0]
    return cls_token
cls_token=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',z[0])
cls_token
</code></pre>
<p>There is only example for batch size 1. How to implement it for 48k entries and form an appropriately-sized Tensor afterwards?</p>
",Text Generation & LLMs,use batch size bigger one bert sequence classification hugging face documentation describes sequence classification using bert model code using csv dataset example batch size implement k entry form appropriately sized tensor afterwards
NLP - Specify custom vocabulary / word list for text generation,"<p>I'm experimenting with text generators, like OpenAI's GPT-2, Hugging Face's transformers, and Facebook's ParlAI, and I'm wondering if I can limit or weight the output to a specified list of words? For example, how can I limit the output to only words that start with the letter 'a'?</p>

<p>One obvious idea is to train on a dataset that is limited by that vocabulary, but I only have a laundry list of words, not a natural corpus that only has those words.</p>
",Text Generation & LLMs,nlp specify custom vocabulary word list text generation experimenting text generator like openai gpt hugging face transformer facebook parlai wondering limit weight output specified list word example limit output word start letter one obvious idea train dataset limited vocabulary laundry list word natural corpus ha word
How bert [cls] can collect the relevant information from the rest of the hidden states,"<p>How bert [cls] can collect the relevant information from the rest of the hidden states.??. Does [cls] has mlm information?  If i train my bert using only mlm, in this case cls works?</p>
",Text Generation & LLMs,bert cl collect relevant information rest hidden state bert cl collect relevant information rest hidden state doe cl ha mlm information train bert using mlm case cl work
How to generate descriptive text based on a key-value property list using Python?,"<p>I have a list of properties, name:value style. The name and value could be anything. I would like to generate gramatically correct descriptive text that considers the entire set of name:value pairs. The generator should be smart enough to recognize the type of the property based on the property name and generate appropriate text.</p>

<p>For example:</p>

<pre><code>name:John
age: 26
height:6ft 2inches
eyecolor: blue
profession: cowboy
Expected output - something along the lines of:
John is a cowboy, aged 26. His height is 6ft 2 inches and he has blue eyes.

ApplicationName: Goole maps
Developer: Google
Usage: Geo navigation
available on: desktop, notebook, tablet, mobile phones
competition: Apple, Facebook, Microsoft
Expected output:
Google Maps is a geo navigation app developed by Google. It is available on desktops, notebooks,tablets and mobile phones. it's main competitors are Apple, Yahoo and Facebook maps.
</code></pre>

<p>How should I approach this problem? Would this be a machine learning problem? Or can I implement this using plain NLP without the need for ML? Any pointers appreciated.</p>
",Text Generation & LLMs,generate descriptive text based key value property list using python list property name value style name value could anything would like generate gramatically correct descriptive text considers entire set name value pair generator smart enough recognize type property based property name generate appropriate text example approach problem would machine learning problem implement using plain nlp without need ml pointer appreciated
Build and train BERT from scratch on my own vocab with Masked Language Modeling MLM,"<p>I d like to build a Bert model MLM from scratch without using Bert-tockenizer, I have my own vocab
(which looks like gibberish: &quot;xddccfff988, 9900zxzxxx, zzzTzz&quot;). At the end, I d like to get embeddings of these gibberish words too.</p>
<p>I v been struggling with this for days and any help is highly appreciated.</p>
",Text Generation & LLMs,build train bert scratch vocab masked language modeling mlm like build bert model mlm scratch without using bert tockenizer vocab look like gibberish xddccfff zxzxxx zzztzz end like get embeddings gibberish word v struggling day help highly appreciated
How to calculate corpus blue score,"<p>I have trained my text encoder BERT model. Now I want to calculate corpus_bleu score on prediction data. but I don't have any idea where to implement <code>corpus_bleu</code> score in my code.</p>
<p>It is a caption based model for this, I have trained two models one is image and another on is text model. after training both the model I loaded them like below</p>
<pre><code>vision_encoder = keras.models.load_model(&quot;vision_encoder&quot;)
text_encoder = keras.models.load_model(&quot;text_encoder&quot;)
</code></pre>
<p>Reading image file</p>
<pre><code>def read_image(image_path):
image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)
return tf.image.resize(image_array, (299, 299))
</code></pre>
<p>Converting caption to tensor</p>
<pre><code>def read_text(caption):
return tf.convert_to_tensor(caption)
</code></pre>
<p>Loading whole dataframe with 3851 captions</p>
<pre><code>j = df['findings'].astype(str)
j.shape
(3851,)
</code></pre>
<p>Making predictions of the above dataframe using trained <code>text_encoder</code> model.</p>
<pre><code>text_embeddings = text_encoder.predict(
tf.data.Dataset.from_tensor_slices(j)
.map(read_text).batch(batch_size),
verbose=1,
)
print(f&quot;Text embeddings shape: {text_embeddings.shape}.&quot;)
1926/1926 [==============================] - 25s 13ms/step
Text embeddings shape: (3851, 128, 256).
</code></pre>
<p>code for finding top matches of predicted captions with relevant image</p>
<pre><code>def find_matches(t_embeddings, queries, k=9, normalize=True):

image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)
imgr = tf.expand_dims(image_array, axis=0)
i_embedding = vision_encoder(tf.image.resize(imgr, (299, 299)))
# Normalize the query and the image embeddings.
if normalize:
    image_embeddings = tf.math.l2_normalize(t_embeddings, axis=1)
    query_embedding = tf.math.l2_normalize(i_embedding, axis=1)
# Compute the dot product between the query and the image embeddings.
dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)
print(dot_similarity.shape)
# Retrieve top k indices.
results = tf.math.top_k(dot_similarity, k).indices.numpy()
print(results.shape)
# Return matching image paths.
return [[df['findings'][idx] for idx in indices] for indices in results]
</code></pre>
<p>applying the above function to find relevant captions with an image</p>
<pre><code>img = &quot;/content/image.png&quot;
matches = find_matches(t_embeddings, 
                   [img], 
                   normalize=True)[1]

for i in range(9):
print((matches[i]))
</code></pre>
",Text Generation & LLMs,calculate corpus blue score trained text encoder bert model want calculate corpus bleu score prediction data idea implement score code caption based model trained two model one image another text model training model loaded like reading image file converting caption tensor loading whole dataframe caption making prediction dataframe using trained model code finding top match predicted caption relevant image applying function find relevant caption image
xlm-roberta tokenizer sticks all words together,"<p>I am trying to use a xlm-roberta model I have fine-tuned for token classification, but no matter what I do, I always get as an output all tokens stuck together, like:</p>
<pre><code>[{'entity_group': 'LABEL_0',
'score': 0.4824247,
'word': 'Thedogandthecatwenttothehouse',
'start': 0,
'end': 325}]
</code></pre>
<p>What could I do to get the words properly separated as an output as it happens with other models, like Bert?</p>
<p>I have tried to conduct the training with <code>add_prefix_space=True</code> but it does not seem to have any effect:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('MMG/xlm-roberta-large-ner-spanish', add_prefix_space=True)
model = AutoModelForTokenClassification.from_pretrained(&quot;xlm-roberta-large-finetuned-conll03-english&quot;, use_cache=None, num_labels=NUM_LABELS, ignore_mismatched_sizes=True)
pipe = pipeline(task=&quot;token-classification&quot;, model=model.to(&quot;cpu&quot;), binary_output=True, tokenizer=tokenizer, aggregation_strategy=&quot;average&quot;)
</code></pre>
<p>Thanks a lot in advance for your help.</p>
",Text Generation & LLMs,xlm roberta tokenizer stick word together trying use xlm roberta model fine tuned token classification matter always get output token stuck together like could get word properly separated output happens model like bert tried conduct training doe seem effect thanks lot advance help
Can you make Q&amp;A language model stay on topic?,"<p>I’m thinking of fine-tuning a pre-trained language model for a Q&amp;A task. More specifically, I’d like to fine-tune the model on a single chapter in a classic college textbook. Afterward, the reader of the chapter should be able to engage in a Q&amp;A session with the model about the content of the chapter. But how do I make sure that the model stays on topic and doesn’t go out of a tangent? I know it is possible when looking at what <a href=""https://play.aidungeon.io/"" rel=""nofollow noreferrer"">https://play.aidungeon.io/</a> has achieved, but I don’t know if it will require me to build a model from the ground for each chapter. Can anyone tell me if I’m out of my mind or if it’s feasible?
Best,</p>
",Text Generation & LLMs,make q language model stay topic thinking fine tuning pre trained language model q task specifically like fine tune model single chapter classic college textbook afterward reader chapter able engage q session model content chapter make sure model stay topic go tangent know possible looking ha achieved know require build model ground chapter anyone tell mind feasible best
How can we pass a list of strings to a fine tuned bert model?,"<p>I want  to pass a list of strings instead of a single string input to my fine tuned bert question classification model.
This is my code which accept a single string input.</p>
<pre><code>questionclassification_model = tf.keras.models.load_model('/content/drive/MyDrive/questionclassification_model')
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

def prepare_data(input_text):
    token = tokenizer.encode_plus(
        input_text,
        max_length=256, 
        truncation=True, 
        padding='max_length', 
        add_special_tokens=True,
        return_tensors='tf'
    )
    return {
        'input_ids': tf.cast(token['input_ids'], tf.float64),
        'attention_mask': tf.cast(token['attention_mask'], tf.float64)
    }

def make_prediction(model, processed_data, classes=['Easy', 'Medium', 'Hard']):
    probs = model.predict(processed_data)[0]
    return classes[np.argmax(probs)],probs;
</code></pre>
<p>I don't want to use a for loop over the list as it takes more execution time.
when I tried to pass a list as input to the tokenizer it was returning same output for every input.</p>
<pre><code>input_text = [&quot;What is gandhi commonly considered to be?,Father of the nation in india&quot;,&quot;What is the long-term warming of the planets overall temperature called?, Global Warming&quot;]
processed_data = prepare_data(input_text)
</code></pre>
<hr />
<p>{'input_ids': &lt;tf.Tensor: shape=(1, 256), dtype=float64, numpy=
array([[101., 100., 100., 102.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
0.,   0.,   0.]])&gt;, 'attention_mask': &lt;tf.Tensor: shape=(1, 256), dtype=float64, numpy=
array([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])&gt;}</p>
<hr />
<p>and that is not the right tokens for the input text.</p>
<p>Thanks in advance...</p>
",Text Generation & LLMs,pas list string fine tuned bert model want pas list string instead single string input fine tuned bert question classification model code accept single string input want use loop list take execution time tried pas list input tokenizer wa returning output every input input id tf tensor shape dtype float numpy array attention mask tf tensor shape dtype float numpy array right token input text thanks advance
Running BERT on CPU instead of GPU,"<p>I am trying to execute BERT's <code>run_clasifier.py</code> script using terminal as below:</p>

<pre><code>python run_classifier.py --task_name=cola --do_predict=true --data_dir=&lt;data-dir&gt; --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=&lt;output_dir&gt;/model.ckpt-1603 --max_seq_length=512 --output_dir=&lt;output_dir&gt;
</code></pre>

<p>This by default executes on GPU. Instead, I want to execute it on the CPU. Is there a way to do it for only a single trial of execution than for all the subsequent trials.?</p>
",Text Generation & LLMs,running bert cpu instead gpu trying execute bert script using terminal default executes gpu instead want execute cpu way single trial execution subsequent trial
How does ntlk&#39;s language model assign a score?,"<p>So, I built a simple n-gram language model based on the documentation here: <a href=""https://www.nltk.org/api/nltk.lm.html"" rel=""nofollow noreferrer"">https://www.nltk.org/api/nltk.lm.html</a></p>
<p>But I am a little confused by the score that the language model is producing. Please see the snapshot from the same document:<a href=""https://i.sstatic.net/qwO7h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qwO7h.png"" alt=""enter image description here"" /></a></p>
<p>Below, I check the count of a bigram:
<a href=""https://i.sstatic.net/sQ7eS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sQ7eS.png"" alt=""enter image description here"" /></a></p>
<p>But when I use <code>model.score(&quot;floral&quot;,[&quot;print&quot;])</code>, I get <code>0</code> as the output. I am not able to understand why. Can someone please explain?</p>
",Text Generation & LLMs,doe ntlk language model assign score built simple n gram language model based documentation little confused score language model producing please see snapshot document check count bigram use get output able understand someone please explain
What is the range of BERT CLS values?,"<p>As you can see in my title, I am interested in the value range of BERT.
I read through the BERT paper but it is all still a little confusing for me. At the end of a Classification, BERT has the calculated CLS Token (Classification Token) which is a 768 long float array.</p>
<p>But what are the maximum/minimum possible values in that array?</p>
",Text Generation & LLMs,range bert cl value see title interested value range bert read bert paper still little confusing end classification bert ha calculated cl token classification token long float array maximum minimum possible value array
Using SMOTE for BERT inputs,"<p>I have some imbalanced data which I need to classify. I want to use SMOTE to balance it. But I don't really understand how to use it since I have BERT multiple inputs. Do I need to use it for input_ids? Or attention_masks? Or both? Also, a piece of code would be really useful :)</p>
",Text Generation & LLMs,using smote bert input imbalanced data need classify want use smote balance really understand use since bert multiple input need use input id attention mask also piece code would really useful
How to train a ML model converting text to code,"<p>Looking for a working example Colab/Notebook showing training or fine-tuning of a text generation model capable of converting &quot;short text&quot; -&gt; &quot;programming code text&quot;.</p>
<p>I'm learning the topic and would like to fine-tune it with a custom metric on some public GitHub repos.</p>
<p>All I found so far are models that &quot;continue a sentence&quot; or simply generate the text out of the blue. Many thanks!</p>
",Text Generation & LLMs,train ml model converting text code looking working example colab notebook showing training fine tuning text generation model capable converting short text programming code text learning topic would like fine tune custom metric public github repos found far model continue sentence simply generate text blue many thanks
Gpt 3 keywords extractor,"<p>I'm getting accustomed to gpt and want to build a keywords extractor for book summaries. Can someone point me to the references that'd help for my use case ?</p>
",Text Generation & LLMs,gpt keywords extractor getting accustomed gpt want build keywords extractor book summary someone point reference help use case
Evaluate BERT Model param.requires_grad,"<p>I have a doubt regarding the evaluation on the test set of my bert model. During the eval part param.requires_grad is suppose to be True or False? indipendently if I did a full fine tuning during training or not. My model is in model.eval() mode but I want to be sure to not force nothing wrong in the Model() class when i call it for evaluation. Thanks !</p>
<pre><code>  if freeze_bert == 'True':
        for param in self.bert.parameters():
            param.requires_grad = False
            #logging.info('freeze_bert: {}'.format(freeze_bert)) 
            #logging.info('param.requires_grad: {}'.format(param.requires_grad))
    if freeze_bert == 'False':
        for param in self.bert.parameters():
            param.requires_grad = True
</code></pre>
",Text Generation & LLMs,evaluate bert model param requires grad doubt regarding evaluation test set bert model eval part param requires grad suppose true false indipendently full fine tuning training model model eval mode want sure force nothing wrong model class call evaluation thanks
Getting random output every time on running Next Sentence Prediction code using BERT,"<p>Based on the code provided below, I am trying to run NSP (Next Sentence Prediction) on a custom dataset. The loss after training the model is different every time and the model give different accuracies every time. What am I missing or doing wrong?</p>
<pre><code>pip install transformers[torch]
from transformers import BertTokenizer, BertForNextSentencePrediction 
import torch  
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') 
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
with open('clean.txt', 'r') as fp:
    text = fp.read().split('\n')
bag = [item for sentence in text for item in sentence.split('.') if item != '']
bag_size = len(bag)
import random
 
sentence_a = []
sentence_b = []
label = []
 
for paragraph in text:
    sentences = [
        sentence for sentence in paragraph.split('.') if sentence != ''
    ]
    num_sentences = len(sentences)
    if num_sentences &gt; 1:
        start = random.randint(0, num_sentences-2)
        # 50/50 whether is IsNextSentence or NotNextSentence
        if random.random() &gt;= 0.5:
            # this is IsNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(sentences[start+1])
            label.append(0)
        else:
            index = random.randint(0, bag_size-1)
            # this is NotNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(bag[index])
            label.append(1)
 
inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
inputs['labels'] = torch.LongTensor([label]).T
class MeditationsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)
 
dataset = MeditationsDataset(inputs)
loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
from transformers import AdamW
 
# activate training mode
model.train()
# initialize optimizer
optim = AdamW(model.parameters(), lr=5e-6)
 
from tqdm import tqdm  # for our progress bar
 
epochs = 2
 
for epoch in range(epochs):
    # setup loop with TQDM and dataloader
    loop = tqdm(loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())
</code></pre>
<p>In the code below I am testing the model on unseen data:</p>
<pre><code>from torch.nn import functional as f
from torch.nn.functional import softmax
prompt = &quot;sentence 1 text&quot;
prompt2 = &quot;sentence 2 text&quot;
output = tokenizer.encode_plus(prompt,prompt2, return_tensors=&quot;pt&quot;)
result = model(**output)[0]
prob = softmax(result, dim=1)
print(prob)
</code></pre>
<p>So, the value of prob and loss is different every single time for the same unseen data which to the best of my knowledge should be same.</p>
",Text Generation & LLMs,getting random output every time running next sentence prediction code using bert based code provided trying run nsp next sentence prediction custom dataset loss training model different every time model give different accuracy every time missing wrong code testing model unseen data value prob loss different every single time unseen data best knowledge
Save a Bert model with custom forward function and heads on Hugginface,"<p>I have created my own BertClassifier model, starting from a pretrained and then added my own classification heads composed by different layers. After the fine-tuning, I want to save the model using model.save_pretrained() but when I print it upload it from pretrained i don't see my classifier head.
The code is the following. How can I save the all structure on my model and make it full accessible with <code> AutoModel.from_preatrained('folder_path')</code> ?
. Thanks!</p>
<pre><code>class BertClassifier(PreTrainedModel):
    &quot;&quot;&quot;Bert Model for Classification Tasks.&quot;&quot;&quot;
    config_class = AutoConfig
    def __init__(self,config, freeze_bert=True): #tuning only the head
        &quot;&quot;&quot;
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        &quot;&quot;&quot;
        #super(BertClassifier, self).__init__()
        super().__init__(config)

        # Instantiate BERT model
        # Specify hidden size of BERT, hidden size of our classifier, and number of labels
        self.D_in = 1024 #hidden size of Bert
        self.H = 512
        self.D_out = 2
 
        # Instantiate the classifier head with some one-layer feed-forward classifier
        self.classifier = nn.Sequential(
            nn.Linear(self.D_in, 512),
            nn.Tanh(),
            nn.Linear(512, self.D_out),
            nn.Tanh()
        )
 


    def forward(self, input_ids, attention_mask):


         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

</code></pre>
<pre><code>configuration=AutoConfig.from_pretrained('Rostlab/prot_bert_bfd')
model = BertClassifier(config=configuration,freeze_bert=False)
</code></pre>
<p>Saving the model after fine-tuning</p>
<pre><code>model.save_pretrained('path')
</code></pre>
<p>Loading the fine-tuned model</p>
<pre><code>model = AutoModel.from_pretrained('path') 
</code></pre>
<p>Printing the model after loading shows I have as the last layer the following and missing my 2 linear layer:</p>
<pre><code> (output): BertOutput(
          (dense): Linear(in_features=4096, out_features=1024, bias=True)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (adapters): ModuleDict()
          (adapter_fusion_layer): ModuleDict()
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (activation): Tanh()
  )
  (prefix_tuning): PrefixTuningPool(
    (prefix_tunings): ModuleDict()
  )
)
</code></pre>
",Text Generation & LLMs,save bert model custom forward function head hugginface created bertclassifier model starting pretrained added classification head composed different layer fine tuning want save model using model save pretrained print upload pretrained see classifier head code following save structure model make full accessible thanks saving model fine tuning loading fine tuned model printing model loading show last layer following missing linear layer
Multi-label classification with frequencies - NLP (BERT),"<p>I have dataset that looks like this:</p>
<pre><code>entence          emotions
bla bla bla       {'kind':105, 'sad':32, 'angry':5}
.....             {'surprised': 65, 'kind': 10} 
</code></pre>
<p>Numbers here represents how many times people had this emotion while reading the post</p>
<p>Basically, post <em>'bla bla bla'</em> seemed <em>'kind'</em> to 105 people while 32 people felt <em>'sad'</em> about it.</p>
<p>Basically, my goal is to predict a type of emotion a post most likely to have</p>
<p>I am going to use a BERT transformer.</p>
<p>Question: How to deal with the frequency that each emotion has? I know that frequency is important especially when one post has 105 people that smiled and the other 5 felt angry and if I remove frequency then it would be smile and angry together and that can cause misunderstanding. How should I design my model? Should I just remove frequency? or should I remove all the other emotion type except for the one that has the most frequency?</p>
",Text Generation & LLMs,multi label classification frequency nlp bert dataset look like number represents many time people emotion reading post basically post bla bla bla seemed kind people people felt sad basically goal predict type emotion post likely going use bert transformer question deal frequency emotion ha know frequency important especially one post ha people smiled felt angry remove frequency would smile angry together cause misunderstanding design model remove frequency remove emotion type except one ha frequency
Write generator function for LSTM text generation model,"<p>i have a LSTM model for text generation but when trying to increase the amount of data to input, I run into RAM issues so I found out that I can use fit_generator function to load the data in step by step.</p>
<p>The problem is currently that <strong>keras.utils.to_categorical</strong> takes to much space when the amount of unique words increases.</p>
<p>So i want to convert this code block into a generator function:</p>
<pre><code>x_values, labels = input_seqs[:, :-1], input_seqs[:, -1]
y_values = tf.keras.utils.to_categorical(labels, num_classes=total_unique_words)

#Shape of x_values: (152250, 261)
#Shape of y_values: (152250, 4399)
</code></pre>
<p>And i got something like this but I'm not sure how to assign the right values to <em>batch_x</em> and <em>batch_y</em></p>
<pre><code>def generator(input_seq, batch_size):

    index = 0 
    while True:
      batch_x = np.zeros((batch_size, max_seq_length-1))
      batch_y = np.zeros((batch_size, total_unique_words))
      for i in range(batch_size):
        batch_x[i] = input_seqs[:, :-1][i]
        batch_y[i] = tf.keras.utils.to_categorical(input_seqs[:, -1][i], num_classes=total_unique_words)
        index = index + 1
        if index == len(input_seq):
          index = 0

      yield batch_x, batch_y


</code></pre>
<p>Full code for better overview:</p>
<pre><code>tokenizer = Tokenizer()
tokenizer.fit_on_texts(review_list)
word_index = tokenizer.word_index
total_unique_words = len(tokenizer.word_index) + 1 

input_sequences = []
for line in review_list:
  token_list = tokenizer.texts_to_sequences([line])[0]
  for i in range(1, len(token_list)):
    n_gram_seqs = token_list[:i+1]
    input_sequences.append(n_gram_seqs)

max_seq_length = max([len(x) for x in input_sequences])
input_seqs = np.array(pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre'))

x_values, labels = input_seqs[:, :-1], input_seqs[:, -1]
y_values = tf.keras.utils.to_categorical(labels, num_classes=total_unique_words)

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
K.clear_session()
model = tf.keras.Sequential([
tf.keras.layers.Embedding(input_dim = total_unique_words, output_dim=100, input_length=max_seq_length-1),
tf.keras.layers.LSTM(256, return_sequences=True), 
tf.keras.layers.Dropout(0.2), 
tf.keras.layers.LSTM(256), 
tf.keras.layers.Dropout(0.2),
tf.keras.layers.Dense(128, activation='relu'),
tf.keras.layers.Dense(total_unique_words , activation='softmax')])
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) 

</code></pre>
",Text Generation & LLMs,write generator function lstm text generation model lstm model text generation trying increase amount data input run ram issue found use fit generator function load data step step problem currently kera utils categorical take much space amount unique word increase want convert code block generator function got something like sure assign right value batch x batch full code better overview
NLP - support comments analysis,"<p>I am new to NLP and looking for some direction since after all my reading I haven't found a suitable approach and the subject matter is vast. The project is to focus on specific fields of support comments using NLP and Python. The goal is that from the comments I want to verify that the comment is in fact a well made comment for that field. Some requirements is the context of entered text is relevant to the field it has been entered in, it is informative based on the specific field, it is not just a few unhelpful words entered, it can detect that words/sentences have similar semantic meaning (e.g problem is, issue encountered etc).</p>
<p>I first looked into TF-IDF but it doesn't consider context. I have been reading into other deep learning models such as BERT as it includes context. I have read though that BERT was designed more for sentence prediction and missing words rather than for comparing semantic meaning of multiple sentences. USE would be better for comparing if sentences are similar although all the comments will have a general theme and I doubt they will be similar enough to design a system around. Topic modelling sounded like what i'm looking for but I think it's more for general topics you find in newspapers and books rather than comment sections. For the text summarisation option maybe there is not enough text in each comment to give a helpful summary. Is there something in fine tuning a pretrained model such as BERT to analyse all the comments for a specific field and somehow extract from that if a new comment is informative and relevant by comparing it to the existing ones?</p>
<p>Any direction is appreciated or if you know of existing material or a tutorial.</p>
<p>Thanks</p>
",Text Generation & LLMs,nlp support comment analysis new nlp looking direction since reading found suitable approach subject matter vast project focus specific field support comment using nlp python goal comment want verify comment fact well made comment field requirement context entered text relevant field ha entered informative based specific field unhelpful word entered detect word sentence similar semantic meaning e g problem issue encountered etc first looked tf idf consider context reading deep learning model bert includes context read though bert wa designed sentence prediction missing word rather comparing semantic meaning multiple sentence use would better comparing sentence similar although comment general theme doubt similar enough design system around topic modelling sounded like looking think general topic find newspaper book rather comment section text summarisation option maybe enough text comment give helpful summary something fine tuning pretrained model bert analyse comment specific field somehow extract new comment informative relevant comparing existing one direction appreciated know existing material tutorial thanks
BERT outputs explained,"<p>The keys of the BERT encoder's output are <code>default</code>, <code>encoder_outputs</code>, <code>pooled_output</code> and <code>sequence_output</code></p>
<p>As far as I can know, <code>encoder_outputs</code> are the output of each encoder, <code>pooled_output</code> is the output of the global context and <code>sequence_output</code> is the output context of each token (correct me if I'm wrong please). But what is <code>default</code>? Can you give me a more detailed explanation of each one?</p>
<p><a href=""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"" rel=""nofollow noreferrer"">This is the link to the encoder</a></p>
",Text Generation & LLMs,bert output explained key bert encoder output far know output encoder output global context output context token correct wrong please give detailed explanation one link encoder
"why do pooler use tanh as a activation func in bert, rather than gelu?","<p>class BERTPooler(nn.Module):
def <strong>init</strong>(self, config):
super(BERTPooler, self).<strong>init</strong>()
self.dense = nn.Linear(config.hidden_size, config.hidden_size)
self.activation = nn.Tanh()</p>
<pre><code>def forward(self, hidden_states):
    # We &quot;pool&quot; the model by simply taking the hidden state corresponding
    # to the first token.
    first_token_tensor = hidden_states[:, 0]
    pooled_output = self.dense(first_token_tensor)
    pooled_output = self.activation(pooled_output)
    return pooled_output
</code></pre>
",Text Generation & LLMs,pooler use tanh activation func bert rather gelu class bertpooler nn module def init self config super bertpooler self init self dense nn linear config hidden size config hidden size self activation nn tanh
Comparing 2 pdf files to check for their similarity using BERT,"<p>I have 2 PDF files which I want to compare with each other and get similarity score, from what I have been able to research online and find is a sentence or a paragraph being compared to each other, but I want to compare the between the pdfs as a whole and not a paragraph. I want to do this using BERT.</p>
<p>Any help is appreciated, Thanks.</p>
",Text Generation & LLMs,comparing pdf file check similarity using bert pdf file want compare get similarity score able research online find sentence paragraph compared want compare pdfs whole paragraph want using bert help appreciated thanks
How to replace [UNK] tokens with original tokens in BERT nlpaug,"<p>I am trying to use nlpaug to swap some words out but am having issue with it replacing tokens permanently with the [UNK] token. I am using the docs here: <a href=""https://nlpaug.readthedocs.io/en/latest/augmenter/word/context_word_embs.html"" rel=""nofollow noreferrer"">https://nlpaug.readthedocs.io/en/latest/augmenter/word/context_word_embs.html</a></p>
<p>My code an example is as such:</p>
<pre><code>aug = naw.ContextualWordEmbsAug(action='substitute', top_k=10,
                            aug_min=2, aug_max=4, stopwords=stops, batch_size=25)

aug.augment('You deposit the minimum amount (which is Rs 25/-) and buy tickets (Tickets are your entry fee) and start playing the game with others.')
</code></pre>
<p>Output</p>
<pre><code>'[UNK] deposit the entire amount ( which is rs 10 / - ) and buy tickets ( tickets are your entry cost ) and start playing the game with ease.'
</code></pre>
<p>I have lost you in the sentence and would like to keep this. This happens across a universe of sentences so a simple one case fix will not be the answer here. Any help would be greatly appreciated.</p>
",Text Generation & LLMs,replace unk token original token bert nlpaug trying use nlpaug swap word issue replacing token permanently unk token using doc code example output lost sentence would like keep happens across universe sentence simple one case fix answer help would greatly appreciated
How do I retrain BERT model with new data,"<p>I have already trained a bert model and saved it in the .pb format and I want to retrain the model with new datasets that i custom made, so in order to not to lose the previous training and such, how do I train the model with the new data so the model could update it self
any approaches?
this is my training code down below</p>
<pre><code>optimizer = Adam(lr=1e-5, decay=1e-6)
model.compile(loss='binary_crossentropy',
                  optimizer=optimizer,
                  metrics=['accuracy'])    
history = model.fit(
        x={'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']},
        #x={'input_ids': x['input_ids']},
        y={'outputs': train_y},
        validation_split=0.1,
        batch_size=32,
        epochs=1)
</code></pre>
",Text Generation & LLMs,retrain bert model new data already trained bert model saved pb format want retrain model new datasets custom made order lose previous training train model new data model could update self approach training code
Generative LSTM always generates same word,"<p>I'm working on a simple LSTM for text generation. I largely worked based on <a href=""https://www.analyticsvidhya.com/blog/2022/02/explaining-text-generation-with-lstm/"" rel=""nofollow noreferrer"">this tutorial</a>. When running the below simple reproducible example, however, I noticed that the text generation part doesn't work as expected: no matter what input sequence is provided, it always returns the same next word. In the below case, when inputting the one-word seed &quot;i&quot; and running the text generation repeatedly, the generated text would be: &quot;i that that that that that...&quot; which is obviously not a well-functioning text generation model.</p>
<p>Code:</p>
<pre><code>tokenizer = Tokenizer()
def get_sequence_of_tokens(corpus):    
    tokenizer.fit_on_texts(corpus)    
    total_words = len(tokenizer.word_index) + 1    
    input_sequences = []
    for line in corpus:
        token_list = tokenizer.texts_to_sequences([line])[0]
        input_sequences.append(token_list)
    return input_sequences, total_words

inp_sequences, total_words = get_sequence_of_tokens(train_data)
print(&quot;Total words:&quot;, total_words)

word_to_int = tokenizer.word_index
int_to_word = dict([(idx, word) for idx, word in enumerate(word_to_int)])

# Set maximum length 
max_len = 25

# Pad all sequences
def generate_padded_sequences(input_sequences, max_len=max_len):
    input_sequences = np.array(pad_sequences(input_sequences, padding='pre', maxlen=max_len+1))    
    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]
    labels = to_categorical(label, num_classes=total_words)
    return predictors, labels

predictors, labels = generate_padded_sequences(inp_sequences)

def create_model(input_len, total_words):
    model = Sequential()
    model.add(Embedding(total_words, 32, input_length=input_len))
    model.add(LSTM(64))
    model.add(Dropout(0.1))
    model.add(Dense(total_words, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    return model

model_lstm = create_model(input_len=max_len, total_words=total_words)

# Train LSTM
history_lstm = model_lstm.fit(predictors, labels, epochs=5)

# Trying out text generation
seed_text = &quot;i&quot;
token_list = tokenizer.texts_to_sequences([seed_text])[0]
token_list = pad_sequences([token_list], maxlen=max_len, padding='pre')
print(&quot;Next word:&quot;, int_to_word[np.argmax(model_lstm.predict(token_list))])    # &lt;-- this will always return same!

</code></pre>
<p>Imports and data:</p>
<pre><code>import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM , Embedding, Flatten
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.utils.all_utils import to_categorical

train_data = ['o by no means honest ventidius i gave it freely ever and theres none can truly say he gives if our betters play at that game we must not dare to imitate them faults that are rich are fair', 'but was not this nigh shore', 'impairing henry strengthening misproud york the common people swarm like summer flies and whither fly the gnats but to the sun', 'what while you were there', 'chill pick your teeth zir come no matter vor your foins', 'thanks dear isabel', 'come prick me bullcalf till he roar again', 'go some of you knock at the abbeygate and bid the lady abbess come to me', 'an twere not as good deed as drink to break the pate on thee i am a very villain', 'beaufort it is thy sovereign speaks to thee', 'but say lucetta now we are alone wouldst thou then counsel me to fall in love', 'for being a bawd for being a bawd', 'all blest secrets all you unpublishd virtues of the earth spring with my tears', 'what likelihood', 'o find him', 'ay pilgrim lips that they must use in prayer', 'wouldst have me kneel', 'marry i fare well for here is cheer enough', 'that i may call thee something more than man and after that trust to thee', 'give me your hand']
</code></pre>
",Text Generation & LLMs,generative lstm always generates word working simple lstm text generation largely worked based tutorial running simple reproducible example however noticed text generation part work expected matter input sequence provided always return next word case inputting one word seed running text generation repeatedly generated text would obviously well functioning text generation model code import data
Method to generate keywords out of a scientific text?,"<p>Which method of text analysis should I use if I need to get a number of multiword keywords, say (up to) 5 per text, analysing a scientific text of some length? In particular, the text could be</p>
<ol>
<li>a title,</li>
<li>or an abstract.</li>
</ol>
<p>Preferably a method already scripted on Python.
Thank you!</p>
",Text Generation & LLMs,method generate keywords scientific text method text analysis use need get number multiword keywords say per text analysing scientific text length particular text could title abstract preferably method already scripted python thank
GPT-3 long input posts for Question Answering,"<p>From my understanding, GPT-3 is &quot;trained&quot; for a specific task by including some labelled examples before the desired/test example. In Question Answering, this includes a context and a question. In this situation, the input prompt can become long. How do people address this?</p>
<p>I am using the Hugging Face GPT-J implementation, and there is an input token limit (of 2000). However, when including multiple qa examples in the prompt (especially with the contexts), it quickly reaches this limit, limitting the amount of example prompts to be inputted. Does anyone know how this issue is handled in a GPT-J setting, especially for QA?</p>
",Text Generation & LLMs,gpt long input post question answering understanding gpt trained specific task including labelled example desired test example question answering includes context question situation input prompt become long people address using hugging face gpt j implementation input token limit however including multiple qa example prompt especially context quickly reach limit limitting amount example prompt inputted doe anyone know issue handled gpt j setting especially qa
Difficulty in understanding the tokenizer used in Roberta model,"<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel, AutoTokenizer

tokenizer1 = AutoTokenizer.from_pretrained(""roberta-base"")
tokenizer2 = AutoTokenizer.from_pretrained(""bert-base-cased"")

sequence = ""A Titan RTX has 24GB of VRAM""
print(tokenizer1.tokenize(sequence))
print(tokenizer2.tokenize(sequence))
</code></pre>

<p>Output:</p>

<p>['A', 'ĠTitan', 'ĠRTX', 'Ġhas', 'Ġ24', 'GB', 'Ġof', 'ĠVR', 'AM']</p>

<p>['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']</p>

<p>Bert model uses WordPiece tokenizer. Any word that does not occur in the WordPiece vocabulary is broken down into sub-words greedily. For example, 'RTX' is broken into 'R', '##T' and '##X' where ## indicates it is a subtoken. </p>

<p>Roberta uses BPE tokenizer but I'm unable to understand </p>

<p>a) how BPE tokenizer works? </p>

<p>b) what does G represents in each of tokens?</p>
",Text Generation & LLMs,difficulty understanding tokenizer used roberta model output titan rtx ha gb vr titan r x ha gb v ra bert model us wordpiece tokenizer word doe occur wordpiece vocabulary broken sub word greedily example rtx broken r x indicates subtoken roberta us bpe tokenizer unable understand bpe tokenizer work b doe g represents token
How to get tokens to words in BERT tokenizer,"<p>I have a list, using higgingface bert tokenizer I can get the mapping numerical representation.</p>
<pre><code>X = ['[CLS]', '[MASK]', 'love', 'this', '[SEP]']
tokens = tokenizer.convert_tokens_to_ids(X)
toekns: [101, 103, 2293, 2023, 102]
</code></pre>
<p>Is there any function so that I can get tokens=[101, 103, 2293, 2023, 102] to words ['[CLS]', '[MASK]', 'love', 'this', '[SEP]']?</p>
<p>One possible way is to mapping, but is there any defined function to do it easily ?</p>
",Text Generation & LLMs,get token word bert tokenizer list using higgingface bert tokenizer get mapping numerical representation function get token word cl mask love sep one possible way mapping defined function easily
How is the number of parameters be calculated in BERT model?,"<p>The paper &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&quot; by Devlin &amp; Co. calculated for the base model size 110M parameters (i.e. L=12, H=768, A=12) where L = number of layers, H = hidden size and A = number of self-attention operations. As far as I know parameters in a neural network are usually the count of &quot;weights and biases&quot; between the layers. So how is this calculated based on the given information? 12<em>768</em>768*12?</p>
",Text Generation & LLMs,number parameter calculated bert model paper bert pre training deep bidirectional transformer language understanding devlin co calculated base model size parameter e l h l number layer h hidden size number self attention operation far know parameter neural network usually count weight bias layer calculated based given information
what does this command do for a bert transformers?,"<pre><code>!pip install transformers 

from transformers import InputExample, InputFeatures
</code></pre>
<p>What are InputExample and InputFeatures here?</p>
<p>thanks.</p>
",Text Generation & LLMs,doe command bert transformer inputexample inputfeatures thanks
Is it possible to train the transformer model in google colab? (Not gpt2),"<p>i understand that gpt2 is based on the transformer architecture but where is the source code, there are limited resources and no tutorial on how to write one..
I am new to NLP and also if i had to generate novels, would training the transformer on multiple novels help or  one?</p>
",Text Generation & LLMs,possible train transformer model google colab gpt understand gpt based transformer architecture source code limited resource tutorial write one new nlp also generate novel would training transformer multiple novel help one
Sentence generation from a normalized transition matrix in Python,"<p>I am trying to train an algorithm to generate text based on input from a text file.
So far I have created a dataframe which normalizes the counts of two words appearing together and displays them as probabilities (a normalized transition matrix).</p>
<p><a href=""https://i.sstatic.net/R9SnB.png"" rel=""nofollow noreferrer"">A glimpse of the transition matrix</a></p>
<p>I wanted to understand how i can pick a random word from a row in my df and generate sentences based on probabilities. In essence, sample all columns against one row (word) and pick the next word which has the greatest probability of appearing.</p>
<ol>
<li>For e.g. first word: &quot;A&quot;,</li>
<li>Second word prob: &quot;long - 0.4&quot;, &quot;while - 0.3&quot;, &quot;night - 0.3&quot;</li>
<li>Second word picked: long</li>
<li>Sentence thus far: &quot;A long&quot;</li>
<li>Repeat steps two to five to choose 3rd word.</li>
</ol>
<p>We can repeat the process for filling up the sentence until N words have been picked.</p>
",Text Generation & LLMs,sentence generation normalized transition matrix python trying train algorithm generate text based input text file far created dataframe normalizes count two word appearing together display probability normalized transition matrix glimpse transition matrix wanted understand pick random word row df generate sentence based probability essence sample column one row word pick next word ha greatest probability appearing e g first word second word prob long night second word picked long sentence thus far long repeat step two five choose rd word repeat process filling sentence n word picked
Can I fine-tune BERT using only masked language model and next sentence prediction?,"<p>So if I understand correctly there are mainly two ways to adapt BERT to a specific task: fine-tuning (all weights are changed, even pretrained ones) and feature-based (pretrained weights are frozen). However, I am confused.</p>
<ol>
<li>When to use which one? If you have unlabeled data (unsupervised learning), should you then use fine-tuning?</li>
<li>If I want to fine-tuned BERT, isn't the only option to do that using masked language model and next sentence prediction? And also: is it necessary to put another layer of neural network on top?</li>
</ol>
<p>Thank you.</p>
",Text Generation & LLMs,fine tune bert using masked language model next sentence prediction understand correctly mainly two way adapt bert specific task fine tuning weight changed even pretrained one feature based pretrained weight frozen however confused use one unlabeled data unsupervised learning use fine tuning want fine tuned bert option using masked language model next sentence prediction also necessary put another layer neural network top thank
GPT-3 question answering based on keywords,"<p>I am currently getting accustomed to GPT3, and I am trying to generate questions from a text by also inputting some keywords from that text. Ideally, they would be the answers to that question.</p>
<p>What I tried was to input the text, and simply write <code>Keywords: dog, cat, mouse</code> etc., so just enumerating the words, and then input some question examples. But obviously, it is not used to this structure and I was wondering if it was even possible to do it like that.</p>
",Text Generation & LLMs,gpt question answering based keywords currently getting accustomed gpt trying generate question text also inputting keywords text ideally would answer question tried wa input text simply write etc enumerating word input question example obviously used structure wa wondering wa even possible like
extract and concanate the last 4 hidden states from bert model for each input,"<p>I want to extract and concanate 4 last hidden states from bert for each input sentance and save them
I use this code but i got last hidden state only</p>
<pre><code>class MixModel(nn.Module):
    def __init__(self,pre_trained='bert-base-uncased'):
        super().__init__()        
        self.bert =  AutoModel.from_pretrained('distilbert-base-uncased')
        self.hidden_size = self.bert.config.hidden_size
        
      
           
    def forward(self,inputs, mask , labels):
        
        cls_hs = self.bert(input_ids=inputs,attention_mask=mask, return_dict= False,  output_hidden_states=True)        
        print(cls_hs)        
                   
        encoded_layers = cls_hs[0]
        print(len(encoded_layers))

        print(encoded_layers.size())
        #output is [1,64,768]
       
        return encoded_layers
</code></pre>
<p>batch size is 1
padding size is 64</p>
<p>how to extract the last four?</p>
",Text Generation & LLMs,extract concanate last hidden state bert model input want extract concanate last hidden state bert input sentance save use code got last hidden state batch size padding size extract last four
How to create a dialog with Open AI GPT using CoreML?,"<p><a href=""https://i.sstatic.net/4sXaG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4sXaG.png"" alt=""enter image description here"" /></a></p>
<p>With the introduction of CoreML 3 in the slides they show <strong>Open AI GPT</strong>. So my assumptions is they integrated GPT into CoreML. Would that make it possible to have a conversation with a bot?</p>
<p>The problem is I have spent hours on the internet trying to find how to implement this and was not able to find the answer. I did find:</p>
<pre><code>&quot;NLTagger(tagSchemes: [.lemma])&quot; for Lemmatization.
&quot;NLTagger(tagSchemes: [.nameType])&quot; for identifying names.
&quot;NLTagger(tagSchemes: [.sentimentScore])&quot; for sentiment.
&quot;NLLanguageRecognizer&quot; to determine dominant language.
&quot;UITextChecker&quot; for spell checking and correction.
</code></pre>
<p>This is all used for analyzing text. But from my understanding GPT can be used for dialog and text generation as well.</p>
<p><a href=""https://github.com/huggingface/swift-coreml-transformers"" rel=""nofollow noreferrer"">https://github.com/huggingface/swift-coreml-transformers</a></p>
<p>Came close with GPT-2 text generation. However if I download it won't compile because of model issues. Is there anyone that knows how this can be accomplished? Preferabele using CoreML?</p>
",Text Generation & LLMs,create dialog open ai gpt using coreml introduction coreml slide show open ai gpt assumption integrated gpt coreml would make possible conversation bot problem spent hour internet trying find implement wa able find answer find used analyzing text understanding gpt used dialog text generation well came close gpt text generation however download compile model issue anyone know accomplished preferabele using coreml
Fine-tune Bert for specific domain (unsupervised),"<p>I want to fine-tune BERT on texts that are related to a specific domain (in my case related to engineering). The training should be unsupervised since I don't have any labels or anything. Is this possible?</p>
",Text Generation & LLMs,fine tune bert specific domain unsupervised want fine tune bert text related specific domain case related engineering training unsupervised since label anything possible
Getting MemoryError fine-tuning GPT2(355M) model with small datasets (3MB) through aitextgen,"<p>I'm using aitextgen to fine-tune the 355M GPT-2 model using the train function. The datasets are small txt files consisting of lines like these (these are encoded texts for keyword-based text generation, hence the &quot;~^keywords~@&quot;):</p>
<pre><code>&lt;|startoftext|&gt;~^~@&quot;Yes, but one forgets that she is there--or anywhere. She seems as if she were an accident.&quot;&lt;|endoftext|&gt;
&lt;|startoftext|&gt;~^man~@&quot;Then jump out and unharness this horse. A man will come for it to- morrow.&quot;&lt;|endoftext|&gt;
&lt;|startoftext|&gt;~^mind 's~@&quot;It would upset the house terribly,&quot; said Nan; &quot;but I don't mind that. I'm with you, Patty. Let's do it.&quot;&lt;|endoftext|&gt;
&lt;|startoftext|&gt;~^Booth sure say wish~@&quot;I wish I were sure that I had,&quot; said Booth.&lt;|endoftext|&gt;
</code></pre>
<p>I use aitextgen's training function like this:</p>
<pre><code>    gpt2 = aitextgen(tf_gpt2 = &quot;355M&quot;, to_gpu= True)

    gpt2.train(dataset,
               line_by_line = True,
               batch_size= 1,
               num_steps = 50,
               save_every = 10,
               generate_every = 10,
               learning_rate = 1e-3,
               fp16 = False)
</code></pre>
<p>When I run this function, I get this output:</p>
<pre><code>0%|          | 0/10000 [00:00&lt;?, ?it/s]
Windows does not support multi-GPU training. Setting to 1 GPU.
C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\pytorch_lightning\trainer\connectors\callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
  rank_zero_deprecation(
C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\pytorch_lightning\trainer\connectors\callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
  rank_zero_deprecation(
C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\pytorch_lightning\trainer\connectors\callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.
  rank_zero_deprecation(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  0%|          | 0/50 [00:00&lt;?, ?it/s]

Traceback (most recent call last):
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\transformers\modeling_utils.py&quot;, line 1364, in from_pretrained
    state_dict = torch.load(resolved_archive_file, map_location=&quot;cpu&quot;)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 607, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 882, in _load
    result = unpickler.load()
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 857, in persistent_load
    load_tensor(data_type, size, key, _maybe_decode_ascii(location))
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 845, in load_tensor
    storage = zip_file.get_storage_from_record(name, size, dtype).storage()
RuntimeError: [enforce fail at ..\c10\core\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 205852672 bytes.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 125, in _main
    prepare(preparation_data)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\runpy.py&quot;, line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\runpy.py&quot;, line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\Josh\Python Projects\FYP\src\[py file name].py&quot;, line 34, in &lt;module&gt;
    gpt2 = aitextgen(tf_gpt2 = &quot;355M&quot;, to_gpu= True)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\aitextgen\aitextgen.py&quot;, line 166, in __init__
    self.model = GPT2LMHeadModel.from_pretrained(model, config=config)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\transformers\modeling_utils.py&quot;, line 1368, in from_pretrained
    if f.read().startswith(&quot;version&quot;):
MemoryError
</code></pre>
<p>I have tried many methods, including clearing the CUDA cache using <code>torch.cuda.empty_cache()</code>, splitting the files down to even smaller ones. None of them worked.</p>
<p>I'm running this on my local machine (RTX3070, 32GB RAM), I checked the task manager and the RAM usage barely hits 50%. Is there anything wrong with my code that's causing the memory errors?</p>
",Text Generation & LLMs,getting memoryerror fine tuning gpt model small datasets mb aitextgen using aitextgen fine tune gpt model using train function datasets small txt file consisting line like encoded text keyword based text generation hence keywords use aitextgen training function like run function get output tried many method including clearing cuda cache using splitting file even smaller one none worked running local machine rtx gb ram checked task manager ram usage barely hit anything wrong code causing memory error
Getting the vocabulary of Stanford&#39;s glove model,"<p>Does anyone knows if I can get all the vocabulary for the glove model?</p>
<p>I look to do the same thing that this guy does to BERT on this video [on 15:40]: <a href=""https://www.youtube.com/watch?v=zJW57aCBCTk&amp;ab_channel=ChrisMcCormickAI"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=zJW57aCBCTk&amp;ab_channel=ChrisMcCormickAI</a></p>
",Text Generation & LLMs,getting vocabulary stanford glove model doe anyone know get vocabulary glove model look thing guy doe bert video
How to normalize probabilities of words in varying length sentences?,"<p>Let's say we have an RNN model that outputs the probability of a word given context (or no context) trained on a corpus. 
We can chain the probability of each word in a sequence to get the overall probability of the sentence itself. But, because we are chaining, the probability (or likelihood) of the sentence goes down as it's length increases. This is the same case even if we are using log probabilities. </p>

<p>Is there anyway we could normalize these probabilities? This is an interesting subproblem that I am facing while building a language model. I have a corpus of 9 million sentences of whose lengths vary from 2-30. But all of the sentences are valid ones and I am using these as the corpus to train the LM.</p>

<p>Now, I am taking a subset of data and making changes to it like shuffling or cutting the sentence into half, prepending or appending a random word and so on. This is to create a ""fake sentence"" that need not be valid. What I would like to do is get a threshold of some sort over the likelihood of all the valid sentences and then when I use the RNN to compute probability of the fake sentence, it should be fairly smaller or different form the calculated threshold.</p>

<p>tldr;
sentences like </p>

<pre><code>""the cat sat on the red mat""
""the cat sat on a mat""
""a cat sat on the red mat with brown coffee stains""
</code></pre>

<p>should all have a comparable probability/score/metric
while sentences like</p>

<pre><code>""cat cat mat on the brown red sat is""
""not mat in door on cat""
</code></pre>

<p>have a lower score.</p>
",Text Generation & LLMs,normalize probability word varying length sentence let say rnn model output probability word given context context trained corpus chain probability word sequence get overall probability sentence chaining probability likelihood sentence go length increase case even using log probability anyway could normalize probability interesting subproblem facing building language model corpus million sentence whose length vary sentence valid one using corpus train lm taking subset data making change like shuffling cutting sentence half prepending appending random word create fake sentence need valid would like get threshold sort likelihood valid sentence use rnn compute probability fake sentence fairly smaller different form calculated threshold sentence like comparable probability score metric sentence like lower score
"Encoding Title, Subtitle and Text using BERT","<p>I have a use case where I want to generate embeddings using BERT. I have 3 columns viz. Title, Subtitle and Text. I want to generate BERT embeddings using all 3 columns. Now, the simplest way is to concatenate all the 3 columns but I want to introduce the importance of title, subtitle and text. Title and subtitle are more important than the text itself. How do I achieve this?</p>
<p>I am using <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">sentence-transformer</a> in python to generate BERT embeddings.</p>
<p>Any help is appreciated :)</p>
",Text Generation & LLMs,encoding title subtitle text using bert use case want generate embeddings using bert column viz title subtitle text want generate bert embeddings using column simplest way concatenate column want introduce importance title subtitle text title subtitle important text achieve using sentence transformer python generate bert embeddings help appreciated
How can I find the probability of a sentence using GPT-2?,"<p>I'm trying to write a program that, given a list of sentences, returns the most probable one. I want to use GPT-2, but I am quite new to using it (as in I don't really know how to do it). I'm planning on finding the probability of a word given the previous words and multiplying all the probabilities together to get the overall probability of that sentence occurring, however I don't know how to find the probability of a word occurring given the previous words. This is my (psuedo) code:</p>
<pre><code>sentences = # my list of sentences

max_prob = 0
best_sentence = sentences[0]

for sentence in sentences:
    prob = 1 #probability of that sentence

    for idx, word in enumerate(sentence.split()[1:]):
        prob *= probability(word, &quot; &quot;.join(sentence[:idx])) # this is where I need help

    if prob &gt; max_prob:
        max_prob = prob
        best_sentence = sentence

print(best_sentence)
</code></pre>
<p>Can I have some help please?</p>
",Text Generation & LLMs,find probability sentence using gpt trying write program given list sentence return probable one want use gpt quite new using really know planning finding probability word given previous word multiplying probability together get overall probability sentence occurring however know find probability word occurring given previous word psuedo code help please
how to convert .lm file(language model) to .lm.bin(binary file)?,"<p>I am using pocketsphinx for offline speech recognition. I use lmtool to get language model and dictionary.But the language model has extension .lm but pocketsphinx requires .lm.bin file. So, how can I convert this?</p>
",Text Generation & LLMs,convert lm file language model lm bin binary file using pocketsphinx offline speech recognition use lmtool get language model dictionary language model ha extension lm pocketsphinx requires lm bin file convert
How does pipeline work for nlp tasks and what do the accuracy scores imply,"<p>[First I would like to thank the community of SO; for whose support I could actually finish making an end to end data science project (including deployment.)]</p>
<p>Over to my question.</p>
<p>I wanted to create an app that will predict success or failure depending upon textual input data.</p>
<p>In order to construct the model; instead of developing an algorithm from scratch; I used existing algorithms instead.</p>
<p>While training my dataframe [which is a textual corpus]; I frequently wrote:</p>
<pre><code>pipeline = Pipeline(
[
    (&quot;vect&quot;, CountVectorizer()),
    (&quot;tfidf&quot;, TfidfTransformer()),
    (&quot;clf&quot;, my_chosen_model()),
]
)
</code></pre>
<p>That is because I saw several code examples doing so. I however, didn't clearly understand the usage of pipeline. Can anyone please explain it (or share any link which can help people like me from non-programming backgrounds understand it better.)</p>
<p>After completing my experiments I obtained accuracy scores on text data:</p>
<pre><code>RandomForest    SVM    MLP    MultinomialNB
0.80            0.85   0.86    0.80
</code></pre>
<p>When I increased the number of data (from initial 20000 to 50000), accuracy scores were:</p>
<pre><code>RandomForest    SVM    MLP    MultinomialNB
0.85            0.85   0.87    0.82
</code></pre>
<p>I also tried BERT; which gave an accuracy of : 0.72 on text data.</p>
<p>Can someone please explain to me here:</p>
<ol>
<li><p>Why BERT performed so low - despite that BERT is preferred by many. I followed the exact steps as mentioned here for my task: <a href=""https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/</a></p>
</li>
<li><p>What do the accuracy scores imply about different algorithms in regards to data? And will it be wise to choose either MLP / SVM or choose Random forest due to a jump in performance.</p>
</li>
</ol>
",Text Generation & LLMs,doe pipeline work nlp task accuracy score imply first would like thank community whose support could actually finish making end end data science project including deployment question wanted create app predict success failure depending upon textual input data order construct model instead developing algorithm scratch used existing algorithm instead training dataframe textual corpus frequently wrote saw several code example however clearly understand usage pipeline anyone please explain share link help people like non programming background understand better completing experiment obtained accuracy score text data increased number data initial accuracy score also tried bert gave accuracy text data someone please explain bert performed low despite bert preferred many followed exact step mentioned task accuracy score imply different algorithm regard data wise choose either mlp svm choose random forest due jump performance
Processing Time Too much slow in NLP Bert Module,"<p>I am a beginner in python and NLP I have a file that contains a module.
When I give question and answer_text to the module it returns an answer to the question using nlp but the processing time is very large about 15-20 minutes.
Is there a hardware problem or function optimation?</p>
<p>answer_extractor.py</p>
<pre><code>import torch

from transformers import BertForQuestionAnswering

def find_answer(question,answer_text):
    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
    from transformers import BertTokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
    input_ids = tokenizer.encode(question, answer_text)
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    # Search the input_ids for the first instance of the `[SEP]` token.
    sep_index = input_ids.index(tokenizer.sep_token_id)
    # The number of segment A tokens includes the [SEP] token istelf.
    num_seg_a = sep_index + 1
    # The remainder are segment B.
    num_seg_b = len(input_ids) - num_seg_a
    # Construct the list of 0s and 1s.
    segment_ids = [0] * num_seg_a + [1] * num_seg_b
    # There should be a segment_id for every input token.
    assert len(segment_ids) == len(input_ids)
    # Run our example through the model.
    outputs = model(torch.tensor([input_ids]),  # The tokens representing our input text.
                    token_type_ids=torch.tensor([segment_ids]),
                    # The segment IDs to differentiate question from answer_text
                    return_dict=True)
    start_scores = outputs.start_logits
    end_scores = outputs.end_logits
    # Find the tokens with the highest `start` and `end` scores.
    answer_start = torch.argmax(start_scores)
    answer_end = torch.argmax(end_scores)
    # Combine the tokens in the answer and print it out.
    answer = ' '.join(tokens[answer_start:answer_end + 1])
    return answer
</code></pre>
",Text Generation & LLMs,processing time much slow nlp bert module beginner python nlp file contains module give question answer text module return answer question using nlp processing time large minute hardware problem function optimation answer extractor py
Summarization-Text rank algorithm,"<p>What are the advantages of using text rank algorithm for summarization over BERT summarization?
Even though both can be used as extractive summarization method, is there any particular advantage for text rank?</p>
",Text Generation & LLMs,summarization text rank algorithm advantage using text rank algorithm summarization bert summarization even though used extractive summarization method particular advantage text rank
String cleaning/preprocessing for BERT,"<p>So my goal is to train a BERT Model on wikipedia data that I derive right from Wikipedia.
The contents that I scrape from the site look like this (example):</p>
<hr />
<p>&quot;(148975) 2001 XA255, provisional designation: 2001 XA255, is a dark minor planet in the outer Solar System, classified as centaur, approximately 38 kilometers (24 miles) in diameter. [...] \nFour temporary Neptune co-orbitals: (148975) 2001 XA255, (310071) 2010 KR59, (316179) 2010 EN65, and 2012 GX17 de la Fuente Marcos, C., &amp; de la Fuente Marcos, R. 2012, Astronomy and Astrophysics, Volume 547, id.L2, 7 pp.\nIAU list of centaurs and scattered-disk objects\nIAU list of trans-neptunian objects\nAnother list of TNOs\n(148975) 2001 XA255 at the JPL Small-Body Database&quot;</p>
<hr />
<p>The Program that I wrote until now to clean up the strings looks like this:</p>
<pre><code>import nltk
import string
import re
nltk.download('words')
words = set(nltk.corpus.words.words())

def clean_text(text):
    '''
    This function removes punctuation, words containing numbers
    as well as making the whole text lower-case
    '''
    text = text.lower()
    text = text.encode(&quot;ascii&quot;, errors=&quot;ignore&quot;).decode()
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = re.sub('\n', '', text)
    #text = &quot; &quot;.join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())
    text= text.split()
    for word in text:
        if len(word) &lt;=1:
            text.remove(word)
    text = ' '.join([str(elem) for elem in text])    
    return text
</code></pre>
<hr />
<p>By using this code the data is a bit cleaned up but still very messy.
<a href=""https://i.sstatic.net/6Geds.png"" rel=""nofollow noreferrer"">Output</a>
Any suggestions how to improve the function or should it be enough for the BERT model to run decently?</p>
",Text Generation & LLMs,string cleaning preprocessing bert goal train bert model wikipedia data derive right wikipedia content scrape site look like example xa provisional designation xa dark minor planet outer solar system classified centaur approximately kilometer mile diameter nfour temporary neptune co orbitals xa kr en gx de la fuente marcos c de la fuente marcos r astronomy astrophysics volume id l pp niau list centaur scattered disk object niau list trans neptunian object nanother list tnos n xa jpl small body database program wrote clean string look like using code data bit cleaned still messy output suggestion improve function enough bert model run decently
How to fine tune BERT Base (uncased model ) for generating embeddings?,"<p>On internet,all I have found is example for classifcation tasks.But,in my problem there is no label.(I only have a set of tweets).
My task goes as follows :
Generate Word embeddings using BERT,now use this Word embeddings in next task.
My objective :
I want to fine tune BERT to produce better word embeddings.
How to do that?</p>
",Text Generation & LLMs,fine tune bert base uncased model generating embeddings internet found example classifcation task problem label set tweet task go follows generate word embeddings using bert use word embeddings next task objective want fine tune bert produce better word embeddings
How does SimCSE do dropout twice indepently,"<p>How can I pass an input sentence into bert with dropout twice independently?</p>
<p>here is what i try so far, the outputs are identical.</p>
<pre><code>bert = AutoModel.from_pretrained('bert-base-cased')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
sent_dict = tokenizer('Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel', return_tensors='pt')

bert(**sent_dict).pooler_output == bert(**sent_dict).pooler_output

</code></pre>
",Text Generation & LLMs,doe simcse dropout twice indepently pas input sentence bert dropout twice independently try far output identical
implement do_sampling for custom GPT-NEO model,"<pre><code>import numpy as np
from transformers import GPTNeoForCausalLM, GPT2Tokenizer 
import coremltools as ct
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)

sentence_fragment = &quot;The Oceans are&quot;

class NEO(torch.nn.Module):
    def __init__(self, model):
        super(NEO, self).__init__()
        self.next_token_predictor = model
    
    def forward(self, x):
        sentence = x
        predictions, _ = self.next_token_predictor(sentence)
        token = torch.argmax(predictions[-1, :], dim=0, keepdim=True)
        sentence = torch.cat((sentence, token), 0)
        return sentence

token_predictor = GPTNeoForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;, torchscript=True).eval()

context = torch.tensor(tokenizer.encode(sentence_fragment))
random_tokens = torch.randint(10000, (5,))
traced_token_predictor = torch.jit.trace(token_predictor, random_tokens)

model = NEO(model=traced_token_predictor)
scripted_model = torch.jit.script(model)

# Custom model

sentence_fragment = &quot;The Oceans are&quot;

for i in range(10):
    context = torch.tensor(tokenizer.encode(sentence_fragment))
    torch_out = scripted_model(context)
    sentence_fragment = tokenizer.decode(torch_out)
print(&quot;Custom model: {}&quot;.format(sentence_fragment))

# Stock model

model = GPTNeoForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;, torchscript=True).eval()

sentence_fragment = &quot;The Oceans are&quot;

input_ids = tokenizer(sentence_fragment, return_tensors=&quot;pt&quot;).input_ids
gen_tokens = model.generate(input_ids, do_sample=True, max_length=20)
gen_text = tokenizer.batch_decode(gen_tokens)[0]
print(&quot;Stock model: &quot;+gen_text)
</code></pre>
<p>RUN 1</p>
<p>Output:</p>
<hr />
<pre><code>Custom model: The Oceans are the most important source of water for the entire world
</code></pre>
<pre><code>Stock model: The Oceans are on the rise. The American Southwest is thriving, but the southern United States still
</code></pre>
<hr />
<p>RUN 2</p>
<p>Output:</p>
<hr />
<pre><code>Custom model: The Oceans are the most important source of water for the entire world. 
</code></pre>
<pre><code>Stock model: The Oceans are the land of man

This is a short video of the Australian government
</code></pre>
<hr />
<p>The custom model always returns the same output. However with the <code>do_sampling = True</code> stock <code>model.generate</code> return different results on each call. I spent a lot of time figuring out how do_sampling works for transformers so I require help from you guys, appreciate it.</p>
<p>How to code a custom model to have different results on each call?</p>
<p>Thanks!</p>
",Text Generation & LLMs,implement sampling custom gpt neo model run output run output custom model always return output however stock return different result call spent lot time figuring sampling work transformer require help guy appreciate code custom model different result call thanks
How to Create a new language model NLP? - Python,"<p>i use Google Api to transcript some audio files to text with Recognizer class. I found out there are limited numbers of languages available, and the most commonly and internationally used are part of it.
How can i
Create a new language out of vocabulary &amp; Train it, to then
Use the language as a recognizer for audio input</p>
<p>Use it as language as in en-US: `</p>
<pre><code>r = sr.Recognizer()

r.recognize_google(language=&quot;en-US&quot;,audio_text)` 
</code></pre>
<p>Note: I have made several searches but doesn't seem to get the exact answer to what i need... I'm on Python</p>
<p>Thank you</p>
",Text Generation & LLMs,create new language model nlp python use google api transcript audio file text recognizer class found limited number language available commonly internationally used part create new language vocabulary train use language recognizer audio input use language en u note made several search seem get exact answer need python thank
Speed up sentence processing by BERT in Transformers,"<p>I have this code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, AutoModel

model = 'bert-base-uncased' 
tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModel.from_pretrained(model)
Sentence_vectorList = []
for sent in x_train:

  input_sentence = torch.tensor(tokenizer.encode(sent)).unsqueeze(0)
  out = model(input_sentence)
  embeddings_of_last_layer = out[0]
  cls_embeddings = embeddings_of_last_layer[0]

  cls_layer = cls_embeddings.detach().numpy()

  sent_emd = np.average(cls_layer,axis=0)
</code></pre>
<p>The task is to take the sentence vectors and detach them in [n x 768] then I save them as sent2vec. This process is taking a lot of time. Is there a more efficient way to do it?</p>
",Text Generation & LLMs,speed sentence processing bert transformer code task take sentence vector detach n x save sent vec process taking lot time efficient way
The inputs into BERT are token IDs. How do I get the corresponding the input token VECTORs into BERT?,"<p>I am new and learning about transformers.</p>
<p>In a lot of BERT tutorials, I see the input is just the token id of the words. But surely we need to convert this token ID to a vector representation (it can be one hot encoding, or any initial vector representation for each token ID) so that it can be used by the model.</p>
<p>My question is: Where cam I find this initial vector representation for each token?</p>
",Text Generation & LLMs,input bert token id get corresponding input token vector bert new learning transformer lot bert tutorial see input token id word surely need convert token id vector representation one hot encoding initial vector representation token id used model question cam find initial vector representation token
pre-trained BERT model learning wrong way,"<p>I have trained my pre-trained BERT model from the Hugging Face library on the <code>Jigsaw Toxic Comment Classification dataset</code> to detect hateful comments. However, when I try to do infer with the positive sentences, it is giving me wrong results.</p>
<p>For example, if I provide the sentence: <code>You are a nice person</code>, sometimes it predicts as a toxic or insult but for the negative sentences, it is giving the right result.</p>
<pre class=""lang-py prettyprint-override""><code>#####################  Inference part #########
test_comment = &quot;You are a nice person&quot;

encoding = tokenizer.encode_plus(   
test_comment,   
  add_special_tokens=True,   
  max_length=512,    
  return_token_type_ids=False,   
  padding=&quot;max_length&quot;,      
  return_attention_mask=True,   
  return_tensors='pt',     
)

_, test_prediction = trained_model(encoding[&quot;input_ids&quot;], encoding[&quot;attention_mask&quot;])
test_prediction = test_prediction.flatten().numpy()

for label, prediction in zip(LABEL_COLUMNS, test_prediction):
  print(f&quot;{label}: {prediction}&quot;)
</code></pre>
<pre><code>Result: 
toxic: 0.289602130651474 
severe_toxic: 0.012312621809542179 
obscene: 0.26335516571998596 
threat: 0.0017053773626685143 
insult: 0.54698246717453 
identity_hate: 0.0013856851728633046 
</code></pre>
<p>Here are my training details:</p>
<pre><code>BERT model: bert-base-uncased 
epochs: 4 
batch-size: 16 
Learning rate: 2e-5 
Loss function: BCE loss 
</code></pre>
<p>Since the dataset is imbalanced, I have done undersampling for the majority class. I also tested with augmenting the dataset, but it didn't help much.</p>
<p>Is there any reason, why this model is associating the positive sentences with toxicity?
<a href=""https://i.sstatic.net/6AVs9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6AVs9.png"" alt=""Classification report"" /></a></p>
",Text Generation & LLMs,pre trained bert model learning wrong way trained pre trained bert model hugging face library detect hateful comment however try infer positive sentence giving wrong result example provide sentence sometimes predicts toxic insult negative sentence giving right result training detail since dataset imbalanced done undersampling majority class also tested augmenting dataset help much reason model associating positive sentence toxicity
How to fine-tune Allennlp&#39;s RoBERTa text entailment model on custom data?,"<p>I'm working on a project where I need to fine-tune <a href=""https://github.com/allenai/allennlp-models/blob/main/allennlp_models/modelcards/pair-classification-roberta-snli.json"" rel=""nofollow noreferrer"">pair-classification-roberta-snli</a> model offered by AllenNLP. I have prepared my custom dataset in the snli format but couldn't manage to find a way to retrain the model. Currently, I am following <a href=""https://github.com/dh1105/Sentence-Entailment"" rel=""nofollow noreferrer"">this approach</a> to train bert-base model for textual entailment. But how to fine-tune AllenNLP's pair-classification-roberta-snli model?</p>
",Text Generation & LLMs,fine tune allennlp roberta text entailment model custom data working project need fine tune pair classification roberta snli model offered allennlp prepared custom dataset snli format manage find way retrain model currently following approach train bert base model textual entailment fine tune allennlp pair classification roberta snli model
Using Sentence-Bert with other features in scikit-learn,"<p>I have a dataset, one feature is text and 4 more features. Sentence-Bert vectorizer transforms text data into tensors. I can use these sparse matrices directly with a machine learning classifier. Can I replace the text column with tensors? And, how can I train the model. The code below is how I transform the text into vectors.</p>
<pre><code>model = SentenceTransformer('sentence-transformers/LaBSE')
sentence_embeddings = model.encode(X_train['tweet'], convert_to_tensor=True, show_progress_bar=True)
sentence_embeddings1 = model.encode(X_test['tweet'], convert_to_tensor=True, show_progress_bar=True)
</code></pre>
",Text Generation & LLMs,using sentence bert feature scikit learn dataset one feature text feature sentence bert vectorizer transforms text data tensor use sparse matrix directly machine learning classifier replace text column tensor train model code transform text vector
Building a Character-Level Ngram Language Model with NLTK,"<p>I'm trying to build a language model on the <strong>character</strong> level with NLTK's KneserNeyInterpolated function. What I have is a frequency list of words in a pandas dataframe, with the only column being it's frequency (the word itself is the index). I've determined, based on the average length of words, that a 9-gram model would be appropriate.</p>
<pre><code>from nltk.lm.models import KneserNeyInterpolated

lm = KneserNeyInterpolated(9)
for i in range(df.shape[0]):
    lm.fit([list(ngrams(df.index[i], n = 9))])

lm.generate(num_words = 9)
# ValueError: Can't choose from empty population
</code></pre>
<p>Attempt at debugging:</p>
<pre><code>n = 9 # Order of ngram

train_data, padded_sents = padded_everygram_pipeline(4, 'whatisgoingonhere')
model = KneserNeyInterpolated(n) 
model.fit(train_data, padded_sents)

model.generate(num_words = 10)
# ['r', '&lt;/s&gt;', '&lt;/s&gt;', '&lt;/s&gt;', '&lt;/s&gt;', '&lt;/s&gt;', '&lt;/s&gt;', '&lt;/s&gt;', '&lt;/s&gt;', '&lt;/s&gt;']

</code></pre>
<p>This works (I guess?), but I can't seem to extend the functionality to successively training new words to the language model, and I still can't generate realistic words. I feel like I'm missing something basic here on how this module is supposed to work. What has made this a bit difficult is that all tutorials seem to be based on word-level ngrams.</p>
",Text Generation & LLMs,building character level ngram language model nltk trying build language model character level nltk kneserneyinterpolated function frequency list word panda dataframe column frequency word index determined based average length word gram model would appropriate attempt debugging work guess seem extend functionality successively training new word language model still generate realistic word feel like missing something basic module supposed work ha made bit difficult tutorial seem based word level ngrams
How to get multi class confidence score from fine tuned bert model?,"<p>I have used pretrained bert model for intent classification(uncased_L-12_H-768_A-12). The model showing the output as a predicted intent. Below is the code of how i am getting prediction:</p>
<pre class=""lang-py prettyprint-override""><code>sentences = [&quot;i want to open a bank account&quot;]
pred_tokens = map(tokenizer.tokenize, sentences)
pred_tokens = map(lambda tok: [&quot;[CLS]&quot;] + tok + [&quot;[SEP]&quot;], pred_tokens)
pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))
pred_token_ids = map(
    lambda tids: tids +[0]*(data.max_seq_len-len(tids)),
    pred_token_ids)
pred_token_ids = np.array(list(pred_token_ids))
predictions = model.predict(pred_token_ids).argmax(axis=-1)
for text, label in zip(sentences, predictions):
    print(&quot;text:&quot;, text, &quot;\nintent:&quot;, classes[label])
    print()
</code></pre>
<p>The output is:</p>
<pre><code>text: I want to open a bank account
intent: open_account
</code></pre>
<p>Now I want to get the confidence score of the text intent, for example:</p>
<pre><code>intent: open_account
confidence score: 98.30
</code></pre>
",Text Generation & LLMs,get multi class confidence score fine tuned bert model used pretrained bert model intent classification uncased l h model showing output predicted intent code getting prediction output want get confidence score text intent example
Restrict Vocab for BERT Encoder-Decoder Text Generation,"<p>Is there any way to restrict the vocabulary of the decoder in a Huggingface BERT encoder-decoder model? I'd like to force the decoder to choose from a small vocabulary when generating text rather than BERT's entire ~30k vocabulary.</p>
",Text Generation & LLMs,restrict vocab bert encoder decoder text generation way restrict vocabulary decoder huggingface bert encoder decoder model like force decoder choose small vocabulary generating text rather bert entire k vocabulary
Generating synonyms or similar words using BERT word embeddings,"<p>I want to generate synonyms or similar words using BERT words embeddings. 
I started to do this using BERT.
For later software integration, it has to be done in JAVA, so I went for easy-bert
(<a href=""https://github.com/robrua/easy-bert"" rel=""nofollow noreferrer"">https://github.com/robrua/easy-bert</a>). </p>

<p>It appears I can get word embeddings this way:</p>

<pre><code>try(Bert bert = Bert.load(new File(""com/robrua/nlp/easy-bert/bert-uncased-L-12-H-768-A-12""))) {
    float[][] embedding = bert.embedTokens(""A sequence"");
    float[][][] embeddings = bert.embedTokens(""Multiple"", ""Sequences"");

}
</code></pre>

<p>Do you know how I could get similars words from these word embeddings ?</p>

<p>Thanks for your help !</p>
",Text Generation & LLMs,generating synonym similar word using bert word embeddings want generate synonym similar word using bert word embeddings started using bert later software integration ha done java went easy bert appears get word embeddings way know could get similars word word embeddings thanks help
Using BERT model for parsing and doing bigram or multi-gram,"<p>The task I want to do is finding the sentiment of some phrases in a group of sentences (not of all the sentences). For example, I have these sentences:</p>
<blockquote>
<p>Ended up deciding that a pebble was best. Less charging, more simplistic ui, buttons seem easier for an older person.</p>
</blockquote>
<p>And I want to detect these phrases:</p>
<ol>
<li>less charging</li>
<li>more simplistic UI</li>
<li>easier buttons</li>
</ol>
<p>I need something that first detects the phrases, and then identifies their sentiments - they are all positive in terms of smart watches for elderly people.</p>
<p>Does the BERT model solve this problem? Or I have to find another tool/technique?</p>
",Text Generation & LLMs,using bert model parsing bigram multi gram task want finding sentiment phrase group sentence sentence example sentence ended deciding pebble wa best le charging simplistic ui button seem easier older person want detect phrase le charging simplistic ui easier button need something first detects phrase identifies sentiment positive term smart watch elderly people doe bert model solve problem find another tool technique
BERT - Pass input to an intermediate layer,"<p>Let's say all layers in bert are frozen except the last one, and I have different bert models like this one trained for different purposes - thereby, having different weights for the last unfrozen layer, whilst having the same other layers. Now, passing inputs to all models will have redundancy as all layers except the last one are frozen, so their outputs will be the same for all. So, is there a way I can skip passing inputs for each model and instead get the outputs of the 11th layer(12total-1unfrozen) from 1 model and then pass those values as inputs to the 12th layer in the other models? If this is not possible, is there a way to break down bert layers so as to achieve this task?</p>
",Text Generation & LLMs,bert pas input intermediate layer let say layer bert frozen except last one different bert model like one trained different purpose thereby different weight last unfrozen layer whilst layer passing input model redundancy layer except last one frozen output way skip passing input model instead get output th layer total unfrozen model pas value input th layer model possible way break bert layer achieve task
how to add tokens in vocab.txt which decoded as [UNK] bert tokenizer,"<p>i was decoding the tokenized tokens from <strong>bert tokenizer</strong> and it was giving <strong>[UNK]</strong> for € symbol. but i tried by add ##€ token in vocab.txt file. but it was not reflected in prediction result was same as previous it was giving [UNK] again. please let me know to solve this problem did i need to <strong>fine tune</strong> the model for again to reflect the changes in <strong>prediction</strong>. till now i was avoiding fine tuning again because it takes more than 10 hours.
Thanks in advance</p>
",Text Generation & LLMs,add token vocab txt decoded unk bert tokenizer wa decoding tokenized token bert tokenizer wa giving unk symbol tried add token vocab txt file wa reflected prediction result wa previous wa giving unk please let know solve problem need fine tune model reflect change prediction till wa avoiding fine tuning take hour thanks advance
using spacy to extract tensor by token id,"<p>I'm using spacy 3.0 to vectorize a text with a transformer model. Due to data privacy reason the vectorization has to be on a different machine than the one that trains the model. To reduce the amount of data I generate and would have to transfer between machines, I extract the token ids of the text like this:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;de_dep_news_trf&quot;)
doc = nlp(&quot;Eine Bank steht im Park.&quot;)
print(doc._.trf_data.tokens[&quot;input_ids&quot;])
</code></pre>
<p>which returns</p>
<pre><code>tensor([[    3,   917,  2565,  1302,   106,  3087, 26914,     4]])
</code></pre>
<p>Having the ids now, is it possible to extract the correct tensors from the language model (<code>de_dep_news_trf</code>) using spacy?</p>
",Text Generation & LLMs,using spacy extract tensor token id using spacy vectorize text transformer model due data privacy reason vectorization ha different machine one train model reduce amount data generate would transfer machine extract token id text like return id possible extract correct tensor language model using spacy
Can use different transformer model for tokenizer and model?,"<p>Can I use roberta for tokenizer while bert for model?</p>
<pre><code>from transformers import RobertaTokenizerFast

tokenizer = RobertaTokenizerFast.from_pretrained(&quot;./bert_tokenizer&quot;, max_len=512)

from transformers import BertForMaskedLM
config = BertConfig()
bert= BertForMaskedLM(config)
</code></pre>
",Text Generation & LLMs,use different transformer model tokenizer model use roberta tokenizer bert model
"Why are the matrices in BERT called Query, Key, and Value?","<p>Within the transformer units of <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT</a>, there are modules called Query, Key, and Value, or simply Q,K,V.</p>

<p>Based on the BERT <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">paper</a> and <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">code</a> (particularly in <a href=""https://github.com/google-research/bert/blob/master/modeling.py"" rel=""nofollow noreferrer"">modeling.py</a>), my pseudocode understanding of the forward-pass of an attention module (using Q,K,V) with a single attention-head is as follows:</p>

<pre><code>q_param = a matrix of learned parameters
k_param = a matrix of learned parameters
v_param = a matrix of learned parameters
d = one of the matrix dimensions (scalar value)

def attention(to_tensor, from_tensor, attention_mask):
    q = from_tensor * q_param
    k = to_tensor * k_param
    v = to_tensor * v_param

    attention_scores = q * transpose(k) / sqrt(d)
    attention_scores += some_function(attention_mask) #attention_mask is usually just ones
    attention_probs = dropout(softmax(attention_scores))
    context = attention_probs * v

    return context
</code></pre>

<p>Note that BERT uses ""self-attention,"" so <code>from_tensor</code> and <code>to_tensor</code> are the same in BERT; I think both of these are simply the output from the previous layer.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Why are the matrices called Query, Key, and Value?</li>
<li>Did I make any mistakes in my pseudocode representation of the algorithm?</li>
</ol>
",Text Generation & LLMs,matrix bert called query key value within transformer unit bert module called query key value simply q k v based bert paper code particularly modeling py pseudocode understanding forward pas attention module using q k v single attention head follows note bert us self attention bert think simply output previous layer question matrix called query key value make mistake pseudocode representation algorithm
create_pretraining_data.py is writing 0 records to tf_examples.tfrecord while training custom BERT model,"<p>I am writing a custom BERT model on my own corpus, I generated the vocab file using BertWordPieceTokenizer and then running below code</p>
<pre><code>!python create_pretraining_data.py
--input_file=/content/drive/My Drive/internet_archive_scifi_v3.txt
--output_file=/content/sample_data/tf_examples.tfrecord
--vocab_file=/content/sample_data/sifi_13sep-vocab.txt
--do_lower_case=True
--max_seq_length=128
--max_predictions_per_seq=20
--masked_lm_prob=0.15
--random_seed=12345
--dupe_factor=5
</code></pre>
<p>Getting output as :</p>
<p><code>INFO:tensorflow:*** Reading from input files ***</code></p>
<p><code>INFO:tensorflow:*** Writing to output files ***</code></p>
<p><code>INFO:tensorflow: /content/sample_data/tf_examples.tfrecord</code></p>
<p><code>INFO:tensorflow:Wrote 0 total instances</code></p>
<p>Not sure why I am always getting 0 instances in <code>tf_examples.tfrecord</code>, what am I doing wrong?</p>
<p>I am using <code>TF version 1.12</code>
FYI..generated vocab file is 290 KB.</p>
",Text Generation & LLMs,create pretraining data py writing record tf example tfrecord training custom bert model writing custom bert model corpus generated vocab file using bertwordpiecetokenizer running code getting output sure always getting instance wrong using fyi generated vocab file kb
BERT Summarization for a column of texts,"<p>I'm trying to summarize 100 products descriptions from a dataset. To do so i simply tried to use summarizers</p>
<pre><code>!pip install summarizers -q

from summarizers import Summarizers 

import pandas as pd
</code></pre>
<p>It worked pretty well for one sentence at a time.</p>
<pre><code>textpanasonic=&quot;The NN-CS89L offers next-level cooking convenience. Its four distinct cooking methods - steaming, baking, grilling and microwaving ensure your meals are cooked or reheated to perfection. Its multi-function capabilities can be combined to save time without compromising taste, texture or nutritional value. It’s the all-in-one kitchen companion designed for people with a busy lifestyle.&quot;



summ(textpanasonic)

The NN-CS89L offers next-level cooking convenience.
</code></pre>
<p>but <strong>do you know if it is possible to create a new column with a summary for each comment ?</strong></p>
<p>ValueError: text input must of type <code>str</code> (single example), <code>List[str]</code> (batch or single pretokenized example) or <code>List[List[str]]</code> (batch of pretokenized examples).</p>
<p>Thank you in advance ^^</p>
",Text Generation & LLMs,bert summarization column text trying summarize product description dataset simply tried use summarizers worked pretty well one sentence time know possible create new column summary comment valueerror text input must type single example batch single pretokenized example batch pretokenized example thank advance
Is it possible to see all the token rankings for masked language modelling?,"<p>I was just wondering whether it would be possible to see all the predicted tokens for masked language modelling? Specifically, all the tokens with a low probability.</p>
<p>For example, consider this masked language model:</p>
<pre><code>unmasker(&quot;I am feeling &lt;mask&gt; today&quot;)
</code></pre>
<pre><code>[{'score': 0.5322356820106506,
  'sequence': 'I am feeling good today',
  'token': 4,
  'token_str': good'},
 {'score': 0.1725485771894455,
  'sequence': 'I am feeling happy today!',
  'token': 328,
  'token_str': 'happy'},
 {'score': 0.1252109706401825,
  'sequence': 'I am feeling sad today.&quot;',
  'token': 72,
  'token_str': 'sad&quot;'},
 {'score': 0.01904081553220749,
  'sequence': 'I am feeling angry today!&quot;',
  'token': 2901,
  'token_str': 'angry'},
 {'score': 0.012199202552437782,
  'sequence': 'I am feeling fun today…',
  'token': 1174,
  'token_str': 'fun'}]
</code></pre>
<p>As you can see from my output, the top tokens are &quot;good&quot;, &quot;happy&quot;, &quot;sad&quot;, &quot;angry&quot; and &quot;fun&quot;. However, would it be possible to see all the predicted tokens beyond the top 5?</p>
<p>I just want to see all a list of all the predicted tokens: the ones which have the lowest probability - if this is possible.</p>
<p>I don't want to see the top 5 predicted; I want to see all of them.</p>
<p>Thanks.</p>
",Text Generation & LLMs,possible see token ranking masked language modelling wa wondering whether would possible see predicted token masked language modelling specifically token low probability example consider masked language model see output top token good happy sad angry fun however would possible see predicted token beyond top want see list predicted token one lowest probability possible want see top predicted want see thanks
Customize the encode module in huggingface bert model,"<p>I am working on a text classification project using <a href=""https://huggingface.co/transformers/glossary.html#token-type-ids"" rel=""nofollow noreferrer"">Huggingface transformers module</a>. The encode_plus function provides the users with a convenient way of generating the input ids, attention masks, token type ids, etc. For instance:</p>

<pre><code>from transformers import BertTokenizer

pretrained_model_name = 'bert-base-cased'
bert_base_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

sample_text = 'Bamboo poles, ‍installation by an unknown building constructor #discoverhongkong #hongkonginsta'

encoding = bert_base_tokenizer.encode_plus(
        cleaned_tweet, hashtag_string,
        max_length=70,
        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
        return_token_type_ids=True,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',  # Return PyTorch tensors
    )

print('*'*20)
print(encoding['input_ids'])
print(encoding['attention_mask'])
print(encoding['token_type_ids'])
print('*'*20)
</code></pre>

<p>However, my current project requires me to generate <strong>customized ids</strong> for a given text. For instance, for a list of words <code>[HK, US, UK]</code>, I want to generate ids for these words and let other words' ids which do not exist in this list as zero. These ids are used to find embedding in another customized embedding matrix, not from pretrained bert module.</p>

<p>How can I achieve this kind of customized encoder? Any suggestions and solutions are welcomed! Thanks~</p>
",Text Generation & LLMs,customize encode module huggingface bert model working text classification project using huggingface transformer module encode plus function provides user convenient way generating input id attention mask token type id etc instance however current project requires generate customized id given text instance list word want generate id word let word id exist list zero id used find embedding another customized embedding matrix pretrained bert module achieve kind customized encoder suggestion solution welcomed thanks
How to download bert models and load in python?,"<p>How to download bert models and load in python?</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('bert-base-nli-mean-tokens')
</code></pre>
<p><strong>How to save the pretrained model and load in python?</strong></p>
",Text Generation & LLMs,download bert model load python download bert model load python save pretrained model load python
I&#39;m using bert pre-trained model for question and answering. It&#39;s returning correct result but with lot of spaces between the text,"<p><strong>I'm using bert pre-trained model for question and answering. It's returning correct result but with lot of spaces between the text</strong></p>
<p>The code is below :</p>
<pre><code>def get_answer_using_bert(question, reference_text):
  
  bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  input_ids = bert_tokenizer.encode(question, reference_text)
  input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)

  sep_location = input_ids.index(bert_tokenizer.sep_token_id)
  first_seg_len, second_seg_len = sep_location + 1, len(input_ids) - (sep_location + 1)
  seg_embedding = [0] * first_seg_len + [1] * second_seg_len

  model_scores = bert_model(torch.tensor([input_ids]), 
  token_type_ids=torch.tensor([seg_embedding]))
  ans_start_loc, ans_end_loc = torch.argmax(model_scores[0]), torch.argmax(model_scores[1])
  result = ' '.join(input_tokens[ans_start_loc:ans_end_loc + 1])

  result = result.replace('#', '')
  return result
</code></pre>
<p>Followed by code below :</p>
<pre><code>reference_text = 'Mukesh Dhirubhai Ambani was born on 19 April 1957 in the British Crown colony of Aden (present-day Yemen) to Dhirubhai Ambani and Kokilaben Ambani. He has a younger brother Anil Ambani and two sisters, Nina Bhadrashyam Kothari and Dipti Dattaraj Salgaonkar. Ambani lived only briefly in Yemen, because his father decided to move back to India in 1958 to start a trading business that focused on spices and textiles. The latter was originally named Vimal but later changed to Only Vimal His family lived in a modest two-bedroom apartment in Bhuleshwar, Mumbai until the 1970s. The family financial status slightly improved when they moved to India but Ambani still lived in a communal society, used public transportation, and never received an allowance. Dhirubhai later purchased a 14-floor apartment block called Sea Wind in Colaba, where, until recently, Ambani and his brother lived with their families on different floors.'
question = 'What is the name of mukesh ambani brother?'

get_answer_using_bert(question, reference_text)
</code></pre>
<p>And the output is :</p>
<pre><code>'an il am ban i'
</code></pre>
<p><strong>Can anyone help me how to fix this issue. It would be really helpful.</strong></p>
",Text Generation & LLMs,using bert pre trained model question answering returning correct result lot space text using bert pre trained model question answering returning correct result lot space text code followed code output anyone help fix issue would really helpful
What does &quot;fine-tuning of a BERT model&quot; refer to?,"<p>I was not able to understand one thing , when it says &quot;fine-tuning of BERT&quot;,  what does it actually mean:</p>
<ol>
<li>Are we retraining the entire model again with new data.</li>
<li>Or are we just training top few transformer layers with new data.</li>
<li>Or we are training the entire model but considering the pretrained weights as initial weight.</li>
<li>Or there is already few layers of ANN on top of transformer layers which is only getting trained keeping transformer weight freeze.</li>
</ol>
<p>Tried Google but I am getting confused, if someone can help me on this.</p>
<p>Thanks in advance!</p>
",Text Generation & LLMs,doe fine tuning bert model refer wa able understand one thing say fine tuning bert doe actually mean retraining entire model new data training top transformer layer new data training entire model considering pretrained weight initial weight already layer ann top transformer layer getting trained keeping transformer weight freeze tried google getting confused someone help thanks advance
Which algorithm for **DOCUMENT SIMILARITY** is reasonable for calculating on local machine?,"<p>I would like to do <strong>DOCUMENT SIMILARITY</strong> about 3000 times at a program running.</p>
<p>I tried it with the algorithm of <strong><a href=""https://github.com/Tiiiger/bert_score"" rel=""nofollow noreferrer"">BERT-sore</a></strong>. However, this algorithm is too heavy to finish the program running.</p>
<p>So I have a question.</p>
<ul>
<li>BERT another algorithm</li>
<li>Doc2vec</li>
<li>USE</li>
<li>other algorithm</li>
</ul>
<p>Which is the lightest algorithm.</p>
",Text Generation & LLMs,algorithm document similarity reasonable calculating local machine would like document similarity time program running tried algorithm bert sore however algorithm heavy finish program running question bert another algorithm doc vec use algorithm lightest algorithm
What does &quot;word count&quot; refer to when calculating unigram probabilities in an unigram language model?,"<p>I'm using an unigram language model. I want to calculate the probability of each unigram. Should I divide the number of occurrences of an unigram with the number of distinct unigrams, or by the count of all unigrams?</p>
",Text Generation & LLMs,doe word count refer calculating unigram probability unigram language model using unigram language model want calculate probability unigram divide number occurrence unigram number distinct unigrams count unigrams
BERT to XLNET train model,"<p>I'm trying to do something like this in XLNet but I can't find this part in the documentation, any help would be valuable, thanks!</p>
<pre class=""lang-py prettyprint-override""><code># we access the transformer model within our bert object using the bert attribute 
# (eg bert.bert instead of bert)

embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access pooled activations with [1]
</code></pre>
<p>(Instead of bert.bert I'm trying to do it with xlnet)</p>
",Text Generation & LLMs,bert xlnet train model trying something like xlnet find part documentation help would valuable thanks instead bert bert trying xlnet
Which dimensionality reduction technique works well for BERT sentence embeddings?,"<p>I'm trying to cluster hundreds of text documents so that each each cluster represents a distinct topic. Instead of using topic modeling (which I know I could do too), I want to follow a two-step approach:</p>
<ol>
<li>Create document embeddings with Sentence-BERT (using SentenceTransformer)</li>
<li>Feed the embeddings into a cluster algorithm</li>
</ol>
<p>I know I could e.g. use k-means for step 2, but I prefer a soft cluster algorithm as my documents sometimes belong to multiple topics. So I want to get a probability for each response to belong to each cluster.
My embeddings have 768 dimensions and when implementing a soft cluster algorithm (Gaussian Mixture Models), I realized that the high dimensionality caused problems. So I was thinking about using a dimensionality reduction technique (e.g., PCA) and feed the factors into the cluster algorithm.</p>
<p>However, I'm not very familiar with dimensionality reduction in such high-dimensional space and especially not in the context of NLP. Can anyone advice on a good approach / method here?</p>
<p>Thank you!</p>
",Text Generation & LLMs,dimensionality reduction technique work well bert sentence embeddings trying cluster hundred text document cluster represents distinct topic instead using topic modeling know could want follow two step approach create document embeddings sentence bert using sentencetransformer feed embeddings cluster algorithm know could e g use k mean step prefer soft cluster algorithm document sometimes belong multiple topic want get probability response belong cluster embeddings dimension implementing soft cluster algorithm gaussian mixture model realized high dimensionality caused problem wa thinking using dimensionality reduction technique e g pca feed factor cluster algorithm however familiar dimensionality reduction high dimensional space especially context nlp anyone advice good approach method thank
Bert Tokenizer add_token function not working properly,"<p>tokenizer add_tokens is not adding new tokens.
Here is my code:</p>
<pre><code>  from transformers import BertTokenizer

  bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

  new_tokens = []
  text = open(&quot;parsed_data.txt&quot;, &quot;r&quot;)
  for line in text:
     for word in line.split():
        new_tokens.append(word) 

  print(len(new_tokens))      # 53966
  print(len(bert_tokenizer))  # 36369
  bert_tokenizer.add_tokens(new_tokens)
  print(len(bert_tokenizer))  # 36369
</code></pre>
",Text Generation & LLMs,bert tokenizer add token function working properly tokenizer add token adding new token code
fill-mask usage from transformers pipeline,"<p>I fine-tune a gpt2 language model and I am generation the text according to my model by using following lines of code:</p>
<p>generator = pipeline('text-generation', tokenizer='gpt2', model='data/out')
print(generator('Once upon a time', max_length=40)[0]['generated_text'])</p>
<p>Now I want to do the prediction of only next word with the probabilities. I know we can do it by using 'fill-mask' but I don't know how to do it. When I put 'fill-mask' inplace of 'text-generation', I am getting this error:</p>
<p>&quot;Unrecognized configuration class &lt;class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'&gt; for this kind of AutoModel: AutoModelForMaskedLM.
Model type should be one of BigBirdConfig, Wav2Vec2Config, ConvBertConfig, LayoutLMConfig, DistilBertConfig, AlbertConfig, BartConfig, MBartConfig, CamembertConfig, XLMRobertaConfig, LongformerConfig, RobertaConfig, SqueezeBertConfig, BertConfig, MobileBertConfig, FlaubertConfig, XLMConfig, ElectraConfig, ReformerConfig, FunnelConfig, MPNetConfig, TapasConfig, DebertaConfig, DebertaV2Config, IBertConfig.&quot;.</p>
<p>generator = pipeline('fill-mask', tokenizer='gpt2', model='data/out') // this line is giving me the above mentioned error.</p>
<p>Please let me know how can I fix this issue. Any kind of help would be greatly appreciated.
Thanks in advance.</p>
<p>The whole code for better understanding.</p>
<p>from transformers import (
GPT2Tokenizer,
DataCollatorForLanguageModeling,
TextDataset,
GPT2LMHeadModel,
TrainingArguments,
Trainer,
pipeline)</p>
<p>train_path = 'parsed_data.txt'
test_path = 'parsed_data.txt'</p>
<p>tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)</p>
<p>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)</p>
<p>train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=128)
test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=128)</p>
<p>model = GPT2LMHeadModel.from_pretrained('gpt2')</p>
<p>training_args = TrainingArguments(output_dir = 'data/out', overwrite_output_dir = True, per_device_train_batch_size = 32, per_device_eval_batch_size = 32, learning_rate = 5e-5, num_train_epochs = 3,)</p>
<p>trainer = Trainer(model = model, args = training_args, data_collator=data_collator, train_dataset = train_dataset, eval_dataset = test_dataset)</p>
<p>trainer.train()</p>
<p>trainer.save_model()
generator = pipeline('fill-mask', tokenizer='gpt2', model='data/out')</p>
",Text Generation & LLMs,fill mask usage transformer pipeline fine tune gpt language model generation text according model using following line code generator pipeline text generation tokenizer gpt model data print generator upon time max length generated text want prediction next word probability know using fill mask know put fill mask inplace text generation getting error unrecognized configuration class class transformer model gpt configuration gpt gpt config kind automodel automodelformaskedlm model type one bigbirdconfig wav vec config convbertconfig layoutlmconfig distilbertconfig albertconfig bartconfig mbartconfig camembertconfig xlmrobertaconfig longformerconfig robertaconfig squeezebertconfig bertconfig mobilebertconfig flaubertconfig xlmconfig electraconfig reformerconfig funnelconfig mpnetconfig tapasconfig debertaconfig debertav config ibertconfig generator pipeline fill mask tokenizer gpt model data line giving mentioned error please let know fix issue kind help would greatly appreciated thanks advance whole code better understanding transformer import gpt tokenizer datacollatorforlanguagemodeling textdataset gpt lmheadmodel trainingarguments trainer pipeline train path parsed data txt test path parsed data txt tokenizer gpt tokenizer pretrained gpt data collator datacollatorforlanguagemodeling tokenizer tokenizer mlm false train dataset textdataset tokenizer tokenizer file path train path block size test dataset textdataset tokenizer tokenizer file path test path block size model gpt lmheadmodel pretrained gpt training args trainingarguments output dir data overwrite output dir true per device train batch size per device eval batch size learning rate e num train epoch trainer trainer model model args training args data collator data collator train dataset train dataset eval dataset test dataset trainer train trainer save model generator pipeline fill mask tokenizer gpt model data
How to use BERT trained model from Jupyter Notebook to another Ubuntu 20.04 server,"<p>We have finetuned our BERT model for text2text generation. It is working fine on the Jupyter notebook. But when I use the same trained model on another server of Ubuntu, then it shows the issue. This is my first post, so please bear with me. The issue I'm facing is that when I generate output on small sentences, it works fine. But on long sentences, it shows the following error:</p>
<blockquote>
<p>At most 4 tokens in tensor([    2,     2,     2,     2, 44763, 44763,     2, 44763]) can be equal to <code>eos_token_id: 2</code>. Make sure tensor([    2,     2,     2,     2, 44763, 44763,     2, 44763]) are corrected.</p>
</blockquote>
<p>My output generation code is:</p>
<pre><code>from simpletransformers.seq2seq import Seq2SeqModel
#logging.basicConfig(level=logging.INFO)
#transformers_logger = logging.getLogger(&quot;transformers&quot;)
#transformers_logger.setLevel(logging.ERROR)
model = Seq2SeqModel(
    encoder_decoder_type=&quot;bart&quot;, encoder_decoder_name=&quot;PATHOFMODEL&quot;,use_cuda=False,
)
while True:
    original = input(&quot;Enter text to paraphrase: &quot;)
    to_predict = [original]

    preds = model.predict(to_predict)

    print(&quot;---------------------------------------------------------&quot;)
    print(original)

    print()
    print(&quot;Predictions &gt;&gt;&gt;&quot;)
    for pred in preds[0]:
        print(pred)

    print(&quot;---------------------------------------------------------&quot;)
    print()
</code></pre>
",Text Generation & LLMs,use bert trained model jupyter notebook another ubuntu server finetuned bert model text text generation working fine jupyter notebook use trained model another server ubuntu show issue first post please bear issue facing generate output small sentence work fine long sentence show following error token tensor equal make sure tensor corrected output generation code
Fine-tune a BERT model for context specific embeddigns,"<p>I'm trying to find information on how to train a BERT model, possibly from the <a href=""https://huggingface.co/transformers/index.html"" rel=""noreferrer"">Huggingface Transformers</a> library, so that the embedding it outputs are more closely related to the context o the text I'm using.</p>
<p>However, all the examples that I'm able to find, are about fine-tuning the model for another task, such as <a href=""https://huggingface.co/transformers/training.html"" rel=""noreferrer"">classification</a>.</p>
<p>Would anyone happen to have an example of a BERT fine-tuning model for masked tokens or next sentence prediction, that outputs another raw BERT model that is fine-tuned to the context?</p>
<p>Thanks!</p>
",Text Generation & LLMs,fine tune bert model context specific embeddigns trying find information train bert model possibly huggingface transformer library embedding output closely related context text using however example able find fine tuning model another task classification would anyone happen example bert fine tuning model masked token next sentence prediction output another raw bert model fine tuned context thanks
how to create a seq2seq NLP model based on a transformer with BERT as the encoder?,"<p>I am super confused on how to create a seq2seq NLP model based on a transformer with BERT as the encoder.</p>
<p>Please kindly advise if the process below is even correct?
(If you know any materials which would help me figure this out, please kindly share...)</p>
<hr />
<p>[encoer]</p>
<p>-create seq2seq transformer layer</p>
<p>-define BERT-attention</p>
<p>-pool the encoder</p>
<p>[decoder]</p>
<p>-create transformer decoder</p>
<p>[pool]</p>
<p>-concatenate encoder and decoder</p>
<hr />
<p>I would really appreciate any advice or tips! (even just a reference)</p>
",Text Generation & LLMs,create seq seq nlp model based transformer bert encoder super confused create seq seq nlp model based transformer bert encoder please kindly advise process even correct know material would help figure please kindly share encoer create seq seq transformer layer define bert attention pool encoder decoder create transformer decoder pool concatenate encoder decoder would really appreciate advice tip even reference
How can i calculate all recall accuracy precision and f1 measure for multi class classification in BERT?,"<pre><code>from sklearn.metrics import f1_score

def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds_flat, average='weighted')

def accuracy_per_class(preds, labels):
    label_dict_inverse = {v: k for k, v in label_dict.items()}
    
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')
</code></pre>
<p>Need  to calculate classification report for multi class model but it gives accuracy and f1  score only</p>
",Text Generation & LLMs,calculate recall accuracy precision f measure multi class classification bert need calculate classification report multi class model give accuracy f score
BERT from tfhub SLOW and not using GPU,"<p>I'm trying out <a href=""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1"" rel=""nofollow noreferrer"">this BERT model from TFHub</a>.
Sadly it's running extremely slow, and only using 1-2% GPU according to Windows Task Manager. </p>

<p>Is there anything I can do to speed this up?</p>

<pre><code>import tensorflow as tf
import tensorflow_hub as hub

tf.test.is_gpu_available(True) # returns True

max_seq_length = 128

input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=""input_word_ids"")
input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=""input_mask"")
segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=""segment_ids"")

bert_layer = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1"", trainable=False)
pooled_out, seq_out = bert_layer([input_word_ids, input_mask, segment_ids])
model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_out, seq_out])

t = tf.random.uniform((1000, max_seq_length), maxval=1, dtype=tf.int32)
outputs = model.predict([t, t, t]) # super slow...
</code></pre>
",Text Generation & LLMs,bert tfhub slow using gpu trying bert model tfhub sadly running extremely slow using gpu according window task manager anything speed
Iterating through multiple files with BERT for QA returns nothing,"<p>I am trying to ease my job. I need to do some analysis on the answers BERT gives me for thousands of files. My main objective is to iterate through every file and ask A question.</p>
<p>I have been trying to automate it with the following code</p>
<pre><code>import os

directory = '/content/dva/'

for filename in os.listdir(directory):
with open(directory + filename) as infile:
    try:

      nlp({
    'question': 'How is artificial intelligence being used in real time health delivery?',
    'context': data
})
    except:
        print(filename + ' is throwing an error')
</code></pre>
<p>The above code returns nothing. Yet, if I do them one by one. It works fine. So I tried changing it.</p>
<pre><code>x = [&quot;How is artificial intelligence being used in real time health delivery?&quot;,\
     &quot;What adjunctive or supportive methods can help patients?&quot;,\
     &quot;How does hypertension affect patients?&quot;,\
      &quot;What does the computer do?&quot;]

y = [item.strip() for item in x]

def testing(theList):
  nlp = pipeline('question-answering')

  for each_element in theList:
    nlp({'question': each_element,'context': data})


  

testing(y) # returns nothing
print(testing(y)) # returns None
</code></pre>
<p>Does anyone have any insights? The above code works perfectly for Allen's ELMo.</p>
",Text Generation & LLMs,iterating multiple file bert qa return nothing trying ease job need analysis answer bert give thousand file main objective iterate every file ask question trying automate following code code return nothing yet one one work fine tried changing doe anyone insight code work perfectly allen elmo
How can I build domain specific language models using Open-AI GPT for natural language generation?,"<p>I want to build my own language model using Open-AI GPT. How can I build domain specific language models using Open-AI GPT for natural language generation?</p>
",Text Generation & LLMs,build domain specific language model using open ai gpt natural language generation want build language model using open ai gpt build domain specific language model using open ai gpt natural language generation
Backpropagation in bert,"<p>i would like to know when people say pretrained bert model, is it only the final classification neural network is trained</p>
<p>Or</p>
<p>Is there any update inside transformer through back propagation along with classification neural network</p>
",Text Generation & LLMs,backpropagation bert would like know people say pretrained bert model final classification neural network trained update inside transformer back propagation along classification neural network
How do I truncate long document for bert?,"<p>I am trying some bert tutorial in my language, document(korean, non-latin)<br />
however, document is very long. so i have to truncate it. and I dont know how.<br />
if there is a text(ex: 5, &quot;I have a brown cat&quot;) longer than max_length(for ex:3), then which one is the right truncation do i have to make? (dont think about start/end word/mask)<br />
a: [(&quot;I have a&quot;), (&quot;brown cat [pad]&quot;)] or  b:[(&quot;I have a&quot;), (&quot;have a brown&quot;), (&quot;a brown cat&quot;)]<br />
which one should be better?<br />
or is there any better solution?</p>
",Text Generation & LLMs,truncate long document bert trying bert tutorial language document korean non latin however document long truncate dont know text ex brown cat longer max length ex one right truncation make dont think start end word mask brown cat pad b brown brown cat one better better solution
How can I show multiple predictions of the next word in a sentence?,"<p>I am using the GPT-2 pre trained model. the code I am working on will get a sentence and generate the next word for that sentence. I want to print multiple predictions, like the three first predictions with best probabilities! 
for example if I put in the sentence ""I's an interesting ...."" 
predictions:  ""Books""    ""story""    ""news""</p>

<p>is there a way I can modify this code to show these predictions instead of one?! </p>

<p>also there are two parts in the code, I do not understand, what is the meaning of the numbers in  <code>(predictions[0, -1, :])</code>?  and why do we put <code>[0]</code> after <code>predictions = output[0]</code>? </p>

<pre><code>
import torch
from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load pre-trained model tokenizer (vocabulary)
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Encode a text inputs
text = ""The fastest car in the  ""
indexed_tokens = tokenizer.encode(text)


# Convert indexed tokens in a PyTorch tensor
tokens_tensor = torch.tensor([indexed_tokens])


# Load pre-trained model (weights)
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Set the model in evaluation mode to deactivate the DropOut modules
model.eval()

# If you have a GPU, put everything on cuda
#tokens_tensor = tokens_tensor.to('cuda')
#model.to('cuda')

# Predict all tokens
with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0]
    #print(predictions)


# Get the predicted next sub-word
predicted_index = torch.argmax(predictions[0, -1, :]).item()
predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])

# Print the predicted word
#print(predicted_index)
print(predicted_text)
</code></pre>

<p>The result for the above code will be :</p>

<pre><code>The fastest car in the world.
</code></pre>
",Text Generation & LLMs,show multiple prediction next word sentence using gpt pre trained model code working get sentence generate next word sentence want print multiple prediction like three first prediction best probability example put sentence interesting prediction book story news way modify code show prediction instead one also two part code understand meaning number put result code
Number of dimensions for each word vector in a given model &#39;en&#39; is 0,"<p>I want to print the number of dimensions for each word vector from the spacy language model 'en'. I installed both the language model and spacy. I have following lines of code</p>
<pre><code>import spacy
nlp =spacy.load('en')
dim = nlp.vocab.vectors_length
print(dim)
</code></pre>
<p>It gives 0.</p>
<p>Can someone help me with this?</p>
",Text Generation & LLMs,number dimension word vector given model en want print number dimension word vector spacy language model en installed language model spacy following line code give someone help
"Smoothing N-gram language models with Zr = Nr / 0.5 (t - q); What to do with the final frequency? (q, r, and t are successive non-zero frequences)","<p>Question about language modeling, I'm trying to implement a Katz back-off model.</p>
<p>Many PDFs mention the equation I'm asking about: <code>Zr = Nr / 0.5 (t - q)</code></p>
<p>Here's one for example.</p>
<p><a href=""https://www.csie.ntu.edu.tw/%7Eb92b02053/print/good-turing-smoothing-without.pdf"" rel=""nofollow noreferrer"">https://www.csie.ntu.edu.tw/~b92b02053/print/good-turing-smoothing-without.pdf</a></p>
<blockquote>
<p>Following the work in Church and Gale [1991], we average with each
non-zero Nr with the zero Nr’s that surround it: order the non-zero Nr
by r, and let q, r, and t be successive indices of non-zero values. We
replace Nr by Zr=Nr/0. 5 (t−q). In other words we estimate the
expected Nr by the density of Nr for larger. For small r, there is no
difference, because the length of the intervals is unity.</p>
</blockquote>
<p>They use the following table as an example of the data.</p>
<pre><code>+---------+----------+
|frequency|frequency |
|         |of        |
|         |frequency |
+---------+----------+
|r        |Nr        |
+---------+----------+
|1        |268       |
+---------+----------+
|2        |112       |
+---------+----------+
|3        |70        |
+---------+----------+
|4        |41        |
+---------+----------+
|5        |24        |
+---------+----------+
|6        |14        |
+---------+----------+
|7        |15        |
+---------+----------+
|400      |1         |
+---------+----------+
|1918     |1         |
+---------+----------+
</code></pre>
<p>I see that <code>Zr = Nr / 0.5 (t-q)</code> smoothing mentioned in a lot of places but I never see mentioned how the final <code>Nr</code> is dealt with. q, r, and t represent 3 successive elements in a list. Once you get to the last element of the list, when that last element is r, then there is no t. What do you do then? Is it just excluded with the effect being deemed negligible after things are smoothed and a linear regression model is fit?</p>
",Text Generation & LLMs,smoothing n gram language model zr nr q final frequency q r successive non zero frequence question language modeling trying implement katz back model many pdfs mention equation asking one example following work church gale average non zero nr zero nr surround order non zero nr r let q r successive index non zero value replace nr zr nr q word estimate expected nr density nr larger small r difference length interval unity use following table example data see smoothing mentioned lot place never see mentioned final dealt q r represent successive element list get last element list last element r excluded effect deemed thing smoothed linear regression model fit
how can i modify language model before applying patterns,"<p>I have this code :</p>
<pre><code>from spacy.matcher import Matcher,PhraseMatcher
import spacy
from spacy.matcher import Matcher

nlp = spacy.load(&quot;en_core_web_sm&quot;)
matcher = Matcher(nlp.vocab,validate=True)
patterns = [
    [{'POS': 'QUALIF'}, {'POS': 'CCONJ'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],
]
matcher.add(&quot;process_1&quot;, None, *patterns)

texts= [&quot;it is a beautiful and big apple&quot;]
for text in texts:
  doc = nlp(text)
  matches = matcher(doc)
  for _, start, end in matches:
      print(doc[start:end].text)
</code></pre>
<p>So, I want to modify for example the POS of <code>&quot;beautiful&quot;</code> from <code>&quot;ADJ&quot;</code>  to <code>&quot;QUALIF&quot;</code> just before applying patterns matchers (as you know beautiful in spacy language model is <code>&quot;ADJ&quot;</code> but me I want to modify it to <code>&quot;QUALIF&quot;</code> just before applying the matcher.
Help please.</p>
",Text Generation & LLMs,modify language model applying pattern code want modify example po applying pattern matcher know beautiful spacy language model want modify applying matcher help please
how can I get the logit values as probabilities from gpt-2?,"<p>I'm using the gpt-2 simple package: <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">https://github.com/minimaxir/gpt-2-simple</a></p>

<p>I would like to get a probability for all possible next tokens as an output. something like:</p>

<p>[ ['A', 0.25], ['B',0.25], ['C',0.25], ['D',0.25] ]</p>

<p>I've modified the gpt_2_simple python code looks something like this:</p>

<pre><code>full_output = sample.sample_sequence(
    hparams=hparams,
    length=min(length, 1023 - (len(context_tokens) if prefix else 0)),
    start_token=enc.encoder['&lt;|endoftext|&gt;'] if not prefix else None,
    context=context if prefix else None,
    batch_size=batch_size,
    temperature=temperature, top_k=top_k, top_p=top_p
)

logit_output = full_output[:,0:]

out = sess.run(output, feed_dict={context: batch_size * [context_tokens]})

logit_out = sess.run(logit_output, feed_dict={context: batch_size * [context_tokens]})
</code></pre>

<p>I was hoping to link the output tokens to their temperature divided logit values and then decode them, to get probabilities for each token like the example above. </p>

<p>Can anyone help me to reformat this code so that I can access the output token / logit probability combinations?</p>
",Text Generation & LLMs,get logit value probability gpt using gpt simple package would like get probability possible next token output something like b c modified gpt simple python code look something like wa hoping link output token temperature divided logit value decode get probability token like example anyone help reformat code access output token logit probability combination
AllenNLP SRL: Is it possible to get sense info for a lemma?,"<p>I'm using AllenNLP for SRL, but the output doesn't provide role information / frame info / lemmas for the verb. All of that would be helpful.</p>
<p>I see that this information seems to exist here: <a href=""https://github.com/allenai/allennlp-models/blob/ef5d4229f62f3ea8f44345d43b6a7fd1ab2d09fa/allennlp_models/common/ontonotes.py#L255"" rel=""nofollow noreferrer"">on the models github</a>, but I'm not sure if it's accessible for use with the latest/best SRL models that I believe use BERT.</p>
<p>Would it be possible to get the sense, frame, and lemma information alongside the SRL parse, using the state-of-the-art? If so, how is it done?</p>
",Text Generation & LLMs,allennlp srl possible get sense info lemma using allennlp srl output provide role information frame info lemma verb would helpful see information seems exist model github sure accessible use latest best srl model believe use bert would possible get sense frame lemma information alongside srl parse using state art done
What are the inputs to the transformer encoder and decoder in BERT?,"<p>I was reading the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a> and was not clear regarding the inputs to the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">transformer</a> encoder and decoder. </p>

<p>For learning masked language model (Cloze task), the paper says that 15% of the tokens are masked and the network is trained to predict the masked tokens. Since this is the case, what are the inputs to the transformer encoder and decoder?</p>

<p><a href=""https://i.sstatic.net/jIIuo.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/jIIuo.png"" alt=""BERT input representation (from the paper)""></a></p>

<p>Is the input to the transformer encoder this input representation (see image above). If so, what is the decoder input?</p>

<p>Further, how is the output loss computed? Is it a softmax for only the masked locations? For this, the same linear layer is used for all masked tokens?</p>
",Text Generation & LLMs,input transformer encoder decoder bert wa reading bert paper wa clear regarding input transformer encoder decoder learning masked language model cloze task paper say token masked network trained predict masked token since case input transformer encoder decoder input transformer encoder input representation see image decoder input output loss computed softmax masked location linear layer used masked token
How to fine tune BERT to summarize articles,"<p>I'm learning nlp, and as a study project, i'm trying to face the <a href=""https://www.kaggle.com/sunnysai12345/news-summary"" rel=""nofollow noreferrer"">news summarization dataset</a>, using BERT.</p>

<p>The dataset is simple (in the news_summary_more.csv) - it has <strong>articles</strong> and <strong>headlines</strong> columns.</p>

<p>What I think, is to use the <strong>articles</strong> as feature (X), and <strong>headlines</strong> as target (Y).</p>

<p>My question is how do I measure success in the fine-tuning, so the model will learn?</p>

<p>The model prediction will almost never be the same the the real target (y != ^y), because there are so many options to summarize the articles, and almost 100% of the cases the model will have somewhat different summarization than the real headline.</p>

<p>Example:</p>

<pre><code>Team A won the game against team B
</code></pre>

<p>is different from:</p>

<pre><code>Team A is victorious against team B on the game
</code></pre>

<p>Although both has pretty much the same meaning.</p>

<p>So how can I test if the model is getting close to the real target? (maybe somehow using MSE)</p>
",Text Generation & LLMs,fine tune bert summarize article learning nlp study project trying face news summarization dataset using bert dataset simple news summary csv ha article headline column think use article feature x headline target question measure success fine tuning model learn model prediction almost never real target many option summarize article almost case model somewhat different summarization real headline example different although ha pretty much meaning test model getting close real target maybe somehow using mse
How to use GPT-2 for topic modelling?,"<p>I want to generate topics and subtopics from a corpus. It would be great if someone could share the python code.</p>
",Text Generation & LLMs,use gpt topic modelling want generate topic subtopics corpus would great someone could share python code
Input format for BERT fine-tuning on corpus,"<p>I want to fine-tune BERT on a specific language domain using the following git repo:</p>
<p><a href=""https://github.com/cedrickchee/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/README.md"" rel=""nofollow noreferrer"">https://github.com/cedrickchee/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/README.md</a></p>
<p>Regarding the input format, it says:</p>
<blockquote>
<p>The scripts in this folder expect a single file as input, consisting
of untokenized text, with one sentence per line, and one blank line
between documents. The reason for the sentence splitting is that part
of BERT's training involves a next sentence objective in which the
model must predict whether two sequences of text are contiguous text
from the same document or not, and to avoid making the task too easy,
the split point between the sequences is always at the end of a
sentence. The linebreaks in the file are therefore necessary to mark
the points where the text can be split.</p>
</blockquote>
<p>What do they mean with documents in this regard? From my understanding, the <code>.txt</code> file used for fine-tuning just contains a lot of domain specific text with one sentence per line. Just to be sure, is it the correct approach to use this repository if I want to fine tune BERT on a specific language domain?</p>
<p>Thank you for your help!</p>
",Text Generation & LLMs,input format bert fine tuning corpus want fine tune bert specific language domain using following git repo regarding input format say script folder expect single file input consisting untokenized text one sentence per line one blank line document reason sentence splitting part bert training involves next sentence objective model must predict whether two sequence text contiguous text document avoid making task easy split point sequence always end sentence linebreaks file therefore necessary mark point text split mean document regard understanding file used fine tuning contains lot domain specific text one sentence per line sure correct approach use repository want fine tune bert specific language domain thank help
Using GPT-2 with your own dictionary of words,"<p>I'm training the gpt-2 with custom encodings and custom vocab.bpe file. However, when I generate text using gpt-2, the output tokens have range that exceeds the range of my new encodings. 
How can I make gpt-2 work for me then?</p>
",Text Generation & LLMs,using gpt dictionary word training gpt custom encoding custom vocab bpe file however generate text using gpt output token range exceeds range new encoding make gpt work
How many characters can be input into the &quot;prompt&quot; for GPT-2,"<p>I'm using the OpenAI GPT-2 model from <a href=""https://github.com/openai/gpt-2"" rel=""noreferrer"">github</a></p>
<p>I think that the top_k parameter dictates how many tokens are sampled. Is this also the parameter that dictates how large of a prompt can be given?</p>
<p>If top_k = 40, how large can the prompt be?</p>
",Text Generation & LLMs,many character input prompt gpt using openai gpt model github think top k parameter dictate many token sampled also parameter dictate large prompt given top k large prompt
Is there a GPT-2 implementation that allows me to fine-tune and prompt for text completion?,"<p>I wish to to fine-tune a GPT-2 implementation on some text data. I then want to use this model to complete a text prompt. I can do the first part easily enough using Max Woolf's <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">gpt-2-simple</a> implementation. And <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">Neil Shepherd's fork</a> of OpenAI allows for GPT-2 to be trained on new data and completes text.</p>

<p>However, my corpus is too small to train on and not get gibberish back. Is there any way I can combine the two functions? Ideally, I'd like to be able to do this via a python interface (as opposed to CLI), as I'd like to use pandas for data cleaning and what-have-you. Thanks.</p>
",Text Generation & LLMs,gpt implementation allows fine tune prompt text completion wish fine tune gpt implementation text data want use model complete text prompt first part easily enough using max woolf gpt simple implementation neil shepherd fork openai allows gpt trained new data completes text however corpus small train get gibberish back way combine two function ideally like able via python interface opposed cli like use panda data cleaning thanks
"What should be the behavior of trigrams to predict next word, given a input size of 2 words?","<p>My bigram language model works fine when one word is given in input, but when I give two words to my trigram model, it behaves strangely and predicts 'unknown' as the next word.
<strong>My code:</strong></p>
<pre><code>def get_unigram_probability(word):
  if word not in unigram:
      return 0
  return unigram[word] / total_words
    
def get_bigram_probability(words):
  if words not in bigram:
      return 0
  return bigram[words] / unigram[words[0]]
    
V = len(vocabulary)

def get_trigram_probability(words):
  if words not in trigram:
      return 0
  return trigram[words] + 1 / bigram[words[:2]] + V
  
</code></pre>
<p>for bi-gram next word prediction:</p>
<pre><code>def find_next_word_bigram(words):
  candidate_list = []

  # Calculate probability for each word by looping through them
  for word in vocabulary:
    p2 = get_bigram_probability((words[-1], word))
    candidate_list.append((word, p2))
    
  # sort the list with words with often occurence in the beginning
  candidate_list.sort(key=lambda x: x[1], reverse=True)
  # print(candidate_list)
  return candidate_list[0]
</code></pre>
<p>for trigram:</p>
<pre><code>def find_next_word_trigram(words):
  candidate_list = []

  # Calculate probability for each word by looping through them
  for word in vocabulary:
    p3 = get_trigram_probability((words[-2], words[-1], word)) if len(words) &gt;= 3 else 0
    candidate_list.append((word, p3))
    
  # sort the list with words with often occurence in the beginning
  candidate_list.sort(key=lambda x: x[1], reverse=True)
  # print(candidate_list)
  return candidate_list[0]
</code></pre>
<p>I just want to know where in the code should I make changes, so that trigram would predict the next word with a given input size of 2 words.</p>
",Text Generation & LLMs,behavior trigram predict next word given input size word bigram language model work fine one word given input give two word trigram model behaves strangely predicts unknown next word code bi gram next word prediction trigram want know code make change trigram would predict next word given input size word
BERT + custom layer training performance going down with epochs,"<p>I'm training a classification model with custom layers on top of BERT. During this, the training performance of this model is going down with increasing epochs ( after the first epoch ) .. I'm not sure what to fix here - is it the model or the data?</p>
<p>( for the data it's binary labels, and balanced in the number of data points for each label).</p>
<p>Any quick pointers on what the problem could be? Has anyone come across this before?</p>
<p>Edit: Turns out there was a mismatch in the transformers library and tf version I was using. Once I fixed that, the training performance was fine!</p>
<p>Thanks!</p>
",Text Generation & LLMs,bert custom layer training performance going epoch training classification model custom layer top bert training performance model going increasing epoch first epoch sure fix model data data binary label balanced number data point label quick pointer problem could ha anyone come across edit turn wa mismatch transformer library tf version wa using fixed training performance wa fine thanks
word synonym / antonym detection,"<p>I need to create a classifier that takes 2 words and determines if they are synonyms or antonyms. I tried nltk's antsyn-net but it doesn't have enough data.</p>
<p>example:</p>
<ul>
<li>capitalism &lt;-[antonym]-&gt; socialism</li>
<li>capitalism =[synonym]= free market</li>
<li>god &lt;-[antonym]-&gt; atheism</li>
<li>political correctness &lt;-[antonym]-&gt; free speach</li>
<li>advertising =[synonym]= marketing</li>
</ul>
<p>I was thinking about taking a BERT model, because may be some of the relations would be embedded  in it and transfer-learn on a data-set that I found.</p>
",Text Generation & LLMs,word synonym antonym detection need create classifier take word determines synonym antonym tried nltk antsyn net enough data example capitalism antonym socialism capitalism synonym free market god antonym atheism political correctness antonym free speach advertising synonym marketing wa thinking taking bert model may relation would embedded transfer learn data set found
Fluctuating RAM in google colab while running a BERT model,"<p>I am running a simple comment classification task on google colab. I am using DistilBERT for contextual embeddings.I use only 4000 training sample cause the notebook keeps on crashing.
When I run the cell for obtaining the embeddings, I keep a tab on how the RAM utilisation increases. I am seeing that it oscillates from somewhere between 3gb to 8gb.
Should not it be just increasing? Can anyone explain how this works at lower level.</p>
<p>Here is my code, the cell block at last is where I am seeing the above said thing.</p>
<pre class=""lang-py prettyprint-override""><code># For DistilBERT:
model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')

## Want BERT instead of distilBERT? Uncomment the following line:
#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')

# Load pretrained model/tokenizer
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

max_len=80
tokenized = sample['comment_text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True,max_length= max_len)))

padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])

attention_mask = np.where(padded != 0, 1, 0)
attention_mask.shape

input_ids = torch.tensor(padded)  
attention_mask = torch.tensor(attention_mask)

**with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)**
</code></pre>
",Text Generation & LLMs,fluctuating ram google colab running bert model running simple comment classification task google colab using distilbert contextual embeddings use training sample cause notebook keep crashing run cell obtaining embeddings keep tab ram utilisation increase seeing oscillates somewhere gb gb increasing anyone explain work lower level code cell block last seeing said thing
How Bert change max sequence length when we do fine tune task?,"<p>Suppose we use pretrained model with max sequence length 128。<br />
Now I change the config file, reduce the max sequence length from 128 to 64。<br />
Next, do the fine tune task , such as a smiple classifaction task。<br />
My question is, during the fine tune, will the <strong>dimension</strong> of model changed ？ Since it has many part relevent to max sequence length.<br />
How does bert solve this problem when we change the max sequence length for fine tune.</p>
",Text Generation & LLMs,bert change max sequence length fine tune task suppose use pretrained model max sequence length change config file reduce max sequence length next fine tune task smiple classifaction task question fine tune dimension model changed since ha many part relevent max sequence length doe bert solve problem change max sequence length fine tune
tag generation from a small text content (such as tweets),"<p>I have already asked a <a href=""https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content"">similar question</a> earlier but I have notcied that I have big constrain: I am working on small text sets suchs as user Tweets to generate tags(keywords).</p>

<p>And it seems like the accepted suggestion ( point-wise mutual information algorithm) is meant to work on bigger documents.</p>

<p>With this constrain(working on small set of texts), how can I generate tags ?</p>

<p>Regards</p>
",Text Generation & LLMs,tag generation small text content tweet already asked href question earlier notcied big constrain working small text set suchs user tweet generate tag keywords seems like accepted suggestion point wise mutual information algorithm meant work bigger document constrain working small set text generate tag regard
how to predict a masked word in a given sentence,"<p>FitBERT is an useful package , but I have a small doubt on BERT development for masked word prediction as below: I trained a bert model with custom corpus using Google's Scripts like <code>create_pretraining_data.py</code>, <code>run_pretraining.py</code>, <code>extract_features.py</code> etc..as a result I got vocab file, <code>.tfrecord</code> file, <code>.json</code> file and check point files.</p>
<p>Now how to use those file for your package to predict a masked word in a given sentence??</p>
",Text Generation & LLMs,predict masked word given sentence fitbert useful package small doubt bert development masked word prediction trained bert model custom corpus using google script like etc result got vocab file file file check point file use file package predict masked word given sentence
Retrieve elements from a 3D tensor with a 2D index tensor,"<p>I am playing around with GPT2 and I have 2 tensors:</p>
<p><strong>O</strong>: An output tensor of shaped (B, S-1, V) where B is the batch size S is the the number of timestep and V is the vocabulary size. This is the output of a generative model and is softmaxed along the 2nd dimension.</p>
<p><strong>L</strong>: A 2D tensor shaped (B, S-1) where each element is the index of the correct token for each timestep for each sample. This is basically the labels.</p>
<p>I want to extract the predicted probability of the corresponding correct token from tensor <strong>O</strong> based on tensor <strong>L</strong> such that I will end up with a 2D tensor shaped (B, S). Is there an efficient way of doing this apart from using loops?</p>
",Text Generation & LLMs,retrieve element tensor index tensor playing around gpt tensor output tensor shaped b v b batch size number timestep v vocabulary size output generative model softmaxed along nd dimension l tensor shaped b element index correct token timestep sample basically label want extract predicted probability corresponding correct token tensor based tensor l end tensor shaped b efficient way apart using loop
Why BERT model have to keep 10% MASK token unchanged?,"<p>I am reading BERT model paper. In Masked Language Model task during pre-training BERT model, the paper said the model will choose 15% token ramdomly. In the chose token (Ti), 80% it will be replaced with [MASK] token, 10% Ti is unchanged and 10% Ti replaced with another word. I think the model just need to replace with [MASK] or another word is enough. Why does the model have to choose randomly a word and keep it unchanged? Does pre-training process predict only [MASK] token or it predict 15% a whole random token?</p>
",Text Generation & LLMs,bert model keep mask token unchanged reading bert model paper masked language model task pre training bert model paper said model choose token ramdomly chose token ti replaced mask token ti unchanged ti replaced another word think model need replace mask another word enough doe model choose randomly word keep unchanged doe pre training process predict mask token predict whole random token
How to generate a text using keywords which should look similar to the sentence I have?,"<p>I have a text,</p>
<pre><code>    text = 'Morning Sue, I wondered if we could catch up this week, 19th?I wanted to discuss the quote I sent last week B-GreW2020-026 for my aviation policy. I think it had a limit of £30,000,000 and an excess of £200,000. Let me know what works for you? Shall we include Willis into the call from a brokerage perspective?'
</code></pre>
<p>I have an excel file,</p>
<pre><code>                  Ref_no            Limit    Excess 
              Co-MS N2020-501     3471463    520000
</code></pre>
<p>I would like to generate a text like above sentence using the keywords from excel? I am sure there's a way to do this with AI. I ready about text augmentation as well. I know this can be done using regex but I am looking for a way using sentence generator. Kindly help.</p>
<p>Expected output:</p>
<pre><code>  Output = 'Morning Sue, I wondered if we could catch up this week, 19th?I wanted to discuss the quote I sent last week Co-MS N2020-501 for my aviation policy. I think it had a limit of £3471463 and an excess of 520000. Let me know what works for you? Shall we include Willis into the call from a brokerage perspective?'
</code></pre>
<p>Or somewhat related text using the keywords.</p>
",Text Generation & LLMs,generate text using keywords look similar sentence text excel file would like generate text like sentence using keywords excel sure way ai ready text augmentation well know done using regex looking way using sentence generator kindly help expected output somewhat related text using keywords
Fine tune GPT-2 on large text for generate a domain text,"<p>Tryin to train GPT-2 on a very large text, in order to generate text from <strong>specific domain</strong>.<br />
Working with tensorflow2 .</p>
<p>For example, let's say I have all of Harry Potter books :)<br />
And I want to train the GPT-2 on them, so I could later generate text from the Harry Potter domain.</p>
<pre><code>from tensorflow.keras.utils import get_file
from transformers import GPT2Tokenizer, TFGPT2Model

text = '...'
# Length of text: 474429 characters
# 84 unique characters

tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
model = TFGPT2Model.from_pretrained('gpt2-medium')

encoded_input = tokenizer(text, return_tensors='tf') # ERROR
output = model(encoded_input)

input_ids = tokenizer.encode('severus snape', return_tensors='tf')
greedy_output = model.generate(input_ids, max_length=50)
print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))
</code></pre>
<blockquote>
<p>ERROR: Token indices sequence length is longer than the specified
maximum sequence length for this model (149887 &gt; 1024). Running this
sequence through the model will result in indexing errors</p>
</blockquote>
<p>So how would I make it work?<br />
How to feed the model a large new text to train on?</p>
<p><strong>EDIT:</strong><br />
when Trying to concat, tokenizer works, but model doesn't:</p>
<pre><code>from textwrap import wrap
text_batches = wrap(text, 1000)

encoded_input = None

for tb in text_batches:
    current = tokenizer(tb, return_tensors='tf')
  
    if encoded_input == None:
        encoded_input = current
    else:
        encoded_input['input_ids']      = tf.concat([encoded_input['input_ids'], current['input_ids']], axis=-1)
        encoded_input['attention_mask'] = tf.concat([encoded_input['attention_mask'], current['attention_mask']], axis=1)

output = model(encoded_input) # ERROR
</code></pre>
<blockquote>
<p>ERROR:  InvalidArgumentError: indices[0,1024] = 1024 is not in [0,
1024) [Op:ResourceGather]</p>
</blockquote>
<p>What am I missing?</p>
",Text Generation & LLMs,fine tune gpt large text generate domain text tryin train gpt large text order generate text specific domain working tensorflow example let say harry potter book want train gpt could later generate text harry potter domain error token index sequence length longer specified maximum sequence length model running sequence model result indexing error would make work feed model large new text train edit trying concat tokenizer work model error invalidargumenterror index op resourcegather missing
How could I generate questions True/False from a given text with python?,"<p>I would like to generate questions True/False from a given text.</p>
<p>For example:</p>
<p>Text:</p>
<p>&quot;Jim's dog was very hairy and smelled like wet newspaper&quot;</p>
<p>Questions:</p>
<p>-&quot;Jim's dog was very hairy and smelled like a rose&quot; T/F</p>
<p>-&quot;Jim does not have a dog&quot; T/F</p>
",Text Generation & LLMs,could generate question true false given text python would like generate question true false given text example text jim dog wa hairy smelled like wet newspaper question jim dog wa hairy smelled like rose f jim doe dog f
Generate MCQ from a text,"<p>I need to know how to generate MCQ(Multiple Choice Question) questions from a text.</p>

<p><strong>Example:</strong></p>

<p><strong>Text-Input:</strong> ""Bangladesh (the People's Republic of Bangladesh) is a country in South Asia. It shares land borders with India and Myanmar (Burma).""</p>

<p><strong>Output:</strong></p>

<p>i) Where is in Bangladesh?</p>

<ol>
<li>South Asia</li>
<li>Australia</li>
<li>Europe</li>
</ol>

<p>b)Which country does share land border with India and Myanmar?</p>

<ol>
<li>Thailand</li>
<li>Nepal</li>
<li>Bangladesh</li>
</ol>

<p>How can I do this?</p>
",Text Generation & LLMs,generate mcq text need know generate mcq multiple choice question question text example text input bangladesh people republic bangladesh country south asia share land border india myanmar burma output bangladesh south asia australia europe b country doe share land border india myanmar thailand nepal bangladesh
"Why do we need the custom dataset class and use of _getitem_ method in NLP, BERT fine tuning etc","<p>I am a newbie in NLP and has been studying the usage of BERT for NLP tasks. In many notebooks, I See that a custom dataset class is defined and <em>getitem</em> method is defined (along with len).</p>
<p>Tweetdataset class in this notebook - <a href=""https://www.kaggle.com/abhishek/roberta-inference-5-folds"" rel=""nofollow noreferrer"">https://www.kaggle.com/abhishek/roberta-inference-5-folds</a></p>
<p>and text_Dataset class in this notebook - <a href=""https://engineering.wootric.com/when-bert-meets-pytorch"" rel=""nofollow noreferrer"">https://engineering.wootric.com/when-bert-meets-pytorch</a></p>
<p>Can some one please explain the reason, need for defining the custom dataset class and the <em>getitem</em> (and len) method. thank you</p>
",Text Generation & LLMs,need custom dataset class use getitem method nlp bert fine tuning etc newbie nlp ha studying usage bert nlp task many notebook see custom dataset class defined getitem method defined along len tweetdataset class notebook text dataset class notebook one please explain reason need defining custom dataset class getitem len method thank
How to use the outputs of bert model?,"<p>The bert model gives us the two outputs, one gives us the [batch,maxlen,hiddenstates] and other one is [batch, hidden States of cls token]. But I did not understood when to use the specific output. Can anyone tell  me for which task which output should be used??</p>
",Text Generation & LLMs,use output bert model bert model give u two output one give u batch maxlen hiddenstates one batch hidden state cl token understood use specific output anyone tell task output used
How can I get an alignment for two different tokenizations? (e.g. BERT vs spaCy),"<p>I have two tokenizations for a text ""I'll go there"" as follows:</p>

<pre class=""lang-py prettyprint-override""><code>a == [""I"", ""'ll"", ""go"", ""there""]
b == [""I'll"", ""go"", ""there""]
</code></pre>

<p>How can I efficiently get the following alignment?</p>

<pre class=""lang-py prettyprint-override""><code>a2b == [[0], [0], [1], [2]]
b2a == [[0, 1], [2], [3]]
</code></pre>

<p>In addition, if two tokenizations are normalized differently, is there efficient way to get the alignment? 
""two tokenizations are normalized differently"" means, for example:</p>

<pre class=""lang-py prettyprint-override""><code>a == [""à"", ""la"", ""gorge""]
b == [""a"", ""la"", ""gorge""] # dropped accent
</code></pre>

<p>I want the following alignment result:</p>

<pre class=""lang-py prettyprint-override""><code>a2b == [[0], [1], [2]]
b2a == [[0], [1], [2]]
</code></pre>
",Text Generation & LLMs,get alignment two different tokenizations e g bert v spacy two tokenizations text go follows efficiently get following alignment addition two tokenizations normalized differently efficient way get alignment two tokenizations normalized differently mean example want following alignment result
Kaldi&#39;s objects explained in layman&#39;s term,"<p>I am trying to understand the internal workings of Kaldi, however is having trouble understanding the technical details of <a href=""http://kaldi-asr.org/doc/hmm.html"" rel=""nofollow noreferrer"">kaldi's doc</a>. </p>

<p>I want to have a high-level understanding of various objects first in order to help digest what is presented. I would specifically like to know what the .tree, fina.mdl, and HCLG.fst files are, what is needed to generate them and how they are being used. </p>

<p>Vaguely I understand that (please correct me if I am wrong):</p>

<ul>
<li>final.mdl is the acoustic model and contains the probability of transitioning from one phone to another. </li>
<li>HCLG.fst is a graph that given a sequence of phones it will generate the most likely word sequence based on the lexicon, grammar and language model. </li>
<li>decoding-graph is the term for generating the HCLG.fst</li>
<li>not quite sure what adding a self-loop is, is it similar to the Kleene operator?</li>
<li>lattice contain alternative word-sequence for an utterance.</li>
</ul>

<p>I understand there is a lot to cover but any help is appreciated!</p>
",Text Generation & LLMs,kaldi object explained layman term trying understand internal working kaldi however trouble understanding technical detail kaldi doc want high level understanding various object first order help digest presented would specifically like know tree fina mdl hclg fst file needed generate used vaguely understand please correct wrong final mdl acoustic model contains probability transitioning one phone another hclg fst graph given sequence phone generate likely word sequence based lexicon grammar language model decoding graph term generating hclg fst quite sure adding self loop similar kleene operator lattice contain alternative word sequence utterance understand lot cover help appreciated
NLP summerization using textacy/spacy,"<p>I want to generate a summary maybe in one sentence from this text. I am using textacy.py.
Here is my code:</p>
<pre><code>import textacy
import textacy.keyterms
import textacy.extract
import spacy
nlp = spacy.load('en_core_web_sm')
text = '''Sauti said, 'O thou that art blest with longevity, I shall narrate the history of Astika as I heard it from my father. 
          O Brahmana, in the golden age, Prajapati had two daughters. 
          O sinless one, the sisters were endowed with wonderful beauty. 
          Named Kadru and Vinata, they became the wives of Kasyapa. 
          Kasyapa derived great pleasure from his two wedded wives and being gratified he, resembling Prajapati himself, offered to give each of them a boon. 
          Hearing that their lord was willing to confer on them their choice blessings, those excellent ladies felt transports of joy. 
          Kadru wished to have for sons a thousand snakes all of equal splendour. 
          And Vinata wished to bring forth two sons surpassing the thousand offsprings of Kadru in strength, energy, size of body, and prowess. 
          Unto Kadru her lord gave that boon about a multitude of offspring. 
          And unto Vinata also, Kasyapa said, 'Be it so!' Then Vinata, having; obtained her prayer, rejoiced greatly. 
          Obtaining two sons of superior prowess, she regarded her boon fulfilled. 
          Kadru also obtained her thousand sons of equal splendour. 
          'Bear the embryos carefully,' said Kasyapa, and then he went into the forest, leaving his two wives pleased with his blessings.'''

doc = textacy.make_spacy_doc(text, 'en_core_web_sm')
sentobj = nlp(text)
sentences = textacy.extract.subject_verb_object_triples(sentobj)
summary=''
for i, x in enumerate(sentences):
    subject, verb, fact = x
    print('Fact ' + str(i+1) + ': ' + str(subject) + ' : ' + str(verb) + ' : ' + str(fact))
    summary += 'Fact ' + str(i+1) + ': ' + (str(fact))

Results are as follows:
    Fact 1: I : shall narrate : history
    Fact 2: I : heard : it
    Fact 3: they : became : wives
    Fact 4: Kasyapa : derived : pleasure
    Fact 5: ladies : felt : transports
    Fact 6: Kadru : wished : have
    Fact 7: Vinata : wished : to bring
    Fact 8: lord : gave : boon
    Fact 9: Kasyapa : said : Be
    Fact 10: Vinata : obtained : prayer
    Fact 11: she : regarded : boon
    Fact 12: Kadru : obtained : sons
</code></pre>
<p>I tried</p>
<pre><code>textacy.extract.words
textacy.extract.entities
textacy.extract.ngrams
textacy.extract.noun_chunks
textacy.ke.textrank
</code></pre>
<p>Everything is working as per the book but results are not perfect.
I am wanting something like &quot;Kasyapa married Kadru and Vinata sisters&quot; or &quot;Kasyapa gave embroys to Kadru and Vinata&quot;.
Can you please suggest me how to do this? Or suggest me some alternative packages to use?</p>
",Text Generation & LLMs,nlp summerization using textacy spacy want generate summary maybe one sentence text using textacy py code tried everything working per book result perfect wanting something like kasyapa married kadru vinata sister kasyapa gave embroys kadru vinata please suggest suggest alternative package use
BERT fine tuning,"<p>I'm trying to create my model for question answering based on BERT und can't understand what is the meaning of fine tuning. Do I understand it right, that it is like adaption for specific domain? And if I want to use it with Wikipedia corpora, I just need to integrate unchanged pre-trained model in my network?</p>
",Text Generation & LLMs,bert fine tuning trying create model question answering based bert und understand meaning fine tuning understand right like adaption specific domain want use wikipedia corpus need integrate unchanged pre trained model network
Using BERT for extracting Product Features,"<p>How can I use <a href=""https://huggingface.co/transformers/model_doc/bert.html"" rel=""nofollow noreferrer"">BERT</a> to extract <strong>product features from text?</strong></p>
<p>For example, how to extract the ids from the next emails - WSK30015675, KTXFFC156, 4569TT11DRE</p>
<pre><code>Hello John, Please order 15 dresses ID WSK30015675 from our storage. Thanks.

Hello Dan, this week we need to make sure we have KTXFFC156. And please make sure to order more of 4569TT11DRE. tnx :)
</code></pre>
<p>Notes:</p>
<ul>
<li>No regex</li>
<li>The ids are unknown</li>
<li>There is enough tagged data (emails &gt;&gt; ids)</li>
<li>The ids are combinations of numbers and letter in different length</li>
</ul>
",Text Generation & LLMs,using bert extracting product feature use bert extract product feature text example extract id next email wsk ktxffc tt dre note regex id unknown enough tagged data email id id combination number letter different length
"Generating text corpus from a matrix, based on words and their weighted probabilities","<p>I have a matrix, and I am trying to generate text corpus.</p>
<pre><code>             chewbacca  darth  han  leia  luke  obi
chewbacca          0      0    0     0   0.66 0.33
darth              0      0    0     1     0    0
han                0      0    0     0     1    0
leia               0      0    0     0     1    0
luke               0      0    0     0     0    0
obi                0      0    0     0     0    0
</code></pre>
<p>I selected the work <strong>chewbacca</strong> as my first word.</p>
<p>Now I am trying to find pairs for <strong>chewbacca</strong>, based on probabilities. Two words are here - <strong>luke</strong>(0.66) and <strong>obi</strong> (0.33).</p>
<p>The second word must be based on weighted probabilities.</p>
<p>For instance, if &quot;luke&quot; pairs with &quot;chewbacca&quot; as 0.66 and &quot;obi&quot; pairs with &quot;chewbacca&quot; as 0.33, &quot;luke&quot; must be selected twice more likely than &quot;obi&quot;.</p>
<p>How to approach it? Appreciate any tips!</p>
",Text Generation & LLMs,generating text corpus matrix based word weighted probability matrix trying generate text corpus selected work chewbacca first word trying find pair chewbacca based probability two word luke obi second word must based weighted probability instance luke pair chewbacca obi pair chewbacca luke must selected twice likely obi approach appreciate tip
Storing ngram model python,"<p>I am implementing language model as a personal challenge, as a part of simple web application. Still I've avoided using NLTK, however was faced with MemoryError with enough big corpus (vocabulary about 50000 and amount of trigrams was about 440000 - I've used standard python dictionary and after tried numpy array to store all word-ngram probabilities as matrix). So it seems the solution is to use more efficient data structure, something that was mentioned here <a href=""https://stackoverflow.com/questions/38264636/train-a-language-model-using-google-ngrams"">train a language model using Google Ngrams</a> Or store model on disk. In General, could you advise what approach could be better  for storing ngram model (in memory or disk space) and after using it as a part of web-app?</p>
",Text Generation & LLMs,storing ngram model python implementing language model personal challenge part simple web application still avoided using nltk however wa faced memoryerror enough big corpus vocabulary amount trigram wa used standard python dictionary tried numpy array store word ngram probability matrix seems solution use efficient data structure something wa mentioned href language model using google ngrams store model disk general could advise approach could better storing ngram model memory disk space using part web app
What machine instance to use for running GPU workloads in Google Cloud Platform,"<p>I am trying to run Elasticsearch BERT application and would like to understand the minimal configuration for fine-tuning the model using GPU. What machine configuration should I be using?</p>
<p>Reference github: <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""noreferrer"">Fast-Bert</a></p>
",Text Generation & LLMs,machine instance use running gpu workload google cloud platform trying run elasticsearch bert application would like understand minimal configuration fine tuning model using gpu machine configuration using reference github fast bert
How to use pre-trained BERT model for next sentence labeling?,"<p>I”m new to AI and NLP. 
I want to check how bert works. 
I use BERT pre-trained model:
<a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a></p>

<p>I ran extract_features.py example , described in extract features paragraph in readme.md. 
I  got vectors, as output. </p>

<p>Guys, how to transform result, i got in extract_features.py, to get next/ not next label?</p>

<p>I want to run bert to check whether two sentences are related, and see result. </p>

<p>Thanks!</p>
",Text Generation & LLMs,use pre trained bert model next sentence labeling new ai nlp want check bert work use bert pre trained model ran extract feature py example described extract feature paragraph readme md got vector output guy transform result got extract feature py get next next label want run bert check whether two sentence related see result thanks
Why does the BERT NSP head linear layer have two outputs?,"<p>Here's the code in question. </p>

<p><a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491</a></p>

<pre><code>class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score
</code></pre>

<p>I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?</p>
",Text Generation & LLMs,doe bert nsp head linear layer two output code question think wa ranking likely one sentence would follow another one score
calculating loss and perplexity when evaluating GPT2 model even when not defined,"<p>When I'm trying to evaluate GPT2 model for text generation task, I printed loss and perplexity as given below in the code, but it is not defined in the code by me.</p>

<pre><code>with torch.no_grad():

    for _ in range(length):
        outputs = model(generated)
        next_token_logits = outputs[0][:, -1, :] 
        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1) 

        generated = torch.cat((generated, next_token), dim=1)

print(f""loss {loss}"")
print(f""perplexity {perplexity}"")

...
</code></pre>

<p>part from the output:</p>

<pre><code>loss 3.644557237625122
perplexity 38.26582717895508
</code></pre>

<p>This prints out very realistic values for loss and perplexity and I'm not sure from where this is coming from, because if I change it to loss1 and perplexity1 it would give me this error</p>

<blockquote>
<pre><code>     38 
---&gt; 39 print(f""loss {loss1}"")
     40 print(f""perplexity {perplexity}"")
     41 
NameError: name 'loss1' is not defined
</code></pre>
</blockquote>

<p>Although here, for every generation, the loss and the perplexity will be the same.</p>

<p>So, I'm so confused as to how this is calculated automatically and would like some clarification since I'm very new to Pytorch and ML. Any help is appreciated. </p>
",Text Generation & LLMs,calculating loss perplexity evaluating gpt model even defined trying evaluate gpt model text generation task printed loss perplexity given code defined code part output print realistic value loss perplexity sure coming change loss perplexity would give error although every generation loss perplexity confused calculated automatically would like clarification since new pytorch ml help appreciated
How to print the output weights for the output layer in BERT?,"<p>I would like to print the output vector/tensor in BERT an wasn't sure how to do it.  I've been using the following example to walk myself through it: </p>

<p><a href=""https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX</a></p>

<p>Its a simple classification problem, but I want to be able to get the output vector before we classify the training examples.  Can someone point to where in the code I can do this and how?</p>
",Text Generation & LLMs,print output weight output layer bert would like print output vector tensor bert sure using following example walk simple classification problem want able get output vector classify training example someone point code
RNN Text Generation: How to balance training/test lost with validation loss?,"<p>I'm working on a short project that involves implementing a character RNN for text generation. My model uses a single LSTM layer with varying units (messing around with between 50 and 500), dropout at a rate of 0.2, and softmax activation. I'm using RMSprop with a learning rate of 0.01.</p>

<p>My issue is that I can't find a good way to characterize the validation loss. I'm using a validation split of 0.3 and I'm finding that the validation loss starts to become constant after only a few epochs (maybe 2-5 or so) while the training loss keeps decreasing. <strong>Does validation loss carry much weight in this sort of problem?</strong> The purpose of the model is to generate new strings, so quantifying the validation loss with other strings seems... pointless?</p>

<p>It's hard for me to really find the best model since qualitatively I get the sense that the best model is trained for more epochs than it takes for the validation loss to stop changing but also for fewer epochs than it takes for the training loss to start increasing. I would really appreciate any advice you have regarding this problem as well as any general advice about RNN's for text generation, especially regarding dropout and overfitting. Thanks!</p>

<p>This is the code for fitting the model for every epoch. The callback is a custom callback that just prints a few tests. I'm now realizing that history_callback.history['loss'] is probably the training loss isn't it...</p>

<pre><code>for i in range(num_epochs):
    history_callback = model.fit(x, y,
          batch_size=128,
          epochs=1,
          callbacks=[print_callback],
          validation_split=0.3)
    loss_history.append(history_callback.history['loss'])
    validation_loss_history.append(history_callback.history['val_loss'])
</code></pre>

<p>My intention for this model isn't to replicate sentences from the training data, rather, I'd like to generate sentence from the same distribution that I'm training on.</p>
",Text Generation & LLMs,rnn text generation balance training test lost validation loss working short project involves implementing character rnn text generation model us single lstm layer varying unit messing around dropout rate softmax activation using rmsprop learning rate issue find good way characterize validation loss using validation split finding validation loss start become constant epoch maybe training loss keep decreasing doe validation loss carry much weight sort problem purpose model generate new string quantifying validation loss string seems hard really find best model since get sense best model trained epoch take validation loss stop changing also fewer epoch take training loss start increasing would really appreciate advice regarding problem well general advice rnn text generation especially regarding dropout overfitting thanks code fitting model every epoch callback custom callback print test realizing history callback history loss probably training loss intention model replicate sentence training data rather like generate sentence distribution training
What does &#39;theta&#39; mean in a language model?,"<p>I know that if X denotes a text , p(X) denotes the language model of the text. And most often , we use maximum likelihood estimation to estimate the language model. 
But in many cases , I find a parameter $\theta$ used to represent a language model. I don't understand the meaning of this $\theta$ . 
For Example , for a document d in a collection what purpose does $\theta$ serve in ' p(d|$\theta$) ' ? </p>

<p>Does $\theta$ represent a maximum likelihood estimator or a language model ? </p>

<p>Can someone please explain this difference between a language model and $\theta$ in depth ? </p>

<p>Thanks in advance ! </p>
",Text Generation & LLMs,doe theta mean language model know x denotes text p x denotes language model text often use maximum likelihood estimation estimate language model many case find parameter theta used represent language model understand meaning theta example document collection purpose doe theta serve p theta doe theta represent maximum likelihood estimator language model someone please explain difference language model theta depth thanks advance
How to get document embeddings using GPT-2?,"<p>I'm curious if using GPT-2 might yield a higher accuracy for document vectors (with greatly varying length) or not (would it surpass the state of the art?)</p>

<p>Really I'm most interested in document embeddings that are as accurate as possible. I'm wondering if using GPT-2 will get results that are more accurate than Paragraph Vectors for example. </p>

<p>I heard that in order to get vectors from GPT-2 ""you can use a weighted sum and/or concatenation of vector outputs at its hidden layers (typically the last few hidden layers) as a representation of its corresponding words or even ""meaning"" of the entire text, although for this role BERT is used more often as it is bi-directional and takes into account of both forward and backward contexts.""</p>

<p>As a machine learning and NLP beginner, I'd love to know how to go about this, or to be pointed in the right direction to learn more about how to attempt this in Python.</p>

<p>I've tried fine-tuning GPT-2 before but I have no idea how to extract vectors from it for text.</p>
",Text Generation & LLMs,get document embeddings using gpt curious using gpt might yield higher accuracy document vector greatly varying length would surpass state art really interested document embeddings accurate possible wondering using gpt get result accurate paragraph vector example heard order get vector gpt use weighted sum concatenation vector output hidden layer typically last hidden layer representation corresponding word even meaning entire text although role bert used often bi directional take account forward backward context machine learning nlp beginner love know go pointed right direction learn attempt python tried fine tuning gpt idea extract vector text
"Removing commas after processing lists of strings, when &#39; &#39;.join(x) does not work","<p>So I fed in a dataframe of sentences for token prediction in BERT, and I received as output along with the predictions, the sentences split into words. 
  Now i want to revert my dataframe of the split/tokenized sentences and predictions back to the original sentence.(of course i have the original sentence, but i need to do this process so that the predictions are in harmony with the sentence tokens)</p>

<pre><code>original sentence
You couldn't have done any better because if you could have, you would have.

Post processing
['[CLS]', 'You', 'couldn', ""'"", 't', 'have', 'done', 'any', 'better', 'because', 'if', 'you', 'could', 'have', ',', 'you', 'would', 'have', '.', '[SEP]']

</code></pre>

<p>I identified three processes necessary.
1. Remove quote marks 2. removes the CLS ,SEP and their extra quote marks and commas, 3. remove the commas separating the words and merge them.</p>

<pre><code>def fix_df(row):
    sentences = row['t_words'] 
    return remove_edges(sentences)

def remove_edges(sentences):
    x = sentences[9:-9]
    return remove_qmarks(x)

def remove_qmarks(x):
    y = x.replace(""'"", """")
    return join(y)

def join(y):
    z = ' '.join(y)
    return z


a_df['sents'] = a_df.apply(fix_df, axis=1) 

</code></pre>

<p>The first two functions largely worked correctly, but the last one did not. instead, i got a result that looked like this.</p>

<pre><code>Y o u , c o u l d n , "" "" , t , h a v e, d o n e ,...

</code></pre>

<p>The commas didnt go away, and the text got distorted instead. I am definitely missing something. what could that be?</p>
",Text Generation & LLMs,removing comma processing list string join x doe work fed dataframe sentence token prediction bert received output along prediction sentence split word want revert dataframe split tokenized sentence prediction back original sentence course original sentence need process prediction harmony sentence token identified three process necessary remove quote mark remove cl sep extra quote mark comma remove comma separating word merge first two function largely worked correctly last one instead got result looked like comma didnt go away text got distorted instead definitely missing something could
Get the attention vector from the last layers of BERT,"<p>Is there any way to get the attention vector with normalizing values (0-1) from the last layer of BERT? I'm interested in getting the attention value that BERT assigns to each word in a sentence. </p>

<p>I'm working on emotion classification. I want to extract the relevant words associated with emotions. For example:</p>

<p>I feel wonderful today. </p>

<p>The words feel and wonderful are the more relevant words in the sentence for the classifier, so I want to get the attention scores that BERT assigns to each of them. </p>

<p>Thanks in advance</p>
",Text Generation & LLMs,get attention vector last layer bert way get attention vector normalizing value last layer bert interested getting attention value bert assigns word sentence working emotion classification want extract relevant word associated emotion example feel wonderful today word feel wonderful relevant word sentence classifier want get attention score bert assigns thanks advance
Predicting Missing Word in Text,"<p>I know about BERT and other solution when you masking some words and try to predict them.
But let say I have a text:</p>

<blockquote>
  <p>Transformer have taken the of Natural Processing
  by storm, transforming the field by leaps and bounds. New,
  bigger, and better models to crop up almost every , 
  benchmarks in performance across a wide variety of tasks.</p>
</blockquote>

<p>And I cannot in advance say to BERT where masking is. I am looking for algorithm which can understand where missing words are and after that predict them.</p>
",Text Generation & LLMs,predicting missing word text know bert solution masking word try predict let say text transformer taken natural processing storm transforming field leap bound new bigger better model crop almost every benchmark performance across wide variety task advance say bert masking looking algorithm understand missing word predict
How to match number and text in same token - Spacy Matcher?,"<p>I have following sentence and I wanted to extract <strong>'12am'</strong> out of it.</p>

<pre><code>He is working at 12am
</code></pre>

<p>I am using the Spacy Matcher (language model <strong>en_core_web_lg</strong>) and it breaks the text into the following tokens:</p>

<pre><code>[He] [is] [working] [at] [12am]
</code></pre>

<p>And the patterns I tried are:</p>

<pre><code>[{ ""LIKE_NUM"": true }, {""IS_SPACE"": false}, { ""LOWER"": ""am"" }],
[{ ""LIKE_NUM"": true , ""LOWER"": ""am"" }],
[{ ""SHAPE"": 'dd' , ""ORTH"": ""am"" }]
</code></pre>

<p>Nothing works so far. Basically since the token is [12am].</p>

<p>I need help to create pattern for matching: </p>

<p>Any advice appreciated. Thanks </p>
",Text Generation & LLMs,match number text token spacy matcher following sentence wanted extract using spacy matcher language model en core web lg break text following token pattern tried nothing work far basically since token need help create pattern matching advice appreciated thanks
predicting Missing Words in a sentence with additional information,"<p>I have a sentence below:</p>

<pre><code>it claimed that some 250,000 people __ the rally.
</code></pre>

<p>I want to predict the missing word with Bert. And I know some additional information,like the topic of this sentence. what should I do to predict it using additional information? Is there any paper for this problem?</p>
",Text Generation & LLMs,predicting missing word sentence additional information sentence want predict missing word bert know additional information like topic sentence predict using additional information paper problem
spaCy BERT dictionary,"<p>I am trying to access spaCy BERT dictionary, but I receive strange output from the model. For instance for <code>en_core_web_lg</code> model I can extract ~1.3 million tokens like this</p>

<pre><code>nlp = spacy.load(""en_core_web_lg"") 
tokens = [t for t in nlp.vocab]
</code></pre>

<p>When I do the same for <code>en_trf_bertbaseuncased_lg</code> model I only get 478 tokens, </p>

<pre><code>nlp = spacy.load(""en_trf_bertbaseuncased_lg"") 
tokens = [t for t in nlp.vocab]
</code></pre>

<p>while there should be ~30k tokens according to <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">BERT paper</a>. Is there a way I can access them via <code>nlp.vocab</code> or via <a href=""https://spacy.io/usage/processing-pipelines#custom-components-attributes"" rel=""nofollow noreferrer"">custom component attributes</a>?</p>
",Text Generation & LLMs,spacy bert dictionary trying access spacy bert dictionary receive strange output model instance model extract million token like model get token k token according bert paper way access via via custom component attribute
I need a service for generation sentence included a set of words,"<p>I need to generate random but lexically correct sentences with my words included. The sentence doesn't have to consist entirely of my words. But the more of them the better. </p>

<p>I have already searched through a lot of resources related to machine learning, but everywhere they write about generating RANDOM texts. I can't influence the result in any way by specifying the presence of at least a couple of my words as a condition.</p>

<p>Perhaps someone on this resource knows about repositories with similar libraries or APIs where I can get something like this?</p>
",Text Generation & LLMs,need service generation sentence included set word need generate random lexically correct sentence word included sentence consist entirely word better already searched lot resource related machine learning everywhere write generating random text influence result way specifying presence least couple word condition perhaps someone resource know repository similar library apis get something like
"Is there any way to summarize text data which has numbers and tables in python, either extractive way or abstarctive way?","<p>I am dealing with tons of PDF documents (petetions data) filled with text data having numbers, tabular data etc. The objective of client is to summarize any such given document to reduce man-force in reading the entire document. I have tried conventional methods like lSA,Gensim-summarizer, BERT extractive summarizer, Pysummarizer. </p>

<p>The results are not at all good, Please suggest me any way where i can find a industry level summarizer(extrative/abstractive) that would give me a good start to solve this issue .</p>
",Text Generation & LLMs,way summarize text data ha number table python either extractive way abstarctive way dealing ton pdf document petetions data filled text data number tabular data etc objective client summarize given document reduce man force reading entire document tried conventional method like lsa gensim summarizer bert extractive summarizer pysummarizer result good please suggest way find industry level summarizer extrative abstractive would give good start solve issue
Need to Fine Tune a BERT Model to Predict Missing Words,"<p>I'm aware that BERT has a capability in predicting a missing word within a sentence, which can be syntactically correct and semantically coherent. Below is a sample code:</p>

<pre><code>import torch
from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval(); # turning off the dropout

def fill_the_gaps(text):
   text = '[CLS] ' + text + ' [SEP]'
   tokenized_text = tokenizer.tokenize(text)
   indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
   segments_ids = [0] * len(tokenized_text)
   tokens_tensor = torch.tensor([indexed_tokens])
   segments_tensors = torch.tensor([segments_ids])
   with torch.no_grad():
      predictions = model(tokens_tensor, segments_tensors)
   results = []
   for i, t in enumerate(tokenized_text):
       if t == '[MASK]':
           predicted_index = torch.argmax(predictions[0, i]).item()
           predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
           results.append(predicted_token)
   return results

 print(fill_the_gaps(text = 'I bought an [MASK] because its rainy .'))
 print(fill_the_gaps(text = 'Im sad because you are [MASK] .'))
 print(fill_the_gaps(text = 'Im worried because you are [MASK] .'))
 print(fill_the_gaps(text = 'Im [MASK] because you are [MASK] .'))
</code></pre>

<p>Can someone explain to me, do I need to fine Tune a BERT Model to predict missing words or just use the pre-trained BERT model? Thanks.</p>
",Text Generation & LLMs,need fine tune bert model predict missing word aware bert ha capability predicting missing word within sentence syntactically correct semantically coherent sample code someone explain need fine tune bert model predict missing word use pre trained bert model thanks
BERT as a service for classification?,"<p>I have fine-tuned BERT on a classification task. </p>

<p>bert-as-a-service allows getting word embeddings but I would like to get the class of an input text.</p>

<p>The problem is best described here:
<a href=""https://github.com/hanxiao/bert-as-service/issues/213"" rel=""nofollow noreferrer"">https://github.com/hanxiao/bert-as-service/issues/213</a></p>

<p>Any ideas on how to do that?</p>
",Text Generation & LLMs,bert service classification fine tuned bert classification task bert service allows getting word embeddings would like get class input text problem best described idea
"Where do the input_ids, input_mask, and segment_ids variables come from in a BERT model?","<p>I'm trying to work through the Google BERT tutorial in <a href=""https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"" rel=""nofollow noreferrer"">Google Colab</a>, and am having a hard time following some of its steps.  </p>

<p>Specifically, there is a function called <code>create_model</code> which reads like this:</p>

<pre><code>def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,
             num_labels):
""""""Creates a classification model.""""""

bert_module = hub.Module(
  BERT_MODEL_HUB,
  trainable=True)
bert_inputs = dict(
  input_ids=input_ids,
  input_mask=input_mask,
  segment_ids=segment_ids)
bert_outputs = bert_module(
  inputs=bert_inputs,
  signature=""tokens"",
  as_dict=True)

# Use ""pooled_output"" for classification tasks on an entire sentence.
# Use ""sequence_outputs"" for token-level output.
output_layer = bert_outputs[""pooled_output""]

hidden_size = output_layer.shape[-1].value

# Create our own layer to tune for politeness data.
output_weights = tf.get_variable(
  ""output_weights"", [num_labels, hidden_size],
  initializer=tf.truncated_normal_initializer(stddev=0.02))

output_bias = tf.get_variable(
  ""output_bias"", [num_labels], initializer=tf.zeros_initializer())

with tf.variable_scope(""loss""):

# Dropout helps prevent overfitting
output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

logits = tf.matmul(output_layer, output_weights, transpose_b=True)
logits = tf.nn.bias_add(logits, output_bias)
log_probs = tf.nn.log_softmax(logits, axis=-1)

# Convert labels into one-hot encoding
one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)

predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))
# If we're predicting, we want predicted labels and the probabiltiies.
if is_predicting:
  return (predicted_labels, log_probs)

# If we're train/eval, compute loss between predicted and actual label
per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
loss = tf.reduce_mean(per_example_loss)
return (loss, predicted_labels, log_probs)
</code></pre>

<p>This would be fine, but when I look for where the <code>create_model</code> function is called in the notebook, the origin of the <code>input_ids</code>, <code>input_mask</code> and <code>segment_ids</code> arguments is not clear to me.  </p>

<p>This is where the function is referenced later on in the notebook:</p>

<pre><code>def model_fn_builder(num_labels, learning_rate, num_train_steps,
                 num_warmup_steps):
""""""Returns `model_fn` closure for TPUEstimator.""""""
def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
""""""The `model_fn` for TPUEstimator.""""""

input_ids = features[""input_ids""]
input_mask = features[""input_mask""]
segment_ids = features[""segment_ids""]
label_ids = features[""label_ids""]

is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)

# TRAIN and EVAL
if not is_predicting:

  (loss, predicted_labels, log_probs) = create_model(
    is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)
</code></pre>

<p>The problem here is that the <code>features</code> argument is not listed as an argument in the parent function, and it's not defined anywhere else in the notebook.  So I'm not sure why it works, and even more importantly, I'm not sure what things like <code>features['input_ids']</code> are supposed to represent.  Without this being clear to me, it'll be difficult to make sense of what BERT actually does.</p>

<p>Thank you for your help.</p>
",Text Generation & LLMs,input id input mask segment id variable come bert model trying work google bert tutorial google colab hard time following step specifically function called read like would fine look function called notebook origin argument clear function referenced later notebook problem argument listed argument parent function defined anywhere else notebook sure work even importantly sure thing like supposed represent without clear difficult make sense bert actually doe thank help
What is the difference between Sentence Encodings and Contextualized Word Embeddings?,"<p>I have seen both terms used while reading papers about BERT and ELMo so I wonder if there is a difference between them.</p>
",Text Generation & LLMs,difference sentence encoding contextualized word embeddings seen term used reading paper bert elmo wonder difference
Extract compounds and dobj from Dependency tree using Spacy,"<p>For sentence such as : ""Replaced both carbonator float switch and pressure relief valve."" </p>

<p>i would like to extract the following : 
{replaced carbonator float switch} , {replaced pressure relief valve}</p>

<p>while using Spacy for the language model , my dependency tree for that sentence is : </p>

<p><a href=""https://i.sstatic.net/rLe95.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rLe95.png"" alt=""enter image description here""></a></p>

<p>So i know that carbonator is the direct object (djob) of Replaced (verb) , but :
1. carbonator is not on itself , but is part of the triple NOUN ""carbonator float switch"" , and also ""pressure releif valve"" is been replaced , they should also be directed object of replaced.</p>

<p>Any ideas ? </p>
",Text Generation & LLMs,extract compound dobj dependency tree using spacy sentence replaced carbonator float switch pressure relief valve would like extract following replaced carbonator float switch replaced pressure relief valve using spacy language model dependency tree sentence know carbonator direct object djob replaced verb carbonator part triple noun carbonator float switch also pressure releif valve replaced also directed object replaced idea
How to set the max number of CPU/cores to run BERT as service?,"<p>I can install and run BERT model as described in <a href=""https://github.com/hanxiao/bert-as-service"" rel=""nofollow noreferrer"">https://github.com/hanxiao/bert-as-service</a></p>

<p>It will take all the CPUs on the Linux machine when it runs, but I'd like to have it run on only 50 CPUs of the Linux server. </p>

<p>Can set I the max number of CPU/cores to run BERT model?</p>
",Text Generation & LLMs,set max number cpu core run bert service install run bert model described take cpu linux machine run like run cpu linux server set max number cpu core run bert model
ARPA language model documentation,"<p>Where can I find documentation on ARPA language model format?</p>

<p>I am developing simple speech recognition app with pocket-sphinx STT engine. ARPA is recommended there for performance reasons.
I want to understand how much can I do to adjust my language model for my custom needs.</p>

<p>All I found is some very brief ARPA format descriptions:</p>

<ul>
<li><a href=""http://kered.org/blog/2008-08-12/arpa-language-model-file-format/"" rel=""noreferrer"">http://kered.org/blog/2008-08-12/arpa-language-model-file-format/</a></li>
<li><a href=""http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html"" rel=""noreferrer"">http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html</a></li>
<li><a href=""http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html"" rel=""noreferrer"">http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html</a></li>
</ul>

<p>I am beginner to STT and I have trouble to wrap head around this (n-grams, etc...). I am looking for more detailed docs. Something like documentation on JSGF grammar here:</p>

<p><a href=""http://www.w3.org/TR/jsgf/"" rel=""noreferrer"">http://www.w3.org/TR/jsgf/</a></p>
",Text Generation & LLMs,arpa language model documentation find documentation arpa language model format developing simple speech recognition app pocket sphinx stt engine arpa recommended performance reason want understand much adjust language model custom need found brief arpa format description beginner stt trouble wrap head around n gram etc looking detailed doc something like documentation jsgf grammar
Select the missing word in the sentence from the possible options,"<p>I have a sentence with a missing word.</p>

<pre><code>""I want to buy a ___ to drive it.""
</code></pre>

<p>And I have answer options that may be here. </p>

<pre><code>[""car"", ""cat"", ""can""]
</code></pre>

<p>I need to choose the most suitable word. I wanted to do it with BERT, but there is no way to specify which words can be.</p>
",Text Generation & LLMs,select missing word sentence possible option sentence missing word answer option may need choose suitable word wanted bert way specify word
Using BERT in domain-specific corpus,"<p>I am planning to use BERT in my domain-specific corpus. Should I retrain BERT from scratch (pre-training) or can I do fine-tuning instead? How can I add my new vocabulary if I need to do fine-tuning? Thanks!</p>
",Text Generation & LLMs,using bert domain specific corpus planning use bert domain specific corpus retrain bert scratch pre training fine tuning instead add new vocabulary need fine tuning thanks
How to createNLP( natrual language processing domain language) model.....?,"<p>how to design create NLP domain language model or grammar file 
any data structure please let me know</p>

<p>
        </p>

<pre><code>        &lt;one-of&gt;
           &lt;item&gt;
               &lt;tag&gt;fruits=""Apple""&lt;tag/&gt;
                 &lt;one-of&gt;
                      &lt;item&gt;
                           &lt;token&gt;Apple&lt;/token&gt;
                      &lt;/item&gt;
                      &lt;item&gt;
                           &lt;token&gt;i want Apple&lt;/token&gt;
                      &lt;/item&gt;
                      &lt;item&gt;
                           &lt;token&gt;i want to order an apple&lt;/token&gt;
                      &lt;/item&gt;                               
                 &lt;/one-of&gt;
            &lt;/item&gt;
        &lt;/one-of&gt;
&lt;/rule&gt; 
</code></pre>
",Text Generation & LLMs,createnlp natrual language processing domain language model design create nlp domain language model grammar file data structure please let know
how to build keras model on Bert output with different shapes,"<p>i'm trying to build classifier for my study using bert and keras.</p>

<p>i got bert encoding (shape - (1,X,768)) when X is the number of words and spaces in the sentence.</p>

<p>how can i build keras model if X is not consistent?</p>
",Text Generation & LLMs,build kera model bert output different shape trying build classifier study using bert kera got bert encoding shape x x number word space sentence build kera model x consistent
Create SavedModel for BERT,"<p>I'm using <a href=""https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=ERkTE8-7oQLZ"" rel=""nofollow noreferrer"">this</a> Colab for BERT model.</p>

<p>In last cells in order to make predictions we have:</p>

<pre><code>def getPrediction(in_sentences):
  labels = [""Negative"", ""Positive""]
  input_examples = [run_classifier.InputExample(guid="""", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, """" is just a dummy label
  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)
  predictions = estimator.predict(predict_input_fn)
  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]

pred_sentences = [
  ""That movie was absolutely awful"",
  ""The acting was a bit lacking"",
  ""The film was creative and surprising"",
  ""Absolutely fantastic!""
]

predictions = getPrediction(pred_sentences)
</code></pre>

<p>I want to create a 'SavedModel' to be used with TF serving. How to create a SavedModel for this model?</p>

<p>Normally I would define the following:</p>

<pre><code>def serving_input_fn():
    """"""Create serving input function to be able to serve predictions later
    using provided inputs
    :return:
    """"""
    feature_placeholders = {
        'sentence': tf.placeholder(tf.string, [None]),     
    }
    return tf.estimator.export.ServingInputReceiver(feature_placeholders,
                                                    feature_placeholders)


latest_ckpt = tf.train.latest_checkpoint(OUTPUT_DIR)

last_eval = estimator.evaluate(input_fn=test_input_fn, steps=None, checkpoint_path=latest_ckpt)

# Export the model to GCS for serving.
exporter = tf.estimator.LatestExporter('exporter', serving_input_fn, exports_to_keep=None)
exporter.export(estimator, OUTPUT_DIR, latest_ckpt, last_eval, is_the_final_export=True)      
</code></pre>

<p>Not sure how to define my <code>tf.estimator.export.ServingInputReceiver</code></p>
",Text Generation & LLMs,create savedmodel bert using colab bert model last cell order make prediction want create savedmodel used tf serving create savedmodel model normally would define following sure define
How the α value is calculated in katz backoff language model?,"<p>Im currently working on the implementation for katz backoff smoothing language model. i have some confusion about the recursive backoff and α calculation lower order models. Suppose the katz model for trigram is given as </p>

<p>P<sub>katz</sub>(w<sub>i</sub>|w<sub>i-2</sub>w<sub>i-1</sub>)= P(w<sub>i</sub>|w<sub>i-2</sub>w<sub>i-1</sub>), if C(w<sub>i-2</sub>w<sub>i-1</sub>w<sub>i</sub>) > 0 <br/> &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; or  α<sub>1</sub>P(w<sub>i</sub>|w<sub>i-1</sub>), if C(w<sub>i-2</sub>w<sub>i-1</sub>w<sub>i</sub>) = 0 and C(w<sub>i-1</sub>w<sub>i</sub>) > 0<br/> &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;or  α<sub>2</sub>P(w<sub>i</sub>), otherwise</p>

<p>I also know that α1 for bigram case is calculated from the discounts found from trigram model but my confusion is how it calculates the α2 for lower order unigram. Does it use the discounts from the trigram model or it uses discounts from bigram model if both trigram and bigram case has zero evidence? </p>
",Text Generation & LLMs,value calculated katz backoff language model im currently working implementation katz backoff smoothing language model confusion recursive backoff calculation lower order model suppose katz model trigram given pkatz wi wi wi p wi wi wi c wi wi wi p wi wi c wi wi wi c wi wi p wi otherwise also know bigram case calculated discount found trigram model confusion calculates lower order unigram doe use discount trigram model us discount bigram model trigram bigram case ha zero evidence
My current fine-tuned BERT model saved on physical space takes 7GB of space. Is it normal for the model to take such large amount of space.?,"<p>My fine-tuned BERT model for classification taking large amount of storage space, wanted to understand if it is normal or I am not saving the model in a correct manner</p>
",Text Generation & LLMs,current fine tuned bert model saved physical space take gb space normal model take large amount space fine tuned bert model classification taking large amount storage space wanted understand normal saving model correct manner
How to convert Stanford Parser string output to data table format,"<p>I am experimenting with the Stanford NLP Parser in Python, calling Stanford tools from <code>nltk</code> module. I have already parsed a chunk of text, which is returned as a string in the following form:</p>

<pre><code>(ROOT
  (S
    (S
      (NP
        (NP (NNS MEPs))
        (PP (IN from) (PP (IN across) (NP (DT the) (NNP EU)))))
      (VP
        (VBP are)
        (VP
          (VBG organising)
          (S
            (VP
              (TO to)
              (VP
                (VB trigger)


[... continues ]
</code></pre>

<p>Now I need to represent these data into table format (e.g. a <code>numpy</code> array), so that I can run some ML model on it.</p>

<p>I want to feed some processed sequence of words into an RNN. I thought parsing information would be very useful for the understanding of the structure of the sentence, but I don't know how this is usually done. How parsing data are fed into language models?</p>

<p>How to do that?</p>
",Text Generation & LLMs,convert stanford parser string output data table format experimenting stanford nlp parser python calling stanford tool module already parsed chunk text returned string following form need represent data table format e g array run ml model want feed processed sequence word rnn thought parsing information would useful understanding structure sentence know usually done parsing data fed language model
Deep Learning methods for Text Generation (PyTorch),"<p>Greetings to everyone,</p>

<p>I want to design a system that is able to <strong>generate stories</strong> or poetry based on a large dataset of text, <strong>without being needed to feed a text description/start/summary as input at inference time.</strong></p>

<p>So far I did this using <strong>RNN</strong>'s, but as you know they have a lot of flaws. My question is, what are the best methods to achieve this task at the time?
I searched for possibilities using <strong>Attention mechanisms</strong>, but it turns out that they are fitted for translation tasks.</p>

<p>I know about GPT-2, Bert, <strong>Transformer</strong>, etc., but all of them need a text description as input, before the generation and this is not what I'm seeking. I want a system able to generate stories from scratch after training.</p>

<p>Thanks a lot!</p>
",Text Generation & LLMs,deep learning method text generation pytorch greeting everyone want design system able generate story poetry based large dataset text without needed feed text description start summary input inference time far using rnn know lot flaw question best method achieve task time searched possibility using attention mechanism turn fitted translation task know gpt bert transformer etc need text description input generation seeking want system able generate story scratch training thanks lot
On the influences of i-th token&#39;s MASK to i&#39;th token&#39;s predicted distributions in BERT masked language models,"<p>In the paper of XLNet, it is explained that the masking technique has a drawback because only when BERT models are pretrained tokens which are replaced with MASK symbols in part (fraction 15% ~ 20%) but ,in finetune process, no input has masked token.</p>

<p>My first question is :</p>

<p>Can BERT pre-trained masked language models( whose weights are freezed )  output a kind of natural distributions of the i-th token when an input sentence's i-th token is MASKED ? (This setting is meaningless in all applications to other NLP tasks but my intent is to understand the role of MASK.)</p>

<p>In here, unlikely token means two patterns: the first one is ulikeliness in the pretrain corpus such as the sentence ""I bite a dog everyday"" which has more likely sentence ""A dog bite me yesterday"" or ""I pat a dog everyday."" according to the common sense, and the second pattern is a grammar errors like ""I bought a  milk everyday."" instead of ""I buy a milk everyday"". So, my question is asking when ""I MASK a dog everyday"" and ""I byte a dog everyday"" are inputted into the same pretrained BERT model using large corpora with common senses and correct grammars, then do two corresponding prredicted distributions have greatly different shape ?</p>

<p>If the answer to the first question is yes, then can I check the difference of distributions between unusual sentence and the sentence whose unusual tokens are MASKED through KL divergence function of these two predicted distributions ?</p>
",Text Generation & LLMs,influence th token mask th token predicted distribution bert masked language model paper xlnet explained masking technique ha drawback bert model pretrained token replaced mask symbol part fraction finetune process input ha masked token first question bert pre trained masked language model whose weight freezed output kind natural distribution th token input sentence th token masked setting meaningless application nlp task intent understand role mask unlikely token mean two pattern first one ulikeliness pretrain corpus sentence bite dog everyday ha likely sentence dog bite yesterday pat dog everyday according common sense second pattern grammar error like bought milk everyday instead buy milk everyday question asking mask dog everyday byte dog everyday inputted pretrained bert model using large corpus common sens correct grammar two corresponding prredicted distribution greatly different shape answer first question yes check difference distribution unusual sentence sentence whose unusual token masked kl divergence function two predicted distribution
NLP text transformation Changing subject,"<p>I'm new to the world of NLP but would like to know if there's currently any easy way (using a service or OSS etc) of changing the subject of a bulk of text using NLP where the original subject is known (and preferably if such a method is available in multiple languages?) (Equivalent to the method sentences().toPastTense() detailed / available here: <a href=""https://nlp-compromise.github.io"" rel=""nofollow noreferrer"">https://nlp-compromise.github.io</a> )</p>

<p>Lets say that the original text is written about ""you"", and you know that to always be the case, but you want to automatically generate a version of the text that would be changed to be about ""your brother""</p>

<p><strong>(quite nonsensical) example:</strong></p>

<blockquote>
  <p>""You should go down the hall, as you reach the corner you are done.""</p>
</blockquote>

<p>Would turn into</p>

<blockquote>
  <p>""Your brother should go down the hall, as he reaches the corner he is
  done.""</p>
</blockquote>

<p>As far as I understand, this type of text transformation is reliant on lemmatisation (as demonstrated in this post <a href=""https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63"" rel=""nofollow noreferrer"">https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63</a>), but as I've been researching text transformation methods I haven't seen any methods pertaining to the subject of sentences?</p>
",Text Generation & LLMs,nlp text transformation changing subject new world nlp would like know currently easy way using service os etc changing subject bulk text using nlp original subject known preferably method available multiple language equivalent method sentence topasttense detailed available let say original text written know always case want automatically generate version text would changed brother quite nonsensical example go hall reach corner done would turn brother go hall reach corner done far understand type text transformation reliant lemmatisation demonstrated post researching text transformation method seen method pertaining subject sentence
Why is a throw-away column required in Bert format?,"<p>I have recently come across Bert(Bidirectional Encoder Representations from Transformers). I saw that Bert requires a strict format for the train data. The third column needed is described as follows:</p>

<p><b>Column 3:</b> A column of all the same letter — this is a throw-away column that you need to include because the BERT model expects it.</p>

<p>What is a throw-away column and why is this column needed in the dataset  since it is stated that it contains the same letter?</p>

<p>Thank you.</p>
",Text Generation & LLMs,throw away column required bert format recently come across bert bidirectional encoder representation transformer saw bert requires strict format train data third column needed described follows column column letter throw away column need include bert model expects throw away column column needed dataset since stated contains letter thank
How to get sentence embeddings from encoder in Fastai learner language model,"<p>I was able to fine tune a language model using fast ai.  I would like to extract sentence embeddings from the fine-tuned model for sentence similarity.  How do I get the encoder model embeddings? Also can embeddings be compared with dot product like other embeddings from other models such as USE?</p>

<pre><code>data_lm = TextLMDataBunch.from_df(train_df = se1, valid_df = se2, path = """",text_cols='text')
learn = language_model_learner(data_lm,drop_mult=0.7,pretrained=True,arch=AWD_LSTM)
learn.fit_one_cycle(3, 1e-01)
</code></pre>

<p>My code is above how can I get encodings from learn?</p>
",Text Generation & LLMs,get sentence embeddings encoder fastai learner language model wa able fine tune language model using fast ai would like extract sentence embeddings fine tuned model sentence similarity get encoder model embeddings also embeddings compared dot product like embeddings model use code get encoding learn
How to use pretrained checkpoints of BERT model on semantic text similarity task?,"<p>I am unaware to use the derived checkpoints from pre-trained BERT model for the task of semantic text similarity.</p>

<p>I have run a pre-trained BERT model with some domain of corpora from scratch. I have got the checkpoints and graph.pbtxt file from the code below. But I am unaware on how to use those files for evaluating semantic text similarity test file.</p>

<pre><code>!python create_pretraining_data.py \
          --input_file=/input_path/input_file.txt \
          --output_file=/tf_path/tf_examples.tfrecord \
          --vocab_file=/vocab_path/uncased_L-12_H-768_A-12/vocab.txt \
          --do_lower_case=True \
          --max_seq_length=128 \
          --max_predictions_per_seq=20 \
          --masked_lm_prob=0.15 \
          --random_seed=12345 \
          --dupe_factor=5

!python run_pretraining.py \
      --input_file=/tf_path/tf_examples.tfrecord \
      --output_dir=pretraining_output \
      --do_train=True \
      --do_eval=True \
      --bert_config_file=/bert_path/uncased_L-12_H-768_A-12/bert_config.json \
      --init_checkpoint=/bert_path/uncased_L-12_H-768_A-12/bert_model.ckpt\
      --train_batch_size=32 \
      --max_seq_length=128 \
      --max_predictions_per_seq=20 \
      --num_train_steps=20 \
      --num_warmup_steps=10 \
      --learning_rate=2e-5
</code></pre>
",Text Generation & LLMs,use pretrained checkpoint bert model semantic text similarity task unaware use derived checkpoint pre trained bert model task semantic text similarity run pre trained bert model domain corpus scratch got checkpoint graph pbtxt file code unaware use file evaluating semantic text similarity test file
Using BERT for classification given character length or number of words in a sentence,"<p>I have a dataset of titles, their descriptions and 0 or 1s that correspond to whether the description is valid or not. I want to be able to classify whether they are valid or not based on BERT alongside the character/word count of the description. How would I do so?</p>
",Text Generation & LLMs,using bert classification given character length number word sentence dataset title description correspond whether description valid want able classify whether valid based bert alongside character word count description would
How can I perform an action on all occurrences of an element in a list?,"<p>I wanted to make texts readable for BERT-embeddings by inserting the [CLS] and [SEP] tokens. I tokenized my text so I have a list with every word and punctuation mark as element, however, I don't know how exactly I can add elements after every occurrence of '.' in my text. </p>

<p>Does anyone know what I can do? Or do you know if there is something that prepares BERT-readable-texts?</p>
",Text Generation & LLMs,perform action occurrence element list wanted make text readable bert embeddings inserting cl sep token tokenized text list every word punctuation mark element however know exactly add element every occurrence text doe anyone know know something prepares bert readable text
Uni-directional Transformer VS Bi-directional BERT,"<p>I just finished reading the <a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""noreferrer"">Transformer</a> paper and <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">BERT</a> paper. But couldn't figure out why Transformer is uni-directional and BERT is bi-directional as mentioned in BERT paper. As they don't use recurrent networks, it's not so straightforward to interpret the directions. Can anyone give some clue? Thanks.</p>
",Text Generation & LLMs,uni directional transformer v bi directional bert finished reading transformer paper bert paper figure transformer uni directional bert bi directional mentioned bert paper use recurrent network straightforward interpret direction anyone give clue thanks
How to generate embeddings using Bert,"<p>I started using the following Kaggle kernel:</p>

<ul>
<li><a href=""https://www.kaggle.com/taindow/bert-a-fine-tuning-example"" rel=""nofollow noreferrer"">https://www.kaggle.com/taindow/bert-a-fine-tuning-example</a></li>
</ul>

<p>After, I have used the following code which is getting close:</p>

<pre><code>bert_config = modeling.BertConfig.from_json_file(bert_config_file)

processor = ColaProcessor()
label_list = processor.get_labels()

tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)

tpu_cluster_resolver = None
is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2

train_examples = processor.get_train_examples(data_dir)
num_train_steps = int(len(train_examples) / train_batch_size * num_train_epochs)
num_warmup_steps = int(num_train_steps * warmup_proportion)

print(""Feature Test"")
features = convert_examples_to_features(examples=train_examples, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer)
print(""Feature Test Completed"")
</code></pre>

<p>After I do this, I get the following output for my <code>features</code> variable:</p>

<pre><code> &lt;run_classifier.InputFeatures at 0x7f798eece780&gt;,
 &lt;run_classifier.InputFeatures at 0x7f798eefd7b8&gt;,
 &lt;run_classifier.InputFeatures at 0x7f798eece5c0&gt;,
 &lt;run_classifier.InputFeatures at 0x7f798eecec18&gt;,
 &lt;run_classifier.InputFeatures at 0x7f798eece978&gt;,
 &lt;run_classifier.InputFeatures at 0x7f798eeced68&gt;,
 &lt;run_classifier.InputFeatures at 0x7f798eece208&gt;,
 &lt;run_classifier.InputFeatures at 0x7f798eecea58&gt;,
</code></pre>

<p>My confusion is how to convert this to an array of embeddings that I can use for other tasks. I could be missing how Bert works. </p>
",Text Generation & LLMs,generate embeddings using bert started using following kaggle kernel used following code getting close get following output variable confusion convert array embeddings use task could missing bert work
Does BERT implicitly model for word count?,"<p>Given that BERT is bidirectional, does it implicitly model for word count in some given text? I am asking in the case of classifying data column descriptions as valid or not. I am looking for a model based on word count, and was wondering if that even needs to be done given BERT is bidirectional.</p>
",Text Generation & LLMs,doe bert implicitly model word count given bert bidirectional doe implicitly model word count given text asking case classifying data column description valid looking model based word count wa wondering even need done given bert bidirectional
Google&#39;s BERT for NLP: replace foreign characters in vocab.txt to add words?,"<p>I am fine-tuning the BERT model but need to add a few thousand words. I know that one can replace the ~1000 <code>[unused#]</code> lines at the top of the vocab.txt, but I also notice there are thousands of single foreign characters (unicode) in the file, which I will never use. For fine-tuning, is it possible to replace those with my words, fine tune, and have model still work correctly?</p>
",Text Generation & LLMs,google bert nlp replace foreign character vocab txt add word fine tuning bert model need add thousand word know one replace line top vocab txt also notice thousand single foreign character unicode file never use fine tuning possible replace word fine tune model still work correctly
BucketIterator not returning batches of correct size,"<p>I'm implementing a simple LSTM language model in <code>PyTorch</code>, and wanted to check out the <code>BucketIterator</code> that is provided by <code>torchtext</code>. </p>

<p>It turns out that the batch that is returned has the size of my entire corpus, so I must be doing something wrong during its initialisation.</p>

<p>I've already got the <code>BPTTIterator</code> working, but as I want to be able to train on batches of complete sentences as well, I thought the <code>BucketIterator</code> should be the way to go.</p>

<p>I use the following setup, with my corpus a simple txt file containing sentences at each line.</p>

<pre><code>field = Field(use_vocab=True, batch_first=True)
corpus = PennTreebank('project_2_data/train_lines.txt', field)
field.build_vocab(corpus)

iterator = BucketIterator(corpus,
                          batch_size=64,
                          repeat=False,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          )
</code></pre>

<p>I expect a batch from this iterator to have the shape <code>(batch_size, max_len)</code>, but it appends the entire corpus into 1 tensor of shape <code>(1, corpus_size)</code>.</p>

<p>What am I missing in my setup?</p>

<p>Edit: it seems the <code>PennTreebank</code> object is not compatible with a <code>BucketIterator</code> (it contains only 1 <code>Example</code> as noted here <a href=""http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/"" rel=""nofollow noreferrer"">http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/</a>). Using a <code>TabularDataset</code> with only 1 <code>Field</code> got it working.</p>

<p>If someone has an idea how language modelling with padded sentence batches can be done in <code>torchtext</code> in a more elegant manner I'd love to hear it!</p>
",Text Generation & LLMs,bucketiterator returning batch correct size implementing simple lstm language model wanted check provided turn batch returned ha size entire corpus must something wrong initialisation already got working want able train batch complete sentence well thought way go use following setup corpus simple txt file containing sentence line expect batch iterator shape appends entire corpus tensor shape missing setup edit seems object compatible contains noted using got working someone ha idea language modelling padded sentence batch done elegant manner love hear
BERT Masked Language Model,"<p>I've started dissecting the <strong>BERT</strong> paper by google (<a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1810.04805</a>) and stumbled upon few points that I don't understand. </p>

<p>My questions is about Masked Language Model. </p>

<p>First, we mask 15% of the tokens and the learning task is to predict those tokens (it's seems like a long way to converge). </p>

<p>But we don't simple mask it with [MASK] but 10% of the time: we replace it with a random token. <strong>But why?</strong> </p>

<p>Moreover, in another 10% we keep the token unchanged </p>

<blockquote>
  <p>The purpose of this is to bias therepresentation  towards  the  actual
  observed word.</p>
</blockquote>

<p>This citation confused me completely, in 80% cases we actually mask it, how 10% will help us to preserve a bias.</p>

<p>I would appreciate any explanation.</p>
",Text Generation & LLMs,bert masked language model started dissecting bert paper google stumbled upon point understand question masked language model first mask token learning task predict token seems like long way converge simple mask mask time replace random token moreover another keep token unchanged purpose bias therepresentation towards actual observed word citation confused completely case actually mask help u preserve bias would appreciate explanation
Code to create a reliable Language model from my own corpus,"<p>I have a corpus of sentences in a specific domain.
I am looking for an open-source code/package, that I can give the data and it will generate a good, reliable language model. (Meaning, given a context, know the probability for each word).</p>

<p>Is there such a code/project?</p>

<p>I saw this github repo: <a href=""https://github.com/rafaljozefowicz/lm"" rel=""nofollow noreferrer"">https://github.com/rafaljozefowicz/lm</a>, but it didn't work.</p>
",Text Generation & LLMs,code create reliable language model corpus corpus sentence specific domain looking open source code package give data generate good reliable language model meaning given context know probability word code project saw github repo work
Why does loss continue decreasing but performance keep unchanged?,"<p>I am using bert-lstm-crf model, with bert model from <a href=""https://github.com/huggingface/pytorch-pretrained-BERT/"" rel=""nofollow noreferrer"">https://github.com/huggingface/pytorch-pretrained-BERT/</a> and lstm crf models are written by myself.</p>

<p>After training bert-lstm-crf model for 25 epochs, the performance on training set, dev set and test set keep unchanged but the loss continue decreasing. Where should I make a change?</p>

<p>Here is performance:</p>

<p>25th epoch:</p>

<pre><code>tensor(10267.6279, device='cuda:0')
(0.42706720346856614, 0.4595134955014995, 0.4426966292134832)
(0.43147208121827413, 0.4271356783919598, 0.42929292929292934)
(0.4460093896713615, 0.4668304668304668, 0.4561824729891957)
</code></pre>

<p>26th epoch:</p>

<pre><code>tensor(10219.3398, device='cuda:0')
(0.44544364508393286, 0.4951682772409197, 0.46899163642101943)
(0.4469135802469136, 0.4547738693467337, 0.45080946450809467)
(0.45871559633027525, 0.4914004914004914, 0.4744958481613286)
</code></pre>

<p>27th epoch:</p>

<pre><code>tensor(10169.0742, device='cuda:0')
(0.44544364508393286, 0.4951682772409197, 0.46899163642101943)
(0.4469135802469136, 0.4547738693467337, 0.45080946450809467)
(0.45871559633027525, 0.4914004914004914, 0.4744958481613286)
</code></pre>

<p>more epochs:
lower loss with same performance:</p>

<pre><code>(0.44544364508393286, 0.4951682772409197, 0.46899163642101943)
(0.4469135802469136, 0.4547738693467337, 0.45080946450809467)
(0.45871559633027525, 0.4914004914004914, 0.4744958481613286)
</code></pre>

<p>It is really a weird problem, I have no idea how to handle this. Any suggestion would be of great help.</p>

<p>Here is related code:</p>

<pre><code>  for epoch in tqdm(range(200)):

  {loss = train_one_epoch(dataloader=source_train_dataloader,
  model=model, optimizer=optimizer)

  train_perf = test_one_epoch(dataloader=source_train_dataloader_for_test,
  model=model)

  dev_perf = test_one_epoch(dataloader=source_dev_dataloader, model=model)

  test_perf = test_one_epoch(dataloader=source_test_dataloader, 
  model=model)

  base_result_loc = ""bert_char_ps/bert_char_result""

  # store performance result

  add_model_result(
    base_result_loc,
    epoch,
    loss,
    train_perf,
    dev_perf,
    test_perf)
  }
</code></pre>

<p>The performance should change with loss, but now it does not</p>
",Text Generation & LLMs,doe loss continue decreasing performance keep unchanged using bert lstm crf model bert model lstm crf model written training bert lstm crf model epoch performance training set dev set test set keep unchanged loss continue decreasing make change performance th epoch th epoch th epoch epoch lower loss performance really weird problem idea handle suggestion would great help related code performance change loss doe
argmax from probability distribution better policy than random sampling from softmax?,"<p>I am trying to train Echo State Network for text generation with stochastic optimization along the lines of Reinforcement learning, where the optimization depends on the reward signal.</p>

<p>I have observed that during evaluation, when I sample from the probability distribution, the bleu score is bigger than when I argmax from the distribution. The difference is almost more than 0.10 points (BLEU Score is generally between the range 0 and 1 ).
I am not sure why does that happen.
Help needed.</p>
",Text Generation & LLMs,argmax probability distribution better policy random sampling softmax trying train echo state network text generation stochastic optimization along line reinforcement learning optimization depends reward signal observed evaluation sample probability distribution bleu score bigger argmax distribution difference almost point bleu score generally range sure doe happen help needed
Package to generate n-gram language models with smoothing? (Alternatives to NLTK),"<p>I'd like to find some type of package or module (preferably Python or Perl, but others would do) that automatically generate n-gram probabilities from an input text, and can automatically apply one or more smoothing algorithms as well.</p>

<p>That is, I am looking for something like the NLTK <code>NgramModel</code> class. I can't use this for my purposes because there are some bugs with the smoothing functions which make it choke when you ask for the probability of a word it hasn't seen before. </p>

<p>I've read through the dev forums for NLTK and as of now there seems to be no progress on this.  </p>

<p>Any alternatives out there?</p>
",Text Generation & LLMs,package generate n gram language model smoothing alternative nltk like find type package module preferably python perl others would automatically generate n gram probability input text automatically apply one smoothing algorithm well looking something like nltk class use purpose bug smoothing function make choke ask probability word seen read dev forum nltk seems progress alternative
Different &lt;s&gt;&lt;/s&gt;&lt;unk&gt; probabilities between kenlm and berkeleylm,"<p>I build ngram language model using kenlm and berkeleylm, but they give very different probability to token  .</p>

<p>The kenlm gives:</p>

<pre><code>ngram 1=164482
ngram 2=4355352
ngram 3=15629476

\1-grams:
-6.701107   &lt;unk&gt;   0
0   &lt;s&gt; -1.9270477
-1.8337007  &lt;/s&gt;    0
</code></pre>

<p>while berkeleylm gives:</p>

<pre><code>\data\
ngram 1=164481
ngram 2=4291478
ngram 3=15629476

\1-grams:
-99.000000  &lt;s&gt; -2.079426
-1.833699   &lt;/s&gt;
and no &lt;unk&gt; token probability
</code></pre>

<p>I want to know why they handle these differently and how would these differences lead to different results?</p>
",Text Generation & LLMs,different unk probability kenlm berkeleylm build ngram language model using kenlm berkeleylm give different probability token kenlm give berkeleylm give want know handle differently would difference lead different result
Use BERT for feature extraction of a unique word,"<p>I am using BERT for feature extraction of a word given the text where it appears, but it seems current implementation in bert's official github (<a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a>) can only compute the features of all the words in text, which makes it consume too much resources. Is it possible to adapt it for this purporse? Thanks!!</p>
",Text Generation & LLMs,use bert feature extraction unique word using bert feature extraction word given text appears seems current implementation bert official github compute feature word text make consume much resource possible adapt purporse thanks
How to handle &lt;UKN&gt; tokens in text generation,"<p>In my text generation dataset, I have converted all infrequent words into the token  (unknown word), as suggested by most text-generation literature.</p>

<p>However, when training an RNN to take in part of a sentence as input and predict the rest of the sentence, I am not sure how I should stop the network from generating  tokens.
When the network encounters an unknown (infrequent) word in the training set, what should its output be?</p>

<p><strong>Example:</strong><br>
Sentence: <code>I went to the mall and bought a &lt;ukn&gt; and some groceries</code><br>
Network input: <code>I went to the mall and bought a</code><br>
Current network output: <code>&lt;unk&gt; and some groceries</code><br>
Desired network output: <code>??? and some groceries</code><br></p>

<p>What should it be outputting instead of the <code>&lt;unk&gt;</code>? </p>

<p>I don't want to build a generator that outputs words it does not know.</p>
",Text Generation & LLMs,handle ukn token text generation text generation dataset converted infrequent word token unknown word suggested text generation literature however training rnn take part sentence input predict rest sentence sure stop network generating token network encounter unknown infrequent word training set output example sentence network input current network output desired network output outputting instead want build generator output word doe know
Kenlm language model scoring with Java on Windows,"<p>I'm mid-way through a Java project using a '.arpa' file to extract n-gram probabilities.
Ideally I would like to use a '.klm' file (created using the '.arpa' file), similar to:</p>

<pre><code>model = kenlm.LanguageModel('languageModel.klm')
model.score('The dog chased the ball.')
</code></pre>

<p>however kenlm for Java isn't supported on windows.
Does anyone know of any alternatives to score sentences?</p>

<p>Thanks.</p>
",Text Generation & LLMs,kenlm language model scoring java window mid way java project using arpa file extract n gram probability ideally would like use klm file created using arpa file similar however kenlm java supported window doe anyone know alternative score sentence thanks
"Masked language model processing, deeper explanation","<p>I'm looking to BERT model (<a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">you can found the description here</a>) in detail and I'm getting problem to understand clearly the need to keep or replace random word 20% of the time instead or just use [MASK] token always for the masked language model.</p>

<p>We try to train the bidirectional technique and the article explains ""[MASK] token is never seen during fine-tuning"" but it is two different steps for me, we train first bidirectional and after we downstream task.</p>

<p>If someone can explain to me where I'm wrong in my comprehension.</p>
",Text Generation & LLMs,masked language model processing deeper explanation looking bert model found description detail getting problem understand clearly need keep replace random word time instead use mask token always masked language model try train bidirectional technique article explains mask token never seen fine tuning two different step train first bidirectional downstream task someone explain wrong comprehension
Specify the provenance of FHIR Resources generated by applying NLP over medical narratives,"<p><a href=""https://www.hl7.org/fhir/"" rel=""nofollow noreferrer"">FHIR</a>  </p>

<blockquote>
  <p>is a standard for health care data exchange, published by HL7®. </p>
</blockquote>

<p>The <a href=""https://www.hl7.org/fhir/Documentreference.html"" rel=""nofollow noreferrer"">DocumentReference</a> </p>

<blockquote>
  <p>Provides metadata about the document so that the document can be
  discovered and managed</p>
</blockquote>

<p>Through the <a href=""https://www.hl7.org/fhir/provenance.html"" rel=""nofollow noreferrer"">Provenance</a> one can </p>

<blockquote>
  <p>describe entities and processes involved in producing and delivering
  or otherwise influencing that resource</p>
</blockquote>

<p><a href=""https://artificial-intelligence.healthcaretechoutlook.com/cxoinsights/unstructured-data-in-healthcare-nid-506.html"" rel=""nofollow noreferrer""> Nearly 80 percent of clinical information in electronic health
 records(EHRs) is ""unstructured"" and in a format that health
 information technology systems cannot use.</a></p>

<p>It is therefore natural to apply computer techniques to automatically generate structured data from the medical records. For that there are several implementations available both on the market and also fully open source. For example cTAKES, CLAMP, NOBLE, ClarityNLP and others are all freely available solutions targeting this task.</p>

<p>They all address the specific need of generating structured data from unstructured medical notes, however they all deliver the structure using their own format, that eventually could be converted into FHIR.</p>

<p>However, a central problem is on how to represent the Provenance of the extracted information, since FHIR is - to the best of my knowledge - missing the way of connecting to the precise location within the DocumentReference object of where the information has been extracted from , with which technology, and which is the level of ""quality"" of the extracted information.</p>

<p>Before submitting a Change Request <a href=""https://gforge.hl7.org/gf/project/fhir/tracker/?action=TrackerItemBrowse"" rel=""nofollow noreferrer"">https://gforge.hl7.org/gf/project/fhir/tracker/?action=TrackerItemBrowse</a> to the FHIR normative, it is recommended to expose the issue to the widest community and the stackoverflow.com is one of the main recommended channels. </p>

<p>For this purpose I am hereby looking forward opinions on the matter, and namely on how to specify the provenance of  FHIR Resources generated by applying NLP over medical narratives. For example, taking an example from the Adverse Event Corpus of Gurulingappa et al <a href=""https://doi.org/10.1016/j.jbi.2012.04.008"" rel=""nofollow noreferrer"">https://doi.org/10.1016/j.jbi.2012.04.008</a> , </p>

<pre><code>10030778|Intravenous azithromycin-induced ototoxicity.|ototoxicity|43|54|azithromycin|22|34
123456789012345678901234567890123456789012345678901234567890
         1         2         3         4         5
</code></pre>

<p>The question is how to represent into FHIR that such drug induced problem has been extracted from the specific bytes positions 22-34 (drug) and 43-54 (problem) from the text (the Title of the paper 1999 in this example).</p>

<pre><code>{
  ""resourceType"": ""AdverseEvent"",
  ""id"": ""example"",
  ""actuality"": ""actual"",
  ""category"": [
    {
      ""coding"": [
        {
          ""system"": ""http://terminology.hl7.org/CodeSystem/adverse-event-category"",
          ""code"": ""product-use-error"",
          ""display"": ""Product Use Error""
        }
      ]
    }
  ],
  ""event"": {
    ""coding"": [
      {
        ""system"": ""http://snomed.info/sct"",
        ""code"": ""9062008"",
        ""display"": ""Ototoxicity (disorder)""
      }
    ],
    ""text"": ""10030778|Intravenous azithromycin-induced ototoxicity.""
  },
  ""subject"": {
    ""reference"": ""Patient/example""
  },
  ""date"": ""1999-02-29T00:00:00+00:00"",
  ""seriousness"": {
    ""coding"": [
      {
        ""system"": ""http://terminology.hl7.org/CodeSystem/adverse-event-seriousness"",
        ""code"": ""Non-serious"",
        ""display"": ""Non-serious""
      }
    ]
  },
  ""severity"": {
    ""coding"": [
      {
        ""system"": ""http://terminology.hl7.org/CodeSystem/adverse-event-severity"",
        ""code"": ""mild"",
        ""display"": ""Mild""
      }
    ]
  },
  ""recorder"": {
    ""reference"": ""Pharmacotherapy. 1999 Feb;19(2):245-8.""
  },
  ""suspectEntity"": [
    {
      ""instance"": {
        ""reference"": ""Azithromycin""
      }
    }
  ]
}
</code></pre>

<p>Currently the FHIR standard does not allow to represent the precise byte position, the quality of the extraction, and the method used to perform it.</p>
",Text Generation & LLMs,specify provenance fhir resource generated applying nlp medical narrative fhir standard health care data exchange published hl documentreference provides metadata document document discovered managed provenance one describe entity process involved producing delivering otherwise influencing resource nearly percent clinical information electronic health record ehrs unstructured format health information technology system use therefore natural apply computer technique automatically generate structured data medical record several implementation available market also fully open source example ctakes clamp noble claritynlp others freely available solution targeting task address specific need generating structured data unstructured medical note however deliver structure using format eventually could converted fhir however central problem represent provenance extracted information since fhir best knowledge missing way connecting precise location within documentreference object information ha extracted technology level quality extracted information submitting change request fhir normative recommended expose issue widest community stackoverflow com one main recommended channel purpose hereby looking forward opinion matter namely specify provenance fhir resource generated applying nlp medical narrative example taking example adverse event corpus gurulingappa et al question represent fhir drug induced problem ha extracted specific byte position drug problem text title paper example currently fhir standard doe allow represent precise byte position quality extraction method used perform
Finetune Text embeddings using BERT?,"<p>are the text emebddings also fine-tuned when fine-tuning for classification task? Or up to which layer are the encodings fine-tuned (sencond last layer)?</p>
",Text Generation & LLMs,finetune text embeddings using bert text emebddings also fine tuned fine tuning classification task layer encoding fine tuned sencond last layer
Writing annotation schemas for Callisto,"<p>Does anybody know where I can find documentation on how to write annotation schemas for <a href=""http://callisto.mitre.org/"" rel=""nofollow noreferrer"">Callisto</a>? I'm looking to write something a little more complicated than I can generate from a DTD -- that only gives me the ability to tag different kinds of text mentions. I'm looking to create a schema that represents a single type of  relationship between five or six different kinds of textual mentions (and some of these types of mentions have attributes that I need to assign values to), and possibly having a second type of relationship between the first two instances of the first type of relationship.</p>

<p>(Alternatively, does anybody know of any software that would be better for this kind of schema? I've been looking at <a href=""http://wordfreak.sf.net/"" rel=""nofollow noreferrer"">WordFreak</a>, but it's a little clumsy, and it doesn't support attributes on its textual mentions.)</p>
",Text Generation & LLMs,writing annotation schema callisto doe anybody know find documentation write annotation schema callisto looking write something little complicated generate dtd give ability tag different kind text mention looking create schema represents single type relationship five six different kind textual mention type mention attribute need assign value possibly second type relationship first two instance first type relationship alternatively doe anybody know software would better kind schema looking wordfreak little clumsy support attribute textual mention
"How to find a context of paragraph, with a help of a BERT?","<p>I'm looking for a way to run a simple example with BERT.
How to analyse context with google BERT? <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a>
For example, i have a paragraph of a wikipedia text, and pre-trained BERT model from google. How to pass paragraph to BERT and get a text representation of main context?
Thanks!</p>

<p>EDTED: </p>

<p>Example: we have paragraph</p>

<p>""I'm looking for a way to run a simple example with BERT. How to analyse context with google BERT? <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a> For example, i have a paragraph of a wikipedia text, and pre-trained BERT model from google. How to pass paragraph to BERT and get a text representation of main context? Thanks!""</p>

<p>The context should be ""How to analyse context with google BERT?"" or ""How to analyze""</p>

<p>The problem for me is to figure out how to teach model to pick up main sentence from other ones. ^example above.</p>

<p>The other problem: how to work with documents larger than 512 tokens?</p>

<p>Thanks!</p>
",Text Generation & LLMs,find context paragraph help bert looking way run simple example bert analyse context google bert example paragraph wikipedia text pre trained bert model google pas paragraph bert get text representation main context thanks edted example paragraph looking way run simple example bert analyse context google bert example paragraph wikipedia text pre trained bert model google pas paragraph bert get text representation main context thanks context analyse context google bert analyze problem figure teach model pick main sentence one example problem work document larger token thanks
Ensure the presence of a word/token/noun in Encoder-Decoder text generation deep learning models,"<p>I am stuck with a problem where in I want to ensure that specific tokens/words are produced while decoding and generating abstractive-style sentences. </p>

<p>I am working with deep learning models like LSTM and transformer model for generating short sentences(100-200 characters). I want that some words like places or nouns(like brand names) be present in the generated texts.</p>

<p>I am not sure if there has been any research on this, I couldn't really find a paper after an extensive search on it.</p>

<p>TIA, any leads or suggestions are appreciated. :)</p>
",Text Generation & LLMs,ensure presence word token noun encoder decoder text generation deep learning model stuck problem want ensure specific token word produced decoding generating abstractive style sentence working deep learning model like lstm transformer model generating short sentence character want word like place noun like brand name present generated text sure ha research really find paper extensive search tia lead suggestion appreciated
What type of neural network should I be using to generate a paragraph based on some input?,"<p>My knowledge of neural networks is very basic but here is my goal:
Given a set of short inputs (One word strings and numbers) I want the trained network to generate a paragraph of text related to the input data.</p>

<p>I've messed with RNNs before to do basic natural language generation but never based on a given input.
(I played around with <a href=""https://github.com/karpathy/char-rnn"" rel=""nofollow noreferrer"">https://github.com/karpathy/char-rnn</a> for example)</p>

<p>There is so much information out there I'm not sure what sort of model I should be using or where to start.</p>
",Text Generation & LLMs,type neural network using generate paragraph based input knowledge neural network basic goal given set short input one word string number want trained network generate paragraph text related input data messed rnns basic natural language generation never based given input played around example much information sure sort model using start
How could i use another weighting model in whoosh?,"<p>If i want to use the Language Model as the weighting model,how could i do.The default function is BM-25 model,And i have not find the Language model in scoring class,It really confused me.</p>
",Text Generation & LLMs,could use another weighting model whoosh want use language model weighting model could default function bm model find language model scoring class really confused
Load PreComputed Vectors Gensim,"<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=""http://www-nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>

<p>Thanks! </p>
",Text Generation & LLMs,load precomputed vector gensim using gensim python package learn neural language model know provide training corpus learn model however already exist many precomputed word vector available text format e g way initialize gensim word vec model make use precomputed vector rather learn vector scratch thanks
Check perplexity of a Language Model,"<p>I created a language model with Keras LSTM and now I want to assess wether it's good so I want to calculate perplexity.</p>

<p>What is the best way to calc perplexity of a model in Python?</p>
",Text Generation & LLMs,check perplexity language model created language model kera lstm want ass wether good want calculate perplexity best way calc perplexity model python
What is differece between token-level and segment-level in NLP task?,"<p>Actually, I am not so understand about token..
When I read googleresearch/bert model, I see these words.</p>

<pre><code># In the demo, we are doing a simple classification task on the entire   
# segment.  
#   
# If you want to use the token-level output, use model.get_sequence_output()   # instead.
</code></pre>

<p>Can anyone make an example about token-level and segment-level classification?</p>
",Text Generation & LLMs,differece token level segment level nlp task actually understand token read googleresearch bert model see word anyone make example token level segment level classification
Second word completion with python,"<p>I'm new to machine learning and I'm trying to come up with a model that will complete all second words in phrases. I couldn't find solution to this exact problem although there are lots of tutorials on generating text with RNN. </p>

<p>So, consider you have the 2 following files:</p>

<p>1) a word dictionary for training</p>

<p>Say we have a table with 2 columns of word pairs: 'complete' and 'sample' such that the first column includes different word pairs (""Hello dear"", ""my name"", ""What time"", ""He goes"", etc.) and the second one includes first words and only a part (> 2 letters) of second words (""Hello de"", ""my nam"", ""What ti"", ""He goe"", etc.). </p>

<p>2) a table for testing</p>

<p>It's a table that consists of only 'sample' column.</p>

<p>The aim is to add 'complete' column to the second table with complete pairs of words.</p>

<p>I came up with the only way to do this:</p>

<ol>
<li><p>compute the frequences of all first words (P(w1))</p></li>
<li><p>compute the frequences of all complete second words (P(w2))</p></li>
<li><p>compute the frequences of all first words given complete second words (P(w1|w2))</p></li>
<li><p>predict complete second words using Bayes rule:
w2 = argmax_{w2} ( P(w2|w1)) = argmax_{w2} (P(w1|w2) * P(w2))</p></li>
<li><p>for each w1 in the test table w2 is the most probable w2 or the most frequent w2 (if w1 is not in the dict).</p></li>
</ol>

<p>The problem is this algorithm doesn't work sufficiently well. How can I somehow optimise the probabilities (maybe gradient descent might be helpful?)? Is there any other way to address this task? </p>
",Text Generation & LLMs,second word completion python new machine learning trying come model complete second word phrase find solution exact problem although lot tutorial generating text rnn consider following file word dictionary training say table column word pair complete sample first column includes different word pair hello dear name time go etc second one includes first word part letter second word hello de nam ti goe etc table testing table consists sample column aim add complete column second table complete pair word came way compute frequence first word p w compute frequence complete second word p w compute frequence first word given complete second word p w w predict complete second word using bayes rule w argmax w p w w argmax w p w w p w w test table w probable w frequent w w dict problem algorithm work sufficiently well somehow optimise probability maybe gradient descent might helpful way address task
Generate misspelled words (typos),"<p>I have implemented a fuzzy matching algorithm and I would like to evaluate its recall using some sample queries with test data. </p>

<p>Let's say I have a document containing the text:</p>

<pre><code>{""text"": ""The quick brown fox jumps over the lazy dog""}
</code></pre>

<p>I want to see if I can retrieve it by testing queries such as ""sox"" or ""hazy drog"" instead of ""fox"" and ""lazy dog"". </p>

<p>In other words, I want to add noise to strings to generate  misspelled words (typos). </p>

<p><strong>What would be a way of automatically generating words with typos</strong> for evaluating fuzzy search?</p>
",Text Generation & LLMs,generate misspelled word typo implemented fuzzy matching algorithm would like evaluate recall using sample query test data let say document containing text want see retrieve testing query sox hazy drog instead fox lazy dog word want add noise string generate misspelled word typo would way automatically generating word typo evaluating fuzzy search
R: Quickly generate partial sequences,"<p>I'm looking to generate sequences of text based on training an RNN on text snippets (which I've done before in <a href=""https://towardsdatascience.com/using-deep-learning-to-generate-offensive-license-plates-619b163ed937"" rel=""nofollow noreferrer"">articles like this</a>).</p>

<p>One step is to take the snippets of text and break them up into subsequences to train the model on:</p>

<pre><code>c(""E"",""X"",""A"",""M"",""P"",""L"",""E"")
</code></pre>

<p>would become</p>

<pre><code>c(""E"")
c(""E"",""X"")
c(""E"",""X"",""A"")
...
</code></pre>

<p>My current method is to use a map on each word:</p>

<pre><code>require(tidyverse)

data &lt;- data_frame(id = c(1,2),word = list(c(""E"",""X"",""A"",""M"",""P"",""L"",""E""), c(""R"",""S"",""T"",""U"",""D"",""I"",""O"")))

result &lt;- data %&gt;%
  pmap(function(id,word){
    subs &lt;- map(1:length(word),function(i) word[1:i])
    data_frame(id = id, sub = subs)
  }) %&gt;%
  bind_rows()
</code></pre>

<p>But this is <strong>extremely slow</strong> on large datasets. Is there a fast way to generate all of these partial sequences?</p>
",Text Generation & LLMs,r quickly generate partial sequence looking generate sequence text based training rnn text snippet done article like one step take snippet text break subsequence train model would become current method use map word extremely slow large datasets fast way generate partial sequence
Algorithm For Determining Sentence Subject Similarity,"<p>I'm looking to generate an algorithm that can determine the similarity of a series of sentences. Specifically, given a starter sentence, I want to determine if the following sentence is a suitable addition.</p>

<p>For example, take the following:</p>

<blockquote>
  <p>My dog loves to drink water.</p>
</blockquote>

<p>All is good, this is just the first sentence.</p>

<blockquote>
  <p>The dog hates cats.</p>
</blockquote>

<p>All is good, both sentences reference dogs.</p>

<blockquote>
  <p>It enjoys walks on the beach.</p>
</blockquote>

<p>All is good, ""it"" is neutral enough to be an appropriate communication.</p>

<blockquote>
  <p>Pizza is great with pineapple on top.</p>
</blockquote>

<p>This would not be a suitable addition, as the sentence does not build on to the ""narrative"" created by the first three sentences.</p>

<hr>

<p>To outline the project a bit, I've created a library that generated Markov text chains based on the input text. That text is then corrected grammatically to produce viable sentences. I now want to string these sentences together to create coherent paragraphs.</p>
",Text Generation & LLMs,algorithm determining sentence subject similarity looking generate algorithm determine similarity series sentence specifically given starter sentence want determine following sentence suitable addition example take following dog love drink water good first sentence dog hate cat good sentence reference dog enjoys walk beach good neutral enough appropriate communication pizza great pineapple top would suitable addition sentence doe build narrative created first three sentence outline project bit created library generated markov text chain based input text text corrected grammatically produce viable sentence want string sentence together create coherent paragraph
How to implement supervised class based language model in SRILM?,"<p>I found tutorials where class based LM is implemented using Brown clustering passing just number of classes you want but I want to implement a class based model where I give class assignments initially.
I tried this <a href=""http://projects.csail.mit.edu/cgi-bin/wiki/view/SLS/SriLM"" rel=""nofollow noreferrer"">http://projects.csail.mit.edu/cgi-bin/wiki/view/SLS/SriLM</a>. But this gives -99 to all ngrams in LM.
There is very less documentation regarding this, Can anyone help me out?</p>
",Text Generation & LLMs,implement supervised class based language model srilm found tutorial class based lm implemented using brown clustering passing number class want want implement class based model give class assignment initially tried give ngrams lm le documentation regarding anyone help
BLEU score for generation task,"<p>after I generate the new text using LSTM or RNN how can I measure the quality of the new text.can i use BLEU score?, but BLEU is used to evaluate the line-to-line generation task which focuses on the semantic relevance between two lines. here I do not have the target to compare between two sentences because I totally got a new text.   </p>
",Text Generation & LLMs,bleu score generation task generate new text using lstm rnn measure quality new text use bleu score bleu used evaluate line line generation task focus semantic relevance two line target compare two sentence totally got new text
Text completion using Machine Learning,"<p>I have a list of short hand text. All in English Language. Is there a Machine Learning algorithm that can be used to expand these abbreviations? For example, if the short hand is 'txt', it could suggest 'text', 'context', 'textual', etc with varying penalty values.</p>

<p>In addition, when I make a choice on the right word, I want it to learn this such that when next I input same shorthand, my choice get's high ratings.</p>

<p><strong><em>Edit</em></strong></p>

<p>Specifically, I have tried using this Language model described <a href=""http://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">here</a> but it only works for edits up to two levels. The 'edit' function is below:</p>

<pre><code>def edits1(word):
    ""All edits that are one edit away from `word`.""
    letters    = 'abcdefghijklmnopqrstuvwxyz'
    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]
    deletes    = [L + R[1:]               for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)&gt;1]
    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]
    inserts    = [L + c + R               for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)
</code></pre>

<p>It basically starts with one letter and then deletes, transposes, replaces and inserts letters (using letters of the alphabet).</p>

<p>How do I extend this to more than two edits?</p>
",Text Generation & LLMs,text completion using machine learning list short hand text english language machine learning algorithm used expand abbreviation example short hand txt could suggest text context textual etc varying penalty value addition make choice right word want learn next input shorthand choice get high rating edit specifically tried using language model described work edits two level edit function basically start one letter deletes transpose replaces insert letter using letter alphabet extend two edits
Text generation: character prediction RNN vs. word prediction RNN,"<p>I've been researching text generation with RNNs, and it seems as though the common technique is to input text character by character, and have the RNN predict the next character.</p>

<p>Why wouldn't you do the same technique but using words instead of characters.
This seems like a much better technique to me because the RNN won't make any typos and it will be faster to train.</p>

<p>Am I missing something?</p>

<p>Furthermore, is it possible to create a word prediction RNN but with somehow inputting words pre-trained on word2vec, so that the RNN can understand their meaning?</p>
",Text Generation & LLMs,text generation character prediction rnn v word prediction rnn researching text generation rnns seems though common technique input text character character rnn predict next character technique using word instead character seems like much better technique rnn make typo faster train missing something furthermore possible create word prediction rnn somehow inputting word pre trained word vec rnn understand meaning
When loading KenLM language model for scoring sentences should the LM file size be less than RAM size?,"<p>When loading language model for scoring sentence should the LM('bible.klm') filesize be less than RAM size?</p>

<pre><code>import kenlm

model = kenlm.LanguageModel('bible.klm')
model.score('in the beginning was the word')
</code></pre>
",Text Generation & LLMs,loading kenlm language model scoring sentence lm file size le ram size loading language model scoring sentence lm bible klm filesize le ram size
spaCy process document with multiple languages,"<p>Given a document string <code>s</code> of a certain length and a language mask <code>l</code> of the same length I would like to process each part (<code>span</code>?) of the document with the according spacy language model.</p>

<p>say for example</p>

<pre><code>s = 'As one would say in German: Wie man auf englisch zu sagen pflegt'
l = ['en'] * 27 + ['de'] * 37
</code></pre>

<p>I would like to construct a document out of</p>

<pre><code>import spacy
nlp_de = spacy.load('de')
nlp_en = spacy.load('en')

d_de = nlp_de(u"""".join([c for i,c in enumerate(s) if l[i] == ""de""]))
d_en = nlp_en(u"""".join([c for i,c in enumerate(s) if l[i] == ""en""]))
</code></pre>

<p>And now I would somehow have to glue that two parts together. But unfortunately, the document in spacy holds information about the vocabulary. This would thus be ambiguous. </p>

<p>How should I model my multi-language documents with spacy?</p>
",Text Generation & LLMs,spacy process document multiple language given document string certain length language mask length would like process part document according spacy language model say example would like construct document would somehow glue two part together unfortunately document spacy hold information vocabulary would thus ambiguous model multi language document spacy
Predicting next word with text2vec in R,"<p>I am building a language model in R to predict a next word in the sentence based on the previous words. Currently my model is a simple ngram model with Kneser-Ney smoothing. It predicts next word by finding ngram with maximum probability (frequency) in the training set, where smoothing offers a way to interpolate lower order ngrams, which can be advantageous in the cases where higher order ngrams have low frequency and may not offer a reliable prediction. While this method works reasonably well, it 'fails in the cases where the n-gram cannot not capture the context. For example, ""It is warm and sunny outside, let's go to the..."" and ""It is cold and raining outside, let's go to the..."" will suggest the same prediction, because the context of weather is not captured in the last n-gram (assuming n&lt;5). </p>

<p>I am looking into more advanced methods and I found <a href=""https://cran.r-project.org/web/packages/text2vec/index.html"" rel=""nofollow"">text2vec</a> package, which allows to map words into vector space where words with similar meaning are represented with similar (close) vectors. I have a feeling that this representation can be helpful for the next word prediction, but i cannot figure out how exactly to define the training task. My quesiton is if text2vec is the right tool to use for next word prediction and if yes, what is the suitable prediction algorithm that can be used for this task?</p>
",Text Generation & LLMs,predicting next word text vec r building language model r predict next word sentence based previous word currently model simple ngram model kneser ney smoothing predicts next word finding ngram maximum probability frequency training set smoothing offer way interpolate lower order ngrams advantageous case higher order ngrams low frequency may offer reliable prediction method work reasonably well fails case n gram capture context example warm sunny outside let go cold raining outside let go suggest prediction context weather captured last n gram assuming n looking advanced method found text vec package allows map word vector space word similar meaning represented similar close vector feeling representation helpful next word prediction figure exactly define training task quesiton text vec right tool use next word prediction yes suitable prediction algorithm used task
Gensim Doc2Vec model only generates a limited number of vectors,"<p>I am using gensim <strong>Doc2Vec</strong> model to generate my feature vectors. Here is the code I am using (I have explained what my problem is in the code):</p>

<pre><code>cores = multiprocessing.cpu_count()

# creating a list of tagged documents
training_docs = []

# all_docs: a list of 53 strings which are my documents and are very long (not just a couple of sentences)
for index, doc in enumerate(all_docs):
    # 'doc' is in unicode format and I have already preprocessed it
    training_docs.append(TaggedDocument(doc.split(), str(index+1)))

# at this point, I have 53 strings in my 'training_docs' list 

model = Doc2Vec(training_docs, size=400, window=8, min_count=1, workers=cores)

# now that I print the vectors, I only have 10 vectors while I should have 53 vectors for the 53 documents that I have in my training_docs list.
print(len(model.docvecs))
# output: 10
</code></pre>

<p>I am just wondering if I am doing a mistake or if there is any other parameter that I should set?</p>

<blockquote>
  <p><strong>UPDATE</strong>: I was playing with the <strong><em>tags</em></strong> parameter in <strong><em>TaggedDocument</em></strong>, and when I changed it to a mixture of text and numbers like: <em>Doc1, Doc2, ...</em> I see a different number for the count of generated vectors, but still I do not have the same number of feature vectors as expected.</p>
</blockquote>
",Text Generation & LLMs,gensim doc vec model generates limited number vector using gensim doc vec model generate feature vector code using explained problem code wondering mistake parameter set update wa playing tag parameter taggeddocument changed mixture text number like doc doc see different number count generated vector still number feature vector expected
PMML (for text categorization) in use,"<p>I have generated PMML file for text categorization using jpmml-sklearn (in python) using the following code:</p>

<pre><code>Textpipeline = PMMLPipeline([
    (""tf-idf"", TfidfVectorizer(analyzer = ""word"", preprocessor = None, strip_accents = None, token_pattern = None, tokenizer = Splitter(), stop_words = ""english"", ngram_range = (1, 2), norm = None,max_features = 50)),
    (""classifier"", SGDClassifier())
])

Textpipeline.fit(data.data, data.target)

from sklearn2pmml import sklearn2pmml

sklearn2pmml(Textpipeline, ""TextMiningClassifier_SGD_Classifier.pmml"", with_repr = True)
</code></pre>

<p>Now, I want to put this PMML (trained model) in action where I can import it (in some platform, say JAVA or some other platform) and categorize input text. </p>

<p>Would be great to know the steps on how I can import the above PMML model, and generate predictions on a new dataset (in this case just the categorization of input text). </p>

<p>Would appreciate any help in this.</p>
",Text Generation & LLMs,pmml text categorization use generated pmml file text categorization using jpmml sklearn python using following code want put pmml trained model action import platform say java platform categorize input text would great know step import pmml model generate prediction new dataset case categorization input text would appreciate help
Calculate overall sentiment with magnitude and score values,"<p>I'm using Google's Natural Language API to generate the sentiment from a piece of text.</p>

<p>From Google's own docs:</p>

<blockquote>
  <p>Score of the sentiment ranges between -1.0 (negative) and 1.0
  (positive) and corresponds to the overall emotional leaning of the
  text. Magnitude indicates the overall strength of emotion (both
  positive and negative) within the given text, between 0.0 and +inf.
  Unlike score, magnitude is not normalized; each expression of emotion
  within the text (both positive and negative) contributes to the text's
  magnitude (so longer text blocks may have greater magnitudes).</p>
</blockquote>

<p><strong>My goal is to get a single number of the ""general sentiment"" between 1 and 10.</strong> Google's old API used to return sentiment using a <em>single number</em> between -100 and +100 which was easy to map using something along the lines of.</p>

<pre><code>function map_range(value, low1, high1, low2, high2) {
    return low2 + (high2 - low2) * (value - low1) / (high1 - low1);
}
Math.round(map_range(SENTIMENT, -100, 100, 0, 10));
</code></pre>

<p>As Google have made the old API depreciated I've jumped to the new one, but in order to restore functionality to the rest of my app I'll need to map these two numbers back to between 1 and 10. Hopefully that makes sense! </p>
",Text Generation & LLMs,calculate overall sentiment magnitude score value using google natural language api generate sentiment piece text google doc score sentiment range negative positive corresponds overall emotional leaning text magnitude indicates overall strength emotion positive negative within given text inf unlike score magnitude normalized expression emotion within text positive negative contributes text magnitude longer text block may greater magnitude goal get single number general sentiment google old api used return sentiment using single number wa easy map using something along line google made old api depreciated jumped new one order restore functionality rest app need map two number back hopefully make sense
Construction of infinite state space model in reinforcement learning,"<p>Are there any materials or lectures on infinite state space model in reinforcement learning ? Or How to proceed creating an environment which can have infinite state space. I'm looking to generate text through reinforcement learning, so any guidance on above would also be helpful. </p>
",Text Generation & LLMs,construction infinite state space model reinforcement learning material lecture infinite state space model reinforcement learning proceed creating environment infinite state space looking generate text reinforcement learning guidance would also helpful
How can I find a good distracter for a key using python,"<p>What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.</p>
",Text Generation & LLMs,find good distracter key using python trying create multiple choice question mcq generation fill gap style question generator need generate distracters wrong answer key correct answer mcq generated educational text user input trying tackle combining contextual similarity similarity sentence key distractors occur difference term frequency help wa thinking using big data datasets generate related distractors one provided google vision clue achieve python
"&quot;g++ not detected&quot; while data set goes larger, is there any limit to matrix size in GPU?","<p>I got this message in using Keras to train an RNN for language model with a big 3D tensor (generated from a text, one hot encoded, and results a shape of (165717, 25, 7631)):</p>

<pre><code>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to 
execute optimized C-implementations (for both CPU and GPU) and will default to 
Python implementations. Performance will be severely degraded. To remove this 
warning, set Theano flags cxx to an empty string.
ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc 
installation and try again.
</code></pre>

<p>But everything goes well while I limit the size of data set into small. Thus I wonder that does Theano or CUDA limit the size of matrix? </p>

<p>Besides, do I have a better way to do one hot representation? I mean, in the large 3D tensor, most elements are 0 due to the one-hot representation. However, I didn't found a layer which accepts index representation of words.</p>
",Text Generation & LLMs,g detected data set go larger limit matrix size gpu got message using kera train rnn language model big tensor generated text one hot encoded result shape everything go well limit size data set small thus wonder doe theano cuda limit size matrix besides better way one hot representation mean large tensor element due one hot representation however found layer accepts index representation word
library to generate Multi tags from text data?,"<p>I want to generate MultiTag from any text file. Suppose i have subtitles from one of the video from Machine Learning Course.</p>

<blockquote>
  <p><em>""In the last video, we talked about the hypothesis representation for logistic regression. What Id like to do now is tell you about something called the decision boundary, and this will give us a better sense of what the logistic regressions hypothesis function is computing. 
  To recap, this is what we wrote out last time, where we said that the hypothesis is represented as h of x equals g of theta transpose x, where g is this function called the sigmoid function, which looks like this. It slowly increases from zero to one, asymptoting at one"".</em></p>
</blockquote>

<p>from above text i want to generate tag like : <strong>Hypothesis Representation ,logistic regression, decision boundary,sigmoid function</strong></p>

<p>Any Tagger is based on two model <em>Supervised and Unsupervised</em>.Since these text can belong to any domain, not just technical domain, therefore i do not have proper training set to train my model based on Supervised Learning so i choose Unsupervised techniques.</p>

<p>I tried different python library to generate tag, but did not succeed. I used below library:</p>

<blockquote>
  <p>Nltk library, spacy, textBlob,rake,ngram meathod.</p>
</blockquote>

<p>i wrote my own methods to solve this but i am able to generate single tag only. I used Brown nltk corpus to calculate weight for every words in the text and based on the weight i sorted the list of words. But it is not accurate though. Below is the code</p>

<pre><code>import nltk
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
import re
from nltk.corpus import brown
import operator
from _io import open
news_text = brown.words()
fdist = nltk.FreqDist(w.lower() for w in news_text)

def subtitle_to_words( subtitles ):

    #  Remove HTML
    review_text = BeautifulSoup(subtitles, ""lxml"").get_text() 

    #  Remove non-letters        
    letters_only = re.sub(""[^a-zA-Z]"", "" "", review_text) 
    #
    # 3. Convert to lower case, split into individual words
    words = letters_only.lower().split()                             
    #
    #  convert the stop words to a set
    stops = set(stopwords.words(""english""))                  
    # 
    # 5. Remove stop words
    meaningful_words = [w for w in words if not w in stops]   
    #
    # 6. Join the words back into one string separated by space, 
    # and return the result.
    return( meaningful_words) 


text=open('text4.txt','r')
withoutStopWords=subtitle_to_words(text)
keyword_dict={}
abs_occurences_dict={}
for m in withoutStopWords:
    if m not in abs_occurences_dict.keys():
        abs_occurences_dict[m]=1
    else:
        abs_occurences_dict[m]+=1
    keyword_dict[m]=abs_occurences_dict[m]/float((fdist[m.lower()]+1))

sorted_dict = sorted(keyword_dict.items(), key=operator.itemgetter(1),reverse=True)
for i in range(0,int(len(sorted_dict))):
    if len(sorted_dict[i][0])&gt;2:
        print(sorted_dict[i][0])
</code></pre>

<p>Can anyone suggest any method or Python library to solve this problem.</p>
",Text Generation & LLMs,library generate multi tag text data want generate multitag text file suppose subtitle one video machine learning course last video talked hypothesis representation logistic regression id like tell something called decision boundary give u better sense logistic regression hypothesis function computing recap wrote last time said hypothesis represented h x equal g theta transpose x g function called sigmoid function look like slowly increase zero one asymptoting one text want generate tag like hypothesis representation logistic regression decision boundary sigmoid function tagger based two model supervised unsupervised since text belong domain technical domain therefore proper training set train model based supervised learning choose unsupervised technique tried different python library generate tag succeed used library nltk library spacy textblob rake ngram meathod wrote method solve able generate single tag used brown nltk corpus calculate weight every word text based weight sorted list word accurate though code anyone suggest method python library solve problem
Is it possible to process JSON file from AlchemyAPI to generate questions?,"<p><a href=""https://alchemy-language-demo.mybluemix.net/"" rel=""nofollow"">https://alchemy-language-demo.mybluemix.net/</a></p>

<p>I have my REST API and when I fetch the JSON file processed by the API and use it to create questions by removing the keyword on the sentence </p>

<p>{
      ""sentence"": ""In 2009, Elliot Turner launched AlchemyAPI to process the written word, with all of its quirks and nuances, and got immediate traction."",
      ""subject"": {
        ""text"": ""Elliot Turner"",
        ""entities"": [
          {
            ""type"": ""Person"",
            ""text"": ""Elliot Turner""
          }
        ],
        ""keywords"": [
          {
            ""text"": ""Elliot Turner""
          }
        ]
      },</p>

<p>Thank you and hoping to get a reply thanks!</p>
",Text Generation & LLMs,possible process json file alchemyapi generate question rest api fetch json file processed api use create question removing keyword sentence sentence elliot turner launched alchemyapi process written word quirk nuance got immediate traction subject text elliot turner entity type person text elliot turner keywords text elliot turner thank hoping get reply thanks
The natural language sentences generation,"<p>I'm looking for a way how to generate or synthesize natural language sentences. I need to write about 1000 sentences like following: </p>

<p>""Hi there! I would like to go for a walk around the city in the evening. Do not be shy, text me message""</p>

<p>or</p>

<p>""Whats up guys! I'm looking for a company for visiting museum of science. It is more enjoyable to visit it with somebody""</p>

<p>It is users' events. For example a user wants to visit some place, event, seek a company for outdoor or trip together.</p>

<p>I found just one way: write list of first, second and third part of a sentence and then randomly assemble whole sentence from parts. 
But I do not know, maybe there already is similar lists? Maybe there are another way or special techniques for solve my task?</p>
",Text Generation & LLMs,natural language sentence generation looking way generate synthesize natural language sentence need write sentence like following hi would like go walk around city evening shy text message whats guy looking company visiting museum science enjoyable visit somebody user event example user want visit place event seek company outdoor trip together found one way write list first second third part sentence randomly assemble whole sentence part know maybe already similar list maybe another way special technique solve task
"Writing code for A Neural Probabilistic Language Model Bengio, 2003. Not able to understand the model","<p>I'm trying to write code for <a href=""http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"" rel=""nofollow"">A Neural Probabilistic Language Model by yoshua Bengio, 2003</a>, but I'm not able to understand the connections between the input layer and projection matrix and between projection matrix and hidden layer.  I'm not able to get how exactly is the learning for word-vector representation taking place.</p>
",Text Generation & LLMs,writing code neural probabilistic language model bengio able understand model trying write code neural probabilistic language model yoshua bengio able understand connection input layer projection matrix projection matrix hidden layer able get exactly learning word vector representation taking place
Implementing fuzzy search suggestions/word completion,"<p>I have a list of a bunch of phrases. Because this is a fairly long list, I also have a text box which users can type into as a search bar. As of right now, terms that do not exactly contain with the letters in the search bar are filtered out. However, I would like to have it give a list of a few suggestions of what the word might be.</p>

<p><strong>Note:</strong> I am not looking for a ""Did you mean..."" or spell checking algorithm like the ones <a href=""https://stackoverflow.com/questions/5859561/getting-the-closest-string-match"">here</a> or <a href=""https://stackoverflow.com/questions/12239236/google-fuzzy-search-a-k-a-suggestions-what-techniques-are-in-use"">here</a> or <a href=""https://stackoverflow.com/questions/16771754/transliteration-and-fuzzy-search-like-google-suggestions"">here</a> (though <a href=""https://i.sstatic.net/ltCIu.png"" rel=""nofollow noreferrer"">this image</a> from the first link seems good); I want an algorithm that will be able to suggest the best match for an <em>incomplete</em> word or phrase; e.g. the word <code>""bat""</code> should be a better match of the word <code>""battery""</code> than the word <code>""car""</code>.</p>

<p>It would also be impractical to use Google's method of returning the few strings that are most common that start with (approximately) the same letters, because, as far as I know, each element in the list would be equally as common as any other.</p>

<p>Also, I would like to do this in Java (8); however, other language answers are acceptable, as long as they do not use built in functions for which Java has no equivalent. In case it is useful, I wrote a modified version of Levenshtein distance (below) which fills the search string with asterisks signifying ""any character."" This works for single words, e.g. <code>""mud""</code> is a perfect match of <code>""muddy""</code>, but isn't good enough when considering people may use <code>""car""</code> to search for <code>""race car""</code>.</p>

<pre><code>/**
 * &lt;ul&gt;
 * &lt;b&gt;&lt;i&gt;searchDistance&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
 * &lt;br&gt;
 * &lt;code&gt;&amp;nbsp;public static int searchDistance(String key, String match)&lt;/code&gt;&lt;br&gt;
 * &lt;br&gt;
 * Gets the Levenshtein distance between &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;match&lt;/code&gt;. &lt;br&gt;
 * If &lt;code&gt;useAsterisk&lt;/code&gt; is true, then the follwing applies: If &lt;code&gt;key&lt;/code&gt; is shorter than &lt;code&gt;match&lt;/code&gt;, the asterisk &lt;code&gt;'*'&lt;/code&gt; is appended to it until the lengths are equal. Asterisks can be used in &lt;code&gt;key&lt;/code&gt; to signify 'any character.'
 * @param key - The text to search for
 * @param match - The text to compare &lt;code&gt;key&lt;/code&gt; against
 * @param useAsterisk - Whether or not to use asterisks for the purpose described above
 * @return the Levenshtein distance between &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;match&lt;/code&gt;.
 *         &lt;/ul&gt;
 */
public static int searchDistance(String key, String match, boolean useAsterisk) {
    while (key.length() &lt; match.length()) {
        key = key + ""*"";
    }

    int[][] matrix = new int[key.length() + 1][match.length() + 1];

    for (int i = 0; i &lt; matrix.length; i++) {
        matrix[i][0] = i;
    }

    for (int i = 0; i &lt; matrix[0].length; i++) {
        matrix[0][i] = i;
    }

    for (int a = 1; a &lt; matrix.length; a++) {
        for (int b = 1; b &lt; matrix[0].length; b++) {
            matrix[a][b] = Math.min(Math.min(matrix[a - 1][b] + 1, matrix[a][b - 1] + 1), matrix[a - 1][b - 1] + (key.charAt(a - 1) == match.charAt(b - 1) || key.charAt(a - 1) == '*' ? 0 : 1));
        }
    }

    return matrix[matrix.length - 1][matrix[0].length - 1];
}
</code></pre>

<p>TL;DR: Is there a good way to give completion suggestions for search terms?</p>

<p>Thanks in advance!</p>
",Text Generation & LLMs,implementing fuzzy search suggestion word completion list bunch phrase fairly long list also text box user type search bar right term exactly contain letter search bar filtered however would like give list suggestion word might note looking mean spell checking algorithm like one image first link seems good want algorithm able suggest best match incomplete word phrase e g word better match word word would also impractical use google method returning string common start approximately letter far know element list would equally common also would like java however language answer acceptable long use built function java ha equivalent case useful wrote modified version levenshtein distance fill search string asterisk signifying character work single word e g perfect match good enough considering people may use search tl dr good way give completion suggestion search term thanks advance
Input shape for Keras LSTM/GRU language model,"<p>I am trying to train a language model on word level in Keras. </p>

<p>I have my X and Y, both with the shape (90582L, 517L)</p>

<p>When I try fit this model:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(90582, 517)))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributedDense(1))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(x_pad, y_pad, batch_size=128, nb_epoch=2)
</code></pre>

<p>I get the error:</p>

<pre><code>Exception: Error when checking model input: 
expected gru_input_7 to have 3 dimensions, but got array with shape (90582L, 517L)
</code></pre>

<p>I need some guidance as to what the input shape should be? I've done trial and error on all sorts of combinations but it seems I am misunderstanding something fundamental.</p>

<p>In the Keras text generation example, the X matrix had 3 dimensions. I have no idea what the third dimension is supposed to be though.  </p>
",Text Generation & LLMs,input shape kera lstm gru language model trying train language model word level kera x shape l l try fit model get error need guidance input shape done trial error sort combination seems misunderstanding something fundamental kera text generation example x matrix dimension idea third dimension supposed though
Ngram model and smoothing algorithm,"<p>Which smoothing algorithm is easy and effective in case of implementation point of view? </p>

<p>My training corpus is a hex dump looks like,</p>

<pre><code>64 FA EB 63 31 D2 62 22 19 BD 64 B5 63 17 4F 48 62 A8 64 11 0F 62 15 9B 64 9B 1F E1 63 62 BE 63
</code></pre>

<p>I would like to build a 2,3,4,5-gram language model on it. And eventually I need smoothing! Which smoothing algorithm will be suitable and will be easy to implement in this case?</p>
",Text Generation & LLMs,ngram model smoothing algorithm smoothing algorithm easy effective case implementation point view training corpus hex dump look like would like build gram language model eventually need smoothing smoothing algorithm suitable easy implement case
How do I change the Keras text generation example from being on character level to word level?,"<p>The above code is more or less what the Keras documentation gives us as a language model. The thing is that this language model predicts characters, not words. Strictly speaking, a language model is supposed to predict full words. </p>

<p>My question is, how do I change this in order to predict full words?</p>

<pre><code>from __future__ import print_function
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.utils.data_utils import get_file
import numpy as np
import random
import sys

path = ""C:/Users/Cedric     Oeldorf/Desktop/University/Research/Data/Gutenberg/MYDATAFINAL3.txt""
text = open(path).read().lower()
print('corpus length:', len(text))

chars = set(text)
print('total chars:', len(chars))
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

# cut the text in semi-redundant sequences of maxlen characters
maxlen = 40
step = 3
sentences = []
next_chars = []
for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])
print('nb sequences:', len(sentences))

print('Vectorization...')
X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

from keras.callbacks import History
histLSTM = History()

# build the model: 2 stacked LSTM
print('Build model...')
model = Sequential()
model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(LSTM(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

model.fit(X, y, batch_size=128, nb_epoch=4, callbacks=[histLSTM])
</code></pre>

<p>My data preprocessing idea so far is:</p>

<pre><code>path = ""C:/MYDATAFINAL3.txt""
text = open(path).read().lower()
print('corpus length:', len(text))

#tokenize corpus and get list of unique words
tok = gensim.utils.simple_preprocess(text, deacc=False)
words = set(tok)
word_indices = dict((c, i) for i, c in enumerate(words))
indices_word = dict((i, c) for i, c in enumerate(words))

sentences1 = text.split('.')
SYMBOLS = '{}()[].,:;+-*/&amp;|&lt;&gt;=~$'
m = [item.translate(None, SYMBOLS).strip() for item in sentences1]
del text

maxlen = 60
step = 3
sentences = []
next_words = []
for i in range(0, len(tok) - maxlen, step):
    sentences.append(tok[i: i + maxlen])
    next_words.append(tok[i + maxlen])
print('nb sequences:', len(sentences))

X = np.zeros((len(sentences), maxlen), dtype=""int32"")
y = np.zeros((len(sentences),maxlen), dtype=""int32"")
</code></pre>

<p>This step isnt working out:</p>

<pre><code>#In X, change boolean to true for every listed character, same for y
for i, sentence in enumerate(sentences):
    for t, words in enumerate(sentence):
        X[i, t,] = word_indices[words]
    y[i, t] = word_indices[words]
</code></pre>

<p>And I don't know what input shape I should be using:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(len(sentences), maxlen)))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=True))
model.add(Dropout(0.2))
#model.add(Dense(len(chars)))
#Insert this instead:
model.add(TimeDistributedDense(len(words)))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(X, y, batch_size=128, nb_epoch=2)
</code></pre>
",Text Generation & LLMs,change kera text generation example character level word level code le kera documentation give u language model thing language model predicts character word strictly speaking language model supposed predict full word question change order predict full word data preprocessing idea far step isnt working know input shape using
How to calculate perplexity for a language model trained using keras?,"<p>Using Python 2.7 Anaconda on Windows 10</p>

<p>I have trained a GRU neural network to build a language model using keras:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
</code></pre>

<p>How do I calculate the perplexity of this language model? For example, NLTK offers a perplexity calculation function for its models. </p>
",Text Generation & LLMs,calculate perplexity language model trained using kera using python anaconda window trained gru neural network build language model using kera calculate perplexity language model example nltk offer perplexity calculation function model
What&#39;s the probability to calculate in a unigram language model?,"<p>I created a unigram language model for a sentence completion implementation. I have all the words with their occurences number. </p>

<p>I'm confused on how to compare them from here. I would think that I have to calculate the probability of each case and take the biggest one.</p>

<p>So if I have 3 words that can be used, I compare the number of occurences of each word and take the highest ? Is this the proper implementation ? </p>

<p>Or I divide the number of occurences of each word with the number of all (distinct?) words of the training set ?</p>

<p>Thank you.</p>
",Text Generation & LLMs,probability calculate unigram language model created unigram language model sentence completion implementation word occurences number confused compare would think calculate probability case take biggest one word used compare number occurences word take highest proper implementation divide number occurences word number distinct word training set thank
language model with SRILM,"<p>I'm trying to build a language model using SRILM.
I have a list of phrases and I create the model using:</p>

<pre><code>./ngram-count -text corpus.txt -order 3 -ukndiscount -interpolate -unk -lm corpus.lm
</code></pre>

<p>After this I tried to make some example to see the probabilities of different phrases and it turned out that  has a log probability of <em>-0.9</em>.</p>

<p>The problem is that there are some words in the training with a lower log probability. For example there are 5 <em>""abatantuono""</em> and its log probability  is <em>-4.8</em>.</p>

<p>I think this is strange because a phrase <code>&lt;s&gt; &lt;unk&gt; &lt;/s&gt;</code> is more probable  than <code>&lt;s&gt; abatantuono &lt;/s&gt;</code> and in the training set the 3-gram <code>&lt;s&gt; abatantuono &lt;/s&gt;</code> is also present!</p>

<p>This can be seen here:</p>

<pre><code> % ./ngram -lm corpus.lm -ppl ../../../corpus.txt.test -debug 2 -unk
 reading 52147 1-grams
 reading 316818 2-grams
 reading 91463 3-grams
 abatantuono
     p( abatantuono | &lt;s&gt; )     = [2gram] 1.6643e-05 [ -4.77877 ]
     p( &lt;/s&gt; | abatantuono ...)     = [3gram] 0.717486 [ -0.144186 ]
 1 sentences, 1 words, 0 OOVs
 0 zeroprobs, logprob= -4.92296 ppl= 289.386 ppl1= 83744.3

 abatantonno
     p( &lt;unk&gt; | &lt;s&gt; )   = [1gram] 0.00700236 [ -2.15476 ]
     p( &lt;/s&gt; | &lt;unk&gt; ...)   = [1gram] 0.112416 [ -0.949172 ]
 1 sentences, 1 words, 0 OOVs
 0 zeroprobs, logprob= -3.10393 ppl= 35.6422 ppl1= 1270.36

 file ../../../corpus.txt.test: 2 sentences, 2 words, 0 OOVs
 0 zeroprobs, logprob= -8.02688 ppl= 101.56 ppl1= 10314.3
</code></pre>

<p>What do you think the problem could be?</p>

<p>Thank you</p>
",Text Generation & LLMs,language model srilm trying build language model using srilm list phrase create model using tried make example see probability different phrase turned ha log probability problem word training lower log probability example abatantuono log probability think strange phrase probable training set gram also present seen think problem could thank
Next-Word Prediction Engines - which branch of AI do they belong,"<p>Next-word prediction or phrase-prediction engines used in modern keyboards of mobiles and tablets, like swift key &amp; XT9, which predict the next word the user is going to type based on some pre-defined or dynamic corpus, based on n-grams (maximum frequency of last typed 2-3 words plus the current word) based language models (Markov Model).</p>

<p>What I think is that these engines/algos are a part of AI/NLP. But what I am not sure about is what specific branch of AI/NLP they belong to.
Is it machine learning ? Is it data science ? Is it big data ? Is it Computing Intelligence ? Is it decision-making ? Is it data-mining ? Or statistical pattern recognition/ predictive analytics/ Supervised learning/ Unsupervised learning ? Or all/many of these or something else ?</p>
",Text Generation & LLMs,next word prediction engine branch ai belong next word prediction phrase prediction engine used modern keyboard mobile tablet like swift key xt predict next word user going type based pre defined dynamic corpus based n gram maximum frequency last typed word plus current word based language model markov model think engine algos part ai nlp sure specific branch ai nlp belong machine learning data science big data computing intelligence decision making data mining statistical pattern recognition predictive analytics supervised learning unsupervised learning many something else
How do I generate random text in NLTK 3.0?,"<p>The generate method of nltk.text.Text seems to have been removed in NLTK 3.0.</p>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; bible = nltk.corpus.gutenberg.words(u'bible-kjv.txt')
&gt;&gt;&gt; bibleText = nltk.Text(bible)
&gt;&gt;&gt; bibleText.generate()
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'Text' object has no attribute 'generate'
</code></pre>

<p>It may just be that I'm remembering wrongly how to do this, but everything I can find online seems to support the above method. Any ideas what I'm doing wrong?</p>
",Text Generation & LLMs,generate random text nltk generate method nltk text text seems removed nltk example may remembering wrongly everything find online seems support method idea wrong
Is Mark V. Shaney still the best way to generate text?,"<p>I want to take large documents and generate text that resembles them. I know that Markov Chains were used to do this with <a href=""http://en.wikipedia.org/wiki/Mark_V_Shaney"" rel=""noreferrer"">Mark V Shaney</a>. Is there a better way to do it now, or is this approach still basically the best one available?</p>
",Text Generation & LLMs,mark v shaney still best way generate text want take large document generate text resembles know markov chain used mark v shaney better way approach still basically best one available
BerkeleyLM: Get n-gram probability,"<p>I have a BerkeleyLM language <a href=""https://berkeleylm.googlecode.com/svn-history/r529/trunk/doc/edu/berkeley/nlp/lm/io/LmReaders.html#readKneserNeyLmFromTextFile%28java.util.List,%20edu.berkeley.nlp.lm.WordIndexer,%20int,%20edu.berkeley.nlp.lm.ConfigOptions,%20boolean%29"" rel=""nofollow"">model</a> and want to use it to get the probability in that model of a given n-gram (which comes from a sentence).</p>

<p>I have tested both methods as listed below but they do not return probabilities but negative numbers, e.g., -1.111 or -5. What is done wrong here?</p>

<ul>
<li><a href=""https://berkeleylm.googlecode.com/svn-history/r529/trunk/doc/edu/berkeley/nlp/lm/ArrayEncodedProbBackoffLm.html#getLogProb%28java.util.List%29"" rel=""nofollow"">getLogProb</a></li>
<li><a href=""https://berkeleylm.googlecode.com/svn-history/r529/trunk/doc/edu/berkeley/nlp/lm/AbstractArrayEncodedNgramLanguageModel.html#scoreSentence%28java.util.List%29"" rel=""nofollow"">scoreSentence</a></li>
</ul>
",Text Generation & LLMs,berkeleylm get n gram probability berkeleylm language model want use get probability model given n gram come sentence tested method listed return probability negative number e g done wrong getlogprob scoresentence
Probability tree for sentences in nltk employing both lookahead and lookback dependencies,"<p>Does nltk or any other NLP tool allow to construct probability trees based on input sentences thus storing the language model of the input text in a dictionary tree, the following <a href=""https://stackoverflow.com/a/31835523/2305993"">example</a> gives the rough idea, but I need the same functionality such that a word Wt does not just probabilistically modelled  on past input words(history) Wt-n but also on lookahead words like Wt+m. Also the lookback and lookahead word count should also be 2 or more i.e. bigrams or more. Are there any other libraries in python which achieve this?</p>

<pre><code>from collections import defaultdict
import nltk
import math

ngram = defaultdict(lambda: defaultdict(int))
corpus = ""The cat is cute. He jumps and he is happy.""
for sentence in nltk.sent_tokenize(corpus):
    tokens = map(str.lower, nltk.word_tokenize(sentence))
    for token, next_token in zip(tokens, tokens[1:]):
        ngram[token][next_token] += 1
for token in ngram:
    total = math.log10(sum(ngram[token].values()))
    ngram[token] = {nxt: math.log10(v) - total for nxt, v in ngram[token].items()}
</code></pre>

<p>the solution  requires both lookahead and lookback and a specially sub classed dictionary may help in solving this problem. Can also point to relevant resources which talk about implementing such a system. A nltk.models seemed to be doing something similar but is no longer available. Are there any existing design patterns in NLP which implement this idea? skip gram based models are similar to this idea too but I feel this has should have been implemented already somewhere.</p>
",Text Generation & LLMs,probability tree sentence nltk employing lookahead lookback dependency doe nltk nlp tool allow construct probability tree based input sentence thus storing language model input text dictionary tree following href give rough idea need functionality word wt doe probabilistically modelled past input word history wt n also lookahead word like wt also lookback lookahead word count also e bigram library python achieve p solution requires lookahead lookback specially sub classed dictionary may help solving problem also point relevant resource talk implementing system nltk model seemed something similar longer available existing design pattern nlp implement idea skip gram based model similar idea feel ha implemented already somewhere
What is the underlying algorithm for predicting hidden events using a hidden event language model?,"<p>I'm modeling the punctuation prediction problem as arising from a hidden event model, and am trying to follow the algorithm described in Stolcke's paper <a href=""http://www.sri.com/sites/default/files/publications/modeling_the_prosody_of_hidden_events.pdf"" rel=""nofollow noreferrer"" title=""paper"">Modeling the Prosody of Hidden Events for Improved Word Recognition</a>.</p>
<p>After calculating an ngram model, he describes the algorithm for calculating the maximum likelihood sequence of events:</p>
<blockquote>
<p>By using an N-gram model for P(W,S), and decomposing the prosodic likelihoods as in Equation 4, the joint model P(W,S,F)
becomes equivalent to a hidden Markov
model (HMM). The HMM states are the (word,event) pairs, while
prosodic features form the observations. Transition probabilities
are given by the N-gram model; emission probabilities are estimated by the prosodic model described below.  Based on this
construction, we can carry out the summation over all possible
event sequences efficiently  with  the  familiar  forward  dynamic
programming algorithm for HMMs.</p>
</blockquote>
<p>I'm confused how this can be a Markov model with states (word, event) since if our underlying model is an N-gram model, it seems to me that the state needs to encode the N-1 previous words in order to have all necessary information to predict the next state. What's going on here? Thanks!</p>
",Text Generation & LLMs,underlying algorithm predicting hidden event using hidden event language model modeling punctuation prediction problem arising hidden event model trying follow algorithm described stolcke paper modeling prosody hidden event improved word recognition calculating ngram model describes algorithm calculating maximum likelihood sequence event using n gram model p w decomposing prosodic likelihood equation joint model p w f becomes equivalent hidden markov model hmm hmm state word event pair prosodic feature form observation transition probability given n gram model emission probability estimated prosodic model described based construction carry summation possible event sequence efficiently familiar forward dynamic programming algorithm hmms confused markov model state word event since underlying model n gram model seems state need encode n previous word order necessary information predict next state going thanks
"From a pool of webpages, finding pages similar to any given webpage","<p>I am given a set of webpages and I need to build a page recommender. Whichever URL is given to the application, the application should be able to find out pages from the given pool that are similar to the page at the URL. </p>

<hr>

<p>I tried looking for different approaches to do that. The use of word2vec interested me. I am planning to crawl through all the given set of webpages and generate tags for that page based on the content in that page. From these tags I was hoping to use word2vec to calculate a vector value for the page and store it. When searching, I would caclulate vector for the given page in similar way to look for similar values. Is this the correct way of using word2vec? What training vector should be used? Any other better way to do this task?Or just plain text matching would be a better option?</p>
",Text Generation & LLMs,pool webpage finding page similar given webpage given set webpage need build page recommender whichever url given application application able find page given pool similar page url tried looking different approach use word vec interested planning crawl given set webpage generate tag page based content page tag wa hoping use word vec calculate vector value page store searching would caclulate vector given page similar way look similar value correct way using word vec training vector used better way task plain text matching would better option
What is the most efficient way of storing language models in NLP applications?,"<p>How do they usually store and update language models (such as N-gram models)? What kind of structure is the most efficient way for storing these models in databases?</p>
",Text Generation & LLMs,efficient way storing language model nlp application usually store update language model n gram model kind structure efficient way storing model database
Efficient way of resolving unknown words to known words?,"<p>I am designing a text processing program that will generate a list of keywords from a long itemized text document, and combine entries for words that are similar in meaning. There are metrics out there, however I have a new issue of dealing with words that are not in the dictionary that I am using. </p>

<p>I am currently using nltk and python, but my issues here are of a much more abstracted nature. Given a word that is not in a dictionary, what would be an efficient way of resolving it to a word that is within your dictionary? My only current solution involves running through the words in the dictionary and picking the word with the shortest Levenshtein distance (editing distance) from the inputted word.</p>

<p>Obviously this is a very slow and impractical method, and I don't actually need the absolute best match from within the dictionary, just so long as it is a contained word and it is pretty close. Efficiency is more important for me in the solution, but a basic level of accuracy would also be needed. </p>

<p>Any ideas on how to generally resolve some unknown word to a known one in a dictionary? </p>
",Text Generation & LLMs,efficient way resolving unknown word known word designing text processing program generate list keywords long itemized text document combine entry word similar meaning metric however new issue dealing word dictionary using currently using nltk python issue much abstracted nature given word dictionary would efficient way resolving word within dictionary current solution involves running word dictionary picking word shortest levenshtein distance editing distance inputted word obviously slow impractical method actually need absolute best match within dictionary long contained word pretty close efficiency important solution basic level accuracy would also needed idea generally resolve unknown word known one dictionary
What is the approach to generate Topics from text using a wikipedia dump,"<p>I'm new to NLP/text processing</p>

<p>and building an application which requires generating topics (Music, Games, Romance, History etc etc.) from about 2 lines of imput text.</p>

<p>I've decided to use wikipedia's articlebase to help me out in this process,</p>

<p>What would be steps to ""train"" my program to recognize and categorize these topics from my input text?</p>
",Text Generation & LLMs,approach generate topic text using wikipedia dump new nlp text processing building application requires generating topic music game romance history etc etc line imput text decided use wikipedia articlebase help process would step train program recognize categorize topic input text
Scala: large calculation losing value to zero/infinity,"<p>I'm trying calculate a perplexity value for a language model and the calculation uses a lot of large powers. I have tried converting my calculation to log space using BigDecimal, but I'm not having any luck.</p>

<pre><code>var sum=0.0
for(ngram&lt;-testNGrams)
{
  var prob = Math.log(lm.prob(ngram.last, ngram.slice(0,ngram.size-1)))
  if (prob==0.0) sum = sum
  else sum = sum + prob
}
Math.pow(Math.log(Math.exp(sum)),-1.0/wordSize.toDouble)
</code></pre>

<p>How can I perform such a calculation in Scala without losing my large/small values to zero/Infinity? It seems like a trivial question but I haven't managed to do it.</p>

<p>In the above, you can assume that the method lm.prob issues the correct probabilities between 0 and 1, this has been amply tested.</p>
",Text Generation & LLMs,scala large calculation losing value zero infinity trying calculate perplexity value language model calculation us lot large power tried converting calculation log space using bigdecimal luck perform calculation scala without losing large small value zero infinity seems like trivial question managed assume method lm prob issue correct probability ha amply tested
My Bigram Model is not returning any value. Why?,"<p>I am building a Bigram Language Model with Laplace smoothing in Python. I wrote the following class, but it doesn't print anything when I try to print in the score function, since that function is calculating the log probability value of, why is it? What is wrong in the code?
I am giving a corpus which is tagged as the input. Example corpus: STOP to be   to be STOP</p>

<pre><code>class BiGramModel:
        def __init__(self, corpus):
            """"""Initialize your data structures in the constructor.""""""
            self.unigramCounts = {}
            self.bigramCounts = {}
            self.train(corpus)

        def train(self, corpus):

            for sentence in corpus.corpus:
                previous_token = """"
                for datum in sentence.data:
                    token = datum.word
                    if token in self.unigramCounts:
                        self.unigramCounts[token] = self.unigramCounts[token] + 1.0
                    else:
                        self.unigramCounts[token] = 1.0
                    if previous_token != """":
                        bigram = previous_token + "" | "" + token                             
                        if bigram in self.bigramCounts:
                            self.bigramCounts[bigram] = self.bigramCounts[bigram] + 1.0
                        else:
                            self.bigramCounts[bigram] = 1.0
                    previous_token = token

        def score(self, sentence):
    ""It takes a list of strings as argument and returns the log-probability of the 
    sentence using the bigram language model.""
    score = 1.0
    vocabulary = len(self.bigramCounts) + 0.0       
    previous_token = """"
    for token in sentence:
        unigram_find = self.unigramCounts[token] if token in self.unigramCounts else 0.0
        bigram = previous_token + "" | "" + token
        bigram_find = self.bigramCounts[bigram] if bigram in self.bigramCounts else 0.0                     
        score += math.log(bigram_find + 1.0)
        score -= math.log(unigram_find + vocabulary)
        previous_token = token
    return score
</code></pre>

<p>Why is it not giving me any output?</p>
",Text Generation & LLMs,bigram model returning value building bigram language model laplace smoothing python wrote following class print anything try print score function since function calculating log probability value wrong code giving corpus tagged input example corpus stop stop giving output
Mapping of words to stemmed words (Stem dictionary),"<p>I want to generate a mapping of ( word-stemmed word ) which il need for my project.</p>

<p>I am trying to generate the mapping this way</p>

<p>1.i took a text ( in file 1),used rapid miner to stem all the words and saved the resulting text in another file say file 2.</p>

<p>2.i wrote a java program which will take file1 and file 2 as parameters,scan a word from file1 and a word from file2 and store them as a pair in a HAshset.</p>

<p>Will this method work perfectly? Is there any other better method to do this task.</p>
",Text Generation & LLMs,mapping word stemmed word stem dictionary want generate mapping word stemmed word il need project trying generate mapping way took text file used rapid miner stem word saved resulting text another file say file wrote java program take file file parameter scan word file word file store pair hashset method work perfectly better method task
Encryption decryption of natural language model,"<p>I want encryption decryption of natural language model. I want to use natural language characters as key and text for my analysis work as below. How can i achieve that</p>

<pre><code> from Crypto.Cipher import AES
 import os

 BLOCK_SIZE = 32
 PADDING = '0'

 pad = lambda s: s + (BLOCK_SIZE - len(s) % BLOCK_SIZE) * PADDING

 EncodeAES = lambda c, s: c.encrypt(pad(s))

 DecodeAES = lambda c, e: c.decrypt(e.rstrip(PADDING))

 secret = u'ककककक..'

 obj = AES.new(secret)

 message = u'कककककककककककक'

 encoded = EncodeAES(obj, message)

 decoded = DecodeAES(obj, encoded)
 print 'Decrypted string: ', decoded.rstrip('0')
</code></pre>
",Text Generation & LLMs,encryption decryption natural language model want encryption decryption natural language model want use natural language character key text analysis work achieve
Generate a new text using the style of one text and the nouns/verbs of another?,"<p>I want to generate plausible (or less than plausible is okay too) nonsense text similar to the way that a <a href=""http://www.owlnet.rice.edu/~cz1/prog/markov/markov.html"" rel=""nofollow"">markov chain approach would do</a>, but I want the nouns and verbs of the generated text to come from a different source than the analyzed text. </p>

<p>So, for example, let's say that text 1 is from <a href=""http://www.gutenberg.org/files/11592/11592-h/11592-h.htm"" rel=""nofollow"">Little Red Riding Hood</a>, and my list of nouns/verbs is something like the ones listed here: <a href=""http://www.momswhothink.com/reading/list-of-nouns.html"" rel=""nofollow"">nouns</a>, <a href=""http://www.momswhothink.com/reading/list-of-verbs.html"" rel=""nofollow"">verbs</a>.  I'm looking for a way to swap out some/all of the nouns/verbs in text 1 with the new nouns/verbs. Then I would generate a new text from the mashup (perhaps using the markov chain approach). </p>

<p>I'm guessing that I need some sort of initial grammar analysis for text 1, and then perhaps do a swap with appropriately coded words of the insertion noun/verb lists?</p>
",Text Generation & LLMs,generate new text using style one text noun verb another want generate plausible le plausible okay nonsense text similar way markov chain approach would want noun verb generated text come different source analyzed text example let say text little red riding hood list noun verb something like one listed noun verb looking way swap noun verb text new noun verb would generate new text mashup perhaps using markov chain approach guessing need sort initial grammar analysis text perhaps swap appropriately coded word insertion noun verb list
Spelling correction likelihood,"<p>As stated by most spelling corrector tutors, the correct word W^ for an incorrectly spelled word x is:</p>

<p>W^ = argmax<sub>W</sub> P(X|W) P(W)</p>

<p>Where P(X|W) is the likelihood and P(W) is the Language model.</p>

<p>In the tutorial from where i am learning spelling correction, the instructor says that P(X|W) can be computed by using a confusion matrix which keeps track of how many times a letter in our corpus is mistakenly typed for another letter. I am using the World Wide Web as my corpus and it cant be guaranteed that a letter was mistakenly typed for another letter. So is it okay if i use the Levenshtein distance between X and W, instead of using the confusion matrix? Does it make much of a difference?</p>

<p>The way i am going to compute Lev. distance in python is this:</p>

<pre><code>from difflib import SequenceMatcher

def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()
</code></pre>

<p><a href=""https://stackoverflow.com/questions/17388213/python-string-similarity-with-probability"">See this</a></p>

<p>And here's the tutorial to make my question clearer: <a href=""https://coursera.org/course/nlp"" rel=""nofollow noreferrer"">Click here</a> </p>

<p>PS. i am working with Python</p>
",Text Generation & LLMs,spelling correction likelihood stated spelling corrector tutor correct word w incorrectly spelled word x w argmaxw p x w p w p x w likelihood p w language model tutorial learning spelling correction instructor say p x w computed using confusion matrix keep track many time letter corpus mistakenly typed another letter using world wide web corpus cant guaranteed letter wa mistakenly typed another letter okay use levenshtein distance x w instead using confusion matrix doe make much difference way going compute lev distance python click p working python
Spelling correction likelihood,"<p>As stated by most spelling corrector tutors, the correct word W^ for an incorrectly spelled word x is:</p>

<p>W^ = argmax<sub>W</sub> P(X|W) P(W)</p>

<p>Where P(X|W) is the likelihood and P(W) is the Language model.</p>

<p>In the tutorial from where i am learning spelling correction, the instructor says that P(X|W) can be computed by using a confusion matrix which keeps track of how many times a letter in our corpus is mistakenly typed for another letter. I am using the World Wide Web as my corpus and it cant be guaranteed that a letter was mistakenly typed for another letter. So is it okay if i use the Levenshtein distance between X and W, instead of using the confusion matrix? Does it make much of a difference?</p>

<p>The way i am going to compute Lev. distance in python is this:</p>

<pre><code>from difflib import SequenceMatcher

def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()
</code></pre>

<p><a href=""https://stackoverflow.com/questions/17388213/python-string-similarity-with-probability"">See this</a></p>

<p>And here's the tutorial to make my question clearer: <a href=""https://coursera.org/course/nlp"" rel=""nofollow noreferrer"">Click here</a> </p>

<p>PS. i am working with Python</p>
",Text Generation & LLMs,spelling correction likelihood stated spelling corrector tutor correct word w incorrectly spelled word x w argmaxw p x w p w p x w likelihood p w language model tutorial learning spelling correction instructor say p x w computed using confusion matrix keep track many time letter corpus mistakenly typed another letter using world wide web corpus cant guaranteed letter wa mistakenly typed another letter okay use levenshtein distance x w instead using confusion matrix doe make much difference way going compute lev distance python click p working python
N-gram text categorization category size difference compensation,"<p>Lately I've been mucking about with text categorization and language classification based on Cavnar and Trenkle's article ""N-Gram-Based Text Categorization"" as well as other related sources.</p>

<p>For doing language classification I've found this method to be very reliable and useful. The size of the documents used to generate the N-gram frequency profiles is fairly unimportant as long as they are ""long enough"" since I'm just using the most common n N-grams from the documents.</p>

<p>On the other hand well-functioning text categorization eludes me. I've tried with both my own implementations of various variations of the algorithms at hand, with and without various tweaks such as idf weighting and other peoples' implementations. It works quite well as long as I can generate somewhat similarly-sized frequency profiles for the category reference documents but the moment they start to differ just a bit too much the whole thing falls apart and the category with the shortest profile ends up getting a disproportionate number of documents assigned to it.</p>

<p>Now, my question is. What is the preferred method of compensating for this effect? It's obviously happening because the algorithm assumes a maximum distance for any given N-gram that equals the length of the category frequency profile but for some reason I just can't wrap my head around how to fix it. One reason I'm interested in this fix is actually because I'm trying to automate the generation of category profiles based on documents with a known category which can vary in length (and even if they are the same length the profiles may end up being different lengths). Is there a ""best practice"" solution to this?</p>
",Text Generation & LLMs,n gram text categorization category size difference compensation lately mucking text categorization language classification based cavnar trenkle article n gram based text categorization well related source language classification found method reliable useful size document used generate n gram frequency profile fairly unimportant long long enough since using common n n gram document hand well functioning text categorization eludes tried implementation various variation algorithm hand without various tweak idf weighting people implementation work quite well long generate somewhat similarly sized frequency profile category reference document moment start differ bit much whole thing fall apart category shortest profile end getting disproportionate number document assigned question preferred method compensating effect obviously happening algorithm assumes maximum distance given n gram equal length category frequency profile reason wrap head around fix one reason interested fix actually trying automate generation category profile based document known category vary length even length profile may end different length best practice solution
Single KeyWord Extraction from A Document,"<p>I wish to generate keywords from small text such as user tweets. I have already checked out these links.
<a href=""https://stackoverflow.com/questions/2764116/tag-generation-from-a-small-text-content-such-as-tweets"">tag generation from a small text content (such as tweets)</a> <br />
<a href=""https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content/"">tag generation from a text content</a></p>

<p>The problem is they use bigram or trigram collocations, hence they find only multi-word key phrases. I wish to find a single keyword as the topic. How should I modify them ?</p>
",Text Generation & LLMs,single keyword extraction document wish generate keywords small text user tweet already checked link problem use bigram trigram collocation hence find multi word key phrase wish find single keyword topic modify
nltk language model (ngram) calculate the prob of a word from context,"<p>I am using Python and NLTK to build a language model as follows:</p>



<pre class=""lang-py prettyprint-override""><code>from nltk.corpus import brown
from nltk.probability import LidstoneProbDist, WittenBellProbDist
estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
lm = NgramModel(3, brown.words(categories='news'), estimator)
# Thanks to miku, I fixed this problem
print lm.prob(""word"", [""This is a context which generates a word""])
&gt;&gt; 0.00493261081006
# But I got another program like this one...
print lm.prob(""b"", [""This is a context which generates a word""]) 
</code></pre>

<p>But it doesn't seem to work. The result is as follows:</p>

<pre><code>&gt;&gt;&gt; print lm.prob(""word"", ""This is a context which generates a word"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.6/dist-packages/nltk/model/ngram.py"", line 79, in prob
    return self._alpha(context) * self._backoff.prob(word, context[1:])
  File ""/usr/local/lib/python2.6/dist-packages/nltk/model/ngram.py"", line 79, in prob
    return self._alpha(context) * self._backoff.prob(word, context[1:])
  File ""/usr/local/lib/python2.6/dist-packages/nltk/model/ngram.py"", line 82, in prob
    ""context %s"" % (word, ' '.join(context)))
TypeError: not all arguments converted during string formatting
</code></pre>

<p>Can anyone help me out? Thanks!</p>
",Text Generation & LLMs,nltk language model ngram calculate prob word context using python nltk build language model follows seem work result follows anyone help thanks
NLTK text.generate(). Doing this with different ngram models,"<p>I have to generate random sentences using nltk. However, it seems like text.generate() only gives us sentences with trigrams. Is there any way I can expand this to include unigrams as well as bigrams?</p>

<p>My current code is:</p>

<pre><code>exclude = set(string.punctuation)
ln = ''.join(ch for ch in ln if ch not in exclude)


words = nltk.word_tokenize(ln)
my_bigrams = nltk.bigrams(words)
my_trigrams = nltk.trigrams(words)


tText = Text(words)
tText1 = Text(my_bigrams)
tText2 = Text(my_trigrams)
print tText.generate()
print tText1.generate()
print tText2.generate()
</code></pre>

<p>Change in generate() function:</p>

<pre><code>def generate(self, length=100, c=3):
    """"""
    Print random text, generated using a trigram language model.

    :param length: The length of text to generate (default=100)
    :type length: int
    :seealso: NgramModel
    """"""
    if '_trigram_model' not in self.__dict__:
        print ""Building ngram index...""
        estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
        self._trigram_model = NgramModel(c, self, estimator=estimator)
    text = self._trigram_model.generate(length)
    print tokenwrap(text)
</code></pre>
",Text Generation & LLMs,nltk text generate different ngram model generate random sentence using nltk however seems like text generate give u sentence trigram way expand include unigrams well bigram current code change generate function
longest common sequence group,"<p>Given the following lines of text</p>

<pre><code>TOKYO-BLING.1 H02-AVAILABLE
TOKYO-BLING.1 H02-MIDDLING
TOKYO-BLING.1 H02-TOP
TOKYO-BLING.2 H04-USED
TOKYO-BLING.2 H04-AVAILABLE
TOKYO-BLING.2 H04-CANCELLED
WAY-VERING.1 H03-TOP
WAY-VERING.2 H03-USED
WAY-VERING.2 H03-AVAILABLE
WAY-VERING.1 H03-CANCELLED
</code></pre>

<p>I would like to do some parsing to generate somewhat sensible groupings. The list above can be grouped as follows</p>

<pre><code>TOKYO-BLING.1 H02-AVAILABLE
TOKYO-BLING.1 H02-MIDDLING
TOKYO-BLING.1 H02-TOP

TOKYO-BLING.2 H04-USED
TOKYO-BLING.2 H04-AVAILABLE
TOKYO-BLING.2 H04-CANCELLED

WAY-VERING.2 H03-USED
WAY-VERING.2 H03-AVAILABLE

WAY-VERING.1 H03-TOP
WAY-VERING.1 H03-CANCELLED
</code></pre>

<p>Can anyone suggest an algorithm(or some method) that can scan through a given amount of text and work out that the text can be grouped as above. Obviously each group can be further. I guess i am looking for a good solution to looking at a list of phrases and working out how best to group them by some common string sequence.</p>
",Text Generation & LLMs,longest common sequence group given following line text would like parsing generate somewhat sensible grouping list grouped follows anyone suggest algorithm method scan given amount text work text grouped obviously group guess looking good solution looking list phrase working best group common string sequence
Predicting next char in &#39;random&#39; text generation based on some input file,"<p>I am writing a program that generates random text based on the Markov model. I am running into a problem, with some files that have a lot of spaces in between words, the initial seed is seen to be a space. <strong>The problem is that all the next characters are seen as spaces as well and so the random text that is generated is just a blank documents as nextChosenChar is always a space.</strong> </p>

<p>Can someone suggest some solution to this problem? </p>

<p>I tried to come up with a solution as seen the latter part of the code below, but to no avail. </p>

<pre><code>char ChooseNextChar(string seed, int order, string fileName){
    Map&lt;string, Vector&lt;char&gt; &gt; nextCharMap;
    ifstream inputStream;
    inputStream.open(fileName.c_str());
    int offset = 0;
    Vector&lt;char&gt; charsFollingSeedVector;
    inputStream.clear();
    char* buffer = new char [order + 1];
    char charFollowingSeed;
    static int consecutiveSpaces = 0;
    while (!inputStream.eof()) {    
        inputStream.seekg(offset);
        inputStream.read(buffer, order + 1);
        string key(buffer, order);
        if (equalsIgnoreCase(key, seed)) {
            //only insert key if not present otherwise overwriting old info 
            if (!nextCharMap.containsKey(seed)) {
                nextCharMap.put(seed, charsFollingSeedVector);
            }
            //read the char directly following seed
            charFollowingSeed = buffer[order];
            nextCharMap[seed].push_back(charFollowingSeed);
        }
        offset++;
    }
    //case where no chars following seed
    if (nextCharMap[seed].isEmpty()) {
        return EOF;
    }
    //determine which is the most frequent following char
    char nextChosenChar = MostFequentCharInVector(seed, nextCharMap);

    //TRYING TO FIX PROBLEM OF ONLY OUTPUTTING SPACES**********
     if (nextChosenChar == ' ') {
        consecutiveSpaces++;
        if (consecutiveSpaces &gt;= 1) {
            nextChosenChar = nextCharMap[seed].get(randomInteger(0, nextCharMap[seed].size()-1));
            consecutiveSpaces = 0;
        }
    }
    return nextChosenChar;
}
</code></pre>
",Text Generation & LLMs,predicting next char random text generation based input file writing program generates random text based markov model running problem file lot space word initial seed seen space problem next character seen space well random text generated blank document nextchosenchar always space someone suggest solution problem tried come solution seen latter part code avail
How to understand this formula in Lingpipe language model?,"<p><a href=""http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/NGramProcessLM.html"" rel=""nofollow noreferrer"">This is from manual of Lingpipe doc</a> in building a language model. But I only partly understand the theory behind it. </p>

<p>I especially do not know the base probability.</p>

<p><img src=""https://i.sstatic.net/e9Ud0.gif"" alt=""enter image description here""></p>

<p><img src=""https://i.sstatic.net/WMNyN.gif"" alt=""enter image description here""></p>

<p>Here, how to get base p(d). If below is portion of token and their freq in unigram file.</p>

<pre><code>ab  20
aba 3
abd 2
abef 2
abkk 3
</code></pre>

<p>Under such condition, what is lamda(),1-lamda(), extcount, numExtentions and Base P(ab)?
This is one question but they are chained. </p>

<p>Thanks a lot.</p>
",Text Generation & LLMs,understand formula lingpipe language model manual lingpipe doc building language model partly understand theory behind especially know base probability get base p portion token freq unigram file condition lamda lamda extcount numextentions base p ab one question chained thanks lot
Controlled Natural Language scheme for describing system architectures?,"<p>Are there any good Controlled Natural Language models, written in something like W3C Metalog PNL, for describing system architectures?</p>

<p>I'm looking for a text-based alternative to UML diagrams... to represent System Components, Relationships, Dependencies, Use Cases, Data Flows, etc. The goal is to have a model that can be programatically queried (unlike a diagram) to answer simple questions like, ""what other components connect to this component?"".</p>

<p>Thanks</p>
",Text Generation & LLMs,controlled natural language scheme describing system architecture good controlled natural language model written something like w c metalog pnl describing system architecture looking text based alternative uml diagram represent system component relationship dependency use case data flow etc goal model programatically queried unlike diagram answer simple question like component connect component thanks
How can I programmatically generate relevant tags for a database of URLs?,"<p>I'm writing an RSS reader in python as a learning exercise, and I would really like to be able to tag individual entries with keywords for searching.  Unfortunately, most real-world feeds don't include keyword metadata.  I currently have about 60,000 entries in my test database from about 600 feeds, so manually tagging is not going to be effective.  So far I have only been able to find two solutions:</p>

<p><strong>1: Use <a href=""http://code.google.com/p/nltk/"" rel=""noreferrer"">Natural Language Toolkit</a> to extract keywords:</strong></p>

<ul>
<li>Pros: flexible; no dependencies on external services;</li>
<li>Cons: can only index the article summary, not the article; non-trivial: writing a high quality keyword extraction tool is a project in itself;</li>
</ul>

<p><strong>2: Use the <a href=""http://code.google.com/apis/adwords/docs/reference/latest/TargetingIdeaService.RelatedToUrlSearchParameter.html"" rel=""noreferrer"">Google Adwords API</a> to fetch keyword suggestions from the article url:</strong></p>

<ul>
<li>Pros: Super high quality keywords; based on entire article text; easy to use;</li>
<li>Cons: Not free(?); Query rate limits unknown; I'm terrified of getting my account banned and not being able to run adwords campaigns for my commercial sites;</li>
</ul>

<p>Can anyone offer any suggestions?  Are my fears about getting my adwords account banned unfounded?</p>
",Text Generation & LLMs,programmatically generate relevant tag database url writing r reader python learning exercise would really like able tag individual entry keywords searching unfortunately real world feed include keyword metadata currently entry test database feed manually tagging going effective far able find two solution use natural language toolkit extract keywords pro flexible dependency external service con index article summary article non trivial writing high quality keyword extraction tool project use google adwords api fetch keyword suggestion article url pro super high quality keywords based entire article text easy use con free query rate limit unknown terrified getting account banned able run adwords campaign commercial site anyone offer suggestion fear getting adwords account banned unfounded
What&#39;s the best way to generate keywords from a given Text?,"<p>I want to generate Keywords for my CMS.</p>

<p>Does someone know a good PHP Script (or something else) which generates keywords?</p>

<p>I have a HTML Site like this: <a href=""http://pastebin.com/ZU8vdyeP"" rel=""nofollow"">http://pastebin.com/ZU8vdyeP</a> </p>
",Text Generation & LLMs,best way generate keywords given text want generate keywords cm doe someone know good php script something else generates keywords html site like
Build a natural language model that fixes misspellings,"<p>What are books about how to build a natural language parsing program like this:</p>

<pre>
input: I got to TALL you
output: I got to TELL you

input: Big RAT box
output: Big RED box

in: hoo un thum zend three
out: one thousand three

</pre>

<p>It must have the language model that allows to predict what words are misspelled !</p>

<p>What are the best books on how to build such a tool??</p>

<p>p.s. Are there free webservices to spell-check? From Google maybe?..</p>
",Text Generation & LLMs,build natural language model fix misspelling book build natural language parsing program like input got tall output got tell input big rat box output big red box hoo un thum zend three one thousand three must language model allows predict word misspelled best book build tool p free webservices spell check google maybe
