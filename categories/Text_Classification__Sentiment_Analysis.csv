Title,Description,category,combined_text
Zero Accuracy in Sentiment Analysis FFNN Model (Pytorch),"<p>I'm constructing a feedforward neural net (FFNN) for binary sentiment classification using movie review dataset. It's a simplified task with only two class labels, i.e., positive (1) and negative (0). It FFNN makes word embeddings, averages them, runs them through two layers and activation later, and then applies a sigmoid too get a probability for prediction.</p>
<p>The model runs, but I keep getting zeroes. I am in desperate need of help.
<a href=""https://colab.research.google.com/drive/1nbK_G7snlw0Jxtxzv5ro_qPOHBvDnsfV?usp=sharing"" rel=""nofollow noreferrer"">Here's the link to the colab</a></p>
<p>I tried the following:</p>
<ul>
<li>Changing the loss function</li>
<li>Rearranging the model order</li>
<li>Changing the embedding dimensions</li>
</ul>
<h1>Below is the following:</h1>
<ol>
<li>The model</li>
<li>The assignment of the FFNN</li>
<li>The training</li>
</ol>
<h2>Here's the code:</h2>
<h3><em><strong>THE MODEL</strong></em></h3>
<pre class=""lang-py prettyprint-override""><code>from collections import OrderedDict

import torch
from typing import List

import torch.nn as nn

class FeedForwardNeuralNetClassifier(nn.Module):
    &quot;&quot;&quot;
    The Feed-Forward Neural Net sentiment classifier.
    &quot;&quot;&quot;
    def __init__(self, vocab_size, emb_dim, n_hidden_units):
        &quot;&quot;&quot;
        In the __init__ function, you will define modules in FFNN.
        :param vocab_size: size of vocabulary
        :param emb_dim: dimension of the embedding vectors
        :param n_hidden_units: dimension of the hidden units
        &quot;&quot;&quot;
        super(FeedForwardNeuralNetClassifier, self).__init__()
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.n_hidden_units = n_hidden_units
       
        # TODO: implement a randomly initialized word embedding matrix using nn.Embedding
        # It should have a size of (vocab_size x emb_dim)
        self.word_embeddings = nn.Embedding(self.vocab_size, self.emb_dim) # replace me

        # Define the rest of the model
        self.layer1 = nn.Linear(self.emb_dim, self.n_hidden_units)
        self.layer2 = nn.Linear(self.n_hidden_units, 1)
        self.activation = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, batch_inputs: torch.Tensor, batch_lengths: torch.Tensor) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        The forward function, which defines how FFNN should work when given a batch of inputs and their actual sent lengths (i.e., before PAD)
        :param batch_inputs: a torch.Tensor object of size (n_examples, max_sent_length_in_this_batch), which is the *indexed* inputs
        :param batch_lengths: a torch.Tensor object of size (n_examples), which describes the actual sentence length of each example (i.e., before PAD)
        :return the logits of FFNN (i.e., the unnormalized hidden units before sigmoid) of shape (n_examples)
        &quot;&quot;&quot;

        # Lookup word embeddings
        embeddings = self.word_embeddings(batch_inputs)

        # Element-wise averaging layer
        averaged = torch.mean(embeddings, dim=1)

        # Hidden layer ReLU
        hidden = self.activation(self.layer1(averaged))

        # Output layer
        return self.layer2(hidden)

        

    def batch_predict(self, batch_inputs: torch.Tensor, batch_lengths: torch.Tensor) -&gt; List[int]:
        &quot;&quot;&quot;
        Make predictions for a batch of inputs. This function may directly invoke forward (which passes the input through FFNN and returns the output logits)

        :param batch_inputs: a torch.Tensor object of size (n_examples, max_sent_length_in_this_batch), which is the *indexed* inputs
        :param batch_lengths: a torch.Tensor object of size (n_examples), which describes the actual sentence length of each example (i.e., before PAD)
        :return: a list of predicted classes for this batch of data, either 0 for negative class or 1 for positive class
        &quot;&quot;&quot;
        output = self.forward(batch_inputs, batch_lengths)
         
        # Sigmoid
        probability = self.sigmoid(output)

        # Convert logits to predicted labels
        predicted_labels = (probability &gt; 0.5).int().tolist()

        return predicted_labels
</code></pre>
<h3><strong>THE ASSIGNMENT</strong></h3>
<pre class=""lang-py prettyprint-override""><code>
model = FeedForwardNeuralNetClassifier(vocab_size=len(vocab), emb_dim=300, n_hidden_units=300)

# FeedForwardNeuralNetClassifier(
# (word_embeddings): Embedding(4818, 300)
# (layer1): Linear(in_features=300, out_features=300, bias=True)
# (layer2): Linear(in_features=300, out_features=1, bias=True)
#  (activation): ReLU()
#  (sigmoid): Sigmoid()
# )



device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(device)
model = model.to(device)



optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # replace me
</code></pre>
<h3><strong>THE TRAINING</strong></h3>
<pre class=""lang-py prettyprint-override""><code>
import time

BATCH_SIZE=32
N_EPOCHS=20

# create a batch iterator for the training data
batch_iterator = SentimentExampleBatchIterator(
    train_exs, batch_size=BATCH_SIZE, PAD_idx=PAD_IDX, shuffle=True)

#loss function
loss_func = nn.BCEWithLogitsLoss()

# training
best_epoch = -1
best_acc = -1
start_time = time.time()
for epoch in range(N_EPOCHS):
    print(&quot;Epoch %i&quot; % epoch)

    batch_iterator.refresh() # initiate a new iterator for this epoch

    model.train() # turn on the &quot;training mode&quot;
    batch_loss = 0.0
    batch_example_count = 0
    batch_data = batch_iterator.get_next_batch()
    while batch_data is not None:
        batch_inputs, batch_lengths, batch_labels = batch_data
        # project to the device
        batch_inputs = batch_inputs.to(device)
        batch_lengths = batch_lengths.to(device)
        batch_labels = batch_labels.unsqueeze(1)
        batch_labels = batch_labels.to(device)
        

        
        # TODO: clean up the gradients for this batch
        optimizer.zero_grad()
        
        # TODO: call the model and get the loss
        outputs = model(batch_inputs, batch_lengths).float()
        
        batch_labels = batch_labels.float()
        loss = loss_func(outputs, batch_labels)
        # record the loss and number of examples, so we could report some stats
        batch_example_count += len(batch_labels)
        batch_loss += loss.item() * len(batch_labels)

        # TODO: backpropagation using loss
        # backpropagation using loss
        loss.backward()

        # update the model parameters
        optimizer.step()

        # get another batch
        batch_data = batch_iterator.get_next_batch()

    print(&quot;Avg loss: %.5f&quot; % (batch_loss / batch_example_count))

    # evaluate on dev set
    model.eval() # turn on the &quot;evaluation mode&quot;
    acc, _, _, _ = evaluate(model, dev_exs, return_metrics=True)
    if acc &gt; best_acc:
        best_acc = acc
        best_epoch = epoch
        print(&quot;Secure a new best accuracy %.3f in epoch %d!&quot; % (best_acc, best_epoch))
        
        # Save the current best model parameters
        print(&quot;Save the best model checkpoint as best_model.ckpt!&quot;)
        torch.save(model.state_dict(), &quot;best_model.ckpt&quot;)
    
    print(&quot;Time elapsed: %s&quot; % time.strftime(&quot;%Hh%Mm%Ss&quot;, time.gmtime(time.time()-start_time)))
    print(&quot;-&quot; * 10)

print(&quot;End of training! The best accuracy %.3f was obtained in epoch %d.&quot; % (best_acc, best_epoch))

# Load back the best checkpoint on dev set
model.load_state_dict(torch.load(&quot;best_model.ckpt&quot;))


</code></pre>
",Text Classification / Sentiment Analysis,zero accuracy sentiment analysis ffnn model pytorch constructing feedforward neural net ffnn binary sentiment classification using movie review dataset simplified task two class label e positive negative ffnn make word embeddings average run two layer activation later applies sigmoid get probability prediction model run keep getting zero desperate need help link colab tried following changing loss function rearranging model order changing embedding dimension following model assignment ffnn training code model assignment training
sentiment analysis for tweets,"<pre><code>tweets = [
    &quot;Wow, what a great day today!! #sunshine&quot;,
    &quot;I feel sad about the things going on around us. #covid19&quot;,
    &quot;I'm really excited to learn Python with @JovianML #zerotopandas&quot;,
    &quot;This is a really nice song. #linkinpark&quot;,
    &quot;The python programming language is useful for data science&quot;,
    &quot;Why do bad things happen to me?&quot;,
    &quot;Apple announces the release of the new iPhone 12. Fans are excited.&quot;,
    &quot;Spent my day with family!! #happy&quot;,
    &quot;Check out my blog post on common string operations in Python. #zerotopandas&quot;,
    &quot;Freecodecamp has great coding tutorials. #skillup&quot;
]


happy_words = ['great', 'excited', 'happy', 'nice', 'wonderful', 'amazing', 'good', 'best']

sad_words = ['sad', 'bad', 'tragic', 'unhappy', 'worst']
</code></pre>
<p>I am not able to find the neutral tweets using loops, can anyone help me out?</p>
",Text Classification / Sentiment Analysis,sentiment analysis tweet able find neutral tweet using loop anyone help
How is polarity calculated for a sentence in sentiment analysis?,"<p>How is polarity of words in a statement are calculated? Like</p>
<pre><code>&quot;i am successful in accomplishing the task,but in vain&quot; 
</code></pre>
<p>how each word is scored? (like - successful- 0.7 accomplishing- 0.8 but - -0.5
vain - - 0.8)</p>
<p>how is it calculated? how is each word given a value or score? what is the thing that's going on behind?</p>
",Text Classification / Sentiment Analysis,polarity calculated sentence sentiment analysis polarity word statement calculated like word scored like successful accomplishing vain calculated word given value score thing going behind
Text classification in python - (NLTK Sentence based),"<p>I need to classify text and i am using Text blob python module to achieve it.I can use either Naive Bayes classifier/Decision tree. I am concern about the below mentioned points.</p>
<ol>
<li><p>I Need to classify <strong>sentences</strong> as argument/ Not an argument. I am using two classifiers and training the model using apt data sets. My question is all about do i need to train the model with only keywords? or i can train the data set with all possible argument and non argument <strong>sample sentences</strong>? Which would be the best approach in terms of text classification accuracy and time to retrieve?</p>
</li>
<li><p>Since the classification would be either argument/not an argument, which classifier would fetch exact results? It is Naive Bayes /Decision tree/Positive Naive bayes?</p>
</li>
</ol>
",Text Classification / Sentiment Analysis,text classification python nltk sentence based need classify text using text blob python module achieve use either naive bayes classifier decision tree concern mentioned point need classify sentence argument argument using two classifier training model using apt data set question need train model keywords train data set possible argument non argument sample sentence would best approach term text classification accuracy time retrieve since classification would either argument argument classifier would fetch exact result naive bayes decision tree positive naive bayes
How can I use BERT for long text classification?,"<p>We know that BERT has a maximum length limit of tokens = 512. So if an article has a length of much bigger than 512, such as 10000 tokens in text, how can BERT be used?</p>
",Text Classification / Sentiment Analysis,use bert long text classification know bert ha maximum length limit token article ha length much bigger token text bert used
Using WN-Affect to detect emotion/mood of a string,"<p>I  downloaded <a href=""http://wndomains.fbk.eu/wnaffect.html"" rel=""noreferrer"">WN-Affect</a>. I am however not sure how to use it to detect the mood of a sentence. For example if I have a string ""I hate football."" I want to be able to detect whether the mood is bad and the emotion is fear. WN-Affect has no tutorial on how to do it, and I am kind of new to python. Any help would be great!</p>
",Text Classification / Sentiment Analysis,using wn affect detect emotion mood string downloaded wn affect however sure use detect mood sentence example string hate football want able detect whether mood bad emotion fear wn affect ha tutorial kind new python help would great
Pyspark sentiment analysis invalid output,"<p>I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy?</p>
<p>My code</p>
<pre><code>from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from transformers import pipeline
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Define the filter_stopwords function
def filter_stopwords(sentence):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(sentence)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    return &quot; &quot;.join(filtered_sentence)

# Initialize the sentiment analysis pipeline with a different model
sentiment_pipeline = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)

# Define a function to get sentiment using the pipeline
def get_sentiment(text):
    filtered_text = filter_stopwords(text)
    result = sentiment_pipeline(filtered_text)[0]
    return result['label'].lower()  # returns 'positive', 'negative', etc.

# Register the function as a UDF
sentiment_udf = udf(get_sentiment, StringType())

# df = df.withColumn(&quot;sentiment&quot;, sentiment_udf(col(&quot;text_column&quot;)))
</code></pre>
<p>Input data</p>
<ol>
<li><p>Didn't get it right the first 2 times but when it was fixed it was fixed well.</p>
</li>
<li><p>The Response time was grate -- <strong>Note</strong> Looks like spelling mistake but his review is positive</p>
</li>
<li><p>The initial agent contact could not resolve my issue but escalated it quickly to someone who could.</p>
</li>
</ol>
<p>for this inputs i am expecting all should be <strong>positive</strong> instead i am getting <strong>negative</strong></p>
",Text Classification / Sentiment Analysis,pyspark sentiment analysis invalid output trying perform sentiment analysis use case time giving correct result case even positive comment marked negative fix code achieve better accuracy code input data get right first time wa fixed wa fixed well response time wa grate note look like spelling mistake review positive initial agent contact could resolve issue escalated quickly someone could input expecting positive instead getting negative
"Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing","<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>
<p><a href=""https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment"" rel=""nofollow noreferrer"">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>
<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>
<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import pipeline
import torch

model_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;
model = RobertaForSequenceClassification.from_pretrained(model_name)
tokenizer = RobertaTokenizer.from_pretrained(model_name)

classifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)

result1 = classifier('Fhdf')
result2 = classifier('Als gisteren inclusief blauw')
print(result1)
print(result2)
</code></pre>
<p>Output:</p>
<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]
[{'label': 'Negative', 'score': 0.7538396120071411}]
</code></pre>
",Text Classification / Sentiment Analysis,dutch sentiment analysis robbertje output positive negative label netural label missing run dutch sentiment analysis robbertje output positive negative label netural label missing data obvious neutral sentence word e g fhdf nonsense al gisteren inclusief blauw neutral evaluate positive negative way get neutral label example robbertje output
How to Log Custom Metrics with Metadata in Hugging Face Trainer during Evaluation?,"<p>I'm working on a sentence regression task using Hugging Face’s Trainer. Each sample consists of:</p>
<p>input_ids: The tokenized sentence.
labels: A numerical scalar target (for regression).
metadata: A categorical string field (e.g., project_name or task_type).
My Goal:
I want to log custom metrics with respect to the metadata during evaluation. Specifically, I need to calculate the loss grouped by each metadata category (e.g., loss per project or task). the main loss for gradient computation is still the global loss but i want to pass the metadata to the compute_metrics function</p>
<p>What I’ve Tried:
Modified the Dataset:
My dataset returns a dictionary containing:</p>
<pre><code>{
    &quot;input_ids&quot;: input_ids,
    &quot;labels&quot;: torch.tensor([numerical_score]),  # Scalar target
    &quot;metadata&quot;: project_name  # Metadata field
}
</code></pre>
<p>Updated the Collator:
I ensured that all elements (including metadata) are passed correctly to the model.</p>
<p>Trainer Setup:
I enabled include_inputs_for_metrics=True to access inputs in compute_metrics:</p>
<pre><code>Trainer(..., compute_metrics=compute_metrics, args=TrainingArguments(include_inputs_for_metrics=True))
</code></pre>
<p>However, metadata still gets lost in the evaluation loop, and I can’t access it directly inside compute_metrics.</p>
<p>The Problem:
The compute_metrics function only receives predictions and labels, and not the additional metadata. I need a way to log metrics with respect to metadata without completely overriding the Trainer class or the entire evaluation loop, as this would be cumbersome to maintain, especially in parallel runs.</p>
<p>What I Want to Achieve:
Log metrics grouped by metadata categories.
For example, calculate the loss per project or task.
Minimize complex modifications to the Hugging Face Trainer class.
Ensure the solution works with the default Trainer evaluation loop to maintain scalability and parallelism.</p>
<p>Potential Solutions Explored:
i tried to make a metadata string, encode it using the tokenizer and pass it along as metadata or as labels, but it causes an infinite hang which i suspect is related to the gather function not expecting something about the shape.</p>
<p>Is there a clean and efficient way to pass metadata through the evaluation loop and use it in compute_metrics? How can I log metrics grouped by metadata without having to rewrite the Trainer class or evaluation loop entirely?</p>
<p>Any suggestions or best practices would be greatly appreciated!</p>
",Text Classification / Sentiment Analysis,log custom metric metadata hugging face trainer evaluation working sentence regression task using hugging face trainer sample consists input id tokenized sentence label numerical scalar target regression metadata categorical string field e g project name task type goal want log custom metric respect metadata evaluation specifically need calculate loss grouped metadata category e g loss per project task main loss gradient computation still global loss want pas metadata compute metric function tried modified dataset dataset return dictionary containing updated collator ensured element including metadata passed correctly model trainer setup enabled include input metric true access input compute metric however metadata still get lost evaluation loop access directly inside compute metric problem compute metric function receives prediction label additional metadata need way log metric respect metadata without completely overriding trainer class entire evaluation loop would cumbersome maintain especially parallel run want achieve log metric grouped metadata category example calculate loss per project task minimize complex modification hugging face trainer class ensure solution work default trainer evaluation loop maintain scalability parallelism potential solution explored tried make metadata string encode using tokenizer pas along metadata label cause infinite hang suspect related gather function expecting something shape clean efficient way pas metadata evaluation loop use compute metric log metric grouped metadata without rewrite trainer class evaluation loop entirely suggestion best practice would greatly appreciated
"How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?","<p>I'm working in a sentiment analysis problem the data looks like this:</p>
<pre><code>label instances
    5    1190
    4     838
    3     239
    1     204
    2     127
</code></pre>
<p>So my data is unbalanced since 1190 <code>instances</code> are labeled with <code>5</code>. For the classification Im using scikit's <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"" rel=""nofollow noreferrer"">SVC</a>. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches:</p>
<p>First:</p>
<pre><code>wclf = SVC(kernel='linear', C= 1, class_weight={1: 10})
wclf.fit(X, y)
weighted_prediction = wclf.predict(X_test)

print 'Accuracy:', accuracy_score(y_test, weighted_prediction)
print 'F1 score:', f1_score(y_test, weighted_prediction,average='weighted')
print 'Recall:', recall_score(y_test, weighted_prediction,
                              average='weighted')
print 'Precision:', precision_score(y_test, weighted_prediction,
                                    average='weighted')
print '\n clasification report:\n', classification_report(y_test, weighted_prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, weighted_prediction)
</code></pre>
<p>Second:</p>
<pre><code>auto_wclf = SVC(kernel='linear', C= 1, class_weight='auto')
auto_wclf.fit(X, y)
auto_weighted_prediction = auto_wclf.predict(X_test)

print 'Accuracy:', accuracy_score(y_test, auto_weighted_prediction)

print 'F1 score:', f1_score(y_test, auto_weighted_prediction,
                            average='weighted')

print 'Recall:', recall_score(y_test, auto_weighted_prediction,
                              average='weighted')

print 'Precision:', precision_score(y_test, auto_weighted_prediction,
                                    average='weighted')

print '\n clasification report:\n', classification_report(y_test,auto_weighted_prediction)

print '\n confussion matrix:\n',confusion_matrix(y_test, auto_weighted_prediction)
</code></pre>
<p>Third:</p>
<pre><code>clf = SVC(kernel='linear', C= 1)
clf.fit(X, y)
prediction = clf.predict(X_test)


from sklearn.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, \
    accuracy_score, f1_score

print 'Accuracy:', accuracy_score(y_test, prediction)
print 'F1 score:', f1_score(y_test, prediction)
print 'Recall:', recall_score(y_test, prediction)
print 'Precision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test,prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)


F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.
  sample_weight=sample_weight)
 0.930416613529
</code></pre>
<p>However, Im getting warnings like this:</p>
<pre><code>/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172:
DeprecationWarning: The default `weighted` averaging is deprecated,
and from version 0.18, use of precision, recall or F-score with 
multiclass or multilabel data or pos_label=None will result in an 
exception. Please set an explicit value for `average`, one of (None, 
'micro', 'macro', 'weighted', 'samples'). In cross validation use, for 
instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;
</code></pre>
<p>How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?</p>
",Text Classification / Sentiment Analysis,compute precision recall accuracy f score multiclass case scikit learn working sentiment analysis problem data look like data unbalanced since labeled classification im using scikit svc problem know balance data right way order compute accurately precision recall accuracy f score multiclass case tried following approach first second third however im getting warning like deal correctly unbalanced data order compute right way classifier metric
How to make t-sne plots with BERT Models,"<p><a href=""https://i.sstatic.net/dzZVH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dzZVH.png"" alt=""enter image description here"" /></a></p>
<p>I have two text classification models
<a href=""https://huggingface.co/samanjoy2/banglaclickbert_finetuned_sequence_classification_clickbait"" rel=""nofollow noreferrer"">https://huggingface.co/samanjoy2/banglaclickbert_finetuned_sequence_classification_clickbait</a>
<a href=""https://huggingface.co/samanjoy2/banglabert_finetuned_sequence_classification_clickbait"" rel=""nofollow noreferrer"">https://huggingface.co/samanjoy2/banglabert_finetuned_sequence_classification_clickbait</a></p>
<p>I want to plot their hidden states in a T-SNE plor. How can I do this? A example of this has been shown in the picture.</p>
",Text Classification / Sentiment Analysis,make sne plot bert model two text classification model want plot hidden state sne plor example ha shown picture
How do RNNs handle negation in sentiment analysis,"<p>I'm trying to understand how Recurrent Neural Networks (RNNs), such as LSTM or GRU models or just a simple RNN, handle negation in sentiment analysis. Specifically, I'm curious about how these models manage to correctly interpret sentences where negation changes the sentiment, such as &quot;The movie is not good.&quot;</p>
<p>A simple model using word embeddings + simple averaging fails to handle negation properly. For example, if &quot;good&quot; has a positive sentiment score and &quot;bad&quot; has a negative sentiment score, a model might misinterpret &quot;not good&quot; by simply averaging the scores of &quot;not&quot; and &quot;good&quot;.</p>
<h3>Example Without Negation</h3>
<p>Consider the following sentences with sentiment words:</p>
<ul>
<li>&quot;The movie is good.&quot;</li>
<li>&quot;The movie is awesome.&quot;</li>
<li>&quot;The movie is terrible.&quot;</li>
</ul>
<p>Suppose we have the following word embeddings representing sentiment scores:</p>
<ul>
<li>&quot;good&quot; = [10]</li>
<li>&quot;awesome&quot; = [12]</li>
<li>&quot;terrible&quot; = [-10]</li>
</ul>
<p>Neutral words (assuming embeddings around 0):</p>
<ul>
<li>&quot;the&quot; = [0]</li>
<li>&quot;movie&quot; = [0]</li>
<li>&quot;is&quot; = [0]</li>
</ul>
<p>For these sentences, a simple global average of the sentiment scores works well:</p>
<ul>
<li>&quot;The movie is good&quot; = average([0, 0, 0, 10]) = 10 / 4 = 2.5 (positive sentiment)</li>
<li>&quot;The movie is awesome&quot; = average([0, 0, 0, 12]) = 12 / 4 = 3 (positive sentiment)</li>
<li>&quot;The movie is terrible&quot; = average([0, 0, 0, -10]) = -10 / 4 = -2.5 (negative sentiment)</li>
</ul>
<h3>Example With Negation</h3>
<p>Now, consider sentences with negation:</p>
<ul>
<li>&quot;The movie is not good.&quot;</li>
<li>&quot;The movie is not bad.&quot;</li>
</ul>
<p>For the embeddings:</p>
<ul>
<li>&quot;not&quot; = [-5]</li>
<li>&quot;good&quot; = [10]</li>
<li>&quot;bad&quot; = [-9]</li>
</ul>
<p>Simple averaging might fail to handle these correctly:</p>
<ul>
<li>&quot;The movie is not good&quot; = average([0, 0, 0, -5, 10]) = (0 + 0 + 0 - 5 + 10) / 5 = 5 / 5 = 1 (incorrectly positive)</li>
<li>&quot;The movie is not bad&quot; = average([0, 0, 0, -5, -9]) = (0 + 0 + 0 - 5 - 9) / 5 = -14 / 5 = -2.8 (incorrectly negative)</li>
</ul>
<h3>How RNNs Handle Negation</h3>
<p>Intuitively, an RNN should be able to handle this because it is designed to model sequences of words, capturing the dependencies between them over time.</p>
<p>Can someone explain, with a concrete example, how an RNN model like an LSTM or GRU can correctly understand and model negation in a sentence? Specifically, I'm interested in:</p>
<ul>
<li>How the recurrent connections and memory cells capture the relationship between words like &quot;not&quot; and &quot;good&quot; or &quot;not&quot; and &quot;bad&quot;.</li>
<li>An example with numerical values to illustrate the process.</li>
</ul>
",Text Classification / Sentiment Analysis,rnns handle negation sentiment analysis trying understand recurrent neural network rnns lstm gru model simple rnn handle negation sentiment analysis specifically curious model manage correctly interpret sentence negation change sentiment movie good simple model using word embeddings simple averaging fails handle negation properly example good ha positive sentiment score bad ha negative sentiment score model might misinterpret good simply averaging score good example without negation consider following sentence sentiment word movie good movie awesome movie terrible suppose following word embeddings representing sentiment score good awesome terrible neutral word assuming embeddings around movie sentence simple global average sentiment score work well movie good average positive sentiment movie awesome average positive sentiment movie terrible average negative sentiment example negation consider sentence negation movie good movie bad embeddings good bad simple averaging might fail handle correctly movie good average incorrectly positive movie bad average incorrectly negative rnns handle negation intuitively rnn able handle designed model sequence word capturing dependency time someone explain concrete example rnn model like lstm gru correctly understand model negation sentence specifically interested recurrent connection memory cell capture relationship word like good bad example numerical value illustrate process
How to continue training a model from where it left off?,"<p>I would like to know how I can save checkpoints when training a text classification model so that I can continue training from where it left off.</p>
<p>I’m having trouble and don’t know how to configure my code to save the checkpoints with the appropriate files so that I can continue training at the point where it ended previously, such as “trainer_state.json”.</p>
<p>Here is my training code:</p>
<pre><code>def executar_treinamento(self, base_treinada):
    training_args = TrainingArguments(
        output_dir=self.output_dir,
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=8,
        weight_decay=0.01,
        evaluation_strategy=&quot;epoch&quot;,
        save_strategy=&quot;epoch&quot;,
        save_only_model=False,
        load_best_model_at_end=False
    )

    self.trainer = Trainer(
        model=self.model,
        args=training_args,
        compute_metrics=self.calcular_metricas,
        train_dataset=base_treinada['train'],
        eval_dataset=base_treinada['validation'],
        tokenizer=self.tokenizar_textos
    )
    
    self.trainer.train()

def avaliar_modelo(self, base_treinada):
    self.trainer.evaluate(base_treinada['test'])

def salvar_modelo(self, caminho):
    self.model.save_pretrained(caminho)
    self.tokenizer.save_pretrained(caminho)
</code></pre>
<p>I tried using the following parameters:</p>
<pre><code>            save_strategy=&quot;steps&quot;,       # Save checkpoints every X steps
            save_steps=80,              # Customize how often to save (in steps)
</code></pre>
<p>However, even so, checkpoints with files like &quot;trainer_state.json&quot; were not saved.</p>
",Text Classification / Sentiment Analysis,continue training model left would like know save checkpoint training text classification model continue training left trouble know configure code save checkpoint appropriate file continue training point ended previously trainer state json training code tried using following parameter however even checkpoint file like trainer state json saved
Keep training pytorch model on new data,"<p>I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:</p>
<ol>
<li>Load and process the text.</li>
<li>Use a TF-IDF Vectorizer.</li>
<li>Build the neural network and save the TF-IDF Vectorizer and model to predict new data.</li>
</ol>
<p>However, every day I need to classify new comments and correct any wrong classifications.</p>
<p>Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).</p>
<p>Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.</p>
<p>So the mains questions are:</p>
<ol>
<li>How could i check if the propossed way to solve this problem work as i expect?</li>
<li>What can i do with the vectorizer when it face new tokens, can i just do a <code>.fit_transform()</code> or i would loose the original vectorizer?</li>
</ol>
<p>Here its the full training process:</p>
<pre><code>import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import LabelEncoder
import polars as pl
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

set1 = (
    pl
    .read_csv(
        &quot;set1.txt&quot;,
        separator=&quot;;&quot;,
        has_header=False,
        new_columns=[&quot;text&quot;,&quot;label&quot;]
    )
)

# since the dateset its unbalanced, im going to force to have more balance

fear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)
joy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)
sadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)
anger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)

train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])

&quot;&quot;&quot;
The text its already clean, so im going to change the labels to numeric
and then split it on train, test ,val
&quot;&quot;&quot;

label_mapping = {
    &quot;anger&quot;: 0,
    &quot;fear&quot;: 1,
    &quot;joy&quot;: 2,
    &quot;sadness&quot;: 3
}

train_mapped = (
    train_df
    .with_columns(
        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)
    )
   
)

train_set, pre_Test = train_test_split(train_mapped,
                                    test_size=0.4,
                                    random_state=42,
                                    stratify=train_mapped[&quot;label&quot;])

test_set, val_set = train_test_split(pre_Test,
                                    test_size=0.5,
                                    random_state=42,
                                    stratify=pre_Test[&quot;label&quot;]) 

# Vectorize text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))

X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()
X_val_tfidf = vectorizer.transform(val_set['text']).toarray()
X_test_tfidf = vectorizer.transform(test_set['text']).toarray()

y_train = train_set['label']
y_val = val_set['label']
y_test = test_set['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label
    
train_dataset = TextDataset(X_train_tfidf, y_train)
val_dataset = TextDataset(X_val_tfidf, y_val)
test_dataset = TextDataset(X_test_tfidf, y_test)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
input_dim = X_train_tfidf.shape[1]
model = TextClassificationModel(input_dim, 4)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

# Training loop
num_epochs = 17
best_val_acc = 0.0
best_model_path = &quot;modelbest.pth&quot;

for epoch in range(num_epochs):
    model.train()
    for texts, labels in train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for texts, labels in val_loader:
            texts, labels = texts.float(), labels.long()
            outputs = model(texts)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    val_acc = correct / total
    if val_acc &gt; best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), best_model_path)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Test the model
model.eval()
correct, total = 0, 0
with torch.no_grad():
    for texts, labels in test_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
test_acc = correct / total
print(f'Test Acc: {test_acc:.3f}')


# Save the TF-IDF vectorizer
vectorizer_path = &quot;tfidf_vectorizer.pkl&quot;
joblib.dump(vectorizer, vectorizer_path)

# Save the PyTorch model
model_path = &quot;text_classification_model.pth&quot;
torch.save(model.state_dict(), model_path)

</code></pre>
<p>Proposed code:</p>
<pre><code>import torch
import joblib
import polars as pl
from sklearn.model_selection import train_test_split
from torch import nn
from torch.utils.data import Dataset, DataLoader

# Load the saved TF-IDF vectorizer
vectorizer_path = &quot;tfidf_vectorizer.pkl&quot;
vectorizer = joblib.load(vectorizer_path)

input_dim = len(vectorizer.get_feature_names_out())

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
# Load the saved PyTorch model
model_path = &quot;text_classification_model.pth&quot;
model = TextClassificationModel(input_dim, 4)
model.load_state_dict(torch.load(model_path))

# Map labels to numeric values
label_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}
sentiments = [&quot;fear&quot;,&quot;joy&quot;,&quot;sadness&quot;,&quot;anger&quot;]

new_data = (
    pl
    .read_csv(
        &quot;set2.txt&quot;,
        separator=&quot;;&quot;,
        has_header=False,
        new_columns=[&quot;text&quot;,&quot;label&quot;]
    )
    .filter(pl.col(&quot;label&quot;).is_in(sentiments))
    .with_columns(
        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)
    )
    
)
# Vectorize the new text data using the loaded TF-IDF vectorizer
X_new = vectorizer.transform(new_data['text']).toarray()
y_new = new_data['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label

batch_size = 10
   
# Create DataLoader for the new training data
new_train_dataset = TextDataset(X_new, y_new)
new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

num_epochs = 5
new_best_model_path = &quot;modelbest.pth&quot;
for epoch in range(num_epochs):
    model.train()
    for texts, labels in new_train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        torch.save(model.state_dict(), new_best_model_path)
        
print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Save the PyTorch model
new_best_model_path = &quot;new_moedl.pth&quot;
torch.save(model.state_dict(), new_best_model_path)
</code></pre>
<p>The dataset can be found <a href=""https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp"" rel=""nofollow noreferrer"">here</a></p>
",Text Classification / Sentiment Analysis,keep training pytorch model new data working text classification task decided use pytorch model purpose process mainly involves following step load process text use tf idf vectorizer build neural network save tf idf vectorizer model predict new data however every day need classify new comment correct wrong classification currently approach add new comment correct classification dataset retrain entire model process time consuming new comment lost validation would like create new dataset newly classified text continue training new data new comment classified manually label correct using gpt online code write desired process however im sure working expected im making silly mistake happen main question could check propossed way solve problem work expect vectorizer face new token would loose original vectorizer full training process proposed code dataset found
Capitalized words in sentiment analysis,"<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .
A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>
<p>this is my current code :</p>
<pre><code>from itertools import chain

def is_upper_case(text):
  return [word for word in text.split() if word.isupper() and word != 'I']

unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))
print(unique_upper_words)
</code></pre>
",Text Classification / Sentiment Analysis,capitalized word sentiment analysis currently working data customer review product sephora task classify sentiment negative neutral positive common technique text preprocessing lower case word situation upper case word like amazing hide significant emotion behind turning word lower case cause information loss would happy opinion subject still lower case word personally think creating class distinction sentiment good good positive include importance upper case word current code
CPU Memory Leak While Inference Models in Infinite Loop,"<p>I'm experiencing a CPU memory leak while running a Python script that processes text using various NLP models in an infinite loop. The script includes language translation, sentiment analysis, and topic classification. Here's a simplified version of the problematic code:</p>
<pre><code>import ctranslate2 
import torch
import torch.nn.functional as F
from transformers import  AutoTokenizer, AutoModelForSequenceClassification , DistilBertForSequenceClassification, DistilBertTokenizer
import spacy
spacy.require_gpu()
ner_model = spacy.load('ner_model_path', disable=[&quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;]) 

OTHER_LANG_DICT = {'hi': 'nllb_hi'}
LANGUAGE_MODEL = ctranslate2.Translator('nllb-200-3.3B-int8',  device=&quot;cuda&quot;)

def convertToEng(input_data):
    try:
        for k,v in input_data.items():
            text = v['text']
            lang = v['lang']
        if lang and lang == 'en':
                translated_text = text
        if lang and lang in OTHER_LANG_DICT:
            tokenizer = AutoTokenizer.from_pretrained(LANGUAGE_MODEL, src_lang=OTHER_LANG_DICT[lang])
            tokens = tokenizer.encode(text, return_tensors=&quot;pt&quot;)
            tokens_list = tokenizer.convert_ids_to_tokens(tokens[0])
            results = LANGUAGE_MODEL.translate_batch([tokens_list], target_prefix=[[&quot;eng_Latn&quot;]])
            target = results[0].hypotheses[0][1:]
            translated_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(target), skip_special_tokens=True)
        return translated_text
    except Exception as e:
         print(str(e))

MC_TOKENIZER = AutoTokenizer.from_pretrained('bert_model_path')
MC_MODEL = AutoModelForSequenceClassification.from_pretrained('bert_model_path').to(&quot;cuda&quot;)
MC_MODEL.eval()
THRESHOLD = 0.3

MODEL_3_MODEL = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased_model_path').to(torch.device(&quot;cuda&quot;))
MODEL_3_TOKENIZER = DistilBertTokenizer.from_pretrained('distilbert-base-uncased_model_path')


def model2_prediction(input_text):
    try:
        tokens = MC_TOKENIZER(input_text, add_special_tokens=True, return_tensors=&quot;pt&quot;, padding=True)
        tokens = {key: value.to('cuda') for key, value in tokens.items()}
        with torch.no_grad():
            logits = MC_MODEL(**tokens)[0].to('cuda')
        pred = F.softmax(logits, dim=1)
        filtered_classes = [[i for i, class_prob in enumerate(prob) if class_prob &gt;= THRESHOLD] for prob in pred]
        return filtered_classes
    except Exception as e:
        print(str(e))

def model2_labelling(input_data: str):
    try:
        classes = []
        selected_classes = model2_prediction(input_data)
        for class_id in selected_classes[0]:
            class_name = MC_MODEL.config.id2label[class_id]
            classes.append(class_name)
        return classes
    except Exception as e:
        print(str(e))


def Model3(input_data: dict):
    try:
        id_val = input_data[&quot;id&quot;]
        text_val = str(input_data[&quot;text&quot;]).lower()
        tokens = MODEL_3_TOKENIZER(text_val, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
        tokens = {k: v.to(torch.device(&quot;cuda&quot;)) for k, v in tokens.items()}
        with torch.no_grad():
            outputs = MODEL_3_MODEL(**tokens)
            pred = F.softmax(outputs.logits, dim=1).tolist()[0]
            pred.insert(1, pred.pop(2))
            result = {&quot;id&quot;: id_val, &quot;sentiment&quot;: pred}
            return result
    except Exception as e:
        print(str(e))

def text_preprocessing(text):
    pass

def ner_pred(text):
    text = text_preprocessing(text)
    doc = ner_model(text)
    entity = []
    for ent in doc.ents:
        if ent.label_ == &quot;PERSON&quot;:
            entity.append(ent.text)
    return entity

def ner_result(text):
    pass # post processing of the result

def get_result(queue_name):
    try:
        data = queue_name
            
        data = convertToEng(input_data = data)
        data_text = data.get('text')
        
        if data_text:
            id = data['id']

            # sentiment = SentimentAnalysis(input_data = {&quot;id&quot;: id, &quot;text&quot;: ' '.join(data_text.split()[:20])})
            sentiment = Model3(input_data = {&quot;id&quot;: id, &quot;text&quot;: ' '.join(data_text.split()[:20])}) 
            topics = model2_labelling(input_data = data_text)
            final_result = [id,{'sentiment':sentiment['sentiment'],'topic':topics}]
        return final_result
    except Exception as e:
        print(e)


data = {&quot;doc1&quot;:{'text': 'ram is a good boy.', 'lang': 'en'}}

if __name__ == &quot;__main__&quot;:
    while True:
        status = get_result(queue_name=data)
</code></pre>
<p>I an using following library along with python version 3.10 and i have NVIDIA GeForce RTX 3070, Driver Version: 535.183.01, CUDA Version: 12.2:</p>
<ul>
<li><p>nvidia-cublas-cu12         12.1.3.1</p>
</li>
<li><p>nvidia-cuda-cupti-cu12     12.1.105</p>
</li>
<li><p>nvidia-cuda-nvrtc-cu12     12.1.105</p>
</li>
<li><p>nvidia-cuda-runtime-cu12   12.1.105</p>
</li>
<li><p>nvidia-cudnn-cu12          8.9.2.26</p>
</li>
<li><p>nvidia-cufft-cu12          11.0.2.54</p>
</li>
<li><p>nvidia-curand-cu12         10.3.2.106</p>
</li>
<li><p>nvidia-cusolver-cu12       11.4.5.107</p>
</li>
<li><p>nvidia-cusparse-cu12       12.1.0.106</p>
</li>
<li><p>nvidia-nccl-cu12           2.18.1</p>
</li>
<li><p>nvidia-nvjitlink-cu12      12.5.40</p>
</li>
<li><p>nvidia-nvtx-cu12           12.1.105</p>
</li>
<li><p>torch                      2.1.0</p>
</li>
<li><p>spacy                      3.7.4</p>
</li>
<li><p>spacy-alignments           0.9.1</p>
</li>
<li><p>spacy-curated-transformers 0.2.2</p>
</li>
<li><p>spacy-legacy               3.0.12</p>
</li>
<li><p>spacy-loggers              1.0.5</p>
</li>
<li><p>spacy-transformers         1.3.5</p>
</li>
<li><p>accelerate                 0.29.3</p>
</li>
<li><p>transformers               4.36.2</p>
</li>
</ul>
<p>I have tried to clear the cache using <code>gc</code> and also i have set the environment to <code>os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.8'</code> and
<code>os.environ['ONEDNN_PRIMITIVE_CACHE_CAPACITY'] = '0'</code></p>
<p>Also i have deleted all the variable used in the function after they return the result in try block. i deleted in <code>finally</code>block. BUT NO IMPROVEMENT.</p>
",Text Classification / Sentiment Analysis,cpu memory leak inference model infinite loop experiencing cpu memory leak running python script process text using various nlp model infinite loop script includes language translation sentiment analysis topic classification simplified version problematic code using following library along python version nvidia rtx driver version cuda version nvidia cublas cu nvidia cuda cupti cu nvidia cuda nvrtc cu nvidia cuda runtime cu nvidia cudnn cu nvidia cufft cu nvidia curand cu nvidia cusolver cu nvidia cusparse cu nvidia nccl cu nvidia nvjitlink cu nvidia nvtx cu torch spacy spacy alignment spacy curated transformer spacy legacy spacy logger spacy transformer accelerate transformer tried clear cache using also set environment also deleted variable used function return result try block deleted block improvement
What are the most challenging issues in Sentiment Analysis(opinion mining)?,"<p>Opinion Mining/Sentiment Analysis is a somewhat recent subtask of Natural Language processing.Some compare it to text classification,some take a more deep stance towards it. What do you think about the most challenging issues in Sentiment Analysis(opinion mining)? Can you name a few?</p>
",Text Classification / Sentiment Analysis,challenging issue sentiment analysis opinion mining opinion mining sentiment analysis somewhat recent subtask natural language processing compare text classification take deep stance towards think challenging issue sentiment analysis opinion mining name
"Trying to run GermanSentiment in Python on 10k to 30k texts, keeps crashing? Possibly too large dataset?","<p>I want to do sentiment analysis on 2 datasets of tweets, one with 9k strings and one with 30k strings. I have imported GermanSentiment and it ran just fine with the demo code from GitHub, but when I applied it to the 30k or 9k set, it caused my CPU to spike so fast that I had to put my laptop into standby and then kill the command line to regain control.
Believing it to be an issue of computing power, I tried it on my tower PC with more CPU+GPU power, it also causes CPU to spike to 100 % and gives me
<code>Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\torch\csrc\utils\tensor_numpy.cpp:84.)</code> despite NumPy being installed.</p>
<p>Is there some limit to GermanSentiment? Am I doing something wrong?</p>
<p>Added my code snippets as best I could. On my PC, I also get:</p>
<pre><code>NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy&lt;2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.
</code></pre>
<p>Code:</p>
<pre><code>from germansentiment import SentimentModel
import json
from tqdm import tqdm
#import csv
import re
import argparse
import xmltodict
import mdb_reader
import csvReader
import jsonReader
from pathlib import Path

def check_sentiment(tweet_set_1, tweet_set_2, sm_model):
    model = sm_model

    for tweet_set in tqdm([tweet_set_2]):
        texts = [tweet_set[tweet][&quot;text&quot;] for tweet in tweet_set]
    #breakpoint()
        
        result = model.predict_sentiment(tqdm(texts))
        print(result)

### Sentiment Analyse ###

    model = SentimentModel()

    #texts = [
    #&quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    #&quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    #&quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;]
       
    #result = model.predict_sentiment(texts)
    #print(result)

    check_sentiment(tweet_poli, tweet_other, model)

</code></pre>
",Text Classification / Sentiment Analysis,trying run germansentiment python k k text keep crashing possibly large dataset want sentiment analysis datasets tweet one k string one k string imported germansentiment ran fine demo code github applied k k set caused cpu spike fast put laptop standby kill command line regain control believing issue computing power tried tower pc cpu gpu power also cause cpu spike give despite numpy installed limit germansentiment something wrong added code snippet best could pc also get code
Syuzhet not accepting UTF-8 characters?,"<p>Morning all,</p>
<p>I am trying to conduct sentiment analysis on a large Swedish Twitter dataset with a custom lexicon specifically for Swedish research. However, even if the text I am wanting to analyse and the lexicon are encoded as UTF-8, the syuzhet package completely ignores words with the Swedish å ä and ö characters. Is this an inherent issue with the package (i.e. I need to recode the charachters in the data + lexicon, or am I missing something glaringly obvious?</p>
<p>I tried the following dummy, and evidently the get_sentiment function does not accept the Swedish characters.</p>
<pre><code>test &lt;- c(&quot;kärnvapen&quot;, &quot;kärnkraftsolycka&quot;, &quot;glad&quot;, &quot;körd&quot;, &quot;kåk&quot;, &quot;ledsen&quot;)

SWE_lexicon &lt;- read.delim(&quot;sensaldo-base-v02.txt&quot;, header=FALSE, encoding = &quot;UTF-8&quot;)
colnames(SWE_lexicon)[colnames(SWE_lexicon) == 'V1'] &lt;- 'word'
colnames(SWE_lexicon)[colnames(SWE_lexicon) == 'V2'] &lt;- 'value'

score &lt;- get_sentiment(test, method=&quot;custom&quot;, lexicon = SWE_lexicon)

head(score)
[1]  0  0  1  0  0 -1
</code></pre>
<p>When recoding the charachters into e.g. ae for ä it assigned the correct values to the word, which is:</p>
<pre><code>[1]  0  -1  1  -1  0 -1
</code></pre>
",Text Classification / Sentiment Analysis,syuzhet accepting utf character morning trying conduct sentiment analysis large swedish twitter dataset custom lexicon specifically swedish research however even text wanting analyse lexicon encoded utf syuzhet package completely ignores word swedish character inherent issue package e need recode charachters data lexicon missing something glaringly obvious tried following dummy evidently get sentiment function doe accept swedish character recoding charachters e g ae assigned correct value word
How to remove words from a sentence that carry no positive or negative sentiment?,"<p>Im trying a sentiment analysis based approach on youtube comments, but the comments many times have words like mrbeast, tiger/'s, lion/'s, pewdiepie, james, etc which do not add any feeling in the sentence. I've gone through nltk's average_perception_tagger but it didn't work well as it gave the results as</p>
<p>my input:</p>
<pre><code>&quot;mrbeast james lion tigers bad sad clickbait fight nice good&quot;
</code></pre>
<p>words that i need in my sentence:</p>
<pre><code>&quot;bad sad clickbait fight nice good&quot;
</code></pre>
<p>what i got using average_perception_tagger:</p>
<pre><code>[('mrbeast', 'NN'),
 ('james', 'NNS'),
 ('lion', 'JJ'),
 ('tigers', 'NNS'),
 ('bad', 'JJ'),
 ('sad', 'JJ'),
 ('clickbait', 'NN'),
 ('fight', 'NN'),
 ('nice', 'RB'),
 ('good', 'JJ')]

</code></pre>
<p>so as you can see if i remove mrbeast i.e NN the words like clickbait, fight will also get removed which than ultimately remove expressions from that sentence.</p>
",Text Classification / Sentiment Analysis,remove word sentence carry positive negative sentiment im trying sentiment analysis based approach youtube comment comment many time word like mrbeast tiger lion pewdiepie james etc add feeling sentence gone nltk average perception tagger work well gave result input word need sentence got using average perception tagger see remove mrbeast e nn word like clickbait fight also get removed ultimately remove expression sentence
How can I implement regression after multi-class multi-label classification?,"<p>I have a dataset where some objects (15%) belong to different classes and have a property value for each of those classes. How can I make a model that predicts multi-label or multi-class and then make a regression prediction based on the output of the classifier? I also need to output the probabilities for each class. unfortunately I can't delete this 15%.
<a href=""https://i.sstatic.net/TM6Pn3AJ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I have no idea how to put it together. I have only found how to implement it separately. Any advice?</p>
",Text Classification / Sentiment Analysis,implement regression multi class multi label classification dataset object belong different class property value class make model predicts multi label multi class make regression prediction based output classifier also need output probability class unfortunately delete enter image description idea put together found implement separately advice
PyABSA for Aspect Based Sentiment ANalysis,"<p>I am working on Aspect based Sentiment Analysis and came across this nice package - <a href=""https://pypi.org/project/pyabsa/1.16.27/"" rel=""nofollow noreferrer"">PyABSA</a>.
I've found that version 2 is problematic with the models not getting loaded, so I'm using version 1.</p>
<p>There is a slight problem being that it works with downloading the checkpoints from Google Drive. This means that after a number of calls, Google blocks the IP Address from making calls. There is the option to download the checkpoint and use it manually.</p>
<p>I'm not sure how to go about this, or how to call it when manually downloaded, and there isn't much documentation. Just wondering if anyone here has used it and downloaded the checkpoints?</p>
<p>Here is a sample of the usage on the page, showing how the checkpoints are used when downloaded from online directly.</p>
<pre><code>from pyabsa.functional import ATEPCCheckpointManager #actual line of code


examples = ['But the staff was so nice to us .',
            'But the staff was so horrible to us .',
            r'Not only was the food outstanding , but the little ` perks \' were great .',
            'It took half an hour to get our check , which was perfect since we could sit , have drinks and talk !',
            'It was pleasantly uncrowded , the service was delightful , the garden adorable , '
            'the food -LRB- from appetizers to entrees -RRB- was delectable .',
            'How pretentious and inappropriate for MJ Grill to claim that it provides power lunch and dinners !'
            ]

inference_source = ABSADatasetList.Restaurant14
aspect_extractor = ATEPCCheckpointManager.get_aspect_extractor(checkpoint='english')
atepc_result = aspect_extractor.extract_aspect(inference_source=inference_source,
                                               save_result=True,
                                               print_result=True,  # print the result
                                               pred_sentiment=True,  # Predict the sentiment of extracted aspect terms
                                               )
</code></pre>
",Text Classification / Sentiment Analysis,pyabsa aspect based sentiment analysis working aspect based sentiment analysis came across nice package pyabsa found version problematic model getting loaded using version slight problem work downloading checkpoint google drive mean number call google block ip address making call option download checkpoint use manually sure go call manually downloaded much documentation wondering anyone ha used downloaded checkpoint sample usage page showing checkpoint used downloaded online directly
Hugging Face - Trainer object returning constant values for metrics every epoch,"<p>This is my first time playing around around with the Hugging Face library: my goal is a simple binary text classification task. Since my text data is in several different languages my aim is to fine-tune the XLM-RoBERTa model. My code is as follows:</p>
<pre><code>import transformers
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import Dataset, DatasetDict
import torch

# setting seed
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
transformers.set_seed(SEED)

# tokenize
model_ckpt = 'xlm-roberta-base'
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def tokenize(batch):
 return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)

dataset_encoded = joint_dataset.map(tokenize, batched=True, batch_size=None)

# FYI dataset_encoded -&gt;
# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label', 'input_ids', 'attention_mask'],
#         num_rows: 3395
#     })
#     validation: Dataset({
#         features: ['text', 'label', 'input_ids', 'attention_mask'],
#         num_rows: 425
#     })
#     test: Dataset({
#         features: ['text', 'label', 'input_ids', 'attention_mask'],
#         num_rows: 425
#     })
# })

# train model
num_labels = 2
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    recall = recall_score(labels, preds, average='macro')
    precision = precision_score(labels, preds, average='macro')
    f1 = f1_score(labels, preds, average='macro')
    return {'accuracy': acc, 'macro_f1': f1, 'macro_recall': recall, 'macro_precision': precision}

batch_size = 16
num_train_epochs = 3
logging_steps = len(dataset_encoded['train']) // batch_size
output_dir = './results'
logging_dir = './logs'
os.makedirs(output_dir, exist_ok=True)
os.makedirs(logging_dir, exist_ok=True)
training_args = TrainingArguments(output_dir=output_dir,
                                  num_train_epochs=num_train_epochs,
                                  learning_rate=1e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy=&quot;epoch&quot;,
                                  disable_tqdm=False,
                                  logging_dir=logging_dir,
                                  logging_steps=logging_steps,
                                  push_to_hub=False,
                                  log_level=&quot;error&quot;)

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset_encoded['train'],
                  eval_dataset=dataset_encoded['validation'],
                  tokenizer=tokenizer)

trainer.train()
</code></pre>
<p>The code runs smoothly however the 'computed' metrics remain the same after the first epoch. At first I thought the issue might be related to the learning rate but after changing it I keep getting the same results regardless of the batch size and epoch number.</p>
<p><a href=""https://i.sstatic.net/yrDjafY0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yrDjafY0.png"" alt=""Progress Callback"" /></a></p>
<p>It is also worthy of note that I also get the same metric values when making predictions on the test set:</p>
<pre><code>preds_output = trainer.predict(dataset_encoded['test'])
print(preds_output.metrics)
</code></pre>
<p><a href=""https://i.sstatic.net/JpO9Se92.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpO9Se92.png"" alt=""Test set Predictions"" /></a></p>
",Text Classification / Sentiment Analysis,hugging face trainer object returning constant value metric every epoch first time playing around around hugging face library goal simple binary text classification task since text data several different language aim fine tune xlm roberta model code follows code run smoothly however computed metric remain first epoch first thought issue might related learning rate changing keep getting result regardless batch size epoch number also worthy note also get metric value making prediction test set
Passing multiple sentences to BERT?,"<p>I have a dataset with paragraphs that I need to classify into two classes. These paragraphs are usually 3-5 sentences long. The overwhelming majority of them are less than 500 words long. I would like to make use of BERT to tackle this problem.</p>
<p>I am wondering how I should use BERT to generate vector representations of these paragraphs and especially, whether it is fine to just pass the whole paragraph into BERT?</p>
<p>There have been informative discussions of related problems <a href=""https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification/63413589#63413589"">here</a> and <a href=""https://stackoverflow.com/questions/63671085/how-to-use-bert-for-long-sentences"">here</a>. These discussions focus on how to use BERT for representing whole documents. In my case the paragraphs are not that long, and indeed could be passed to BERT without exceeding its maximum length of 512. However, BERT was trained on sentences. Sentences are relatively self-contained units of meaning. I wonder if feeding multiple sentences into BERT doesn't conflict fundamentally with what the model was designed to do (although this appears to be done regularly).</p>
",Text Classification / Sentiment Analysis,passing multiple sentence bert dataset paragraph need classify two class paragraph usually sentence long overwhelming majority le word long would like make use bert tackle problem wondering use bert generate vector representation paragraph especially whether fine pas whole paragraph bert informative discussion related problem
How to interpret scikit&#39;s learn confusion matrix and classification report?,"<p>I have a sentiment analysis task, for this Im using this <a href=""http://pastebin.com/ikbKQcsc"" rel=""noreferrer"">corpus</a> the opinions have 5 classes (<code>very neg</code>, <code>neg</code>, <code>neu</code>, <code>pos</code>, <code>very pos</code>), from 1 to 5. So I do the classification as follows:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,
                            sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd

df = pd.read_csv('/corpus.csv',
                     header=0, sep=',', names=['id', 'content', 'label'])

X = tfidf_vect.fit_transform(df['content'].values)
y = df['label'].values


from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,
                                                    y, test_size=0.33)


from sklearn.svm import SVC
svm_1 = SVC(kernel='linear')
svm_1.fit(X, y)
svm_1_prediction = svm_1.predict(X_test)
</code></pre>

<p>Then with the metrics I obtained the following confusion matrix and classification report, as follows:</p>

<pre><code>print '\nClasification report:\n', classification_report(y_test, svm_1_prediction)
print '\nConfussion matrix:\n',confusion_matrix(y_test, svm_1_prediction)
</code></pre>

<p>Then, this is the result:</p>

<pre><code>Clasification report:
             precision    recall  f1-score   support

          1       1.00      0.76      0.86        71
          2       1.00      0.84      0.91        43
          3       1.00      0.74      0.85        89
          4       0.98      0.95      0.96       288
          5       0.87      1.00      0.93       367

avg / total       0.94      0.93      0.93       858


Confussion matrix:
[[ 54   0   0   0  17]
 [  0  36   0   1   6]
 [  0   0  66   5  18]
 [  0   0   0 273  15]
 [  0   0   0   0 367]]
</code></pre>

<p>How can I interpret the above confusion matrix and classification report. I tried reading the <a href=""http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"" rel=""noreferrer"">documentation</a> and this <a href=""https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix"">question</a>. But still can interpretate what happened here particularly with this data?. Wny this matrix is somehow ""diagonal""?. By the other hand what means the recall, precision, f1score and support for this data?. What can I say about this data?. Thanks in advance guys</p>
",Text Classification / Sentiment Analysis,interpret scikit learn confusion matrix classification report sentiment analysis task im using corpus opinion class classification follows metric obtained following confusion matrix classification report follows result interpret confusion matrix classification report tried reading documentation
How to save all of the recently recorded real-time audio into a .mp3 or a .wav file?,"<p>I hope you are all well. I was trying to implement a multimodal sentiment analysis fyp. I was modifying my audio component. I was curious how to save all of a recently recorded real-time audio into a .wav or a .mp3 file?</p>
<p>For example suppose there was a program in which i was performing audio analysis, i dont know how to record all of audio into a .wav or a .mp3 file. Here is my current audio sentiment analysis code:</p>
<pre><code>import pyttsx3
engine = pyttsx3.init()


# Import necessary libraries
import speech_recognition as sr
import pyaudio
import wave
#import sentiment_analysis_library
#import llm_library
#import text_to_speech_library
from transformers import pipeline
from gtts import gTTS





# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()

# Function to transcribe audio
def transcribe_audio(audio):
    return r.recognize_google(audio)

# Function to analyze waveform
def analyze_waveform(audio):
    # Your waveform analysis code here
    pass

# Function to analyze sentiment
def analyze_sentiment(text):
    classifier = pipeline(&quot;text-classification&quot;,model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=False)
    prediction = classifier(text)
    return prediction

# Function to get response from llm
def get_llm_response(text):
    return llm_library.get_response(text)

# Function to convert text to speech
def text_to_speech(text):
    engine.say(str(text))
    engine.runAndWait()
    

# Main function to handle real-time audio
def handle_real_time_audio():
    while True:
        with sr.Microphone() as source:
            print(&quot;Listening...&quot;)
            audio = r.listen(source)
            
            # Transcribe the audio to text
            text = transcribe_audio(audio)
            
            # Analyze the waveform
            analyze_waveform(audio)
            
            # Analyze the sentiment
            sentiment = analyze_sentiment(text)
            print(&quot;Current sentiment is: &quot; + str(sentiment))
            
            # Feed the transcribed speech into an llm
            llm_response = get_llm_response(text)
            
            # Convert the llm response to audio
            text_to_speech(str(llm_response))

# Call the main function
handle_real_time_audio()
</code></pre>
",Text Classification / Sentiment Analysis,save recently recorded real time audio mp wav file hope well wa trying implement multimodal sentiment analysis fyp wa modifying audio component wa curious save recently recorded real time audio wav mp file example suppose wa program wa performing audio analysis dont know record audio wav mp file current audio sentiment analysis code
Stanford CoreNLP Sentiment Analysis with Text Classification,"<p>I am working on my individual bachelor's degree final project which is due in about 50 days. The website that I'm looking to create is one where users can share links to articles that they find amusing (something that makes them happy). It's a reddit like format where users can post globally and others can vote up or down articles based on how happy it makes them. The top trending posts will be shown at the top of the list and the least popular at the bottom.</p>
<p>The more exciting part of the project is implementing an NLP machine learning service that crawls the web for articles similar to the top trending ones and automatically posting articles to the website (without user input apart from the voting). In order to do this, I was thinking about having a Stanford CoreNLP service running on the server that picked out the the top trending articles, classified them based on what they're about (e.g. an article on Donald Trump should automatically generate tags such as 'Donald Trump', 'Republican', 'politics', etc.) Then by carrying out sentiment analysis on the article, using the Stanford CoreNLP sentiment annotator, I could see what the public's opinion is on the topics of the article (i.e. the tags). Then by using a web crawler, extracting articles from the web, and carrying out similar sentiment analysis on the articles extracted, I can find suitable articles to post to the website.</p>
<p>However, I haven't been able to find any annotators for text classification in Stanford CoreNLP. Is there any way I can implement what I have in mind. Better yet, are there any better ways of carrying out what I'm looking to achieve.</p>
",Text Classification / Sentiment Analysis,stanford corenlp sentiment analysis text classification working individual bachelor degree final project due day website looking create one user share link article find amusing something make happy reddit like format user post globally others vote article based happy make top trending post shown top list least popular bottom exciting part project implementing nlp machine learning service crawl web article similar top trending one automatically posting article website without user input apart voting order wa thinking stanford corenlp service running server picked top trending article classified based e g article donald trump automatically generate tag donald trump republican politics etc carrying sentiment analysis article using stanford corenlp sentiment annotator could see public opinion topic article e tag using web crawler extracting article web carrying similar sentiment analysis article extracted find suitable article post website however able find annotator text classification stanford corenlp way implement mind better yet better way carrying looking achieve
Is there any sentiment forum dataset for unsupervised training available?,"<p>I recently finished a machine learning course and would like to make a forum sentiment analysis tool, to apply it in stock-related forums.</p>
<p>The idea is to:</p>
<ol>
<li>Capture (text mining) users with their comments, and evaluate their comment's sentiment (positive, negative, neutral).</li>
<li>Capture what happens (stock market) after those comments, and assign a weight to the user accordingly (bigger weight if the user's sentiments is spot-on and the market follows the same direction)</li>
<li>Use the comments as a tool to predict market direction.</li>
</ol>
<p>Actually, I do this myself (pay attention on forums) plus my own technical analysis and the obligatory due diligence, and it has been working very well for me. I just wanted to try to automate it a little bit and maybe even allow a program to play with some of my accounts (paper trading first, and if it performs decently assign some money in a real account)</p>
<p>This would be my first machine learning project (just as a proof-of-concept) so any comments would be very kindly appreciated.</p>
<p>The biggest problem that I find is that I would like to make an <strong>unsupervised training</strong>, and I need a sample dataset to do the training.</p>
<p><strong>Is there any known forum-sentiment dataset available to be used for unsupervised training?</strong></p>
<p>I've found several sentiment datasets (twitter, imbd, amazon reviews) but they are very specific to their niche (short messages, movies, products...) but I'm looking for something more general.</p>
",Text Classification / Sentiment Analysis,sentiment forum dataset unsupervised training available recently finished machine learning course would like make forum sentiment analysis tool apply stock related forum idea capture text mining user comment evaluate comment sentiment positive negative neutral capture happens stock market comment assign weight user accordingly bigger weight user sentiment spot market follows direction use comment tool predict market direction actually pay attention forum plus technical analysis obligatory due diligence ha working well wanted try automate little bit maybe even allow program play account paper trading first performs decently assign money real account would first machine learning project proof concept comment would kindly appreciated biggest problem find would like make unsupervised training need sample dataset training known forum sentiment dataset available used unsupervised training found several sentiment datasets twitter imbd amazon review specific niche short message movie product looking something general
What does the value of LIME mean?,"<p>I'm exploring the workings of <strong>LIME</strong> in <strong>NLP</strong> models to understand how it elucidates positive and negative words.</p>
<p>I possess a trove of documents resembling this excerpt:</p>
<blockquote>
<p>The UN children’s agency says the 'world cannot stand by and watch' the suffering in Gaza.</p>
</blockquote>
<blockquote>
<p>'Intensifying conflict, malnutrition, and disease in Gaza are creating a deadly cycle that is threatening over 1.1 million children,' UNICEF said in a social media post.</p>
</blockquote>
<blockquote>
<p>At least 249 Palestinians have been killed and 510 wounded in the previous 24 hours in Gaza, the health ministry says.&quot;</p>
</blockquote>
<p>My classification model distinguishes each document as either &quot;<strong>United Nation Related</strong>&quot; or &quot;<strong>NON-UN Related</strong>&quot; denoted by labels <strong>1</strong> (United Nation Related) or <strong>0</strong> (NON-UN).</p>
<p>Here's a snippet of the code implementation:</p>
<pre class=""lang-py prettyprint-override""><code>exp = explainer.explain_instance(X_test.values[i], clf.predict_proba, num_features=LIMEMaxFeatures)
lst = exp.as_list()
</code></pre>
<p>Initially, everything seems fine. However, an issue arises when employing LIME for certain documents classified as NON-UN Related.</p>
<p>Consider this example:</p>
<blockquote>
<p>&quot;Guterres invoked this responsibility, saying he believed the
situation in Israel and the occupied Palestinian territories, 'may
aggravate existing threats to the maintenance of international peace
and security'.&quot;</p>
</blockquote>
<p>Despite this text, my model classifies it as <strong>NON-UN</strong> Related. Upon using LIME to delve into the reasons behind this classification, the results are as follows:</p>
<pre><code>    Word        Value
    occupied    -0.130118107160623
    situation   -0.284915997715762
    Guterres    0.22668070156952
    Gaza        0.144198872750898
</code></pre>
<p>My query pertains to the word &quot;Guterres,&quot; where the LIME value is <strong>POSITIVE</strong>. Does this imply that &quot;Guterres&quot; supports the decision of label <strong>0</strong> prediction (NON-UN Related)? <em>In essence, does a higher value for &quot;Guterres&quot; signify the model's stronger confidence in labeling the document as NON-UN Related?</em></p>
<p>Alternatively, does the <strong>POSITIVE</strong> value for &quot;Guterres&quot; signify a tendency towards label <strong>1</strong> (United Nation Related)? <em>Does a higher positive value for &quot;Guterres&quot; indicate a stronger inclination towards labeling the document as United Nation Related?</em></p>
",Text Classification / Sentiment Analysis,doe value lime mean exploring working lime nlp model understand elucidates positive negative word posse trove document resembling excerpt un child agency say world stand watch suffering gaza intensifying conflict malnutrition disease gaza creating deadly cycle threatening million child unicef said social medium post least palestinian killed wounded previous hour gaza health say classification model distinguishes document either united nation related non un related denoted label united nation related non un snippet code implementation initially everything seems fine however issue arises employing lime certain document classified non un related consider example guterres invoked responsibility saying believed situation israel occupied palestinian territory may aggravate existing threat maintenance international peace security despite text model classifies non un related upon using lime delve reason behind classification result follows query pertains word guterres lime value positive doe imply guterres support decision label prediction non un related essence doe higher value guterres signify model stronger confidence labeling document non un related alternatively doe positive value guterres signify tendency towards label united nation related doe higher positive value guterres indicate stronger towards labeling document united nation related
How to download hugging face sentiment-analysis pipeline to use it offline?,"<p><strong>How to download hugging face sentiment-analysis pipeline to use it offline?</strong> I'm unable to use hugging face sentiment analysis pipeline without internet. How to download that pipeline?</p>
<p>The basic code for sentiment analysis using hugging face is</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis') #This code will download the pipeline
classifier('We are very happy to show you the 🤗 Transformers library.')
</code></pre>
<p>And the output is</p>
<pre><code>[{'label': 'POSITIVE', 'score': 0.9997795224189758}]
</code></pre>
",Text Classification / Sentiment Analysis,download hugging face sentiment analysis pipeline use offline download hugging face sentiment analysis pipeline use offline unable use hugging face sentiment analysis pipeline without internet download pipeline basic code sentiment analysis using hugging face output
Sentence ML/DL classification based on keywords or use NLP rule based approach?,"<p>I am trying to classify sentences using machine and deep learning methods. Am using this script - <a href=""https://github.com/17765670591/twitter-suicidal-ideation-detection/blob/main/Notebook/Twitter_Suicidal_Ideation_Detection.ipynb"" rel=""nofollow noreferrer"">https://github.com/17765670591/twitter-suicidal-ideation-detection/blob/main/Notebook/Twitter_Suicidal_Ideation_Detection.ipynb</a></p>
<p>However, I am not getting any reasonable results since syntactically both these sets of labelled sentences are very similar (see the two sets of sentences below).
Am now wondering whether to use ML/DL methods or try to code in an NLP rule-based approach which considers sentence POS, such as here <a href=""https://github.com/nature-of-eu-rules/regulatory-statement-classification/blob/main/rule-based-classification.py"" rel=""nofollow noreferrer"">https://github.com/nature-of-eu-rules/regulatory-statement-classification/blob/main/rule-based-classification.py</a></p>
<p>There are many keywords, based on which these sentences are captured.
One way to differentiate the following sentences more clearly is the context around the 'keyword' which is different.</p>
<p>I just want to know how to proceed as ML/DL approaches may not apply. Or maybe there is a setting within the ML/DL (BERT) that I can set that focuses on the keyword NLP POS.
<strong>Or should I try to manually analyse the sentences for each keyword and identify and code the NLP POS patterns for each to capture them?</strong>
I found a recent thesis which takes this approach.</p>
<p><strong>For the keyword 'hang', these are the sentences that are labelled as 'dangerous'</strong></p>
<p>I want to <strong>hang</strong> quite badly.<br />
It looks like you tired to <strong>hang</strong> yourself!<br />
I tried to <strong>hang</strong> myself before I found him.<br />
I've seen photos of people after they <strong>hang</strong> themselves and it's disturbing and undignified.<br />
Was just looking up at the rafters of my garage and wondering if I could <strong>hang</strong> myself.<br />
At this stage I came downstairs, followed him outside where I realised he was trying to <strong>hang</strong> himself using his truck which had a crane on the back.</p>
<p><strong>For the same keyword, these are the sentences that are labelled as 'not dangerous'</strong></p>
<p>I don’t like my social life, the people I <strong>hang</strong> out with are great and my friends care about me, but I feel like i have a sort of social anxiety where I feel uncomfortable around people even thought I’ve known for quiet some time.<br />
im weak and i <strong>hang</strong> on her every word, i fell for her and i feel psychotic because it was so fast, but noones really ever showed me compassion or love before.<br />
For these moments, if you are lucky enough to have someone to reach out to tell you to <strong>hang</strong> on in there, just to sit with you and be present, to hold your hand, to give you a hug, whilst you eventually come to, and return to reality and tell you that YOUR LIFE IS WORTH LIVING, then yes, I BELIEVE you can get through it.<br />
I did some dishes and <strong>hang</strong> up the laundry from 2 days ago.</p>
",Text Classification / Sentiment Analysis,sentence ml dl classification based keywords use nlp rule based approach trying classify sentence using machine deep learning method using script however getting reasonable result since syntactically set labelled sentence similar see two set sentence wondering whether use ml dl method try code nlp rule based approach considers sentence po many keywords based sentence captured one way differentiate following sentence clearly context around keyword different want know proceed ml dl approach may apply maybe setting within ml dl bert set focus keyword nlp po try manually analyse sentence keyword identify code nlp po pattern capture found recent thesis take approach keyword hang sentence labelled dangerous want hang quite badly look like tired hang tried hang found seen photo people hang disturbing undignified wa looking rafter garage wondering could hang stage came downstairs followed outside realised wa trying hang using truck crane back keyword sentence labelled dangerous like social life people hang great friend care feel like sort social anxiety feel uncomfortable around people even thought known quiet time im weak hang every word fell feel psychotic wa fast noones really ever showed compassion love moment lucky enough someone reach tell hang sit present hold hand give hug whilst eventually come return reality tell life worth living yes believe get dish hang laundry day ago
Softmax output and probabilities not matching up?,"<p>I'm trying to test how well a GPT model can classify verbs according to the left-side context in a given input sentence with a masked term.
For example,</p>
<p>Input sentence:</p>
<pre><code>&quot;The ballerinas' costumes that the thieves stole from the theatre last night [MASK] found at the abandoned condo.&quot;
</code></pre>
<p>Input answer choices: &quot;are&quot;, &quot;is&quot; and &quot;were&quot;.</p>
<p>Desired output: conditional probability of each of the three answers according to the model.</p>
<p>Ideally, if the model is performing well, the correct answer (&quot;were&quot;) should have the highest probability % and softmax. But this isn't the case for me.</p>
<pre><code>!pip install transformers


import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel


# Load tokenizer and model
model_name = &quot;gpt2&quot;
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set the device to GPU if available
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

def calculate_conditional_probabilities(context, answer_choices):
    context_tokens = tokenizer.encode(context, add_special_tokens=True, return_tensors=&quot;pt&quot;)
    context_tokens = context_tokens.to(device)

    conditional_probs = []
    for choice in answer_choices:
        # Encode the choice and convert it to a tensor
        choice_tokens = torch.tensor(tokenizer.encode(choice, add_special_tokens=True)).unsqueeze(0).to(device)

        # Combine context and choice into a single input
        input_ids = torch.cat((context_tokens, choice_tokens), dim=-1)

        # Generate predictions using the model
        with torch.no_grad():
            logits = model(input_ids).logits

        # Calculate the conditional probability of the choice
        choice_id = tokenizer.encode(choice, add_special_tokens=True)[0]
        choice_prob = torch.softmax(logits[0, -1, :], dim=-1)[choice_id].item()
        conditional_probs.append(choice_prob)

    return conditional_probs
# Test the function
input_sentence = &quot;The ballerinas' costumes that the thieves stole from the theatre last night [MASK] found at the abandoned condo.&quot;
answer_choices = [&quot;are&quot;, &quot;is&quot;, &quot;were&quot;]
conditional_probs = calculate_conditional_probabilities(input_sentence, answer_choices)

## printint softmax outputs directly 
for choice, prob in zip(answer_choices, conditional_probs):
    print(f&quot;Softmax output of '{choice}':&quot;)
    print(prob)  
    prob_percentage = round(prob * 100, 2)
    print(f&quot;Conditional probability of '{choice}': {prob_percentage:.2f}%&quot;)
</code></pre>
<p>The output:</p>
<pre><code>Softmax output of 'are':
3.5287127957417397e-06
Conditional probability of 'are': 0.00%
Softmax output of 'is':
5.8688110584625974e-05
Conditional probability of 'is': 0.01%
Softmax output of 'were':
1.574901915546434e-07
Conditional probability of 'were': 0.00%
</code></pre>
<p>Should softmax values not add up to 1 or close to 1? Also, how come the output with the lowest softmax value has the highest probability % (the opposite of what should be the case)? Also I find it hard to believe that GPT can't be more that 0.01% about any of the output options in such a simple grammar solving problem...</p>
<p>Edit: I realize the softmax and probabilities do match up (completely glossed over e-6). But I'm still wondering why the output probabilities aren't any better than 0.01% for such a simple grammar problem (simple verb conjugation; solving for verb number given a subject), that too with GPT?</p>
",Text Classification / Sentiment Analysis,softmax output probability matching trying test well gpt model classify verb according left side context given input sentence masked term example input sentence input answer choice desired output conditional probability three answer according model ideally model performing well correct answer highest probability softmax case output softmax value add close also come output lowest softmax value ha highest probability opposite case also find hard believe gpt output option simple grammar solving problem edit realize softmax probability match completely glossed e still wondering output probability better simple grammar problem simple verb conjugation solving verb number given subject gpt
"GPT3 : from next word to Sentiment analysis, Dialogs, Summary, Translation ....?","<p>How does GPT3 or other model goes from next word prediction to do Sentiment analysis, Dialogs, Summaries, Translation .... ?</p>
<p>what is the idea and algorithms ?
How does it work ?</p>
<p>F.e. generating paragraph is generate next word then the next ..next..</p>
<p>On the other hand Sentiment analysis task is paragraph of text is Good/Bad, which is a classification ?
Extracting meaningful sentence from paragraph is even more different task.</p>
<p>How do we go from next token to ...... !</p>
<hr />
<p>Andre thanks for the replies.</p>
<p>It seems my question is not clear enough. So let me elaborate.
Next-token prediction can be trained on normal text corpus.</p>
<pre><code>word1 w2 w3 w4 .....
</code></pre>
<p>Next Sentiment can be trained on sentence=&gt;marker=&gt;label</p>
<pre><code>sent1: word1 w2 w3 w4 ..... marker label1
sent2: word1 w2 w3 w4 ..... marker label2
sent3: word1 w2 w3 w4 ..... marker label3
....
</code></pre>
<p>It is no longer corpus-next-token-generation. It is next-token generation.
The problem is you need to have the LABALED data !!</p>
<p>How about text summation ... lets use keyword extraction (and eventually sentence selection based on those keywords)
Again u need even more complex labeling.</p>
<pre><code>  paragraph1 =&gt; kw1
  paragraph1 =&gt; kw2
  paragraph2 =&gt; kw3
  paragraph3 =&gt; kw4         
</code></pre>
<p>it still can be thought of as next-token prediction but you need again specialized LABELED data.</p>
<blockquote>
<p>So my question given ONLY corpus text, how do you do the Sentiment, Text summary .... etc ?</p>
</blockquote>
<p>Otherwise GPT3 is simply scaled DNN with thousands of man hours for labeling data !!</p>
<p>WHERE is the LEAP ?</p>
",Text Classification / Sentiment Analysis,gpt next word sentiment analysis dialog summary translation doe gpt model go next word prediction sentiment analysis dialog summary translation idea algorithm doe work f e generating paragraph generate next word next next hand sentiment analysis task paragraph text good bad classification extracting meaningful sentence paragraph even different task go next token andre thanks reply seems question clear enough let elaborate next token prediction trained normal text corpus next sentiment trained sentence marker label longer corpus next token generation next token generation problem need labaled data text summation let use keyword extraction eventually sentence selection based keywords u need even complex labeling still thought next token prediction need specialized labeled data question given corpus text sentiment text summary etc otherwise gpt simply scaled dnn thousand man hour labeling data leap
How do I identify the ID number of test data misclassified by BERT?,"<p>I am using BERT to classify text segments extracted from ASR-generated transcripts. There are multiple segments per participant. I have stored the data in a dataframe with the following column names: Participant_ID, Segment_Text and Diagnosis (i.e. label). I have successfully trained my dataset and am trying to identify which segments of text (i.e. Segment_Text) and Participant_ID the model misclassified - how would I go about doing this? I have provided the code for the custom Dataset class and Evaluation class (i.e. testing the BERT model) below:</p>
<p>Custom Dataset Class</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
labels = {'HC':0, 'PK':1} 

class Dataset(torch.utils.data.Dataset):

def __init__(self, df):

    self.participantIDs = df['Participant_ID']
    self.labels = [labels[label] for label in df['Diagnosis']]
    self.texts = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=&quot;pt&quot;) for text in df['Transcript_Segment']]

def classes(self):
    return self.labels

def __len__(self):
    return len(self.labels)

def get_batch_labels(self, idx):
    # Fetch a batch of labels
    return np.array(self.labels[idx])

def get_batch_texts(self, idx):
    # Fetch a batch of inputs
    return self.texts[idx]

# def get_batch_participant_ids(self, idx):
#     # Fetch a batch of inputs
#     return self.participantIDs[idx]

def __getitem__(self, idx):

    batch_texts = self.get_batch_texts(idx)
    batch_y = self.get_batch_labels(idx)
    # batch_participant_ids = self.get_batch_participant_ids(idx)

    return batch_texts, batch_y
</code></pre>
<p>Evaluate BERT model class</p>
<pre><code>def evaluate(model, test_data):

incorrect_samples = []
test = Dataset(test_data)

test_dataloader = torch.utils.data.DataLoader(test, batch_size=8)

use_cuda = torch.cuda.is_available()
device = torch.device(&quot;cuda&quot; if use_cuda else &quot;cpu&quot;)

if use_cuda:

    model = model.cuda()

total_acc_test = 0
with torch.no_grad():

    for test_input, test_label in test_dataloader:

          test_label = test_label.to(device)
          mask = test_input['attention_mask'].to(device)
          input_id = test_input['input_ids'].squeeze(1).to(device)

          output = model(input_id, mask)
          _, pred = torch.max(output,1)
          idxs_mask = ((pred == test_label) == False).nonzero()
          print(idxs_mask)
          incorrect_samples.append(input_id[idxs_mask].cpu().detach().numpy())

          acc = (output.argmax(dim=1) == test_label).sum().item()
          total_acc_test += acc

print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')
print(incorrect_samples)
</code></pre>
",Text Classification / Sentiment Analysis,identify id number test data misclassified bert using bert classify text segment extracted asr generated transcript multiple segment per participant stored data dataframe following column name participant id segment text diagnosis e label successfully trained dataset trying identify segment text e segment text participant id model misclassified would go provided code custom dataset class evaluation class e testing bert model custom dataset class evaluate bert model class
How can I train distilBERT more efficiently on my large text classification task?,"<p>I've been thrown into the deep end a bit with a task at work. I need to use <em>DistilBERT</em> for a multi-class text classification problem, but here's the kicker the dataset is gigantic - we're talking <strong>millions of samples!</strong></p>
<p>I've been messing around with it, and <em>DistilBERT</em> does seem to do the job well. However, training takes forever So, here are my dilemmas:</p>
<p><code>Model Training</code>: How can I make <em>DistilBERT</em> handle this beast of a dataset more efficiently? Anyone got experience tweaking the training strategy, batch size, learning rate, etc.?
<code>Hardware Constraints</code>: Any hardware magic tricks to pull off? Is splurging on a fancy GPU the only way, or are there some tricks I don't know about?
<code>Inference Speed</code>: I also need to make sure the model can quickly classify new data after training. What are my options?</p>
<p>Any help would be a lifesaver!</p>
",Text Classification / Sentiment Analysis,train distilbert efficiently large text classification task thrown deep end bit task work need use distilbert multi class text classification problem kicker dataset gigantic talking million sample messing around distilbert doe seem job well however training take forever dilemma make distilbert handle beast dataset efficiently anyone got experience tweaking training strategy batch size learning rate etc hardware magic trick pull splurging fancy gpu way trick know also need make sure model quickly classify new data training option help would lifesaver
How to improve the results of this neural network of finetuned BERT model?,"<p>I'm working on a NLP classification problem where I'm trying to classify training courses into 99 categories. I managed to make a few models including <a href=""https://stackoverflow.com/questions/76490589/valueerror-when-using-model-fit-even-with-the-vectors-being-aligned"">the Bayesian classifier</a> but it had an accuracy of 55% (very bad).</p>
<p>Given those results, I tried to fine-tune the camemBERT model (my data is in french) to improve the model results but I never used these methods before so I tried to follow this <a href=""https://www.kaggle.com/code/houssemayed/camembert-for-french-tweets-classification/comments"" rel=""nofollow noreferrer"">example</a> and adapt it to my code.</p>
<p>In the example above, there are 2 labels while I have 99 labels.</p>
<p>I left certain parts intact</p>
<pre><code>epochs = 5
MAX_LEN = 128
batch_size = 16
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)
</code></pre>
<p>I selected the same variable names, in text you have the feature column and in labels you have the labels</p>
<pre><code>text = training['Intitulé (Ce champ doit respecter la nomenclature suivante : Code action – Libellé)_x']
labels = training['Domaine sou domaine ']
</code></pre>
<p>I tokenized and padded the sequences using the same values in the example because I didn't know which values are right for my data</p>
<pre><code>#user tokenizer to convert sentences into tokenizer
input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN) for sent in text]

# Pad our input tokens
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=&quot;long&quot;, truncating=&quot;post&quot;, padding=&quot;post&quot;)

# Create attention masks
attention_masks = []
# Create a mask of 1s for each token followed by 0s for padding
for seq in input_ids:
    seq_mask = [float(i &gt; 0) for i in seq]
    attention_masks.append(seq_mask)
</code></pre>
<p>I noticed that the labels are numeric in the example above so I changed my labels to numeric using this code</p>
<pre><code>label_map = {label: i for i, label in enumerate(set(labels))}
numeric_labels = [label_map[label] for label in labels]
labels = numeric_labels
</code></pre>
<p>I started building the model starting with the tensors</p>
<pre><code># Use train_test_split to split our data into train and validation sets for training
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(
    input_ids, labels, random_state=42, test_size=0.1
)

train_masks, validation_masks = train_test_split(
    attention_masks, random_state=42, test_size=0.1
)

# Convert the data to torch tensors
train_inputs = torch.tensor(train_inputs)
validation_inputs = torch.tensor(validation_inputs)
train_labels = torch.tensor(train_labels)
validation_labels = torch.tensor(validation_labels)
train_masks = torch.tensor(train_masks)
validation_masks = torch.tensor(validation_masks)

# Create data loaders
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)
# Define the model architecture
model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=99)

# Move the model to the appropriate device
model.to(device)                                                           
</code></pre>
<p>the output is:</p>
<pre><code>CamembertForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(32005, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=99, bias=True)
  )
)
</code></pre>
<p>Then I proceeded with creating the neural network</p>
<pre><code>param_optimizer = list(model.named_parameters())
optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer], 'weight_decay_rate': 0.01}]
optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)

# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

train_loss_set = []

# trange is a tqdm wrapper around the normal python range
for _ in trange(epochs, desc=&quot;Epoch&quot;):  
    # Tracking variables for training
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
  
    # Train the model
    model.train()
    for step, batch in enumerate(train_dataloader):
        # Add batch to device CPU or GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch
        # Clear out the gradients (by default they accumulate)
        optimizer.zero_grad()
        # Forward pass
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        # Get loss value
        loss = outputs[0]
        # Add it to train loss list
        train_loss_set.append(loss.item())    
        # Backward pass
        loss.backward()
        # Update parameters and take a step using the computed gradient
        optimizer.step()
    
        # Update tracking variables
        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1

    print(&quot;Train loss: {}&quot;.format(tr_loss / nb_tr_steps))

    # Tracking variables for validation
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    # Validation of the model
    model.eval()
    # Evaluate data for one epoch
    for batch in validation_dataloader:
        # Add batch to device CPU or GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch
        # Telling the model not to compute or store gradients, saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions
            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
            loss, logits = outputs[:2]
    
        # Move logits and labels to CPU if GPU is used
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        tmp_eval_accuracy = flat_accuracy(logits, label_ids)
    
        eval_accuracy += tmp_eval_accuracy
        nb_eval_steps += 1

    print(&quot;Validation Accuracy: {}&quot;.format(eval_accuracy / nb_eval_steps))
</code></pre>
<p>And the code worked, but the accuracy level was at 30%, which is way worse than a Bayesian classifier that uses a very simple algorithm and straightforward calculation. This made me realize that I must have fine-tuned the model wrongly, but I don't understand fine-tuning well enough to know where I went wrong.</p>
",Text Classification / Sentiment Analysis,improve result neural network finetuned bert model working nlp classification problem trying classify training course category managed make model including example adapt code example label label left certain part intact selected variable name text feature column label label tokenized padded sequence using value example know value right data noticed label numeric example changed label numeric using code started building model starting tensor output proceeded creating neural network code worked accuracy level wa way worse bayesian classifier us simple algorithm straightforward calculation made realize must fine tuned model wrongly understand fine tuning well enough know went wrong
Multiclass text classification using hugging face models,"<p>I am trying to do sentiment analysis on customer feedback and for that I am using hugging face models (required). The issue is that all the responses I am getting are either Positive or negative , I haven't gotten a neutral response.</p>
<p>this is how my dataset looks like</p>
<pre><code>import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np


# Example DataFrame
df = pd.DataFrame({'text': ['This movie is great!','neutral','Happy this movie!' ,'I feel bored.', 'The weather is nice.',np.nan]})

    # Function to predict sentiment
    def predict_sentiment(text):
        # Load tokenizer and model
        if pd.isna(text):
            return 'N/A'  # Return a default value for NaN
        tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
        model = AutoModelForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-imdb&quot;)
        tokens = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
        outputs = model(**tokens)
        predicted_class = outputs.logits.argmax().item()
        sentiment_classes = ['negative','positive', 'neutral']
        predicted_sentiment = sentiment_classes[predicted_class]
        return predicted_sentiment
    
    # Apply sentiment prediction on DataFrame column
    df['predicted_sentiment'] = df['text'].apply(predict_sentiment)

             text                      predicted_sentiment
0   This movie is great!                  positive
1   neutral                               positive
2   Happy this movie!                     positive
3   I feel bored.                         negative
4   The weather is nice.                  positive
5   NaN                                     N/A
</code></pre>
<p>Now, if I switch the lables like this ['negative','neutral','positive'] I only get results</p>
<pre><code>             text                      predicted_sentiment
0   This movie is great!                  neutral
1   neutral                               neutral
2   Happy this movie!                     neutral
3   I feel bored.                         negative
4   The weather is nice.                  neutral
5   NaN                                     N/A
</code></pre>
<p>whereas the results should be</p>
<pre><code>         text                      predicted_sentiment
0   This movie is great!                  positive
1   neutral                               neutral
2   Happy this movie!                     positive
3   I feel bored.                         negative
4   The weather is nice.                  positive
5   NaN                                     N/A
</code></pre>
",Text Classification / Sentiment Analysis,multiclass text classification using hugging face model trying sentiment analysis customer feedback using hugging face model required issue response getting either positive negative gotten neutral response dataset look like switch lables like negative neutral positive get result whereas result
Classifying texts using pre-trained DistilBERT transformer on Multiple GPUS,"<p>I have a pre-trained DistilBERT transformer to classify the texts in the <code>user_posts_cat</code> col:</p>
<pre class=""lang-py prettyprint-override""><code>user_agg = pd.DataFrame({&quot;user_username&quot;: [&quot;user1&quot;, &quot;user2&quot;], 
                         &quot;user_posts_cat&quot;: [
                                           [&quot;Stackoverflow Community is Great&quot;, &quot;Sample Text&quot;],
                                           [&quot;I Love Transformers&quot;, &quot;Sample Text 2&quot;, &quot;Test&quot;],
                                           ]}).set_index(&quot;user_username&quot;)
</code></pre>
<p>I am currently using the following code to identify whether a user in <code>user_agg</code>'s index above is a bot, using the most common predicted labels from a user's posts:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

def load_nlp(directory_path, num_labels=2, gpu_index=&quot;cuda:0&quot;):
    device = torch.device(gpu_index if torch.cuda.is_available() else &quot;cpu&quot;)

    tokenizer_ = AutoTokenizer.from_pretrained(directory_path)
    model_ = AutoModelForSequenceClassification.from_pretrained(
        directory_path, num_labels=num_labels).to(device)

    pipe = pipeline(&quot;text-classification&quot;, model=model_, tokenizer=tokenizer_, device=device.index)

    return pipe

saved_model_local_path = &quot;model_path&quot;
pipe = load_nlp(saved_model_local_path, num_labels=2)

from collections import Counter

def label_cls(posts, pipe=pipe) -&gt; str:
    labels = pipe(posts, padding=True, truncation=True, max_length=512, add_special_tokens=True)
    most_common_label = Counter([item[&quot;label&quot;] for item in labels]).most_common(1)[0][0]
    return most_common_label

user_agg[&quot;user_label&quot;] = [label_cls(posts) for posts in user_agg.user_posts_cat]
user_agg[&quot;user_label&quot;] = np.where(user_agg.user_label == &quot;LABEL_1&quot;, &quot;bot&quot;, &quot;human&quot;)
</code></pre>
<p>The code above uses only single GPU. I want to run the above predictions on all of my four GPUs. I have tried using the <code>torch.nn.DataParallel</code> module but failed with an error indicating that <code>DataParallel does not have the config attribute</code> from the transformers module.</p>
<p>I am using:</p>
<pre class=""lang-py prettyprint-override""><code>python: 3.10.2
CUDA: 12.0
A100 40GB X4 - Driver: 525.105.17
pandas: 2.0.0
keras: 2.11.0
torch: 1.13.1
transformers: 4.30.2
</code></pre>
",Text Classification / Sentiment Analysis,classifying text using pre trained distilbert transformer multiple gpus pre trained distilbert transformer classify text col currently using following code identify whether user index bot using common predicted label user post code us single gpu want run prediction four gpus tried using module failed error indicating transformer module using
sentiment classification using doc2vec and LSTM Models,"<p>I am building a text classification model based on sentiment analysis,
the data contains text and sentiment[Positive, Natural, Negative]<br />
As first step, I clean the data and normalize it,
then create doc2vec embedding:</p>
<pre><code># Convert the data to TaggedDocument format for Doc2Vec
documents = [TaggedDocument(words=text.split(), tags=[label]) for text, label in zip(data[&quot;text&quot;], data[&quot;sentiment&quot;])]
print(documents)
model = Doc2Vec(vector_size=10, window=2, min_count=1, workers=4, epochs=100)
model.build_vocab(documents)
model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>then split the data:</p>
<pre><code>X_train = [model.infer_vector(text.split()) for text in data[&quot;text&quot;]]
print(X_train)
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
label_encoder = LabelEncoder()
y_trainEmbedding = label_encoder.fit_transform(data['sentiment'])
onehot_encoder = OneHotEncoder(sparse=False)
y_trainEmbedding = onehot_encoder.fit_transform(y_trainEmbedding.reshape(-1, 1))
</code></pre>
<p>then build LSTM model:</p>
<pre><code>import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense

num_classes = len(np.unique(data[&quot;sentiment&quot;]))
model_lstm = Sequential()
model_lstm.add(LSTM(64, input_shape=(10, 1)))
model_lstm.add(Dense(32, activation=&quot;relu&quot;))
model_lstm.add(Dense(num_classes, activation=&quot;softmax&quot;))
model_lstm.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])
X_train_lstm = np.array(X_train).reshape(-1, 10, 1)
y_train_lstm = np.array(y_trainEmbedding)
model_lstm.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32)
</code></pre>
<p>the result is good and the accuracy is 0.99</p>
<p>but when I try to predict the label of new text such as below:</p>
<pre><code># Use the trained model to predict the sentiment of new texts
text = &quot;هذا البيت جميل &quot;
text=remove_punctuations(text)
text=remove_repeating_char(text)
text=remove_english_char(text)
text=remove_diacritics(text)
text=remove_noise_char(text)
text=tokenizer(text)
text=remove_stop_word(text)
text=stemming(text) 
new_embedding = model.infer_vector(text.split())
print(new_embedding)
new_embedding_lstm = np.array(new_embedding).reshape(-1, 10, 1)
print(new_embedding)

y_pred = model_lstm.predict(new_embedding_lstm)
print(y_pred)

predicted_label = label_encoder.inverse_transform(np.argmax(y_pred))
print(predicted_label)
</code></pre>
<p>this error occured:</p>
<pre><code> 18 
---&gt; 19 predicted_label = label_encoder.inverse_transform(np.argmax(y_pred))
     20 print(predicted_label)

1 frames
/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py in column_or_1d(y, dtype, warn)
   1200         return _asarray_with_order(xp.reshape(y, -1), order=&quot;C&quot;, xp=xp)
   1201 
-&gt; 1202     raise ValueError(
   1203         &quot;y should be a 1d array, got an array of shape {} instead.&quot;.format(shape)
   1204     )

ValueError: y should be a 1d array, got an array of shape () instead.
</code></pre>
<p>is my process correct?
and Anyone can help me solve it?</p>
",Text Classification / Sentiment Analysis,sentiment classification using doc vec lstm model building text classification model based sentiment analysis data contains text sentiment positive natural negative first step clean data normalize create doc vec embedding split data build lstm model result good accuracy try predict label new text error occured process correct anyone help solve
Is there any way to classify text without target labels?,"<p>I was wondering if there was any way to classify text data into different groups/categories based on the words in the text using a combination of Python and Sklearn Machine Learning?</p>
<p>For example:</p>
<pre><code>text = [[&quot;request approval for access&quot;, &quot;request approval to enter premises&quot;, &quot;Laptop not working&quot;], [&quot;completed bw table loading&quot;]]
</code></pre>
<p>So can I get categories like:</p>
<pre><code>category_label = [[0,0,2], [1]]
categories = [[&quot;approval request&quot;, &quot;approval request&quot;, &quot;Laptop working&quot;], [&quot;bw table&quot;]]
</code></pre>
<p>where</p>
<pre><code>  0 = approval request
  2 = laptop working
  1 = bw table
</code></pre>
<p>Basically the above would imply that there is no labelled training data or target labels.</p>
",Text Classification / Sentiment Analysis,way classify text without target label wa wondering wa way classify text data different group category based word text using combination python sklearn machine learning example get category like basically would imply labelled training data target label
I want a machine to learn to categorize short texts,"<p>I have a ton of short stories about 500 words long and I want to categorize them into one of, let's say, 20 categories:</p>
<ul>
<li>Entertainment</li>
<li>Food</li>
<li>Music</li>
<li>etc</li>
</ul>
<p>I can hand-classify a bunch of them, but I want to implement machine learning to guess the categories eventually. What's the best way to approach this? Is there a standard approach to machine learning I should be using? I don't think a decision tree would work well since it's text data...</p>
",Text Classification / Sentiment Analysis,want machine learn categorize short text ton short story word long want categorize one let say category entertainment food music etc hand classify bunch want implement machine learning guess category eventually best way approach standard approach machine learning using think decision tree would work well since text data
Apply Text classification on Social Media Comments,"<p>I need to perform Classification on Instagram comments.
I searched a lot but I am getting options for Sentiment Analysis.</p>
<p>Since the comments are not very long text, summarization also doesnt look like an option.</p>
<p>I want the classification to be like the below.</p>
<p><a href=""https://i.sstatic.net/gSfWs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gSfWs.png"" alt=""enter image description here"" /></a></p>
",Text Classification / Sentiment Analysis,apply text classification social medium comment need perform classification instagram comment searched lot getting option sentiment analysis since comment long text summarization also doesnt look like option want classification like
Is it possible to build a text classifier using existing LLM like chatgpt?,"<p>Pre LLM, when I want to build a text classifier (e.g., a sentiment analysis model, when given an input text, it returns &quot;positive&quot; or &quot;neutral&quot; or &quot;negative&quot;), I'll have to gather tons of data, choose a model architecture, and spend resources training the model.</p>
<p><a href=""https://i.sstatic.net/eGV2r.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eGV2r.png"" alt=""enter image description here"" /></a></p>
<p>Now as the LLMs like ChatGPT and Google Bard are very smart, I'm wondering if it is possible to build the same text classifier based on those LLMs. (I'm assuming this will require less data and less resources.)</p>
<p>Is this possible? Is there a walk through or tutorial I can follow? Thanks.</p>
",Text Classification / Sentiment Analysis,possible build text classifier using existing llm like chatgpt pre llm want build text classifier e g sentiment analysis model given input text return positive neutral negative gather ton data choose model architecture spend resource training model llm like chatgpt google bard smart wondering possible build text classifier based llm assuming require le data le resource possible walk tutorial follow thanks
Python Text Classification Accuracy Measurement Inconsistency,"<p>I'm trying to get accuracy, recall and precision measurements form NLTK movie review corpus but I get three undesirable outcomes:</p>
<ul>
<li>
<ol>
<li>I follow NLTK guide that has <code>random.shuffle</code> method, which makes accuracy, etc., different each time. This isn't good.</li>
</ol>
</li>
<li>
<ol start=""2"">
<li>I delete the <code>shuffle</code> line, but precision and recall don't work anymore and show 0.0 and &quot;none&quot; respectively.</li>
</ol>
</li>
<li>
<ol start=""3"">
<li>I delete the <code>shuffle</code> line and change the training and test sets to [500:1500] and [:1500] respectively, like in this thread: <a href=""https://stackoverflow.com/questions/45466041/how-to-get-the-precision-and-recall-from-a-nltk-classifier"">How to get the precision and recall from a nltk classifier?</a> The recall and precision do work now, but by doing so, the testing set is larger than the training one, which works at first glance but I believe you can't do that.</li>
</ol>
</li>
</ul>
<pre><code>import nltk
import random
import collections
from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize

documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]
random.shuffle(documents)



all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features[word] = (word in document_words)
    return features

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[300:], featuresets[:300]
print ('train on', len(train_set), 'instances, test on', len(test_set), 'instances')
classifier = nltk.NaiveBayesClassifier.train(train_set)
print(nltk.classify.accuracy(classifier, test_set))
classifier.show_most_informative_features(10)

refsets = collections.defaultdict(set)
testsets = collections.defaultdict(set)
for i, (feats, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = classifier.classify(feats)
    testsets[observed].add(i)
print('Precision:', nltk.precision(refsets['pos'], testsets['pos']))
print('Recall:', nltk.recall(refsets['pos'], testsets['pos']))
print('F_Measure:', nltk.f_measure(refsets['pos'], testsets['pos']))
</code></pre>
<p>Is there anything I can do, or am I just misunderstanding something?</p>
<p>Edit: a sufficient solution is to use <code>random.seed(x)</code> before shuffling, or to average several runs. It doesn't explain, however, why deleting the shuffle breaks the program.</p>
",Text Classification / Sentiment Analysis,python text classification accuracy measurement inconsistency trying get accuracy recall precision measurement form nltk movie review corpus get three undesirable outcome follow nltk guide ha method make accuracy etc different time good delete line precision recall work anymore show none respectively delete line change training test set respectively like thread href get precision recall nltk classifier recall precision work testing set larger training one work first glance believe anything misunderstanding something edit sufficient solution use shuffling average several run explain however deleting shuffle break program
How to finetune a zero-shot model for text classification,"<p>I need a model that is able to classify text for an unknown number of classes (i.e. the number might grow over time). The <a href=""https://arxiv.org/abs/1909.00161"" rel=""nofollow noreferrer"">entailment approach</a> for zero-shot text classification seems to be the solution to my problem, the model I tried <a href=""https://huggingface.co/facebook/bart-large-mnli"" rel=""nofollow noreferrer"">facebook/bart-large-mnli</a> doesn't perform well on my annotated data. Is there a way to fine-tune it without losing the robustness of the model?</p>
<p>My dataset looks like this:</p>
<pre><code># http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html
World, &quot;Afghan Army Dispatched to Calm Violence KABUL, Afghanistan - Government troops intervened in Afghanistan's latest outbreak of deadly fighting between warlords, flying from the capital to the far west on U.S. and NATO airplanes to retake an air base contested in the violence, officials said Sunday...&quot;
Sports, &quot;Johnson Helps D-Backs End Nine-Game Slide (AP) AP - Randy Johnson took a four-hitter into the ninth inning to help the Arizona Diamondbacks end a nine-game losing streak Sunday, beating Steve Trachsel and the New York Mets 2-0.&quot; 
Business, &quot;Retailers Vie for Back-To-School Buyers (Reuters) Reuters - Apparel retailers are hoping their\back-to-school fashions will make the grade among\style-conscious teens and young adults this fall, but it could\be a tough sell, with students and parents keeping a tighter\hold on their wallets.&quot;
</code></pre>
<p>P.S.: This is an artificial question that was created because this topic came up in the comment section of this <a href=""https://stackoverflow.com/q/76170604/6664872"">post</a> which is related to this <a href=""https://stackoverflow.com/questions/76099140/hugging-face-transformers-bart-cuda-error-cublas-status-not-initialize"">post</a>.</p>
",Text Classification / Sentiment Analysis,finetune zero shot model text classification need model able classify text unknown number class e number might grow time entailment approach zero shot text classification seems solution problem model tried facebook bart large mnli perform well annotated data way fine tune without losing robustness model dataset look like p artificial question wa created topic came comment section href related href
How do I reshape data to calculate ROC and AUC for binary text classification?,"<p>I'm very new to python and need to calculate the ROC and AUC of two binary classification models using NLP data. I can't seem to get my head around sparse vs dense arrays (I mean, I get that sparse arrays contain a ton of zeros, and dense arrays do not), data shape, and dimensionality.</p>
<p>I think I can produce pretty good preprocessed data, but inputting that into my classifiers in a way they can read has me stymied.</p>
<p>In my code below, you'll note that I have tried more than one train test split. I get</p>
<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>
<p>if I don't convert x and y to dense.</p>
<p>I get</p>
<pre><code>ValueError: y should be a 1d array, got an array of shape (1594, 286579) instead

UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
</code></pre>
<p>when I do the dense conversion.</p>
<p>And I get</p>
<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [1594, 399]
</code></pre>
<p>when (if I'm remembering correctly) using the commented out train test split.</p>
<p>Here is my messy, redundant code:</p>
<pre><code>import joblib
import re
import string
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.naive_bayes import MultinomialNB

categories = ['rec.sport.baseball', 'rec.sport.hockey']

news_group_data = fetch_20newsgroups(subset=&quot;all&quot;, remove=(&quot;headers&quot;, &quot;footers&quot;, &quot;quotes&quot;), categories=categories)

df = pd.DataFrame(dict(text=news_group_data[&quot;data&quot;],target=news_group_data[&quot;target&quot;]))
df[&quot;target&quot;] = df.target.map(lambda x: categories[x])

def process_text(text):
    text = str(text).lower()
    text = re.sub(f&quot;[{re.escape(string.punctuation)}]&quot;, &quot; &quot;, text)
    text = &quot; &quot;.join(text.split())
    return text

df[&quot;clean_text&quot;] = df.text.map(process_text)

#df_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)

vec = CountVectorizer(ngram_range=(1, 3), stop_words=&quot;english&quot;,)

x = vec.fit_transform(df.clean_text)
y = vec.transform(df.clean_text)


#X = vec.fit_transform(df_train.clean_text)
#Y = vec.transform(df_test.clean_text)

X = x.toarray()
Y = y.toarray()

#y_train = df_train.target
#y_test = df_test.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,
                                                    random_state=0)

from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features=5,
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, #min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=500,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)

nb = GaussianNB()
nb.fit(X_train, Y_train)

r_probs = [0 for _ in range(len(Y_test))]
rf_probs = rf.predict_proba(X_test)
nb_probs = nb.predict_proba(X_test)

rf_probs = rf_probs[:, 1]
nb_probs = nb_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score

r_auc = roc_auc_score(Y_test, r_probs)
rf_auc = roc_auc_score(Y_test, rf_probs)
nb_auc = roc_auc_score(Y_test, nb_probs)

print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))
print('Random Forest: AUROC = %.3f' % (rf_auc))
print('Naive Bayes: AUROC = %.3f' % (nb_auc))

r_fpr, r_tpr, _ = roc_curve(Y_test, r_probs)
rf_fpr, rf_tpr, _ = roc_curve(Y_test, rf_probs)
nb_fpr, nb_tpr, _ = roc_curve(Y_test, nb_probs)

import matplotlib.pyplot as plt

plt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)
plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest (AUROC = %0.3f)' % rf_auc)
plt.plot(nb_fpr, nb_tpr, marker='.', label='Naive Bayes (AUROC = %0.3f)' % nb_auc)

plt.title('ROC Plot')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
</code></pre>
",Text Classification / Sentiment Analysis,reshape data calculate roc auc binary text classification new python need calculate roc auc two binary classification model using nlp data seem get head around sparse v dense array mean get sparse array contain ton zero dense array data shape dimensionality think produce pretty good preprocessed data inputting classifier way read ha stymied code note tried one train test split get convert x dense get dense conversion get remembering correctly using commented train test split messy redundant code
Empty string with length &gt; 0 in R,"<p>I got a weird case in my dataframe while working with emojis in R. I want to delete all emojis for a sentiment analysis. When I do this I got some cases, where the string should be empty, but isn't. What is the problem? I would like to replace empty fields with <code>NA</code>. Here a little example:</p>
<pre><code>library(tidyverse)

df &lt;- data.frame(x = c(&quot;test&quot;,&quot;♥️♥️🙌♥&quot;))

nchar(df$x[2])

df_new &lt;- df |&gt;
  mutate(x = str_remove_all(x, &quot;[[:emoji:]]&quot;))

is_empty(df_new$x[2])
</code></pre>
<p>Now I would like to use the following command, but this doesn't work, because the string is not empty.</p>
<pre><code>tmp &lt;- df_new |&gt;
  mutate(x = na_if(x, &quot;&quot;))
</code></pre>
<p>What is the problem here and how I can solve this?</p>
<p>Thank you in advance,</p>
<p>Aaron</p>
",Text Classification / Sentiment Analysis,empty string length r got weird case dataframe working emojis r want delete emojis sentiment analysis got case string empty problem would like replace empty field little example would like use following command work string empty problem solve thank advance aaron
model interaction between words for a sentiment analysis task,"<p>I am wondering what is the most appropriate way to model the interaction between two words/variables in a language model for a sentiment analysis task. For example, in the following dataset:</p>
<pre><code>You didn't solve my problem,NEU
I never made that purchase,NEU
You never solve my problems,NEG
</code></pre>
<p>The words &quot;solve&quot; and &quot;never&quot;, in isolation, doesn't have a negative sentiment. But, when they appear together, they do. Formally speaking: assuming we have a feature «solve» that takes the value 0 when the word «solve» is absent, and 1 when the word is present, and another feature «never» with the same logic: the difference in the probability of Y=NEG between «solve»=0 and «solve»=1 is different when «never»=0 and «never»=1.</p>
<p>But a basic logistic regression (using, for example, <code>sklearn</code>), wouldn't be able to handle this kind of situation.
With <code>sklearn.preprocessing.PolynomialFeatures</code> it is possible to add interaction coefficients, but it is hardly the most effective option.</p>
",Text Classification / Sentiment Analysis,model interaction word sentiment analysis task wondering appropriate way model interaction two word variable language model sentiment analysis task example following dataset word solve never isolation negative sentiment appear together formally speaking assuming feature solve take value word solve absent word present another feature never logic difference probability neg solve solve different never never basic logistic regression using example able handle kind situation possible add interaction coefficient hardly effective option
Increasing loss and decreasing accuracy in BERT model with every epoch,"<p>I am trying to fine tune a BERT model for text classification. However I am encountering an issue where the accuracy falls drastically after 1 epoch and loss at the same time increases. I don't believe it is the dataset size that is the limiting factor, as I have tried to increase the dataset significantly without any effect on the performance.</p>
<p>I have tried to adjust learn rate from 5e-5 to 0.1 (in increments) and batch size ranging from 8-32 without[enter image description here] much effect.</p>
<p><a href=""https://i.sstatic.net/g0WuQ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Any recommendations for what to look out for?</p>
",Text Classification / Sentiment Analysis,increasing loss decreasing accuracy bert model every epoch trying fine tune bert model text classification however encountering issue accuracy fall drastically epoch loss time increase believe dataset size limiting factor tried increase dataset significantly without effect performance tried adjust learn rate e increment batch size ranging without enter image description much effect enter image description recommendation look
Sentiment Analysis Dictionaries,"<p>I was wondering if anybody knew where I could obtain dictionaries of positive and negative words.  I'm looking into sentiment analysis and this is a crucial part of it.</p>
",Text Classification / Sentiment Analysis,sentiment analysis dictionary wa wondering anybody knew could obtain dictionary positive negative word looking sentiment analysis crucial part
Training data for sentiment analysis,"<p>Where can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.</p>

<p>I find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?</p>
",Text Classification / Sentiment Analysis,training data sentiment analysis get corpus document already classified positive negative sentiment corporate domain want large corpus document provide review company like review company provided analyst medium find corpus review product movie corpus business domain including review company match language business
"How can I optimize KNN, GNB nd SVC sklearn algorithms to reduce exec time?","<p>I'm currently evaluating which classifier have the best performance for movie reviews sentiment analysis task. So far I have evaluate Logistic Regression, Linear Regression, Random Forest and Decision tree but I also want to consider KNN, GNB and SVC models as well. The problem is that each execution of those algorithms (particulary KNN) has a large exec time. Even using RandomizedSearch in KNN I have to wait about 1 hour with 10 iterations. Here are some snippets:</p>
<p>KNN Classifier</p>
<pre><code> #KNearestNeighbors X -&gt; large execution time
    knn=KNeighborsClassifier()
    k_range=list(range(1,50))
    options=['uniform', 'distance']
    param_grid = dict(n_neighbors=k_range, weights=options)
    rand_knn = RandomizedSearchCV(knn, param_grid, cv=10, scoring='accuracy', n_iter=10, random_state=0)
    rand_knn.fit(x_train_bow, y_train)
    print(rand_knn.best_score_)
    print(rand_knn.best_params_)
    confm_knn = confusion_matrix(y_test, y_pred_knn)
    print_confm(confm_knn)
    print(&quot;=============K NEAREST NEIGHBORS============&quot;)
    print_metrics(y_test,y_pred_knn)
    print(&quot;============================================&quot;)
</code></pre>
<p>I waited for the execution of the code above for about 85 minutes but it never finished and I had to cut the execution. In order to get any result (at least anything) I try to choose the best k manually with a for loop but still each iteration takes over 12 - 17 minutes.</p>
<pre><code>def testing_k_neighbors(x_train_bow,y_train,x_test_bow,y_test):
    accuracy_hist = []
    for i in range (1,21):
        knn=KNeighborsClassifier(n_neighbors=i)
        knn.fit(x_train_bow, y_train)
        yi_pred_knn = knn.predict(x_test_bow)
        acc_i = accuracy_score(y_test, yi_pred_knn)
        accuracy_hist.append(acc_i)
        print(f&quot;K: {i}, accuracy: {acc_i}&quot;)
    print(accuracy_hist)
</code></pre>
<p>output:</p>
<pre><code>K: 1, accuracy: 0.7384384634613782
K: 2, accuracy: 0.7435213732188984
K: 3, accuracy: 0.7574368802599784
K: 4, accuracy: 0.7678526789434214
K: 5, accuracy: 0.7681859845012916
K: 6, accuracy: 0.7745187901008249
K: 7, accuracy: 0.7729355887009416
K: 8, accuracy: 0.7774352137321889
K: 9, accuracy: 0.7742688109324223
K: 10, accuracy: 0.7810182484792934
K: 11, accuracy: 0.7776851929005916
K: 12, accuracy: 0.7854345471210732
K: 13, accuracy: 0.783101408215982
K: 14, accuracy: 0.7866844429630864
K: 15, accuracy: 0.784934588784268
K: 16, accuracy: 0.78860094992084
K: 17, accuracy: 0.7873510540788268
K: 18, accuracy: 0.7893508874260479
K: 19, accuracy: 0.7856011999000083
K: 20, accuracy: 0.7916006999416715
</code></pre>
<p>Also SVC and GNB takes similar time to get any result:</p>
<pre><code>    #Support Vector Macine  X -&gt; large execution time
    #svc=SVC(C = 100, kernel = 'linear', random_state=123)
    #svc.fit(x_train_bow,y_train)
    #y_pred_svc = svc.predict(x_test_bow)
    #print(&quot;=============SUPPORT VECTOR MACHINE============&quot;)
    #print_metrics(y_test,y_pred_svc)
    #print(&quot;============================================&quot;)   
</code></pre>
<pre><code>    #Gaussian Naive Bayes
    gnbc=GaussianNB()
    gnbc.fit(x_train_bow.toarray(),y_train)
    y_pred_gnbc = gnbc.predict(x_test_bow)
    print(&quot;=============GAUSSIAN NAIVE BAYES============&quot;)
    print_metrics(y_test,y_pred_gnbc)
    print(&quot;============================================&quot;)   
</code></pre>
<p>Is there any way to tune my code reduce execution time and mantain or improve models performance?</p>
<p>Im expecting to tune my code prioritzing both efficiency and performance</p>
",Text Classification / Sentiment Analysis,optimize knn gnb nd svc sklearn algorithm reduce exec time currently evaluating classifier best performance movie review sentiment analysis task far evaluate logistic regression linear regression random forest decision tree also want consider knn gnb svc model well problem execution algorithm particulary knn ha large exec time even using randomizedsearch knn wait hour iteration snippet knn classifier waited execution code minute never finished cut execution order get result least anything try choose best k manually loop still iteration take minute output also svc gnb take similar time get result way tune code reduce execution time mantain improve model performance im expecting tune code prioritzing efficiency performance
Is splitting a long document of a dataset for BERT considered bad practice?,"<p>I am fine-tuning a BERT model on a labeled dataset with many documents longer than the 512 token limit set by the tokenizer.
Since truncating would lose a lot of data I would rather use, I started looking for a workaround. However I noticed that simply splitting the documents after 512 tokens (or another heuristic) and creating new entries in the dataset with the same label was never mentioned.</p>
<p>In <a href=""https://stackoverflow.com/a/58643108/11614170"">this</a> answer, someone mentioned that you would need to recombine the predictions, is that necessary when splitting the documents?</p>
<p>Is this generally considered bad practice or does it mess with the integrity of the results?</p>
",Text Classification / Sentiment Analysis,splitting long document dataset bert considered bad practice fine tuning bert model labeled dataset many document longer token limit set tokenizer since truncating would lose lot data would rather use started looking workaround however noticed simply splitting document token another heuristic creating new entry dataset label wa never mentioned href answer someone mentioned would need recombine prediction necessary splitting document p generally considered bad practice doe mess integrity result
sentiment analysis-training for 3 classes sentiment using 2 classes dataset,"<p>I'm going to build a model for twitter sentiment on Financial market using transformers. I have access to StockTwits data for training, which only contains positive and negative sentiments. However, I would like to consider 3 classes, positive, negative and neutral. My question is it a safe way to use 2 class data for training a model with 3 classes?</p>
",Text Classification / Sentiment Analysis,sentiment analysis training class sentiment using class dataset going build model twitter sentiment financial market using transformer access stocktwits data training contains positive negative sentiment however would like consider class positive negative neutral question safe way use class data training model class
Training over an already trained transformer model,"<p>I took a pretrained BERT model and fine tuned it for text classification using a dataset(~3mn records, 46 categories).</p>
<p>Now I want to add some data(~5k records, 10 categories) to the model while keeping the original 46 categories. I just want the model to have all the latest data.</p>
<p>I want to avoid retraining with the full(3mn+5k) data because of time and costs and also because can be recurring activity (3-4 times a week)</p>
<p>Is there a way to do this?</p>
<p>Below is my code setup. I am using HF's trainer</p>
<pre><code># imports
import torch
from transformers import TrainerCallback
from transformers import TrainingArguments, Trainer
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import EarlyStoppingCallback

# constants
device = torch.device(&quot;cuda&quot;)
MODEL_NAME = 'bert-large-uncased'
TRAINING_EPOCHS = 20
TRAINING_BATCH_SIZE = 400
EVAL_BATCH_SIZE = 100

# dataset from pandas df
tr_dataset = Dataset(x_tr, tr_df.label_encoded.values.tolist())
te_dataset = Dataset(x_te, te_df.label_encoded.values.tolist())

# download model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=n_out, ).to(device)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# define training args
args = TrainingArguments(
    output_dir=SAVE_BERT_PATH,
    overwrite_output_dir=True,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;no&quot;,
    per_device_train_batch_size=TRAINING_BATCH_SIZE,
    per_device_eval_batch_size=EVAL_BATCH_SIZE,
    num_train_epochs=TRAINING_EPOCHS,
    seed=42,
    fp16=True,
    dataloader_num_workers = 10,
    load_best_model_at_end=False,
    metric_for_best_model=&quot;eval_loss&quot;,
    greater_is_better=False,
    logging_strategy='epoch',
    logging_first_step=True
    
)

# define trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tr_dataset,
    eval_dataset=te_dataset,
    compute_metrics=compute_metrics)

# train and eval
trainer.train()
trainer.evaluate()
</code></pre>
",Text Classification / Sentiment Analysis,training already trained transformer model took pretrained bert model fine tuned text classification using dataset mn record category want add data k record category model keeping original category want model latest data want avoid retraining full mn k data time cost also recurring activity time week way code setup using hf trainer
How can I extract all phrases from a sentence?,"<p>I want to extract all the possible meaningful phrases from a sentence
For example:
""Food was fantastic in the local restaurant and the restaurant was perfectly romantic.""
I want:
Food was fantastic
Food was fantastic in the local restaurant
the restaurant was perfectly romantic
etc</p>

<p>I don't mind if there are some additional phrases that come up as I am planning to use Vader sentiment analysis to remove neutral phrases. Another approach that would work for me is if there is a way to extract phrases around a keyword, then I can use python rake to get the keywords</p>

<p>This is a project to extract all possible positive and negative phrases for UGC reviews that we collect, our initial approach was to use regex patterns to extract phrases and then pass them through Vader to get sentiments but this was omitting a lot of phrases, now we are trying to shortlist sentences with a sentiment and then extracting phrases from  it,</p>
",Text Classification / Sentiment Analysis,extract phrase sentence want extract possible meaningful phrase sentence example food wa fantastic local restaurant restaurant wa perfectly romantic want food wa fantastic food wa fantastic local restaurant restaurant wa perfectly romantic etc mind additional phrase come planning use vader sentiment analysis remove neutral phrase another approach would work way extract phrase around keyword use python rake get keywords project extract possible positive negative phrase ugc review collect initial approach wa use regex pattern extract phrase pas vader get sentiment wa omitting lot phrase trying shortlist sentence sentiment extracting phrase
Which pre-trained model do I need to use for long text classification in BERT?,"<p>We know that bert has a max length limit of tokens = 512, So if an acticle has a length of much bigger than 512, such as 10000 tokens in text. In this case, How can I use BERT?</p>
",Text Classification / Sentiment Analysis,pre trained model need use long text classification bert know bert ha max length limit token acticle ha length much bigger token text case use bert
Make nn.Transformer work for Text Generation,"<p>I am trying to make a Transformer work for paraphrase generation but the generations are not useful (the same everytime, full of BOS tokens or &quot;?&quot; tokens).
I followed <a href=""https://pytorch.org/tutorials/beginner/translation_transformer.html"" rel=""nofollow noreferrer"">this tutorial</a> for reference. My implementation is embedded into a framework which requires an Encoder and a Decoder:</p>
<p>The encoder is like this:</p>
<pre><code>class TransformerEncoder(nn.Module):
    def __init__(
        self,
        vocab_size,
        pad_token_id=None,
        embedding_size=256,
        num_heads=8,
        num_layers=3,
        ffnn_size=512,
        dropout=0.1,
    ):
        super(TransformerEncoder, self).__init__()
        self.vocab_size = vocab_size
        self.pad_token_id = pad_token_id

        self.embedding_size = embedding_size
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.ffnn_size = ffnn_size

        self.embed_tokens = TokenEmbedding(vocab_size, embedding_size)
        self.embed_positions = PositionalEmbedding(embedding_size, dropout=dropout)

        encoder_layer = nn.TransformerEncoderLayer(
            embedding_size,
            num_heads,
            ffnn_size,
            dropout,
        )
        encoder_norm = nn.LayerNorm(embedding_size)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers, encoder_norm)

    def forward(
        self,
        input_ids,
    ):

        # seq_len = input_ids.shape[1]
        # device = next(self.parameters()).device

        embedded_tokens = self.embed_positions(self.embed_tokens(input_ids))
        # B x T x C -&gt; T x B x C
        embedded_tokens = embedded_tokens.transpose(0, 1)

        memory = self.encoder(embedded_tokens)

        return (memory,)
</code></pre>
<p>The decoder is like this:</p>
<pre><code>class TransformerDecoder(nn.Module):
    def __init__(
        self,
        vocab_size,
        pad_token_id=None,
        embedding_size=256,
        num_heads=8,
        num_layers=3,
        ffnn_size=512,
        dropout=0.1,
    ):

        super(TransformerDecoder, self).__init__()
        self.vocab_size = vocab_size
        self.pad_token_id = pad_token_id

        self.embedding_size = embedding_size
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.ffnn_size = ffnn_size

        self.dropout_module = nn.Dropout(p=dropout)

        self.embed_tokens = TokenEmbedding(vocab_size, embedding_size)
        self.embed_positions = PositionalEmbedding(embedding_size, dropout=dropout)

        decoder_layer = nn.TransformerDecoderLayer(
            embedding_size, num_heads, ffnn_size, dropout
        )
        decoder_norm = nn.LayerNorm(embedding_size)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers, decoder_norm)
        self.fc_out = nn.Linear(embedding_size, vocab_size)

    def forward(
        self,
        input_ids,
        encoder_out,
    ):
        seq_len = input_ids.shape[1]

        device = next(self.parameters()).device
        mask = generate_square_subsequent_mask(seq_len).to(device)

        embedded_tokens = self.embed_positions(self.embed_tokens(input_ids))

        # B x T x C -&gt; T x B x C
        embedded_tokens = embedded_tokens.transpose(0, 1)

        output = self.decoder(embedded_tokens, encoder_out[0], tgt_mask=mask)

        # T x B x C -&gt; B x T x C
        output = output.transpose(1, 0)

        return (self.fc_out(output),)
</code></pre>
<p>TokenEmbedding and PositionalEmbedding are as in the tutorial.
The main model just invokes encoder and decoder like:</p>
<pre><code>        encoder_outputs = self.encoder(input_ids=input_ids, **kwargs)

        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            encoder_out=encoder_outputs,
            **kwargs,
        )
</code></pre>
<p>The labels are shifted one token to the right to be fed to the decoder using:</p>
<pre><code>def shift_tokens_right(self, input_ids: torch.Tensor, decoder_start_token_id: int):
   shifted_input_ids = input_ids.new_zeros(input_ids.shape)
   shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()
   shifted_input_ids[:, 0] = decoder_start_token_id
   return shifted_input_ids
</code></pre>
<p>The loss is calculated as:</p>
<pre><code>loss_fct = nn.CrossEntropyLoss(ignore_index=self.pad_token_id)
loss = loss_fct(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))
</code></pre>
<p>The loss is going down, but the generations are real bad. Following is an example of the generations:
<strong>Source:</strong> &lt; s &gt; Can I jailbreak iOS 10 ? &lt; /s &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt;
<strong>Preds:</strong> &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt;
<strong>Target:</strong> &lt; s &gt; Can you jailbreak iOS 10 ? &lt; /s &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt;</p>
<p>As you can see, the predictions in this case are only BOS tokens. The output of the decoder on each decoder step is always almost the same for every iteration. The model does not seem to be learning. I have tried learning rates from 0.1 to 1e-4. For a brief moment at the second or third epoch, there were produced intelligible sentences, but quickly after that the generations reverted back to just BOS or PAD tokens.</p>
<p>Do you have an intuition on what might be wrong? Sorry for the question not being self-contained. Thanks in advance for any help you can provide.</p>
",Text Classification / Sentiment Analysis,make nn transformer work text generation trying make transformer work paraphrase generation generation useful everytime full bos token token followed tutorial reference implementation embedded framework requires encoder decoder encoder like decoder like tokenembedding positionalembedding tutorial main model invokes encoder decoder like label shifted one token right fed decoder using loss calculated loss going generation real bad following example generation source jailbreak io pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad preds target jailbreak io pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad see prediction case bos token output decoder decoder step always almost every iteration model doe seem learning tried learning rate e brief moment second third epoch produced intelligible sentence quickly generation reverted back bos pad token intuition might wrong sorry question self contained thanks advance help provide
NLP - Worse result when adding stemming or lemmitization for Sentiment Analysis,"<p>I'm trying to create a full pipeline of results for sentiment analysis for a smaller subset of the IMDB reviews (only 2k pos, 2k neg) so I'm tryna show results at each stage</p>
<p>i.e. without any pre-processing, then basic cleaning (remove specials, stopwords, lowercasing) then testing both stemming and lemmitization (seperately) on top of the basic cleaning.</p>
<p>After basic cleaning I'm jumping from 50% (only binary classification so makes sense) to mid-to-low 80%'s. Then after adding stemming and lemming, it either doesn't change or for random forest gets the recall below 80%.</p>
<p>Why's this the case? Are my results normal? If so how do you justify using either one?</p>
<p>Also to note all of the models and feature extractions are using default parameters from sklearn so I haven't gotten to the model optimization part, should I try that for these 3 cases and then see if they perform worse?</p>
<p>Feature Extractions: Bag of Words and TF-Idf</p>
<p>Models: SVM, Logistic Regression, Multinomial Naive Bayes and Random Forest</p>
<p>Results:</p>
<p>Basic Cleaning (remove specials, stopwords, lowercasing)</p>
<pre><code>SVM BOW
              precision    recall  f1-score   support

    Positive       0.85      0.85      0.85       530
    Negative       0.83      0.83      0.83       470

    accuracy                           0.84      1000
   macro avg       0.84      0.84      0.84      1000
weighted avg       0.84      0.84      0.84      1000


SVM TF-IDF
              precision    recall  f1-score   support

    Positive       0.85      0.88      0.86       530
    Negative       0.86      0.83      0.84       470

    accuracy                           0.85      1000
   macro avg       0.86      0.85      0.85      1000
weighted avg       0.86      0.85      0.85      1000


LR BOW
              precision    recall  f1-score   support

    Positive       0.87      0.85      0.86       530
    Negative       0.83      0.85      0.84       470

    accuracy                           0.85      1000
   macro avg       0.85      0.85      0.85      1000
weighted avg       0.85      0.85      0.85      1000


LR TF-IDF
              precision    recall  f1-score   support

    Positive       0.89      0.82      0.85       530
    Negative       0.81      0.88      0.84       470

    accuracy                           0.85      1000
   macro avg       0.85      0.85      0.85      1000
weighted avg       0.85      0.85      0.85      1000


MNB BOW
              precision    recall  f1-score   support

    Positive       0.83      0.85      0.84       530
    Negative       0.82      0.81      0.82       470

    accuracy                           0.83      1000
   macro avg       0.83      0.83      0.83      1000
weighted avg       0.83      0.83      0.83      1000


MNB TF-IDF
              precision    recall  f1-score   support

    Positive       0.86      0.84      0.85       530
    Negative       0.82      0.85      0.83       470

    accuracy                           0.84      1000
   macro avg       0.84      0.84      0.84      1000
weighted avg       0.84      0.84      0.84      1000


RFC BOW
              precision    recall  f1-score   support

    Positive       0.85      0.80      0.82       530
    Negative       0.79      0.84      0.81       470

    accuracy                           0.82      1000
   macro avg       0.82      0.82      0.82      1000
weighted avg       0.82      0.82      0.82      1000


RFC TF-IDF
              precision    recall  f1-score   support

    Positive       0.84      0.81      0.83       530
    Negative       0.80      0.83      0.81       470

    accuracy                           0.82      1000
   macro avg       0.82      0.82      0.82      1000
weighted avg       0.82      0.82      0.82      1000

</code></pre>
<p>Basic Cleaning + Stemming</p>
<pre><code>SVM BOW
              precision    recall  f1-score   support

    Positive       0.85      0.82      0.83       530
    Negative       0.80      0.83      0.82       470

    accuracy                           0.82      1000
   macro avg       0.82      0.82      0.82      1000
weighted avg       0.82      0.82      0.82      1000


SVM TF-IDF
              precision    recall  f1-score   support

    Positive       0.85      0.85      0.85       530
    Negative       0.83      0.83      0.83       470

    accuracy                           0.84      1000
   macro avg       0.84      0.84      0.84      1000
weighted avg       0.84      0.84      0.84      1000


LR BOW
              precision    recall  f1-score   support

    Positive       0.85      0.83      0.84       530
    Negative       0.81      0.84      0.83       470

    accuracy                           0.83      1000
   macro avg       0.83      0.83      0.83      1000
weighted avg       0.83      0.83      0.83      1000


LR TF-IDF
              precision    recall  f1-score   support

    Positive       0.89      0.81      0.85       530
    Negative       0.80      0.88      0.84       470

    accuracy                           0.84      1000
   macro avg       0.84      0.85      0.84      1000
weighted avg       0.85      0.84      0.84      1000


MNB BOW
              precision    recall  f1-score   support

    Positive       0.83      0.84      0.84       530
    Negative       0.82      0.81      0.82       470

    accuracy                           0.83      1000
   macro avg       0.83      0.83      0.83      1000
weighted avg       0.83      0.83      0.83      1000


MNB TF-IDF
              precision    recall  f1-score   support

    Positive       0.87      0.83      0.85       530
    Negative       0.82      0.86      0.84       470

    accuracy                           0.84      1000
   macro avg       0.84      0.84      0.84      1000
weighted avg       0.84      0.84      0.84      1000


RFC BOW
              precision    recall  f1-score   support

    Positive       0.84      0.77      0.80       530
    Negative       0.76      0.83      0.79       470

    accuracy                           0.80      1000
   macro avg       0.80      0.80      0.80      1000
weighted avg       0.80      0.80      0.80      1000


RFC TF-IDF
              precision    recall  f1-score   support

    Positive       0.83      0.79      0.81       530
    Negative       0.78      0.81      0.80       470

    accuracy                           0.80      1000
   macro avg       0.80      0.80      0.80      1000
weighted avg       0.80      0.80      0.80      1000
</code></pre>
<p>Basic Cleaning + Lemmitization</p>
<pre><code>SVM BOW
              precision    recall  f1-score   support

    Positive       0.84      0.83      0.83       530
    Negative       0.81      0.82      0.82       470

    accuracy                           0.83      1000
   macro avg       0.83      0.83      0.83      1000
weighted avg       0.83      0.83      0.83      1000


SVM TF-IDF
              precision    recall  f1-score   support

    Positive       0.85      0.86      0.86       530
    Negative       0.84      0.83      0.84       470

    accuracy                           0.85      1000
   macro avg       0.85      0.85      0.85      1000
weighted avg       0.85      0.85      0.85      1000


LR BOW
              precision    recall  f1-score   support

    Positive       0.86      0.84      0.85       530
    Negative       0.82      0.84      0.83       470

    accuracy                           0.84      1000
   macro avg       0.84      0.84      0.84      1000
weighted avg       0.84      0.84      0.84      1000


LR TF-IDF
              precision    recall  f1-score   support

    Positive       0.88      0.81      0.84       530
    Negative       0.80      0.87      0.84       470

    accuracy                           0.84      1000
   macro avg       0.84      0.84      0.84      1000
weighted avg       0.84      0.84      0.84      1000


MNB BOW
              precision    recall  f1-score   support

    Positive       0.82      0.85      0.83       530
    Negative       0.82      0.80      0.81       470

    accuracy                           0.82      1000
   macro avg       0.82      0.82      0.82      1000
weighted avg       0.82      0.82      0.82      1000


MNB TF-IDF
              precision    recall  f1-score   support

    Positive       0.85      0.83      0.84       530
    Negative       0.81      0.84      0.82       470

    accuracy                           0.83      1000
   macro avg       0.83      0.83      0.83      1000
weighted avg       0.83      0.83      0.83      1000


RFC BOW
              precision    recall  f1-score   support

    Positive       0.84      0.78      0.81       530
    Negative       0.77      0.83      0.80       470

    accuracy                           0.80      1000
   macro avg       0.80      0.81      0.80      1000
weighted avg       0.81      0.80      0.80      1000


RFC TF-IDF
              precision    recall  f1-score   support

    Positive       0.84      0.81      0.82       530
    Negative       0.80      0.82      0.81       470

    accuracy                           0.82      1000
   macro avg       0.82      0.82      0.82      1000
weighted avg       0.82      0.82      0.82      1000

</code></pre>
",Text Classification / Sentiment Analysis,nlp worse result adding stemming lemmitization sentiment analysis trying create full pipeline result sentiment analysis smaller subset imdb review k po k neg tryna show result stage e without pre processing basic cleaning remove special stopwords lowercasing testing stemming lemmitization seperately top basic cleaning basic cleaning jumping binary classification make sense mid low adding stemming lemming either change random forest get recall case result normal justify using either one also note model feature extraction using default parameter sklearn gotten model optimization part try case see perform worse feature extraction bag word tf idf model svm logistic regression multinomial naive bayes random forest result basic cleaning remove special stopwords lowercasing basic cleaning stemming basic cleaning lemmitization
Node: &#39;Cast_1&#39; Cast string to float is not supported [[{{node Cast_1}}]] [Op:__inference_train_function_24202] for Roberta,"<p>I am getting this problem called &quot;</p>
<pre><code>Node: 'Cast_1'
Cast string to float is not supported
     [[{{node Cast_1}}]] [Op:__inference_train_function_24202]
</code></pre>
<p>&quot;
<a href=""https://i.sstatic.net/SyyFK.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>

</code></pre>
<p>I wrote a code about IMDB sentiment analysis for 5000 data in Google Colab</p>
<pre><code>#importing the necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

#importing the data
data = pd.read_excel('/content/drive/MyDrive/499A_Project/Dataset/IMDB5000.xlsx')

#spliting the dataset into train and test
train_data, test_data = train_test_split(data, test_size = 0.3, random_state = 42)

#preprocessing the data
#tokenizng the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['Review'].values)

#converting the text into sequences
train_sequences = tokenizer.texts_to_sequences(train_data['Review'].values)
test_sequences = tokenizer.texts_to_sequences(test_data['Review'].values)


#padding the sequences
max_length = max([len(s.split()) for s in data['Review']])
train_padded = pad_sequences(train_sequences, maxlen = max_length)
test_padded = pad_sequences(test_sequences, maxlen = max_length)

#preparing the labels
train_labels = train_data['Sentiment'].values
test_labels = test_data['Sentiment'].values

#importing Roberta model from transformers
from transformers import TFBertForSequenceClassification

#instantiating the Roberta model
model = TFBertForSequenceClassification.from_pretrained('roberta-base')

#compiling the model
model.compile(loss = 'sparse_categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

#training the model
model.fit(train_padded, train_labels,
          batch_size = 32,
          epochs = 10,
          validation_data = (test_padded, test_labels))


</code></pre>
<p>This is the code I wrote for my dataset but it is not working and show the erros</p>
",Text Classification / Sentiment Analysis,node cast cast string float supported node cast op inference train function roberta getting problem called enter image description wrote code imdb sentiment analysis data google colab code wrote dataset working show erros
"How to combine X_test, y test, and y predictions after text analytics prediction?","<p>After using logitics Reg on text analytics, I was trying to combine the X_test, y_arr_test (label), and y_predictions to ONE dataframe, but don't know how to do it. Need help.</p>
<p>'''</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer 
vectorizer = CountVectorizer()
vectorizer.fit(X_arr_train)
X_train = vectorizer.transform(X_arr_train)
X_test  = vectorizer.transform(X_arr_test)
X_train 
</code></pre>
<p>'''</p>
<p>'''</p>
<pre><code># logistic Reg
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_arr_train)
score = classifier.score(X_test, y_arr_test) 
y_predictions=classifier.predict(X_test)
</code></pre>
<p>'''</p>
<p><code>X_test</code>
return: &lt;1333x5676 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
with 26934 stored elements in Compressed Sparse Row format&gt;</p>
<p><code>np.shape(y_arr_test)</code>
return: (1333,)</p>
<p>Then I dont know how to put X_test, y_arr_test (label), and y_predictions to ONE dataframe. The goal is to show the wrong predictions and know why.
Thanks.</p>
",Text Classification / Sentiment Analysis,combine x test test prediction text analytics prediction using logitics reg text analytics wa trying combine x test arr test label prediction one dataframe know need help return x sparse matrix type class numpy int stored element compressed sparse row format return dont know put x test arr test label prediction one dataframe goal show wrong prediction know thanks
Pythonic way to create dataset for multilabel text classification,"<p>I have a text dataset that looks like this.</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'Sentence': ['Hello World',
                                'The quick brown fox jumps over the lazy dog.',
                                'Just some text to make third sentence!'
                               ],
                   'label': ['greetings',
                             'dog,fox',
                             'some_class,someother_class'
                            ]})
</code></pre>
<p><a href=""https://i.sstatic.net/aNbAW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aNbAW.jpg"" alt=""enter image description here"" /></a></p>
<p>I want to transform this data into something like this.
<a href=""https://i.sstatic.net/9jgsV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9jgsV.jpg"" alt=""This is how dataframe should look like after transformation."" /></a></p>
<p>Is there a pythonic way to make this transformation for multilabel classification?</p>
",Text Classification / Sentiment Analysis,pythonic way create dataset multilabel text classification text dataset look like want transform data something like pythonic way make transformation multilabel classification
How can I classify HTML Files?,"<p>I am trying to classify my HTML files based on their contents. Using JSoup, I have retrieved the title and description portion of the HTML file. And then, using the opennlp Sentence Detector I have identified an array of sentences.</p>

<p>However, I am not sure how to proceed further. I can simply look for certain keywords in those sentences and do the classification, but then again that feels like I am writing a simple <code>if..else..</code> statement without using the full potential of NLP.</p>

<p>I would like to train my code to do the classification, but I am not sure how that can be achieved.</p>
",Text Classification / Sentiment Analysis,classify html file trying classify html file based content using jsoup retrieved title description portion html file using opennlp sentence detector identified array sentence however sure proceed simply look certain keywords sentence classification feel like writing simple statement without using full potential nlp would like train code classification sure achieved
Hyperparameter search criteria - how to define?,"<p>I’m finetuning a language model (vesteinn/ScandiBERT) on a multiclass text classification (pos, neg, neu) task and I wish to run a hyperparameter search to find the optimal hyperparameters. As I’m starting to read up on it I realize that HP tuning is a wild west with no consensus on which HP to tune on and which ranges to specify for these. I’m not an NLP expert so I might be missing some conceptual knowledge which is why I’m hoping that one of you would take a look at how I initiate my HP tuning using Optuna:
I looks like this:</p>
<pre><code>!pip install -q optuna

def model_init():
    return model #model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=3) (defined earlier)

training_args = TrainingArguments(output_dir=model_name, 
                                  evaluation_strategy = &quot;epoch&quot;,
                                  save_strategy = &quot;epoch&quot;, 
                                  num_train_epochs = epochs, 
                                  per_device_train_batch_size = batch_size,
                                  per_device_eval_batch_size = batch_size,
                                  learning_rate = learning_rate,
                                  weight_decay=0.01,
                                  load_best_model_at_end=True,
                                  report_to=&quot;wandb&quot;)

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;test&quot;],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

def my_hp_space(trial):
    return {
        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-6, 1e-4, log=True),
        &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 5, 15),
        #&quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 1, 40),
        &quot;per_device_train_batch_size&quot;: trial.suggest_categorical(&quot;per_device_train_batch_size&quot;, [4, 8, 16, 32, 64]),
        &quot;weight_decay&quot;: trial.suggest_loguniform('weight_decay', 1e-4, 1e-2)
    }

import optuna
from optuna.samplers import TPESampler
sampler = optuna.samplers.TPESampler()
pruner = optuna.pruners.SuccessiveHalvingPruner()

best_run = trainer.hyperparameter_search(
    n_trials=100, 
    direction=&quot;maximize&quot;, 
    hp_space=my_hp_space, 
    backend = &quot;optuna&quot;,
    sampler = sampler,
    pruner = pruner
    )

for n, v in best_run.hyperparameters.items():
    setattr(trainer.args, n, v) # for running the experiment with the best hyperparameters from the hyperparameters search

trainer.train() # argument trial can be used for hyperparameter search
</code></pre>
<p>My code does run but I’m uncertain of whether I’m missing anything important in my value definitions. Especially since the trial gets pruned from run 3 - 99.</p>
",Text Classification / Sentiment Analysis,hyperparameter search criterion define finetuning language model vesteinn scandibert multiclass text classification po neg neu task wish run hyperparameter search find optimal hyperparameters starting read realize hp tuning wild west consensus hp tune range specify nlp expert might missing conceptual knowledge hoping one would take look initiate hp tuning using optuna look like code doe run uncertain whether missing anything important value definition especially since trial get pruned run
Minimum term/doc frequency for Seeded Latent Dirichlet Allocation:?,"<p>I'm a newbie at language technology using R's <code>seededlda</code> package to perform SLDA on a document term matrix created from a corpus of ~400k texts (scientific abstracts) to classify each document into one of two categories, say category &quot;A&quot; and &quot;B&quot;.</p>
<p>I get wildly different results based on how I shorten the document term matrix. I first decided to limit the DTM to words used at least 1,000 times in 10,000 different documents. This got me some weird results on the distribution of categories across documents--based on my knowledge of the problem I'm studying, I was expecting category A to outweigh B at least 1.5:1, if not 2:1 or greater. Instead, Category B outweighed A 3:1.</p>
<p>However, if I do not impose any limits on term and document frequency, I get results that align with my prior expectations. Since I'm not a computational linguist, I'm not sure which &quot;approach&quot; is more desirable.</p>
<p><strong>Is there any literature out there on &quot;optimal&quot; minimum document/term frequency when performing (seeded) LDA?</strong> On a quick Google search, I haven't seem to find anything that would help me decide which of my results are closer to the &quot;correct&quot; answer, or what the threshold should be for cutting terms by document or term frequency.</p>
",Text Classification / Sentiment Analysis,minimum term doc frequency seeded latent dirichlet allocation newbie language technology using r package perform slda document term matrix created corpus k text scientific abstract classify document one two category say category b get wildly different result based shorten document term matrix first decided limit dtm word used least time different document got weird result distribution category across document based knowledge problem studying wa expecting category outweigh b least greater instead category b outweighed however impose limit term document frequency get result align prior expectation since computational linguist sure approach desirable literature optimal minimum document term frequency performing seeded lda quick google search seem find anything would help decide result closer correct answer threshold cutting term document term frequency
Google Cloud Vertex AI with .Net,"<p>I am new to google cloud service VERTEX AI.</p>
<p>I am looking to Create, train, and deploy an AutoML text classification model through .Net application. I did not find anything for .Net with Vertex AI. If someone can please guide my to the location or any .Net code samples, will be really helpful.</p>
",Text Classification / Sentiment Analysis,google cloud vertex ai net new google cloud service vertex ai looking create train deploy automl text classification model net application find anything net vertex ai someone please guide location net code sample really helpful
NLTK: Adding negative words for sentiment analysis,"<p>I am working on Sentiment Analysis using nltk and SpaCy. While working, I need to add new words into the negative variables so that it will show negative polarity value when those words appears in any sentence. I don't know how to do that, could someone help me please?</p>
",Text Classification / Sentiment Analysis,nltk adding negative word sentiment analysis working sentiment analysis using nltk spacy working need add new word negative variable show negative polarity value word appears sentence know could someone help please
masked image and language modelling using visualbert,"<p>I was coding this <a href=""https://colab.research.google.com/drive/1Q7e7IzFlnnjWArlIJHhcHjzVNI0BJVD6?usp=sharing"" rel=""nofollow noreferrer"">piece</a> of code which heavily relies on the <a href=""https://github.com/huggingface/transformers/blob/main/examples/research_projects/visual_bert/demo.ipynb"" rel=""nofollow noreferrer"">demo</a> of visual question answering, and I'm masking inputs while feeding it to the bert using [MASK] token, and providing a label which accompanies the mask. Visual embeddings are being extracted through rcnn, giving me 36 such vectors, in which I'm taking the mean of all 36 vectors as shown below :</p>
<pre><code>features = torch.mean(output_dict.get(&quot;roi_features&quot;), axis=1).reshape(1,1,2048)
</code></pre>
<p>which is being fed to the visualbert for pretraining model, thus giving me prediction_logits. So, now as you can see in the notebook and here too, after taking argmax, prediction logits are :</p>
<pre><code>prediction_logits[0].argmax(-1)

&gt;&gt; tensor([1012, 1037, 6302, 1997, 1037, 5723, 1012, 2003])
</code></pre>
<p>Now, when I'm trying to get words using the above predictions and the vocabulary of the tokenizer, this is what's being outputted :</p>
<pre><code>.
a
photo
of
a
bathroom
.
is
</code></pre>
<p>Instead of bathroom, I should've got cat or atleast near cat but there seems to be difference of 10 values between bathroom (which is voted highest in our output, with score of 9.5069) and cat (with a score of 6.3830). Can we somehow get the score of cat up and make it most desirable output?</p>
",Text Classification / Sentiment Analysis,masked image language modelling using visualbert wa coding piece code heavily relies demo visual question answering masking input feeding bert using mask token providing label accompanies mask visual embeddings extracted rcnn giving vector taking mean vector shown fed visualbert pretraining model thus giving prediction logits see notebook taking argmax prediction logits trying get word using prediction vocabulary tokenizer outputted instead bathroom got cat atleast near cat seems difference value bathroom voted highest output score cat score somehow get score cat make desirable output
Looking for dataset for sentiment analysis that consists of sentences with slang words,"<p>I am developing a machine learning model to predict the sentiment polarity of customers' comments about some product.</p>
<p>Currently, I use the pretrained <a href=""https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment"" rel=""nofollow noreferrer"">twitter-roberta-base-sentiment</a> as the base model.</p>
<p>It is works well most of the time except when predicting text contains slang words.</p>
<p>For example, it predict &quot;The product is idiot proof.&quot; wrongly as Negative.</p>
<p>So, I want to add some labeled example sentences contains slang words into the training dataset in order to improve the model's performance at sentences contains slang.</p>
<p>For example:</p>
<pre class=""lang-json prettyprint-override""><code>[
{&quot;doc&quot;:&quot;I am having a blast with this game.&quot;, &quot;sentiment&quot;: &quot;Postive&quot;},
{&quot;doc&quot;:&quot;This game is like pigeon chess&quot;, &quot;sentiment&quot;: &quot;Negative&quot;},
...
]
</code></pre>
<p>I found <a href=""http://liangwu.me/slangsd/"" rel=""nofollow noreferrer"">SlangSD</a>, a sentiment lexicon of slang words. For my project, it has 2 drawback as a training dataset.</p>
<ol>
<li>it has only words, not sentences in each entry;</li>
<li>it contains not only slang words but also many ordinary words, such as &quot;have&quot;,&quot;project&quot;,&quot;dictionary&quot;,etc.</li>
</ol>
",Text Classification / Sentiment Analysis,looking dataset sentiment analysis consists sentence slang word developing machine learning model predict sentiment polarity customer comment product currently use pretrained twitter roberta base sentiment base model work well time except predicting text contains slang word example predict product idiot proof wrongly negative want add labeled example sentence contains slang word training dataset order improve model performance sentence contains slang example found slangsd sentiment lexicon slang word project ha drawback training dataset ha word sentence entry contains slang word also many ordinary word project dictionary etc
"Cross Validation, weight initialization and early stopping in spacy 3.0 without using config","<p>I am using spacy for a text classification problem and need to achieve the following in code:</p>
<ol>
<li>early stopping</li>
<li>cross-validation</li>
<li>set seed for weight initialization</li>
</ol>
<p>For cross-validation, I am currently manually calling the <code>evaluate</code> function on the validation dataset after each epoch. Something like:</p>
<pre><code>for i in range(25):
    losses={}
    
    for batch in minibatch():
        nlp.update(batch, optimizer, losses=losses) #need a way to provide early stopping param and seed for weights here
    
    print(losses['textcat'])
    
    #validation phase
    score = nlp.evaluate(validation_data_set)
    print('validation score', score)
</code></pre>
<p>Is there a way to do this programmatically (without using the *.cfg file)? I am having some issues using that as of now</p>
",Text Classification / Sentiment Analysis,cross validation weight initialization early stopping spacy without using config using spacy text classification problem need achieve following code early stopping cross validation set seed weight initialization cross validation currently manually calling function validation dataset epoch something like way programmatically without using cfg file issue using
Selecting algorithms to test their performance on sentiment analysis performance,"<p>Hello and thank you for your time. I am a budding data analyst and working on my first extensive project which is on NLP and sentiment analysis.</p>
<p>I have made use of Random Forest, Multinominal Naive Bayes, BERT and such models. However, I am trying to implement logistic regression too and came across the codes for it but they don't mention &quot;Multinominal&quot;.  When I read online, I can see there exists Multinominal Logistic Regression.</p>
<p>My question is : DO I USE LOGISTIC REGRESSION OR MULTINOMINAL LOGISTIC REGRESSION? WHAT'S THE DIFFERENCE BETWEEN THEM? THEY TALK ABOUT PROBABILITY BEING BINARY? HOW AND WHAT DO I CHOOSE SINCE SENTIMENT CONTAIN &quot;NEUTRAL&quot; EMOTIONS ALONG WITH &quot;POSITIVE&quot; AND &quot;NEGATIVE&quot; EMOTIONS AND AREN'T BINARY?</p>
",Text Classification / Sentiment Analysis,selecting algorithm test performance sentiment analysis performance hello thank time budding data analyst working first extensive project nlp sentiment analysis made use random forest multinominal naive bayes bert model however trying implement logistic regression came across code mention multinominal read online see exists multinominal logistic regression question use logistic regression multinominal logistic regression difference talk probability binary choose since sentiment contain neutral emotion along positive negative emotion binary
NLP Paragraph Level Predictions vs Doc Level Predictions? What Strategy to deploy,"<p>currently wanted to understand which model an approach I am incorporating for model development, I currently have a TF-IDF NLP model that reads in paragraphs for a document and makes a prediction based upon how many paragraphs scored a 1 label with that paragraph.</p>
<p>I am not sure if that is correct form of logic, should I just go with an document level model? what are the benefits and trade-offs of predicting at a paragraph level and rolling it up into a total prediction for the document vs just classifying the document itself.</p>
<p>Any Thoughts?</p>
<p>Thanks!</p>
",Text Classification / Sentiment Analysis,nlp paragraph level prediction v doc level prediction strategy deploy currently wanted understand model approach incorporating model development currently tf idf nlp model read paragraph document make prediction based upon many paragraph scored label paragraph sure correct form logic go document level model benefit trade offs predicting paragraph level rolling total prediction document v classifying document thought thanks
"Tensoflow: Input 0 of layer &quot;conv1d_17&quot; is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16)","<p>Although I could find similar questions, I have not been able to fix this issue.</p>
<p>I am trying to classify text, but got this error:</p>
<p>Input 0 of layer &quot;conv1d_17&quot; is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16)</p>
<blockquote>
<p>Blockquote
# Training a Text Classification Model using a Convolutional Layer</p>
</blockquote>
<pre><code>from sklearn.model_selection import train_test_split

import pandas as pd

import numpy as np

df = pd.read_csv(&quot;Financeiro_Teste_V3.csv&quot;)

df.head(3)

df.info()

df = df.dropna()

X = df.loc[:, &quot;HISTÓRICO&quot;]
X

y = df.loc[:, ['PROCESSO']]
y.head(7)

training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(X, y, random_state=0, train_size = .75)

training_sentences = training_sentences.to_numpy()

testing_sentences = testing_sentences.to_numpy() 

# testing_sentences

#for x in testing_sentences:
#  print(type(x), x)

## Data preprocessing

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 10000
max_length = 120
trunc_type='post'
padding_type='post'
oov_tok = &quot;&lt;OOV&gt;&quot;

# Initialize the Tokenizer class
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)

# Generate the word index dictionary
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index

# Generate and pad the training sequences
training_sequences = tokenizer.texts_to_sequences(training_sentences)
training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Generate and pad the testing sequences
testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Convert the labels lists into numpy arrays
training_labels = np.array(training_labels)
testing_labels = np.array(testing_labels)

## Build and Compile the Model

import tensorflow as tf

# Parameters
embedding_dim = 16
filters = 128
kernel_size = 1
dense_dim = 6

# Model Definition with Conv1D
model_conv = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(9, activation='softmax')
])

# Set the training parameters
    model_conv.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',metrics=['accuracy'])
    
    # Print the model summary
    model_conv.summary()
    
    ## Train the Model
    
    NUM_EPOCHS = 20
    
    # Train the model
    history_conv = model_conv.fit(training_padded, training_labels, epochs=NUM_EPOCHS, validation_data=(testing_padded, testing_labels))
    
    import matplotlib.pyplot as plt
    
    # Plot Utility
    def plot_graphs(history, string):
      plt.plot(history.history[string])
      plt.plot(history.history['val_'+string])
      plt.xlabel(&quot;Epochs&quot;)
      plt.ylabel(string)
      plt.legend([string, 'val_'+string])
      plt.show()
    
    # Plot the accuracy and loss history
    plot_graphs(history_conv, 'accuracy')
    plot_graphs(history_conv, 'loss')
    
    model_conv.predict(testing_sentences)
      
WARNING:tensorflow:Model was constructed with shape (None, 120) for input KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.float32, name='embedding_3_input'), name='embedding_3_input', description=&quot;created by layer 'embedding_3_input'&quot;), but it was called on an input with incompatible shape (None,).
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-69-98046d2f0914&gt; in &lt;module&gt;
----&gt; 1 model_conv.predict(testing_sentences)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1145           except Exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1801, in predict_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1790, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1783, in run_step  **
        outputs = model.predict_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1751, in predict_step
        return self(x, training=False)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py&quot;, line 228, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; '

    ValueError: Exception encountered when calling layer &quot;sequential_3&quot; (type Sequential).
    
    Input 0 of layer &quot;conv1d_3&quot; is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 16)
</code></pre>
",Text Classification / Sentiment Analysis,tensoflow input layer conv incompatible layer expected min ndim found ndim full shape received none although could find similar question able fix issue trying classify text got error input layer conv incompatible layer expected min ndim found ndim full shape received none blockquote training text classification model using convolutional layer
NLP text classification (flair),"<p>So I have basic question which I did not get answered by reading the documentation.</p>
<ol>
<li><p>If I want to do text classification (sentiment) about lets say an Articel with a topic.
I already have the plain text without the html stuff through python libraries. Is it better  to make an analysis about each sentence and then combine the results or just pass the whole text as a string and have already combined result (I have already tried the whole text option with flair and it worked quite well I guess).</p>
</li>
<li><p>The next thing would be how you could check if the sentences are about the asked topic and how to check it.</p>
</li>
</ol>
<p>If you could give me some guidelines or hints how to approach these problems I would be happy.</p>
",Text Classification / Sentiment Analysis,nlp text classification flair basic question get answered reading documentation want text classification sentiment let say articel topic already plain text without html stuff python library better make analysis sentence combine result pas whole text string already combined result already tried whole text option flair worked quite well guess next thing would could check sentence asked topic check could give guideline hint approach problem would happy
How to get the word on which the text classification has been made?,"<p>I am doing a multi-label text classification using a pre-trained model of BERT. Here is an example of the prediction that has been made for one sentence-
<a href=""https://i.sstatic.net/phwu9.jpg"" rel=""nofollow noreferrer"">pred_image</a></p>
<p>I want to get those words from the sentence on which the prediction has been made. Like this one - <a href=""https://i.sstatic.net/Xc4ij.jpg"" rel=""nofollow noreferrer"">right_one</a></p>
<p>If anyone has any idea, Please enlighten me.</p>
",Text Classification / Sentiment Analysis,get word text classification ha made multi label text classification using pre trained model bert example prediction ha made one sentence pred image want get word sentence prediction ha made like one right one anyone ha idea please enlighten
Get closest text entry of a list from a string,"<p>I am trying to build a RNN model for text classification and I am currently building my dataset.
I am trying to do some of the work automatically and I'm using an API that gets me some information for each text I send to it.
So basically :
I have, for each text on my dataframe, I have a df['label'] that contain a 1 to 3 word string.
I have a list of vocabulary (my futur classes) and for each on the df['label'] item, and want to attribute one of the vocabulary list item, depending on which is closest in meaning.
So I need to measure how close each of the labels are close in meaning to my vocabulary list.
Any help ?</p>
",Text Classification / Sentiment Analysis,get closest text entry list string trying build rnn model text classification currently building dataset trying work automatically using api get information text send basically text dataframe df label contain word string list vocabulary futur class df label item want attribute one vocabulary list item depending closest meaning need measure close label close meaning vocabulary list help
Is it possible to evaluate confidence score from accuracy of an NVIDIA NeMo Text Classification model?,"<p>Is it possible to calculate the Confidence score while predicting the class labels from the NVIDIA NeMo Text Classification model?</p>
<p>I have attached the script for predicting the accuracy of the Nemo Text Classification model for reference.</p>
<pre><code># Test the model
with torch.no_grad():
    correct = 0
    total = 0
    for i, batch in enumerate(validation_dataloader):
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch
        # Forward pass
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
        # print (outputs)
        prediction = torch.argmax(outputs[0],dim=1)
        total += b_labels.size(0)
        correct+=(prediction==b_labels).sum().item()

print('Test Accuracy of the model on the data is: {} %'.format(100 * correct / total))
</code></pre>
",Text Classification / Sentiment Analysis,possible evaluate confidence score accuracy nvidia nemo text classification model possible calculate confidence score predicting class label nvidia nemo text classification model attached script predicting accuracy nemo text classification model reference
Is it necessary to mitigate class imbalance problem in multiclass text classification?,"<p>I am performing multi-class text classification using BERT in python. The dataset that I am using for retraining my model is highly imbalanced. Now, I am very clear that the class imbalance leads to a poor model and one should balance the training set by undersampling, oversampling, etc. before model training.</p>
<p>However, it is also a fact that the distribution of the training set should be similar to the distribution of the production data.</p>
<p>Now, if I am sure that the data thrown at me in the production environment will also be imbalanced, i.e., the samples to be classified will likely belong to one or more classes as compared to some other classes, should I balance my training set?</p>
<p>OR</p>
<p>Should I keep the training set as it is as I know that the distribution of the training set is similar to the distribution of data that I will encounter in the production?</p>
<p>Please give me some ideas, or provide some blogs or papers for understanding this problem.</p>
",Text Classification / Sentiment Analysis,necessary mitigate class imbalance problem multiclass text classification performing multi class text classification using bert python dataset using retraining model highly imbalanced clear class imbalance lead poor model one balance training set undersampling oversampling etc model training however also fact distribution training set similar distribution production data sure data thrown production environment also imbalanced e sample classified likely belong one class compared class balance training set keep training set know distribution training set similar distribution data encounter production please give idea provide blog paper understanding problem
NLP Text Classification Model with defined categories / context / intent,"<p>This is more of a guideline question rather than a technical query. I am looking to create a classification model that classifies documents based on a specific list of strings. However, as it turns out from the data, that that output is more useful when you pass labelled intents / topics, instead of allowing the model to guess. Based on my research so far, I found that Bertopic might be the right way to achieve this as it allows guided topic modeling but the only caveat is that the guided topics should contain words of similar meaning (see link below).</p>
<p>It might be more clear from the example below on what I want to achieve. Suppose we have a chat text extract from a conversation between a customer and store associate, below is what the customer asking for.</p>
<blockquote>
<p>Hi, the product I purchased from your store is defective and I would like to get it replaced or refunded. Here is the receipt number and other details...</p>
</blockquote>
<p>If my list of intended labels is as follows: ['defective or damaged', 'refund request', 'wrong label description',...], we can see that the above extract qualifies into 'defective or damaged' and 'refund request'. For the sake of simplicity, I would pick the one that model returns with highest score so that we only have 1 label per request. Now, if my data does not have these labels defined, I may be able to use zero-shot classification to &quot;best guess&quot; the intent from this list. However, as I understand the use of zero-shot classification or even the guided topic modeling of BERTopic, the categories that I want above may not be derived since the individual words in those categories do not mean that same.</p>
<p>For example, in BERTopic, an intended classified label could be like [&quot;space&quot;, &quot;launch&quot;, &quot;orbit&quot;, &quot;lunar&quot;] for a &quot;Space Related&quot; topic, but in my case let us say for the 3rd label it would be [&quot;wrong&quot;, &quot;label&quot;, &quot;description&quot;] which would not be best suited as it would try to find all records that have mentions of wrong address, wrong department, wrong color etc., so I am essentially looking for a combination of those 3 words in the context. Also, those 3 words may not always be together or in same order. For example, in this sentence -</p>
<blockquote>
<p>The item had description which was labelled incorrectly.</p>
</blockquote>
<p>This same challenge would be for zero-shot classification where the labels are expected to be one word or a combination of words that mean the same thing. Let me know if this clarifies the question more or if I can help further clarify it.</p>
<p>Approach mentioned above:</p>
<p><a href=""https://maartengr.github.io/BERTopic/getting_started/guided/guided.html#semi-supervised-topic-modeling"" rel=""nofollow noreferrer"">https://maartengr.github.io/BERTopic/getting_started/guided/guided.html#semi-supervised-topic-modeling</a></p>
",Text Classification / Sentiment Analysis,nlp text classification model defined category context intent guideline question rather technical query looking create classification model classifies document based specific list string however turn data output useful pas labelled intent topic instead allowing model guess based research far found bertopic might right way achieve allows guided topic modeling caveat guided topic contain word similar meaning see link might clear example want achieve suppose chat text extract conversation customer store associate customer asking hi product purchased store defective would like get replaced refunded receipt number detail list intended label follows defective damaged refund request wrong label description see extract qualifies defective damaged refund request sake simplicity would pick one model return highest score label per request data doe label defined may able use zero shot classification best guess intent list however understand use zero shot classification even guided topic modeling bertopic category want may derived since individual word category mean example bertopic intended classified label could like space launch orbit lunar space related topic case let u say rd label would wrong label description would best suited would try find record mention wrong address wrong department wrong color etc essentially looking combination word context also word may always together order example sentence item description wa labelled incorrectly challenge would zero shot classification label expected one word combination word mean thing let know clarifies question help clarify approach mentioned
"Sentiment Analysis instead of Positive Negative Neutral, I want to sentiment by category of products","<p>Im doing a categorical product sentiment analysis. But I dont quite know what keyword to search for and what methods should I use. Im using this dataset for this <a href=""https://huggingface.co/datasets/viewer/?dataset=amazon_reviews_multi&amp;config=en"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/viewer/?dataset=amazon_reviews_multi&amp;config=en</a> Im doint a neural search project and my model would be the Review text and product category.</p>
",Text Classification / Sentiment Analysis,sentiment analysis instead positive negative neutral want sentiment category product im categorical product sentiment analysis dont quite know keyword search method use im using dataset im doint neural search project model would review text product category
how to classified and digitalized huge amount of paper using python,"<p>I have an archive papers in a company representing different business operation form different sections.
I want to scan all these documents and after that I want a way to classify all these scanned document into different category and sub-category based on custom preference such as (name, age, section, ..etc).</p>
<p>I want the end result to be digital files categorized according to the preferences that I set.</p>
<p>How can I do this using <strong>Python NLP</strong> or any other <strong>machine learning approach</strong></p>
",Text Classification / Sentiment Analysis,classified digitalized huge amount paper using python archive paper company representing different business operation form different section want scan document want way classify scanned document different category sub category based custom preference name age section etc want end result digital file categorized according preference set using python nlp machine learning approach
Is It Fundamentally Correct To The Text Classification Model To Train First Without Pre-Trained Word Vectors And Then With Pre-Trained Word Vectors?,"<p>Is this solution fundamentally correct to the text classification (sentiment analysis) model to train it by these three steps:</p>
<ol>
<li>train the model without pre-trained word vectors until reaches the minimum loss or maximum accuracy or even stopped by a callback.</li>
<li>get the embedding layer weights and substitute the weights of the words which are represented in pre-trained word vectors and lock the embedding layer.</li>
<li>train the model again with the embedding layer which includes weights of the known pre-trained word vectors represented in it and unknown pre-trained word vectors based on the previous training.</li>
</ol>
<p>thanks for your companion</p>
",Text Classification / Sentiment Analysis,fundamentally correct text classification model train first without pre trained word vector pre trained word vector solution fundamentally correct text classification sentiment analysis model train three step train model without pre trained word vector reach minimum loss maximum accuracy even stopped callback get embedding layer weight substitute weight word represented pre trained word vector lock embedding layer train model embedding layer includes weight known pre trained word vector represented unknown pre trained word vector based previous training thanks companion
Is This Solution Fundamentally Correct To The Text Classification Model With Pre-Trained Word Vectors?,"<p>Is it fundamentally correct to training text classification (sentiment analysis) model with pre-trained word vectors; first with the locked embedding layer, and then train again with locked additional layers and unlocked embedding layer?</p>
<p>as you know there are so many words that not exist in the pre-trained word vectors, so by creating embedding layer weights with zeros elements, the model would miss these words same as out of vocabulary words?</p>
<p>or even how about the other same ways; define callback to lock and unlock layers on the end of each epoch sequentially, or even lock only the dense classifier layers and unlock the embedding layer and  the other ones.</p>
<p>thanks for your companion</p>
",Text Classification / Sentiment Analysis,solution fundamentally correct text classification model pre trained word vector fundamentally correct training text classification sentiment analysis model pre trained word vector first locked embedding layer train locked additional layer unlocked embedding layer know many word exist pre trained word vector creating embedding layer weight zero element model would miss word vocabulary word even way define callback lock unlock layer end epoch sequentially even lock dense classifier layer unlock embedding layer one thanks companion
Using for loop to search through string and create data frame,"<p>I am trying to use openNLP to look through rows of text and classify sentences into thematic buckets. Here is a sample df:</p>
<pre><code>dat &lt;- data.frame(text=c(&quot;A fluffy crab discovered off the coast of Western Australia has been named after the ship that carried Charles Darwin around the world. The new species, Lamarckdromia beagle, belongs to the Dromiidae family, commonly known as sponge crabs. Crustaceans in this family fashion and use sea sponges and ascidians – animals including sea squirts – for protection. They trim the creatures using their claws and wear them like hats. &quot;,
                         &quot;The inadmissibility of such actions, which violate the relevant legal and political obligations of the European Union and lead to an escalation of tensions, was pointed out, the ministry said in a statement. Speaking shortly after the meeting, Ederer said he had called on the Russian government to remain calm and resolve this issue diplomatically, the Russian news agency Tass reported.&quot;),
                  date=c(as.Date(&quot;2020-12-26&quot;),as.Date(&quot;2020-12-31&quot;)), 
                  id= c(&quot;1&quot;, &quot;2&quot;))
</code></pre>
<p>Ive gotten as splitting the text into sentences, and then searching for the keywords using the following code:</p>
<pre><code>
#split sentences search for keywords

all_sentence &lt;- as.String(dat$text)

sent_annotator &lt;- Maxent_Sent_Token_Annotator()
annotation &lt;- annotate(all_sentence, sent_annotator)

split_text &lt;- all_sentence[annotation]

# word list to search for 

word_dat &lt;- data.frame(words=c(&quot;animal&quot;, &quot;species&quot;, &quot;political&quot;, &quot;government&quot;),
                  theme=c(&quot;nature&quot;, &quot;nature&quot;, &quot;geopolitics&quot;, &quot;geopolitics&quot;))

stem_keyword &lt;- wordStem(word_dat$words, language = &quot;english&quot;)


for(kw in stem_keyword) {
  x=grep(kw, split_text)
  print(split_text[x])
  print(stem_keyword[x])
}
</code></pre>
<p>However my for loop doesnt print exactly what im looking for.. for example, print(stem_keyword) is giving me the wrong keyword for the wrong sentence. In the end I dont want to print, I want to write the results to a new dataframe with this structure:</p>
<pre><code>final_df &lt;- data.frame(text=c(&quot;A fluffy crab discovered off the coast of Western Australia has been named after the ship that carried Charles Darwin around the world.&quot;, &quot;The new species, Lamarckdromia beagle, belongs to the Dromiidae family, commonly known as sponge crabs.&quot;,&quot;Crustaceans in this family fashion and use sea sponges and ascidians – animals including sea squirts – for protection.&quot;, &quot;They trim the creatures using their claws and wear them like hats.&quot;,
                              &quot;The inadmissibility of such actions, which violate the relevant legal and political obligations of the European Union and lead to an escalation of tensions, was pointed out, the ministry said in a statement.&quot;,
                              &quot;Speaking shortly after the meeting, Ederer said he had called on the Russian government to remain calm and resolve this issue diplomatically, the Russian news agency Tass reported.&quot;),
                  keyword=c(&quot;null&quot;, &quot;species&quot;, &quot;animal&quot;, &quot;null&quot;, &quot;political&quot;, &quot;government&quot;),
                  theme=c(&quot;null&quot;, &quot;nature&quot;, &quot;nature&quot;, &quot;null&quot;, &quot;geopolitics&quot;, &quot;geopolitics&quot;), 
                  id= c(&quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;))
</code></pre>
<p>Any advice or help getting my for loop to where I need it to be? TIA</p>
<p>EDIT: I would also like for sentences that cannot be classified to appear in the final dataframe with 'null' keywords and themes</p>
",Text Classification / Sentiment Analysis,using loop search string create data frame trying use opennlp look row text classify sentence thematic bucket sample df ive gotten splitting text sentence searching keywords using following code however loop doesnt print exactly im looking example print stem keyword giving wrong keyword wrong sentence end dont want print want write result new dataframe structure advice help getting loop need tia edit would also like sentence classified appear final dataframe null keywords theme
Does Fine-tunning Bert Model in multiple times with different dataset make it more accuracy?,"<p>i'm totally new in NLP and Bert Model.
What im trying to do right now is <em><strong>Sentiment Analysis on Twitter Trending Hashtag</strong></em> (&quot;neg&quot;, &quot;neu&quot;, &quot;pos&quot;) by using DistilBert Model, but the accurazcy was about 50% ( I tried w Label data taken from Kaggle).
So here is my idea:
(1) First, I will Fine-tunning Distilbertmodel (Model 1) with IMDB dataset,
(2) After that since i've got some data took from Twitter post,  i will sentiment analysis them my Model 1 and get Result 2.
(3) Then I will refine-tunning Model 1 with the Result 2 and expecting to have Model (3).</p>
<p>Im not really sure this process has any meaning to make the model more accuracy or not.
Thanks for reading my post.</p>
",Text Classification / Sentiment Analysis,doe fine tunning bert model multiple time different dataset make accuracy totally new nlp bert model im trying right sentiment analysis twitter trending hashtag neg neu po using distilbert model accurazcy wa tried w label data taken kaggle idea first fine tunning distilbertmodel model imdb dataset since got data took twitter post sentiment analysis model get result refine tunning model result expecting model im really sure process ha meaning make model accuracy thanks reading post
How to calculate ticket classification after putting in a sentence? (Python/NLP),"<p>I trained a model to classify tickets into 2 categories. I'm using GradientBoostClassifier. Now, I want to call on a function, where if I put any sentence in, the trained model would calculate the probability whether it will be category 1 or category 2. How do I write a code for this?</p>
<p>Let's imagine the sentence that I want to use is the ticket description: &quot;Lab Research Assistant is trying to create a Clinical Activity Report&quot;</p>
<pre><code>def function(sentence):
    #split the sentence into different words
    Counter(&quot; &quot;.join(descr).split()).most_common
    #remove stop words in this sentence
    sentence.apply(remove_stopwords)

    return list(sentence)

ticket = function('Lab Research Assistant is trying to create a Clinical Activity Report')
ticket

model.predict(ticket)
model.predict_proba(ticket)
</code></pre>
<p>Thank you!</p>
",Text Classification / Sentiment Analysis,calculate ticket classification putting sentence python nlp trained model classify ticket category using gradientboostclassifier want call function put sentence trained model would calculate probability whether category category write code let imagine sentence want use ticket description lab research assistant trying create clinical activity report thank
Fine-tuning BERT For Sequence Classification on sentiment140 dataset gives very POOR results,"<p>I'm using :</p>
<ul>
<li><a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> dataset</li>
<li><a href=""https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/bert#transformers.BertTokenizerFast"" rel=""nofollow noreferrer"">BertTokenizerFast</a> for text tokenization</li>
<li><a href=""https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/bert#transformers.TFBertForSequenceClassification"" rel=""nofollow noreferrer"">TFBertForSequenceClassification</a> for text classification</li>
</ul>
<p>And I want to <strong>fine-tune the</strong> model (<code>TFBertForSequenceClassification</code>) on the dataset (<code>sentiment140</code>).</p>
<p>When doing so, the performances of my model are really bad.</p>
<p>With 10K tweets (~1h training) :</p>
<ul>
<li>ROC AUC score :  0.131</li>
<li>Average Precision score :  0.325</li>
</ul>
<p>With 1M tweets (~9h training) :</p>
<ul>
<li>ROC AUC score :  0.883</li>
<li>Average Precision score :  0.822</li>
</ul>
<p>The notebook run is available in <a href=""https://www.kaggle.com/clementfleury/oc-p7/log?scriptVersionId=86383932"" rel=""nofollow noreferrer"">Kaggle</a>.</p>
<p>I must be missing something obvious, but I really can't find what... Is it &quot;just&quot; an amount of training data problem ? Or am I not using the right parameters / metrics / optimizer ?</p>
<p>This is basically my code :</p>
<pre class=""lang-py prettyprint-override""><code>from tqdm import tqdm

# Maths modules
import numpy as np
import pandas as pd
import tensorflow as tf

# Load data from CSV
df = pd.read_csv(
    &quot;../input/sentiment140/training.1600000.processed.noemoticon.csv&quot;,
    names=[&quot;target&quot;, &quot;id&quot;, &quot;date&quot;, &quot;flag&quot;, &quot;user&quot;, &quot;text&quot;],
    encoding=&quot;ISO-8859-1&quot;,
)

# Drop useless columns
df.drop(columns=[&quot;id&quot;, &quot;date&quot;, &quot;flag&quot;, &quot;user&quot;], inplace=True)


# Replace target values with labels
df.target.replace(
    {
        0: &quot;NEGATIVE&quot;,
        2: &quot;NEUTRAL&quot;,
        4: &quot;POSITIVE&quot;,
    },
    inplace=True,
)

# And back to binary values
df.target.replace(
    {
        &quot;NEGATIVE&quot;: 0,
        &quot;POSITIVE&quot;: 1,
    },
    inplace=True,
)


# Sample data for development
TEXT_SAMPLE_SIZE = 10000  # &lt;= 0 for all

# Sample data
if TEXT_SAMPLE_SIZE &gt; 0:
    df = df.groupby(&quot;target&quot;, group_keys=False).apply(
        lambda x: x.sample(
            n=int(TEXT_SAMPLE_SIZE / df[&quot;target&quot;].nunique()), random_state=42
        )
    ).reset_index(drop=True)



# Bert Tokenizers
from transformers import BertTokenizerFast

BERT_MODEL = &quot;bert-base-uncased&quot;

tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL, do_lower_case=True)

input_ids = np.asarray([tokenizer(sent, padding=&quot;max_length&quot;, truncation=True)[&quot;input_ids&quot;] for sent in tqdm(df.text)])
attention_mask = np.asarray([tokenizer(sent,padding=&quot;max_length&quot;,truncation=True)[&quot;attention_mask&quot;] for sent in tqdm(df.text)])
token_type_ids = np.asarray([tokenizer(sent,padding=&quot;max_length&quot;,truncation=True)[&quot;token_type_ids&quot;] for sent in tqdm(df.text)])



from sklearn.model_selection import train_test_split


# Train-test split
(
    texts_train,
    texts_test,
    input_ids_train,
    input_ids_test,
    attention_mask_train,
    attention_mask_test,
    token_type_ids_train,
    token_type_ids_test,
    labels_train,
    labels_test,
) = train_test_split(
    df.text.values,
    input_ids,
    attention_mask,
    token_type_ids,
    df.target.values,
    test_size=0.2,
    stratify=df.target.values,
    random_state=42,
)



from transformers import TFBertForSequenceClassification
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import BinaryAccuracy


# Define NN model
print(&quot;Defining model...&quot;)
model = TFBertForSequenceClassification.from_pretrained(
    BERT_MODEL, num_labels=2
)

# compile NN network
print(&quot;Compiling model...&quot;)
model.compile(
    loss=BinaryCrossentropy(),
    optimizer=Adam(learning_rate=2e-5), # Value recommended by the Bert team
    metrics=BinaryAccuracy(),
)

# fit NN model
print(&quot;Fitting model...&quot;)
model.fit(
    [input_ids_train, attention_mask_train, token_type_ids_train],
    labels_train,
    epochs=10,
    batch_size=8,
    validation_split=0.2,
    callbacks=[
        EarlyStopping(monitor=&quot;val_loss&quot;, patience=2),
    ],
    workers=4,
    use_multiprocessing=True,
)

print(model.summary())


# Get predictions
y_pred = model.predict([input_ids_test, attention_mask_test, token_type_ids_test])
y_pred_proba = [float(x[1]) for x in tf.nn.softmax(y_pred.logits)]
y_pred_label = [0 if x[0] &gt; x[1] else 1 for x in tf.nn.softmax(y_pred.logits)]


# Evaluate the model
from sklearn.metrics import (
    confusion_matrix,
    roc_auc_score,
    average_precision_score,
)

print(&quot;Confusion Matrix : &quot;)
print(confusion_matrix(labels_test, y_pred_label))

print(&quot;ROC AUC score : &quot;, round(roc_auc_score(labels_test, y_pred_proba), 3))

print(&quot;Average Precision score : &quot;, round(average_precision_score(labels_test, y_pred_proba), 3))

</code></pre>
<p>With this, I get these logs while training :</p>
<pre><code>Defining model...

All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Compiling model...
Fitting model...
Epoch 1/10

[...]

[==============================] - 5756s 72ms/step - loss: 0.4057 - binary_accuracy: 0.8482 - val_loss: 0.4579 - val_binary_accuracy: 0.8421
Model: &quot;tf_bert_for_sequence_classification&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  109482240 
_________________________________________________________________
dropout_37 (Dropout)         multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 109,483,778
Trainable params: 109,483,778
Non-trainable params: 0
_________________________________________________________________
None

</code></pre>
<p>And these classification performances :</p>
<pre><code>Confusion Matrix : 
[[17631 82369]
[ 415 99585]]
ROC AUC score :  0.883
Average Precision score :  0.822
</code></pre>
<p>Thanks for your help !</p>
",Text Classification / Sentiment Analysis,fine tuning bert sequence classification sentiment dataset give poor result using sentiment dataset berttokenizerfast text tokenization tfbertforsequenceclassification text classification want fine tune model dataset performance model really bad k tweet h training roc auc score average precision score tweet h training roc auc score average precision score notebook run available kaggle must missing something obvious really find amount training data problem using right parameter metric optimizer basically code get log training classification performance thanks help
How to share Salesforce Einstein models between few accounts,"<p>I started to work with Salesforce Einstein API, in particular with Einstein Intent &amp; Sentiment analysis, and I faced an issue - when I created new models under one of my account, I couldn't see these models under my other account. I checked out the official documentation &amp; tried to research a bit, but I didn't find any ways how I can share Einstein models between several accounts. Is it possible at all?</p>
",Text Classification / Sentiment Analysis,share salesforce einstein model account started work salesforce einstein api particular einstein intent sentiment analysis faced issue created new model one account see model account checked official documentation tried research bit find way share einstein model several account possible
How to analyse multiple texts in a single request using CommunitySentiment Model of Salesforce Einstein?,"<p>I was working on text analysis and wanted to classify text as <code>Positive</code>, <code>Negative</code>, or <code>Neutral</code>. Salesforce Einstein's Community Model has been successful in achieving the task. However, the main problem I am facing is:</p>
<p>Currently to analyze each text, I need to make a separate request. Since I have 1000s of records I cannot make such required number of requests to Salesforce Einstein. Is there a workaround for this problem?</p>
<p>This is my function call, which takes a parameter <code>textStatus</code> - A single text statement.</p>
<pre><code>apiCall(SENTIMENT, textStatus, 'CommunitySentiment');
</code></pre>
<p>I want to pass all text records as a list or in another way that ensures the whole task being performed in a single request.</p>
",Text Classification / Sentiment Analysis,analyse multiple text single request using communitysentiment model salesforce einstein wa working text analysis wanted classify text salesforce einstein community model ha successful achieving task however main problem facing currently analyze text need make separate request since record make required number request salesforce einstein workaround problem function call take parameter single text statement want pas text record list another way ensures whole task performed single request
NLP to analyse requests,"<p>Hi I am trying to analyse descriptions of around 30000 requests to identify common requests as the data has no tags or titles.</p>
<p>I’ve looked at a lot of content on sentiment analysis and I’m currently thinking I need to train a model from a small random sample to better classify the data.</p>
<p>Is there a better approach I should be following?</p>
",Text Classification / Sentiment Analysis,nlp analyse request hi trying analyse description around request identify common request data ha tag title looked lot content sentiment analysis currently thinking need train model small random sample better classify data better approach following
fasttext train_supervised model: get top predicted labels,"<p>I have used fasttext train_supervised utility to train a  classification model according to their webpage <a href=""https://fasttext.cc/docs/en/supervised-tutorial.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/supervised-tutorial.html</a> .</p>
<pre><code>model = fasttext.train_supervised(input='train.txt', autotuneValidationFile='validation.txt', autotuneDuration=600)
</code></pre>
<p>After I got the model how could I explore what kind of best parameters for the model like in sklearn after  a set of best parameters trained, we could always check the values for these parameters but I could not find any document to explain this.</p>
<p>I also used this trained model to make prediction on my data</p>
<pre><code>model.predict(test_df.iloc[2, 1])
</code></pre>
<p>It will return the label with a probability like this</p>
<pre><code>(('__label__2',), array([0.92334366]))
</code></pre>
<p>I'm wondering if I have 5 labels, every time when make prediction,is it possible for each  text to  get  all the probability for each label?
Like  for the above test_df text,
I could get something like</p>
<pre><code>model.predict(test_df.iloc[2, 1])

(('__label__2',), array([0.92334366])),(('__label__1',), array([0.82334366])),
(('__label__3',), array([0.52333333])),(('__label__0',), array([0.07000000])),
(('__label__4',), array([0.00002000]))
</code></pre>
<p>could find anything related to make change to get such prediction results.
Any suggestion?
Thanks.</p>
",Text Classification / Sentiment Analysis,fasttext train supervised model get top predicted label used fasttext train supervised utility train classification model according webpage got model could explore kind best parameter model like sklearn set best parameter trained could always check value parameter could find document explain also used trained model make prediction data return label probability like wondering label every time make prediction possible text get probability label like test df text could get something like could find anything related make change get prediction result suggestion thanks
Text Classification on a custom dataset with spacy v3,"<p>I am really struggling to make things work with the new spacy v3 version. The documentation is full. However, I am trying to run a training loop in a script.</p>
<p>(I am also not able to perform text classification training with CLI approach).</p>
<p>Data are publically available <a href=""https://raw.githubusercontent.com/koaning/tokenwiser/main/data/oos-intent.jsonl"" rel=""nofollow noreferrer"">here</a>.</p>
<pre class=""lang-py prettyprint-override""><code>
import pandas as pd
from spacy.training import Example
import random

TRAIN_DATA = pd.read_json('data.jsonl', lines = True)

nlp = spacy.load('en_core_web_sm')

config = {
    &quot;threshold&quot;: 0.5,
}

textcat = nlp.add_pipe(&quot;textcat&quot;, config=config, last=True)

label = TRAIN_DATA['label'].unique()

for label in label:
    textcat.add_label(str(label))

nlp = spacy.blank(&quot;en&quot;)
nlp.begin_training()


# Loop for 10 iterations
for itn in range(100):
    # Shuffle the training data
    losses = {}
    
    TRAIN_DATA = TRAIN_DATA.sample(frac = 1)

    # Batch the examples and iterate over them
    for batch in spacy.util.minibatch(TRAIN_DATA.values, size=4):
        texts = [nlp.make_doc(text) for text, entities in batch]
        annotations = [{&quot;cats&quot;: entities} for text, entities in batch]

        # uses an example object rather than text/annotation tuple
        print(texts)
        print(annotations)
        examples = [Example.from_dict(a)]
        
        nlp.update(examples, losses=losses)
    if itn % 20 == 0:
        print(losses)
</code></pre>
",Text Classification / Sentiment Analysis,text classification custom dataset spacy v really struggling make thing work new spacy v version documentation full however trying run training loop script also able perform text classification training cli approach data publically available
The best and simple way to convert labeled text classification data to spaCy v3 format,"<p>Let's suppose we have labeled data for text classification in a nice CSV file. We have 2 columns - &quot;text&quot; and &quot;label&quot;. I am kind of struggling to understand spacy V3. documentation. If I understand the correctly main sources of examples of spacy v3 documentation are THIS PROJECTS ()<a href=""https://github.com/explosion/projects/tree/v3/tutorials"" rel=""nofollow noreferrer"">https://github.com/explosion/projects/tree/v3/tutorials</a>).</p>
<p>However, the training data are already prepared in the expected JSON nested structure format.</p>
<p>If I want to perform costume text classification in spacy v3 I need to convert the data to the example structure - e.g LIKE HERE (<a href=""https://github.com/explosion/projects/blob/v3/tutorials/textcat_docs_issues/assets/docs_issues_eval.jsonl"" rel=""nofollow noreferrer"">https://github.com/explosion/projects/blob/v3/tutorials/textcat_docs_issues/assets/docs_issues_eval.jsonl</a>).</p>
<p>How to get from pandas data frame to here? Does prodigy support labeled data to spacy format? Let's have small example of the dataset</p>
<pre class=""lang-py prettyprint-override""><code>pd.DataFrame({
    &quot;TEXT&quot;:[
    &quot;i really like this post&quot;,
    &quot;thanks for that comment&quot;,
    &quot;i enjoy this friendly forum&quot;,
    &quot;this is a bad post&quot;,
    &quot;i dislike this article&quot;,
    &quot;this is not well written&quot;,
    &quot;who came up with this stupid idea?&quot;,
    &quot;This is just completely wrong!!&quot;,
    &quot;Get out of here now!!!!&quot;],
    &quot;LABEL&quot;: [
        &quot;POS&quot;, &quot;POS&quot;, &quot;POS&quot;, &quot;NEG&quot;, &quot;NEG&quot;, &quot;NEG&quot;, &quot;RUDE&quot;, &quot;RUDE&quot;, &quot;RUDE&quot;
    ]

})
</code></pre>
",Text Classification / Sentiment Analysis,best simple way convert labeled text classification data spacy v format let suppose labeled data text classification nice csv file column text label kind struggling understand spacy v documentation understand correctly main source example spacy v documentation project however training data already prepared expected json nested structure format want perform costume text classification spacy v need convert data example structure e g like get panda data frame doe prodigy support labeled data spacy format let small example dataset
Text classification using POS taggers of spacy,"<p>I'm very new to NLP and new to spacy as well and trying to learn things by myself.
My question is, is there a way to classify if a text is sarcastic or if it's a warning or an order, using spacy's pos taggers which are already there to use? For example &quot;Open that door&quot; is an order, how to classify it?</p>
<p>My end goal is I want to classify tweets so I can do more practice and advance a little bit, but for starters I just want to learn how to classify simple sentences.</p>
<p>Thanks in advance.</p>
",Text Classification / Sentiment Analysis,text classification using po tagger spacy new nlp new spacy well trying learn thing question way classify text sarcastic warning order using spacy po tagger already use example open door order classify end goal want classify tweet practice advance little bit starter want learn classify simple sentence thanks advance
kwic() function returns less rows than it should,"<p>I'm currently trying to perform a sentiment analysis on a <code>kwic</code> object, but I'm afraid that the <code>kwic()</code> function does not return all rows it should return. I'm not quite sure what exactly the issue is which makes it hard to post a reproducible example, so I hope that a detailed explanation of what I'm trying to do will suffice.</p>
<p>I subsetted the original dataset containing speeches I want to analyze to a new data frame that only includes speeches mentioning certain keywords. I used the following code to create this subset:</p>
<pre><code>ostalgie_cluster &lt;- full_data %&gt;%
  filter(grepl('Schwester Agnes|Intershop|Interflug|Trabant|Trabi|Ostalgie',
                speechContent,
                ignore.case = TRUE))
</code></pre>
<p>The resulting data frame consists of 201 observations. When I perform <code>kwic()</code> on the same initial dataset using the following code, however, it returns a data frame with only 82 observations. Does anyone know what might cause this? Again, I'm sorry I can't provide a reproducible example, but when I try to create a reprex from scratch it just.. works...</p>
<pre><code>#create quanteda corpus object
qtd_speeches_corp &lt;- corpus(full_data,
                            docid_field = &quot;id&quot;,
                            text_field = &quot;speechContent&quot;)

#tokenize speeches
qtd_tokens &lt;- tokens(qtd_speeches_corp, 
                     remove_punct = TRUE,
                     remove_numbers = TRUE,
                     remove_symbols = TRUE,
                     padding = FALSE) %&gt;%
  tokens_remove(stopwords(&quot;de&quot;), padding = FALSE) %&gt;%
  tokens_compound(pattern = phrase(c(&quot;Schwester Agnes&quot;)), concatenator = &quot; &quot;)

ostalgie_words &lt;- c(&quot;Schwester Agnes&quot;, &quot;Intershop&quot;, &quot;Interflug&quot;, &quot;Trabant&quot;, &quot;Trabi&quot;, &quot;Ostalgie&quot;)

test_kwic &lt;- kwic(qtd_tokens,
                  pattern = ostalgie_words,
                  window = 5)
</code></pre>
",Text Classification / Sentiment Analysis,kwic function return le row currently trying perform sentiment analysis object afraid function doe return row return quite sure exactly issue make hard post reproducible example hope detailed explanation trying suffice subsetted original dataset containing speech want analyze new data frame includes speech mentioning certain keywords used following code create subset resulting data frame consists observation perform initial dataset using following code however return data frame observation doe anyone know might cause sorry provide reproducible example try create reprex scratch work
Extract a 100-Character Window around Keywords in Text Data with R (Quanteda or Tidytext Packages),"<p>This is my first time asking a question on here so I hope I don't miss any crucial parts. I want to perform sentiment analysis on windows of speeches around certain keywords. My dataset is a large csv file containing a number of speeches, but I'm only interest in the sentiment of the words immediately surrounding certain key words.</p>
<p>I was told that the quanteda package in R would likely be my best bet for finding such a function, but I've been unsuccessful in locating it so far. If anyone knows how to do such a task it would be <em>greatly</em> appreciated !!!</p>
<p>Reprex (I hope?) below:</p>
<pre><code>speech = c(&quot;This is the first speech. Many words are in this speech, but only few are relevant for my research question. One relevant word, for example, is the word stackoverflow. However there are so many more words that I am not interested in assessing the sentiment of&quot;, &quot;This is a second speech, much shorter than the first one. It still includes the word of interest, but at the very end. stackoverflow.&quot;, &quot;this is the third speech, and this speech does not include the word of interest so I'm not interested in assessing this speech.&quot;)

data &lt;- data.frame(id=1:3, 
                   speechContent = speech)
</code></pre>
",Text Classification / Sentiment Analysis,extract character window around keywords text data r quanteda tidytext package first time asking question hope miss crucial part want perform sentiment analysis window speech around certain keywords dataset large csv file containing number speech interest sentiment word immediately surrounding certain key word wa told quanteda package r would likely best bet finding function unsuccessful locating far anyone know task would greatly appreciated reprex hope
Generating Trigrams with Gensim&#39;s Phraser Package in Python,"<p>I have the following code snippet which I created with the help of <a href=""https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483"" rel=""nofollow noreferrer"">this</a> tutorial for unsupervised sentiment analysis purposes:</p>
<pre><code>sent = [row for row in file_model.message]
phrases = Phrases(sent, min_count=1, progress_per=50000)
bigram = Phraser(phrases)
sentences = bigram[sent]
sentences[1]

file_export = file_model.copy()
file_export['old_message'] = file_export.message
file_export.old_message = file_export.old_message.str.join(' ')
file_export.message = file_export.message.apply(lambda x: ' '.join(bigram[x]))

file_export.to_csv('cleaned_dataset.csv', index=False)
</code></pre>
<p>Since now I want to have bigrams as well as trigrams, I tried it by adjusting it to:</p>
<pre><code>sent = [row for row in file_model.message]
phrases = Phrases(sent, min_count=1, progress_per=50000)
bigram = Phraser(phrases)
trigram = Phraser(bigram[phrases])
sentences = trigram[sent]
sentences[1]

file_export = file_model.copy()
file_export['old_message'] = file_export.message
file_export.old_message = file_export.old_message.str.join(' ')
file_export.message = file_export.message.apply(lambda x: ' '.join(trigram[x]))

file_export.to_csv('cleaned_dataset.csv', index=False)
</code></pre>
<p>But when I run this, I get <strong><code>TypeError: 'int' object is not iterable</code></strong> which I assume refers to my adjustment to <code>trigram = Phraser(bigram[phrases])</code>. I am using <code>gensim 4.1.2</code>.
Unfortunately, I have no computer science background and solutions I find online don't help out.</p>
",Text Classification / Sentiment Analysis,generating trigram gensim phraser package python following code snippet created help tutorial unsupervised sentiment analysis purpose since want bigram well trigram tried adjusting run get assume refers adjustment using unfortunately computer science background solution find online help
How do you sort the &quot;compound&quot; column in text sentiment analysis,"<p>In my dataframe, I want to sort - by ascending order - the compound column</p>
<p>code:</p>
<pre><code>df=pd.DataFrame(text_sentiment)
df.head()


text    date    compound    positive    negative    neutral
0   HOUSTON, March 25 (Reuters) - California diese...   2022-03-25T22:22:00Z    0.0000  0.000   0.000   1.000
1   March 31 (Reuters) - Major Russian oil refiner...   2022-03-31T09:01:00Z    -0.2960 0.053   0.109   0.838
2   Multiple US government agencies issued a joint...   2022-04-14T01:12:58Z    0.5859  0.274   0.137   0.590
3   HOUSTON (Reuters) - California diesel prices c...   2022-03-25T22:22:36Z    0.0000  0.000   0.000   1.000
4   THEGIFT777/E+ via Getty Images  2022-03-30T18:45:51Z    0.0000  0.000   0.000   1.000
</code></pre>
",Text Classification / Sentiment Analysis,sort compound column text sentiment analysis dataframe want sort ascending order compound column code
Process and progress for natural language analysis of company communication?,"<p>Assume there is a large record of all different kinds of inter-employee and customer communications (e.g. mails, chat transcripts, OCRed letters) which should be analyzed, for sentiment analysis.</p>
<p>I am trying to explain my customers how NLP works using BPMN. The challenge is to find an appropriate level of detail. Decision makers tend to see text analytics as a magic wand which delivers reproducible, object answers immediately. I somehow <strong>want to emphasize the iterative nature</strong> of the whole process. BPMN seems a nice way of doing so, because <strong>one could visualize the process of our efforts</strong>: If I have a huge dataset, I could have tokens at different tasks, showing how many texts of the corpora are already at what stage, and this even can be automatically.</p>
<p>Before reinventing the wheel: Has anybody already married NLP and BPMN in some way? Are there any major flaws in my diagram? Is there a better way (maybe avoiding BPMN) to describe the process and to measure progress?</p>
<p><a href=""https://i.sstatic.net/hjaIy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hjaIy.png"" alt=""Suggestion"" /></a></p>
<p>Source code for the diagram: <a href=""https://pastebin.com/raw/zMzuLvJH"" rel=""nofollow noreferrer"">https://pastebin.com/raw/zMzuLvJH</a> (feel free to use or modify)</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;bpmn:definitions xmlns:bpmn=&quot;http://www.omg.org/spec/BPMN/20100524/MODEL&quot; xmlns:bpmndi=&quot;http://www.omg.org/spec/BPMN/20100524/DI&quot; xmlns:di=&quot;http://www.omg.org/spec/DD/20100524/DI&quot; xmlns:dc=&quot;http://www.omg.org/spec/DD/20100524/DC&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; id=&quot;Definitions_1&quot; targetNamespace=&quot;http://bpmn.io/schema/bpmn&quot; exporter=&quot;Camunda Modeler&quot; exporterVersion=&quot;1.8.2&quot;&gt;
  &lt;bpmn:process id=&quot;Process_1&quot; isExecutable=&quot;false&quot;&gt;
...
</code></pre>
<h3>References</h3>
<ul>
<li><a href=""https://stackoverflow.com/questions/33619635/"">Set of rules for textual analysis - Natural language processing</a></li>
<li>Julia Silge and David Robinson: <a href=""https://www.tidytextmining.com/"" rel=""nofollow noreferrer"">Text Mining with R - A Tidy Approach</a></li>
</ul>
",Text Classification / Sentiment Analysis,process progress natural language analysis company communication assume large record different kind inter employee customer communication e g mail chat transcript ocred letter analyzed sentiment analysis trying explain customer nlp work using bpmn challenge find appropriate level detail decision maker tend see text analytics magic wand delivers reproducible object answer immediately somehow want emphasize iterative nature whole process bpmn seems nice way one could visualize process effort huge dataset could token different task showing many text corpus already stage even automatically reinventing wheel ha anybody already married nlp bpmn way major flaw diagram better way maybe avoiding bpmn describe process measure progress source code diagram feel free use modify reference text mining r tidy approach
Custom action after Rasa response selection,"<p>I have a working Rasa chatbot that can utter responses based on a story. For some utterances, there are multiple options, and everything works as it should.</p>
<p>However, I want to be able to perform sentiment analysis based on the bot's chosen response.</p>
<p>So my question is how do I perform a custom action after Rasa has selected the response?</p>
",Text Classification / Sentiment Analysis,custom action rasa response selection working rasa chatbot utter response based story utterance multiple option everything work however want able perform sentiment analysis based bot chosen response question perform custom action rasa ha selected response
getting increase in val-loss and decrease in val-accuracy while running a deep learning model for test classification,"<p>I am trying to classify text with label 0,1 and doing it with Bi-lstm. Its giving me a bit good accuracy on training time but when it comes to validation the loss goes to increase and validation accuracy tends to decrease.. please suggest me some solution how I can I improve it.
shape of data: (1043708, 2)</p>
<p>here is my model</p>
<pre><code>model=tf.keras.Sequential([
    # add an embedding layer
    tf.keras.layers.Embedding(word_count, 16, input_length=max_len),
     # add dropout layer to prevent overfitting
    tf.keras.layers.Dropout(0.2),
    # add the bi-lstm layer
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),
    # add a dense layer
    tf.keras.layers.Dense(32, activation=tf.keras.activations.relu),
    tf.keras.layers.Dense(32, activation=tf.keras.activations.relu),
    tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax),
    # add the prediction layer
    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),
])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])

model.summary()
history = model.fit(XPAD_train, Y_train, validation_data=(XPAD_test, Y_test), epochs = 10, batch_size=batch_size, callbacks = [callback_func], verbose=1)
</code></pre>
",Text Classification / Sentiment Analysis,getting increase val loss decrease val accuracy running deep learning model test classification trying classify text label bi lstm giving bit good accuracy training time come validation loss go increase validation accuracy tends decrease please suggest solution improve shape data model
SpaCy - TextCategorizer - Bag Of Words: Is there a way to show the vectorized document?,"<p>I just trained and implemented a text categorizer using Space 3.0. Everything went smooth but I'd like to visualize the vectorized document (<code>[13, 0, 0, 120..etc]</code>) in order to better understand what feature (words) drove the bag-of-words (BoW) model to classify the document in a specific class.</p>
<pre><code>nlp = spacy.load('./nlp_single_label_cli/output/model-best')    
documents = pd.read_csv(target_directory+'_ocr.csv')
...
test_texts = documents['text'].values
test_docs = [nlp.tokenizer(text) for text in test_texts]

text_categorizer = nlp.get_pipe('textcat')
scores = text_categorizer.predict(test_docs)
predicted_labels = scores.argmax(axis=1)
</code></pre>
<p>I've created the model from scratch with just the TextCategorizer layer (following the space 3.0 guidelines). By doing this, the documents have no <code>.vector</code> attribute.</p>
<p>this is my config.cfg</p>
<pre><code>[nlp]
lang = &quot;it&quot;
pipeline = [&quot;textcat&quot;]
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.textcat]
factory = &quot;textcat&quot;
scorer = {&quot;@scorers&quot;:&quot;spacy.textcat_scorer.v1&quot;}
threshold = 0.5

[components.textcat.model]
@architectures = &quot;spacy.TextCatEnsemble.v2&quot;
nO = null

[components.textcat.model.linear_model]
@architectures = &quot;spacy.TextCatBOW.v2&quot;
exclusive_classes = true
ngram_size = 1
no_output_layer = false
nO = null

[components.textcat.model.tok2vec]
@architectures = &quot;spacy.Tok2Vec.v2&quot;
...
</code></pre>
",Text Classification / Sentiment Analysis,spacy textcategorizer bag word way show vectorized document trained implemented text categorizer using space everything went smooth like visualize vectorized document order better understand feature word drove bag word bow model classify document specific class created model scratch textcategorizer layer following space guideline document attribute config cfg
Sentiment analysis hashtag column conversion,"<p>So, While learning Sentiment analysis, I got a column in my dataframe which include data looks like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>Hashtag_info</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>{'hashtags': [{'text': 'SEVENTEEN', 'indices': [139, 149]}, {'text': 'SVT_POWER_OF_LOVE_THE_MOVIE', 'indices': [150, 178]}], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'url_text', 'expanded_url': 'url_text', 'display_url': 'url_text', 'indices': [114, 137]}], 'media': [{'id': 1505695832837804032, 'id_str': '1505695832837804032', 'indices': [179, 202], 'media_url': 'url_text', 'media_url_https': 'url_text', 'url': 'url_text', 'display_url': 'url_text', 'expanded_url': 'url_text', 'type': 'photo', 'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'}, 'large': {'w': 1920, 'h': 1080, 'resize': 'fit'}, 'medium': {'w': 1200, 'h': 675, 'resize': 'fit'}, 'small': {'w': 680, 'h': 383, 'resize': 'fit'}}}]}</td>
</tr>
<tr>
<td>1</td>
<td>{'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': [], 'media': [{'id': 1505486045957300230, 'id_str': '1505486045957300230', 'indices': [264, 287], 'media_url': 'url_text', 'media_url_https': 'url_text', 'url': 'url_text', 'display_url': 'url_text', 'expanded_url': 'url_text', 'type': 'photo', 'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'}, 'small': {'w': 467, 'h': 680, 'resize': 'fit'}, 'large': {'w': 1408, 'h': 2048, 'resize': 'fit'}, 'medium': {'w': 825, 'h': 1200, 'resize': 'fit'}}}]}</td>
</tr>
</tbody>
</table>
</div>
<p>and so on for nearly 20k row</p>
<p>So my question how to convert this data type of object O
into something like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>hashtag_text</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>['SEVENTEEN','SVT_POWER_OF_LOVE_THE_MOVIE']</td>
<td>photo</td>
</tr>
<tr>
<td>1</td>
<td>[]</td>
<td>video</td>
</tr>
</tbody>
</table>
</div>
<p>Like extract text and type only</p>
",Text Classification / Sentiment Analysis,sentiment analysis hashtag column conversion learning sentiment analysis got column dataframe include data look like index hashtag info hashtags text seventeen index text svt power love movie index symbol user mention url url url text expanded url url text display url url text index medium id id str index medium url url text medium url url text url url text display url url text expanded url url text type photo size thumb w h resize crop large w h resize fit medium w h resize fit small w h resize fit hashtags symbol user mention url medium id id str index medium url url text medium url url text url url text display url url text expanded url url text type photo size thumb w h resize crop small w h resize fit large w h resize fit medium w h resize fit nearly k row question convert data type object something like index hashtag text type seventeen svt power love movie photo video like extract text type
"Idenfying bigrams using Gensim Phraser that contain the word &quot;not,&quot; for sentiment analysis","<p>I am working on a sentiment analysis project where I am analyzing a corpus of documents, and I am specifically <em>not</em> removing the word &quot;not&quot; as a stopword, so that I can use it to determine if a text agrees or disagrees with something. For instance, there is a difference between &quot;not effective&quot; and &quot;effective&quot; when discussing the COVID vaccine.</p>
<p>However, my phraser is not identifying any bigrams with the word &quot;not.&quot; I presume this is because that token exists in such large numbers (particularly because I expanded contractions, so &quot;isn't&quot; -&gt; &quot;is not&quot;), that the scoring function simply scores all bigrams with &quot;not&quot; too low. This would be because the standard phrase scoring function is:</p>
<p><a href=""https://i.sstatic.net/BPX0W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BPX0W.png"" alt=""bigram scoring function"" /></a></p>
<p>(where <code>min_count</code> is a hyper parameter)</p>
<p>So, since &quot;not&quot; exists many thousands of times in the database, <code>worda_count</code> will be <strong>very</strong> large, leading to a large denominator and dropping the score considerably.</p>
<p>Is there a way to get around this, so &quot;not&quot; bigrams are scored effectively?</p>
<p>I can think of a few options off the top of my head:</p>
<ol>
<li><p>Write my own scoring function that effectively has two scoring formula: the standard scoring formula, and a different scoring formula if the first word is &quot;not&quot;.</p>
</li>
<li><p>I could include &quot;not&quot; in a list of <code>connector_words</code>, but <code>gensim.models.phrases.Phraser</code> <em>specifically</em> indicates that these connector words cannot be at the beginning or end of a phrase.</p>
</li>
</ol>
",Text Classification / Sentiment Analysis,idenfying bigram using gensim phraser contain word sentiment analysis working sentiment analysis project analyzing corpus document specifically removing word stopword use determine text disagrees something instance difference effective effective discussing covid vaccine however phraser identifying bigram word presume token exists large number particularly expanded contraction scoring function simply score bigram low would standard phrase scoring function hyper parameter since exists many thousand time database large leading large denominator dropping score considerably way get around bigram scored effectively think option top head write scoring function effectively ha two scoring formula standard scoring formula different scoring formula first word could include list specifically indicates connector word beginning end phrase
How to understand loss-learning rate (log scale) plot using learner.lr_plot in ktrain package?,"<p>I am using ktrain package to classify text. My experiment is shown as:</p>
<p><a href=""https://i.sstatic.net/7rBep.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7rBep.png"" alt=""enter image description here"" /></a></p>
<p>lr_find and lr_plot are functions in ktrain. They can be used to highlight the best learning rate, which is shown as the red dot in the plot.</p>
<p>I do not understand how to understand this plot:</p>
<ol>
<li>How to transfer log scale to the normal linear one?</li>
<li>Why the best scale is the red dot?</li>
</ol>
",Text Classification / Sentiment Analysis,understand loss learning rate log scale plot using learner lr plot ktrain package using ktrain package classify text experiment shown lr find lr plot function ktrain used highlight best learning rate shown red dot plot understand understand plot transfer log scale normal linear one best scale red dot
Sentiment Analysis: Is there a way to extract positive and negative aspects in reviews?,"<p>Currently, I'm working on a project where I need to extract the relevant aspects used in positive and negative reviews in real time.</p>
<p>For the notions of more negative and positive, it will be a question of contextualizing the word. Distinguish between a word that sounds positive in a negative context (consider irony).</p>
<p>Here is an example:
<em>Very nice welcome!!! We ate very well with traditional dishes as at home, the quality but also the quantity are in appointment!!!</em>*</p>
<p><strong>Positive aspects:</strong> welcome, traditional dishes, quality, quantity</p>
<p>Can anyone suggest to me some tutorials, papers or ideas about this topic?</p>
<p>Thank you in advance.</p>
",Text Classification / Sentiment Analysis,sentiment analysis way extract positive negative aspect review currently working project need extract relevant aspect used positive negative review real time notion negative positive question contextualizing word distinguish word sound positive negative context consider irony example nice welcome ate well traditional dish home quality also quantity appointment positive aspect welcome traditional dish quality quantity anyone suggest tutorial paper idea topic thank advance
Logistic Regression and Sentiment Analysis,"<p>I need help with the following code everywhere that says &quot;YOUR CODE HERE&quot;.
Any help is appreciated.
Thank you!</p>
<p>#Use the lexicons to create two lexicon features.
A feature 'POSLEX' whose value indicates how many tokens belong to the positive lexicon.
A feature 'NEGLEX' whose value indicates how many tokens belong to the negative lexicon.</p>
<pre><code> def two_lexicon_features(tokens):
    feats = {'POSLEX': 0, 'NEGLEX': 0}
    # YOUR CODE HERE

 return feats
</code></pre>
<p>#If a word from the positive lexicon (e.g. 'like') appears N times in the document (e.g. 5 times), add a positive lexicon feature 'POSLEX_word' for that word that is associated that value (e.g. {'POSLEX_like' : 5}.
Similarly, if a word from the negative lexicon (e.g. 'dislike') appears N times in the document (e.g. 5 times), add a negative lexicon feature 'NEGLEX_word' for that word that is associated that value (e.g. {'NEGLEX_dislike' : 5}</p>
<pre><code>def lexicon_features(tokens):
  feats = {}
  # YOUR CODE HERE
  # Assume the positive and negative lexicons are available in poslex and neglex, respectively.

return feats
</code></pre>
<p>#Add a feature 'DOC_LEN' whose value is the natural logarithm of the document length (use <em>math.log</em> to compute logarithms)</p>
<pre><code>import math
def len_feature(tokens):
   feat = {'DOC_LEN': 'YOUR CODE HERE'}

return feat
</code></pre>
<p>#Add a feature 'DEICTIC_COUNT' that counts the number of 1st and 2nd person pronouns in the document.</p>
<pre><code>def deictic_feature(tokens):
   pronouns = set(('i', 'my', 'me', 'we', 'us', 'our', 'you', 'your'))
   count = 0
   # YOUR CODE HERE

return {'DEICTIC_COUNT': count}
</code></pre>
",Text Classification / Sentiment Analysis,logistic regression sentiment analysis need help following code everywhere say code help appreciated thank use lexicon create two lexicon feature feature poslex whose value indicates many token belong positive lexicon feature neglex whose value indicates many token belong negative lexicon word positive lexicon e g like appears n time document e g time add positive lexicon feature poslex word word associated value e g poslex like similarly word negative lexicon e g dislike appears n time document e g time add negative lexicon feature neglex word word associated value e g neglex dislike add feature doc len whose value natural logarithm document length use math log compute logarithm add feature deictic count count number st nd person pronoun document
How to convert emojis/emoticons to their meanings in python?,"<p>I am trying to clean up tweets to analyze their sentiments. I want to turn emojis to what they mean. </p>

<p>For instance, I want my code to convert</p>

<pre><code>'I ❤ New York' 
'Python is 👍'
</code></pre>

<p>to </p>

<pre><code>'I love New York' 
'Python is cool'
</code></pre>

<p>I have seen packages such as <code>emoji</code> but they turn the emoji's to what they represent, not what they mean. for instance, they turn my tweets to :</p>

<pre><code>print(emoji.demojize('Python is 👍'))
'Python is :thumbs_up:'

print(emoji.demojize('I ❤ New York'))
'I :heart: New York'
</code></pre>

<p>since ""heart"" or ""thumbs_up"" do not carry a positive or negative meaning in <code>textblob</code>, this kind of conversion is useless. But if ""❤"" is converted to ""love"", the results of sentiment analysis will improve drastically.</p>
",Text Classification / Sentiment Analysis,convert emojis emoticon meaning python trying clean tweet analyze sentiment want turn emojis mean instance want code convert seen package turn emoji represent mean instance turn tweet since heart thumb carry positive negative meaning kind conversion useless converted love result sentiment analysis improve drastically
How can I categorize tweets with Google Cloud Natural Language API - if possible?,"<p>I am trying to use Google Cloud Natural Language API to classify/categorize tweets in order to filter out tweets that are not relevant to my audience (weather related). I can understand it must be tricky for an AI solution to make a classification on a short amount of text but I would imagine it would at least have a guess on text like this:</p>
<blockquote>
<p>Wind chills of zero to -5 degrees are expected in Northwestern
Arkansas into North-Central Arkansas extending into portions of
northern Oklahoma during the 6-9am window . #arwx #okwx</p>
</blockquote>
<p>I have tested several tweets but only very few get a categorization, the rest gets no result (or &quot;No categories found. Try a longer text input.&quot; if I try it through <a href=""https://cloud.google.com/natural-language"" rel=""nofollow noreferrer"">the GUI</a>).</p>
<p>Is it pointless to hope for this to work? Or, is it possible to decrease the threshold for the categorization? An &quot;educated guess&quot; from the NLP-solution would be better than no filter at all. Is there an alternate solution (outside training my own NLP-model)?</p>
<p>Edit: In order to clarify:</p>
<p>I am, in the end, using the Google Cloud Platform Natural language API in order to classify tweets. In order to test it I am using the GUI (linked above). I can see that quite few of the tweets I test (in the GUI) gets a categorization from GCP NLP, i.e. the category is empty.</p>
<p>The desired state I want is for GCP NLP to provide a category guess of a tweet text, rather than providing an empty result. I assume the NLP model removes any results with a confidence less than X%. It would be interesting to know if that threshold could be configured.</p>
<p>I assume the categorization of tweets must have been done before, and if there is any other way to solve this?</p>
<p>Edit 2: ClassifyTweet-code:</p>
<pre><code>async function classifyTweet(tweetText) {
   const language = require('@google-cloud/language');
   const client = new language.LanguageServiceClient({projectId, keyFilename});
   //const tweetText = &quot;Some light snow dusted the ground this morning, adding to the intense snow fall of yesterday. Here at my Warwick station the numbers are in, New Snow 19.5cm and total depth 26.6cm. A very good snow event. Photos to be posted. #ONStorm #CANWarnON4464 #CoCoRaHSON525&quot;
   const document = {
      content: tweetText,
      type: 'PLAIN_TEXT',
   };   
   const [classification] = await client.classifyText({document});
   
   console.log('Categories:');
   classification.categories.forEach(category =&gt; {
     console.log(`Name: ${category.name}, Confidence: ${category.confidence}`);
   });
   
   return classification.categories
}
</code></pre>
",Text Classification / Sentiment Analysis,categorize tweet google cloud natural language api possible trying use google cloud natural language api classify categorize tweet order filter tweet relevant audience weather related understand must tricky ai solution make classification short amount text would imagine would least guess text like wind chill zero degree expected northwestern arkansas north central arkansas extending portion northern oklahoma window arwx okwx tested several tweet get categorization rest get result category found try longer text input try gui hope work possible decrease threshold categorization educated guess nlp solution would better filter alternate solution outside training nlp model edit order clarify end using google cloud platform natural language api order classify tweet order test using gui linked see quite tweet test gui get categorization gcp nlp e category empty desired state want gcp nlp provide category guess tweet text rather providing empty result assume nlp model remove result confidence le x would interesting know threshold could configured assume categorization tweet must done way solve edit classifytweet code
Multi-label text classification/regression using torchtext,"<p>I would like to use torchtext to implement a Transformer-based neural network for a multi-label classification task, where the labels are a list of values.</p>
<p>This is how my training data looks like:</p>
<pre><code>0: {&quot;sentence&quot;: ['word_1', 'word_2', 'word_3', 'word_4', 'word_5', 'word_6'], 
    &quot;labels&quot;: [('word_2', [&lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;]), 
        ('word_3', [&lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;]),
        ('word_6', [&lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;])]} 

1: {&quot;sentence&quot;: ['word_1', 'word_2', 'word_3', 'word_4'], 
    &quot;labels&quot;: [('word_4', [&lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;])}
</code></pre>
<p>The main characteristics of the dataset are:</p>
<ul>
<li>the sentences have different lengths</li>
<li>the number of target words in labels is not the same</li>
<li>the target words are different in each training instance</li>
<li>each target word is accompanied by a list of decimal values (fixed length, e.g. 4)</li>
</ul>
<p>The input to the system is a tokenized sentence, and the output should be a multi-output regression, e.g. <code>[&lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;, &lt;DECIMAL&gt;]</code>.</p>
<p>Since I am new to torchtext, and I didn't found datasets having such characteristics on the web, I need some help/ideas/opinions on how to prepare this kind of data to feed it to a transformer using torchtext. I would like to know (1) if it is feasible to implement this task with torchtext, and (2) how should I define the <code>labels</code> for the <code>fields</code>?</p>
<p>Have a nice day!</p>
",Text Classification / Sentiment Analysis,multi label text classification regression using torchtext would like use torchtext implement transformer based neural network multi label classification task label list value training data look like main characteristic dataset sentence different length number target word label target word different training instance target word accompanied list decimal value fixed length e g input system tokenized sentence output multi output regression e g since new torchtext found datasets characteristic web need help idea opinion prepare kind data feed transformer using torchtext would like know feasible implement task torchtext define nice day
NLP model for binary classification outputs a class for each word,"<p>I am basically running the code from Francois Chollet's Deep learning with python chapter 11.
It is a binary sentiment classification. For each sentence the label is 0 or 1.
After running the model as in the book, I try to make a prediction on one of the &quot;validation&quot; sentences.
The full code is a public kaggle notebook that can be found here:
<a href=""https://www.kaggle.com/louisbunuel/deep-learning-with-python"" rel=""nofollow noreferrer"">https://www.kaggle.com/louisbunuel/deep-learning-with-python</a>
It is part of the notebook here:
<a href=""https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb"" rel=""nofollow noreferrer"">https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb</a></p>
<p>the only thing I added is my &quot;extraction&quot; of a tokenized sentence from the tokenized tensorflow dataset so that I can see an example of an output. I was expecting a number from 0 to 1 (a probability indeed) but instead I get an array of numbers from 0 to 1, one for each word in the sentence. In other words, it looks as if the model does not assign labels to each sentence but to each word.<br />
Can anybody explain me what am I doing wrong? Is it my way of &quot;extracting&quot; a sentence from the tensorflow dataset?
Here's the code from the book/github, in the notebook</p>
<pre><code>!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz
!rm -r aclImdb/train/unsup


import os, pathlib, shutil, random
from tensorflow import keras
batch_size = 32
base_dir = pathlib.Path(&quot;aclImdb&quot;)
val_dir = base_dir / &quot;val&quot;
train_dir = base_dir / &quot;train&quot;
for category in (&quot;neg&quot;, &quot;pos&quot;):
    os.makedirs(val_dir / category)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    num_val_samples = int(0.2 * len(files))
    val_files = files[-num_val_samples:]
    for fname in val_files:
        shutil.move(train_dir / category / fname,
                    val_dir / category / fname)

train_ds = keras.utils.text_dataset_from_directory(
    &quot;aclImdb/train&quot;, batch_size=batch_size
)
val_ds = keras.utils.text_dataset_from_directory(
    &quot;aclImdb/val&quot;, batch_size=batch_size
)
test_ds = keras.utils.text_dataset_from_directory(
    &quot;aclImdb/test&quot;, batch_size=batch_size
)
text_only_train_ds = train_ds.map(lambda x, y: x)
</code></pre>
<p>preparing integer sequence datasets</p>
<pre><code>from tensorflow.keras import layers

max_length = 600
max_tokens = 20000
text_vectorization = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode=&quot;int&quot;,
    output_sequence_length=max_length,
)
text_vectorization.adapt(text_only_train_ds)

int_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)

embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)

inputs = keras.Input(shape=(None,), dtype=&quot;int64&quot;)
embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation=&quot;sigmoid&quot;)(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer=&quot;rmsprop&quot;,
              loss=&quot;binary_crossentropy&quot;,
              metrics=[&quot;accuracy&quot;])
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint(&quot;embeddings_bidir_gru.keras&quot;,
                                    save_best_only=True)
]
model.fit(int_train_ds, validation_data=int_val_ds, epochs=2, callbacks=callbacks)
model = keras.models.load_model(&quot;embeddings_bidir_gru.keras&quot;)
print(f&quot;Test acc: {model.evaluate(int_test_ds)[1]:.3f}&quot;)
</code></pre>
<p>My &quot;addition&quot; to the code is this part. After the model is ran, i take out a sentence like this:</p>
<pre><code>ds = int_val_ds.take(1)     # int_val_ds is the dataframe that is already vectorized to numbers
for sentence, label in ds:  # example is (sentence, label)
  print(sentence.shape, label)

&gt;&gt; (32, 600) tf.Tensor([1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0], shape=(32,), dtype=int32)
</code></pre>
<p>So it's a batch of 32 sentences with 36 corresponding labels
If I look at the shape of one element</p>
<pre><code>sentence[2].shape

&gt;&gt; TensorShape([600])
</code></pre>
<p>If I type</p>
<pre><code>model.predict(sentence[2])

&gt;&gt; array([[0.49958456],
       [0.50042397],
       [0.50184965],
       [0.4992085 ],...
       [0.50077164]], dtype=float32)
</code></pre>
<p>with 600 elements. I was expecting a single number between 0 and 1. What went wrong?</p>
",Text Classification / Sentiment Analysis,nlp model binary classification output class word basically running code francois chollet deep learning python chapter binary sentiment classification sentence label running model book try make prediction one validation sentence full code public kaggle notebook found part notebook thing added extraction tokenized sentence tokenized tensorflow dataset see example output wa expecting number probability indeed instead get array number one word sentence word look model doe assign label sentence word anybody explain wrong way extracting sentence tensorflow dataset code book github notebook preparing integer sequence datasets addition code part model ran take sentence like batch sentence corresponding label look shape one element type element wa expecting single number went wrong
How can spacy classify text as belonging to one of several labels,"<p>How can I classify projects into one of several categories based on their titles and/or descriptions. I have a list of several projects with titles and descriptions, and each one can be classed as one of several categories depending on the keywords within those categories. So for example, in the first image, the first project would be classed as healthcare and the second project would be classed as materials, based on the keywords in the second image.</p>
<p><a href=""https://i.sstatic.net/rInmN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rInmN.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/96Tf3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/96Tf3.png"" alt=""enter image description here"" /></a></p>
<p>I've been searchng for some tutorials, but they mostly seem to show how to categorise text as being either positive or negative, which isn't what I need. Any pointers to tutorials would also help</p>
",Text Classification / Sentiment Analysis,spacy classify text belonging one several label classify project one several category based title description list several project title description one classed one several category depending keywords within category example first image first project would classed healthcare second project would classed material based keywords second image searchng tutorial mostly seem show categorise text either positive negative need pointer tutorial would also help
Remove SPECIAL stopwords for NLP,"<p>I am having a text classification task. I want to classify a set of documents in 4 categories (Business, Entertainment, Health, Technology). I create wordcloud for every category(i removed stopwords) and each wordcloud still contains stopwords such as (april, tuesday, yesterday, hundred). I merged the stopwords sets from SpaCy, NLTK, gensim in a complete set of stopwords. I performed a &quot;remove_stopwords&quot; function, but I realized that many special stopwords remain in the text.</p>
<p><strong>Question1</strong>
I want to remove the following:</p>
<p><strong>Location Stopwords</strong> – Country names, Cities names etc</p>
<p><strong>Time Stopwords</strong> – Name of the months and days (january, february, monday, tuesday, today, tomorrow …) etc</p>
<p><strong>Numerals Stopwords</strong> – Words describing numerical terms ( hundred, thousand, … etc)</p>
<p>Doing this by hand, its a time consuming task. Is there any better solution?</p>
<p><strong>Question2</strong></p>
<p>In another text classification problem with 4 classes(business, science, sports, world). Take a look for example at <strong>worlds</strong> column. Is it a good practice to use words like &quot;monday, yesterday&quot; to classify a text in &quot;worlds&quot; category?
<a href=""https://i.sstatic.net/XDh8Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XDh8Z.png"" alt=""enter image description here"" /></a></p>
",Text Classification / Sentiment Analysis,remove special stopwords nlp text classification task want classify set document category business entertainment health technology create wordcloud every category removed stopwords wordcloud still contains stopwords april tuesday yesterday hundred merged stopwords set spacy nltk gensim complete set stopwords performed remove stopwords function realized many special stopwords remain text question want remove following location stopwords country name city name etc time stopwords name month day january february monday tuesday today tomorrow etc numeral stopwords word describing numerical term hundred thousand etc hand time consuming task better solution question another text classification problem class business science sport world take look example world column good practice use word like monday yesterday classify text world category
Unsupervised Sentiment Analysis on product reviews,"<p>I would like to perform an unsupervised sentiment analysis on the reviews posted by customers on different product web-page. Most of the online resources use supervised methods and the examples/tutorials always have a labelled training data-set.</p>

<p>My objective is not to just deduce the polarity of the review but also do content/subjective analysis. Any online implementation or suggest high level approach for this ?</p>
",Text Classification / Sentiment Analysis,unsupervised sentiment analysis product review would like perform unsupervised sentiment analysis review posted customer different product web page online resource use supervised method example tutorial always labelled training data set objective deduce polarity review also content subjective analysis online implementation suggest high level approach
Clustering text data based on sentiment?,"<p>I am scraping reviews off Amazon with the intent to perform sentiment analysis to classify them into <strong>positive</strong>, <strong>negative</strong> and <strong>neutral</strong>. Now the data I would get would be text and unlabeled.</p>
<p>My approach to this problem would be as following:-</p>
<p>1.) Label the data using clustering algorithms like <strong>DBScan</strong>, <strong>HDBScan</strong> or <strong>KMeans</strong>. The number of clusters would obviously be 3.</p>
<p>2.) Train a <strong>Classification</strong> algorithm on the labelled data.</p>
<p>Now I have never performed clustering on text data but I am familiar with the basics of clustering. So my question is:</p>
<ol>
<li><p>Is my approach correct?</p>
</li>
<li><p>Any articles/blogs/tutorials I can follow for text based clustering since I am kinda new to this?</p>
</li>
</ol>
",Text Classification / Sentiment Analysis,clustering text data based sentiment scraping review amazon intent perform sentiment analysis classify positive negative neutral data would get would text unlabeled approach problem would following label data using clustering algorithm like dbscan hdbscan kmeans number cluster would obviously train classification algorithm labelled data never performed clustering text data familiar basic clustering question approach correct article blog tutorial follow text based clustering since kinda new
Unable to make classification(Sentiment analysis) after pickling (XXX.h5),"<p>I Have created Lstm Model for sentiment analysis. The model is predicting fine before saving it to XXX.h5. I am saving the using the following codes:</p>
<pre><code>from keras.models import load_model
model.save('XXX.h5')  # creates a HDF5 file 'XXX.h5'
</code></pre>
<p>Now, I am trying to load the model using following code:</p>
<pre><code>model1 = load_model('XXX.h5')
</code></pre>
<p>The model is being loaded successfully. I want sentiment analysis on the new statement that is written as shown in below codes</p>
<pre><code>from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import load_model 
from keras.preprocessing.text import Tokenizer
import numpy as np

X = ['he is a good person'] 

texts = X
vocabSize = 2000
tokenizer = Tokenizer(num_words=vocabSize, split=' ')
tokenizer.fit_on_texts(texts)
model1 = load_model('XXX.h5')
#vectorizing the tweet by the pre-fitted tokenizer instance
X = tokenizer.texts_to_sequences(X)
#padding the tweet to have exactly the same shape as `embedding_2` input
X = pad_sequences(X, maxlen=28, dtype='int32', value=0)
#print(X)
sentiment = model1.predict(X,batch_size=1,verbose = 2)[0]
if(np.argmax(sentiment) == 0):
    print(&quot;negative&quot;)
elif (np.argmax(sentiment) == 1):
    print(&quot;positive&quot;)
</code></pre>
<p>The Result is restricted to only positive and Negative In this case but I get negative as the answer irrespective of the sentence. I want The above sentence to be predicted as positive.</p>
<p>where did I go wrong?</p>
",Text Classification / Sentiment Analysis,unable make classification sentiment analysis pickling xxx h created lstm model sentiment analysis model predicting fine saving xxx h saving using following code trying load model using following code model loaded successfully want sentiment analysis new statement written shown code result restricted positive negative case get negative answer irrespective sentence want sentence predicted positive go wrong
How to define log-count ratio for multiclass text dataset (fastai)?,"<p>I am trying to follow Rachel Thomas path of sentiment classification with Naive Bayes. In the video she uses a binary dataset (pos. and neg. movie reviews). When it comes to apply Naive Bayes, <a href=""https://youtu.be/dt7sArnLo1g?t=1185"" rel=""nofollow noreferrer"">this is what she does</a>:</p>

<p><strong>Defintion:</strong> log-count ratio r for each word f:</p>

<pre><code>r = log (ratio of feature f in positive documents) / (ratio of feature f in negative documents)
</code></pre>

<p>where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents.</p>

<pre><code>p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))
p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))

pr1 = (p1+1) / ((y.items==positive).sum() + 1)
pr0 = (p0+1) / ((y.items==negative).sum() + 1)

r = np.log(pr1/pr0)
</code></pre>

<p>--> it is very simple to apply the log-count-ratio to a dataset with 2 labels!</p>

<p><strong>Problem:</strong> 
My dataset is not binary! Lets assume I have 5 labels: label_1,...,label_5</p>

<p>How do I get the log-count ratio r for multilabel dataset?</p>

<p><strong>My approach:</strong></p>

<pre><code>p4 = np.squeeze(np.asarray(x[y.items==label_5].sum(0)))
p3 = np.squeeze(np.asarray(x[y.items==label_4].sum(0)))
p2 = np.squeeze(np.asarray(x[y.items==label_3].sum(0)))
p1 = np.squeeze(np.asarray(x[y.items==label_2].sum(0)))
p0 = np.squeeze(np.asarray(x[y.items==label_1].sum(0)))

log-count-ratio:
pr1 = (p1+1) / ((y.items==label_2).sum() + 1)
pr1_not = (p1+1) / ((y.items!=label_2).sum() + 1)
r_1 = np.log(pr1/pr1_not)

log-count-ratio:
pr2 = (p2+1) / ((y.items==label_3).sum() + 1)
pr2_not = (p2+1) / ((y.items!=label_3).sum() + 1)
r_2 = np.log(pr2/pr2_not)
...

</code></pre>

<p>Is this correct? Does it mean I get multiple ratios?</p>
",Text Classification / Sentiment Analysis,define log count ratio multiclass text dataset fastai trying follow rachel thomas path sentiment classification naive bayes video us binary dataset po neg movie review come apply naive bayes doe defintion log count ratio r word f ratio feature f positive document number time positive document ha feature divided number positive document simple apply log count ratio dataset label problem dataset binary let assume label label label get log count ratio r multilabel dataset approach correct doe mean get multiple ratio
How to determine &#39;did&#39; or &#39;did not&#39; on something,"<p>What's the straightforward way to distinguish between this two:</p>
<ol>
<li><code>the movie received critical acclaim</code></li>
<li><code>the movie did not attain critical acclaim</code>.</li>
</ol>
<p>Seems to me 'sentiment analysis' of nlp could do it for me. So I'm using <code>Textblob</code> <a href=""https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis"" rel=""nofollow noreferrer"">sentiment analysis</a>. But both sentences' <code>polarity</code> is <code>0.0</code>.</p>
",Text Classification / Sentiment Analysis,determine something straightforward way distinguish two seems sentiment analysis nlp could using sentiment analysis sentence
What should be the size of Training data set for machine-learning?,"<p>1) I want to perform sentiment analysis on twitter tweets. So, I choose to use the <strong>datumbox-framework</strong>. I have small doubt what should be the size of my training samples? &amp; if I'm collecting the training samples of positive,negative,neutral should I maintain the same size for all the training examples? (i.e., can I collect 10 pos,5 neg,15 neutral as my training sets or I should collect and maintain all of pos,neg,neutral of same size pos=10;neg=10;neutral=10 in my training set) algorithm I'm using for twitter sentiment.
is <strong>navies Bayes</strong>.</p>

<p>2) is there any size limit for training data set?</p>
",Text Classification / Sentiment Analysis,size training data set machine learning want perform sentiment analysis twitter tweet choose use datumbox framework small doubt size training sample collecting training sample positive negative neutral maintain size training example e collect po neg neutral training set collect maintain po neg neutral size po neg neutral training set algorithm using twitter sentiment navy bayes size limit training data set
Is it possible to do emojis sentiment analysis using textblob?,"<p>I am running sentiment analysis on WhatsApp chats using textblob and the result is working on just text but I am getting neutral results on emojis.
Is there a way that textblob would see smiling emojis and return a positive result?</p>
",Text Classification / Sentiment Analysis,possible emojis sentiment analysis using textblob running sentiment analysis whatsapp chat using textblob result working text getting neutral result emojis way textblob would see smiling emojis return positive result
Invalid Prediction Results | PredictProba,"<p>I'm trying to classify youtube comments as Clickbait(label=1) or Fair/Non-Clickbait(label=0).</p>
<p>I've written the following code</p>
<pre><code>from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

training_data, testing_data = train_test_split(combine, test_size=0.3, random_state=2)

estimators = [('vectorize', CountVectorizer()),
              ('reduce_dimension', SelectKBest(chi2)), 
              ('var', VarianceThreshold()),
             ('classifier', LogisticRegression())
             ]
pipe = Pipeline(estimators)

param_grid = dict(vectorize__max_df=[0.90], 
                  vectorize__min_df=[3], 
                  vectorize__max_features=[1000, 700, 670, 850, 890], 
                  vectorize__stop_words=['english'],
                  reduce_dimension__k=[100],
                  var__threshold = [0.002],
                  classifier__random_state=[0],
                  classifier__solver=['lbfgs'])

grid_search = GridSearchCV(pipe, param_grid=param_grid, cv = 5)
grid_search.fit(training_data['Tidy_Comments']
              , training_data['label'])

print(&quot;Test data: &quot;)
print(testing_data.head(30))

predicted = grid_search.predict_proba(testing_data['Tidy_Comments'])
print(&quot; \n Prediction Results: &quot;)
print(predicted[:100])


                              ------------------------------

The output:

Test data: 
                                                comment  ...                                      Tidy_Comments
1604  OMG HAVE YOU SEEN WILBUR SCOOTS NEW VID ITS SO...  ...  omg have you seen wilbur scoot new vid it SO f...
849    Bro i saw a grapillaying hook in your. Inventory  ...           bro saw grapillay hook in your inventori
625   Now we know where the taliban got their weapon...  ...  now we know where the taliban got their weapon...
2482                  I like that I love you ðŸ˜˜â¤ï¸  ...                                 like that love you
1293          I love the song baby ðŸ’“ðŸ’“ðŸ’“ðŸ’“ðŸ˜  ...                                 love the song babi
2813  PLEASE PLEASE PLEASE PLEASE TELL ME HOW THIS C...  ...  pleas pleas pleas pleas tell ME how thi channe...
171   They could change to melt in your mouth not O...  ...  they could chang to melt in your mouth not ON ...
4061  Ahh come on. All of those examples are just co...  ...  ahh come on all of those exampl are just coinc...
739   Paul IS SCARRED OF BRRROCCCK LESSSNERRRðŸ˜‚ðŸ˜...  ...               paul IS scar OF brrroccck lesssnerrr
2515  the ppl had such bad reactions to getting the ...  ...    the ppl had such bad reaction to get the airpod
3328                                             à¦¤à¦¾  ...                                                   
4284                                     How you doing?  ...                                         how you do
368      You should make a video about the mill grind 3  ...         you should make video about the mill grind
4089  Just saying. Not Danny Bonadoose. Danny Bonadu...  ...  just say not danni bonadoos danni bonaduch lea...
2247                                               Halk  ...                                               halk
3818                               Godzilla is that you  ...                               godzilla is that you
4180                     But Evee have more evaluations  ...                            but eve have more evalu
3614                    I want to see free guy unmasked  ...                        want to see free guy unmask
3527                    L like. YOur. VIdeo. JAke. PAul  ...                          like your video jake paul
3860  This site is experiencing multiple keter and e...  ...  thi site is experienc multipl keter and euclid...
1577                                          Rip dolph  ...                                          rip dolph
3336                          Why you are done put ring  ...                          whi you are done put ring
4126   I PUT MY EYES REALLY CLOSE TO IT AND IT LOOKE...  ...  put MY eye realli close TO IT and IT look like...
129                                                Good  ...                                               good
1298                                              A jay  ...                                                jay
1398  I slowed it down and why is rip beating loyd u...  ...           slow it down and whi is rip beat loyd up
3340                       Yes my daughter make a dolls  ...                           ye my daughter make doll
3223  I can do cool tricks with my tongue itâ€™s eas...  ...  can do cool trick with my tongu it easi just b...
744                            roman reigns vs goldberg  ...                            roman reign vs goldberg
3305                                                Red  ...                                                red

[30 rows x 3 columns]
 
 Prediction Results: 
[[0.80092569 0.19907431]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.53783207 0.46216793]
 [0.78627905 0.21372095]
 [0.08711146 0.91288854]
 [0.43872882 0.56127118]
 [0.58584449 0.41415551]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.51263466 0.48736534]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.18465366 0.81534634]
 [0.46557161 0.53442839]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.54056743 0.45943257]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.6883806  0.3116194 ]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.76529343 0.23470657]
 [0.07886884 0.92113116]
 [0.24400596 0.75599404]
 [0.53783207 0.46216793]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.14150512 0.85849488]
 [0.0713452  0.9286548 ]
 [0.43872882 0.56127118]
 [0.46557161 0.53442839]
 [0.43872882 0.56127118]
 [0.72834201 0.27165799]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.6583404  0.3416596 ]
 [0.43872882 0.56127118]
 [0.71652564 0.28347436]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.86098908 0.13901092]
 [0.78611001 0.21388999]
 [0.71191436 0.28808564]
 [0.31142753 0.68857247]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.58998975 0.41001025]
 [0.43872882 0.56127118]
 [0.5124826  0.4875174 ]
 [0.43872882 0.56127118]
 [0.46557161 0.53442839]
 [0.43872882 0.56127118]
 [0.81219853 0.18780147]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.1043104  0.8956896 ]
 [0.17718162 0.82281838]
 [0.91031567 0.08968433]
 [0.88660894 0.11339106]
 [0.17009882 0.82990118]
 [0.63742452 0.36257548]
 [0.43872882 0.56127118]
 [0.64319105 0.35680895]
 [0.43872882 0.56127118]
 [0.5124826  0.4875174 ]
 [0.80392858 0.19607142]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.28411768 0.71588232]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.46557161 0.53442839]
 [0.98244671 0.01755329]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.43872882 0.56127118]
 [0.10581828 0.89418172]
 [0.73585843 0.26414157]
 [0.78018361 0.21981639]
 [0.53965163 0.46034837]
 [0.51263466 0.48736534]
 [0.43872882 0.56127118]
 [0.74754482 0.25245518]
 [0.43872882 0.56127118]
 [0.46557161 0.53442839]
 [0.43872882 0.56127118]]
......... and more
</code></pre>
<p>so as you can see, the predict_proba prediction value is exactly the same for many of my test_comments even though the comments are completely different.
So can anyone help me correct this?</p>
",Text Classification / Sentiment Analysis,invalid prediction result predictproba trying classify youtube comment clickbait label fair non clickbait label written following code see predict proba prediction value exactly many test comment even though comment completely different anyone help correct
How to extract COMPLAINT features from texts in order to classify complaints from non-complaints texts,"<p>I have a corpus of around 6000 texts with comments from social network (FB, twitter), news content from general and regional news and magazines, etc. I have gone through first 300 of these texts and tag each of these 300 texts' content as either customer complaint or non-complaint. </p>

<p>Instead of naive way of bag of words, I am wondering how can I accurately extract the features of these complaints and non-complaints texts? My goal is to use SVM or other classification algorithm/ library such as Liblinear to most accurately classify the rest of these texts as either complaint or non-complaint with the current training set of 300 texts. Is this procedure similar to sentiment analysis? If not, where should I start?</p>
",Text Classification / Sentiment Analysis,extract complaint feature text order classify complaint non complaint text corpus around text comment social network fb twitter news content general regional news magazine etc gone first text tag text content either customer complaint non complaint instead naive way bag word wondering accurately extract feature complaint non complaint text goal use svm classification algorithm library liblinear accurately classify rest text either complaint non complaint current training set text procedure similar sentiment analysis start
The label tag mixed with comment in sentiment analysis data frame,"<p>I have below data frame and my label column as u can see is part of sentence row and its separated by \ character. My question is how can I delete these zero and ones or replace them with &quot; &quot; character and transition them to new Label column beside this comment column?</p>
<p>thanks for you're help.</p>
<p><a href=""https://i.sstatic.net/t4hnW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t4hnW.png"" alt=""enter image description here"" /></a></p>
",Text Classification / Sentiment Analysis,label tag mixed comment sentiment analysis data frame data frame label column u see part sentence row separated character question delete zero one replace character transition new label column beside comment column thanks help
Binary vs. multi-class text classification,"<p>I have a dataset with social media posts that looks as below, but its in Farsi and I could not find a readily available R/Python sentiment analysis package.</p>
<pre><code>post/tweet
&quot;we are tired of this regime and need to make a change happen now&quot;
</code></pre>
<p>Ideally, I want to classify each statement as having a negative, positive, or neutral sentiment. Therefore, I created a small dictionary where I classify words into either negative or positive ones.</p>
<pre><code>library(tidyverse)
library(stringr)
library(readxl)
#install.packages(&quot;tidyverse&quot;)
#install.packages(&quot;stringr&quot;)
#install.packages(&quot;readxl&quot;)
</code></pre>
<h2>With three categories: positive, negative, and neutral</h2>
<pre><code>Raw_data_on_posts %&gt;% mutate(p_count = str_count(post, str_c(Dictionary$positive, collapse = '|')), 
                 n_count = str_count(post, str_c(Dictionary$negative, collapse = '|'))) %&gt;% 
           mutate(label = case_when(p_count &gt; n_count ~ 'positive',
                                    p_count &lt; n_count ~ 'negative',
                                    TRUE ~ 'neutral')) %&gt;% select(post, label)
</code></pre>
<p>I ended up having most statements as neutral, although based on my reading of the social media posts are either pro or anti the Iranian regime. Specifically, I believe this occurred because it is classifying words that I neither classified as negative nor positive, as neutral. But is it possible to instead only compare whether a statement has more negative or positive words?</p>
<pre><code>post/tweet                                 sentiment 
&quot;we are tired of this regime               neutral 
and need to make a change happen now&quot;
</code></pre>
<p>I wonder if the issue above is caused by R not recognizing/reading Farsi words and letters? I am asking because I ran the same code above to test it with English words below and it worked well:</p>
<pre><code>post
&lt;chr&gt;
label
&lt;chr&gt;
bad and good       neutral          
really good        positive         
in the middle      neutral          
bad                negative
</code></pre>
",Text Classification / Sentiment Analysis,binary v multi class text classification dataset social medium post look farsi could find readily available r python sentiment analysis package ideally want classify statement negative positive neutral sentiment therefore created small dictionary classify word either negative positive one three category positive negative neutral ended statement neutral although based reading social medium post either pro anti iranian regime specifically believe occurred classifying word neither classified negative positive neutral possible instead compare whether statement ha negative positive word wonder issue caused r recognizing reading farsi word letter asking ran code test english word worked well
stacked Bi-LSTM compare to 1 layer of Bi-LSTM,"<p>I trained my model on BI LSTM for multi class text classification, but my result is not different when I use 2 stacked  BI LSTM like below compare to just 1 layer of BI LSTM, any idea about that ?</p>
<pre><code>max_len = 409
max_words = 17666
emb_dim = 100


BB = Sequential()
BB.add(Embedding(max_words, emb_dim,weights=[embedding_matrix], input_length=max_len))
BB.add(Bidirectional(LSTM(64, return_sequences=True,dropout=0.4, recurrent_dropout=0.4)))
BB.add(Bidirectional(LSTM(34, return_sequences=False,dropout=0.4, recurrent_dropout=0.4)))

BB.add(Dropout(0.5))
BB.add(Dense(3, activation='softmax'))

BB.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
BB.summary()
</code></pre>
",Text Classification / Sentiment Analysis,stacked bi lstm compare layer bi lstm trained model bi lstm multi class text classification result different use stacked bi lstm like compare layer bi lstm idea
How do I use NLP to find which group of words a sentence is closes to?,"<p>I am trying to use NLP to see how well survey responses fit into predetermined categories. I can't use normal text-classification methods since a given response usually contains multiple categories.</p>
<p>Instead, I've pulled out the 10-20 words most commonly used in each category, and I want to build a script that inputs a survey response and computes how much it aligns with each list of words. Ideally I'd like it to recognize similar words to the ones in each list as well. The final result should a vector describing how much the response aligns with each group of words.</p>
<p>My only idea so far is to use a for loop that loops over every word in a response, while each group has a counter that goes up if a word matches. However, this wouldn't be useful in dealing with synonyms or similar words. Is there any way to work this out?</p>
",Text Classification / Sentiment Analysis,use nlp find group word sentence close trying use nlp see well survey response fit predetermined category use normal text classification method since given response usually contains multiple category instead pulled word commonly used category want build script input survey response computes much aligns list word ideally like recognize similar word one list well final result vector describing much response aligns group word idea far use loop loop every word response group ha counter go word match however useful dealing synonym similar word way work
Improve speed of python algorithm,"<p>I have used Sentiment140 dataset for twitter for sentiment analysis</p>
<p>Code:</p>
<p>getting words from tweets:</p>
<pre><code>tweet_tokens = []
[tweet_tokens.append(dev.get_tweet_tokens(idx)) for idx, item in enumerate(dev)]
</code></pre>
<p>getting unknown words from tokens</p>
<pre><code>words_without_embs = []
[[words_without_embs.append(w) for w in tweet if w not in word2vec] for tweet in tweet_tokens]
len(words_without_embs)
</code></pre>
<p>last part of code, calculate vector as the mean of left and right words (context)</p>
<pre><code>vectors = {} # alg
for word in words_without_embs:
  mean_vectors = []
  for tweet in tweet_tokens:
    if word in tweet:
      idx = tweet.index(word)
      try:
        mean_vector = np.mean([word2vec.get_vector(tweet[idx-1]), word2vec.get_vector(tweet[idx+1])], axis=0)
        mean_vectors.append(mean_vector)
      except:
        pass

    if tweet == tweet_tokens[-1]: # last iteration
      mean_vector_all_tweets = np.mean(mean_vectors, axis=0)
      vectors[word] = mean_vector_all_tweets
</code></pre>
<p>There are 1058532 words and the last part of this code works very slow, about 250 words per minute.</p>
<p>How can you improve the speed of this algorithm?</p>
",Text Classification / Sentiment Analysis,improve speed python algorithm used sentiment dataset twitter sentiment analysis code getting word tweet getting unknown word token last part code calculate vector mean left right word context word last part code work slow word per minute improve speed algorithm
Topic Correlation Analysis for Cross-Domain Text Classification using Python [or R],"<p>do you have any pointers on how to perform NLP topic correlation with Python. I have used pyLDAvis for a visual topic correlation but am unable to find a method to get the correlation in tabular format?</p>
<p>Below is a reference link to the subject to make it clearer: <a href=""https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"" rel=""nofollow noreferrer"">https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0</a></p>
<p>Thank you!</p>
",Text Classification / Sentiment Analysis,topic correlation analysis cross domain text classification using python r pointer perform nlp topic correlation python used pyldavis visual topic correlation unable find method get correlation tabular format reference link subject make clearer thank
Emojis are regarded as unknown(UNK) in BERT,"<p>My research interest is effect of emojis in text. I am trying to classify sarcastic tweets in text. A month ago I have used a dataset where I added the tokens using:</p>
<blockquote>
<p>tokenizer.add_tokens('List of Emojis').</p>
</blockquote>
<p>So when I tested the BERT model had successfully added the tokens. But 2 days ago when I did the same thing for another dataset, BERT model has categorized then as 'UNK' tokens. My question is, is there a recent change in the BERT model? I have tried it with the following tokenizer,</p>
<blockquote>
<p>BertTokenizer.from_pretrained('bert-base-uncased')</p>
</blockquote>
<p>This is same for distilbert. It does not recognize the emojis despite explicitly adding them. At first I read somewhere there is no need to add them in the tokenizer because BERT or distilbert has already those emojis in the 30000 tokens but I tried both. By adding and without adding. For both cases it does not recognize the emojis.</p>
<p>What can I do to solve this issue. Your thoughts on this would be appreciated.</p>
",Text Classification / Sentiment Analysis,emojis regarded unknown unk bert research interest effect emojis text trying classify sarcastic tweet text month ago used dataset added token using tokenizer add token list emojis tested bert model successfully added token day ago thing another dataset bert model ha categorized unk token question recent change bert model tried following tokenizer berttokenizer pretrained bert base uncased distilbert doe recognize emojis despite explicitly adding first read somewhere need add tokenizer bert distilbert ha already emojis token tried adding without adding case doe recognize emojis solve issue thought would appreciated
Get score on fileds that detect angry behaviour - Python Sentiment Analysis,"<p>I have reviews for products in a dataframe 'dfa' from customers in the format below. I want to add another column with score/flag on how 'angry' the review sounds.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Service_id</th>
<th style=""text-align: left;"">Review</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">a1</td>
<td style=""text-align: left;"">Pathetic service, waste of money</td>
</tr>
<tr>
<td style=""text-align: left;"">b2</td>
<td style=""text-align: left;"">The service was average and the cleanliness could have been better</td>
</tr>
<tr>
<td style=""text-align: left;"">v2</td>
<td style=""text-align: left;"">satisfied</td>
</tr>
</tbody>
</table>
</div>
<p>In the example above, positivity and anger will be treated differently. b2 has a negative comment but it should not be considered angry.</p>
<p>Example (output)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Service_id</th>
<th style=""text-align: left;"">Review</th>
<th style=""text-align: left;"">Anger_flag</th>
<th style=""text-align: left;"">Anger_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">a1</td>
<td style=""text-align: left;"">Pathetic service, waste of money</td>
<td style=""text-align: left;"">Y</td>
<td style=""text-align: left;"">0.9</td>
</tr>
<tr>
<td style=""text-align: left;"">b2</td>
<td style=""text-align: left;"">The service was average and the cleanliness could have been better</td>
<td style=""text-align: left;"">N</td>
<td style=""text-align: left;"">0.2</td>
</tr>
<tr>
<td style=""text-align: left;"">v2</td>
<td style=""text-align: left;"">satisfied</td>
<td style=""text-align: left;"">N</td>
<td style=""text-align: left;"">0.0</td>
</tr>
</tbody>
</table>
</div>
<p>Also like the profanity library, does python have a list of words to detect anger ?</p>
",Text Classification / Sentiment Analysis,get score fileds detect angry behaviour python sentiment analysis review product dataframe dfa customer format want add another column score flag angry review sound service id review pathetic service waste money b service wa average cleanliness could better v satisfied example positivity anger treated differently b ha negative comment considered angry example output service id review anger flag anger score pathetic service waste money b service wa average cleanliness could better n v satisfied n also like profanity library doe python list word detect anger
Extracting Emojis from a dataframe,"<p>My research interest is the effect of emojis in text for sentiment analysis. I would like to extract all the emojis from my dataset. So far I have done the following:</p>
<pre><code> import re 
 from emoji import UNICODE_EMOJI


 emoji_1 = re.compile('[\\u203C-\\u3299\\U0001F000-\\U0001F644]')

 emoji_list= list(filter(emoji_1.match, df['Tweet text']))

 emo_found= ' '.join(emoji for emoji in emoji_list)

  

 def get_emoji_set(text):
     return {letter for letter in text if letter in UNICODE_EMOJI['en'] }

 c = get_emoji_set(emo_found)

  Print(c)
</code></pre>
<p>But it is not extracting all the images. So far I got only the following emojis using the above code:</p>
<p>{'🙀', '🎉', '🎅', '👊', '😂', '😱', '😒', '🔫'}</p>
<p>However these are only the partial emojis that are present in the dataset. There are also the following emojis present in my dataset which am not getting in the result:</p>
<p>😡, 👭 , 😍, 😘, 💅,🙅,💁,👍, 😊 + more emojis</p>
<p>Why is my code not extracting all the emojis from my dataset, is there any emojis left as i defined in emoji_1? Is there any more ranges that i should compile using regex ?</p>
<p>I have tried the following answer, but it does not return anything. I get an empty column.</p>
<p><a href=""https://stackoverflow.com/questions/63762570/extract-emoji-from-series-of-text"">Extract emoji from series of text</a></p>
",Text Classification / Sentiment Analysis,extracting emojis dataframe research interest effect emojis text sentiment analysis would like extract emojis dataset far done following extracting image far got following emojis using code however partial emojis present dataset also following emojis present dataset getting result emojis code extracting emojis dataset emojis left defined emoji range compile using regex tried following answer doe return anything get empty column href emoji series text
How to use a language model for prediction after fine-tuning?,"<p>I've trained/fine-tuned a <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">Spanish RoBERTa</a> model that has recently been pre-trained for a variety of NLP tasks except for text classification.</p>
<p>Since the baseline model seems to be promising, I want to fine-tune it for a different task: text classification, more precisely, sentiment analysis of Spanish Tweets and use it to predict labels on scraped tweets I have.</p>
<p>The preprocessing and the training seem to work correctly. However, I don't know how I can use this mode afterwards for prediction.</p>
<p>I'll leave out the preprocessing part because I don't think there seems to be an issue.</p>
<h3>Code:</h3>
<pre><code># Training with native TensorFlow 
from transformers import TFAutoModelForSequenceClassification

## Model Definition
model = TFAutoModelForSequenceClassification.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;, from_pt=True, num_labels=3)

## Model Compilation
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.metrics.SparseCategoricalAccuracy()
model.compile(optimizer=optimizer, 
              loss=loss,
              metrics=metric) 

## Fitting the data 
history = model.fit(train_dataset.shuffle(1000).batch(64), epochs=3, batch_size=64)
</code></pre>
<h3>Output:</h3>
<pre><code>/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  &quot;Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 &quot;
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']
- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/5
16/16 [==============================] - 35s 1s/step - loss: 1.0455 - sparse_categorical_accuracy: 0.4452
Epoch 2/5
16/16 [==============================] - 18s 1s/step - loss: 0.6923 - sparse_categorical_accuracy: 0.7206
Epoch 3/5
16/16 [==============================] - 18s 1s/step - loss: 0.3533 - sparse_categorical_accuracy: 0.8885
Epoch 4/5
16/16 [==============================] - 18s 1s/step - loss: 0.1871 - sparse_categorical_accuracy: 0.9477
Epoch 5/5
16/16 [==============================] - 18s 1s/step - loss: 0.1031 - sparse_categorical_accuracy: 0.9714
</code></pre>
<h3>Question:</h3>
<p>How can I use the model after fine-tuning for text classification/sentiment analysis? (I want to create a predicted label for each tweet I scraped.)
<br>What would be a good way of approaching this?</p>
<p>I've tried to save the model, but I don't know where I can find it and use then:</p>
<pre><code># Save the model
model.save_pretrained('Twitter_Roberta_Model')
</code></pre>
<p>I've also tried to just add it to a HuggingFace pipeline like the following. But I'm not sure if this works correctly.</p>
<pre><code>classifier = pipeline('sentiment-analysis', 
model=model, 
tokenizer=AutoTokenizer.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;))
</code></pre>
",Text Classification / Sentiment Analysis,use language model prediction fine tuning trained fine tuned spanish roberta model ha recently pre trained variety nlp task except text classification since baseline model seems promising want fine tune different task text classification precisely sentiment analysis spanish tweet use predict label scraped tweet preprocessing training seem work correctly however know use mode afterwards prediction leave preprocessing part think seems issue code output question use model fine tuning text classification sentiment analysis want create predicted label tweet scraped would good way approaching tried save model know find use also tried add huggingface pipeline like following sure work correctly
My training and validation loss suddenly increased in power of 3,"<p><a href=""https://i.sstatic.net/CCIO8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CCIO8.png"" alt=""Training"" /></a></p>
<p><strong>train function</strong></p>
<pre><code>def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    for i, batch in enumerate(iterator):
        optimizer.zero_grad()
        output = model(batch.text)
        loss = criterion(output, torch.unsqueeze(batch.labels, 1))
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(iterator)
</code></pre>
<p><strong>main_script</strong></p>
<pre><code>def main(
        train_file,
        test_file,
        config_file,
        checkpoint_path,
        best_model_path
    ):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    with open(config_file, 'r') as j:
        config = json.loads(j.read())

    for k,v in config['model'].items():
        v = float(v)
        if v &lt; 1.0:
            config['model'][k] = float(v)
        else:
            config['model'][k] = int(v)

    for k,v in config['training'].items():
        v = float(v)
        if v &lt; 1.0:
            config['training'][k] = float(v)
        else:
            config['training'][k] = int(v)

    train_itr, val_itr, test_itr, vocab_size = data_pipeline(
        train_file,
        test_file,
        config['training']['max_vocab'],
        config['training']['min_freq'],
        config['training']['batch_size'],
        device
    )

    model = CNNNLPModel(
        vocab_size,
        config['model']['emb_dim'],
        config['model']['hid_dim'],
        config['model']['model_layer'],
        config['model']['model_kernel_size'],
        config['model']['model_dropout'],
        device
    )
    optimizer = optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss()
    num_epochs = config['training']['n_epoch']
    clip = config['training']['clip']
    is_best = False
    best_valid_loss = float('inf')
    model = model.to(device)
    for epoch in tqdm(range(num_epochs)):

        train_loss = train(model, train_itr, optimizer, criterion, clip)
        valid_loss = evaluate(model, val_itr, criterion)

        if (epoch + 1) % 2 == 0:
            print(&quot;training loss {}, validation_loss{}&quot;.format(train_loss,valid_loss))
</code></pre>
<p>I was training a Convolution Neural Network for binary Text classification. Given a sentence, it detects its a hate speech or not. Training loss and validation loss was fine till 5 epoch after that suddenly the training loss and validation loss shot up suddenly from <strong>0.2 to 10,000</strong>.</p>
<p>What could be the reason for such huge increase is loss suddenly?</p>
",Text Classification / Sentiment Analysis,training validation loss suddenly increased power train function main script wa training convolution neural network binary text classification given sentence detects hate speech training loss validation loss wa fine till epoch suddenly training loss validation loss shot suddenly could reason huge increase loss suddenly
Python Fast text Text Classification,"<p>I am attempting to classify how &quot;good&quot; short work reports are using fast text text classification. At this stage I made only one label &quot;interfering behavior&quot; which I' calling __label__int, because I just want to see if it will work. I want to compare texts with how closely they match sentences taken from good reports. I made my own training text document - a sample of which is:</p>
<p>__label__int Aggression data are low and stable at occurrences.<br/>
__label__int Elopement frequency has decreased to occurrences.<br/>
__label__int Property destruction data are low and stable at occurrences.<br/>
__label__int Non-compliance frequency is stagnant at occurrences. <br/>
__label__int Tantrum duration is low and stable at minutes.<br/>
__label__int Aggression frequency is on an increasing trend.<br/>
__label__int Crying percentage is on a decreasing trend.<br/>
__label__int Elopement frequency is on a decreasing trend.<br/></p>
<p>and my code I have written is:</p>
<pre><code>import fasttext

model = fasttext.train_supervised(input = 'Interfering Behavior Train.txt')
model.save_model(&quot;model_int-behavior.bin&quot;)

print_results(*model.test(&quot;test_valid.txt&quot;))
</code></pre>
<p>but I keep getting the following output:</p>
<p>Read 0M words
Number of words:  94
Number of labels: 1
N   0
P@1 nan
R@1 nan
Progress: 100.0% words/sec/thread:   12881 lr:  0.000000 avg.loss:  0.000000 ETA:   0h 0m 0s</p>
<p>text_valid.txt is one of the files I know has these terms in it, so im expecting a good comparison.
I could not find anything online about how to write the custom labeled data sets. Is there an issue with maybe my training data? too many words? Or is my code incomplete?</p>
",Text Classification / Sentiment Analysis,python fast text text classification attempting classify good short work report using fast text text classification stage made one label interfering behavior calling label int want see work want compare text closely match sentence taken good report made training text document sample label int aggression data low stable occurrence label int elopement frequency ha decreased occurrence label int property destruction data low stable occurrence label int non compliance frequency stagnant occurrence label int tantrum duration low stable minute label int aggression frequency increasing trend label int cry percentage decreasing trend label int elopement frequency decreasing trend code written keep getting following output read word number word number label n p nan r nan progress word sec thread lr avg loss eta h text valid txt one file know ha term im expecting good comparison could find anything online write custom labeled data set issue maybe training data many word code incomplete
Improve text classification accuracy by using POS tagging - NLP,"<p>I am doing a project which category a tweet into Health and politics categories. I used the Naive Bayes algorithm for classification.</p>
<p>I am trying to improve the accuracy of the Naive Bayes classification by applying POS tagging. Because I think, assign linguistic information will improve the classification efficiency.</p>
<p>My dataset consists like below after preprocessing and applying pos tagging:</p>
<pre><code>ID      tweet                               Category   pos_tagged_tweet
1   හාමුදුරුවරු සියලූම පූජකයන්ගේ මානසික සෞඛ්යය  Health    [(හාමුදුරුවරු, NNC), (සියලූම, NNC), (පූජකයන්ගේ, NNC), (මානසික, JJ), (සෞඛ්යය, NNC), (., FS)]
2   ද්විපාර්ශවික එකඟතා ජන ජීවිත සෞඛ්යය මනාව    Politics  [(ද්විපාර්ශවික, NNP), (එකඟතා, NNP), (ජන, JJ), (ජීවිත, NNJ), (සෞඛ්යය, NNC), (මනාව, RB),  (., FS)]
3   කරැනාකර චින නිෂ්පාදිත එන්නත ලබාගත්         Health    [(කරැනාකර, NNC), (චින, VP), (නිෂ්පාදිත, VP), (එන්නත, NNC), (ලබාගත්, VP),(., FS)]
'
'
'
</code></pre>
<p>I need to know how to apply the <strong>pos_tagged_tweet</strong> column and <strong>Category</strong> column for the Naive Bayes algorithm to categorize whether a tweet is a Health based tweet or a political tweet. I am using python and NLTK for my implementations.</p>
",Text Classification / Sentiment Analysis,improve text classification accuracy using po tagging nlp project category tweet health politics category used naive bayes algorithm classification trying improve accuracy naive bayes classification applying po tagging think assign linguistic information improve classification efficiency dataset consists like preprocessing applying po tagging need know apply po tagged tweet column category column naive bayes algorithm categorize whether tweet health based tweet political tweet using python nltk implementation
Python: Text Classification with BERT stuck at 0%,"<p>Being new to text classification, I was playing with <a href=""https://www.kaggle.com/abhishek/aaamlp/version/1?select=imdb_folds.csv"" rel=""nofollow noreferrer"">kaggle data</a> named <code>imdb_folds.csv</code>, and then the running of my code got stuck while showing this message:</p>
<pre><code>0%|                                      | 0/1250 [00:00&lt;?, ?it/s]
</code></pre>
<p>My is code is:</p>
<pre><code>## Import Packages
import tez
import torch
import torch.nn as nn
import transformers
from transformers import AdamW, get_linear_schedule_with_warmup
from sklearn import metrics
import pandas as pd

## Create a data loader using a class named BERTDataset 
class BERTDataset:
    def __init__(self, texts, targets, max_len = 64):
        self.texts = texts
        self.targets = targets
        self.tokenizer = transformers.BertTokenizer.from_pretrained(
            &quot;bert-base-uncased&quot;, #model name
            do_lower_case = False
        )
        self.max_len = max_len
        
    # length function
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        inputs = self.tokenizer.encode_plus(
        texts,
        None,
        add_special_tokens = True,
        max_length = self.max_len,
        padding = &quot;max_length&quot;,
        truncation = True
        )
        resp = {
            &quot;ids&quot;: torch.tensor(inputs[&quot;input_ids&quot;], dtype = torch.long),
            &quot;mask&quot;: torch.tensor(inputs[&quot;attention_mask&quot;], dtype = torch.long),
            &quot;token_type_ids&quot;: torch.tensor(inputs[&quot;token_type_ids&quot;], dtype = torch.long),
            &quot;targets&quot;: torch.tensor(self.targets[idx], dtype = torch.float),
            
            ## for multiclass classification, convert change dtype from torch.float to torch.long
            #&quot;targets&quot;: torch.tensor(self.targets[idx], dtype = torch.long),
        }
        return resp

## Build the model
class TextModel(tez.Model):
    def __init__(self, num_classes, num_train_steps):
        
        super().__init__()
        self.bert = transformers.BertModel.from_pretrained(
            &quot;bert-base-uncased&quot;, return_dict = False
        )
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768, num_classes) # num_classes is 1 or &gt; 1 if it is binary multiclass classification respectively
        self.num_train_steps = num_train_steps
        
        
    # optimizer
    def fetch_optimizer(self):
        opt = AdamW(self.parameters(), lr = 1e-4)
        return opt
    
    # scheduler
    def fetch_scheduler(self):
        sch = get_linear_schedule_with_warmup(
            self.optimizer, num_warmup_steps = 0, num_training_steps = self.num_train_steps        
        )
        return sch
    
    # loss
    def loss(self, outputs, targets):
        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))
    
        ## include the next line if you have multiclass classification 
        # return nn.CrossEntropyLoss()(outputs, targets)
    
    # calculate accuracy
    def monitor_metrics(self, outputs, targets):
        outputs = torch.sigmoid(outputs).cpu().detach().numpy() &gt;= 0.5
        
       
        targets = targets.cpu().detach().numpy()
        return {&quot;accuracy&quot;: metrics.accuracy_score(targets, outputs)}
    
    # forward function
    def forward(self, ids, mask, token_type_ids, targets = None):
        _, x = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)
        x = self.bert_drop(x)
        x = self.out(x)
        if targets is not None:
            loss = self.loss(outputs, targets)
            met = self.monitor_metrics(outputs, targets)
            return x, loss, met
        return x, 0, {} # if there is no target, return 0        


## Read Dataset
def train_model(fold):
    df = pd.read_csv(&quot;imdb_folds.csv&quot;) # read file
    df_train = df[df.kfold != fold].reset_index(drop=True)
    df_valid = df[df.kfold == fold].reset_index(drop=True)
    
    train_dataset = BERTDataset(df_train.review.values, df_train.sentiment.values)
    valid_dataset = BERTDataset(df_valid.review.values, df_valid.sentiment.values)
    
    # n_train_steps = int(len(df_train) / TRAIN_BS * EPOCHS)
    n_train_steps = int(len(df_train) / 32 * 10)
    model = TextModel(num_classes = 1, num_train_steps = n_train_steps)
    
    es = tez.callbacks.EarlyStopping(monitor = &quot;valid_loss&quot;, patience = 3, model_path = &quot;model.bin&quot;)
    model.fit(
        train_dataset, 
        valid_dataset = valid_dataset, 
        device = &quot;cuda&quot;, 
        epochs = 10, 
        train_bs = 32,
        callbacks = [es],
    )
    
    
if __name__==&quot;__main__&quot;:
    train_model(fold = 0)
</code></pre>
<p>I found similar problem on <a href=""https://stackoverflow.com/questions/67506630/python-tqdm-progress-bar-stuck-at-0"">stackoverflow</a> but could not adapt the solution. Any help to fix this ll be appreciated.</p>
",Text Classification / Sentiment Analysis,python text classification bert stuck new text classification wa playing kaggle data named running code got stuck showing message code found similar problem href could adapt solution help fix appreciated p
How to measure similarity between sentences inside a cluster after clustering?,"<p>I'm conducting topic modeling analysis on messages from public Telegram groups, super new to this area so just learning.</p>
<p>I've been following this example here (<a href=""https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6"" rel=""nofollow noreferrer"">https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6</a>), and tried swapping out the HDBSCAN clustering algorithm with the one in BERT's documentation util.community_detection (<a href=""https://www.sbert.net/docs/package_reference/util.html"" rel=""nofollow noreferrer"">https://www.sbert.net/docs/package_reference/util.html</a>).</p>
<p>When I output the results of the clusters in this example (4899 Telegram messages), I get something that looks like this.</p>
<ul>
<li>Topic: just a cluster label</li>
<li>Doc: all the messages in that cluster combined together</li>
<li>0: top keywords found via tf-idf</li>
</ul>
<p><a href=""https://i.sstatic.net/Vo9ff.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Vo9ff.png"" alt=""enter image description here"" /></a></p>
<p>The problem I'm concerned with is that, there are clearly a ton of messages that are basically identical to each other, I've marked them in yellow. A few examples,</p>
<ul>
<li>Cluster 3: this is just a bunch of &quot;hellos&quot; and variations thereof</li>
<li>Cluster 5: this is just a bunch of &quot;Ok&quot;s, people saying yes / ok</li>
<li>Cluster 7: people just saying thanks and variations on that</li>
<li>Cluster 9: some variations and misspellings of the word &quot;gas&quot;</li>
<li>Cluster 19: just &quot;siap&quot; which I think means &quot;sorry if I already posted&quot;</li>
</ul>
<p>To a human reader I feel like this type of text should just be excluded from the analysis altogether, the question is how do I detect it.</p>
<p>Since they're already grouped together by the clustering algorithm, the algorithm must have ways to measure the &quot;similarity&quot; between these messages within a cluster. But I don't seem to be able to find these values exposed anywhere or what it's called. Like for example the HDBSCAN algorithm (<a href=""https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#"" rel=""nofollow noreferrer"">https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#</a>), I skimmed through the doc a few times and didn't find any such property or measure exposed, am I missing something here?</p>
<p>My hypothesis is that for the cases where it's just a word or a short phrase repeated over and over again, this similarity value must be super super high, and I'd just say &quot;clusters whose internal similarity is higher than this threshold are getting thrown out&quot;.</p>
<p>Any help &amp; advice would be greatly appreciated, thanks!</p>
",Text Classification / Sentiment Analysis,measure similarity sentence inside cluster clustering conducting topic modeling analysis message public telegram group super new area learning following example tried swapping hdbscan clustering algorithm one bert documentation util community detection output result cluster example telegram message get something look like topic cluster label doc message cluster combined together top keywords found via tf idf problem concerned clearly ton message basically identical marked yellow example cluster bunch hello variation thereof cluster bunch ok people saying yes ok cluster people saying thanks variation cluster variation misspelling word gas cluster siap think mean sorry already posted human reader feel like type text excluded analysis altogether question detect since already grouped together clustering algorithm algorithm must way measure similarity message within cluster seem able find value exposed anywhere called like example hdbscan algorithm skimmed doc time find property measure exposed missing something hypothesis case word short phrase repeated similarity value must super super high say cluster whose internal similarity higher threshold getting thrown help advice would greatly appreciated thanks
Feature Selection and Reduction for Text Classification,"<p>I am currently working on a project, a <strong>simple sentiment analyzer</strong> such that there will be <strong>2 and 3 classes</strong> in <strong>separate cases</strong>. I am using a <strong>corpus</strong> that is pretty <strong>rich</strong> in the means of <strong>unique words</strong> (around 200.000). I used <strong>bag-of-words</strong> method for <strong>feature selection</strong> and to reduce the number of <strong>unique features</strong>, an elimination is done due to a <strong>threshold value</strong> of <strong>frequency of occurrence</strong>. The <strong>final set of features</strong> includes around 20.000 features, which is actually a <strong>90% decrease</strong>, but <strong>not enough</strong> for intended <strong>accuracy</strong> of test-prediction. I am using <strong>LibSVM</strong> and <strong>SVM-light</strong> in turn for training and prediction (both <strong>linear</strong> and <strong>RBF kernel</strong>) and also <strong>Python</strong> and <strong>Bash</strong> in general.</p>

<p>The <strong>highest accuracy</strong> observed so far <strong>is around 75%</strong> and I <strong>need at least 90%</strong>. This is the case for <strong>binary classification</strong>. For <strong>multi-class training</strong>, the accuracy falls to <strong>~60%</strong>. I <strong>need at least 90%</strong> at both cases and can not figure how to increase it: via <strong>optimizing training parameters</strong> or <strong>via optimizing feature selection</strong>?</p>

<p>I have read articles about <strong>feature selection</strong> in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows:</p>

<ul>
<li>Frequency approach of <strong>bag-of-words</strong> (BOW)</li>
<li><strong>Information Gain</strong> (IG)</li>
<li><strong>X^2 Statistic</strong> (CHI)</li>
</ul>

<p>The first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of <strong>IG</strong> and <strong>CHI</strong> and looking for any help to guide me in that way.</p>

<p>Thanks a lot, and if you need any additional info for help, just let me know.</p>

<hr>

<ul>
<li><p>@larsmans: <strong>Frequency Threshold</strong>: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.   </p></li>
<li><p>@TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found <strong>Proportional Difference</strong> (PD) method the best for feature selection, where features are uni-grams and <strong>Term Presence</strong> (TP) for the weighting (I didn't understand why you tagged <strong>Term-Frequency-Inverse-Document-Frequency</strong> (TF-IDF) as an indexing method, I rather consider it as a <strong>feature weighting</strong> approach).  <strong>Pre-processing</strong> is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as <strong>morphological parsing</strong> and <strong>stemming</strong>. Also note that I am working on <strong>Turkish</strong>, which has <strong>different characteristics</strong> compared to English. Finally, I managed to reach <strong>~88% accuracy</strong> (f-measure) for <strong>binary</strong> classification and <strong>~84%</strong> for <strong>multi-class</strong>. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried <strong>LDA</strong> and <strong>LSI</strong> and moving on to <strong>moVMF</strong> and maybe <strong>spherical models</strong> (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between <strong>feature space dimension reduction</strong> methods (LDA, LSI, moVMF etc.) and <strong>clustering methods</strong> (k-means, hierarchical etc.).</p></li>
</ul>
",Text Classification / Sentiment Analysis,feature selection reduction text classification currently working project simple sentiment analyzer class separate case using corpus pretty rich mean unique word around used bag word method feature selection reduce number unique feature elimination done due threshold value frequency occurrence final set feature includes around feature actually decrease enough intended accuracy test prediction using libsvm svm light turn training prediction linear rbf kernel also python bash general highest accuracy observed far around need least case binary classification multi class training accuracy fall need least case figure increase via optimizing training parameter via optimizing feature selection read article feature selection text classification found three different method used actually clear correlation among method follows frequency approach bag word bow information gain ig x statistic chi first method already one use use simply need guidance better use order obtain high enough accuracy also lacking knowledge practical implementation ig chi looking help guide way thanks lot need additional info help let know larsmans frequency threshold looking occurrence unique word example word occurring different example frequently enough included feature set unique feature themanwithnoname first thanks effort explaining general concern document classification examined experimented method bring forward others found proportional difference pd method best feature selection feature uni gram term presence tp weighting understand tagged term frequency inverse document frequency tf idf indexing method rather consider feature weighting approach pre processing also important aspect task mentioned used certain type string elimination refining data well morphological parsing stemming also note working turkish ha different characteristic compared english finally managed reach accuracy f measure binary classification multi class value solid proof success model used done far working clustering reduction model tried lda lsi moving movmf maybe spherical model lda movmf seems work better corpus objective nature like news corpus information guidance issue appreciate need info especially setup interface python oriented open source feature space dimension reduction method lda lsi movmf etc clustering method k mean hierarchical etc
TfidfVectorizer returns empty elements,"<p>I am currently doing sentiment analysis project. I fit the vectorizer with my train data in dataframe format. Then I transform the test data with the same vectorizer but it returns nothing for me. I did check the TfidfVectorizer.get_feature_names() and the desired transform word already exist inside the features. What is wrong with my vectorizer?</p>
<p>Code:</p>
<pre><code>vectorizer = TfidfVectorizer(analyzer=lambda x: x)
x = vectorizer.fit_transform(data['clean_text'])

print(vectorizer.get_feature_names()[9427])
# output sad
print(vectorizer.transform([&quot;sad&quot;]))
# empty result
print(vectorizer.transform([&quot;sad&quot;]).toarray())
# return a whole 0 array

Sample data format (dataframe)
    sentiment   clean_text
0     0        [respond, go]
1     1        [sooo, sad]
2     1        [bulli]
3     1        [leav, alon]
4     1        [cry]
</code></pre>
",Text Classification / Sentiment Analysis,tfidfvectorizer return empty element currently sentiment analysis project fit vectorizer train data dataframe format transform test data vectorizer return nothing check tfidfvectorizer get feature name desired transform word already exist inside feature wrong vectorizer code
How to get the accuracy of predicted model python,"<p>I made model from <code>sklearn</code> for multiclass text classification</p>
<p>I want to know the accuracy of predicted sentence I give to model</p>
<pre><code>level0_model.predict(['I sold 4 apples for 4 Dollar'])
</code></pre>
<blockquote>
<p>Output : sell</p>
</blockquote>
<p>I want something like that</p>
<blockquote>
<p>Output : sell accuracy : %99</p>
</blockquote>
",Text Classification / Sentiment Analysis,get accuracy predicted model python made model multiclass text classification want know accuracy predicted sentence give model output sell want something like output sell accuracy
How to do cross validation in DistilBERT,"<p>I have made a simple model to do text classification using DistilBERT. The problem is I am unable to figure out how to do cross-validation while training. My code implementation is provided below.</p>
<p>Can anyone help me to implement cross-validation while training?</p>
<p>Thank you in advance.</p>
<pre><code>    #Split into Train-Test-Validation    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 0)
    X_val, X_test, y_val, y_test = train_test_split(X_test,y_test, test_size=0.10, random_state=42)
    
    
    #Encoding text for train data
    train_encoded = tokenizer(X_train, truncation=True, padding=True, return_tensors=&quot;tf&quot;)
    train_data = tf.data.Dataset.from_tensor_slices((dict(train_encoded), y_train))
    
    #Encoding text for validation data
    val_encoded = tokenizer(X_val, truncation=True, padding=True, return_tensors=&quot;tf&quot;)
    val_data = tf.data.Dataset.from_tensor_slices((dict(val_encoded), y_val))
    
    #Encoding text for testing data
    test_data = tf.data.Dataset.from_tensor_slices((dict(test_encoded), y_test))
    test_encoded = tokenizer(X_test, truncation=True, padding=True, return_tensors=&quot;tf&quot;)
    
    #Load distil bert model
    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)
    model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])
    model.fit(train_data.batch(16), epochs=10, batch_size=16)
</code></pre>
",Text Classification / Sentiment Analysis,cross validation distilbert made simple model text classification using distilbert problem unable figure cross validation training code implementation provided anyone help implement cross validation training thank advance
Using Keras Tokenizer to generate n-grams,"<p>Is it possible to use n-grams in Keras?</p>

<p>E.g., sentences contain in X_train dataframe with ""sentences"" column.</p>

<p>I use tokenizer from Keras in the following manner:</p>

<pre><code>tokenizer = Tokenizer(lower=True, split=' ')
tokenizer.fit_on_texts(X_train.sentences)
X_train_tokenized = tokenizer.texts_to_sequences(X_train.sentences)
</code></pre>

<p>And later I pad the sentences thus:</p>

<pre><code>X_train_sequence = sequence.pad_sequences(X_train_tokenized)
</code></pre>

<p>Also I use a simple LSTM network:</p>

<pre><code>model = Sequential()
model.add(Embedding(MAX_FEATURES, 128))
model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2,
               activation='tanh', return_sequences=True))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2, activation='tanh'))
model.add(Dense(number_classes, activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop',
              metrics=['accuracy'])
</code></pre>

<p>In this case, tokenizer execution.
In Keras docs: <a href=""https://keras.io/preprocessing/text/"" rel=""noreferrer"">https://keras.io/preprocessing/text/</a>
I see character processing is possible, but that is not appropriate for my case.</p>

<p>My main question: Can I use n-grams for NLP tasks (not only Sentiment Analysis but rather any NLP task)</p>

<p>For clarification: I'd like to consider not just words but combination of words.  I'd like to try and see if it helps to model my task.</p>
",Text Classification / Sentiment Analysis,using kera tokenizer generate n gram possible use n gram kera e g sentence contain x train dataframe sentence column use tokenizer kera following manner later pad sentence thus also use simple lstm network case tokenizer execution kera doc see character processing possible appropriate case main question use n gram nlp task sentiment analysis rather nlp task clarification like consider word combination word like try see help model task
Labels of clustered data and KMeans cluster centers,"<p>Relating to the question <a href=""https://stackoverflow.com/questions/54240144/distance-between-nodes-and-the-centroid-in-a-kmeans-cluster"">Starting question</a> I have doubts regarding calculating coordinates of cluster centres and labeling the centres:</p>
<p><code>kmeans.cluster_centers_</code></p>
<p>gives</p>
<pre><code>[[ 4.87744023 -0.48344163]
[ 8.29540909  6.7398487 ]
[ 1.05638163  3.84314976]]
</code></pre>
<p>I'm confused with the order of centres. The first one is 'green' cluster (label 2 in the plot), the second one is the 'red' cluster (label 0 in the plot) and last one is the 'blue' one with the label 1 in the plot. What is the logic behind it?</p>
<p>Also, what in case if I have labeled data for clustering as a starting point for clustering - for example Wine quality dataset <a href=""https://archive.ics.uci.edu/ml/datasets/wine+quality"" rel=""nofollow noreferrer"">WineQuality</a> or Twitter sentiment analysis <a href=""https://www.kaggle.com/crowdflower/twitter-airline-sentiment"" rel=""nofollow noreferrer"">Twitter sentiment analisys</a>. I know the labels for clusters and would like to perserve them as labels for clusters and of course to relate them to cluster centre?</p>
",Text Classification / Sentiment Analysis,label clustered data kmeans cluster center relating question winequality twitter sentiment analysis twitter sentiment analisys know label cluster would like perserve label cluster course relate cluster centre
Plot Confusion Matrix from Roberta Model,"<p>I wrote the text classification code with two classes using the Roberta model and now I want to draw the confusion matrix.
How to go about plotting the confusion matrix based of a Roberta model?</p>
<pre><code>RobertaTokenizer = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=False)
roberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2)

input_ids=[]
attention_masks=[]

for sent in sentences:
    bert_inp=RobertaTokenizer.encode_plus(sent,add_special_tokens = True,max_length =128,pad_to_max_length = True,return_attention_mask = True)
    input_ids.append(bert_inp['input_ids'])
    attention_masks.append(bert_inp['attention_mask'])
    

input_ids=np.asarray(input_ids)
attention_masks=np.array(attention_masks)
labels=np.array(labels)
#split
train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.5)
print('Train inp shape {} Val input shape {}\nTrain label shape {} Val label shape {}\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))
#
log_dir='tensorboard_data/tb_roberta'
model_save_path='/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py'

callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]

print('\nBert Model',roberta_model.summary())

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)

roberta_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])

history=roberta_model.fit([train_inp,train_mask],train_label,batch_size=16,epochs=2,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)
</code></pre>
",Text Classification / Sentiment Analysis,plot confusion matrix roberta model wrote text classification code two class using roberta model want draw confusion matrix go plotting confusion matrix based roberta model
Is it possible to retrieve only selected messages from Whatsapp via Python?,"<p>I'm working on a project, where I want to analyze text messages from Whatsapp via Python to see if they indicate that a person shows signs of chronic stress. For this, the user should choose if all messages should be used or select only certain messages that are crucial for the analysis. My goal is to obtain messages and feed them into an ML model to classify the text. After that, an analysis will be performed and the user will be informed when the analysis is finished.</p>
<p>Now I'm looking at potential libraries that could help me retrieve these messages. Unfortunately, the ones that I found are missing some of the key functionality that I require:</p>
<p><a href=""https://github.com/eddyharrington/WhatSoup"" rel=""nofollow noreferrer"">WhatSoup</a> - exports the entire chat of a WhatsApp conversation and saves it into a .txt, .csv or .HTML file. While that would be more than enough, I want to allow the user also to only export certain messages, if required, and not the whole chat from a certain date.</p>
<p><a href=""https://www.twilio.com/docs/whatsapp/quickstart/python"" rel=""nofollow noreferrer"">Twilio</a> - enables applications to send and retrieve messages. While the sending part would be nice, there does not seem to be an option to select messages from the chats that should be retrieved.</p>
<p><a href=""https://github.com/mukulhase/WebWhatsapp-Wrapper"" rel=""nofollow noreferrer"">WebWhatsApp-Wrapper</a> - is similar to Twilio but with more functionalities. But the function to let the user select specific messages is not implemented.</p>
<p>Does anybody know how I could achieve my idea?</p>
<p>I would appreciate any input :)</p>
",Text Classification / Sentiment Analysis,possible retrieve selected message whatsapp via python working project want analyze text message whatsapp via python see indicate person show sign chronic stress user choose message used select certain message crucial analysis goal obtain message feed ml model classify text analysis performed user informed analysis finished looking potential library could help retrieve message unfortunately one found missing key functionality require whatsoup export entire chat whatsapp conversation save txt csv html file would enough want allow user also export certain message required whole chat certain date twilio enables application send retrieve message sending part would nice doe seem option select message chat retrieved webwhatsapp wrapper similar twilio functionality function let user select specific message implemented doe anybody know could achieve idea would appreciate input
Document classification using machine learning,"<p>I am currently working on a project, where I need to be able to dynamically classify incoming documents. These documents can come in text PDF files as well as scanned PDF files.</p>

<p>I have the following labels:</p>

<ul>
<li>Invoice</li>
<li>Packing list</li>
<li>Certificate</li>
</ul>

<p>I am trying to figure out how I should approach this problem. </p>

<h1>My initial thoughts</h1>

<p>I was thinking the best way to solve this issue would be to perform text classification, based on the document text.</p>

<p><strong>Step 1 - Train a model</strong></p>

<ul>
<li>First convert the PDF files to text.</li>
<li>Then label the text content by one of the three labels. (Do this for a large dataset)</li>
</ul>

<p><strong>Step 2 - Use the model</strong></p>

<ul>
<li>Once the model is trained, for new incoming documents, convert it to text.</li>
<li>Run the text content through the model to get the text classification.</li>
</ul>

<p>Is there another way to do this? My concerns are that I am not sure if you can perform NLP on <em>entire</em> text documents? Maybe object detection (Computer Vision) is needed instead?</p>
",Text Classification / Sentiment Analysis,document classification using machine learning currently working project need able dynamically classify incoming document document come text pdf file well scanned pdf file following label invoice packing list certificate trying figure approach problem initial thought wa thinking best way solve issue would perform text classification based document text step train model first convert pdf file text label text content one three label large dataset step use model model trained new incoming document convert text run text content model get text classification another way concern sure perform nlp entire text document maybe object detection computer vision needed instead
Using mutliple predictors for text classification prediction through Multinomial Bayes,"<p>My dataset contains only categorical variables. I have little problems using one categorical column to predict another of a dataframe, but I find it hard to wrap my head around how to use multiple columns/predictors to predict.</p>
<p>Let's say my dataset looks something like this:</p>
<pre><code>ItemCode  ItemDescription  Kind_of_food 
273          Snicker         Chocolate 
230          Lay's Chips       Chips
274          KitKat          Chocolate
123          Gummy Bears       Candy
124          Oreo            Cookies 
123          Gummy Bears       Candy  
273          Snicker        Chocolate          

. . . x 1000000 rows.
</code></pre>
<p>If I were to predict Item Code using only Item Description, I first cleaned the dataset which is not shown below ( removing stopwords, apostrophes, etc.). I would then run it through train_test_split.</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.metric import accuracy_score
from nltk.stem.porter import PorterStemmer()



x_train, x_test, y_train, y_test = train_test_split(df['ItemDescription'], df['ItemCode'], , train_size = 100000, test_size = 30000, stratify = df['ItemCode']

stemmer = PorterStemmer()
analyzer = CountVectorizer().build_analyzer()

def stemmed(doc):
  return(stemmer.stem(w) for w in analyzer(doc))

vect = CountVectorizer(ngram = range(2,2), max_features = 500, stop_words = stopWords, analyzer = stemmed_words, tokenizer = word_tokenizer) # stopWords is defined earlier and not showed in code, 

X_train = vect.fit_transform(x_train)
X_test = vect.transform(x_test)

multiNB = MultinomialNB(alpha = 0.2)
multiNB.fit(X_train, y_train)
predicted = multiNB.predict(X_test)

print(&quot;accuracy of test model is: &quot;, accuracy_score(predicted, y_test))
</code></pre>
<p>This code works fine for 1 predictor but if I were to combine the Kind of Foods column through dummy variables.</p>
<pre><code>dummies = pd.getDummies(df.Kind_of_food)
df = pd.concat([df, dummies], axis = 'columns')
df = df.drop(['ItemCode', 'Cookies'], axis = 1)
</code></pre>
<p>I then make a new variable,</p>
<pre><code>X = df[['ItemDescription', 'Cookies', 'Chips', 'Candy', 'Chocolate']] 
</code></pre>
<p>and change train_test_split from:</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(df['ItemDescription'], df['ItemCode'], , train_size = 100000, test_size = 30000, stratify = df['ItemCode']
</code></pre>
<p>to:</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size = 100000, test_size = 30000, stratify = Y)
</code></pre>
<p>I would get</p>
<pre><code>Found input variables with inconsistent number of samples [3, 100000]
</code></pre>
<p>when I try to run the same code.</p>
<p>The code breaks on the multiNB.fit line when trying to fit x_train (100000, 3) and y_train (100000), How should I adjust my code and proceed?</p>
",Text Classification / Sentiment Analysis,using mutliple predictor text classification prediction multinomial bayes dataset contains categorical variable little problem using one categorical column predict another dataframe find hard wrap head around use multiple column predictor predict let say dataset look something like predict item code using item description first cleaned dataset shown removing stopwords apostrophe etc would run train test split code work fine predictor combine kind food column dummy variable make new variable change train test split would get try run code code break multinb fit line trying fit x train train adjust code proceed
Saving Sentiment Analysis Model,"<p>I am new to machine learning and doing a project on sentiment analysis for tourism reviews. I first preprocessed the reviews and performed feature extraction and then I chose Naïve Bayes classifier and get the accuracy of 78%. But now I am stuck here that how to save this model and vector features so that I could predict the review in future using this model. Here is the code...</p>
<pre><code>vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(1,1))
X = vectorizer.fit_transform(reviews).toarray()
feature_names = vectorizer.get_feature_names()



X_train,X_test,Y_train,Y_test=train_test_split(X,polarity,test_size=0.30,random_state=0)
classifier=MultinomialNB()
classifier.fit(X_train,Y_train) 
Y_predict=classifier.predict(X_test)
report=metrics.classification_report(Y_test, Y_predict)
accuracy=metrics.accuracy_score(Y_test, Y_predict) 
</code></pre>
<p>Can anybody can guide?</p>
",Text Classification / Sentiment Analysis,saving sentiment analysis model new machine learning project sentiment analysis tourism review first preprocessed review performed feature extraction chose na bayes classifier get accuracy stuck save model vector feature could predict review future using model code anybody guide
How to use Hugging Face sentiment analysis for a specified subject in a sentence?,"<p>When using Hugging Face for sentiment analysis, it looks like you get the sentiment for a sentence as a whole. However if you have a sentence that compares 2 subject and one surpasses the other, it would look at the sentence as a whole. But the sentiment of one subject would be more positive while the other would be more negative.</p>
<p>Example:</p>
<blockquote>
<p>'People enjoy Hagen Dazs over Breyers'</p>
</blockquote>
<p>That's good for Hagen Dazs, but bad for Breyers. Hugging Face returns &quot;Positive&quot;.</p>
<blockquote>
<p>'You should buy Hagen Dazs instead of Breyers'</p>
</blockquote>
<p>Hugging Face returns &quot;Negative&quot; here which would apply to Breyers but not Hagen Dazs.</p>
<pre><code># Example script using the above text
from transformers import pipeline
classifier = pipeline(&quot;sentiment-analysis&quot;)
classifier('People enjoy Hagen Dazs over Breyers')

&gt;&gt; [{'label': 'POSITIVE', 'score': 0.9575461745262146}]

classifier('You should buy Hagen Dazs instead of Breyers')

&gt;&gt; [{'label': 'NEGATIVE', 'score': 0.9988893270492554}]
</code></pre>
<p>Is is possible to specify the topic to get the sentiment for in a sentence?</p>
",Text Classification / Sentiment Analysis,use hugging face sentiment analysis specified subject sentence using hugging face sentiment analysis look like get sentiment sentence whole however sentence compare subject one surpasses would look sentence whole sentiment one subject would positive would negative example people enjoy hagen dazs breyers good hagen dazs bad breyers hugging face return positive buy hagen dazs instead breyers hugging face return negative would apply breyers hagen dazs possible specify topic get sentiment sentence
Import TextLMDataBunch from Fastai,"<p>I am following <a href=""https://www.jashds.com/blog/guide-to-text-classification-withnbspfastai"" rel=""nofollow noreferrer"">this tutorial</a> to build a NLP sentiment analysis model.</p>
<pre><code>from fastai.text import *
</code></pre>
<p>This is the only import specified that includes fastai.</p>
<p>Unfortunately the <a href=""http://%20https://fastai1.fast.ai/text.data.html#TextLMDataBunch"" rel=""nofollow noreferrer"">TextLMDataBunch</a> is undefined.</p>
<p>What import should I used to have this class avaialable?</p>
<p>I have already tried:</p>
<pre><code>from fastai.text.data import TextLMDataBunch
</code></pre>
<p>But apparently fastai.text.data is not even a package.</p>
",Text Classification / Sentiment Analysis,import textlmdatabunch fastai following tutorial build nlp sentiment analysis model import specified includes fastai unfortunately textlmdatabunch undefined import used class avaialable already tried apparently fastai text data even package
"Iterate code over each text review in the list of texts, writing a loop","<p>I am implementing Aspect-Based-Sentiment-Analysis.
My data look like:</p>
<pre><code>        product  rating  body                   price_aspect    packaging_aspect   quality_aspect scent_aspect  difference_aspect

0   0   Elemis ...  1   I love ememis but...    I love ememis but..None None    None    None
1   1   Elemis ...  5   Great                       None           None None    None    None
3   3   Elemis ...  5   My husband loves it...      None           None None    None    None
4   4   Elemis ...  4   This product, although...   None           None None    None    None
</code></pre>
<p>The column &quot;body&quot; is the text of the review. The columns &quot;price/packaging/etc/_aspect&quot; include the text of the reviews (from column &quot;body&quot;) where a certain aspect was mentioned. So here you see that aspect &quot;price&quot; was mentioned in the first review.</p>
<p>My goal is to define a sentiment of the review in column &quot;price_aspect&quot;. I implement Aspect-Based-Sentiment-Analysis for this.
My code is here:</p>
<pre><code>recognizer = absa.aux_models.BasicPatternRecognizer()
nlp = absa.load(pattern_recognizer=recognizer)
text = df.price_aspect[0]
completed_task = nlp(text=text, aspects=['price', 'packaging', 'quality', 'scent', 'difference'])
price, packaging, quality, scent, difference = completed_task.examples
absa.summary(price)
</code></pre>
<p>My output when I call absa.summary(price):</p>
<pre><code>Sentiment.negative for &quot;price&quot;
Scores (neutral/negative/positive): [0.003 0.954 0.043]
</code></pre>
<p>This is nice and that is what I need. However, I want to iterate this code over each line in the column &quot;price_aspect&quot; at once, not only for one line (line 0 in my case). So I need to change <code>text = df.price_aspect[0]</code> in my code that will refer me to the next line (not only line 0). Do I need a loop here? Or should I write a loop here:<code>completed_task = nlp(text=text, aspects=['price', 'packaging', 'quality', 'scent', 'difference'])</code>?
Moreover, when I get the result in the form &quot;Sentiment.negative for price&quot; I want to append it into a new column as a string.
I am new to Python. Would you refer me to any source that solves my request?</p>
",Text Classification / Sentiment Analysis,iterate code text review list text writing loop implementing aspect based sentiment analysis data look like column body text review column price packaging etc aspect include text review column body certain aspect wa mentioned see aspect price wa mentioned first review goal define sentiment review column price aspect implement aspect based sentiment analysis code output call absa summary price nice need however want iterate code line column price aspect one line line case need change code refer next line line need loop write loop moreover get result form sentiment negative price want append new column string new python would refer source solves request
Pointwise mutual information on text,"<p>I was wondering how one would calculate the pointwise mutual information for text classification. To be more exact, I want to classify tweets in categories. I have a dataset of tweets (which are annotated), and I have a dictionary per category of words which belong to that category. Given this information, how is it possible to calculate the PMI for each category per tweet, to classify a tweet in one of these categories.</p>
",Text Classification / Sentiment Analysis,pointwise mutual information text wa wondering one would calculate pointwise mutual information text classification exact want classify tweet category dataset tweet annotated dictionary per category word belong category given information possible calculate pmi category per tweet classify tweet one category
Retrieving attention weights for sentences? Most attentive sentences are zero vectors,"<p>I have a document classification task, that classifies documents as good (1) or bad (0), and I use some sentence embeddings for each document to classify the documents accordingly.</p>
<p><strong>What I like to do is retrieving the attention scores for each document, to obtain the most &quot;relevant&quot; sentences (i.e., those with high attention scores)</strong></p>
<p><em>I padded each document to the same length</em> (i.e., 1000 sentences per document). So my tensor for 5000 documents looks like this <code>X = np.ones(shape=(5000, 1000, 200))</code> (5000 documents with each having a 1000 sequence of sentence vectors and each sentence vector consisting of 200 features).</p>
<p>My network looks like this:</p>
<pre><code>no_sentences_per_doc = 1000
sentence_embedding = 200

sequence_input  = Input(shape=(no_sentences_per_doc, sentence_embedding))
gru_layer = Bidirectional(GRU(50,
                          return_sequences=True
                          ))(sequence_input)
sent_dense = Dense(100, activation='relu', name='sent_dense')(gru_layer)  
sent_att,sent_coeffs = AttentionLayer(100,return_coefficients=True, name='sent_attention')(sent_dense)
preds = Dense(1, activation='sigmoid',name='output')(sent_att)  
model = Model(sequence_input, preds)

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=[TruePositives(name='true_positives'),
                      TrueNegatives(name='true_negatives'),
                      FalseNegatives(name='false_negatives'),
                      FalsePositives(name='false_positives')
                      ])

history = model.fit(X, y, validation_data=(x_val, y_val), epochs=10, batch_size=32)
</code></pre>
<p>After training I retrieved the attention scores as follows:</p>
<pre><code>sent_att_weights = Model(inputs=sequence_input,outputs=sent_coeffs)

## load a single sample
## from file with 150 sentences (one sentence per line)
## each sentence consisting of 200 features
x_sample = np.load(x_sample)
## and reshape to (1, 1000, 200)
x_sample = x_sample.reshape(1,1000,200) 

output_array = sent_att_weights.predict(x_sample)
</code></pre>
<p><em>However,</em> if I show the top 3 attention scores for the sentences, I also obtain sentence indices that are, for example, <code>[432, 434, 999]</code> for a document that has only 150 sentences (the rest is padded, i.e., just zeros).</p>
<p><strong>Does that make sense or am I doing something wrong here?</strong> (is there a mistake in my attention layer? Or is due to a low F-score?)</p>
<p>The attention layer I use is the following:</p>
<pre><code>class AttentionLayer(Layer):
    &quot;&quot;&quot;
    https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/
    &quot;&quot;&quot;
    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):
        # Initializer 
        self.supports_masking = True
        self.return_coefficients = return_coefficients
        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution
        self.attention_dim = attention_dim
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # Builds all weights
        # W = Weight matrix, b = bias vector, u = context vector
        assert len(input_shape) == 3
        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')
        self.b = K.variable(self.init((self.attention_dim, )),name='b')
        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')
        self.trainable_weights = [self.W, self.b, self.u]

        super(AttentionLayer, self).build(input_shape)

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, hit, mask=None):
        # Here, the actual calculation is done
        uit = K.bias_add(K.dot(hit, self.W),self.b)
        uit = K.tanh(uit)
        
        ait = K.dot(uit, self.u)
        ait = K.squeeze(ait, -1)
        ait = K.exp(ait)
        
        if mask is not None:
            ait *= K.cast(mask, K.floatx())

        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        ait = K.expand_dims(ait)
        weighted_input = hit * ait
        
        if self.return_coefficients:
            return [K.sum(weighted_input, axis=1), ait]
        else:
            return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        if self.return_coefficients:
            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]
        else:
            return input_shape[0], input_shape[-1]
</code></pre>
<p>Note that I use <code>keras</code> with <code>tensorflow</code> backend version 2.1.; the attention layer was originally written for theano, but I use <code>import tensorflow.keras.backend as K</code></p>
",Text Classification / Sentiment Analysis,retrieving attention weight sentence attentive sentence zero vector document classification task classifies document good bad use sentence embeddings document classify document accordingly like retrieving attention score document obtain relevant sentence e high attention score padded document length e sentence per document tensor document look like document sequence sentence vector sentence vector consisting feature network look like training retrieved attention score follows however show top attention score sentence also obtain sentence index example document ha sentence rest padded e zero doe make sense something wrong mistake attention layer due low f score attention layer use following note use backend version attention layer wa originally written theano use
Detecting questions in text,"<p>I have a project where I need to analyze a text to extract some information if the user who post this text need help in something or not, I tried to use sentiment analysis but it didn't work as expected, my idea was to get the negative post and extract the main words in the post and suggest to him some articles about that subject, if there is another way that can help me please post it below and thanks.</p>

<p>for the dataset i useed, it was a dataset for sentiment analyze, but now I found that it's not working and I need a dataset use for this subject.</p>
",Text Classification / Sentiment Analysis,detecting question text project need analyze text extract information user post text need help something tried use sentiment analysis work expected idea wa get negative post extract main word post suggest article subject another way help please post thanks dataset useed wa dataset sentiment analyze found working need dataset use subject
Information retrieval data collection,"<p>I am looking for datasets designated for information retrieval. I found some datasets that are used for other NLP tasks like text categorization or sentiment analysis, but I haven't had any luck finding something specifically for information retrieval yet. Are there any places to look? </p>

<p>Thank you for any help.</p>
",Text Classification / Sentiment Analysis,information retrieval data collection looking datasets designated information retrieval found datasets used nlp task like text categorization sentiment analysis luck finding something specifically information retrieval yet place look thank help
Text Classification - Handling text that does not fit into any category,"<p>I am working on developing a text classifier and have found some solid options. However the one thing I am struggling with is handling text that does not fit into any predefined categories. This is definitely going to be something we see with real data so I need to understand how to handle it.</p>
<p>I have noticed that when I look at the predict_proba output, (for naive bayes/boosting) the values need to add up to 1. I think this is a problem because if the text meets no category, the algorithms still require that output to equal one, and it will still assign some arbitrary label to that text, even though it really should not.</p>
<p>I have come up with some solutions to take the max probability if within a .90 threshold and assign that category, but I feel this is probably not the best approach.</p>
<p>Does any one have any suggestions on some methods I may be able to try to solve for this?</p>
<p>Thanks</p>
",Text Classification / Sentiment Analysis,text classification handling text doe fit category working developing text classifier found solid option however one thing struggling handling text doe fit predefined category definitely going something see real data need understand handle noticed look predict proba output naive bayes boosting value need add think problem text meet category algorithm still require output equal one still assign arbitrary label text even though really come solution take max probability within threshold assign category feel probably best approach doe one suggestion method may able try solve thanks
Is there a way to download TextBlob corpora to Google Cloud Run?,"<p>I am using Python with TextBlob for sentiment analysis. I want to deploy my app (build in Plotly Dash) to Google Cloud Run with Google Cloud Build (without using Docker). When using locally on my virtual environment all goes fine, but after deploying it on the cloud the corpora is not downloaded. Looking at the requriements.txt file, there was also no reference to this corpora.</p>
<p>I have tried to add <code>python -m textblob.download_corpora</code> to my requriements.txt file but it doesn't download when I deploy it. I have also tried to add</p>
<pre><code>import textblob
import subprocess
cmd = ['python','-m','textblob.download_corpora']
subprocess.run(cmd)
</code></pre>
<p>and</p>
<pre><code>import nltk
nltk.download('movie_reviews')
</code></pre>
<p>to my script (callbacks.py, I am using Plotly Dash to make my app), all without success.</p>
<p>Is there a way to add this corpus to my requirements.txt file? Or is there another workaround to download this corpus? How can I fix this?</p>
<p>Thanks in advance!</p>
<p>Vijay</p>
",Text Classification / Sentiment Analysis,way download textblob corpus google cloud run using python textblob sentiment analysis want deploy app build plotly dash google cloud run google cloud build without using docker using locally virtual environment go fine deploying cloud corpus downloaded looking requriements txt file wa also reference corpus tried add requriements txt file download deploy also tried add script callback py using plotly dash make app without success way add corpus requirement txt file another workaround download corpus fix thanks advance vijay
it is normal that CNN give me better accuracy compared to LSTM in text classification?,"<blockquote>
<p>For a text classification, I have data of 1000 reviews and I tried different neural networks. For the CNN I got an accuracy of 0.94 but with the LSTM I got a lower accuracy (0.88) is this normal because as far as I know the LSTM is specialized for text classification and it preserves the order of the word sequence?</p>
</blockquote>
",Text Classification / Sentiment Analysis,normal cnn give better accuracy compared lstm text classification text classification data review tried different neural network cnn got accuracy lstm got lower accuracy normal far know lstm specialized text classification preserve order word sequence
How to fine tuning again of a bert fined tuned model,"<p>I did a fine tuning bert model for text classification using ktrain.
Again i want to do fine tuning this model on another text classification data. How i can do?</p>
",Text Classification / Sentiment Analysis,fine tuning bert fined tuned model fine tuning bert model text classification using ktrain want fine tuning model another text classification data
Unable predict the text for sentiment analysis using pickle file for multiple comments,"<p><strong>Unable predict the text for sentiment analysis using pickle file for multiple comments</strong></p>
<p>For example i have code below which returns sentiment for only one comment</p>
<pre><code>vectorizer = pickle.load(open(vectorizer_path,'rb'))
model = pickle.load(open(model_path,'rb'))

pred = model.predict(vectorizer.transform([&quot;My name is rohit&quot;]))[0]
print (&quot;predicted class is : &quot;, pred)
</code></pre>
<p>And the output is :</p>
<pre><code>predicted class is :  Positive
</code></pre>
<p>If i have a dataframe with one column which consists multiple comments</p>
<pre><code>    Comments
0   My name is rohit
1   You're too bad
2   I like your voice
3   I'm very sick
4   I'm suffering with cold
</code></pre>
<p>I've iterated the above rows present in comments column and appended it to empty list</p>
<pre><code>word = []

for index, row in data.iterrows():
    word.append(str(row[0]))
</code></pre>
<p>How to predict all the sentiments for the above rows once at a time? What changes should i make in the below code</p>
<pre><code>pred = model.predict(vectorizer.transform([&quot;My name is rohit&quot;]))[0]
print (&quot;predicted class is : &quot;, pred)
</code></pre>
",Text Classification / Sentiment Analysis,unable predict text sentiment analysis using pickle file multiple comment unable predict text sentiment analysis using pickle file multiple comment example code return sentiment one comment output dataframe one column consists multiple comment iterated row present comment column appended empty list predict sentiment row time change make code
why input_mask is all the same number in BERT language model?,"<p>For a text classification task I applied Bert(fine tune) and the output that I got is as below:
Why input_mask is all 1 ?</p>
<pre><code>#to_feature_map is a function.
to_feature_map(&quot;hi how are you doing&quot;,0)
</code></pre>
<pre><code>({'input_mask': &lt;tf.Tensor: shape=(64,), dtype=int32, numpy=
  array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        dtype=int32)&gt;,
  'input_type_ids': &lt;tf.Tensor: shape=(64,), dtype=int32, numpy=
  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        dtype=int32)&gt;,
  'input_word_ids': &lt;tf.Tensor: shape=(64,), dtype=int32, numpy=
  array([ 101, 7632, 2129, 2024, 2017, 2725,  102,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)&gt;},
 &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;)```
</code></pre>
",Text Classification / Sentiment Analysis,input mask number bert language model text classification task applied bert fine tune output got input mask
Fast.ai &amp; NLP: Remove unknown words in fast.ai tokenizer,"<p>I'm building an NLP model to classify real estate text. The issue is that, when creating a data bunch, many of the words are given an <code>xxunk</code> special token.</p>
<p>The code I used to generate a TextClasDataBunch object:</p>
<pre><code>count = 0
error = True
while error:
    try: 
        data_clas = TextClasDataBunch.from_csv(path, 'text.csv', vocab=data_lm.train_ds.vocab, bs=2)
        error = False
        print(f'failure count is {count}\n')    
    except:
        count = count + 1
        print(f'failure count is {count}')
</code></pre>
<p>The code above classifies <code>parcel control number, property account number, parcel id, Property SBL, and PARID</code> as <code>xxbos parcel xxunk number,xxbos parcel number,xxbos xxunk xxunk,xxbos xxunk acres,xxbos xxmaj xxunk</code>. The preponderance of <code>xxunk</code> is likely decreasing the effectiveness of my model.</p>
<p>Is there a way to include rare words in the fast.ai tokenizer? I've looked at <a href=""https://stackoverflow.com/questions/66993922/how-to-include-rare-words-in-fast-ai-tokenizer-for-nlp"">this</a> question, but there's currently no answer.</p>
",Text Classification / Sentiment Analysis,fast ai nlp remove unknown word fast ai tokenizer building nlp model classify real estate text issue creating data bunch many word given special token code used generate textclasdatabunch object code classifies preponderance likely decreasing effectiveness model way include rare word fast ai tokenizer looked href question currently answer p
How to Increase model accuracy when using classification algorithm for text classification,"<p>I am using SVM classification algorithm to classify the articles but I am confused, why the accuracy of model is getting decreased as the number of samples increases. below are the accuracy stats that I captured with different C values that's hyper parameter.</p>
<p>can anyone help me to increase further accuracy with increased number of samples?</p>
<p>Task that is being performed:</p>
<blockquote>
<p>classify the news articles on various categories like sports,politics,entertainment etc.</p>
</blockquote>
<p>Sample code &gt;&gt;</p>
<pre><code># dummy data (in real problem this is a big list)
glbl_list = [['samples_id','sample_prediction','sample_article'],['samples_id','sample_prediction','sample_article']]

# shuffling the whole data
order = [j for j in range(len(glbl_list))]
random.shuffle(glbl_list)
glbl_list = [glbl_list[i] for i in order]

# creating a dataframe out of above list
Corpus = pd.DataFrame(glbl_list, columns = ['id', 'label', 'text'])

# perform various data cleaning task like stopwords,punc. marks removal etc

tfidf = TfidfVectorizer()
x = tfidf.fit_transform(Corpus['text'].values)

encoder = LabelEncoder() 
y = encoder.fit_transform(Corpus['label'])

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)

clf = svm.SVC(C=1, kernel='linear')
clf.fit(x_train, y_train)

print(&quot;Accuracy: {}&quot;.format(clf.score(x_train, y_train)))
print(&quot;Accuracy: {}&quot;.format(clf.score(x_test, y_test)))
</code></pre>
<blockquote>
<p>accuracy captured(at Best possible C values):<br />
samples:(500)<br />
when C = 1<br />
Train_Accuracy: 0.9933333333333333<br />
Test_Accuracy: 0.98</p>
<p>samples:(1000)<br />
when C = 2.5<br />
Train_Accuracy: 0.9833333333333333<br />
Test_Accuracy: 0.9</p>
<p>samples:(5000)<br />
when C = 1<br />
Train_Accuracy: 0.8533333333333334<br />
Test_Accuracy: 0.81</p>
<p>samples:(8000)<br />
when C = 1<br />
Train_Accuracy: 0.8983333333333333<br />
Test_Accuracy: 0.8475</p>
<p>samples:(10000)<br />
when C = 1<br />
Train_Accuracy: 0.7488888888888889<br />
Test_Accuracy: 0.69</p>
<p>samples:(12000)<br />
when C = 0.6<br />
Train_Accuracy: 0.6548148148148148<br />
Test_Accuracy: 0.6333333333333333</p>
</blockquote>
",Text Classification / Sentiment Analysis,increase model accuracy using classification algorithm text classification using svm classification algorithm classify article confused accuracy model getting decreased number sample increase accuracy stats captured different c value hyper parameter anyone help increase accuracy increased number sample task performed classify news article various category like sport politics entertainment etc sample code accuracy captured best possible c value sample c train accuracy test accuracy sample c train accuracy test accuracy sample c train accuracy test accuracy sample c train accuracy test accuracy sample c train accuracy test accuracy sample c train accuracy test accuracy
How to handle repeating text data but with Different Labels or Classes?,"<p>I am doing a Multi-class Text Classification. However, I have data that are repeating in the dataset. However, these are not duplicates, as they belong to different classes. The data is valid, these two classes are close to each other, The repeated text training data is not of the same class, but of diff classes with the same shared URLs. What can I do, so that my Text classifier effectively working at predicting the future inputs with higher probability without sharing probability with the other counterpart? Also are there any other techniques
TO NOTE: Only 10 % of training data is repeated with diff classes.</p>
",Text Classification / Sentiment Analysis,handle repeating text data different label class multi class text classification however data repeating dataset however duplicate belong different class data valid two class close repeated text training data class diff class shared url text classifier effectively working predicting future input higher probability without sharing probability counterpart also technique note training data repeated diff class
How to get news feed out of Bloomberg API regarding a particular security(equity) and date range?,"<p>I'm working on a project that requires I source news articles from the Bloomberg API regarding a particular security ( say Netflix) within a specific date range. I want to do this in Python and get the news articles in a structure (JSON/XML) format. I believe this can be done using EDTF(Event-Driven Trading Feed) using the Bloomberg Terminal, but I want to do this using the Bloomberg API.</p>

<p>I need these news articles to perform a sentiment-analysis on the articles.</p>

<p>I read the answer to this question: <a href=""https://stackoverflow.com/questions/49547440/scrape-news-feed-from-bloomberg-terminal"">Scrape News feed from Bloomberg Terminal</a> </p>

<p>I understand I do have access to EDTF feed but don't know how to get the feed out programmatically in Python as there is really little to no documentation around it. If I could use PDBLP (<a href=""https://matthewgilbert.github.io/pdblp/api.html"" rel=""noreferrer"">https://matthewgilbert.github.io/pdblp/api.html</a>), it would be even greater!</p>

<p>Please link some documentation, code examples as to how to go about this problem. If you've worked on a similar project on Bloomberg, it would be great if you could share some code examples. Thank you!</p>
",Text Classification / Sentiment Analysis,get news feed bloomberg api regarding particular security equity date range working project requires source news article bloomberg api regarding particular security say netflix within specific date range want python get news article structure json xml format believe done using edtf event driven trading feed using bloomberg terminal want using bloomberg api need news article perform sentiment analysis article read answer question would even greater please link documentation code example go problem worked similar project bloomberg would great could share code example thank
Saliency score keeps changing allennlp,"<p>I am trying to get the saliency score for sentiment analysis task. Every time I run the code I get different saliency scores. Should this be the case? I am attaching my code for more reference.</p>
<pre><code>from allennlp.predictors.predictor import Predictor
import nltk
from allennlp.interpret.saliency_interpreters import SmoothGradient


data = &quot;purchase costume year old grandson halloween arrive one week earlier expect happy grandson absolutely love glad order larger size size barely fit material durable well make think wear many time play since halloween happy purchase worth dollars spend&quot;

words = nltk.word_tokenize(data)

predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/stanford-sentiment-treebank-roberta.2021-03-11.tar.gz&quot;)
predicted = predictor.predict(words)
saliency_scores = SmoothGradient(predictor).saliency_interpret_from_json({'sentence':words})
</code></pre>
<p>Every time I print saliency scores for the same data the values keep changing. Also the tokens that the model generates are distorted, for example halloween breaks into hall, ow and een. How can I fix this? Any help would be appreciated.</p>
",Text Classification / Sentiment Analysis,saliency score keep changing allennlp trying get saliency score sentiment analysis task every time run code get different saliency score case attaching code reference every time print saliency score data value keep changing also token model generates distorted example halloween break hall ow een fix help would appreciated
"SMOTE, Oversampling on text classification in Python","<p>I am doing a text classification and I have very imbalanced data like </p>

<pre><code>Category | Total Records
Cate1    | 950
Cate2    |  40
Cate3    |  10
</code></pre>

<p>Now I want to over sample Cate2 and Cate3 so it at least have 400-500 records, I prefer to use SMOTE over random sampling, Code </p>

<pre><code>from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
X_train, X_test, y_train, y_test = train_test_split(fewRecords['text'],
                                   fewRecords['category'])

sm = SMOTE(random_state=12, ratio = 1.0)
x_train_res, y_train_res = sm.fit_sample(X_train, y_train)
</code></pre>

<p>It does not work as it can't generate the sample synthetic text, Now when I covert it into vector like </p>

<pre><code>count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')
count_vect.fit(fewRecords['category'])

# transform the training and validation data using count vectorizer object
xtrain_count =  count_vect.transform(X_train)
ytrain_train =  count_vect.transform(y_train)
</code></pre>

<p>I am not sure if it is right approach and how to convert vector to real text when  I want to predict real category after classification </p>
",Text Classification / Sentiment Analysis,smote oversampling text classification python text classification imbalanced data like want sample cate cate least record prefer use smote random sampling code doe work generate sample synthetic text covert vector like sure right approach convert vector real text want predict real category classification
Adding &#39;-&#39; sign to negative flair sentiment analysis,"<p>I am creating a sentiment analysis code for stock market analysis. This is the heart of the code:</p>
<pre><code>import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
columns = ['ticker', 'date', 'time', 'headline']
parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)
sentiment = []
for head in parsed_and_scored_news['headline']:
    s = flair.data.Sentence(head)
    flair_sentiment.predict(s)
    total_sentiment = s.labels
    sentiment.append(total_sentiment)
    scores_df = pd.DataFrame(sentiment)
    parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')
    
# Convert the date column from string to datetime
parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.dateparsed_and_scored_news.head()
</code></pre>
<p>The following output is produced:</p>
<pre><code>    ticker     date      time              headline                                    0
0   AMZN    2021-03-26  02:37PM Tech stocks are going to do vey well going for...   POSITIVE (0.9986)
1   AMZN    2021-03-26  01:17PM Amazon mocked idea its drivers urinated in bot...   NEGATIVE (0.9855)
2   AMZN    2021-03-26  01:11PM ThredUp CEO on IPO day: Dont tax resale and Am...   NEGATIVE (0.6743)
3   AMZN    2021-03-26  12:54PM Why this retailer is seeing a triple-digit sal...   POSITIVE (0.9597)
4   AMZN    2021-03-26  12:07PM How to secure your smart home camera                POSITIVE (0.9981)
        
</code></pre>
<p>Since I want to feed the data into an ML model I need the score to be numeric. I know that using <code>probability = sentence.labels[0].score</code> gives us only the scores, but that means there is no way to classify whether a statement is positive is negative. Is there a way to add a '-'(negation) sign behind the scores classified as negative. For e.g - <code>NEGATIVE (0.9855) = -9855</code>. This will ensure that the information is numeric as well as useful.</p>
",Text Classification / Sentiment Analysis,adding sign negative flair sentiment analysis creating sentiment analysis code stock market analysis heart code following output produced since want feed data ml model need score numeric know using give u score mean way classify whether statement positive negative way add negation sign behind score classified negative e g ensure information numeric well useful
Supervised Latent Dirichlet Allocation for Document Classification?,"<p>I have a bunch of already human-classified documents in some groups. </p>

<p>Is there a modified version of lda which I can use to train a model and then later classify unknown documents with it?</p>
",Text Classification / Sentiment Analysis,supervised latent dirichlet allocation document classification bunch already human classified document group modified version lda use train model later classify unknown document
How to get word embeddings back from Keras?,"<p>Say you create your own custom word embeddings in the process of some arbitrary task, say text classification. How do you get a dictionary like structure of <code>{word: vector}</code> back from Keras?</p>
<p><code>embeddings_layer.get_weights()</code> gives you the raw embeddings...but it's unclear which word corresponds to what vector element.</p>
",Text Classification / Sentiment Analysis,get word embeddings back kera say create custom word embeddings process arbitrary task say text classification get dictionary like structure back kera give raw embeddings unclear word corresponds vector element
How can I make sentiment analysis with new sentence on trained model?,"<p>I trained a model by using Naive Bayes. I have high accuracy, but now I want to give a sentence then I want to see it's sentiment. Here it is my code:</p>
<pre><code># data Analysis
import pandas as pd

# data Preprocessing and Feature Engineering
from textblob import TextBlob
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Model Selection and Validation
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import joblib

import warnings
import mlflow

warnings.filterwarnings(&quot;ignore&quot;)

train_tweets = pd.read_csv('data/train.csv')

tweets = train_tweets.tweet.values
labels = train_tweets.label.values

processed_features = []

for sentence in range(0, len(tweets)):
    # Remove all the special characters
    processed_feature = re.sub(r'\W', ' ', str(tweets[sentence]))

    # remove all single characters
    processed_feature= re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature)

    # Remove single characters from the start
    processed_feature = re.sub(r'\^[a-zA-Z]\s+', ' ', processed_feature)

    # Substituting multiple spaces with single space
    processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I)

    # Removing prefixed 'b'
    processed_feature = re.sub(r'^b\s+', '', processed_feature)

    # Converting to Lowercase
    processed_feature = processed_feature.lower()

    processed_features.append(processed_feature)


vectorizer = TfidfVectorizer(max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))
processed_features = vectorizer.fit_transform(processed_features).toarray()

X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)

text_classifier = MultinomialNB()
text_classifier.fit(X_train, y_train)

predictions = text_classifier.predict(X_test)

print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))
print(accuracy_score(y_test, predictions))


joblib.dump(text_classifier, 'model.pkl')
</code></pre>
<p>As you can see, I'm saving my model. Now, I want give an input like this:</p>
<pre><code>new_sentence = &quot;I am very happy today&quot;
model.predict(new_sentence)
</code></pre>
<p>And I want see something like this as an output:</p>
<pre><code>sentence = &quot;I am very happy today&quot;
sentiment = Positive
</code></pre>
<p>How can I do that?</p>
",Text Classification / Sentiment Analysis,make sentiment analysis new sentence trained model trained model using naive bayes high accuracy want give sentence want see sentiment code see saving model want give input like want see something like output
Document classification using pretrained models like BERT,"<p>I am looking for methods to classify documents. For ex. I have a bunch of documents with text and I want to label the document on whether it belongs to sports, food, politics etc.
Can I use BERT (for documents with words &gt; 500) for this or are there any other models that do this task efficiently?</p>
",Text Classification / Sentiment Analysis,document classification using pretrained model like bert looking method classify document ex bunch document text want label document whether belongs sport food politics etc use bert document word model task efficiently
Extract probabilities and labels from FARM TextClassification,"<p>I have spent a few days exploring the excellent FARM library and its modular approach to building models. The default output (result) however is very verbose, including a multiplicity of texts, values and ASCII artwork. For my research I only require the predicted labels from my NLP text classification model, together with the individual probabilities. How do I do that? I have been experimenting with nested lists/dictionaries but am unable to neatly produce a simple list of output labels and probabilities.</p>
<pre><code>enter code here


# Test your model on a sample (Inference)
from farm.infer import Inferencer
from pprint import PrettyPrinter

infer_model = Inferencer(processor=processor, model=model, task_type=&quot;text_classification&quot;, gpu=True)

basic_texts = [
               # a snippet or two from Dickens
    {&quot;text&quot;: &quot;Mr Dombey had remained in his own apartment since the death of his wife, absorbed in visions of the youth, education, and destination of his baby son. Something lay at the bottom of his cool heart, colder and heavier than its ordinary load; but it was more a sense of the child’s loss than his own, awakening within him an almost angry sorrow.&quot;},
    {&quot;text&quot;: &quot;Soon after seven o'clock we went down to dinner, carefully, by Mrs. Jellyby's advice, for the stair-carpets, besides being very deficient in stair-wires, were so torn as to be absolute traps.&quot;},
    {&quot;text&quot;: &quot;Walter passed out at the door, and was about to close it after him, when, hearing the voices of the brothers again, and also the mention of his own name, he stood irresolutely, with his hand upon the lock, and the door ajar, uncertain whether to return or go away.&quot;},
               # from Lewis Carroll
    {&quot;text&quot;: &quot;I have kept one for many years, and have found it of the greatest possible service, in many ways: it secures my _answering_ Letters, however long they have to wait; it enables me to refer, for my own guidance, to the details of previous correspondence, though the actual Letters may have been destroyed long ago;&quot;},
    {&quot;text&quot;: &quot;The Queen gasped, and sat down: the rapid journey through the air had quite taken away her breath and for a minute or two she could do nothing but hug the little Lily in silence.&quot;},
    {&quot;text&quot;: &quot;Rub as she could, she could make nothing more of it: she was in a little dark shop, leaning with her elbows on the counter, and opposite to her was an old Sheep, sitting in an arm-chair knitting, and every now and then leaving off to look at her through a great pair of spectacles.&quot;}, 
               # G K Chesterton
    {&quot;text&quot;: &quot;Basil and I walked rapidly to the window which looked out on the garden. It was a small and somewhat smug suburban garden; the flower beds a little too neat and like the pattern of a coloured carpet; but on this shining and opulent summer day even they had the exuberance of something natural, I had almost said tropical. &quot;},
    {&quot;text&quot;: &quot;This is the whole danger of our time. There is a difference between the oppression which has been too common in the past and the oppression which seems only too probable in the future.&quot;},
    {&quot;text&quot;: &quot;But whatever else the worst doctrine of depravity may have been, it was a product of spiritual conviction; it had nothing to do with remote physical origins. Men thought mankind wicked because they felt wicked themselves. &quot;},
]
result = infer_model.inference_from_dicts(dicts=basic_texts)
PrettyPrinter().pprint(result)
#print(result)
</code></pre>
",Text Classification / Sentiment Analysis,extract probability label farm textclassification spent day exploring excellent farm library modular approach building model default output result however verbose including multiplicity text value ascii artwork research require predicted label nlp text classification model together individual probability experimenting nested list dictionary unable neatly produce simple list output label probability
Feature Selection in TfIdfVectorizer,"<p>I want to classify two groups of documents using <code>TfIdfVectorizer</code>. But <code>TfIdfVectorizer</code> lists words based on frequency in both documents. For instance, in the example below, the words Tom and Jerry are the defining words, while the <code>max_features</code> parameter retrieves the frequent ones ('hi', 'is', 'my'). Obviously, the document differences are important for classification, not similarities. So, how can I extract the determinant words in each document? also, stopword removal does not really help in this scenario.</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd


corpus = [
    'hi, my name is Tom.',
    'hi, my name is Jerry.'
]

vectorizer = TfidfVectorizer(max_features=3, ngram_range=(1, 1))
X = vectorizer.fit_transform(corpus).todense()


df = pd.DataFrame(X, columns=vectorizer.get_feature_names())
df.to_csv('test.csv')
</code></pre>
<p>The output:</p>
<pre><code>,hi,is,my
0,0.5773502691896258,0.5773502691896258,0.5773502691896258
1,0.5773502691896258,0.5773502691896258,0.5773502691896258
</code></pre>
<p>The expected output:</p>
<pre><code>,jerry,tom
0,0.0,0.5749618667993135
1,0.5749618667993135,0.0
</code></pre>
",Text Classification / Sentiment Analysis,feature selection tfidfvectorizer want classify two group document using list word based frequency document instance example word tom jerry defining word parameter retrieves frequent one hi obviously document difference important classification similarity extract determinant word document also stopword removal doe really help scenario output expected output
how can I simplify BoWs?,"<p>I'm trying to apply some binary text classification but I don't feel that having millions of &gt;1k length vectors is a good idea. So, which alternatives are there for the basic BOW model?</p>
",Text Classification / Sentiment Analysis,simplify bow trying apply binary text classification feel million k length vector good idea alternative basic bow model
detecting emotions in sentiment analysis python,"<p>I was curious if anyone had any thoughts about how one might detect major emotions displayed in a text? Are there any python packages or examples that do this?</p>

<p>TO clarify:</p>

<p>I know that there is already something called sentiment analysis. However, I'm noticing this only looks at positive/negative sentiment.
I'm wondering if it's actually possible to find emotions (like sadness, joy, despair, etc) linked to certain texts. </p>
",Text Classification / Sentiment Analysis,detecting emotion sentiment analysis python wa curious anyone thought one might detect major emotion displayed text python package example clarify know already something called sentiment analysis however noticing look positive negative sentiment wondering actually possible find emotion like sadness joy despair etc linked certain text
FastAPI return BERT model result and metrics,"<p>I have sentiment analysis model using BERT and I want to get the result from predicting text via FastAPI but it always give negative answer (I think it is because the prediction didn't give prediction result).</p>
<p>This is my code:</p>
<pre><code>import uvicorn
from fastapi import FastAPI
import joblib

# models
sentiment_model = open(&quot;sentiment-analysis-model.pkl&quot;, &quot;rb&quot;)
sentiment_clf = joblib.load(sentiment_model)

# init app
app = FastAPI()

# Routes
@app.get('/')
async def index():
    return {&quot;text&quot;: &quot;Hello World! huehue&quot;}

@app.get('/predict/{text}')
async def predict(text):
    prediction, raw_outputs = sentiment_clf.predict(text)
    if prediction == 0:
        result = &quot;neutral&quot;
    elif prediction == 1:
        result = &quot;positive&quot;
    else:
        result = &quot;negative&quot;
    return{&quot;text&quot;: text, &quot;prediction&quot;:result}

if __name__ == '__main__':
    uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=8000)
</code></pre>
<p>Also I want to print accuracy, F1 Score etc.</p>
<p>I'm using this model</p>
<pre><code>from simpletransformers.classification import ClassificationModel

model = ClassificationModel('bert', 'bert-base-multilingual-uncased', num_labels=3, use_cuda=False, 
                            args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 1},
                            weight=[3, 0.5, 1])
</code></pre>
",Text Classification / Sentiment Analysis,fastapi return bert model result metric sentiment analysis model using bert want get result predicting text via fastapi always give negative answer think prediction give prediction result code also want print accuracy f score etc using model
twitter_samples in both corpus and download,"<p>I am trying to implement a basic twitter sentiment analysis project. For this, I import from a corpus twitter_samples as following</p>
<pre><code>from nltk.corpus import twitter_samples
</code></pre>
<p>but it also needs to use the following command;</p>
<pre><code>nltk.download('twitter_samples')
</code></pre>
<p>to work properly.</p>
<p>My intuition about this is that, the <code>twitter_samples</code> from <code>from nltk.corpus import twitter_samples</code> is a function and the <code>twitter_samples</code> in <code>nltk.download('twitter_samples')</code> is the dataset. Am I correct or is it something else? Please explain. Also is there any specific reason as to why the name <code>twitter_samples</code> is same in both?</p>
",Text Classification / Sentiment Analysis,twitter sample corpus download trying implement basic twitter sentiment analysis project import corpus twitter sample following also need use following command work properly intuition function dataset correct something else please explain also specific reason name
"Is is possible to do sentiment analysis other than just positive, negative and neutral in Python or other programing language","<p>I have searched the internet and there is more or less the same sentiment analysis of a sentence i.e Positive, Negative or Neutral. I want to build a sentiment analyzer that look for the following sentiments/emotions for a sentence. </p>

<pre><code>happy , sad , angry , disaapointed , surprised, proud, in love, scared
</code></pre>
",Text Classification / Sentiment Analysis,possible sentiment analysis positive negative neutral python programing language searched internet le sentiment analysis sentence e positive negative neutral want build sentiment analyzer look following sentiment emotion sentence
multiclass text classification,"<p>Why my <code>lstm</code> model is getting better accuracy than my bi <code>lstm</code> model? (multi-class text classification with 5 classes using word2vec and <code>lstm</code>) I tried to find the answer in any paper but I can't find it, almost all the paper said <code>bilstm</code> can improve the accuracy, can someone explain and give the references? Thanks</p>
<p>This is for 5 classes using <code>lstm</code></p>
<pre><code>Epoch 45/50
205/205 [==============================] - 284s 1s/step - loss: 0.6703 - accuracy: 0.7712 - val_loss: 0.9680 - val_accuracy: 0.6946
Epoch 46/50
205/205 [==============================] - 286s 1s/step - loss: 0.6571 - accuracy: 0.7709 - val_loss: 0.9682 - val_accuracy: 0.6937
Epoch 47/50
205/205 [==============================] - 282s 1s/step - loss: 0.6682 - accuracy: 0.7687 - val_loss: 0.9707 - val_accuracy: 0.6995
Epoch 48/50
205/205 [==============================] - 292s 1s/step - loss: 0.6658 - accuracy: 0.7681 - val_loss: 0.9847 - val_accuracy: 0.6961
Epoch 49/50
205/205 [==============================] - 288s 1s/step - loss: 0.6650 - accuracy: 0.7658 - val_loss: 0.9901 - val_accuracy: 0.6961
Epoch 50/50
205/205 [==============================] - 279s 1s/step - loss: 0.6629 - accuracy: 0.7711 - val_loss: 0.9821 - val_accuracy: 0.6921

this is for 5 classes using bilstm
Epoch 45/50
205/205 [==============================] - 313s 2s/step - loss: 0.6071 - accuracy: 0.7859 - val_loss: 0.9831 - val_accuracy: 0.7025
Epoch 46/50
205/205 [==============================] - 310s 2s/step - loss: 0.5971 - accuracy: 0.8002 - val_loss: 0.9834 - val_accuracy: 0.6888
Epoch 47/50
205/205 [==============================] - 316s 2s/step - loss: 0.5976 - accuracy: 0.7966 - val_loss: 1.0056 - val_accuracy: 0.6989
Epoch 48/50
205/205 [==============================] - 346s 2s/step - loss: 0.5858 - accuracy: 0.8020 - val_loss: 0.9978 - val_accuracy: 0.6964
Epoch 49/50
205/205 [==============================] - 322s 2s/step - loss: 0.5941 - accuracy: 0.7977 - val_loss: 1.0099 - val_accuracy: 0.6912
Epoch 50/50
205/205 [==============================] - 327s 2s/step - loss: 0.5886 - accuracy: 0.7987 - val_loss: 1.0049 - val_accuracy: 0.6986
</code></pre>
",Text Classification / Sentiment Analysis,multiclass text classification model getting better accuracy bi model multi class text classification class using word vec tried find answer paper find almost paper said improve accuracy someone explain give reference thanks class using
Using BERT for next sentence prediction,"<p>Google's <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT</a> is pretrained on next sentence prediction tasks, but I'm wondering if it's possible to call the next sentence prediction function on new data. </p>

<p>The idea is: given sentence A and given sentence B, I want a probabilistic label for whether or not sentence B follows sentence A. BERT is pretrained on a huge set of data, so I was hoping to use this next sentence prediction on new sentence data. I can't seem to figure out if this next sentence prediction function can be called and if so, how. Thanks for your help!</p>
",Text Classification / Sentiment Analysis,using bert next sentence prediction google bert pretrained next sentence prediction task wondering possible call next sentence prediction function new data idea given sentence given sentence b want probabilistic label whether sentence b follows sentence bert pretrained huge set data wa hoping use next sentence prediction new sentence data seem figure next sentence prediction function called thanks help
How to clean the 20newsgroup dataset for nlp tasks,"<p>I am trying to practice a classification task on NLP. I am using <a href=""https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"" rel=""nofollow noreferrer"">20newsgroup</a> dataset and I want to implement a classification model. Before training model, I want to implement:</p>
<ol>
<li>stopword removal</li>
<li>punctuation removal</li>
<li>converting to lower case - since it's not Sentiment analysis task, so case distinction doesn't matter here according to me.</li>
</ol>
<p>I am using the following code:</p>
<pre><code>max_len = 0
for sent in x_train:

    tokenizer_out = tokenizer(sent)
    # convert numerical tokens to alphabetical tokens
    encoded_tok = tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids)
    tokens_without_sw = [word for word in encoded_tok if not word in stopwords.words()]
    new_ids = tokenizer.convert_tokens_to_ids(tokens_without_sw)
    max_len = max(max_len, len(new_ids))
</code></pre>
<p>I will be using pretrained BERT from hugging face. And before implementing the code above, I had done the following to remove unnecessary lines:</p>
<pre><code>def clean(post: str, remove_it: tuple):
  new_lines = []
  for line in post.splitlines():
        if not line.startswith(remove_it):
            new_lines.append(line)
  return '\n'.join(new_lines)

remove_it = (
      'From:',
      'Subject:',
      'Reply-To:',
      'In-Reply-To:',
      'Nntp-Posting-Host:',
      'Organization:',
      'X-Mailer:',
      'In article &lt;',
      'Lines:',
      'NNTP-Posting-Host:',
      'Summary:',
      'Article-I.D.:'
  )
x_train = [clean(p, remove_it) for p in x_train]
x_test = [clean(p, remove_it) for p in x_test]
</code></pre>
<p>My next goal is to clean it further. With my classification, I am able to achieve  90% accuracy but I want to increase it further. SO, I want to remove the stopwords and punctuations, convert to lower case and see what happens. But with the code I use, its taking like forever to run, so I want a faster approach.</p>
<p>Can anyone help me?</p>
",Text Classification / Sentiment Analysis,clean newsgroup dataset nlp task trying practice classification task nlp using newsgroup dataset want implement classification model training model want implement stopword removal punctuation removal converting lower case since sentiment analysis task case distinction matter according using following code using pretrained bert hugging face implementing code done following remove unnecessary line next goal clean classification able achieve accuracy want increase want remove stopwords punctuation convert lower case see happens code use taking like forever run want faster approach anyone help
Understanding ConvNet Prediction on Text Classification,"<p>I'm trying to debug a model that uses 1D convolutions to classify text that was labeled by humans as being &quot;appropriate&quot; vs &quot;not appropriate&quot; to be posted on some website. Looking at false positives (wrongly predicted &quot;appropriate&quot;), I see that the text has mostly neutral/positive sounding words, but the <em>idea</em> conveyed is bad (ex: talking about &quot;capping population&quot;). To address a case like this, I can think of ways to help the model learn that the subject of capping population (in this example) should not be classified as &quot;appropriate&quot; for this particular task.</p>
<p>The problem I'm having is understanding what caused the model to predict &quot;not appropriate&quot; for messages that are in fact appropriate. For example, the following message should be considered &quot;appropriate&quot;:</p>
<blockquote>
<p>&quot;The blame lies with the individual who commits the crime.&quot;</p>
</blockquote>
<p>The model thinks that's not appropriate, but according to the labeling criteria of the dataset, that's a valid message.</p>
<h1>Question</h1>
<p>Given a model with an embedding layer for each word, followed by several 1D convs + dense layer, what are some techniques that can help me what is causing the model to classify that message as such, and potential ways to help the model learn that's ok?</p>
<h2>Update</h2>
<p>Turns out if I take the example phrase above and replace one word at a time, then see how the model classifies the resulting phrase, it classifies the phrase as being &quot;appropriate&quot; when I replace the word &quot;lies&quot; with just about any other &quot;positive&quot; or &quot;neutral&quot; word. So it seems like the model learned that &quot;lies&quot; is a really, really bad word. Question is: how do I create a feature(s) or otherwise help the model generalize beyond that?</p>
",Text Classification / Sentiment Analysis,understanding convnet prediction text classification trying debug model us convolution classify text wa labeled human appropriate v appropriate posted website looking false positive wrongly predicted appropriate see text ha mostly neutral positive sounding word idea conveyed bad ex talking capping population address case like think way help model learn subject capping population example classified appropriate particular task problem understanding caused model predict appropriate message fact appropriate example following message considered appropriate blame lie individual commits crime model think appropriate according labeling criterion dataset valid message question given model embedding layer word followed several convs dense layer technique help causing model classify message potential way help model learn ok update turn take example phrase replace one word time see model classifies resulting phrase classifies phrase appropriate replace word lie positive neutral word seems like model learned lie really really bad word question create feature otherwise help model generalize beyond
Cleaned text has significantly worse classification accuracy?,"<p>I'm trying to classify whether or not I liked books that I've read this year based on the text in the books. I'm using the preprocessing described <a href=""https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"" rel=""nofollow noreferrer"">here</a>, and a variety of sklearn classification models.</p>
<p>At first I was just feeding the models the raw text, but I cleaned it based on GloVe embeddings (a process described <a href=""https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings"" rel=""nofollow noreferrer"">here</a>). The text was improved from 40% vocab, 80% coverage to 80% vocab, 98% coverage based on GloVe embeddings. However, for some reason, after cleaning the text, the accuracy of the classification models seemed to be the same or lower.</p>
<p>Uncleaned text model results:
<a href=""https://i.sstatic.net/Hc0FH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Hc0FH.png"" alt=""Uncleaned text model results"" /></a></p>
<p>Cleaned text model results:
<a href=""https://i.sstatic.net/3O611.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3O611.png"" alt=""Cleaned text model results"" /></a></p>
<p>One thing to note is that the classes are quite imbalanced (75% of books were good as compared to 25% bad), so accuracy above 75% should be expected, since 75% is what the model would get if it guessed good every single time.</p>
<p>I've linked my full notebook <a href=""https://deepnote.com/project/c3a30d34-9496-46b5-acca-29c0a25aa9a0"" rel=""nofollow noreferrer"">here</a> so you can check out the specific code if that will be helpful for solving this issue. I'm incredibly confused; I can't see where I'm going wrong, but it can't be right that cleaning the text data has zero or negative impact on model accuracy.</p>
",Text Classification / Sentiment Analysis,cleaned text ha significantly worse classification accuracy trying classify whether liked book read year based text book using preprocessing described variety sklearn classification model first wa feeding model raw text cleaned based glove embeddings process described text wa improved vocab coverage vocab coverage based glove embeddings however reason cleaning text accuracy classification model seemed lower uncleaned text model result cleaned text model result one thing note class quite imbalanced book good compared bad accuracy expected since model would get guessed good every single time linked full notebook check specific code helpful solving issue incredibly confused see going wrong right cleaning text data ha zero negative impact model accuracy
How to predict if a phrase is related to a short text or an article using supervised learning?,"<p>I have set of short phrases and a set of texts. I want to predict if a phrase is related to an article. A phrase that isn't appearing in the article may still be related.</p>
<p>Some examples of annotated data (not real) is like this:</p>
<p>Example 1</p>
<blockquote>
<p><strong>Phrase:</strong>  Automobile</p>
<p><strong>Text:</strong> Among the more affordable options in the electric-vehicle marketplace, the 2021 Tesla Model 3 is without doubt the one with the
most name recognition. It borrows some styling cues from the company's
Model S sedan and Model X SUV, but goes its own way with a unique
interior design and an all-glass roof. Acceleration is quick, and the
Model 3's chassis is playful as well—especially the Performance
model's, which receives a sportier suspension and a track driving
mode. But EV buyers are more likely interested in driving range than
speediness or handling, and the Model 3 delivers there too. The base
model offers up to 263 miles of driving range according to the EPA,
and the more expensive Long Range model can go up to 353 per charge.</p>
<p><strong>Label:</strong> Related (PS: For a given text, one and only one phrase is labeled 'Related' with it. All others are 'Unrelated')</p>
</blockquote>
<p>Example 2</p>
<blockquote>
<p><strong>Phrase:</strong> Programming languages</p>
<p><strong>Text:</strong> Python 3.9 uses a new parser, based on PEG instead of LL(1). The new parser’s performance is roughly comparable to that of the old
parser, but the PEG formalism is more flexible than LL(1) when it
comes to designing new language features. We’ll start using this
flexibility in Python 3.10 and later.</p>
<p>The ast module uses the new parser and produces the same AST as the
old parser.</p>
<p>In Python 3.10, the old parser will be deleted and so will all
functionality that depends on it (primarily the parser module, which
has long been deprecated). In Python 3.9 only, you can switch back to
the LL(1) parser using a command line switch (-X oldparser) or an
environment variable (PYTHONOLDPARSER=1).</p>
<p><strong>Label:</strong> Related(i.e. all other phrases are 'Unrelated')</p>
</blockquote>
<p>I think I may have to use, for example, pre-trained BERT, because this kind of prediction needs additional knowledge. But this does not seem like a standard classification problem so I can't find out-of-the-box codes. May I have some advice on how to combine existing wheels and train it?</p>
",Text Classification / Sentiment Analysis,predict phrase related short text article using supervised learning set short phrase set text want predict phrase related article phrase appearing article may still related example annotated data real like example phrase automobile text among affordable option electric vehicle marketplace tesla model without doubt one name recognition borrows styling cue company model sedan model x suv go way unique interior design glass roof acceleration quick model chassis playful well especially performance model receives sportier suspension track driving mode ev buyer likely interested driving range speediness handling model delivers base model offer mile driving range according epa expensive long range model go per charge label related p given text one one phrase labeled related others unrelated example phrase programming language text python us new parser based peg instead new parser performance roughly comparable old parser peg formalism flexible come designing new language feature start using flexibility python later ast module us new parser produce ast old parser python old parser deleted functionality depends primarily parser module ha long deprecated python switch back parser using command line switch x oldparser environment variable pythonoldparser label related e phrase unrelated think may use example pre trained bert kind prediction need additional knowledge doe seem like standard classification problem find box code may advice combine existing wheel train
How to do Topic Detection in Unsupervised Aspect Based Sentiment Analysis,"<p>I want to make an ABSA using Python where the sentiment of pre-defined aspects (e.g. delivery, quality, service) is analyzed from online reviews. I want to do it unsupervised because this will save me from manually labeling reviews and I can analyze a lot more review data (looking at around 100k reviews). Therefore, my datasets consists of only reviews and no ratings. I would like to have a model that can first detect the aspect category and then assign the sentiment polarity. E.g. when the review says ""The shipment went smoothly, but the product is broken"" I want the model to assign the word ""shipment"" to the aspect category ""delivery"" and ""smoothly"" relates to a positive sentiment. </p>

<p>I have searched for approaches to take and I would like to know if anyone has experience with this and could guide me into a direction that could help me. It will be highly appreciated!</p>
",Text Classification / Sentiment Analysis,topic detection unsupervised aspect based sentiment analysis want make absa using python sentiment pre defined aspect e g delivery quality service analyzed online review want unsupervised save manually labeling review analyze lot review data looking around k review therefore datasets consists review rating would like model first detect aspect category assign sentiment polarity e g review say shipment went smoothly product broken want model assign word shipment aspect category delivery smoothly relates positive sentiment searched approach take would like know anyone ha experience could guide direction could help highly appreciated
Sentiment Analysis using LSTM (Model has not not generate good output),"<p>I Make a sentiment analysis model using LSTM but my model gives very bad prediction.</p>
<p><a href=""https://github.com/BUBAIMITRA2018/AI-ml/blob/master/lstm_sentimentanalysis_model.ipynb"" rel=""nofollow noreferrer"">Here is the complete code</a></p>
<p><a href=""https://www.kaggle.com/subrata2019/amazonreview"" rel=""nofollow noreferrer"">Dataset for amazon review</a></p>
<p>My LSTM model looks like this:</p>
<pre><code>def ltsm_model(input_shape, word_to_vec_map, word_to_index):
&quot;&quot;&quot;
Function creating the ltsm_model model's graph.

Arguments:
input_shape -- shape of the input, usually (max_len,)
word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

Returns:
model -- a model instance in Keras
&quot;&quot;&quot;

### START CODE HERE ###
# Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).
sentence_indices =  Input(shape=input_shape, dtype='int32')

# Create the embedding layer pretrained with GloVe Vectors (≈1 line)
embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)

# Propagate sentence_indices through your embedding layer, you get back the embeddings
embeddings = embedding_layer(sentence_indices)   

# Propagate the embeddings through an LSTM layer with 128-dimensional hidden state
# Be careful, the returned output should be a batch of sequences.
X = LSTM(128, return_sequences=True)(embeddings)
# Add dropout with a probability of 0.5
X = Dropout(0.5)(X)
# Propagate X trough another LSTM layer with 128-dimensional hidden state
# Be careful, the returned output should be a single hidden state, not a batch of sequences.
X = LSTM(128, return_sequences=False)(X)
# Add dropout with a probability of 0.5
X = Dropout(0.5)(X)
# Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.
X = Dense(2, activation='relu')(X)
# Add a softmax activation
X = Activation('softmax')(X)

# Create Model instance which converts sentence_indices into X.
model = Model(inputs=[sentence_indices], outputs=X)

### END CODE HERE ###

return model
</code></pre>
<p>Here is what my training dataset looks like:</p>
<p><a href=""https://i.sstatic.net/OrUDw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OrUDw.png"" alt="""" /></a></p>
<p>This is my testing data:</p>
<pre><code>x_test = np.array(['amazing!: this soundtrack is my favorite music..'])
X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)
print(x_test[0] +' '+  str(np.argmax(model.predict(X_test_indices))))
</code></pre>
<p>I got following out for this:</p>
<blockquote>
<p>amazing!: this soundtrack is my favorite music.. 0</p>
</blockquote>
<p>But it should be positive sentiment and should be 1</p>
<p>Also this my fit model output:</p>
<p><a href=""https://i.sstatic.net/GHSU6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GHSU6.png"" alt=""Fit model output"" /></a></p>
<p>How can I improve my model performance? This pretty bad model I suppose.</p>
",Text Classification / Sentiment Analysis,sentiment analysis using lstm model ha generate good output make sentiment analysis model using lstm model give bad prediction complete code dataset amazon review lstm model look like training dataset look like testing data got following amazing soundtrack favorite music positive sentiment also fit model output improve model performance pretty bad model suppose
Finding the Antonym of a Word,"<p>I'm working on an aspect-based sentiment analysis model using spaCy. I managed to extract aspects and adjectives as pairs in a list. I also included &quot;not&quot; before any adjective to handle any negations. I want to swap the adjective with its antonym if there is &quot;not&quot; before the adjective. I know spaCy has some similarity detection tools but I couldn't find anything about antonyms. Is it possible to do this with spaCy? If not how can I do it or is there a better way to handle the negations?</p>
<pre><code>import spacy
from spacy.matcher import Matcher
nlp = spacy.load('en_core_web_sm')

txt = &quot;The performance of the product is not great but The price is fair.&quot;
txt = txt.lower()

output = []
doc = nlp(txt)

matcher = Matcher(nlp.vocab, validate=True)
matcher.add(&quot;mood&quot;,None,[{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;is&quot;,&quot;are&quot;]}},{&quot;LOWER&quot;:{&quot;IN&quot;:[&quot;no&quot;,&quot;not&quot;]},&quot;OP&quot;:&quot;?&quot;},{&quot;DEP&quot;:&quot;advmod&quot;,&quot;OP&quot;:&quot;?&quot;},{&quot;DEP&quot;:&quot;acomp&quot;}])
for nc in doc.noun_chunks:
    d = doc[nc.root.right_edge.i+1:nc.root.right_edge.i+1+3]
    matches = matcher(d)
    if matches:
        _, start, end = matches[0]
        output.append((nc.text, d[start+1:end].text))
    
print(output)
</code></pre>
<p>Expected output:</p>
<pre><code>[('the performance', 'not great'), ('the product', 'not great'), ('the price', 'fair')]
</code></pre>
",Text Classification / Sentiment Analysis,finding antonym word working aspect based sentiment analysis model using spacy managed extract aspect adjective pair list also included adjective handle negation want swap adjective antonym adjective know spacy ha similarity detection tool find anything antonym possible spacy better way handle negation expected output
BI LSTM with attention layer in python for text classification,"<p>I want to apply this method to implement Bi-LSTM with attention. The method is discussed here: <a href=""https://stackoverflow.com/questions/52867069/bi-lstm-attention-model-in-keras"">Bi-LSTM Attention model in Keras</a></p>
<p>I get the following error:
<code>'module' object is not callable</code></p>
<p>It can not apply multiply in this line:
<code>sent_representation = merge([lstm, attention], mode='mul')</code></p>
<pre class=""lang-py prettyprint-override""><code>from keras.layers import merge
import tensorflow as tf
from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Permute, RepeatVector
from tensorflow.keras.layers import Bidirectional

inp =Input(shape=(maxlen,), dtype='float32')
x = Embedding(max_features, embed_size, weights=[emb_matrix])(inp)
lstm = Bidirectional(LSTM(50, return_sequences=True), name=&quot;bi_lstm_0&quot;)(x)

attention = Dense(1, activation='tanh')(lstm)
attention = Flatten()(attention)
attention = Activation('softmax')(attention)
attention = RepeatVector(max_features*2)(attention)
attention = Permute([2,1])(attention)

sent_representation = merge([lstm, attention], mode='mul')
sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)

output = Dense(3, activation=&quot;softmax&quot;)(sent_representation)
</code></pre>
",Text Classification / Sentiment Analysis,bi lstm attention layer python text classification want apply method implement bi lstm attention method discussed
"My model is not learning, text classification with LSTM","<p>My data can be found here:</p>
<p><a href=""https://www.dropbox.com/sh/53ii1gpm155f1x8/AADoYZk3cQt5Zw7tfuSV6kZBa?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/sh/53ii1gpm155f1x8/AADoYZk3cQt5Zw7tfuSV6kZBa?dl=0</a></p>
<p>The <code>DataFram</code> is about (I have 3 writers and text belong to them)</p>
<p>The task that I am doing is: sentiment analysis with <code>LSTM</code> layer, which means in the end I want to test the model on unseen data and it tells me that the text belongs to which writers.</p>
<p>I am working on this for many hours and hours, but in the end, my model accuracy does not change with the number of epcho.</p>
<p>as I am new to deep learning, I don't know where I went wrong.
and what else I can do to increase the accuracy?</p>
<p>can anyone help me please?</p>
<p>So the code is like below:</p>
<pre><code>import pandas as pd
import numpy as np
import xgboost as xgb
from tqdm import tqdm
from sklearn.svm import SVC
from keras.models import Sequential
from keras.layers.recurrent import LSTM, GRU
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.embeddings import Embedding
from keras.layers.normalization import BatchNormalization
from keras.utils import np_utils
from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D
from keras.preprocessing import sequence, text
from keras.callbacks import EarlyStopping
from nltk import word_tokenize
from nltk.corpus import stopwords
stop_words = stopwords.words('English')


traincleaned=pd.read_csv('../input/dataclean/data.csv')

traincleaned.drop('Unnamed: 0', axis=1,inplace=True)  
traincleaned[:3]

data_list=train_sentence.clean_text.apply(str).tolist()
test_list=evalu.clean_text.apply(str).tolist()


X = traincleaned.clean_text
Y = traincleaned.author

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1000)
print(X_train.shape)
print(y_train.shape)


# transform labels into numbers

from sklearn.preprocessing import LabelEncoder
labels2numbers = LabelEncoder()
y_train = labels2numbers.fit_transform(y_train)
y_val = labels2numbers.fit_transform(y_test)

print(y_train.shape)

#Converts a class vector (integers) to binary class matrix.

label_encoder = LabelEncoder()
integer_encoded1 = label_encoder.fit_transform(y_train)
integer_encoded2 = labels2numbers.fit_transform(y_test)


le_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))
print(&quot;Label Encoding Classes as &quot;)
print(le_name_mapping)

y_train=np_utils.to_categorical(integer_encoded1,num_classes=3)
y_val=np_utils.to_categorical(integer_encoded2,num_classes=3)
print(&quot;One Hot Encoded class shape &quot;)
print(y_train.shape)
print(y_train[0])



X_train0 = X_train.values.astype(str)
#y_train = df0_train['sentiment'].values

X_val0 = X_test.values.astype(str)
#y_val = df0_val['sentiment'].values
print('df_train shape: {}'.format(df0_train.shape))
print('df_val shape: {}'.format(df0_val.shape))





from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences    
tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(X_train0)
X_train = tokenizer.texts_to_sequences(X_train0)
X_test = tokenizer.texts_to_sequences(X_val0)
vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index
from keras.preprocessing.sequence import pad_sequences
maxlen = 300
padded_sequence_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
padded_sequence_test = pad_sequences(X_test, padding='post', maxlen=maxlen)
print(vocab_size)
print(X_train[2])
print(X_train0[2])



#find the max lenght of sentecnce for padding
maxlen = train_sentence.clean_text.str.len().max()
maxlen



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout
from tensorflow.keras.layers import SpatialDropout1D
from tensorflow.keras.layers import Embedding

embedding_vector_length = 50

model = Sequential()

model.add(Embedding(vocab_size, embedding_vector_length,     
                                         input_length=300) )

model.add(SpatialDropout1D(0.25))
model.add(LSTM(30, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))
model.add(Dense(3, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam', 
                           metrics=['accuracy'])
print(model.summary())


history = model.fit(padded_sequence_train, y_train,validation_data=(padded_sequence_test, y_val),epochs=20,batch_size=70)


</code></pre>
<p>The model summery is as below:</p>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_17 (Embedding)     (None, 300, 50)           881900    
_________________________________________________________________
spatial_dropout1d_16 (Spatia (None, 300, 50)           0         
_________________________________________________________________
lstm_17 (LSTM)               (None, 30)                9720      
_________________________________________________________________
dropout_16 (Dropout)         (None, 30)                0         
_________________________________________________________________
dense_17 (Dense)             (None, 3)                 93        
=================================================================
Total params: 891,713
Trainable params: 891,713
Non-trainable params: 0
_________________________________________________________________
None

</code></pre>
<p>Also when I fit model the result is as below:</p>
<pre><code>
Train on 15663 samples, validate on 3916 samples
Epoch 1/30
15663/15663 [==============================] - 92s 6ms/step - loss: 0.6338 - acc: 0.6667 - val_loss: 0.6318 - val_acc: 0.6667
Epoch 2/30
15663/15663 [==============================] - 93s 6ms/step - loss: 0.6331 - acc: 0.6666 - val_loss: 0.6317 - val_acc: 0.6667
Epoch 3/30
15663/15663 [==============================] - 92s 6ms/step - loss: 0.6326 - acc: 0.6667 - val_loss: 0.6318 - val_acc: 0.6667
Epoch 4/30
15663/15663 [==============================] - 92s 6ms/step - loss: 0.6327 - acc: 0.6667 - val_loss: 0.6315 - val_acc: 0.6667
Epoch 5/30
15663/15663 [==============================] - 91s 6ms/step - loss: 0.6321 - acc: 0.6667 - val_loss: 0.6316 - val_acc: 0.6667
Epoch 6/30
15663/15663 [==============================] - 94s 6ms/step - loss: 0.6321 - acc: 0.6667 - val_loss: 0.6316 - val_acc: 0.6667
Epoch 7/30
15663/15663 [==============================] - 91s 6ms/step - loss: 0.6320 - acc: 0.6667 - val_loss: 0.6316 - val_acc: 0.6667
Epoch 8/30
15663/15663 [==============================] - 92s 6ms/step - loss: 0.6321 - acc: 0.6667 - val_loss: 0.6318 - val_acc: 0.6667
Epoch 9/30
15610/15663 [============================&gt;.] - ETA: 0s - loss: 0.6318 - acc: 0.6667
</code></pre>
",Text Classification / Sentiment Analysis,model learning text classification lstm data found writer text belong task sentiment analysis layer mean end want test model unseen data tell text belongs writer working many hour hour end model accuracy doe change number epcho new deep learning know went wrong else increase accuracy anyone help please code like model summery also fit model result
Interpreting attention in Keras Transformer official example,"<p>I have implemented a model as explained in (Text classification with Transformer) <a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_with_transformer/</a></p>
<p>I would like to access the attention values for a specific example.</p>
<p>I understand attention is calculated somewhere around this point:</p>
<pre><code>class TransformerBlock(layers.Layer):
    [...]

def call(self, inputs, training):
    attn_output = self.att(inputs)
    attn_output = self.dropout1(attn_output, training=training)
    out1 = self.layernorm1(inputs + attn_output)
    ffn_output = self.ffn(out1)
    ffn_output = self.dropout2(ffn_output, training=training)
    return self.layernorm2(out1 + ffn_output)
</code></pre>
<p>[...]</p>
<pre><code>embed_dim = 32  # Embedding size for each token

num_heads = 2  # Number of attention heads
ff_dim = 32  # Hidden layer size in feed forward network inside transformer

inputs = layers.Input(shape=(maxlen,))
embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
x = transformer_block(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.1)(x)
x = layers.Dense(20, activation=&quot;relu&quot;)(x)
x = layers.Dropout(0.1)(x)
outputs = layers.Dense(2, activation=&quot;softmax&quot;)(x)
</code></pre>
<p>If I do:</p>
<pre><code>A=(model.layers[2].att(model.layers[1](model.layers[0]((X_train[0,:])))))
</code></pre>
<p>I can retrieve a matrix sized as <code>maxlen</code> x<code>num_heads</code> .</p>
<p>How should I interpret these coefficients?</p>
",Text Classification / Sentiment Analysis,interpreting attention kera transformer official example implemented model explained text classification transformer would like access attention value specific example understand attention calculated somewhere around point retrieve matrix sized x interpret coefficient
Classify a noun into abstract or concrete using NLTK or similar,"<p>How can I categorize a list of nouns into abstract or concrete in Python?</p>

<p>For example: </p>

<pre><code>""Have a seat in that chair.""
</code></pre>

<p>In above sentence <code>chair</code> is noun and can be categorized as concrete.</p>
",Text Classification / Sentiment Analysis,classify noun abstract concrete using nltk similar categorize list noun abstract concrete python example sentence noun categorized concrete
Deploying a text classification model on new (unseen) text,"<p>I am working on a text classification problem. I have attached a simple dummy snippet of a text classification model I have trained.</p>
<p>How do I deploy the model on new_text? When the model is used on <code>check_predictions</code>, it classifies text correctly, however, when new data is used, the classification is incorrect.</p>
<p>Is this because the <code>new_text</code> would need to be vectorised? Am I missing something fundamental?</p>
<pre><code>from collections import Counter
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score

df = pd.read_csv(&quot;/Users/veg.csv&quot;)
print (df)
</code></pre>
<p><a href=""https://i.sstatic.net/OXgSQm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OXgSQm.png"" alt=""first 15 rows of df"" /></a></p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'],random_state=1, test_size=0.2)
cv = CountVectorizer()

X_train_vectorized = cv.fit_transform(X_train)
X_test_vectorized = cv.transform(X_test)

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train_vectorized, y_train)
predictions = naive_bayes.predict(X_test_vectorized)

print(&quot;Accuracy score: &quot;, accuracy_score(y_test, predictions))
print('accuracy %s' % accuracy_score(predictions, y_test))
print(classification_report(y_test, predictions))
</code></pre>
<p><a href=""https://i.sstatic.net/pjrzMm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pjrzMm.png"" alt=""Output"" /></a></p>
<pre><code>check_predictions = []
for i in range(len(X_test)):   
    if predictions[i] == 0:
        check_predictions.append('vegetable')
    if predictions[i] == 1:
        check_predictions.append('fruit')
    if predictions[i] == 2:
        check_predictions.append('tree')
        
dummy_df = pd.DataFrame({'actual_label': list(y_test), 'prediction': check_predictions, 'Text':list(X_test)})
dummy_df.replace(to_replace=0, value='vegetable', inplace=True)
dummy_df.replace(to_replace=1, value='fruit', inplace=True)
dummy_df.replace(to_replace=2, value='tree', inplace=True)
print(&quot;DUMMY DF&quot;)
print(dummy_df.head(10))

</code></pre>
<p><a href=""https://i.sstatic.net/F5o1am.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F5o1am.png"" alt=""test df"" /></a></p>
<pre><code>new_data=['carrot', 'grapes',
          'banana', 'potato',
          'birch','carrot', 'grapes',
          'banana', 'potato', 'birch','carrot','grapes',
          'banana', 'potato',
          'birch','carrot', 'grapes',
          'banana', 'potato', 'birch','grapes',
          'banana', 'potato', 'birch']

new_predictions = []
for i in range(len(new_data)):    
    if predictions[i] == 0:
        new_predictions.append('vegetable')
    if predictions[i] == 1:
        new_predictions.append('fruit')
    if predictions[i] == 2:
        new_predictions.append('tree')
        
new_df = pd.DataFrame({'actual_label': list(y_test), 'prediction': new_predictions, 'Text':list(new_data)})        
new_df.replace(to_replace=0, value='vegetable', inplace=True)
new_df.replace(to_replace=1, value='fruit', inplace=True)
new_df.replace(to_replace=2, value='tree', inplace=True)
print(&quot;NEW DF&quot;)
print(new_df.head(10))

</code></pre>
<p><a href=""https://i.sstatic.net/zBOi0m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zBOi0m.png"" alt=""New data df"" /></a></p>
",Text Classification / Sentiment Analysis,deploying text classification model new unseen text working text classification problem attached simple dummy snippet text classification model trained deploy model new text model used classifies text correctly however new data used classification incorrect would need vectorised missing something fundamental
How to make prediction on new text dataset using saved text classification model,"<p>I trained a text classifier under this guide: <a href=""https://developers.google.com/machine-learning/guides/text-classification/step-4"" rel=""nofollow noreferrer"">https://developers.google.com/machine-learning/guides/text-classification/step-4</a></p>
<p>And save model as</p>
<pre><code>model.save('~./output/model.h5')
</code></pre>
<p>In this case, how i use this model to classify texts on another new dataset?</p>
<p>Thank you</p>
",Text Classification / Sentiment Analysis,make prediction new text dataset using saved text classification model trained text classifier guide save model case use model classify text another new dataset thank
bespoke sentiment analysis: scoring documents based on words and their respective scores - R NLP,"<p>I am trying to score documents based on the words that occur in them. I have two types of scores for each word occuring in the corpus. It is essentially like a sentiment analysis but with a bespoke dictionary and respective scores. THANK YOU &lt;3</p>
<pre><code>#documents to be scored on 2 dimensions: score1 and score2
documents &lt;- data.frame(textID = 1:3, text = c(&quot;Hello everybody, pleased to see everyone together&quot;, &quot; DHL postmen have faced difficulties this year&quot;, &quot;divorcees have trouble finding jobs in this country&quot;), scored1 = rep(NA,3), scored2=rep(NA,3) )

#first scoring dimension
scores1 &lt;- as.matrix(data.frame(words = c(&quot;hello&quot;, &quot;everybody&quot;, &quot;pleased&quot;, &quot;to&quot; ,&quot;see&quot;, &quot;everyone&quot;,&quot;together&quot;, &quot;DHL&quot;, &quot;postmen&quot;, &quot;have&quot;, &quot;faced&quot;,&quot;difficulties&quot;,&quot;this&quot;, &quot;year&quot;, &quot;divorcees&quot;, &quot;trouble&quot;, &quot;finding&quot;, &quot;jobs&quot;, &quot;in&quot;, &quot;country&quot; ), scores = 1:20))

#second scoring dimension
scores2 &lt;- as.matrix(data.frame(words = c(&quot;hello&quot;, &quot;everybody&quot;, &quot;pleased&quot;, &quot;to&quot; ,&quot;see&quot;, &quot;everyone&quot;,&quot;together&quot;, &quot;DHL&quot;, &quot;postmen&quot;, &quot;have&quot;, &quot;faced&quot;,&quot;difficulties&quot;,&quot;this&quot;, &quot;year&quot;, &quot;divorcees&quot;, &quot;trouble&quot;, &quot;finding&quot;, &quot;jobs&quot;, &quot;in&quot;, &quot;country&quot; ), scores = 10:29))

#the result should look like this, where each text receives a score that represents the sum of #individual word scores: 

#textID                                                  text      scored1 scored2
#1      1   Hello everybody, pleased to see everyone together       28        91
#2      2       DHL postmen have faced difficulties this year       77        140
#3      3 divorcees have trouble finding jobs in this country       128       200

</code></pre>
",Text Classification / Sentiment Analysis,bespoke sentiment analysis scoring document based word respective score r nlp trying score document based word occur two type score word occuring corpus essentially like sentiment analysis bespoke dictionary respective score thank
Gridsearch for NLP - How to combine CountVec and other features?,"<p>I am doing a basic NLP project on Sentiment Analysis, and I would like to use GridsearchCV to optimise my model.</p>
<p>The code below shows a sample dataframe I am working with. 'Content' is the column to pass to CountVectorizer, 'label' is the y column to be predicted, and feature_1, feature_2 are columns I wish to include in my model as well.</p>
<pre><code>'content': 'Got flat way today Pot hole Another thing tick crap thing happen week list',
'feature_1': '1', 
'feature_2': '34', 
'label':1}, 
{'content': 'UP today Why doe head hurt badly',
'feature_1': '5', 
'feature_2': '142', 
'label':1},
{'content': 'spray tan fail leg foot Ive scrubbing foot look better ',
 'feature_1': '7', 
'feature_2': '123', 
'label':0},])
</code></pre>
<p>I am following a stackoverflow answer: <a href=""https://stackoverflow.com/questions/53841913/perform-feature-selection-using-pipeline-and-gridsearch"">Perform feature selection using pipeline and gridsearch</a></p>
<pre><code>from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.base import TransformerMixin, BaseEstimator
class CustomFeatureExtractor(BaseEstimator, TransformerMixin):
    def __init__(self, feature_1=True, feature_2=True):
        self.feature_1=feature_1
        self.feature_2=feature_2
        
    def extractor(self, tweet):
        features = []

        if self.feature_2:
            
            features.append(df['feature_2'])

        if self.feature_1:
            features.append(df['feature_1'])
        
          
        return np.array(features)

    def fit(self, raw_docs, y):
        return self

    def transform(self, raw_docs):
        
        return np.vstack(tuple([self.extractor(tweet) for tweet in raw_docs]))

</code></pre>
<p>Below is the gridsearch I tried to fit my dataframe on:</p>
<pre><code>lr = LogisticRegression()

# Pipeline
pipe = Pipeline([('features', FeatureUnion([(&quot;vectorizer&quot;, CountVectorizer(df['content'])),
                                            (&quot;extractor&quot;, CustomFeatureExtractor())]))
                 ,('classifier', lr())
                ])
But yields results: TypeError: 'LogisticRegression' object is not callable

</code></pre>
<p>Wonder if there are any other easier ways to do this?</p>
<p>I have already referred to the threads below, however, to no avail:
<a href=""https://stackoverflow.com/questions/48573174/how-to-combine-tfidf-features-with-other-features"">How to combine TFIDF features with other features</a>
<a href=""https://stackoverflow.com/questions/53841913/perform-feature-selection-using-pipeline-and-gridsearch"">Perform feature selection using pipeline and gridsearch</a></p>
",Text Classification / Sentiment Analysis,gridsearch nlp combine countvec feature basic nlp project sentiment analysis would like use gridsearchcv optimise model code show sample dataframe working content column pas countvectorizer label column predicted feature feature column wish include model well following stackoverflow answer href feature selection using pipeline gridsearch
BERT model classification with many classes,"<p>I want to train a BERT model to perform a multiclass text classification. I use transformers and followed this tutorial (<a href=""https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613</a>) to train it on Google Colab.</p>
<p>The issue is that I have a huge number of classes (about 600) and I feel like it affects the performance that is quite disappointing.</p>
<p>I looked a bit on Stackoverflow and found this thread (<a href=""https://stackoverflow.com/questions/54850657/intent-classification-with-large-number-of-intent-classes"">Intent classification with large number of intent classes</a>) that answered my question but I don't know how to implement it.</p>
<p>The answer to the similar question was: &quot;If you could classify your intents into some coarse-grained classes, you could train a classifier to specify which of these coarse-grained classes your instance belongs to. Then, for each coarse-grained class train another classifier to specify the fine-grained one. This hierarchical structure will probably improve the results. Also for the type of classifier, I believe a simple fully connected layer on top of BERT would suffice.&quot;</p>
<p>Do I have to train my models separately and use &quot;if&quot; conditions to build tbhe workflow or is there a way to train all your BERT models simultaneously and have one unifying model ?</p>
<p>Thanks in advance</p>
",Text Classification / Sentiment Analysis,bert model classification many class want train bert model perform multiclass text classification use transformer followed tutorial train google colab issue huge number class feel like affect performance quite disappointing looked bit stackoverflow found thread href classification large number intent class answered question know implement answer similar question wa could classify intent coarse grained class could train classifier specify coarse grained class instance belongs coarse grained class train another classifier specify fine grained one hierarchical structure probably improve result also type classifier believe simple fully connected layer top bert would suffice train model separately use condition build tbhe workflow way train bert model simultaneously one unifying model thanks advance
One-hot encoding labels for binary text classification that are already 0s and 1s?,"<p>I am doing a simple binary text classification, and my label data are already in the format 0 and 1. I am wondering if I still need to perform a one-hot encoding so that they're in a [0,1] and [1,0] format?</p>
<p>When I feed the labels into my Keras <code>Sequential()</code> model as  <code>&lt;class 'numpy.ndarray'&gt;</code> , it works for the model and I get decent accuracy. But I still wonder if I should one-hot encode them beforehand?</p>
",Text Classification / Sentiment Analysis,one hot encoding label binary text classification already simple binary text classification label data already format wondering still need perform one hot encoding format feed label kera model work model get decent accuracy still wonder one hot encode beforehand
Alexandria Contextual Text Analysis with ibapi,"<p>I am using the <strong>Interactive Brokers API</strong> with python.  I notice in the <strong>Trader Work Station</strong> there is a window with news updates.  I understand how to get the news updates using the <code>reqMkData</code>.  This <a href=""https://stackoverflow.com/questions/54917982/how-to-obtain-news-contract-details-from-the-interactive-brokers-api"">post</a> does a great job explaining that much.</p>
<p>My question:</p>
<p>Is it possible to get <strong>Alexandria Contextual Text Analysis</strong> (ACTA) data that appears in the <strong>Trader Work Station</strong>?  I did not see anything in the <a href=""http://interactivebrokers.github.io/tws-api/news.html"" rel=""nofollow noreferrer"">documentation</a> about news updates.  Here is a screenshot: <a href=""https://i.sstatic.net/nJkqr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nJkqr.png"" alt=""News Window in Trader Work Station"" /></a> The ACTA analysis is the far-right column labeled <strong>Rank</strong>.</p>
<p>I figured I would ask here first before I go through the process of writing a sentiment analysis myself.  Any help is greatly appreciated, thank you.</p>
",Text Classification / Sentiment Analysis,alexandria contextual text analysis ibapi using interactive broker api python notice trader work station window news update understand get news update using documentation news update screenshot acta analysis far right column labeled rank figured would ask first go process writing sentiment analysis help greatly appreciated thank
how to use bert for long sentences?,"<p>I am trying to classify given text into news, clickbait or others. The texts which I have for training are long.<a href=""https://i.sstatic.net/R5N6l.png"" rel=""nofollow noreferrer"">distribution of lengths is shown here.</a>
Now, the question is should I trim the text at the middle and make it 512 tokens long? But, I have even documents with circa 10,000 words so won't I loose the gist by truncation? Or, should I split my text into sub texts of 512 length. If so, then the sub text of one text may be similar to subtext of another text but the labels will be different. Doesn't it become noisy data? Or, should I just use bidirectional LSTM's here and pad to max_len?</p>
",Text Classification / Sentiment Analysis,use bert long sentence trying classify given text news clickbait others text training long distribution length shown question trim text middle make token long even document circa word loose gist truncation split text sub text length sub text one text may similar subtext another text label different become noisy data use bidirectional lstm pad max len
Expanding Twitter sentiment analysis,"<p>The code below analyzes twitter sentiment: whether it is positive, negative or neutral. However, it is fairly inaccurate for many tweets such as if it includes ""someone gave him a middle fingered saulte"", I want to train the program to recognize that middle fingered implies disrepect, even though it includes the word salute in the sentence. </p>

<p>Any suggestions would be appreciated.</p>

<p>import re
import tweepy
from tweepy import OAuthHandler
from textblob import TextBlob</p>

<pre><code>class TwitterClient(object):
    '''
    Generic Twitter Class for sentiment analysis.
    '''
    def __init__(self):
        '''
        Class constructor or initialization method.
        '''
        # keys and tokens from the Twitter Dev Console
        consumer_key = 'WHexAxkRn6uEJkzS2CKpeQejI'
        consumer_secret = 'fSxjGVM247YS6Y6BpkWXaIfr6ThXdoSUg2y0aR259vNXVPPfob'
        access_token = '915324744140025862-jnGvcTPkJHOObkeydiVburK8SdAngEk'
        access_token_secret = 'JGgkWI9Lq0rJU1K0C8JLplRnSrEuw8pj3anOlIsn3YdiO'


        # attempt authentication
        try:
            # create OAuthHandler object
            self.auth = OAuthHandler(consumer_key, consumer_secret)
            # set access token and secret
            self.auth.set_access_token(access_token, access_token_secret)
            # create tweepy API object to fetch tweets
            self.api = tweepy.API(self.auth)
        except:
            print(""Error: Authentication Failed"")

    def clean_tweet(self, tweet):
        '''
        Utility function to clean tweet text by removing links, special characters
        using simple regex statements.
        '''
        return ' '.join(re.sub(""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"", "" "", tweet).split())

    def get_tweet_sentiment(self, tweet):
        '''
        Utility function to classify sentiment of passed tweet
        using textblob's sentiment method
        '''
        # create TextBlob object of passed tweet text
        analysis = TextBlob(self.clean_tweet(tweet))
        # set sentiment
        if analysis.sentiment.polarity &gt; 0:
            return 'positive'
        elif analysis.sentiment.polarity == 0:
            return 'neutral'
        else:
            return 'negative'

    def get_tweets(self, query, count = 30):
        '''
        Main function to fetch tweets and parse them.
        '''
        # empty list to store parsed tweets
        tweets = []

        try:
            # call twitter api to fetch tweets
            fetched_tweets = self.api.search(q = query, count = count)

            # parsing tweets one by one
            for tweet in fetched_tweets:
                # empty dictionary to store required params of a tweet
                parsed_tweet = {}

                # saving text of tweet
                parsed_tweet['text'] = tweet.text
                # saving sentiment of tweet
                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)

                # appending parsed tweet to tweets list
                if tweet.retweet_count &gt; 0:
                    # if tweet has retweets, ensure that it is appended only once
                    if parsed_tweet not in tweets:
                        tweets.append(parsed_tweet)
                else:
                    tweets.append(parsed_tweet)

            # return parsed tweets
            return tweets

        except tweepy.TweepError as e:
            # print error (if any)
            print(""Error : "" + str(e))

def main():
    # creating object of TwitterClient Class
    api = TwitterClient()
    # calling function to get tweets
    tweets = api.get_tweets(query = 'Donald Trump', count = 200)

    # picking positive tweets from tweets
    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive']
    # percentage of positive tweets
    print(""Positive tweets percentage: {} %"".format(100*len(ptweets)/len(tweets)))
    # picking negative tweets from tweets
    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']
    # percentage of negative tweets
    print(""Negative tweets percentage: {} %"".format(100*len(ntweets)/len(tweets)))
    # percentage of neutral tweets
    print(""Neutral tweets percentage:{}%"".format(100*(len(tweets) - len(ntweets) - len(ptweets))/len(tweets)))

    # printing first 5 positive tweets
    print(""\n\nPositive tweets:"")
    for tweet in ptweets[:20]:
        print(tweet['text'])

    # printing first 5 negative tweets
    print(""\n\nNegative tweets:"")
    for tweet in ntweets[:20]:
        print(tweet['text'])

if __name__ == ""__main__"":
    # calling main function
    main()
</code></pre>
",Text Classification / Sentiment Analysis,expanding twitter sentiment analysis code analyzes twitter sentiment whether positive negative neutral however fairly inaccurate many tweet includes someone gave middle fingered saulte want train program recognize middle fingered implies disrepect even though includes word salute sentence suggestion would appreciated import import tweepy tweepy import oauthhandler textblob import textblob
How can we classify a post on a blog as being inappropriate for underage readers say 16?,"<p>Update: How would one approach the task of classifying any text on public forums such as Games, or blogs such that derogatory comments/texts before bring posted are filtered.</p>
<p>Original: &quot;
I want to filter out adult content from tweets (or any text for that matter).</p>
<p>For spam detection, we have datasets that check whether a particular text is spam or ham.</p>
<p>For adult content, I found a dataset I want to use (extract below):</p>
<pre><code>arrBad = [
'acrotomophilia',
'anal',
'anilingus',
'anus',
.
. etc.
.
'zoophilia']
</code></pre>
<p><strong>Question</strong></p>
<p>How can I use that dataset to filter text instances?
&quot;</p>
",Text Classification / Sentiment Analysis,classify post blog inappropriate underage reader say update would one approach task classifying text public forum game blog derogatory comment text bring posted filtered original want filter adult content tweet text matter spam detection datasets check whether particular text spam ham adult content found dataset want use extract question use dataset filter text instance
How to get the predictions of the textdata in fasttext python?,"<p>I am building a supervised model for text classification in fasttext.</p>
<p>To predict the output of a particular string we can use this in python</p>
<pre><code>model.predict(&quot;Why not put knives in the dishwasher?&quot;)
</code></pre>
<p>But how to get the predictions for the whole test set by a python command? In the commandline it can be done like this</p>
<pre><code>./fasttext predict &lt;path to model&gt; &lt;path to test file&gt; k &gt; &lt;path to prediction file&gt;
</code></pre>
",Text Classification / Sentiment Analysis,get prediction textdata fasttext python building supervised model text classification fasttext predict output particular string use python get prediction whole test set python command commandline done like
Unsupervised Sentiment Analysis,"<p>I've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.</p>

<p>My question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?</p>
",Text Classification / Sentiment Analysis,unsupervised sentiment analysis reading lot article explain need initial set text classified either positive negative sentiment analysis system really work question ha anyone attempted rudimentary check positive adjective v negative adjective taking account simple negators avoid classing happy positive article discus strategy realistic
How can I tag or give a document of text a topic?,"<p>I have set of documents and corresponding set of tags for those documents</p>
<p>ex.</p>
<p>Document-&quot;Learned Counsel appearing for the Appellants however points out that in the..etc etc&quot;</p>
<p>Tags - &quot;Compensation, Fundamental Right&quot;</p>
<p>Now I have multiple documents with their corresponding tags and I another test set of data without any tags what NLP techniques do I use to give these documents tag? Do I use text classification or topic modeling can someone please guide or suggest some ideas.</p>
",Text Classification / Sentiment Analysis,tag give document text topic set document corresponding set tag document ex document learned counsel appearing appellant however point etc etc tag compensation fundamental right multiple document corresponding tag another test set data without tag nlp technique use give document tag use text classification topic modeling someone please guide suggest idea
what is the meaning of hunspell_spell,"<p>I am working on text classifications and faced misspelling problem. I am trying to solve it with spacy_hunspell but can`t get what is the point of hunspell_spell property, cause it always returns None.</p>
<pre><code>for token in  nlp('I can haz cheezeburger.'):
    print(token._.hunspell_spell)
    if not token._.hunspell_spell:
        print(token.text, token._.hunspell_suggest)
</code></pre>
<p>Code listed above returns:</p>
<pre><code>None
I ['I', 'Ia', 'In', 'Ir', 'It', 'Io', 'IE', 'IA', 'AI', 'IN', 'RI', 'IT', 'IL', 'ID', 'DI']
None
can ['Can', 'van', 'cab', 'cam', 'ca', 'an', 'cane', 'cans', 'scan', 'cant', 'clan', 'cyan', 'Scan', 'car', 'ran']
None
haz ['ha', 'haze', 'hazy', 'has', 'hat', 'had', 'hag', 'ham', 'hap', 'hay', 'haw', 'ha z']
None
cheezeburger ['cheeseburger', 'vegeburger']
None
. []
</code></pre>
<p>At the beginning i thought that this property returns is token misspelled or not.<br/>
Does anybody know what is the meaning of property hunspell_spell?<br/>
How to check that word is misspelled with spacy_hunspell (does not comply with morphological rules)?<br/>
I think my implementation is not ideal.</p>
<pre><code>for token in  nlp('I can haz cheezeburger.'):
    is_correct = any([token.text == str.lower(suggest) for suggest in token._.hunspell_suggest])
    is_correct = &quot;correct&quot; if is_correct else &quot;misspeled&quot;
    print(f&quot;{token.text} is {is_correct}&quot;)
</code></pre>
<p>Code listed above returns:</p>
<pre><code>I is misspeled
can is correct
haz is misspeled
cheezeburger is misspeled
. is misspeled
</code></pre>
",Text Classification / Sentiment Analysis,meaning hunspell spell working text classification faced misspelling problem trying solve spacy hunspell get point hunspell spell property cause always return none code listed return beginning thought property return token misspelled doe anybody know meaning property hunspell spell check word misspelled spacy hunspell doe comply morphological rule think implementation ideal code listed return
Keras model.predict giving same values,"<p>I am using Keras <code>model.predict</code> to get label for new sentences not present in the dataset. But the prediction is always giving the same value irrespective of the sentence.</p>
<p>Here is my prediction code</p>
<pre><code>from sklearn.preprocessing import LabelEncoder

maxlen = 300
### PREDICT NEW UNSEEN DATA ###
tokenizer = Tokenizer()
label_enc = LabelEncoder()
label_enc.fit(tar_list)
X_test = ['asdsadav dawd','this is boring', 'wow i like this you did a great job', 'ima cry tht was mean','1 nov 1968 george harrison became the first beatle to release a solo album in the u k with the soundtrack to ']

X_test = tokenizer.texts_to_sequences(X_test)
X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

print(X_test)

a = (model.predict(X_test)&gt;0.5).astype(int).ravel()
print(a)

reverse_pred = label_enc.inverse_transform(a.ravel())
print(reverse_pred)

print(model.predict(X_test))
</code></pre>
<p>And here is the output</p>
<pre><code> [[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
[1 0 1 0 1 0 1 0 1 0]
[1 0 1 0 1 0 1 0 1 0]
[[0.988675   0.01132498]
 [0.988675   0.01132498]
 [0.988675   0.01132498]
 [0.988675   0.01132498]
 [0.988675   0.01132498]]
</code></pre>
<p>As we can see, the probability output is same for every sentence.</p>
<p>My code for training model is</p>
<pre><code>model = Sequential()
model.add(Embedding(max_words, 300, input_length=max_len))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(SpatialDropout1D(0.5))
model.add(Conv1D(16, kernel_size=3, activation='relu'))
model.add(Bidirectional(LSTM(16)))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))
model.summary()
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer = 'adam')
</code></pre>
<p>This is the tokenizer fitting for X_train</p>
<pre><code>max_words = 3000
max_len = 300
tok = Tokenizer(num_words = max_words)
tok.fit_on_texts(X_train)
sequences = tok.texts_to_sequences(X_train)
sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)
print(sequences_matrix)
Y_train = np.array(Y_train)
Y_test = np.array(Y_test)
</code></pre>
<p>Output is</p>
<pre><code>[[  0   0   0 ...  11  28  33]
 [  0   0   0 ...   2 125  63]
 [  0   0   0 ...   9 184  91]
 ...
 [  0   0   0 ... 105  22  85]
 [  0   0   0 ...  22  42 512]
 [  0   0   0 ...   9  28 406]]
</code></pre>
",Text Classification / Sentiment Analysis,kera model predict giving value using kera get label new sentence present dataset prediction always giving value irrespective sentence prediction code output see probability output every sentence code training model tokenizer fitting x train output
BERT Multi-class Sentiment Analysis got low accuracy?,"<p>I am working on a small data set which:</p>
<ul>
<li><p>Contains 1500 pieces of news articles.</p>
</li>
<li><p>All of these articles were ranked by human beings with regard to their sentiment/degree of positive on a 5-point scale.</p>
</li>
<li><p>Clean in terms of spelling errors. I used google sheet to check spelling before import into the analysis. There are still some characters that are not correctly coded, but not much.</p>
</li>
<li><p>The average length is greater than 512 words.</p>
</li>
<li><p>slightly-imbalanced data set.</p>
</li>
</ul>
<p>I regard this as a multi-class classification problem and I want to fine-tune BERT with this data set. In order to do that, I used <code>Ktrain</code> package and basically follows the tutorial. Below is my code:</p>
<pre><code>(x_train, y_train), (x_test, y_test), preproc = text.texts_from_array(
                                                                    x_train=x_train, 
                                                                    y_train=y_train,
                                                                    x_test=x_test, 
                                                                    y_test=y_test,
                                                                    class_names=categories,
                                                                    preprocess_mode='bert',
                                                                    maxlen= 510,
                                                                    max_features=35000)

model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)
learner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=6)
learner.fit_onecycle(2e-5, 4)
</code></pre>
<p>However, I only get a validation accuracy at around 25%, which is way too low.</p>
<pre><code>          precision-recall f1-score support

   1       0.33      0.40      0.36        75
   2       0.27      0.36      0.31        84
   3       0.23      0.24      0.23        58
   4       0.18      0.09      0.12        54
   5       0.33      0.04      0.07        24
</code></pre>
<pre><code>accuracy                               0.27       295
macro avg          0.27      0.23      0.22       295
weighted avg       0.26      0.27      0.25       295
</code></pre>
<p>I also tried the head+tail truncation strategy since some of the articles are pretty long, however, the performance remains the same.</p>
<p>Can anyone give me some suggestions?</p>
<p>Thank you very much!</p>
<p>Best</p>
<p>Xu</p>
<p>================== Update 7.21=================</p>
<p>Following Kartikey's advice, I tried the find_lr. Below is the result. It seems that 2e^-5 is a reasonable learning rate.</p>
<pre><code>simulating training for different learning rates... this may take a few 
moments...
Train on 1182 samples
Epoch 1/2
1182/1182 [==============================] - 223s 188ms/sample - loss: 1.6878 
- accuracy: 0.2487
Epoch 2/2
432/1182 [=========&gt;....................] - ETA: 2:12 - loss: 3.4780 - 
accuracy: 0.2639
done.
Visually inspect loss plot and select learning rate associated with falling 
loss
</code></pre>
<p><a href=""https://i.sstatic.net/5o1ZY.png"" rel=""nofollow noreferrer"">learning rate.jpg</a></p>
<p>And I just tried to run it with some weighting:</p>
<pre><code>{0: 0,
 1: 0.8294736842105264,
 2: 0.6715909090909091,
 3: 1.0844036697247708,
 4: 1.1311004784688996,
 5: 2.0033898305084747}
</code></pre>
<p>Here is the result. Not much changed.</p>
<pre><code>          precision    recall  f1-score   support

       1       0.43      0.27      0.33        88
       2       0.22      0.46      0.30        69
       3       0.19      0.09      0.13        64
       4       0.13      0.13      0.13        47
       5       0.16      0.11      0.13        28

accuracy                            0.24       296
macro avg       0.23      0.21      0.20       296
weighted avg    0.26      0.24      0.23       296

array([[24, 41,  9,  8,  6],
       [13, 32,  6, 12,  6],
       [ 9, 33,  6, 14,  2],
       [ 4, 25, 10,  6,  2],
       [ 6, 14,  0,  5,  3]])
</code></pre>
<p>============== update 7.22 =============</p>
<p>To get some baseline results, I collapse the classification problem on a 5-point scale into a binary one, which is just to predict positive or negative. This time the accuracy increased to around 55%. Below is the detailed description of my strategy:</p>
<pre><code>training data: 956 samples (excluding those classified as neutural)
truncation strategy: use the first 128 and last 128 tokens
(x_train,  y_train), (x_test, y_test), preproc_l1 = 
                     text.texts_from_array(x_train=x_train, y_train=y_train,    
                     x_test=x_test, y_test=y_test                      
                     class_names=categories_1,                      
                     preprocess_mode='bert',                                                          
                     maxlen=  256,                                                                  
                     max_features=35000)
Results:
              precision    recall  f1-score   support

       1       0.65      0.80      0.72       151
       2       0.45      0.28      0.35        89

accuracy                               0.61       240
macro avg          0.55      0.54      0.53       240
weighted avg       0.58      0.61      0.58       240

array([[121,  30],
       [ 64,  25]])
</code></pre>
<p>However, I think 55% is still not a satisfactory accuracy, slightly better than random guess.</p>
<p>============ update 7.26 ============</p>
<p>Following Marcos Lima's suggestion, I made several additional steps into my procedures:</p>
<ol>
<li><p>remove all numbers, punctuation and redundant spaces before being pre-processed by the Ktrain pkg. (I thought the Ktrain pkg would do this for me, but not sure)</p>
</li>
<li><p>I use the first 384 and last 128 tokens of any text in my sample. This is what I called &quot;Head+Tail&quot; strategy.</p>
</li>
<li><p>The task is still binary classification (positive vs negative)</p>
</li>
</ol>
<p>This is the figure for learning curve. It remains the same as the one I posted before. And it still looks very different to the one posted by Marcos Lima:</p>
<p><a href=""https://i.sstatic.net/CqO0T.png"" rel=""nofollow noreferrer"">The updated learning curve</a></p>
<p>Below are my results, which are probably the best set of results that I have got.</p>
<pre><code>begin training using onecycle policy with max lr of 1e-05...
Train on 1405 samples
Epoch 1/4
1405/1405 [==============================] - 186s 133ms/sample - loss: 0.7220 
- accuracy: 0.5431
Epoch 2/4
1405/1405 [==============================] - 167s 119ms/sample - loss: 0.6866 
- accuracy: 0.5843
Epoch 3/4
1405/1405 [==============================] - 166s 118ms/sample - loss: 0.6565 
- accuracy: 0.6335
Epoch 4/4
1405/1405 [==============================] - 166s 118ms/sample - loss: 0.5321 
- accuracy: 0.7587

             precision    recall  f1-score   support

       1       0.77      0.69      0.73       241
       2       0.46      0.56      0.50       111

accuracy                           0.65       352
macro avg       0.61      0.63      0.62       352
weighted avg       0.67      0.65      0.66       352

array([[167,  74],
       [ 49,  62]])
</code></pre>
<p>Note: I think maybe the reason why it is so difficult for the pkg to work well on my task is that this task is like a combination of classification and sentiment analysis. The classical classification task for news articles is to classify which category a news belongs, for example, biology, economics, sports. The words used in different categories are pretty different. On the other hand, the classical example for classifying sentiment is to analyse Yelp or IMDB reviews. My guess is these texts are pretty straightforward in expressing their sentiment whereas texts in my sample, economic news, are kind of polished and well organized before publication, so the sentiment might always appear in some implicit way which BERT may not be able to detect.</p>
",Text Classification / Sentiment Analysis,bert multi class sentiment analysis got low accuracy working small data set contains piece news article article ranked human regard sentiment degree positive point scale clean term spelling error used google sheet check spelling import analysis still character correctly coded much average length greater word slightly imbalanced data set regard multi class classification problem want fine tune bert data set order used package basically follows tutorial code however get validation accuracy around way low also tried head tail truncation strategy since article pretty long however performance remains anyone give suggestion thank much best xu update following kartikey advice tried find lr result seems e reasonable learning rate learning rate jpg tried run weighting result much changed update get baseline result collapse classification problem point scale binary one predict positive negative time accuracy increased around detailed description strategy however think still satisfactory accuracy slightly better random guess update following marcos lima suggestion made several additional step procedure remove number punctuation redundant space pre processed ktrain pkg thought ktrain pkg would sure use first last token text sample called head tail strategy task still binary classification positive v negative figure learning curve remains one posted still look different one posted marcos lima updated learning curve result probably best set result got note think maybe reason difficult pkg work well task task like combination classification sentiment analysis classical classification task news article classify category news belongs example biology economics sport word used different category pretty different hand classical example classifying sentiment analyse yelp imdb review guess text pretty straightforward expressing sentiment whereas text sample economic news kind polished well organized publication sentiment might always appear implicit way bert may able detect
Unsupervised Sentiment Analysis pycaret,"<p>Is it possible to conduct unsupervised sentiment analysis with Pycaret library if you have an unlabel dataset?</p>
<p>Any valid alternative and suggestion will be appreciated too</p>
",Text Classification / Sentiment Analysis,unsupervised sentiment analysis pycaret possible conduct unsupervised sentiment analysis pycaret library unlabel dataset valid alternative suggestion appreciated
Find subject in incomplete sentence with NLTK,"<p>I have a list of products that I am trying to classify into categories.  They will be described with incomplete sentences like:</p>

<p>""Solid State Drive Housing""</p>

<p>""Hard Drive Cable""</p>

<p>""1TB Hard Drive""</p>

<p>""500GB Hard Drive, Refurbished from Manufacturer""</p>

<p>How can I use python and NLP to get an output like ""Housing, Cable, Drive, Drive"", or a tree that describes which word is modifying which?
Thank you in advance</p>
",Text Classification / Sentiment Analysis,find subject incomplete sentence nltk list product trying classify category described incomplete sentence like solid state drive housing hard drive cable tb hard drive gb hard drive refurbished manufacturer use python nlp get output like housing cable drive drive tree describes word modifying thank advance
How do emoticons and hashags affect the accuracy of Google Natural Language Classify Text?,"<p>I have a project where I have to find categories of text using google natural language classify text. </p>

<p><a href=""https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/classifyText"" rel=""nofollow noreferrer"">https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/classifyText</a></p>

<p>My text documents will be having emoticons and hashtags. 
Can anyone tell me how these will affect the score? Should I get rid of them before calling the API or let them be?</p>

<p>I have tried multiple documents by myself and am getting conflicting results. Can anyone guide me with this?</p>
",Text Classification / Sentiment Analysis,emoticon hashags affect accuracy google natural language classify text project find category text using google natural language classify text text document emoticon hashtags anyone tell affect score get rid calling api let tried multiple document getting conflicting result anyone guide
Am I using word-embeddings correctly?,"<p><strong>Core question : Right way(s) of using word-embeddings to represent text ?</strong> </p>

<p>I am building sentiment classification application for tweets. Classify tweets as - negative, neutral and positive.
I am doing this using Keras on top of theano and using word-embeddings (google's word2vec or Stanfords GloVe).</p>

<p>To represent tweet text I have done as follows:</p>

<ol>
<li>used a pre-trained model (such as word2vec-twitter model) [<strong>M</strong>] to map words to their embeddings.</li>
<li>Use the words in the text to query <strong>M</strong> to get corresponding vectors. So if the tweet (<strong>T</strong>) is ""Hello world"" and <strong>M</strong> gives vectors <strong>V1</strong> and <strong>V2</strong> for the words 'Hello' and 'World'.</li>
<li>The tweet <strong>T</strong> can then be represented (<strong>V</strong>) as either <strong>V1</strong>+<strong>V2</strong> (add vectors) or <strong>V1V2</strong> (concatinate vectors)[These are 2 different strategies] [Concatenation means juxtaposition, so if <strong>V1, V2</strong> are d-dimension vectors, in my example <strong>T</strong> is 2d dimension vector]</li>
<li>Then, the tweet <strong>T</strong> is represented by vector <strong>V</strong>.</li>
</ol>

<p>If I follow the above, then My Dataset is nothing but vectors (which are sum or concatenation of word vectors depending on which strategy I use). 
I am training a deepnet such as FFN, LSTM on this dataset. But my results arent coming out to be great.</p>

<p>Is this the right way to use word-embeddings to represent text ? What are the other better ways ?</p>

<p>Your feedback/critique will be of immense help.</p>
",Text Classification / Sentiment Analysis,using word embeddings correctly core question right way using word embeddings represent text building sentiment classification application tweet classify tweet negative neutral positive using kera top theano using word embeddings google word vec stanford glove represent tweet text done follows used pre trained model word vec twitter model map word embeddings use word text query get corresponding vector tweet hello world give vector v v word hello world tweet represented v either v v add vector v v concatinate vector different strategy concatenation mean juxtaposition v v dimension vector example dimension vector tweet represented vector v follow dataset nothing vector sum concatenation word vector depending strategy use training deepnet ffn lstm dataset result arent coming great right way use word embeddings represent text better way feedback critique immense help
Predictions from model.predict() not correct,"<p>I am using model.predict to predict labels for unseen data in a sentence classification task. But the prediction is always 1 even if I give sentence of opposite label.</p>
<p>Here is the code</p>
<pre><code>from sklearn.preprocessing import LabelEncoder

maxlen = 1152  

### PREDICT NEW UNSEEN DATA ###
tokenizer = Tokenizer()
label_enc = LabelEncoder()
label_enc.fit(tar_list)
X_test = ['asdsadav dawd','this is boring', 'wow i like this you did a great job', 'ima cry tht was mean','1 nov 1968 george harrison became the first beatle to release a solo album in the u k with the soundtrack to ']

X_test = tokenizer.texts_to_sequences(X_test)
X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

a = (model.predict(X_test)&gt;0.5).astype(int).ravel()
print(a)

reverse_pred = label_enc.inverse_transform(a.ravel())
print(reverse_pred)
</code></pre>
<p>And here is the output</p>
<pre><code>[1 1 1 1 1]
[1 1 1 1 1]
</code></pre>
<p>The output of <code>model.predict(X_test)</code> is</p>
<pre><code>[[0.8882162]
 [0.8882162]
 [0.8882162]
 [0.8882162]
 [0.8882162]]
</code></pre>
<p>As I can see the first string means absolutely nothing. Still it has got a positive label. Also, the 2nd string is a negative one but it also has a positive label.</p>
",Text Classification / Sentiment Analysis,prediction model predict correct using model predict predict label unseen data sentence classification task prediction always even give sentence opposite label code output output see first string mean absolutely nothing still ha got positive label also nd string negative one also ha positive label
how to override downloader directory with local directory in polyglot in python,"<p>I run polyglot sentiment detection. When I upload it to the server I cannot run the <code>downloader.download(&quot;TASK:sentiment2&quot;)</code> command, so I downloaded the sentiment2 folder and saved it in the same folder as the python file.</p>
<p>I tried to set <code>downloader.download_dir = os.path.join(os.getcwd(),'polyglot_data')</code> pointing at the sentiment2 folder location as it says in the <a href=""https://polyglot.readthedocs.io/en/latest/polyglot.html#module-polyglot.downloader"" rel=""nofollow noreferrer"">polyglot documentation</a> but it doesnt work.</p>
<p><strong>How do I override downloader directory so it will access the sentiment2 local folder when it executes the sentiment analysis?</strong></p>
<p>Please see the full code below. This code works on my computer and localhost but returns zero when I run it on the server.</p>
<pre><code>from polyglot.downloader import downloader
#downloader.download(&quot;TASK:sentiment2&quot;)
from polyglot.text import Text

downloader.download_dir = os.path.join(os.getcwd(),'polyglot_data')

def get_text_sentiment(text):
   result = 0
   ttext = Text(text)
   for w in ttext.words:
      try:
         result += w.polarity
      except ValueError:
         pass
   if result: 
      return result/ len(ttext.words)
   else:
      return 0

text = &quot;he is feeling proud with ❤&quot;
print(get_text_sentiment(text))

</code></pre>
<p>my localhost returns - 0.1666</p>
<p>the server returns - 0.0</p>
",Text Classification / Sentiment Analysis,override downloader directory local directory polyglot python run polyglot sentiment detection upload server run command downloaded sentiment folder saved folder python file tried set pointing sentiment folder location say polyglot documentation doesnt work override downloader directory access sentiment local folder executes sentiment analysis please see full code code work computer localhost return zero run server localhost return server return
How to Do Topic Modelling and Classification on Each Sentence Comment in a Data Frame in R?,"<p>Is there a way to do topic modelling and classification on a data frame of comments in R?</p>
<p>I have 10 columns of comments (where each comment is a open ended sentence of a topic related to a question) and I want to classify each of these comments by topic for each column of comments.</p>
<p>I tried to use LDA (Latent Dirichlet Allocation) using the topicmodels package in R (and use DocumentTermMatrix and Corpus before I applied the LDA model). I tried to find the optimal number of topics using the lowest perplexity.</p>
<p>The issue is that I don't know what topic each sentence of a comment is classified by. It does put words into a similar topic but not by sentence. So it's a little confusing.</p>
<p>I don't know where to go from there and need advice on how to do this.</p>
<p>I was able to apply Sentimental Analysis on the same data frame in R using the sentimentr package and it worked but I can't do the same for topic modelling and classification.</p>
<p>How can I do this in R for each sentence of comment in a column (for a total of 10 columns)?</p>
<p><img src=""https://i.sstatic.net/fsAHJ.png"" alt=""image of data"" /></p>
",Text Classification / Sentiment Analysis,topic modelling classification sentence comment data frame r way topic modelling classification data frame comment r column comment comment open ended sentence topic related question want classify comment topic column comment tried use lda latent dirichlet allocation using topicmodels package r use documenttermmatrix corpus applied lda model tried find optimal number topic using lowest perplexity issue know topic sentence comment classified doe put word similar topic sentence little confusing know go need advice wa able apply sentimental analysis data frame r using sentimentr package worked topic modelling classification r sentence comment column total column
How to proceed after annotating text data for ML?,"<p>I am currently working on a project where I want to classify some text. For that, I first had to annotate text data. I did it using a web tool and have now the corresponding json file (containing the annotations) and the plain txt files (containing the raw text).
I now want to use different classifiers to train the data and eventually predict the desired outcome.</p>
<p>However, I am struggling with where to start. I haven't really found what I've been looking for in the internet so that's why I try it here.</p>
<p>How would I proceed with the json and txt. files? As far as I understood I'd have to somehow convert these info to a .csv where I have information about the labels, the text but also &quot;none&quot; for thext that has not been annotated. So I guess that's why I use the .txt files to somehow merge them with the annotations files and being able to detect if a text sentence (or word) has a label or not. And then I could use the .csv data to load it into the model.</p>
<p>Could someone give me a hint on where to start or how I should proceed now?
Everything I've found so far is covering the case that data is already converted and ready to preprocess but I am struggling with what to do with the results from the annotation process.</p>
<p>My JSON looks something like that:</p>
<pre><code>{&quot;annotatable&quot;:{&quot;parts&quot;:[&quot;s1p1&quot;]},
 &quot;anncomplete&quot;:true,
 &quot;sources&quot;:[],
 &quot;metas&quot;:{},
 &quot;entities&quot;:[{&quot;classId&quot;:&quot;e_1&quot;,&quot;part&quot;:&quot;s1p1&quot;,&quot;offsets&quot;: 
 [{&quot;start&quot;:11,&quot;text&quot;:&quot;This is the text&quot;}],&quot;coordinates&quot;:[],&quot;confidence&quot;: 
 {&quot;state&quot;:&quot;pre-added&quot;,&quot;who&quot;:[&quot;user:1&quot;],&quot;prob&quot;:1},&quot;fields&quot;:{&quot;f_4&quot;: 
 {&quot;value&quot;:&quot;3&quot;,&quot;confidence&quot;:{&quot;state&quot;:&quot;pre-added&quot;,&quot;who&quot;: 
 [&quot;user:1&quot;],&quot;prob&quot;:1}}},&quot;normalizations&quot;:{}},&quot;normalizations&quot;:{}}],
 &quot;relations&quot;:[]}
</code></pre>
<p>Each text is given a <code>classId</code> (<code>e_1</code> in this case) and a <code>field_value</code> (<code>f_4</code> given the value <code>3</code> in this case). I'd need to extract it step by step. First extracting the entity with the corresponding text (and adding &quot;none&quot; to where no annotation has been annotated) and in a second step retrieving the field information with the corresponding text.
The corresponding .txt file is just simply like that:
This is the text</p>
<p>I have all .json files in one folder and all .txt in another.</p>
",Text Classification / Sentiment Analysis,proceed annotating text data ml currently working project want classify text first annotate text data using web tool corresponding json file containing annotation plain txt file containing raw text want use different classifier train data eventually predict desired outcome however struggling start really found looking internet try would proceed json txt file far understood somehow convert info csv information label text also none thext ha annotated guess use txt file somehow merge annotation file able detect text sentence word ha label could use csv data load model could someone give hint start proceed everything found far covering case data already converted ready preprocess struggling result annotation process json look something like text given case given value case need extract step step first extracting entity corresponding text adding none annotation ha annotated second step retrieving field information corresponding text corresponding txt file simply like text json file one folder txt another
How can I transform user Input exactly as processed data that I used to train the classifier? I want to perform sentiment analysis on user input text,"<p>I want to perform sentiment analysis using NLTK in python on a sentence that is an input from the user as either voice input or text, but I cannot understand how to perform sentiment analysis. I have searched but most of the search results show movie reviews data or twitter sentiment analysis. Kindly help.
The code I used to Train the classifier is as:</p>
<pre><code>import nltk
import random
from nltk.corpus import movie_reviews
import pickle

documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

# print(len(documents))

all_words = []

for word in movie_reviews.words():
    all_words.append(word.lower())

# print(len(all_words))
all_words = nltk.FreqDist(all_words)
# print(&quot;First 20 Most Repeated Words are: &quot;, all_words.most_common(20))
# word_in = input(&quot;Enter a word to find out how many times it appears in the reveiws:   &quot;)
# print(f&quot;Number of Times the Word {0} Appeared : &quot;.format(word_in), all_words[word_in])

word_features = list(all_words.keys())[:3000]




def find_features(document):
    words = set(document)
    featurs = {}
    for w in word_features:
        featurs[w] = (w in words)
    return featurs

# print(find_features(movie_reviews.words('neg/cv000_29416.txt')))
featuresets = [(find_features(rev), category) for (rev, category) in documents]

train_set = featuresets[:1900]

test_set = featuresets[1900:]

# print(test_set[0])


classifier = nltk.NaiveBayesClassifier.train(train_set)

print(&quot;Classifier Accuracy: &quot;, (nltk.classify.accuracy(classifier, test_set))*100
</code></pre>
<p>Instead of <code>test_set</code> i want to use my own sentence</p>
",Text Classification / Sentiment Analysis,transform user input exactly processed data used train classifier want perform sentiment analysis user input text want perform sentiment analysis using nltk python sentence input user either voice input text understand perform sentiment analysis searched search result show movie review data twitter sentiment analysis kindly help code used train classifier instead want use sentence
Is there an Amazon AWS service we can leverage to detect correlation between two texts?,"<p>We're a database of Q&amp;A from customers (much like StackOverflow :-P) and we're working on a bot to automatically identify the most likely response to a ticket being opened. e.g.</p>
<pre><code>N;Q;A
1;how to recover my password?;Please go to https://.../resetPWD
2;which are the office hours?;office is open 9-13 and 14-18
</code></pre>
<p>Basically we want to create suggestions for customer care operators pointing them to known answers.</p>
<p>We're using Amazon AWS as a platform of choice and we'd like to offload this to existing APIs (rather then resorting to deploying a dedicated NPL solution).</p>
<p><strong>To be clear</strong> we don't want to write/deploy any code, we just want to use AWS APIs/services</p>
<p>Is there any service which will automatically detect/classify the &quot;most similar request&quot; and allow us to provide suggestions? e.g. some kind of text correlation APIs?</p>
<p>e.g. something like</p>
<pre><code>input: &quot;I lost my password and need to recover it!&quot;
response: &quot;1;how to recover my password?;Please go to https://.../resetPWD&quot;
</code></pre>
",Text Classification / Sentiment Analysis,amazon aws service leverage detect correlation two text database q customer much like stackoverflow p working bot automatically identify likely response ticket opened e g basically want create suggestion customer care operator pointing known answer using amazon aws platform choice like offload existing apis rather resorting deploying dedicated npl solution clear want write deploy code want use aws apis service service automatically detect classify similar request allow u provide suggestion e g kind text correlation apis e g something like
Google cloud NLP integration with UiPath,"<p>I am trying to integrate the google NLP (text Classification) with UiPath. But I am facing some problems, please let me know how can I do that.</p>
",Text Classification / Sentiment Analysis,google cloud nlp integration uipath trying integrate google nlp text classification uipath facing problem please let know
BERT sentence classification,"<p>I am using the the Python BERT models: <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a></p>

<p>My goal is to build a binary classification model to predict if a news headline is relevant to a specific category. I have a training set of data which has news headline sentences as well as binary values to indicate if the headline is valid or invalid.</p>

<p>I tried to run the run_classifier.py script and the results I obtained do not seem to make sense. The test results file has two columns with the same two numbers being repeated on each row :
<a href=""https://i.sstatic.net/rGR6X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rGR6X.png"" alt=""enter image description here""></a> </p>

<p>Also in the model parameters for task_name I have it set as: cola, after reading the academic paper for BERT <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1810.04805.pdf</a> I feel as if this is not an appropriate task name. The paper lists several other tasks on pages 14 and 15 but none of them seem to be appropriate for the binary categorization of sentences based on content.</p>

<p>How can I properly use BERT to classify sentences? I tried using <a href=""https://appliedmachinelearning.blog/2019/03/04/state-of-the-art-text-classification-using-bert-model-predict-the-happiness-hackerearth-challenge/"" rel=""nofollow noreferrer"">this guide</a>. 
But it did not yield the results I had expected. </p>
",Text Classification / Sentiment Analysis,bert sentence classification using python bert model goal build binary classification model predict news headline relevant specific category training set data ha news headline sentence well binary value indicate headline valid invalid tried run run classifier py script result obtained seem make sense test result file ha two column two number repeated row also model parameter task name set cola reading academic paper bert feel appropriate task name paper list several task page none seem appropriate binary categorization sentence based content properly use bert classify sentence tried using guide yield result expected
How to make BERT model converge?,"<p>I am trying to use BERT for sentiment analysis but I suspect I am doing something wrong. In my code I am fine tuning bert using <code>bert-for-tf2</code> but after 1 epoch I am getting an accuracy of 42% when a simple GRU model was getting around 73% accuracy. What should I be doing different to effectively use BERT. I suspect I am traning the bert layers from the first batch which may be an issue as the dense layer is randomly initialized. Any advice would be appreciated, Thanks!</p>

<pre><code>import bert-for-tf2 #gets imported as bert but relabeled for clarity
model_name = ""uncased_L-12_H-768_A-12""
model_dir = bert.fetch_google_bert_model(model_name, "".models"")
model_ckpt = os.path.join(model_dir, ""bert_model.ckpt"")

bert_params = bert.params_from_pretrained_ckpt(model_dir)
l_bert = bert.BertModelLayer.from_params(bert_params, name=""bert"")
max_seq_len = 100
l_input_ids = tensorflow.keras.layers.Input(shape=(max_seq_len,), dtype='int32')

bertLayer = l_bert(l_input_ids)
flat = Flatten()(bertLayer)
output = Dense(1,activation = 'sigmoid')(flat)
model = tensorflow.keras.Model(inputs=l_input_ids, outputs=output)
model.build(input_shape=(None, max_seq_len))

bert.load_bert_weights(l_bert, model_ckpt) 

with open('../preprocessing_scripts/new_train_data.txt', 'r') as f:
  tweets = f.readlines()

with open('../preprocessing_scripts/targets.csv', 'r') as f:
  targets = f.readlines()

max_words = 14000
tokenizer = Tokenizer(num_words=max_words)

trainX = tweets[:6000]
trainY = targets[:6000]
testX = tweets[6000:]
testY = tweets[6000:]
maxlen = 100
tokenizer.fit_on_texts(trainX)

tokenized_version = tokenizer.texts_to_sequences(trainX)

tokenized_version = pad_sequences(tokenized_version, maxlen=maxlen)trainY = np.array(trainY,dtype = 'int32')
model.compile(loss=""binary_crossentropy"",
              optimizer=""adam"",
              metrics=['accuracy']) 

history = model.fit(x=tokenized_version, y=trainY, batch_size = 32, epochs=1, validation_split = 0.2)
</code></pre>
",Text Classification / Sentiment Analysis,make bert model converge trying use bert sentiment analysis suspect something wrong code fine tuning bert using epoch getting accuracy simple gru model wa getting around accuracy different effectively use bert suspect traning bert layer first batch may issue dense layer randomly initialized advice would appreciated thanks
text classification using graphs in natural language processing,"<p>I tried to search but couldn't find much helpful information regarding this topic. That's why I am asking it here...</p>

<p>I know there are various methods to classify texts (like Logistic regression etc.) and also we have neural network. </p>

<p>But, I was wondering if it is possible to 'classify the texts into multiple classes' using graph theory?
If yes, how should I proceed? Please guide me.</p>

<p>Example:</p>

<p>I like jeansp                    -pos</p>

<p>I like toyota                    -pos</p>

<p>I it so-so place                 -neutral</p>

<p>I hated that trip                -neg</p>

<p>I love that shirt                -pos</p>

<p>that place was horrible          -neg</p>

<p>I liked food but service was bad -neutral</p>
",Text Classification / Sentiment Analysis,text classification using graph natural language processing tried search find much helpful information regarding topic asking know various method classify text like logistic regression etc also neural network wa wondering possible classify text multiple class using graph theory yes proceed please guide example like jeansp po like toyota po place neutral hated trip neg love shirt po place wa horrible neg liked food service wa bad neutral
Difference between model accuracy from test data and confusion matrix accuracy,"<ul>
<li>I am working for NLP project where I wanted to do text classification
using neural n/w</li>
<li>I am getting very nice accuracy from the test set as 98%.</li>
<li>But, when I tried to check the confusion matrix accuracy (the accuracy score using confusion matrix) it's just 52%.</li>
<li>How is it possible? What am I missing here? </li>
</ul>

<h1>Question</h1>

<p>What is the difference between both the accuracy's which one should be considered as the actual accuracy? and why?</p>

<h1>Code on test set</h1>

<pre><code>loss, acc = model.evaluate(Xtest, y_test_array)
</code></pre>
",Text Classification / Sentiment Analysis,difference model accuracy test data confusion matrix accuracy working nlp project wanted text classification using neural n w getting nice accuracy test set tried check confusion matrix accuracy accuracy score using confusion matrix possible missing question difference accuracy one considered actual accuracy code test set
How to create a customized trade/law lexicon for r text analysis,"<p>I am planning to do text analysis in R just as sentiment analysis with an own custom dictionary following a ""trade"" versus ""law"" logic.</p>

<p>I have all the required words for the dictionary in an excel file. Looks like this:</p>

<pre><code>&gt; %  1 Trade 2 Law % business   1 exchange  1 industry  1 rule  2
&gt; settlement    2 umpire    2 court 2 tribunal  2 lawsuit   2 bench 2
&gt; courthouse    2 courtroom 2
</code></pre>

<p>What steps do I have to pursue in order to transform this in an R-suitable format and apply it to my text corpus?</p>

<p>Thank you for your help!</p>
",Text Classification / Sentiment Analysis,create customized trade law lexicon r text analysis planning text analysis r sentiment analysis custom dictionary following trade versus law logic required word dictionary excel file look like step pursue order transform r suitable format apply text corpus thank help
"Why do Dense layers perform better than a mix of Conv Layers, Recurrent Layers on Sentiment Analysis with BERT emebddings?","<p>I have used BERT to make embeddings out of the imdb review dataset and I am trying out some models to check their perfomance on sentiment analysis (0 for the bad reviews and 1 for the good ones). I have seen that models with just dense Layers do work better than models which are a mixture of Convolutional with MaxPooling Layers and recurrent units such as LSTM or BiLSTM.I want to know why is this happening since BERT do captures of semantics and time dependencies so second models should perform better. I am speaking about orders from 85% accuracy with dense layers to 75% with the mix of Layers. Thanks in adavance</p>
",Text Classification / Sentiment Analysis,dense layer perform better mix conv layer recurrent layer sentiment analysis bert emebddings used bert make embeddings imdb review dataset trying model check perfomance sentiment analysis bad review good one seen model dense layer work better model mixture convolutional maxpooling layer recurrent unit lstm bilstm want know happening since bert capture semantics time dependency second model perform better speaking order accuracy dense layer mix layer thanks adavance
Sorting Vader Sentiment Analysis Results in Dictionary,"<p>Is there a commonly used method for sorting multiple Vader Sentiment Analysis Results in Dictionary</p>

<p><strong>I am trying to sort by 'compound' Vader Sentiment Analysis Results in the review Dictionary.</strong></p>

<p>I just started learning nlp and Sentiment Analysis and got my first project to 95% so far been learning books, tutorials and here.</p>

<p>I was hoping to <strong>sort closest scores 5 overall sentiments scores</strong> to a particular one in a large set of multiple dictionary results.</p>

<p><strong>this what i tried</strong>
Might be also due to printing it being a string I assume i need to convert this to string or object, just not sure next steps.I was think I might need to loop through dictionary or convert the dictionary. Any pointers would be greatly appreciated.</p>

<pre><code>newlist = sorted(review, key=lambda k: k['compound']) 

</code></pre>

<p><a href=""https://i.sstatic.net/N8gYb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N8gYb.png"" alt=""Error screenshot""></a></p>

<p><strong>I also tried</strong></p>

<pre><code>from operator import itemgetter
newlist = sorted(review, key=itemgetter('compound'))
</code></pre>

<p><a href=""https://i.sstatic.net/UQeI5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UQeI5.png"" alt=""enter image description here""></a></p>

<p><strong>The Vader results are printed from dictionary.</strong></p>

<pre><code>s = pprint.pformat(review)
print(s)
</code></pre>

<p><strong>This is the format of the results which i think is standard output</strong></p>

<pre><code>
 'america-reviews': ""Overall sentiment dictionary is : {'neg': 0.051, 'neu': ""
                    ""0.632, 'pos': 0.316, 'compound': 1.0}, sentence was rated ""
                    'as 5.1% Negative, sentence was rated as 63.2% Neutral, '
                    'sentence was rated as 31.6% Positive, Sentence Overall '
                    'Rated As Positive',
 'amygrant-reviews': ""Overall sentiment dictionary is : {'neg': 0.022, 'neu': ""
                     ""0.734, 'pos': 0.244, 'compound': 0.9998}, sentence was ""
                     'rated as 2.1999999999999997% Negative, sentence was '
                     'rated as 73.4% Neutral, sentence was rated as 24.4% '
                     'Positive, Sentence Overall Rated As Positive',
 'andygarcia-reviews': ""Overall sentiment dictionary is : {'neg': 0.0, 'neu': ""
                       ""0.955, 'pos': 0.045, 'compound': 0.8419}, sentence was ""
                       'rated as 0.0% Negative, sentence was rated as 95.5% '
                       'Neutral, sentence was rated as 4.5% Positive, Sentence '
                       'Overall Rated As Positive',
 'annemurray-reviews': ""Overall sentiment dictionary is : {'neg': 0.02, 'neu': ""
                       ""0.769, 'pos': 0.211, 'compound': 0.9986}, sentence was ""
                       'rated as 2.0% Negative, sentence was rated as 76.9% '
                       'Neutral, sentence was rated as 21.099999999999998% '
                       'Positive, Sentence Overall Rated As Positive',
 'annielennox-reviews': ""Overall sentiment dictionary is : {'neg': 0.03, ""
                        ""'neu': 0.717, 'pos': 0.254, 'compound': 0.9999}, ""
                        'sentence was rated as 3.0% Negative, sentence was '
                        'rated as 71.7% Neutral, sentence was rated as 25.4% '
                        'Positive, Sentence Overall Rated As Positive',
 'artgarfunkel-reviews': ""Overall sentiment dictionary is : {'neg': 0.056, ""
                         ""'neu': 0.642, 'pos': 0.302, 'compound': 1.0}, ""
                         'sentence was rated as 5.6000000000000005% Negative, '
                         'sentence was rated as 64.2% Neutral, sentence was '
                         'rated as 30.2% Positive, Sentence Overall Rated As '
                         'Positive',
 'bangles-reviews': ""Overall sentiment dictionary is : {'neg': 0.054, 'neu': ""
                    ""0.733, 'pos': 0.213, 'compound': 0.9998}, sentence was ""
                    'rated as 5.4% Negative, sentence was rated as 73.3% '
                    'Neutral, sentence was rated as 21.3% Positive, Sentence '
                    'Overall Rated As Positive',
 'barbrastriesand-reviews': ""Overall sentiment dictionary is : {'neg': 0.014, ""
                            ""'neu': 0.815, 'pos': 0.171, 'compound': 0.9982}, ""
                            'sentence was rated as 1.4000000000000001% '
                            'Negative, sentence was rated as 81.5% Neutral, '
                            'sentence was rated as 17.1% Positive, Sentence '
                            'Overall Rated As Positive',
 'barrymanilow-reviews': ""Overall sentiment dictionary is : {'neg': 0.041, ""
                         ""'neu': 0.647, 'pos': 0.313, 'compound': 1.0}, ""
                         'sentence was rated as 4.1000000000000005% Negative, '
                         'sentence was rated as 64.7% Neutral, sentence was '
                         'rated as 31.3% Positive, Sentence Overall Rated As '
                         'Positive',
 'beachboys-reviews': ""Overall sentiment dictionary is : {'neg': 0.016, 'neu': ""
                      ""0.906, 'pos': 0.078, 'compound': 0.945}, sentence was ""
                      'rated as 1.6% Negative, sentence was rated as '
                      '90.60000000000001% Neutral, sentence was rated as 7.8% '
                      'Positive, Sentence Overall Rated As Positive',
 'belindacarlisle-reviews': ""Overall sentiment dictionary is : {'neg': 0.046, ""
                            ""'neu': 0.756, 'pos': 0.197, 'compound': 0.9987}, ""
                            'sentence was rated as 4.6% Negative, sentence was '
                            'rated as 75.6% Neutral, sentence was rated as '
                            '19.7% Positive, Sentence Overall Rated As '
                            'Positive',
 'bernadettepeters-reviews': ""Overall sentiment dictionary is : {'neg': 0.02, ""
                             ""'neu': 0.753, 'pos': 0.227, 'compound': 0.9992}, ""
                             'sentence was rated as 2.0% Negative, sentence '
                             'was rated as 75.3% Neutral, sentence was rated '
                             'as 22.7% Positive, Sentence Overall Rated As '
                             'Positive',
 'bethhart-reviews': ""Overall sentiment dictionary is : {'neg': 0.041, 'neu': ""
                     ""0.592, 'pos': 0.366, 'compound': 1.0}, sentence was ""
                     'rated as 4.1000000000000005% Negative, sentence was '
                     'rated as 59.199999999999996% Neutral, sentence was rated '
                     'as 36.6% Positive, Sentence Overall Rated As Positive',
 'bettemidler-reviews': ""Overall sentiment dictionary is : {'neg': 0.043, ""
                        ""'neu': 0.635, 'pos': 0.322, 'compound': 0.9999}, ""
                        'sentence was rated as 4.3% Negative, sentence was '
                        'rated as 63.5% Neutral, sentence was rated as 32.2% '
                        'Positive, Sentence Overall Rated As Positive',
 'bjork-reviews': ""Overall sentiment dictionary is : {'neg': 0.042, 'neu': ""
                  ""0.696, 'pos': 0.262, 'compound': 1.0}, sentence was rated ""
                  'as 4.2% Negative, sentence was rated as 69.6% Neutral, '
                  'sentence was rated as 26.200000000000003% Positive, '
                  'Sentence Overall Rated As Positive',
 'bluemangroup-reviews': ""Overall sentiment dictionary is : {'neg': 0.047, ""
                         ""'neu': 0.726, 'pos': 0.227, 'compound': 0.9999}, ""
                         'sentence was rated as 4.7% Negative, sentence was '
                         'rated as 72.6% Neutral, sentence was rated as 22.7% '
                         'Positive, Sentence Overall Rated As Positive',
 'bluetravelers-reviews': ""Overall sentiment dictionary is : {'neg': 0.0, ""
                          ""'neu': 0.914, 'pos': 0.086, 'compound': 0.9455}, ""
                          'sentence was rated as 0.0% Negative, sentence was '
                          'rated as 91.4% Neutral, sentence was rated as 8.6% '
                          'Positive, Sentence Overall Rated As Positive',
 'bobbyvinton-reviews': ""Overall sentiment dictionary is : {'neg': 0.0, 'neu': ""
                        ""0.928, 'pos': 0.072, 'compound': 0.9501}, sentence ""
                        'was rated as 0.0% Negative, sentence was rated as '
                        '92.80000000000001% Neutral, sentence was rated as '
                        '7.199999999999999% Positive, Sentence Overall Rated '
                        'As Positive',
 'bonnieRatt-reviews': ""Overall sentiment dictionary is : {'neg': 0.034, ""
                       ""'neu': 0.612, 'pos': 0.354, 'compound': 0.9999}, ""
                       'sentence was rated as 3.4000000000000004% Negative, '
                       'sentence was rated as 61.199999999999996% Neutral, '
                       'sentence was rated as 35.4% Positive, Sentence Overall '
                       'Rated As Positive',
 'boygeorge-reviews': ""Overall sentiment dictionary is : {'neg': 0.039, 'neu': ""
                      ""0.884, 'pos': 0.076, 'compound': 0.9217}, sentence was ""
                      'rated as 3.9% Negative, sentence was rated as 88.4% '
                      'Neutral, sentence was rated as 7.6% Positive, Sentence '
                      'Overall Rated As Positive',
 'brianlittrell-reviews': ""Overall sentiment dictionary is : {'neg': 0.052, ""
                          ""'neu': 0.873, 'pos': 0.074, 'compound': 0.8203}, ""
                          'sentence was rated as 5.2% Negative, sentence was '
                          'rated as 87.3% Neutral, sentence was rated as '
                          '7.3999999999999995% Positive, Sentence Overall '
                          'Rated As Positive',
 'briansetzerorchestra-reviews': ""Overall sentiment dictionary is : {'neg': ""
                                 ""0.046, 'neu': 0.646, 'pos': 0.308, ""
                                 ""'compound': 1.0}, sentence was rated as 4.6% ""
                                 'Negative, sentence was rated as '
                                 '64.60000000000001% Neutral, sentence was '
                                 'rated as 30.8% Positive, Sentence Overall '
                                 'Rated As Positive',
 'brianwilson-reviews': ""Overall sentiment dictionary is : {'neg': 0.042, ""
                        ""'neu': 0.647, 'pos': 0.311, 'compound': 1.0}, ""
                        'sentence was rated as 4.2% Negative, sentence was '
                        'rated as 64.7% Neutral, sentence was rated as 31.1% '
                        'Positive, Sentence Overall Rated As Positive',
 'brucehornsby-reviews': ""Overall sentiment dictionary is : {'neg': 0.0, ""
                         ""'neu': 0.928, 'pos': 0.072, 'compound': 0.9196}, ""
                         'sentence was rated as 0.0% Negative, sentence was '
                         'rated as 92.80000000000001% Neutral, sentence was '
                         'rated as 7.199999999999999% Positive, Sentence '
                         'Overall Rated As Positive',
 'bryanadams-reviews': ""Overall sentiment dictionary is : {'neg': 0.008, ""
                       ""'neu': 0.933, 'pos': 0.059, 'compound': 0.9028}, ""
                       'sentence was rated as 0.8% Negative, sentence was '
                       'rated as 93.30000000000001% Neutral, sentence was '
                       'rated as 5.8999999999999995% Positive, Sentence '
                       'Overall Rated As Positive',

</code></pre>
",Text Classification / Sentiment Analysis,sorting vader sentiment analysis result dictionary commonly used method sorting multiple vader sentiment analysis result dictionary trying sort compound vader sentiment analysis result review dictionary started learning nlp sentiment analysis got first project far learning book tutorial wa hoping sort closest score overall sentiment score particular one large set multiple dictionary result tried might also due printing string assume need convert string object sure next step wa think might need loop dictionary convert dictionary pointer would greatly appreciated also tried vader result printed dictionary format result think standard output
Classify slander content,"<p>I am making an automation application that can collect facebook content, analyze the content to detect the slander comments. Is there any ruby tool to do this such kind of detection? I just found some tool to do sentiment analysis but It is not what I intended. </p>
",Text Classification / Sentiment Analysis,classify slander content making automation application collect facebook content analyze content detect slander comment ruby tool kind detection found tool sentiment analysis intended
Keyword based text classification,"<p>I want to classify some texts based on available keywords in each class. In other words, I have a list of keywords for each category. I need some heuristic methods using these keywords and determine top similar categories for each text. I should say that in the current phase of the project, I didn't want to use a machine learning-based method for text classification.</p>
",Text Classification / Sentiment Analysis,keyword based text classification want classify text based available keywords class word list keywords category need heuristic method using keywords determine top similar category text say current phase project want use machine learning based method text classification
Extracting Key-Phrases from text based on the Topic with Python,"<p>I have a large dataset with 3 columns, columns are text, phrase and topic. 
I want to find a way to extract key-phrases (phrases column) based on the topic.
Key-Phrase can be part of the text value or the whole text value.</p>

<pre><code>import pandas as pd


text = [""great game with a lot of amazing goals from both teams"",
        ""goalkeepers from both teams made misteke"",
        ""he won all four grand slam championchips"",
        ""the best player from three-point line"",
        ""Novak Djokovic is the best player of all time"",
        ""amazing slam dunks from the best players"",
        ""he deserved yellow-card for this foul"",
        ""free throw points""]

phrase = [""goals"", ""goalkeepers"", ""grand slam championchips"", ""three-point line"", ""Novak Djokovic"", ""slam dunks"", ""yellow-card"", ""free throw points""]

topic = [""football"", ""football"", ""tennis"", ""basketball"", ""tennis"", ""basketball"", ""football"", ""basketball""]

df = pd.DataFrame({""text"":text,
                   ""phrase"":phrase,
                   ""topic"":topic})

print(df.text)
print(df.phrase)
</code></pre>

<p>I'm having big trouble with finding a path to do something like this, because I have more than 50000 rows in my dataset and around 48000 of unique values of phrases, and 3 different topics.</p>

<p>I guess that building a dataset with all football, basketball and tennis topics are not really the best solution. So I was thinking about making some kind of ML model for this, but again that  means that I will have 2 features (text and topic) and one result (phrase), but I will have more than 48000 of different classes in my result, and that is not a good approach.</p>

<p>I was thinking about using text column as a feature and applying classification model in order to find sentiment. After that I can use predicted sentiment to extract key features, but I do not know how to extract them. </p>

<p>One more problem is that I get only 66% accuracy when I try to classify sentiment by using <code>CountVectorizer</code> or <code>TfidfTransformer</code> with Random Forest, Decision Tree, or any other classifying algorithm, and also 66% of accuracy if Im using <code>TextBlob</code> for sentiment analysis.</p>

<p>Any help?</p>
",Text Classification / Sentiment Analysis,extracting key phrase text based topic python large dataset column column text phrase topic want find way extract key phrase phrase column based topic key phrase part text value whole text value big trouble finding path something like row dataset around unique value phrase different topic guess building dataset football basketball tennis topic really best solution wa thinking making kind ml model mean feature text topic one result phrase different class result good approach wa thinking using text column feature applying classification model order find sentiment use predicted sentiment extract key feature know extract one problem get accuracy try classify sentiment using random forest decision tree classifying algorithm also accuracy im using sentiment analysis help
tokenizer.texts_to_sequences Keras Tokenizer gives almost all zeros,"<p>I am working to create a text classification code but I having problems in encoding documents using the tokenizer. </p>

<p>1) I started by fitting a tokenizer on my document as in here: </p>

<pre><code>vocabulary_size = 20000
tokenizer = Tokenizer(num_words= vocabulary_size, filters='')
tokenizer.fit_on_texts(df['data'])
</code></pre>

<p>2) Then I wanted to check if my data is fitted correctly so I converted into sequence as in here: </p>

<pre><code>sequences = tokenizer.texts_to_sequences(df['data'])
data = pad_sequences(sequences, maxlen= num_words) 
print(data) 
</code></pre>

<p>which gave me fine output. i.e. encoded words into numbers </p>

<pre><code>[[ 9628  1743    29 ...   161    52   250]
 [14948     1    70 ...    31   108    78]
 [ 2207  1071   155 ... 37607 37608   215]
 ...
 [  145    74   947 ...     1    76    21]
 [   95 11045  1244 ...   693   693   144]
 [   11   133    61 ...    87    57    24]]
</code></pre>

<p>Now, I wanted to convert a text into a sequence using the same method. 
Like this: </p>

<pre><code>sequences = tokenizer.texts_to_sequences(""physics is nice "")
text = pad_sequences(sequences, maxlen=num_words)
print(text)
</code></pre>

<p>it gave me weird output: </p>

<pre><code>[[   0    0    0    0    0    0    0    0    0  394]
 [   0    0    0    0    0    0    0    0    0 3136]
 [   0    0    0    0    0    0    0    0    0 1383]
 [   0    0    0    0    0    0    0    0    0  507]
 [   0    0    0    0    0    0    0    0    0    1]
 [   0    0    0    0    0    0    0    0    0 1261]
 [   0    0    0    0    0    0    0    0    0    0]
 [   0    0    0    0    0    0    0    0    0 1114]
 [   0    0    0    0    0    0    0    0    0    1]
 [   0    0    0    0    0    0    0    0    0 1261]
 [   0    0    0    0    0    0    0    0    0  753]]
</code></pre>

<p>According to Keras documentation (<a href=""https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/"" rel=""noreferrer"">Keras</a>): </p>

<blockquote>
  <p>texts_to_sequences(texts)</p>
  
  <p>Arguments: texts: list of texts to turn to sequences. </p>
  
  <p>Return: list of
  sequences (one per text input).</p>
</blockquote>

<p>is it not supposed to encode each word to its corresponding number? then pad the text if it shorter than 50 to 50? 
Where is the mistake ? </p>
",Text Classification / Sentiment Analysis,tokenizer text sequence kera tokenizer give almost zero working create text classification code problem encoding document using tokenizer started fitting tokenizer document wanted check data fitted correctly converted sequence gave fine output e encoded word number wanted convert text sequence using method like gave weird output according kera documentation kera text sequence text argument text list text turn sequence return list sequence one per text input supposed encode word corresponding number pad text shorter mistake
Classify web pages with neural net?,"<p>So I am working on a java project and I'm have a webcrawler that retrieves and downloads the html from websites it finds. You input a word, such as the word Java, and my program then searches google for websites and downloads the html data.</p>

<p>So now I am trying to use a Neural Net to predict if the website is what I am looking for, yes I am aware that I am going to need a lot of data and have to manually feed the network. Where I am stuck is with converting the html into numbers for the neural net.</p>

<p>At first I though about just converting the html into plain text and then assign a number to each letter, and just have a document that is made up of a bunch of numbers that make up words. I am unsure though how a neural net would view this, would all these numbers be just one input or would each word be an input to the neural net, which then would cause the problem of having an unspecified number of inputs, and if the net allows this as one input would this be a viable way of classifying if a website is what I want or not?</p>
",Text Classification / Sentiment Analysis,classify web page neural net working java project webcrawler retrieves downloads html website find input word word java program search google website downloads html data trying use neural net predict website looking yes aware going need lot data manually feed network stuck converting html number neural net first though converting html plain text assign number letter document made bunch number make word unsure though neural net would view would number one input would word input neural net would cause problem unspecified number input net allows one input would viable way classifying website want
Obtaining word polarity in each review,"<p>I'm working on a domain-specific sentiment analysis, and I want to get each independent word polarity in that specific corpus (not a general score like ""SentiWordNet"" or other lexicons).</p>

<p>At first I thought using the following formula would help:</p>

<pre class=""lang-none prettyprint-override""><code>positive_word_polarity = # word occurrence in positive reviews / # all words in pos and neg reviews    
</code></pre>

<pre class=""lang-none prettyprint-override""><code>negative_word_polarity = # word occurrence in negative reviews / # all words in pos and neg reviews    
</code></pre>

<p>but then I found some issues regarding to this solution:</p>

<ol>
<li>We have ""good"" in positive review and negative review ""negative review"".</li>
<li>There might be some words with a lot of occurrences but lower effect or vice versa.</li>
</ol>

<p>So basically my inputs are reviews and their polarities and I need a lexicon containing words and their polarities.</p>
",Text Classification / Sentiment Analysis,obtaining word polarity review working domain specific sentiment analysis want get independent word polarity specific corpus general score like sentiwordnet lexicon first thought using following formula would help found issue regarding solution good positive review negative review negative review might word lot occurrence lower effect vice versa basically input review polarity need lexicon containing word polarity
How do you fetch JSON from Google&#39;s Cloud Natural Language API based on existing text using client-side JS?,"<p>Does anyone know how to retrieve data from Google's Cloud Natural Language (NL) API using client-side javascript? What is incorrect with the code block below - likely line 29?</p>

<p>I've been looking at a lot of documentation today to figure out how to perform sentiment analysis using Google's Cloud NL API. Many of them have not been very clear.</p>

<p>This <a href=""https://rominirani.com/tutorial-analyzing-reviews-using-google-sheets-and-cloud-natural-language-api-240ec8f3090c"" rel=""nofollow noreferrer"">tutorial</a> helped me understand how Google's Cloud NL API works to a large degree.</p>

<p>I still have a few gaps.</p>

<p>This is the block of code I derived after looking at a few docs.</p>

<p><strong>One area I think that may be a potential issue, line 29.</strong>
I've never fetched using two arguments.</p>

<pre><code>1  sentiApiKey = ***API KEY***
2  sentiEndPoint = ***Google's Cloud Natural Language endpoint***
3  //dictData is existing JSON data

5  function searchSenti(dictData) {
6     const sentiParams = {
7         key: sentiApiKey,
8     }

10    const sentiApiKeyString = formatQueryParams(sentiParams)
11    const sentiUrl = sentiAnEndPoint + ""?"" + sentiApiKeyString;

13    const def = dictData[0].shortdef[0]
14    const dict = {
15        language: 'en-us',
16        type: 'PLAIN_TEXT',
17        content: def
18    };

20    const nlApiData = {
21        document: dict,
22        encodingType: 'UTF8'
23    };

25    const nlCallOptions = {
26        method: 'post',
27        contentType: 'application/json',
28        payload: JSON.stringify(nlApiData)
29    }

31    fetch(sentiUrl, nlCallOptions)
32    .then(response =&gt; {
33        if (response.ok) {
34            return response.json();
35        }
36        throw new Error(response.statusText);
37    })
38    .then(sentiData =&gt; parseSenti(sentiData))
39    .catch(err =&gt; {
40        $(""#error-message"").removeClass(""hidden"");
41        $(""#js-error-message"").text(`Something went wrong with the 
          Sentiment API: ${err.message}`);
43    });
44 }

46 function parseSenti(sentiData) {
47    const data = JSON.parse(sentiData);
48    const sentiment = 0.0;

50    if (data &amp;&amp; data.documentSentiment &amp;&amp; data.documentSentiment.score){
51       sentiment = data.documentSentiment.score;
52    }

54    console.log(sentiment);
55 }
</code></pre>
",Text Classification / Sentiment Analysis,fetch json google cloud natural language api based existing text using client side j doe anyone know retrieve data google cloud natural language nl api using client side javascript incorrect code block likely line looking lot documentation today figure perform sentiment analysis using google cloud nl api many clear tutorial helped understand google cloud nl api work large degree still gap block code derived looking doc one area think may potential issue line never fetched using two argument
Chat Data for NLP Text classification,"<p>I have a set of chat conversations (each conversations with 100 sentences) and corresponding labels (No other meta features). but there are only 5 observations per labels. I know that we need lots of observations per label to create a good classification model. So to increase the training data , is it a good practice to split the large text conversation into different sentences and considering each sentence as a distinct observation.(Now I will have 500 observations per label).What would be the effect on the performance of the classification model ? Would it increase or decrease or would have no effect on the performance? if there is change in performance, why is it so?</p>
",Text Classification / Sentiment Analysis,chat data nlp text classification set chat conversation conversation sentence corresponding label meta feature observation per label know need lot observation per label create good classification model increase training data good practice split large text conversation different sentence considering sentence distinct observation observation per label would effect performance classification model would increase decrease would effect performance change performance
How to merge sentiment analysis results (dfm) with original readtext object in Quanteda?,"<p>I have been using Quanteda's basic <code>tokens_lookup</code> function with the Young Soroka Sentiment Dictionary to count the number of positive and negative words in Tweets by politicians. </p>

<p>Once I get the results, is there a way I can then add these columns back into the original readtext object with the various docvars? </p>

<pre><code>head(dat)
readtext object consisting of 6 documents and 11 docvars.
# Description: df[,13] [6 × 13]
  doc_id   text       date      username   to      replies retweets favorites geo   mentions   hashtags        id permalink               
* &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                   
1 trump.c… ""\""Sleepy… 2020-05-… realDonal… MZHemi…    5415    13062     39680 NA    @AjitPaiF… """"       1.84e-224 https://twitter.com/rea…
2 trump.c… ""\""He got… 2020-05-… realDonal… mikand…   20406    39081    111370 NA    """"         """"       1.84e-224 https://twitter.com/rea…
3 trump.c… ""\""Thank … 2020-05-… realDonal… mikand…    5733    17293     66992 NA    """"         """"       1.84e-224 https://twitter.com/rea…
4 trump.c… ""\"".@CBS … 2020-05-… realDonal… """"        22215    25834     93625 NA    @CBS @60M… """"       1.83e-224 https://twitter.com/rea…
5 trump.c… ""\""This b… 2020-05-… realDonal… GreggJ…    5379    11403     39869 NA    """"         """"       1.81e-224 https://twitter.com/rea…
6 trump.c… ""\""OBAMAG… 2020-05-… realDonal… """"        55960    89664    320171 NA    """"         """"       1.81e-224 https://twitter.com/rea…
&gt; corp &lt;- corpus(dat)
&gt; toks &lt;- tokens(corp, remove_punct = TRUE)
&gt; toks_lsd &lt;- tokens_lookup(toks, dictionary =  data_dictionary_LSD2015[1:2])
&gt; dfmat_lsd &lt;- dfm(toks_lsd)
&gt; head(dfmat_lsd)
Document-feature matrix of: 6 documents, 2 features (66.7% sparse).
6 x 2 sparse Matrix of class ""dfm""
             features
docs          negative positive
  trump.csv.1        2        0
  trump.csv.2        0        0
  trump.csv.3        0        1
  trump.csv.4        2        1
  trump.csv.5        0        0
  trump.csv.6        0        0
</code></pre>

<p>I've tried taking the required columns from the readtext object and making a new data.frame with them, which works okay, but it'd be great if I could instead merge the dfm results back into the other data.</p>
",Text Classification / Sentiment Analysis,merge sentiment analysis result dfm original readtext object quanteda using quanteda basic function young soroka sentiment dictionary count number positive negative word tweet politician get result way add column back original readtext object various docvars tried taking required column readtext object making new data frame work okay great could instead merge dfm result back data
Need helping performing sentiment analysis on customer reviews and for a string of text,"<p>This is a 2 part code question.</p>

<p>1.) Need to perform sentiment analysis on a csv file for customer reviews.</p>

<p>2.) Need to perform sentiment analysis on a harry potter book review saved as a .txt</p>

<p>1.) The name of this Dataframe is ""reviews"" and what I want to do is display the sentiment score for each of these 5 reviews under the ""sent"" column.  Thank you so much!!! If you can provide the code with the ""sent"" column filled  with its sentiment analysis score for each row that would be awesome!!</p>

<p>reviews.head()</p>

<pre><code> ID  Customer Name    Review                                       Sent  

 1   Jack             Beautiful cover up. My only 
                      feedback is that it is a tad larger 
                      than expected, but since it's a cover 
                      up, it doesn't need to be fitted. The 
                      waist tassels also allow you to adjust 
                      to fit your waist which is nice. 
                      Otherwise, its exactly as expected!

 2   Rachel           This tunic is very cute in person. It's 
                      more sheer than I'd like, but I imagine 
                      I'll wear it a ton on vacation.

 3   Ryan             Just got this sweet little dress in 
                      blue. It's a great little dress for a 
                      pool cover up. I can envision myself 
                      wearing it on our winter getaway for 
                      breakfast or on a walk. I'm not sure how 
                      see through it is. I think I could get 
                      away wearing it as a dress. The length 
                      is great, not too short. The quality is 
                      great. I got a size S. Fits true to size. 
                      I am usually a size 2, 34b, 129lb, slim 
                      build. Very happy with this.

 4   Jennifer         Love this hat! Kept the sun off my face 
                      and neck/chest in the intense tropical 
                      sun! Choose white - so I stayed cool.

 5   Alex             What I like about bikinis is that they 
                      always fit you perfectly. You won't 
                      realize how gorgeous they are and how 
                      attractive they make your body look 
                      until you put one on. As for the bra-part 
                      it gives good support and sits well. I 
                      also like the fabric: it stretches well 
                      without losing its shape, the color 
                      doesn't fade. This bikini is no exception. 
                      is far better at making bikinis than 
                      anybody else, I would say! 
</code></pre>

<p>For this string I just to know what the overall sentiment score is... thanks!!</p>

<p>2.) </p>

<p>""Parents need to know that Harry Potter and the Sorcerer's Stone is a thrill-a-minute story, the first in J.K. Rowling's Harry Potter series. It respects kids' intelligence and motivates them to tackle its greater length and complexity, play imaginative games, and try to solve its logic puzzles. It's the lightest in the series, but it still has some scary stuff for sensitive readers: a three-headed dog, an attacking troll, a violent life-size chess board, a hooded figure over a dead and bleeding unicorn, as well as a discussion of how Harry's parents died years ago.""</p>
",Text Classification / Sentiment Analysis,need helping performing sentiment analysis customer review string text part code question need perform sentiment analysis csv file customer review need perform sentiment analysis harry potter book review saved txt name dataframe review want display sentiment score review sent column thank much provide code sent column filled sentiment analysis score row would awesome review head string know overall sentiment score thanks parent need know harry potter sorcerer stone thrill minute story first j k rowling harry potter series respect kid intelligence motivates tackle greater length complexity play imaginative game try solve logic puzzle lightest series still ha scary stuff sensitive reader three headed dog attacking troll violent life size chess board hooded figure dead bleeding unicorn well discussion harry parent died year ago
Aspect Based Sentiment Assignment,"<p>I was working on Aspect based sentiment analysis using Spacy dependent parser . In simple sentences such as "" Car is good"" it works properly , but when a slightly complex sentences arrives such as "" Car is good in terms of mileage "" it fails to assign positive sentiment for mileage and assigns it to car instead. Iam not able to relate mileage with good in following spacy parser. How to establish such a relation , If somebody could give a high level idea. 
<a href=""https://i.sstatic.net/gl921.png"" rel=""nofollow noreferrer"">Good and mileage are not at all related here</a></p>
",Text Classification / Sentiment Analysis,aspect based sentiment assignment wa working aspect based sentiment analysis using spacy dependent parser simple sentence car good work properly slightly complex sentence arrives car good term mileage fails assign positive sentiment mileage assigns car instead iam able relate mileage good following spacy parser establish relation somebody could give high level idea good mileage related
How to tune maximum entropy&#39;s parameter?,"<p>I am doing text classification with scikit learn's logistic regression function (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>). I am using grid search in order to choose a value for the C parameter. Do I need to do the same for max_iter parameter? why?</p>

<p>Both C and max_iter parameters have default values in Sklearn, which means they need to be tuned. But, from what I understand, early stopping and l1/l2 regularization are two desperate methods for avoiding overfitting and performing one of them is enough. Am I incorrect in assuming that tunning the value of max_iter is equivalent to early stopping?</p>

<p>To summarize, here are my main questions: </p>

<p>1- Does max_iter need tuning? why? (the documentation says it is only useful for certain solvers)</p>

<p>2- Is tuning the max_iter equivalent to early stopping?</p>

<p>3- Should we perform early stopping and L1/L2 regularization at the same time?</p>
",Text Classification / Sentiment Analysis,tune maximum entropy parameter text classification scikit learn logistic regression function using grid search order choose value c parameter need max iter parameter c max iter parameter default value sklearn mean need tuned understand early stopping l l regularization two desperate method avoiding overfitting performing one enough incorrect assuming tunning value max iter equivalent early stopping summarize main question doe max iter need tuning documentation say useful certain solver tuning max iter equivalent early stopping perform early stopping l l regularization time
Computing classification metrics for sequence labelling task,"<p>I intend to calculate accuracy/precision/recall/F1 measures for sentence classification task. I previously have computed it for whole text classification which is quite easy, but got confused at doing it for sentence classification as we perform at sentence-level and not text-/sentence(s)-level. Note that a text might contain several sentences... Here is an example:</p>

<p>Suppose we have the following text, with predicted labels in []:</p>

<blockquote>
  <p>Seq2seq networks are a good way of learning sequences. [0] They perform reasonably fine at generating long sequences. [1] These networks are utilized in downstream tasks such as NMT and text summarization [0]. blah blah blah [2]</p>
</blockquote>

<p>So the prediction is [0, 1, 0, 2] and suppose the gold labels for the sentences above are: [1, 1, 0, 0]. </p>

<p>So is the accuracy of this equal to <code>correct / total = (1 + 1) / 4 = 0.5</code>? What about other metrics such as Precision, Recall, and F1? Any ideas? </p>
",Text Classification / Sentiment Analysis,computing classification metric sequence labelling task intend calculate accuracy precision recall f measure sentence classification task previously computed whole text classification quite easy got confused sentence classification perform sentence level text sentence level note text might contain several sentence example suppose following text predicted label seq seq network good way learning sequence perform reasonably fine generating long sequence network utilized downstream task nmt text summarization blah blah blah prediction suppose gold label sentence accuracy equal metric precision recall f idea
how to convert dataframe to list and then to corpus?what is differnce between Vcorpus and Corpus in R?,"<p>i am new to r learning sentiment analysis using twitter tweets using R.
when i extract tweets in r they are in list class type but, then i need to convert them to dataframe to consider only one column with text type only because other types are irrelevant for my study like screenname, created, replyToSID, and others, and hence, i created dataframe and kept only one column with text. </p>

<p>but, when i convert the dataframe in list to further convert it into corpus for data cleaning using tm. i am getting output i expect in single list to do my study. below are my code lines..</p>

<pre><code>covid_tweets &lt;- searchTwitter('#COVID-19', n = 100, since = '2020-02-01')
options(stringsAsFactors = FALSE)

df_tweets &lt;- twListToDF(covid_tweets)
df_text_tweets &lt;- df_tweets[-c(2:16)]
View(df_text_tweets)

library(BBmisc)
data&lt;-convertRowsToList(df_text_tweets)
datacorpus &lt;- Corpus(VectorSource(data))

Covid_data&lt;-tm_map(datacorpus, stripWhitespace)
Covid_data&lt;-tm_map(Covid_data,tolower)
Covid_data&lt;-tm_map(Covid_data,removeNumbers)
Covid_data&lt;-tm_map(Covid_data,removePunctuation)
Covid_data&lt;-tm_map(Covid_data,removeWords, stopwords('english'))
</code></pre>

<p>output is not satisfactory.. when i use view function i can see 1 with 2 list i.e. content and metadata. 
I just want only text in the output in a list form..</p>

<p>if any further information needed please, free to ask me..or explain me how i simply convert the tweets into dataframe then to list followed by creating corpus and cleaning data using tm. Much appreciated in advance.</p>

<p>attached, the screenshots</p>

<p><a href=""https://i.sstatic.net/lzuCj.png"" rel=""nofollow noreferrer"">output of df tweets</a>
<a href=""https://i.sstatic.net/YEvdx.png"" rel=""nofollow noreferrer"">code i ran</a></p>
",Text Classification / Sentiment Analysis,convert dataframe list corpus differnce vcorpus corpus r new r learning sentiment analysis using twitter tweet using r extract tweet r list class type need convert dataframe consider one column text type type irrelevant study like screenname created replytosid others hence created dataframe kept one column text convert dataframe list convert corpus data cleaning using tm getting output expect single list study code line output satisfactory use view function see list e content metadata want text output list form information needed please free ask explain simply convert tweet dataframe list followed creating corpus cleaning data using tm much appreciated advance attached screenshots output df tweet code ran
Roberta detokenization without losing labels,"<p>So I ran a custom Roberta Token Classification, and I have a tokenized output id that looks like this, each of the output ID's has a corresponding label.</p>

<pre><code>  Ġlives   
  Ġo
  Ġn
  ĠCali
</code></pre>

<p>Each of these tokens comes with its prediction label. Detokenizing the text is trivial, but it loses the match with the prediction labels.</p>

<pre><code>detokenized = ''.join(tokens).replace('Ġ', ' ')

</code></pre>

<p>However, the challenge is to match the labels to the detokenized text. if anyone can point me to a script or method to use to solve this problem, would be really nice. Also below is my prediction script which was modified from a BERT script.</p>

<pre><code>tags, t_words = [], []                                         #tokenize
for row in test_df:
  tokenized_sentence = tokenizer.encode(row)
  input_ids = torch.tensor([tokenized_sentence]).cuda()

  with torch.no_grad():                                                 #predict
    output = model(input_ids)
    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)

     # join split tokens
    new_tokens, new_labels = [], []
    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0] )
    for token, label_idx in zip(tokens, label_indices[0]):
        new_labels.append(tag_values[label_idx])
        new_tokens.append(token)
    tags.append(new_labels)
    t_words.append(new_tokens)

</code></pre>

<p>but is there any way to have the prediction labels match the detokenized inputs, else the entire attempt at prediction is pointless.  </p>
",Text Classification / Sentiment Analysis,roberta detokenization without losing label ran custom roberta token classification tokenized output id look like output id ha corresponding label token come prediction label detokenizing text trivial loses match prediction label however challenge match label detokenized text anyone point script method use solve problem would really nice also prediction script wa modified bert script way prediction label match detokenized input else entire attempt prediction
How to get the sentiment score of each word in sentence based sentiment classification using RNN/LSTM?,"<p>If I use RNN/LSTM for sentence-based sentiment analysis, how do I get the sentiment/distribution/confidence of each word?  I just read one article shown here: <a href=""http://spectrum.ieee.org/computing/software/the-neural-network-that-remembers"" rel=""nofollow noreferrer"">Neural network that remembers</a></p>

<p>It includes one picture as below, how to get the likelihood of each character what if I only do the classification in sentence-level? I know how to use LSTM for sentence classification, but we only use the last hidden representation for classification, so how to get the likelihood of each character/word?</p>

<p>An practical example showing what is the input and output would be great! </p>

<p><a href=""https://i.sstatic.net/sVWrk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sVWrk.jpg"" alt=""enter image description here""></a></p>
",Text Classification / Sentiment Analysis,get sentiment score word sentence based sentiment classification using rnn lstm use rnn lstm sentence based sentiment analysis get sentiment distribution confidence word read one article shown neural network remembers includes one picture get likelihood character classification sentence level know use lstm sentence classification use last hidden representation classification get likelihood character word practical example showing input output would great
what is the accurate Twitter sentiment analysis solution with Python?,"<p>I have a CSV file of 20K tweets with all information such as location, username, and date which I want to assign a label positive/neutral/negative to each tweet by Python. 
I used the following Python code from textblob library for <strong>Tweets Sentiment Analysis</strong>. </p>

<pre><code>import csv
from textblob import TextBlob
import sys

# Do some version specific stuff
if sys.version[0] == '3':
    from importlib import reload
    sntTweets = csv.writer(open(""sentimentTweets.csv"", ""w"", newline=''))

if sys.version[0] == '2':
    reload(sys)
    sys.setdefaultencoding(""utf-8"")
    sntTweets = csv.writer(open(""sentimentTweets.csv"", ""w""))

alltweets = csv.reader(open(""Corona.csv"", 'r'))

for row in alltweets:
    blob = TextBlob(row[2])
    print (blob.sentiment.polarity)
    if blob.sentiment.polarity &gt; 0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""positive""])
    elif blob.sentiment.polarity &lt; 0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""negative""])
    elif blob.sentiment.polarity == 0.0:
        sntTweets.writerow([row[0], row[1], row[2], row[3], blob.sentiment.polarity, ""neutral""])
</code></pre>

<p>this code runs perfect and produces the sentimentTweets.csv file. I like the idea that for each tweet, it gives me two labels: a number between -1 and 1, and also classify tweet to negative/neutral/positive.</p>

<p>but it is not accurate. for example for the following tweet, it assigns positive with the number:0.285714285714285.
<em>""RT @eliyudin: ‚ÄúI‚Äôll have a Corona... hold the virus!‚Äù -a dad on vacation somewhere in Florida right now""</em><br>
but as you can understand, the sentiment of the above tweet should be negative.
How can I make it accurate? and how can I find the accuracy of my output?</p>
",Text Classification / Sentiment Analysis,accurate twitter sentiment analysis solution python csv file k tweet information location username date want assign label positive neutral negative tweet python used following python code textblob library tweet sentiment analysis code run perfect produce sentimenttweets csv file like idea tweet give two label number also classify tweet negative neutral positive accurate example following tweet assigns positive number rt eliyudin corona hold virus dad vacation somewhere florida right understand sentiment tweet negative make accurate find accuracy output
Visualize text classes in a scatter-plot,"<p>I am looking for ways to investigate in my train data 'modellability' and check if the classes are well distinguished in terms of vocabulary... etc.</p>

<p>I am a bit embarrassed but I was wondering if it is possible to do a scatter plot for text classification model in torch? or any other approach to investigate in the data quality.</p>
",Text Classification / Sentiment Analysis,visualize text class scatter plot looking way investigate train data modellability check class well distinguished term vocabulary etc bit embarrassed wa wondering possible scatter plot text classification model torch approach investigate data quality
topic classification using k-gram index,"<p>I have a set of topics each described with a list of keywords. <code>{Sports:['Ronaldo Messi Zidane','Football Baseball', 'Barcelona Real']...}</code> </p>

<p>The task is to classify a particular document. The classification can be also multi-label. A document can belong to topic1,topic2 etc. I don't have enough data thus can't approach the problem using machine learning. Because I want to retrieve highly precise documents I approached the problem using <code>k-gram</code> index.</p>

<p>I treat a given set of topic keywords as queries and built a <code>k-gram</code> index around it. So I have all the keys as character bigrams and the values as terms which contain the bigram. These terms are terms present in the document that I want to classify. After traversing the postings list for every keyword of a topic I get a set of candidate terms and their corresponding jaccard similarity score. </p>

<ol>
<li>Within a topic How do I combine jaccard score of all candidate terms ? </li>
<li>Within all topics how do I decide which topic this document belongs to ? </li>
<li>Do you think this approach can give me results with high precision ? </li>
</ol>

<p>Thank you. </p>
",Text Classification / Sentiment Analysis,topic classification using k gram index set topic described list keywords task classify particular document classification also multi label document belong topic topic etc enough data thus approach problem using machine learning want retrieve highly precise document approached problem using index treat given set topic keywords query built index around key character bigram value term contain bigram term term present document want classify traversing posting list every keyword topic get set candidate term corresponding jaccard similarity score within topic combine jaccard score candidate term within topic decide topic document belongs think approach give result high precision thank
why are predictions of the multi label model blank?,"<p>I am trying to do multi label classification; The data set is mainly title column contains titles of the posts and tags column contains the tags. The number of tags for a post is not fixed. The dataset is like this:
<a href=""https://i.sstatic.net/gSwWe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gSwWe.png"" alt=""enter image description here""></a></p>

<p>and the code I wrote is:</p>

<pre><code>X_train, y_train = train['title'].values, train['tags'].values
X_val, y_val = validation['title'].values, validation['tags'].values
##I did some preprocessing on the data(eg.lowering, removing stop words etc.) then:
tfidf=TfidfVectorizer(ngram_range=(1,2),min_df=2,max_df=.9,token_pattern='(\S+)').fit(X_train)

X_train=tfidf.transform(X_train)
X_test=tfidf.transform(X_test) 
X_val=tfidf.transform(X_val) 
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))##tags_counts.keys are all the tags contained in the dataset
y_train = mlb.fit_transform(y_train)
y_val = mlb.fit_transform(y_val)
model=OneVsRestClassifier(LogisticRegression(C=10)).fit(X_train_tfidf, y_train) 
y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val[0])
</code></pre>

<p>this prediction give me an array of all zeros which means that it did not predict any tag in this record, and when I use the inverse to get the tags in letter as follows:</p>

<pre><code>
</code></pre>

<p>I got the prediction blank <code>[()]</code>. Any ideas?</p>
",Text Classification / Sentiment Analysis,prediction multi label model blank trying multi label classification data set mainly title column contains title post tag column contains tag number tag post fixed dataset like code wrote prediction give array zero mean predict tag record use inverse get tag letter follows got prediction blank idea
TypeError: sentiment_analysis() missing 1 required positional argument: &#39;polarity&#39;?,"<p>def sentiment_analysis(polarity):</p>
<pre><code>if x &lt; 0:
    print(&quot;neg&quot;)
elif x &gt;0:
        print(&quot;pos&quot;)
else:
            print(&quot;neutral&quot;)


    
#analysis the text sentiment
text = processed_file.get('1.0',tk.END)
new_text = TextBlob(text)
processed_text = new_text.sentiment
result = '\nSubjectivity:{}, Polarity
                {}'.format(new_text.sentiment.subjectivity,new_text.sentiment.polarity)
x= result.sentiment.polarity
return sentiment_analysis(x)
show.insert(tk.END,x)
                     
</code></pre>
<h1>Can someone explain why I am getting an typeerror</h1>
",Text Classification / Sentiment Analysis,typeerror sentiment analysis missing required positional argument polarity def sentiment analysis polarity someone explain getting typeerror
Adding column of values to pandas DataFrame,"<p>I'm doing a simple sentiment analysis and am stuck on something that I feel is very simple. I'm trying to add an new column with a set of values, in this example <code>compound</code> values. But after the for loop iterates it adds the same value for all the rows rather than a value for each iteration. The <code>compound</code> values are the last column in the DataFrame. There should be a quick fix. thanks!</p>

<pre><code>for i, row in real.iterrows():
   real['compound'] = sid.polarity_scores(real['title'][i])['compound']


title   text    subject date                                                        compound
0   As U.S. budget fight looms, Republicans flip t...   WASHINGTON (Reuters) - The head of a conservat...   politicsNews    December 31, 2017   0.2263
1   U.S. military to accept transgender recruits o...   WASHINGTON (Reuters) - Transgender people will...   politicsNews    December 29, 2017   0.2263
2   Senior U.S. Republican senator: 'Let Mr. Muell...   WASHINGTON (Reuters) - The special counsel inv...   politicsNews    December 31, 2017   0.2263
3   FBI Russia probe helped by Australian diplomat...   WASHINGTON (Reuters) - Trump campaign adviser ...   politicsNews    December 30, 2017   0.2263
4   Trump wants Postal Service to charge 'much mor...   SEATTLE/WASHINGTON (Reuters) - President Donal...   politicsNews    December 29, 2017   0.2263
</code></pre>

<p><a href=""https://i.sstatic.net/iCsKS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iCsKS.png"" alt=""enter image description here""></a></p>
",Text Classification / Sentiment Analysis,adding column value panda dataframe simple sentiment analysis stuck something feel simple trying add new column set value example value loop iterates add value row rather value iteration value last column dataframe quick fix thanks
Text Classification - DNN,"<p>I am performing text classification using a Deep Neural network. My problem is that I am receiving high accuracy 98 on train data whereas my validation accuracy is 49.</p>

<p>I have tried the following:</p>

<ol>
<li>Shuffled the data</li>
<li>My train and validation data is 80:20 split</li>
<li>I am using 100 dimensions Glov vector</li>
</ol>

<p>Any suggestions?</p>

<pre class=""lang-py prettyprint-override""><code>def get_Model():
    model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights= . [embeddings_matrix], trainable=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(5, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy',optimizer=""adam"",metrics=['acc'])
    model.summary()
    return model
</code></pre>
",Text Classification / Sentiment Analysis,text classification dnn performing text classification using deep neural network problem receiving high accuracy train data whereas validation accuracy tried following shuffled data train validation data split using dimension glov vector suggestion
"I fine tuned a pre-trained BERT for sentence classification, but i cant get it to predict for new sentences","<p>below is the result of my fine-tuning.</p>

<pre><code>Training Loss   Valid. Loss Valid. Accur.   Training Time   Validation Time
epoch                   
1   0.16    0.11    0.96    0:02:11 0:00:05
2   0.07    0.13    0.96    0:02:19 0:00:05
3   0.03    0.14    0.97    0:02:22 0:00:05
4   0.02    0.16    0.96    0:02:21 0:00:05
</code></pre>

<p>next i tried to use the model to predict labels from a csv file. i created a label column, set the type to int64 and run the prediction.</p>

<pre><code>print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))
model.eval()
# Tracking variables 
predictions , true_labels = [], []
# Predict 
for batch in prediction_dataloader:
  # Add batch to GPU
  batch = tuple(t.to(device) for t in batch)

  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels = batch

  # Telling the model not to compute or store gradients, saving memory and 
  # speeding up prediction
  with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]

  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()

  # Store predictions and true labels
  predictions.append(logits)
  true_labels.append(label_ids)


</code></pre>

<p>however, while i am able to print out the predictions[4.235, -4.805] etc, and the true_labels[NaN,NaN.....], i am unable to actually get the predicted labels{0 or 1}. Am i missing something here?</p>
",Text Classification / Sentiment Analysis,fine tuned pre trained bert sentence classification cant get predict new sentence result fine tuning next tried use model predict label csv file created label column set type int run prediction however able print prediction etc true label nan nan unable actually get predicted label missing something
Not enough memory while using the Tokenizer in keras.preprocessing.text,"<p>I want to build a RNN model using keras to classify sentences.</p>

<p>I tried the following code:</p>

<pre><code>docs = []
with open('all_dga.txt', 'r') as f:
    for line in f.readlines():
        dga_domain, _ = line.split(' ')
        docs.append(dga_domain)

t = Tokenizer()
t.fit_on_texts(docs)
encoded_docs = t.texts_to_matrix(docs, mode='count')
print(encoded_docs)
</code></pre>

<p>but got a MemoryError. It seemed that I couldn't load all data into the memory. This is the output:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 11, in &lt;module&gt;
    encoded_docs = t.texts_to_matrix(docs, mode='count')
  File ""/home/yurzho/anaconda3/envs/deepdga/lib/python3.6/site-packages/keras/preprocessing/text.py"", line 273, in texts_to_matrix
    return self.sequences_to_matrix(sequences, mode=mode)
  File ""/home/yurzho/anaconda3/envs/deepdga/lib/python3.6/site-packages/keras/preprocessing/text.py"", line 303, in sequences_to_matrix
    x = np.zeros((len(sequences), num_words))
MemoryError
</code></pre>

<p>If anyone familiar with keras, please tell me how to pre-process the dataset.</p>

<p>Thanks in advance!</p>
",Text Classification / Sentiment Analysis,enough memory using tokenizer kera preprocessing text want build rnn model using kera classify sentence tried following code got memoryerror seemed load data memory output anyone familiar kera please tell pre process dataset thanks advance
How to classify texts that are related to the bible based on their content,"<p>I have a database of texts from comments of social networks (FB,Twitter).
My goal is to classify texts that have strong relation to the bible based on their content (for example if there are cites or ""biblical"" words that are used.
This is a binary classification problem and i need help to figure out how to approach it (maybe use the bible as a dictionary somehow). Thanks!</p>
",Text Classification / Sentiment Analysis,classify text related bible based content database text comment social network fb twitter goal classify text strong relation bible based content example cite biblical word used binary classification problem need help figure approach maybe use bible dictionary somehow thanks
Text classification with imbalanced data,"<p>Am trying to classify 10000 samples of text into 20 classes. 4 of the classes have just 1 sample each, I tried SMOTE to address this imbalance, but I am unable to generate new samples for classes that have only one record, though I could generate samples for classes with more than 1 sample. Any suggestions?</p>
",Text Classification / Sentiment Analysis,text classification imbalanced data trying classify sample text class class sample tried smote address imbalance unable generate new sample class one record though could generate sample class sample suggestion
emotion detection for messages,"<p>I'm working on a project that requires NLP. The scenario involves two or more participants exchanging messages in a chat app. I want to use am NLP/ML model to auto-tag message for the participants' emotions as the conversation is happening (e.g happy, sad, anger, frustrated, I'm not looking for sentiment analysis). My prior knowledge of NLP is limited, and I had a hard time finding the right model to use. </p>

<p>I found this repo <a href=""https://github.com/SenticNet/conv-emotion"" rel=""nofollow noreferrer"">conv-emotion</a> and spent almost two days trying to make it work. Currently reaching out to the authors for help. I like their repo is because it's the model is applied on a conversation level. But unfortunately, their README isn't well written.</p>

<p>Any suggestions going forward? Do you know any model or even API that I can use? Am I on the right track?</p>
",Text Classification / Sentiment Analysis,emotion detection message working project requires nlp scenario involves two participant exchanging message chat app want use nlp ml model auto tag message participant emotion conversation happening e g happy sad anger frustrated looking sentiment analysis prior knowledge nlp limited hard time finding right model use found repo conv emotion spent almost two day trying make work currently reaching author help like repo model applied conversation level unfortunately readme well written suggestion going forward know model even api use right track
How to give one sample text input to a pre-trained LSTM model,"<p>I am trying to do toxic comment classification. I found a dataset in <a href=""https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge</a>. It has many comments with corresponding values for label class ""toxic"", ""severe_toxic"" ..etc. I want to classify my single text input to the corresponding labeled class.
I have created and trained a model using LSTM. Now I want to give a single text sentence as input to the model to predict the output.
But I don't know how to convert the text input and give it to the trained model. </p>

<p>The source-code has been obtained from <a href=""https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras"" rel=""nofollow noreferrer"">kaggle challange</a>.</p>

<p><a href=""https://i.sstatic.net/b6WoX.png"" rel=""nofollow noreferrer"">summary of the model</a></p>
",Text Classification / Sentiment Analysis,give one sample text input pre trained lstm model trying toxic comment classification found dataset ha many comment corresponding value label class toxic severe toxic etc want classify single text input corresponding labeled class created trained model using lstm want give single text sentence input model predict output know convert text input give trained model source code ha obtained kaggle challange summary model
How to get &quot;no label&quot; from a supervised multi-label machine learning model?,"<p>I'm working on a supervised multi-label ML model which for now works by predicting a ""tag"" (label) for an input from the user, based on the dataset it was trained with. The training dataset had two columns - posts &amp; tags. </p>

<p>Whenever I give anything as an input, the prediction is a relevant tag for it. But when the input is something which is not in the dataset, the output is again some random tag. I'm looking for a solution in the context of creating a chatbot, which would return something like ""Sorry, I couldn't understand that"" if the input is something different from what the machine has been trained for. How do I do that? Is there any way to get an ""empty label"" in such a case so that I can simply map my responses accordingly for that condition?</p>

<p>Thanks in advance!</p>
",Text Classification / Sentiment Analysis,get label supervised multi label machine learning model working supervised multi label ml model work predicting tag label input user based dataset wa trained training dataset two column post tag whenever give anything input prediction relevant tag input something dataset output random tag looking solution context creating chatbot would return something like sorry understand input something different machine ha trained way get empty label case simply map response accordingly condition thanks advance
How can I evaluate the last number in a dictionary and replace the number with text?,"<p>I was fiddling around with a few different algos to do text sentiment analysis.  So far, all were squirley, except for one.  This one looks like it's pretty accurate.</p>

<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
df['sentiment'] = df['review_text'].apply(lambda x: sid.polarity_scores(x))
</code></pre>

<p>That gives me dictionary results, like this:</p>

<pre><code>{'neg': 0.315, 'neu': 0.593, 'pos': 0.093, 'compound': -0.7178}
{'neg': 0.215, 'neu': 0.556, 'pos': 0.229, 'compound': 0.0516}
{'neg': 0.373, 'neu': 0.133, 'pos': 0.493, 'compound': 0.2263}
{'neg': 0.242, 'neu': 0.547, 'pos': 0.211, 'compound': -0.1027}
{'neg': 0.31, 'neu': 0.69, 'pos': 0.0, 'compound': -0.6597}
</code></pre>

<p>I'm trying to figure out how to evaluate the last number in each row (-0.7178, 0.0516, 0.2263, -0.1027, -0.6597) and apply the following logic:</p>

<pre><code>If compound &lt;= 0 Then negative
ElseIf compound &gt; .2 Then positive
Else neutral
</code></pre>

<p>I tried to find a substring within the dictionary, like this:</p>

<pre><code>sub = '''compound':'''
df['Indexes'] = df['sentiment'].str.find(sub)  
df 
</code></pre>

<p>I was thinking of finding the position, and then get the last number, and then run the logic I described above.  I starting to think that's not the right approach.  What's the best way to solve this problem?</p>
",Text Classification / Sentiment Analysis,evaluate last number dictionary replace number text wa fiddling around different algos text sentiment analysis far squirley except one one look like pretty accurate give dictionary result like trying figure evaluate last number row apply following logic tried find substring within dictionary like wa thinking finding position get last number run logic described starting think right approach best way solve problem
"Given many examples of a text, classify some never before seen text","<p>I have a bunch of example texts for specific labels. Using this data, I want to be able to classify a new piece of text for the lable.</p>

<p>I have a dozen to a hundred of rows of data like this:</p>

<pre><code>Product Description | Price | Batteries | Reviews

The HP notebook is a bargain...., $200, Included, 37
Acer just released a new laptop...., $500, Not Included, 0
Lenovo Thinkpad is hitting the market .., $800, None, 23
</code></pre>

<p>and then for a new text like this, I want to be able to pull out the labels.</p>

<pre><code>Samsung Ultrabook is sleek and new..., $900, n/a, 2 reviews
</code></pre>

<p>I've looked at the python library Spacy's rule based matching, but I'm not sure how to recognize a chunk of text like the product description. The price, reviews are fairly easy to define rules for but it's not clear for the product description as the length is all over the place. However, from the 100s of rows there exists a minimum and maximum, and I was thinking of training some sort of neural network that will be able to detect a given text and classify it.</p>

<p>The core question is, how do I teach a neural network many examples of a label, and then detect it on some new corpus? </p>

<p>update: this is the output I expect for (the order is unpredictable and may contain more items than below) </p>

<p><code>Samsung Ultrabook is sleek and new..., $900, n/a, 2 reviews</code></p>

<p>so I will loop over each item and it should identify the one of the labels <code>['product description', 'price', 'batteries', 'reviews']</code>. If no label is found from the input, it is ignored.</p>
",Text Classification / Sentiment Analysis,given many example text classify never seen text bunch example text specific label using data want able classify new piece text lable dozen hundred row data like new text like want able pull label looked python library spacy rule based matching sure recognize chunk text like product description price review fairly easy define rule clear product description length place however row exists minimum maximum wa thinking training sort neural network able detect given text classify core question teach neural network many example label detect new corpus update output expect order unpredictable may contain item loop item identify one label label found input ignored
How do I make my algo work with KNN text classification?,"<p>Trying to make my classification accepting a text (string) and not just a number (numeric). Working with data, carrying a load of pulled articles, I want the classification algo to show which ones to proceed with and which ones to drop. Applying a number, things are working just fine, yet this is not very intuitive, although I know that the number represents a relationship to one of the two classes I am working with.</p>

<p>How do I change the logic in the algo to make it accept a text as search criteria and not just an anonymous number, picked from the 'Unique_id' column? Columns are, btw...'Title', 'Abstract', 'Relevant', 'Label', 'Unique_id'. The reason for concatenating df's at algo end is that I want to compare results. Finally. it should be noted that the col 'Label' consists of a list of keywords, so basically I want the algo to read from that col.</p>

<p>I did try, reading from data sources, changing the '<em>index_col='Unique_id</em>' to '<em>index_col='Label</em>', but that did not work out either.</p>

<p>An example of what I want:</p>

<pre><code>print(""\nPrint KNN1"")
print(get_closest_neighs1('search word'), ""\n"")

print(""\nPrint KNN2"")
print(get_closest_neighs2('search word'), ""\n"")

print(""\nPrint KNN3"")
print(get_closest_neighs3('search word'), ""\n"")
</code></pre>

<p>This is the full code (view end of algo to see above example as it runs today, using a number to identify nearest neighbor):</p>

<pre><code>import pandas as pd

print(""\nPerforming Analysis using Text Classification"")
data = pd.read_csv('File_1_coltest_demo.csv', sep=';',  encoding=""ISO-8859-1"").dropna()

data['Unique_id'] = data.groupby(['Title', 'Abstract', 'Relevant']).ngroup()

data.to_csv('File_2_coltest_demo_KNN.csv', sep=';', encoding=""ISO-8859-1"", index=False)

data1 = pd.read_csv('File_2_coltest_demo_KNN.csv', sep=';', encoding=""ISO-8859-1"", index_col='Unique_id')

data2 = pd.DataFrame(data1, columns=['Abstract', 'Relevant'])

data2.to_csv('File_3_coltest_demo_KNN_reduced.csv', sep=';', encoding=""ISO-8859-1"", index=False)

print(""\nData top 25 items"")
print(data2.head(25))

print(""\nData info"")
print(data2.info())

print(""\nData columns"")
print(data2.columns)

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 1), tokenizer=token.tokenize)
text_counts = cv.fit_transform(data2['Abstract'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
text_counts, data2['Abstract'], test_size=0.5, random_state=1)

print(""\nTF IDF"")
from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer()
text_tf = tf.fit_transform(data2['Abstract'])
print(text_tf)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
text_tf, data2['Abstract'], test_size=0.3, random_state=123)

from sklearn.neighbors import NearestNeighbors
import pandas as pd

nbrs = NearestNeighbors(n_neighbors=20, metric='euclidean').fit(text_tf)

def get_closest_neighs1(Abstract):
    row = data2.index.get_loc(Abstract)
    distances, indices = nbrs.kneighbors(text_tf.getrow(row))
    names_similar = pd.Series(indices.flatten()).map(data2.reset_index()['Abstract'])
    result = pd.DataFrame({'distance1' : distances.flatten(), 'Abstract' : names_similar}) # 'Unique_id' : names_similar,
    return result

def get_closest_neighs2(Unique_id):
    row = data2.index.get_loc(Unique_id)
    distances, indices = nbrs.kneighbors(text_tf.getrow(row))
    names_similar = pd.Series(indices.flatten()).map(data2.reset_index()['Unique_id'])
    result1 = pd.DataFrame({'Distance' : distances.flatten() / 10, 'Unique_id' : names_similar}) # 'Unique_id' : names_similar,
    return result1

def get_closest_neighs3(Relevant):
    row = data2.index.get_loc(Relevant)
    distances, indices = nbrs.kneighbors(text_tf.getrow(row))
    names_similar = pd.Series(indices.flatten()).map(data2.reset_index()['Relevant'])
    result2 = pd.DataFrame({'distance2' : distances.flatten(), 'Relevant' : names_similar}) # 'Unique_id' : names_similar,
    return result2

print(""\nPrint KNN1"")
print(get_closest_neighs1(114), ""\n"")

print(""\nPrint KNN2"")
print(get_closest_neighs2(114), ""\n"")

print(""\nPrint KNN3"")
print(get_closest_neighs3(114), ""\n"")

data3 = pd.DataFrame(get_closest_neighs1(114))
data4 = pd.DataFrame(get_closest_neighs2(114))
data5 = pd.DataFrame(get_closest_neighs3(114))

del data5['distance2']

data6 = pd.concat([data3, data4, data5], axis=1).reindex(data3.index)

del data6['distance1']

data6.to_csv('File_4_coltest_demo_KNN_results.csv', sep=';', encoding=""ISO-8859-1"", index=False)
</code></pre>
",Text Classification / Sentiment Analysis,make algo work knn text classification trying make classification accepting text string number numeric working data carrying load pulled article want classification algo show one proceed one drop applying number thing working fine yet intuitive although know number represents relationship one two class working change logic algo make accept text search criterion anonymous number picked unique id column column btw title abstract relevant label unique id reason concatenating df algo end want compare result finally noted col label consists list keywords basically want algo read col try reading data source changing index col unique id index col label work either example want full code view end algo see example run today using number identify nearest neighbor
Best way to classify Messy Text Data of Product Description,"<p>A pipeline of Word Embeddings + Feature Extraction + Classifier is often used in Text Classification (categories such as Dress, Toys, Food, etc.) but <strong>this assumes a lot of things in the data being well-structured</strong>. 
What about techniques for classifying text data that are full of typographical errors (e.g. <em>""RUBBER DUCK TYS""</em>) , missing spaces (e.g. <em>""PINKPOLYESTERDRESSES""</em>), bunch of inserted random words (e.g. <em>""INTEL CHIP 220mg 1104 OLD TOWN ST.</em>), etc.?</p>

<p>I know n-gram approach + a classifier can be used here but</p>

<ol>
<li>using n-gram is probably computationally expensive (imagine having a
dataset composed of 100 million product entries that are like that</li>
<li>impossible to use embeddings in n-grams (DRESS VS SHIRT using
trigrams)</li>
</ol>

<p>What do you think are some approaches applicable to this?</p>
",Text Classification / Sentiment Analysis,best way classify messy text data product description pipeline word embeddings feature extraction classifier often used text classification category dress toy food etc assumes lot thing data well structured technique classifying text data full typographical error e g rubber duck tys missing space e g pinkpolyesterdresses bunch inserted random word e g intel chip mg old town st etc know n gram approach classifier used using n gram probably computationally expensive imagine dataset composed million product entry like impossible use embeddings n gram dress v shirt using trigram think approach applicable
serializeTo parameter in ColumnDataClassifier,"<p>I am currently performing a text classification using the ColumnDataClassifier by Stanford NLP group.
I would like to perform the training stage serializing the model through the serializeTo parameter included in the prop file.</p>

<p>Classification results obtained performing training and test stages through the same command line are different from those ones obtained applying the serialized classifier on a new test document. Why this happens?</p>

<p>Example:</p>

<p><strong>First classification</strong></p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop myfile.prop </p>

<p>where in myfile.prop i added values for trainFile and testFile.</p>

<p><a href=""https://i.sstatic.net/kMBQ2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kMBQ2.png"" alt=""enter image description here""></a></p>

<p><strong>Second classification</strong></p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -prop myfile2.prop</p>

<p>where in myfile2.prop i added values for trainFile and serializeTo. I am not including any testFile in myfile2.prop. Once i finish the training stage i want to classify new data using the classifier serialized during the training phase.</p>

<p>java -cp ""*:."" edu.stanford.nlp.classify.ColumnDataClassifier -loadClassifier MyClassifier -testFile myTestFile</p>

<p><a href=""https://i.sstatic.net/hAO7p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hAO7p.png"" alt=""enter image description here""></a></p>

<p>As you can see results are different. In particular, the serialized classifier associates almost all the instances to the class O (the default one).</p>
",Text Classification / Sentiment Analysis,serializeto parameter columndataclassifier currently performing text classification using columndataclassifier stanford nlp group would like perform training stage serializing model serializeto parameter included prop file classification result obtained performing training test stage command line different one obtained applying serialized classifier new test document happens example first classification java cp edu stanford nlp classify columndataclassifier prop myfile prop myfile prop added value trainfile testfile second classification java cp edu stanford nlp classify columndataclassifier prop myfile prop myfile prop added value trainfile serializeto including testfile myfile prop finish training stage want classify new data using classifier serialized training phase java cp edu stanford nlp classify columndataclassifier loadclassifier myclassifier testfile mytestfile see result different particular serialized classifier associate almost instance class default one
Problems in Unsupervised Aspect Based Sentiment Analysis,"<p>I'm working on unsupervised aspect based sentiment analysis. I tried using Vader for it, which gave me good result but the problem is if the topic is negative like 'food waste' then the sentiment is always coming as negative even though content is saying 'and i really hate food waste'.
Can someone help me in tackling this issue, or even suggest me a method better than Vader.
I've also tried using 'Flair' but its' results are not as promising as Vader.</p>
",Text Classification / Sentiment Analysis,problem unsupervised aspect based sentiment analysis working unsupervised aspect based sentiment analysis tried using vader gave good result problem topic negative like food waste sentiment always coming negative even though content saying really hate food waste someone help tackling issue even suggest method better vader also tried using flair result promising vader
How to recognize entities in text that is the output of optical character recognition (OCR)?,"<p>I am trying to do multi-class classification with textual data. Problem I am facing that I have unstructured textual data. I'll explain the problem with an example.
consider this image for example:</p>

<p><a href=""https://i.sstatic.net/mS1fx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mS1fx.jpg"" alt=""example data""></a></p>

<p>I want to extract and classify text information given in image. Problem is when I extract information OCR engine will give output something like this:</p>

<pre><code>18
EURO 46
KEEP AWAY
FROM FIRE
MADE IN CHINA
2226249917581
7412501
DOROTHY
PERKINS
</code></pre>

<p>Now target classes here are:</p>

<pre><code>18 -&gt; size
EURO 46 -&gt; price
KEEP AWAY FROM FIRE -&gt; usage_instructions
MADE IN CHINA -&gt; manufacturing_location
2226249917581 -&gt; product_id
7412501 -&gt; style_id
DOROTHY PERKINS -&gt; brand_name
</code></pre>

<p>Problem I am facing is that input text is not separable, meaning ""multiple lines can belong to same class"" and there can be cases where ""single line can have multiple classes"".</p>

<p>So I don't know how I can split/merge lines before passing it to classification model.<br> Is there any way using NLP I can split paragraph based on target class. In other words given input paragraph split it based on target labels.</p>
",Text Classification / Sentiment Analysis,recognize entity text output optical character recognition ocr trying multi class classification textual data problem facing unstructured textual data explain problem example consider image example want extract classify text information given image problem extract information ocr engine give output something like target class problem facing input text separable meaning multiple line belong class case single line multiple class know split merge line passing classification model way using nlp split paragraph based target class word given input paragraph split based target label
I have to classify user statements in 30 categories using svc ..So What should be done,"<p>I want to solve this problem using ml algorithm of svc.
The problem is: ""I have 2 columns 1 for sentences and other for the category of that statement. So what I want to do is make a model which classifies the user statements into 30 categories""</p>
",Text Classification / Sentiment Analysis,classify user statement category using svc done want solve problem using ml algorithm svc problem column sentence category statement want make model classifies user statement category
Using Sentiwordnet 3.0,"<p>I plan on using Sentiwordnet 3.0 for Sentiment classification. Could someone clarify as to what the numbers associated with words in Sentiwordnet represent? For e.g. what does 5 in rank#5 mean? Also for POS what is the letter used to represent adverbs? Im assuming 'a' is adjectives. I could not find an explanation either on their site or on other sites.</p>
",Text Classification / Sentiment Analysis,using sentiwordnet plan using sentiwordnet sentiment classification could someone clarify number associated word sentiwordnet represent e g doe rank mean also po letter used represent adverb im assuming adjective could find explanation either site site
Understanding DictVectorizer in scikit-learn?,"<p>I'm exploring the different feature extraction classes that <code>scikit-learn</code> provides. Reading the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"" rel=""noreferrer"">documentation</a> I did not understand very well what <code>DictVectorizer</code> can be used for? Other questions come to mind. For example, how can <code>DictVectorizer</code> be used for text classification?, i.e. how does this class help handle labelled textual data? Could anybody provide a short example apart from the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"" rel=""noreferrer"">example</a> that I already read at the documentation web page?</p>
",Text Classification / Sentiment Analysis,understanding dictvectorizer scikit learn exploring different feature extraction class provides reading documentation understand well used question come mind example used text classification e doe class help handle labelled textual data could anybody provide short example apart example already read documentation web page
classify website business domain,"<p>I want to identify the Category/business_domain of the website's business to which it belongs.</p>

<p>For ex. superhuman website. The company made Email client powered by buzzword features &amp; UI.</p>

<p>So in short Category of website can be <strong>Professional Email services</strong>.</p>

<p>So, to get this done, some of my initials thoughts are applying <a href=""https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"" rel=""nofollow"">LDA</a> algorithm (python module) on About_us text of a website &amp; company's Facebook info page, given that we have these both. But still this approach is not working in many cases. Any insights?</p>

<p>LDA details:
using 20000 passes and 1 topic, my results for <a href=""http://aakritiartgallery.com/"" rel=""nofollow"">http://aakritiartgallery.com/</a> website is</p>

<pre><code>[(0, u'0.050*art + 0.020*aakriti + 0.019*contemporary + 0.017*gallery + 0.015*new')]
</code></pre>

<p>How can i narrow down to my business with these term probablities given by LDA?</p>
",Text Classification / Sentiment Analysis,classify website business domain want identify category business domain website business belongs ex superhuman website company made email client powered buzzword feature ui short category website professional email service get done initial thought applying lda algorithm python module u text website company facebook info page given still approach working many case insight lda detail using pass topic result website narrow business term probablities given lda
TextBlob sentiment analysis: nan values,"<p>I'm new to sentiment analysis and I'm exploring with TextBlob.</p>

<p>My data is pre-processed Twitter data. It's in a series and each tweet has been cleaned and tokenized:</p>

<pre><code>    0   [new, leaked, treasury, document, full, sugges...
    1   [tommy, robinson, endorsing, conservative, for...
    2   [thanks, already, watched, catch, tv, morning, ]
    3   [treasury, document, check, today, check, cons...
    4   [utterly, stunning, video, hoped, prayed, woul...
    ... ...
    307370  [trump, disciple, copycat]
    307373  [disgusting]
    307389  [wonder, people, vote, racist, homophobe, like...
    307391  [gary, neville, slam, fuelling, racism, manche...
    307393  [brexit, fault, excuseforeverything]
</code></pre>

<p>When I run textblob sentiment (using help from <a href=""https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe"">Apply textblob in for each row of a dataframe</a>), my result is a column of nan values:</p>

<pre><code>    # Create sentiment column using textblob
    # Source: https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe

    def sentiment_calc(text):
try:
    return TextBlob(text).sentiment
except:
    return None

    boris_data['sentiment'] = boris_data['text'].apply(sentiment_calc)

         text   sentiment
    0   [new, leaked, treasury, document, full, sugges...   None
    1   [tommy, robinson, endorsing, conservative, for...   None
    2   [thanks, already, watched, catch, tv, morning, ]    None
    3   [treasury, document, check, today, check, cons...   None
    4   [utterly, stunning, video, hoped, prayed, woul...   None
       ...  ... ...
    307370  [trump, disciple, copycat]  None
    307373  [disgusting]    None
    307389  [wonder, people, vote, racist, homophobe, like...   None
    307391  [gary, neville, slam, fuelling, racism, manche...   None
    307393  [brexit, fault, excuseforeverything]    None
</code></pre>
",Text Classification / Sentiment Analysis,textblob sentiment analysis nan value new sentiment analysis exploring textblob data pre processed twitter data series tweet ha cleaned tokenized run textblob sentiment using help href textblob row dataframe result column nan value
Text Classification Problem : Name and approach of this type of classification,"<p>I have a labelled data-set comprising of text segments and corresponding labels. Each label consists of three parts, and there can be multiple or zero labels assigned to a given text segment. </p>

<pre><code>Sample Data is given below:

text segment                              action        performed       person  
 ---                                       ---           ---             ---
""I went outside to play and not drink.""   {play,drink}  {yes,no}        {1st,1st}
""He is not playing.""                       play          no              3rd
""The weather is cold today.""               N/A           N/A             N/A
</code></pre>

<p>The task is to predict the label for any given text segment, where each label consists of three parts (action, performed, person), and there may be zero or more labels for a text segment. </p>

<p>There are fifteen classifiers for action, two for performed, and two for person. Annotated data size is 6000 text segments, in which 4000 text segments are assigned at least one label. </p>

<p>What is this type of text classification called (other than multi-class labelling)? </p>

<p>Also, which classification approach is recommended for this type of classification problem? </p>
",Text Classification / Sentiment Analysis,text classification problem name approach type classification labelled data set comprising text segment corresponding label label consists three part multiple zero label assigned given text segment task predict label given text segment label consists three part action performed person may zero label text segment fifteen classifier action two performed two person annotated data size text segment text segment assigned least one label type text classification called multi class labelling also classification approach recommended type classification problem
Approach for classifying job description sentences,"<p>I need to classify/categorize the various sentences in the <code>job_experience</code> section of n=630 job descriptions. I'm particularly interested in extracting the work experience and ability-related sentences, but I need to be able keep them attached to the <code>job_title</code> that they are associated with. </p>

<p><strong>Current state of these job descriptions:</strong> many different ways of saying similar things (e.g., ""Needs Microsoft Office skills."" ""Experience using Microsoft Word, PowerPoint."" ""Minimum 3 years experience in related field."" ""Minimum three years experience in similar role."").</p>

<p><strong>In the future</strong>, we will need to condense these job description statements so that, for example, the same statement can be applied to multiple jobs, and where managers select from a drop-down list of job experience statements.</p>

<p>So I would like to categorize these individual sentences so that we can begin condensing them and deciding on which statements will be used going forward.</p>

<p>I've been researching what I should do and <strong>I would appreciate any suggestions on which approach will be the most efficient.</strong> I am familiar with R but use it mostly for data wrangling and visualization. LDA, kmeans text clustering, feature identification... these are the things I'm finding in my research (scikit-learn.org) and mostly with application in Python. </p>

<ul>
<li>Is Python best for this kind of thing? Can I use R?</li>
<li>Which algorithmic approach is best for a beginner?</li>
<li>I know this isn't magic - just looking for the best approach to this task.</li>
</ul>

<p>My data looks like: </p>

<pre><code>df &lt;- data.frame(job_title = c(""Recruiter"",""Recruiter"",""Recruiter"",""Recruiter"",
                         ""File Clerk"",""File Clerk"",
                         ""Learning &amp; Org. Development Specialist"",""Learning &amp; Org. Development Specialist"",""Learning &amp; Org. Development Specialist"",""Learning &amp; Org. Development Specialist"",
                         ""CNA"",""CNA"",""CNA""),
           job_experience = c(""Minimum 1 year experience in recruitment or related human resources function."",
                              ""Proficient in Microsoft Office Applications."",
                              ""High school diploma required."",
                              ""Bachelors Degree in Human Resources or related field preferred."",
                              ""High School diploma preferred."",
                              ""Ability to use relevant computer systems."",
                              ""Bachelors Degree in related field (e.g., Human Resources, Education, Organizational Development)."",
                              ""Minimum 2 years experience applying L&amp;OD principles and practices in an organizational setting."",
                              ""Previous work experience in Human Resources preferred."",
                              ""Experience with a learning management system (LMS)."",
                              ""High school diploma or GED equivalent."",
                              ""Certified Nursing Assistant, certified by the Virginia Board of Health Professions."",
                              ""CPR certification required at date of hire.""))
</code></pre>

<p>My goal is to have a dataset like this (new column = <code>job_exp_category</code>):</p>

<pre><code>job_title  job_experience                               job_exp_category
""Recruiter""  ""Minimum 1 year experience in recruitment...""  ""Work experience""
""Recruiter""  ""Proficient in Microsoft Office Applicati...""  ""Skill/Ability""
""Recruiter""  ""High school diploma required.""                ""Degree""
...          ...                                            ...   
""CNA""        ""Certified Nursing Assistant, certificati...""  ""Certification/License""
""CNA""        ""CPR certification required at date of hire.""  ""Certification/License""
</code></pre>

<p>Thank you for any insight SO community.</p>
",Text Classification / Sentiment Analysis,approach classifying job description sentence need classify categorize various sentence section n job description particularly interested extracting work experience ability related sentence need able keep attached associated current state job description many different way saying similar thing e g need microsoft office skill experience using microsoft word powerpoint minimum year experience related field minimum three year experience similar role future need condense job description statement example statement applied multiple job manager select drop list job experience statement would like categorize individual sentence begin condensing deciding statement used going forward researching would appreciate suggestion approach efficient familiar r use mostly data wrangling visualization lda kmeans text clustering feature identification thing finding research scikit learn org mostly application python python best kind thing use r algorithmic approach best beginner know magic looking best approach task data look like goal dataset like new column thank insight community
Sentiment analysis for waiting times in hospitals,"<p>I have been trying to infer a timeliness score (a score based on patients' waiting times) from patients' reviews in hospitals. The data available is patients' reviews and their ratings of the overall experience. The problem is I don't have labels for the timeliness scores of the patients but a rating of the whole experience.
reviews like 'I have waited for a very long time but the experience was worth it' which has a good context with a good rating should return a somewhat bad timeliness score, same thing for 'The wait was short but it does not justify the bad service'. So the context/ feeling of the person writing the review also matters. 
I've cleaned the reviews and took the part of the sentences that is relevant to the waiting time(if it exists) and used libraries like TextBlob or VaderSentiment to infer if the sentiment of the patient is positive or not. The problem with that is I had 20% of the cases where I got different results from different libraries and they do not always make the most sense (reviews that have long waiting times but within a good context return a good waiting time but it should at least be neutral ).
I need some sort of way where I can correlate the context of the sentence and the actual rating to how much time they spent waiting and come up with a score, a sort of sentiment analysis that takes into account the whole sentence but looks for a specific context of talking about waiting time.</p>
",Text Classification / Sentiment Analysis,sentiment analysis waiting time hospital trying infer timeliness score score based patient waiting time patient review hospital data available patient review rating overall experience problem label timeliness score patient rating whole experience review like waited long time experience wa worth ha good context good rating return somewhat bad timeliness score thing wait wa short doe justify bad service context feeling person writing review also matter cleaned review took part sentence relevant waiting time exists used library like textblob vadersentiment infer sentiment patient positive problem case got different result different library always make sense review long waiting time within good context return good waiting time least neutral need sort way correlate context sentence actual rating much time spent waiting come score sort sentiment analysis take account whole sentence look specific context talking waiting time
Identifying Grammatically Correct Nonsense Sentences,"<p>I have two files <code>file1.csv</code> and <code>file2.csv</code>. <code>file1.csv</code> contains a <code>stupid</code> sentence in each row. <code>file2.csv</code> identify which column it is (<code>type0</code> corresponding to <code>0</code>, <code>type1</code> corresponding to <code>1</code>). I want to do a NLP classification task and I know usually how to do it. But in this situation I am bit confused and do not know how to arrange and organize my dataset, so that I can train my sentences and labels. Appreciate if someone give me a hint to progress. </p>

<p><code>file1.csv</code> in the following format,</p>

<pre><code>id,type0,type1
0,He married to a dinosaur.,He married to a women.
1,She drinks a beer.,She drinks a banana.
2,He lifted a 500 tons.,He lifted a 50kg.
</code></pre>

<p><code>file2.csv</code> in the following format.</p>

<pre><code>id,stupid
0,0
1,1
2,0
</code></pre>

<p>My purpose is to classify the stupid sentences. </p>
",Text Classification / Sentiment Analysis,identifying grammatically correct nonsense sentence two file contains sentence row identify column corresponding corresponding want nlp classification task know usually situation bit confused know arrange organize dataset train sentence label appreciate someone give hint progress following format following format purpose classify stupid sentence
why an advanced LSTM model result is no better than the result from a simpler one?,"<p>I have implemented the model proposed in <a href=""https://www.aclweb.org/anthology/D15-1167/"" rel=""nofollow noreferrer"">this</a> article which is a text classification model that uses sentence representation rather than only word representation to classify texts.</p>

<pre><code>model=tf.keras.Sequential()
embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=False)
model.add(TimeDistributed(embeding_layer))
model.add(TimeDistributed(tf.keras.layers.LSTM(50)))
model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50)))
model.add(layers.Dense(6,activation='softmax'))
opt=tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',self.f1_m,self.precision_m, self.recall_m])
self.model=model
</code></pre>

<p>and I use a dataset with 40000 documents with 6 different labels to train it. (30000 for train and 10000 for the test). I uses a pretrained word embeding and the input for this model is (sample,sentences,words). it achieves 84% accuracy. the problem is that I can achieve this accuracy very easily with this simple model:</p>

<pre><code>        model=tf.keras.Sequential()
    embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=False)
    model.add(embeding_layer)
    model.add(tf.keras.layers.Bidirectional(layers.LSTM(50)))
    model.add(layers.Dense(6,activation='softmax'))
    opt=tf.keras.optimizers.RMSprop(learning_rate=0.001)
    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',self.f1_m,self.precision_m, self.recall_m])
    self.model=model
</code></pre>

<p>this one is not based on sentence representation and the input for this model is (sample, words).
what is wrong with the first model ? is my implementation wrong? what should I do? </p>

<p>the training process for both models overfits at 84%. I also have used every trick to overcome overfitting but I haven't got any results. any suggestions please?
[![enter image description here][2]][2]</p>
",Text Classification / Sentiment Analysis,advanced lstm model result better result simpler one implemented model proposed article text classification model us sentence representation rather word representation classify text use dataset document different label train train test us pretrained word embeding input model sample sentence word achieves accuracy problem achieve accuracy easily simple model one based sentence representation input model sample word wrong first model implementation wrong training process model overfits also used every trick overcome overfitting got result suggestion please enter image description
How can I know to which class each score corresponds to in LibShortText prediction output file?,"<p>I use <a href=""https://www.csie.ntu.edu.tw/~cjlin/libshorttext/"" rel=""nofollow noreferrer"">LibShortText</a> for short-text classification.</p>

<p>I trained a model and use it to get class predictions on my test set by running:</p>

<pre><code>python text-train.py -L 0 -f ./demo/train_file
python text-predict.py ./demo/train_file train_file.model output
</code></pre>

<p>The <code>output</code> file contains the score of each class for each test sample. She is the beginning of the <code>output</code> file:</p>

<pre><code>version: 1
analyzable: 1
text-src: ./demo/train_file
extra-files:    
model-id: 22d9e6defd38ed92e45662d576262915d10c3374

Tickets Tickets 1.045974012515694   -0.1533289000025808 -0.142460215262256  -0.1530588765291932 -0.1249182478102407 -0.1190708362082807 -0.06841237067728836    0.04587568197139553 -0.2283616562229066 -0.102238591774343
Stamps  Stamps  -0.1187719176481736 1.118188003417143   -0.08034439513604429    -0.1973997029054026 -0.06355109135595602    -0.1786639939826796 -0.1169254102259164 -0.01967861752032143    -0.06964465109882922    -0.2732082235438185
Music   Music   -0.1315596826953709 -0.2641082947449856 1.008713836384851   -0.04068831625284784    -0.1545790157496564 -0.1010212095804389 -0.02069378431571431    -0.02404317930606417    0.008960552873498827    -0.2809809066132714
Jewelry &amp; Watches   Jewelry &amp; Watches   -0.0749032450936907 -0.1369122108940684 -0.2159355702219642 0.9582440549577076  -0.141187218792264  -0.1290355317490395 -0.04287756450848382    -0.0919782002284954 -0.04312539181047169    -0.0822891216592294
Tickets Tickets 0.9291396425612148  -0.1597595507175184 -0.07086077554348413    -0.07087036006347401    -0.1111802245732816 -0.2329161314957608 -0.07080154336497513    -0.07093153970747144    -0.07096098431125453    -0.07085853278399512
Books   Books   -0.03482279197164031    -0.02622229736755784    -0.08576360644172253    -0.1209545478269265 0.9735039690597804  -0.02640896142537765    -0.1511226188239169 -0.1785299152500055 -0.1569282110333412 -0.1927510189192921
Tickets Tickets 1.165624491239117   -0.1643444003616841 -0.279795018266336  -0.05911033737681937    -0.1496733471948844 -0.1774767469424229 -0.1806900189575362 -0.05711408596057094    0.06427848575613292 -0.1616990219349959
Art Art -0.07563152438778584    -0.1926345255861422 -0.1379519287608234 -0.1728869014895525 -0.2081235484009353 0.9764371359082827  -0.06097998223834129    -0.06082239643658216    -0.0434090642865785 -0.0239972643215402
Art Art -0.21374038053991   0.0146962630542977  -0.02279914632208601    -0.001108284295731699   -0.2621058759589903 1.016592310148241   0.01436347343617804 -0.04476369315079338    -0.1246095742882179 -0.3765250920829869
Books   Books   -0.08063364674726788    -0.08053738921453879    -0.08032365427931695    -0.1496633152184083 0.9195583554164264  -0.08011940998873018    -0.08053175336913043    -0.16302082274963   -0.1105339242133948 -0.09419443963601073
</code></pre>

<p>How can I know to which class each score corresponds to? </p>

<p>I know I could infer it by looking at the predicted class and the maximum score for several test samples, but I'm hoping there exist some mmore direct way.</p>
",Text Classification / Sentiment Analysis,know class score corresponds libshorttext prediction output file use libshorttext short text classification trained model use get class prediction test set running file contains score class test sample beginning file know class score corresponds know could infer looking predicted class maximum score several test sample hoping exist mmore direct way
Sentiment analysis using LSTM on imbalanced citation dataset,"<p>I have an extremely unbalanced dataset for sentiment classification. <a href=""https://cl.awaisathar.com/citation-sentiment-corpus/"" rel=""nofollow noreferrer"">https://cl.awaisathar.com/citation-sentiment-corpus/</a></p>

<ul>
<li>Class POSITIVE:829</li>
<li>Class NEGATIVE:280</li>
<li>Class NEUTRAL: 7627</li>
</ul>

<p>Here is my network:</p>

<pre><code>Sentiment_LSTM(
  (embedding): Embedding(5491, 400)
  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=512, out_features=3, bias=True)
  (sig): Sigmoid()
)
</code></pre>

<p>Loss function:</p>

<pre class=""lang-py prettyprint-override""><code>lr=0.001

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(net.parameters(), lr=lr)
</code></pre>

<p>My accuracy is low on the small classes. How can i improve it futher?</p>
",Text Classification / Sentiment Analysis,sentiment analysis using lstm imbalanced citation dataset extremely unbalanced dataset sentiment classification class positive class negative class neutral network loss function accuracy low small class improve futher
New techniques of text classification nlp,"<p>I have to make a text classification program with new techniques (Not using ""bag of words"" and ""TF-IDF"").
I read about EDA but I was confused.
Any ideas?</p>
",Text Classification / Sentiment Analysis,new technique text classification nlp make text classification program new technique using bag word tf idf read eda wa confused idea
is there any way to implement a machine learning model that can predict most occured sentence in a given dataset,"<p>is there any way to implement a machine learning model that can predict the topmost occurred sentences in a given dataset?
let's say I have a dataset that contains sentences like this.</p>

<pre><code>-&gt; this is a machine learning model.
-&gt; this is a machine learning model.
-&gt; A language model can predict the probability of the next word in the sequence.
-&gt; Build a Deep Neural Network for Sentiment Classification.
-&gt; this is a machine learning model.
-&gt; Build a Deep Neural Network for Sentiment Classification.
</code></pre>

<p>so the model should predict results like: 
 this is a machine learning model.
 Build a Deep Neural Network for Sentiment Classification.</p>
",Text Classification / Sentiment Analysis,way implement machine learning model predict occured sentence given dataset way implement machine learning model predict topmost occurred sentence given dataset let say dataset contains sentence like model predict result like machine learning model build deep neural network sentiment classification
How to predict the label after training the dataset in NLP,"<p>I am trying to do sentiment analysis on comments; the data set contains two main colums: the first one is ""review"" which has the reviews of the users, and the second colum is whether it is positive or negative; I got a template from a source to prepocessing the data, the training and testing is okay. However, I want to input a text and want the model to predict whether it is positive or negative. I tried so many forms of the input: string only, list of strings, numpy to array etc. However, I always got erros; any ideas how to input the data to be predicted?
here's my code:</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\t',quoting=3)

import re 
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus=[]
for i in range(0,1000):
    review=re.sub('[^a-zA-Z]',' ',dataset['Review'][i])
    review.lower()
    review=review.split()
    ps=PorterStemmer()
    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review=' '.join(review)
    corpus.append(review)

#the bag of word
from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_features=1500)
X=cv.fit_transform(corpus).toarray()
y=dataset.iloc[:,1].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)



# Fitting Naive Bayes to the Training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Predicting the Test set results
xeval=[""I like it okay""]
prediction=classifier.predict(xeval)```

the error in this case is:
Expected 2D array, got 1D array instead:
array=['I like it okay'].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
",Text Classification / Sentiment Analysis,predict label training dataset nlp trying sentiment analysis comment data set contains two main colums first one review ha review user second colum whether positive negative got template source prepocessing data training testing okay however want input text want model predict whether positive negative tried many form input string list string numpy array etc however always got erros idea input data predicted code
What is a ready to use tool I can use to code survey comments,"<p>I want to code a large number of survey responses that are extracted from a website, I want to use a tool that uses (natural language processing)NLP for text classification and sentiment analysis. I reviewed Microsoft Cognitive and while it seems decent product in the semantic area I am not sure how good it is in text classification. is there is any hustle free tool out there that I can integrate into the website and does the coding automatically? </p>

<p>Thank you</p>
",Text Classification / Sentiment Analysis,ready use tool use code survey comment want code large number survey response extracted website want use tool us natural language processing nlp text classification sentiment analysis reviewed microsoft cognitive seems decent product semantic area sure good text classification hustle free tool integrate website doe coding automatically thank
Is there any way in python to auto-correct spelling mistake in multiple rows of an excel files of a single column?,"<p>I am working on the Sentiment Analysis for a college project. I have an excel file with a ""column"" named ""comments"" and it has ""1000 rows"". The sentences in these rows have spelling mistakes and for the analysis, I need to have them corrected. I don't know how to process this so that I get and column with correct sentences using python code. </p>

<p>All the methods I found were correcting spelling mistakes of a word not sentence and not on the column level with 100s of rows.</p>
",Text Classification / Sentiment Analysis,way python auto correct spelling mistake multiple row excel file single column working sentiment analysis college project excel file column named comment ha row sentence row spelling mistake analysis need corrected know process get column correct sentence using python code method found correcting spelling mistake word sentence column level row
How to deal with a highly imbalanced Issue(Text) classification Dataset?,"<p>Size of the dataset: 81256,
Classes:200,
Range for each class varies from 2757 under a particular class to as low as 10 under particular class.Its highly unbalanced.
How to balance this dataset and what type of algorithm should be used to train the model.
Right now i have used random over sampler for sampling and Linear SVC to train the model.</p>
",Text Classification / Sentiment Analysis,deal highly imbalanced issue text classification dataset size dataset class range class varies particular class low particular class highly unbalanced balance dataset type algorithm used train model right used random sampler sampling linear svc train model
How to train a model to distinguish/categorize words by predefined meanings?,"<p>Semantic analysis in deep learning and NLP is usually about the meaning of a whole sentence, such as sentiment analysis. In many cases, the meaning of a word can be understood by the sentence structure. For example,</p>
<blockquote>
<p>Can you tell this from that?</p>
<p>Can you tell me something about this?</p>
</blockquote>
<p>Is there any established method for training a model by a dataset of</p>
<pre><code>word    meaning_id       sentence
tell    1                Can you tell this from that?
tell    2                Can you tell me something about this?
</code></pre>
<p>Note that the purpose is just to categorize words by predefined meanings/examples.</p>
<p>I use Stanford CoreNLP, but I doubt if there is such a possibility. Any deep learning program is OK.</p>
",Text Classification / Sentiment Analysis,train model distinguish categorize word predefined meaning semantic analysis deep learning nlp usually meaning whole sentence sentiment analysis many case meaning word understood sentence structure example tell tell something established method training model dataset note purpose categorize word predefined meaning example use stanford corenlp doubt possibility deep learning program ok
"Feature importances in linear model text classification, StandardScaler(with_mean=False) yes or no","<p>In a binary text classification with <a href=""https://scikit-learn.org/stable/index.html"" rel=""nofollow noreferrer"">scikit-learn</a> with a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"" rel=""nofollow noreferrer"">SGDClassifier</a> linear model on a TF-IDF representation of a bag-of-words, I want to obtain feature importances per class through the models coefficients. I heard diverging opinions if the columns (features) should be scaled with a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">StandardScaler</a>(with_mean=False) or not for this case.</p>

<p>With sparse data, centering of the data before scaling cannot be done anyway (the with_mean=False part). The <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a> by default also L2 row normalizes each instance already. Based on empirical results such as the self-contained example below, it seems the top features per class make intuitively more sense when not using StandardScaler. For example 'nasa' and 'space' are top tokens for sci.space, and 'god' and 'christians' for talk.religion.misc etc. </p>

<p>Am I missing something? Should StandardScaler(with_mean=False) still be used for obtaining feature importances from a linear model coefficients in such NLP cases? </p>

<p>Are these feature importances without StandardScaler(with_mean=False) in cases like this still somehow unreliable from a theoretical point?</p>

<pre><code># load text from web
from sklearn.datasets import fetch_20newsgroups

newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), 
                                    categories=['sci.space','talk.religion.misc'])
newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), 
                                    categories=['sci.space','talk.religion.misc'])

# setup grid search, optionally use scaling
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler

text_clf = Pipeline([
    ('vect', TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.8)),
    # remove comment below to use scaler
    #('scaler', StandardScaler(with_mean=False)),
    #
    ('clf', SGDClassifier(random_state=0, max_iter=1000))
])

from sklearn.model_selection import GridSearchCV
parameters = {
    'clf__alpha': (0.0001, 0.001, 0.01, 0.1, 1.0, 10.0)
}

# find best model
gs_clf = GridSearchCV(text_clf, parameters, cv=8, n_jobs=-1, verbose=-2)
gs_clf.fit(newsgroups_train.data, newsgroups_train.target)

# model performance, very similar with and without scaling
y_predicted = gs_clf.predict(newsgroups_test.data)
from sklearn import metrics
print(metrics.classification_report(newsgroups_test.target, y_predicted))

# use eli5 to get feature importances, corresponds to the coef_ of the model, only top 10 lowest and highest for brevity of this posting
from eli5 import show_weights
show_weights(gs_clf.best_estimator_.named_steps['clf'], vec=gs_clf.best_estimator_.named_steps['vect'], top=(10, 10))    


# Outputs:

No scaling:
Weight?     Feature
+1.872  god
+1.235  objective
+1.194  christians
+1.164  koresh
+1.149  such
+1.147  jesus
+1.131  christian
+1.111  that
+1.065  religion
+1.060  kent
… 10616 more positive …
… 12664 more negative …
-0.922  on
-0.939  it
-0.976  get
-0.977  launch
-0.994  edu
-1.071  at
-1.098  thanks
-1.117  orbit
-1.210  nasa
-2.627  space 

StandardScaler:
Weight?     Feature
+0.040  such
+0.023  compuserve
+0.021  cockroaches
+0.017  how about
+0.016  com
+0.014  figures
+0.014  inquisition
+0.013  time no
+0.012  long time
+0.010  fellowship
… 11244 more positive …
… 14299 more negative …
-0.011  sherzer
-0.011  sherzer methodology
-0.011  methodology
-0.012  update
-0.012  most of
-0.012  message
-0.013  thanks for
-0.013  thanks
-0.028  ironic
-0.032  &lt;BIAS&gt; 
</code></pre>
",Text Classification / Sentiment Analysis,feature importance linear model text classification standardscaler mean false yes binary text classification scikit learn sgdclassifier linear model tf idf representation bag word want obtain feature importance per class model coefficient heard diverging opinion column feature scaled standardscaler mean false case sparse data centering data scaling done anyway mean false part tfidfvectorizer default also l row normalizes instance already based empirical result self contained example seems top feature per class make intuitively sense using standardscaler example nasa space top token sci space god christian talk religion misc etc missing something standardscaler mean false still used obtaining feature importance linear model coefficient nlp case feature importance without standardscaler mean false case like still somehow unreliable theoretical point
Python NLTK and Pandas - text classifier - (newbie ) - importing my data in a format similar to provided example,"<p>I'm new to text classification, however I get most of the concepts.  In short, I have a list of restaurant reviews in an Excel dataset and I want to use them as my training data.  Where I'm struggling is with the example syntax for importing both the actual review and the classification (1 = pos, 0 = neg) as part of my training dataset.  I understand how to do this if I create my dataset manually in a tuple (i.e., what I have current have #'ed out under train).  Any help is appreciated.  </p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
import pandas as pd

df = pd.read_excel(""reviewclasses.xlsx"")

customerreview= df.customerreview.tolist() #I want this to be what's in 
""train"" below (i.e., ""this is a negative review"")

reviewrating= df.reviewrating.tolist() #I also want this to be what's in 
""train"" below (e.g., 0)

#train = [(""Great place to be when you are in Bangalore."", ""1""),
#  (""The place was being renovated when I visited so the seating was 
limited."", ""0""),
#  (""Loved the ambiance, loved the food"", ""1""),
#  (""The food is delicious but not over the top."", ""0""),
#  (""Service - Little slow, probably because too many people."", ""0""),
#  (""The place is not easy to locate"", ""0""),
#  (""Mushroom fried rice was spicy"", ""1""),
#]

dictionary = set(word.lower() for passage in train for word in 
word_tokenize(passage[0]))

t = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) 
for x in train]

# Step 4 – the classifier is trained with sample data
classifier = nltk.NaiveBayesClassifier.train(t)

test_data = ""The food sucked and I couldn't wait to leave the terrible 
restaurant.""
test_data_features = {word.lower(): (word in 
word_tokenize(test_data.lower())) for word in dictionary}

print (classifier.classify(test_data_features))
</code></pre>
",Text Classification / Sentiment Analysis,python nltk panda text classifier newbie importing data format similar provided example new text classification however get concept short list restaurant review excel dataset want use training data struggling example syntax importing actual review classification po neg part training dataset understand create dataset manually tuple e current ed train help appreciated
How to analyse StackOverflow Q&amp;A for root cause analysis if it expected in the problem?,"<p>Most of the questions on <a href=""https://stackoverflow.com"">SO</a> are about how to do it? Like the question for which solution is sought or this one (<a href=""https://stackoverflow.com/questions/2206712/how-to-get-cpu-id-in-java"">How to get cpu-id in java?</a>) but some of them actually ask to cause of the problem for example (<a href=""https://stackoverflow.com/questions/2483260/why-does-this-fail-without-static-cast"">Why does this fail without static_cast?</a>) or (<a href=""https://stackoverflow.com/questions/9823085/why-do-i-get-a-phpdoc-warning-in-phpstorm-over-this-code"">Why do I get a PHPDoc warning in PhpStorm over this code</a>) that does not necessarily mean it has to be <strong>why</strong> question but the intent is asking for the reason behind the problem. </p>

<p>I want to get the root cause of the problem if it is expected (such as in why questions) and its solution, otherwise for how and what questions- just pick the accepted solution or most-voted ones. </p>

<p>How do you prepare a dataset for such task and how to solve it? I am looking for an idea. A preliminary approach will suffice to proceed. For my background, I have worked on sentiment analysis (using BOW method though), sequential network and have an idea about the latest NLP <a href=""https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"" rel=""nofollow noreferrer"">Transformers</a> model.  </p>
",Text Classification / Sentiment Analysis,analyse stackoverflow q root cause analysis expected problem question actually ask cause problem example doe necessarily mean ha question intent asking reason behind problem want get root cause problem expected question solution otherwise question pick accepted solution voted one prepare dataset task solve looking idea preliminary approach suffice proceed background worked sentiment analysis using bow method though sequential network idea latest nlp transformer model
Spacy text classification scores,"<p>I'm quite new to NLP text classification and trying to apprehend the basics. It seems that Spacy is more suitable for my tasks and experience. I've read through all the docs and run the example code from <a href=""https://spacy.io/usage/training#example-textcat"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#example-textcat</a> with default plac parameters with my own output folder. Then, I wrote a testing file:</p>

<pre><code>import spacy

output_dir=""train_output_orig""

test_text = [
    ""This movie sucked"",
    ""It's a great one"",
    ""I've watched a lot of films of this kind. A lot of them were more attractive for me"",
    ""This is a great movie"",
    ""This movie is terrible"",
    ""I love this movie"",
    ""This is a bad film"",
    ""So fucking dung!"",
    ""Very involving work with developed characters""
    ]
print(""Loading from"", output_dir)
nlp2 = spacy.load(output_dir)
for text in test_text:
    print(text, nlp2(text).cats)
</code></pre>

<p>and got results:</p>

<pre><code>Loading from train_output_orig
This movie sucked {'POSITIVE': 0.6549780368804932}
It's a great one {'POSITIVE': 0.7863456606864929}
I've watched a lot of films of this kind. A lot of them were more attractive for me {'POSITIVE': 0.7664909958839417}
This is a great movie {'POSITIVE': 0.7897435426712036}
This movie is terrible {'POSITIVE': 0.4777064323425293}
I love this movie {'POSITIVE': 0.7530838847160339}
This is a bad film {'POSITIVE': 0.46895521879196167}
So fucking dung! {'POSITIVE': 0.6296740174293518}
Very involving work with developed characters {'POSITIVE': 0.8538092970848083}
</code></pre>

<p>Is it OK for Spacy model, or have I done something wrong? I mean there's quite narrow frontier between ""positive"" and ""negative"" labels. Even definitive ""This is a bad film"" earned 0.46 of ""positive"" rating. ""I love this movie"" got only 0.75 while ""Very involving work with developed characters"" got 0.83. At the same time, suggested in the original Spacy usage docs phrase ""This movie sucked"" got 0.65 ""positive"" score!</p>

<p>Thank you in advance for your answer</p>
",Text Classification / Sentiment Analysis,spacy text classification score quite new nlp text classification trying apprehend basic seems spacy suitable task experience read doc run example code default plac parameter output folder wrote testing file got result ok spacy model done something wrong mean quite narrow frontier positive negative label even definitive bad film earned positive rating love movie got involving work developed character got time suggested original spacy usage doc phrase movie sucked got positive score thank advance answer
Distant Supervision: a rule-based labelling approach?,"<p>I am currently working on entity relations stuff and I found out that a lot of papers implemented distant supervision to label the data. What I understand about distant supervision is that we have an established Knowledge Base (KB) and we do kind of ""rule-based labeling"" by checking the extracted entity pairs whether they exist in the KB or not. If the entity pair exist in KB, it will be labelled as positive, otherwise it will be labelled as negative. </p>

<p>My questions are:</p>

<ol>
<li>Do I understand this distant supervision concept correctly?</li>
<li>If yes, I don't understand why do we train neural networks to classify rule-based system? For example, if in the future we get new sentences that contain entities and we want to check if they have relation to each other, why don't we just refer back to the KB? Why do we train entity relation instead?</li>
</ol>

<p>Thank you</p>
",Text Classification / Sentiment Analysis,distant supervision rule based labelling approach currently working entity relation stuff found lot paper implemented distant supervision label data understand distant supervision established knowledge base kb kind rule based labeling checking extracted entity pair whether exist kb entity pair exist kb labelled positive otherwise labelled negative question understand distant supervision concept correctly yes understand train neural network classify rule based system example future get new sentence contain entity want check relation refer back kb train entity relation instead thank
How do I go from Pandas DataFrame to Tensorflow BatchDataset for NLP?,"<p>I'm honestly trying to figure out how to convert a dataset (format: pandas <code>DataFrame</code> or numpy array) to a form that a simple text-classification tensorflow model can train on for sentiment analysis. The dataset I'm using is similar to IMDB (containing both text and labels (positive or negative)). Every tutorial I've looked at has either prepared data differently, or didn't bother with data preparation and left it to your imagination. (For instance, all the IMDB tutorials import a preprocessed Tensorflow <code>BatchDataset</code> from <code>tensorflow_datasets</code>, which isn't helpful when I'm using my own set of data). My own attempts to convert a Pandas <code>DataFrame</code> to Tensorflow's <code>Dataset</code> types have resulted in ValueErrors or a negative loss during training. Any help would be appreciated.</p>

<p>I had originally prepared my data as follows, where <code>training</code> and <code>validation</code> are already shuffled Pandas <code>DataFrame</code>s containing <code>text</code> and <code>label</code> columns:</p>

<pre><code># IMPORT STUFF

from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf # (I'm using tensorflow 2.0)
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
import pandas as pd
import numpy as np
# ... [code for importing and preparing the pandas dataframe omitted]

# TOKENIZE

train_text = training['text'].to_numpy()
tok = Tokenizer(oov_token='&lt;unk&gt;')
tok.fit_on_texts(train_text)
tok.word_index['&lt;pad&gt;'] = 0
tok.index_word[0] = '&lt;pad&gt;'

train_seqs = tok.texts_to_sequences(train_text)
train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')

train_labels = training['label'].to_numpy().flatten()

valid_text = validation['text'].to_numpy()
valid_seqs = tok.texts_to_sequences(valid_text)
valid_seqs = tf.keras.preprocessing.sequence.pad_sequences(valid_seqs, padding='post')

valid_labels = validation['label'].to_numpy().flatten()

# CONVERT TO TF DATASETS

train_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))
valid_ds = tf.data.Dataset.from_tensor_slices((valid_seqs,valid_labels))

train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
valid_ds = valid_ds.batch(BATCH_SIZE)

# PREFETCH

train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
valid_ds = valid_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
</code></pre>

<p>This resulted train_ds and valid_ds being tokenized and of type <code>PrefetchDataset</code> or <code>&lt;PrefetchDataset shapes: ((None, None, None, 118), (None, None, None)), types: (tf.int32, tf.int64)&gt;</code>.</p>

<p>I then trained as follows, but got a <em>large negative loss and an accuracy of 0</em>.</p>

<pre><code>model = keras.Sequential([
    layers.Embedding(vocab_size, embedding_dim),
    layers.GlobalAveragePooling1D(),
    layers.Dense(1, activation='sigmoid') # also tried activation='softmax'
])

model.compile(optimizer='adam',
              loss='binary_crossentropy', # binary_crossentropy
              metrics=['accuracy'])

history = model.fit(
    train_ds,
    epochs=1,
    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)
</code></pre>

<p>If I don't do the fancy prefetch stuff, <code>train_ds</code> would be of type <code>BatchDataset</code> or <code>&lt;BatchDataset shapes: ((None, 118), (None,)), types: (tf.int32, tf.int64)&gt;</code>, but that also is getting me a negative loss and an accuracy of 0.</p>

<p>And if I just do the following: </p>

<pre><code>x, y = training['text'].to_numpy(), training['label'].to_numpy()
x, y = tf.convert_to_tensor(x),tf.convert_to_tensor(y)
</code></pre>

<p>then <code>x</code> and <code>y</code> are of type <code>EagerTensor</code>, but I can't seem to figure out how to Batch an <code>EagerTensor</code>.</p>

<p><strong>What types and shapes do I really need for <code>train_ds</code>? What am I missing or doing wrong?</strong> </p>

<p>The <a href=""https://www.tensorflow.org/tutorials/keras/text_classification_with_hub"" rel=""noreferrer"">text_classification_with_hub tutorial</a> trains an already prepared imdb dataset as shown:</p>

<pre><code>model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=20,
                    validation_data=validation_data.batch(512),
                    verbose=1)
</code></pre>

<p>In this example, <code>train_data</code> is of form <code>tensorflow.python.data.ops.dataset_ops._OptionsDataset</code>, and <code>train_data.shuffle(1000).batch(512)</code> is <code>tensorflow.python.data.ops.dataset_ops.BatchDataset</code> (or <code>&lt;BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int64)&gt;</code>). </p>

<p>They apparently didn't bother with tokenization with this dataset, but I doubt tokenization is my issue. Why does their <code>train_data.shuffle(10000).batch(512)</code> work but my <code>train_ds</code> not work? </p>

<p>It's possible the issue is with the model setup, the <code>Embedding</code> layer, or with tokenization, but I'm not so sure that's the case. I've already looked at the following tutorials for inspiration:</p>

<p><a href=""https://www.tensorflow.org/tutorials/keras/text_classification_with_hub"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/keras/text_classification_with_hub</a></p>

<p><a href=""https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow"" rel=""noreferrer"">https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow</a></p>

<p><a href=""https://www.tensorflow.org/tutorials/text/image_captioning"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/text/image_captioning</a></p>

<p><a href=""https://www.tensorflow.org/tutorials/text/word_embeddings#learning_embeddings_from_scratch"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/text/word_embeddings#learning_embeddings_from_scratch</a></p>

<p><a href=""https://thedatafrog.com/word-embedding-sentiment-analysis/"" rel=""noreferrer"">https://thedatafrog.com/word-embedding-sentiment-analysis/</a></p>
",Text Classification / Sentiment Analysis,go panda dataframe tensorflow batchdataset nlp honestly trying figure convert dataset format panda numpy array form simple text classification tensorflow model train sentiment analysis dataset using similar imdb containing text label positive negative every tutorial looked ha either prepared data differently bother data preparation left imagination instance imdb tutorial import preprocessed tensorflow helpful using set data attempt convert panda tensorflow type resulted valueerrors negative loss training help would appreciated originally prepared data follows already shuffled panda containing column resulted train valid tokenized type trained follows got large negative loss accuracy fancy prefetch stuff would type also getting negative loss accuracy following type seem figure batch type shape really need missing wrong text classification hub tutorial train already prepared imdb dataset shown example form apparently bother tokenization dataset doubt tokenization issue doe work work possible issue model setup layer tokenization sure case already looked following tutorial inspiration
How to check if a text has request for information,"<p>I have been trying to see if there are ways to classify the text if it is requesting for information. I feel like NLP is the way, is there a better approach?</p>

<p>For example:</p>

<ol>
<li>Hi, Can you share your school name? &lt;&lt;-- Yes</li>
<li>What is your school name? &lt;&lt;-- Yes</li>
<li>My address is XYZ. &lt;&lt;-- No</li>
<li>What is your PIN? &lt;&lt;-- Yes</li>
</ol>

<p>Thanks,
Vinod.</p>
",Text Classification / Sentiment Analysis,check text ha request information trying see way classify text requesting information feel like nlp way better approach example hi share school name yes school name yes address xyz pin yes thanks vinod
Check skills of a classifier in scikit learn,"<p>After training a classifier, I tried passing a few sentences to check if it is going to classify it correctly.</p>
<p>During that testing the results are not appearing well.</p>
<p>I suppose some variables are not correct.</p>
<p><strong>Explanation</strong></p>
<p><strong>I have a dataframe called <code>df</code> that looks like this:</strong></p>
<pre><code>                                              news        type
0   From: mathew &lt;mathew@mantis.co.uk&gt;\n Subject: ...   alt.atheism
1   From: mathew &lt;mathew@mantis.co.uk&gt;\n Subject: ...   alt.space
2   From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...   alt.tech
                                                            ...
#each row in the news column is a document
#each row in the type column is the category of that document
</code></pre>
<p><strong>Preprocessing:</strong></p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn import metrics

vectorizer = TfidfVectorizer( stop_words = 'english')
   
vectors = vectorizer.fit_transform(df.news)
clf =  SVC(C=10,gamma=1,kernel='rbf')

clf.fit(vectors, df.type)
vectors_test = vectorizer.transform(df_test.news)
pred = clf.predict(vectors_test)
</code></pre>
<p><strong>Attempt to check how some sentences are classified</strong></p>
<pre><code>texts = [&quot;The space shuttle is made in 2018&quot;, 
         &quot;stars are shining&quot;,
         &quot;galaxy&quot;]
text_features = vectorizer.transform(texts)
predictions = clf.predict(text_features)
for text, predicted in zip(texts, predictions):
   print('&quot;{}&quot;'.format(text))
   print(&quot;  - Predicted as: '{}'&quot;.format(df.type[pred]))

   print(&quot;&quot;)
</code></pre>
<p><strong>The problem is that it returns this:</strong></p>
<pre><code>&quot;The space shuttle is made in 2018&quot;
  - Predicted as: 'alt.atheism    NaN
alt.atheism    NaN
alt.atheism    NaN
alt.atheism    NaN
alt.atheism    NaN
</code></pre>
<p>What do you think?</p>
<h1>EDIT</h1>
<p>Example</p>
<p>This is kind of how it should look like :</p>
<pre><code>&gt;&gt;&gt; docs_new = ['God is love', 'OpenGL on the GPU is fast']
&gt;&gt;&gt; X_new_counts = count_vect.transform(docs_new)
&gt;&gt;&gt; X_new_tfidf = tfidf_transformer.transform(X_new_counts)

&gt;&gt;&gt; predicted = clf.predict(X_new_tfidf)

&gt;&gt;&gt; for doc, category in zip(docs_new, predicted):
...     print('%r =&gt; %s' % (doc, twenty_train.target_names[category]))
...
'God is love' =&gt; soc.religion.christian
'OpenGL on the GPU is fast' =&gt; comp.graphics
</code></pre>
",Text Classification / Sentiment Analysis,check skill classifier scikit learn training classifier tried passing sentence check going classify correctly testing result appearing well suppose variable correct explanation dataframe called look like preprocessing attempt check sentence classified problem return think edit example kind look like
How to test NLP model against many strings,"<p>I have trained a classifier model using logistic regression on a set of strings that classifies strings into 0 or 1. I currently have it where I can only test one string at a time. How can I have my model run through more than one sentence at a time, maybe from a .csv file so I dont have to input each sentence individually?</p>

<p>def train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y, is_neural_net=False):
        classifier.fit(feature_vector_train, label)</p>

<pre><code># predict the labels on validation dataset
predictions = classifier.predict(feature_vector_valid)

if is_neural_net:
    predictions = predictions.argmax(axis=-1)

return classifier , metrics.accuracy_score(predictions, valid_y)
</code></pre>

<p>then</p>

<pre><code>model, accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count,test_y)
</code></pre>

<p>Currently how I test my model</p>

<pre><code>sent = ['here I copy a string'] 
</code></pre>

<p>converting text to count bag of words vectors</p>

<pre><code>count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}',ngram_range=(1, 2))
x_feature_vector =  count_vect.transform(sent)
pred = model.predict(x_feature_vector)
</code></pre>

<p>and I get the sentence and its prediction </p>

<p>I wanted the model to classify all my new sentences at once and give a classification to each sentence.</p>
",Text Classification / Sentiment Analysis,test nlp model many string trained classifier model using logistic regression set string classifies string currently test one string time model run one sentence time maybe csv file dont input sentence individually def train model classifier feature vector train label feature vector valid valid neural net false classifier fit feature vector train label currently test model converting text count bag word vector get sentence prediction wanted model classify new sentence give classification sentence
Keras LSTM model giving different predictions on same input when size of input has changed?,"<p>I am building a LSTM for text classification in with Keras, and am playing around with different input sentences to get a sense of what is happening, but I'm getting strange outputs. For example:</p>

<p>Sentence 1 = ""On Tuesday, Ms. [Mary] Barra, 51, completed a remarkable personal odyssey when she was named as the next chief executive of G.M.--and the first woman to ascend to the top job at a major auto company.""</p>

<p>Sentence 2 = ""On Tuesday, Ms. [Mary] Barra, 51,  was named as the next chief executive of G.M.--and the first woman to ascend to the top job at a major auto company.""</p>

<p>The model predicts the class ""objective"" (0), output 0.4242 when the Sentence 2 is the only element in the input array.  It predicts ""subjective"" (1), output 0.9061 for Sentence 1.  If they are both (as separate strings) fed as input in the same array, both are classified as ""subjective"" (1) - but Sentence 1 outputs 0.8689 and 2 outputs 0.5607.  It seems as though they are affecting each other's outputs.  It does not matter which index in the input array each sentence is.</p>

<p>Here is the code:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>max_length = 500

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=5000, lower=True,split=' ')
tokenizer.fit_on_texts(dataset[""sentence""].values)
#print(tokenizer.word_index)  # To see the dicstionary
X = tokenizer.texts_to_sequences(dataset[""sentence""].values)
X = pad_sequences(X, maxlen=max_length)

y = np.array(dataset[""label""])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)

import numpy
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
# fix random seed for reproducibility
numpy.random.seed(7)


X_train = sequence.pad_sequences(X_train, maxlen=max_length)
X_test = sequence.pad_sequences(X_test, maxlen=max_length)
embedding_vector_length = 32

###LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
model = Sequential()
model.add(Embedding(5000, embedding_vector_length, input_length=max_length))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='sigmoid'))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(100))
model.add(Dense(1, activation='sigmoid'))
from keras import optimizers
sgd = optimizers.SGD(lr=0.9)
model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
print(model.summary())

model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)

# save model
model.save('LSTM.h5')</code></pre>
</div>
</div>
</p>

<p>I then reloaded the model in a separate script and am feeding it hard-coded sentences:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>model = load_model('LSTM.h5')

max_length = 500

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=5000, lower=True,split=' ')
tokenizer.fit_on_texts(article_sentences)
#print(tokenizer.word_index)  # To see the dicstionary
X = tokenizer.texts_to_sequences(article_sentences)
X = pad_sequences(X, maxlen=max_length)

prediction = model.predict(X)
print(prediction)
for i in range(len(X)):
    print('%s\nLabel:%d' % (article_sentences[i], prediction[i]))</code></pre>
</div>
</div>
</p>

<p>I set the random seed before training the model and in the script where I load the model, am I missing something when loading the model?  Should I be arranging my data differently? </p>
",Text Classification / Sentiment Analysis,kera lstm model giving different prediction input size input ha changed building lstm text classification kera playing around different input sentence get sense happening getting strange output example sentence tuesday mary barra completed remarkable personal odyssey wa named next chief executive g first woman ascend top job major auto company sentence tuesday mary barra wa named next chief executive g first woman ascend top job major auto company model predicts class objective output sentence element input array predicts subjective output sentence separate string fed input array classified subjective sentence output output seems though affecting output doe matter index input array sentence code reloaded model separate script feeding hard coded sentence set random seed training model script load model missing something loading model arranging data differently
How to distinguish the direction of important features from xgboost or random forest?,"<p>I'm now working on binary text classification problem (like sentiment analysis), and it's trivial to pull out top important features of xgboost or random forest just by <code>feature_importances_</code></p>

<p>Suppose we have two labelling 1 and 0 for this classification problem. Then there's any way to print out the direction of the features (positive or negative)? Say, word feature A has an enrichment or high tfidf with labelling 1.</p>

<p>Certainly I could pull out the tfidf column of this specific word feature, and correlate with the labelling with pearson coefficient, and the +/- of coefficient would indicate the direction, right? Any other more elegant way for this or xgboost and random forest has built-in such functions. (I didn't find)</p>

<p>Thanks</p>
",Text Classification / Sentiment Analysis,distinguish direction important feature xgboost random forest working binary text classification problem like sentiment analysis trivial pull top important feature xgboost random forest suppose two labelling classification problem way print direction feature positive negative say word feature ha enrichment high tfidf labelling certainly could pull tfidf column specific word feature correlate labelling pearson coefficient coefficient would indicate direction right elegant way xgboost random forest ha built function find thanks
Multiclass text classification with python and nltk,"<p>I am given a task of classifying a given news text data into one of the following 5 categories - Business, Sports, Entertainment, Tech and Politics
<br/><br/>
About the data I am using:<br/>
Consists of text data labeled as one of the 5 types of news statement (Bcc news data)<br/><br/> 
I am currently using NLP with nltk module to calculate the frequency distribution of every word in the training data with respect to each category(except the stopwords).<br/><br/>
Then I classify the new data by calculating the sum of weights of all the words with respect to each of those 5 categories. The class with the most weight is returned as the output.<br/><br/>
Heres the actual <a href=""https://github.com/sujaybr/NewsClassifier/blob/master/bccNewsReal.py"" rel=""nofollow noreferrer"">code</a>.<br/><br/>
This algorithm does predict new data accurately but I am interested to know about some other simple algorithms that I can implement to achieve better results. I have used Naive Bayes algorithm to classify data into two classes (spam or not spam etc) and would like to know how to implement it for multiclass classification if it is a feasible solution.<br/><br/>
Thank you.</p>
",Text Classification / Sentiment Analysis,multiclass text classification python nltk given task classifying given news text data one following category business sport entertainment tech politics data using consists text data labeled one type news statement bcc news data currently using nlp nltk module calculate frequency distribution every word training data respect category except stopwords classify new data calculating sum weight word respect category class weight returned output actual code algorithm doe predict new data accurately interested know simple algorithm implement achieve better result used naive bayes algorithm classify data two class spam spam etc would like know implement multiclass classification feasible solution thank
How to get Googlenews links for a custom query entered,"<p>I want to write a code in Python 3 that would search Google News with a different query every time.
All the resulting links are to be stored in a list and I plan to scrape those links further for sentiment analysis.
I am using the following GoogleNews package and the following code to get the links, but I am not getting any result. I am getting an empty list. </p>

<pre><code>from GoogleNews import GoogleNews
googlenews = GoogleNews()
googlenews.search('AAPL')
googlenews.getlinks()
</code></pre>

<p><a href=""https://i.sstatic.net/JzvuN.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",Text Classification / Sentiment Analysis,get googlenews link custom query entered want write code python would search google news different query every time resulting link stored list plan scrape link sentiment analysis using following googlenews package following code get link getting result getting empty list enter image description
Treating missing values in sentiment analysis,"<p>I have a medium-sized dataset of encrypted comments and their corresponding labels, which is either of positive or negative. I wonder what the best way is to treat the missing comments given that the missing comments rate is 1%. Below is a toy example of the dataset after applying an extensive data cleaning step.</p>

<pre><code>df=pd.DataFrame({'comments':['xxy uuicz', '', 'jiko bhht'], 'label':['positive', 'negative', 'negative']})

</code></pre>

<p>I am using Gensim (preprocess_string) and removing stopwords through building a customized list of stopwords. The goal is to fit a classifier to predict the sentiment of any given encrypted comment. </p>
",Text Classification / Sentiment Analysis,treating missing value sentiment analysis medium sized dataset encrypted comment corresponding label either positive negative wonder best way treat missing comment given missing comment rate toy example dataset applying extensive data cleaning step using gensim preprocess string removing stopwords building customized list stopwords goal fit classifier predict sentiment given encrypted comment
"Review data sentiment analysis, focusing on extracting negative sentiment?","<p>I am trying to do sentiment analysis on a review dataset. Since I care more about identifying (extracting) negative sentiments in reviews (unlabeled now but I try to manually label a few hundreds or use Alchemy API), if the review is overall neutral or positive but a part has negative sentiment, I'd like my model to consider it more toward as a negative review. Could someone give me advices on how to do this? I'm thinking about using bag of words/word2vect with supervised (random forest, SVM) /unsupervised learning models (Kmeans).</p>
",Text Classification / Sentiment Analysis,review data sentiment analysis focusing extracting negative sentiment trying sentiment analysis review dataset since care identifying extracting negative sentiment review unlabeled try manually label hundred use alchemy api review overall neutral positive part ha negative sentiment like model consider toward negative review could someone give advice thinking using bag word word vect supervised random forest svm unsupervised learning model kmeans
Accuracy not increasing with BERT Large model,"<p>I used both <code>BERT_base_cased</code> and <code>BERT_large_Cased</code> model for multi class text classification. With BERT_base_cased, I got satisfactory results. When I tried with BERT_large_cased model, the accuracy is same for all the epochs</p>

<p><a href=""https://i.sstatic.net/T31Yg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/T31Yg.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/CTiiO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CTiiO.png"" alt=""enter image description here""></a></p>

<p>With <code>BERT_base_cased</code>, there is no such problem. But with <code>BERT_large_cased</code>, why accuracy is same in all the epochs? Any help is really appreciated.............</p>
",Text Classification / Sentiment Analysis,accuracy increasing bert large model used model multi class text classification bert base cased got satisfactory result tried bert large cased model accuracy epoch problem accuracy epoch help really appreciated
Sentiment analysis for Twitter in Python,"<p>I'm looking for an open source implementation, preferably in python, of <strong>Textual Sentiment Analysis</strong> (<a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Sentiment_analysis</a>). Is anyone familiar with such open source implementation I can use?</p>

<p>I'm writing an application that searches twitter for some search term, say ""youtube"", and counts ""happy"" tweets vs. ""sad"" tweets. 
I'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.
I haven't been able to find such sentiment analyzer so far, specifically not in python. 
Are you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.</p>

<p>Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.</p>

<p>BTW, twitter does support the "":)"" and "":("" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.</p>

<p>Thanks!</p>

<p>BTW, an early demo is <a href=""http://twitgraph.appspot.com/?show_inputs=1&amp;duration=30&amp;q=youtube+annotations"" rel=""noreferrer"">here</a> and the code I have so far is <a href=""http://code.google.com/p/twitgraph/"" rel=""noreferrer"">here</a> and I'd love to opensource it with any interested developer.</p>
",Text Classification / Sentiment Analysis,sentiment analysis twitter python looking open source implementation preferably python textual sentiment analysis anyone familiar open source implementation use writing application search twitter search term say youtube count happy tweet v sad tweet using google appengine python like able classify returned search result twitter like python able find sentiment analyzer far specifically python familiar open source implementation use preferably already python hopefully translate python note text analyzing short tweet ideally classifier optimized short text btw twitter doe support operator search aim unfortunately classification provided great figured might give try thanks btw early demo code far love opensource interested developer
Text Classification - what can you do vs. what are your capabilities?,"<p>Text Classification basically works on the input training sentences. Little or less number of variations of in the sentences do work. But when there is a scenario like</p>

<blockquote>
  <p>What can you do &lt;&lt;==>> What are your capabilities</p>
</blockquote>

<p>This scenario does not work well with the regular classification or bot building platforms.</p>

<p>Are there any approaches for classification that would help me achieve this ?</p>
",Text Classification / Sentiment Analysis,text classification v capability text classification basically work input training sentence little le number variation sentence work scenario like capability scenario doe work well regular classification bot building platform approach classification would help achieve
Is there any dataset which says whether a given sentence is a question or not?,"<p>I want to know if there are any publicly available datasets which contain questions and other sentences. I need it to build a question classifier -> a classifier which says whether a given sentence is a question or not. 
If there is no such dataset, what's the best way to go about building one? </p>

<p>I tried looking up online but could only find datasets which have different categories for a question or other text classification. </p>

<p><strong>EDIT</strong>
I ended up using the CoQA dataset. I broke down the story sentences as non questions and used the default questions as questions.</p>
",Text Classification / Sentiment Analysis,dataset say whether given sentence question want know publicly available datasets contain question sentence need build question classifier classifier say whether given sentence question dataset best way go building one tried looking online could find datasets different category question text classification edit ended using coqa dataset broke story sentence non question used default question question
How to feed texts which include labeled sentences into the neural network?,"<p>I'm kind of new in deep learning.
I'm trying to get titles for texts.</p>

<p>So, for each text I have:</p>

<ul>
<li><p>Title</p></li>
<li><p>Sentence</p></li>
<li><p>Labels for these sentences (all are 0s and one (which is closest to the title by ROUGE1 score) is 1).
Like <code>['Title'], ['s1','s2','s3','s4'], [0,1,0,0]</code></p></li>
</ul>

<p>I vectorized everything (titles and sentences, obviously).</p>

<p>The problem is I cannot wrap my head around how to feed it to the network so it would learn to predict for bunch of sentences (one text) which of the sentences would be 1 (the title).</p>

<p>I mean... I know when we do, for example, text classification (or sentence classification) we have the whooooole text and its label.. So we just feed to the network something like</p>

<pre><code>x = ['t1','t2','t3','t4','t5']
y= [0,1,0,0,1]
</code></pre>

<p>But here.... we need to predict labels for each sentence in terms of 1 text.
So, I'll have something like</p>

<pre><code>x = 
['s01','s02','s03','s04'] 
['s11','s12','s13','s14','s15','s16','s17'] 
...
['sn1','sn2','sn3']
y = 
[0,1,0,0]
[0,0,0,0,1,0,0]
...
[1,0,0]
</code></pre>

<p>Plus there somewhere must be titles included.... (i think)</p>

<p>How would (in which form) I feed it to the network if I want to predict later for new text (let's say) <code>['s1','s2','s3','s4','s5','s6']</code> which sentences are 0s and which is 1 ?</p>
",Text Classification / Sentiment Analysis,feed text include labeled sentence neural network kind new deep learning trying get title text text title sentence label sentence one closest title rouge score like vectorized everything title sentence obviously problem wrap head around feed network would learn predict bunch sentence one text sentence would title mean know example text classification sentence classification whooooole text label feed network something like need predict label sentence term text something like plus somewhere must title included think would form feed network want predict later new text let say sentence
How to Classify text based on common words,"<p>this question is about classification of texts based on common words, I don't know if I am approaching the problem right 
I have an excel with texts in the ""Description"" column and a unique ID in the ""ID"" column, I want to iterate through Descriptions and compare them based on percentage or frequency of common words in the text I would like to classify descriptions and give them another ID. Please see example below .... </p>

<pre><code>    #importing pandas as pd 
    import pandas as pd 

     # creating a dataframe 
     df = pd.DataFrame({'ID': ['12 ', '54', '88','9'], 
    'Description': ['Staphylococcus aureus is a Gram-positive, round-shaped 
     bacterium that is a member of the Firmicutes', 'Streptococcus pneumoniae, 
    or pneumococcus, is a Gram-positive, alpha-hemolytic or beta-hemolytic', 
    'Dicyemida, also known as Rhombozoa, is a phylum of tiny parasites ','A 
    television set or television receiver, more commonly called a television, 
    TV, TV set, or telly']}) 
</code></pre>

<pre><code>ID     Description
12  Staphylococcus aureus is a Gram-positive, round-shaped bacterium that is a member of the Firmicutes
54  Streptococcus pneumoniae, or pneumococcus, is a Gram-positive, round-shaped bacterium that is a member beta-hemolytic
88  Dicyemida, also known as Rhombozoa, is a phylum of tiny parasites
9   A television set or television receiver, more commonly called a television, TV, TV set, or telly
</code></pre>

<p>for example 12 and 54 Descriptions have more than 75% common words they 
    will have same ID. output would be like :</p>

<pre><code>ID     Description
12  Staphylococcus aureus is a Gram-positive, round-shaped bacterium that 
is a member of the Firmicutes
12  Streptococcus pneumoniae, or pneumococcus, is a Gram-positive, round- 
shaped bacterium that is a member beta-hemolytic
88  Dicyemida, also known as Rhombozoa, is a phylum of tiny parasites
9   A television set or television receiver, more commonly called a 
television, TV, TV set, or telly
</code></pre>

<p>Here what I tried,I worked with two different dataframes Risk1 &amp; Risk2, I'm not iterating throught rows which I need to do too :</p>

<pre><code>import codecs
import re
import copy
import collections
import pandas as pd
import numpy as np
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import WordPunctTokenizer
import matplotlib.pyplot as plt

%matplotlib inline

nltk.download('stopwords')

from nltk.corpus import stopwords

# creating a dataframe 1
 df = pd.DataFrame({'ID': ['12 '], 
'Description': ['Staphylococcus aureus is a Gram-positive, round-shaped 
 bacterium that is a member of the Firmicutes']})
# creating a dataframe 2
 df = pd.DataFrame({'ID': ['54'], 
'Description': ['Streptococcus pneumoniae, 
or pneumococcus, is a Gram-positive, alpha-hemolytic or beta-hemolytic']})

esw = stopwords.words('english')
esw.append('would')

word_pattern = re.compile(""^\w+$"")

def get_text_counter(text):
    tokens = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))
    tokens = list(map(lambda x: x.lower(), tokens))
    tokens = [token for token in tokens if re.match(word_pattern, token) and token not in esw]
return collections.Counter(tokens), len(tokens)

def make_df(counter, size):
    abs_freq = np.array([el[1] for el in counter])
    rel_freq = abs_freq / size
    index = [el[0] for el in counter]
    df = pd.DataFrame(data = np.array([abs_freq, rel_freq]).T, index=index, columns=['Absolute Frequency', 'Relative Frequency'])
    df.index.name = 'Most_Common_Words'
return df

Risk1_counter, Risk1_size = get_text_counter(Risk1)
make_df(Risk1_counter.most_common(500), Risk1_size)

Risk2_counter, Risk2_size = get_text_counter(Risk2)
make_df(Risk2_counter.most_common(500), Risk2_size)

all_counter = Risk1_counter + Risk2_counter
all_df = make_df(Risk2_counter.most_common(1000), 1)
most_common_words = all_df.index.values


df_data = []
for word in most_common_words:
    Risk1_c = Risk1_counter.get(word, 0) / Risk1_size
    Risk2_c = Risk2_counter.get(word, 0) / Risk2_size
    d = abs(Risk1_c - Risk2_c)
    df_data.append([Risk1_c, Risk2_c, d])
dist_df= pd.DataFrame(data = df_data, index=most_common_words,
                    columns=['Risk1 Relative Freq', 'Risk2 Hight Relative Freq','Relative Freq Difference'])
dist_df.index.name = 'Most Common Words'
dist_df.sort_values('Relative Freq Difference', ascending = False, inplace=True)


dist_df.head(500) 
</code></pre>
",Text Classification / Sentiment Analysis,classify text based common word question classification text based common word know approaching problem right excel text description column unique id id column want iterate description compare based percentage frequency common word text would like classify description give another id please see example example description common word id output would like tried worked two different dataframes risk risk iterating throught row need
Update qdap Dictionary for Sentiment Analysis,"<p>I am using <code>polarity</code> function from <code>qdap</code>. There are few words that I want to add to dictionary as negative when said in combination. For instance.</p>

<blockquote>
  <p>""Pretty Bad""</p>
</blockquote>

<p>The polarity score becomes neutral when this is sent into polarity function.</p>

<pre><code>&gt; polarity(""Pretty Bad"")
  all total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
1 all               1           2            0          NA                 NA
</code></pre>

<p>Because it considers pretty as good word and bad as bad one, hence the aggregate becomes neutral.</p>

<p>I want to get rid of this and want to add couple of custom words.</p>
",Text Classification / Sentiment Analysis,update qdap dictionary sentiment analysis using function word want add dictionary negative said combination instance pretty bad polarity score becomes neutral sent polarity function considers pretty good word bad bad one hence aggregate becomes neutral want get rid want add couple custom word
Visualization and Clustering in Python,"<p>I would like to classify comments based on NLP algorithm (tf-idf). 
I managed to classify these clusters but I want to visualize them graphically (histogram, scatter plot...)</p>

<pre><code>import collections
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from pprint import pprint
import matplotlib.pyplot as plt
import pandas as pd
import nltk
import pandas as pd
import string
data = pd.read_excel (r'C:\Users\cra\One\intern\Book2.xlsx') 
def word_tokenizer(text):
        #tokenizes and stems the text
        tokens = word_tokenize(text)  
        stemmer = PorterStemmer() 
        tokens = [stemmer.stem(t) for t in tokens if t not in 
        stopwords.words('english')] 
        return tokens 

#tfidf convert text data to vectors 

def cluster_sentences(sentences, nb_of_clusters=5):
        tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,

        stop_words=stopwords.words('english'),#enlever stopwords
                                        max_df=0.95,min_df=0.05, 
           lowercase=True) 

        tfidf_matrix = tfidf_vectorizer.fit_transform(sentences) 
        kmeans = KMeans(n_clusters=nb_of_clusters)
        kmeans.fit(tfidf_matrix)
        clusters = collections.defaultdict(list)
        for i, label in enumerate(kmeans.labels_):
                clusters[label].append(i)
        return dict(clusters)
if __name__ == ""__main__"":
         sentences = data.Comment
        nclusters= 20
        clusters = cluster_sentences(sentences, nclusters) #dictionary of 
        #cluster and the index of the comment in the dataframe
        for cluster in range(nclusters):
                print (""cluster "",cluster,"":"")
                for i,sentence in enumerate(clusters[cluster]):
                        print (""\tsentence "",i,"": "",sentences[sentence])
</code></pre>

<p>result that I got for example :
cluster  6 :
        sentence  0 :  26    RIH  DP std
        sentence  1 :  32    RIH  DP std
        sentence  2 :  68    RIH  Liner with DP  std  in hole
        sentence  3 :  105   RIH DP std
        sentence  4 :  118   RIH std no  of  DP in hole
        sentence  5 :  154   RIH DP  std</p>

<p>Could you help me please! thank you</p>
",Text Classification / Sentiment Analysis,visualization clustering python would like classify comment based nlp algorithm tf idf managed classify cluster want visualize graphically histogram scatter plot result got example cluster sentence rih dp std sentence rih dp std sentence rih liner dp std hole sentence rih dp std sentence rih std dp hole sentence rih dp std could help please thank
"I used Naive Bayes Classifier, but now I want to use SVM classifier, what to do?","<p>I am classifying text with 2 categories. One is imperatives, and the other one is non-imperatives. I prepared my text in the way Naive Bayes Classifier needs. But, now, I also need to use SVM. What should I do here? (I need to classify the text and calculate the accuracy, too.)Thank you for reading and trying to answering my questions. </p>

<pre><code>all_words_list = [word for (sent, cat) in train for word in sent]
all_words = nltk.FreqDist(all_words_list)
word_items = all_words.most_common(1000)
word_features = [word for (word, count) in word_items]

def document_features(document, word_features):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features

 featuresets = [(document_features(d, word_features), c) for (d, c) in 
 train]

train_set, test_set = featuresets[360:], featuresets[:360]
classifier = nltk.NaiveBayesClassifier.train(train_set)
print (nltk.classify.accuracy(classifier, test_set))
</code></pre>
",Text Classification / Sentiment Analysis,used naive bayes classifier want use svm classifier classifying text category one imperative one non imperative prepared text way naive bayes classifier need also need use svm need classify text calculate accuracy thank reading trying answering question
What is the X axis of a binary text classification and why does my graph look so messed up?,"<p>I'm trying to input a sentence and classify it as a 1 or 0. I have data with two columns, the first is the sentence text (e.g. ""This is a sentence"") and the second column is a classification (e.g. 0 or 1). </p>

<p>I have predicted values that I'm trying to interpret, only I can't seem to understand the X axis of my graph and why my Regression line looks like the way it does. </p>

<pre class=""lang-py prettyprint-override""><code>import nltk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from os import listdir
from os.path import isfile, join
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score
from sklearn import linear_model

X_train, X_test, Y_train, Y_test = train_test_split(labor_data['text'],labor_data['label_one'],random_state=0)
vect = CountVectorizer(ngram_range=(1,1),min_df=0,max_df=.25).fit(X_train)
X_train_vectorized = vect.transform(X_train)
lr_model = linear_model.LinearRegression()

lr_model.fit(X_train_vectorized,Y_train)
lr_predictions = lr_model.predict(vect.transform(X_test))

plt.scatter(X_test, Y_test,  color='black')
plt.plot(X_test, lr_predictions, color='blue', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()
</code></pre>

<p>I understand the Y Axis is values, but don't understand the X axis or my regression line. I know my lr_predictions are values between 0 and 1, as are all the values on the plot. But shouldn't the line be a downward sloped straight line? </p>

<p>Graph
<a href=""https://i.sstatic.net/5esYE.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/5esYE.jpg</a></p>
",Text Classification / Sentiment Analysis,x axis binary text classification doe graph look messed trying input sentence classify data two column first sentence text e g sentence second column classification e g predicted value trying interpret seem understand x axis graph regression line look like way doe understand axis value understand x axis regression line know lr prediction value value plot line downward sloped straight line graph
Get a dictionary of incorrect spelling words in a dataframe,"<p>Am working on sentiment analysis problem. Tried to use <code>autocorrect</code> but that requires a lot computing power which I don't have access to because of the size of corpus. So came up with a different approach of solving the problem by creating a dictionary of <code>{key = 'incorrect', value = 'correct'}</code> and then manually correcting all words.</p>

<p>The problem is that how should I get that dictionary of miss-spelled words in the dictionary. Is <a href=""https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/#c-removing-misspells"" rel=""nofollow noreferrer"">this link</a> same as the solution to my problem?(Rather than misspelled words should I look for OOV words?) </p>

<p>And if not, please suggest some better method.</p>

<p>Code used for <code>autocorrect</code>:</p>

<pre><code>!pip install autocorrect
from autocorrect import spell 
train['text'] = [' '.join([spell(i) for i in x.split()]) for x in train['text']]
</code></pre>
",Text Classification / Sentiment Analysis,get dictionary incorrect spelling word dataframe working sentiment analysis problem tried use requires lot computing power access size corpus came different approach solving problem creating dictionary manually correcting word problem get dictionary miss spelled word dictionary link solution problem rather misspelled word look oov word please suggest better method code used
How to test unseen sentences for a new classifier in scikit learn,"<p>I have created a model using this dataset and I would like to insert some sentences to see how they would be classified. How can I do that?</p>

<p>Here is the code that makes the model:</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics

cats = ['sci.space','rec.autos']
newsgroups_train = fetch_20newsgroups(subset='train',
                   remove=('headers', 'footers', 'quotes'), categories = cats)
newsgroups_test = fetch_20newsgroups(subset='test',
                   remove=('headers', 'footers', 'quotes'), categories = cats)

vectors_test = vectorizer.transform(newsgroups_test.data)
vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(newsgroups_train.data)
clf = MultinomialNB(alpha=.01)
clf.fit(vectors, newsgroups_train.target)
vectors_test = vectorizer.transform(newsgroups_test.data)
pred = clf.predict(vectors_test)
metrics.f1_score(newsgroups_test.target, pred, average='macro')
</code></pre>

<p>the accuracy it returns is: <code>0.97</code> which shows that there is overfitting.</p>

<p>As mentioned, I would like to test how the classification of unseen data would occur. How can I proceed?</p>

<p>Example I tried:</p>

<pre><code>texts = [""The space shuttle is made in 2018"", 
         ""The exhaust is noisy."",
         ""the windows are transparent.""]
text_features = tfidf.transform(texts)
predictions = model.predict(text_features)
for text, predicted in zip(texts, predictions):
  print('""{}""'.format(text))
  print(""  - Predicted as: '{}'"".format(id_to_category[predicted]))
  print("""")
 #this does not work as it is
</code></pre>

<p>It should classify each sentence to one of the two (sci.space, rec.autos) categories.</p>

<p>Furthermore, any other suggestions you may have about the whole code are welcome. I want to learn these processes very well.</p>
",Text Classification / Sentiment Analysis,test unseen sentence new classifier scikit learn created model using dataset would like insert sentence see would classified code make model accuracy return show overfitting mentioned would like test classification unseen data would occur proceed example tried classify sentence one two sci space rec auto category furthermore suggestion may whole code welcome want learn process well
How to handle text classification problems when multiple features are involved,"<p>I am working on a text classification problem where multiple text features and need to build a model to predict salary range. Please refer the <a href=""https://i.sstatic.net/MedzR.png"" rel=""noreferrer"">Sample dataset</a>
Most of the resources/tutorials deal with feature extraction on only one column and then predicting target. I am aware of the processes such as text pre-processing, feature extraction (CountVectorizer or TF-IDF) and then the applying algorithms. </p>

<p>In this problem, I have multiple input text features. <strong>How to handle text classification problems when multiple features are involved?</strong> These are the methods I have already tried but I am not sure if these are the right methods. Kindly provide your inputs/suggestion.</p>

<p>1) Applied data cleaning on each feature separately followed by TF-IDF and then logistic regression. Here I tried to see if I can use only one feature for classification.   </p>

<p>2) Applied Data cleaning on all the columns separately and then applied TF-IDF for each feature and then merged the all feature vectors to create only one feature vector. Finally logistic regression. </p>

<p>3) Applied Data cleaning on all the columns separately and merged all the cleaned columns to create one feature 'merged_text'. Then applied TF-IDF on this merged_text and followed by logistic regression.</p>

<p>All these 3 methods gave me around 35-40% accuracy on cross-validation &amp; test set. I am expecting at-least 60% accuracy on the test set which is not provided.</p>

<p>Also, I didn't understand how use to <strong>'company_name'</strong> &amp; <strong>'experience'</strong> with text data. there are about 2000+ unique values in company_name. Please provide input/pointer on how to handle numeric data in text classification problem.</p>
",Text Classification / Sentiment Analysis,handle text classification problem multiple feature involved working text classification problem multiple text feature need build model predict salary range please refer sample dataset resource tutorial deal feature extraction one column predicting target aware process text pre processing feature extraction countvectorizer tf idf applying algorithm problem multiple input text feature handle text classification problem multiple feature involved method already tried sure right method kindly provide input suggestion applied data cleaning feature separately followed tf idf logistic regression tried see use one feature classification applied data cleaning column separately applied tf idf feature merged feature vector create one feature vector finally logistic regression applied data cleaning column separately merged cleaned column create one feature merged text applied tf idf merged text followed logistic regression method gave around accuracy cross validation test set expecting least accuracy test set provided also understand use company name experience text data unique value company name please provide input pointer handle numeric data text classification problem
Sentiment analysis on reviews using NLTK in Python,"<p>I have a csv data file containing column 'notes' with satisfaction answers in Hebrew. </p>

<p>I would like to use Sentiment analysis in order to assign a score for each word or bigrm in the data and receive positive/negative probability using logistic regression.</p>

<p>My code so far:</p>

<pre><code>PYTHONIOENCODING=""UTF-8""  
df= pd.read_csv('keep.csv', encoding='utf-8' , usecols=['notes'])

txt = df.notes.str.lower().str.replace(r'\|', ' ').str.cat(sep=' ')
words = nltk.tokenize.word_tokenize(txt)
tokens=[word.lower() for word in words if word.isalpha()]
bigrm = list(nltk.bigrams(tokens))

word_index = {}
current_index = 0
    for token in tokens:
    if token not in word_index:
        word_index[token] = current_index
        current_index += 1

def tokens_to_vector(tokens, label):
    x = np.zeros(len(word_index) + 1) 
    for t in tokens:
        i = word_index[t]
        x[i] += 1
    x = x / x.sum() 
    x[-1] = label
    return x

N= len(word_index)
data = np.zeros((N, len(word_index) + 1))
i = 0
for token in tokens:
xy = tokens_to_vector(tokens, 1)
data[i,:] = xy
i += 1
</code></pre>

<p>This loop isn't working.
How can I generate the data and then receive positive/negative probabilities for each bigrm?</p>
",Text Classification / Sentiment Analysis,sentiment analysis review using nltk python csv data file containing column note satisfaction answer hebrew would like use sentiment analysis order assign score word bigrm data receive positive negative probability using logistic regression code far loop working generate data receive positive negative probability bigrm
Sentiment Analysis with small Dataset,"<p>I want to do a multi class sentiment classification on some reviews that I have, in fact I want to label each review as ""negative"" , ""more negative"",""less negative"" and ""positive"", the problem is that I have a small dataset (that I created myself )  to train the algorithm. So I want some advice on which ML algorithm is appropriate to do a sentiment analysis with a small training set and why. </p>
",Text Classification / Sentiment Analysis,sentiment analysis small dataset want multi class sentiment classification review fact want label review negative negative le negative positive problem small dataset created train algorithm want advice ml algorithm appropriate sentiment analysis small training set
Grouped Text classification,"<p>I have thousands groups of paragraphs and I need to classify these paragraphs. The problem is that I need to classify each paragraph based on other paragraphs in the group! For example, a paragraph individually maybe belongs to class A but according to other paragraph in the group it belongs to class B.</p>

<p>I have tested lots of traditional and deep approaches( in fields like text classification, IR, text understanding, sentiment classification and so on) but those couldn't classify correctly.</p>

<p>I was wondering if anybody has worked in this area and could give me some suggestion. Any suggestions are appreciated. Thank you.</p>

<p><strong>Update 1:</strong></p>

<p>Actually we are looking for manual sentences/paragraph for some fields, so we first need to recognize if a sentence/paragraph is a manual or not second we need to classify it to it's fields and we can recognize its field only based on previous or next sentences/paragraphs.</p>

<p>To classify the paragraphs to manual/no-manual we have developed some promising approaches but the problem come up when we should recognize the field according to previous or next sentences/paragraphs, but which one?? we don't know the answer would be in any other sentences!!.</p>

<p><strong>Update 2:</strong></p>

<p>We can not use whole text of group as input because those are too big (sometimes tens of thousands of words) and contain some other classes and machine can't learn properly which lead to  the drop the accuracy sharply.</p>

<p>Here is a picture that maybe help to better understanding the problem:
<a href=""https://i.sstatic.net/HC2mj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HC2mj.png"" alt=""enter image description here""></a></p>
",Text Classification / Sentiment Analysis,grouped text classification thousand group paragraph need classify paragraph problem need classify paragraph based paragraph group example paragraph individually maybe belongs class according paragraph group belongs class b tested lot traditional deep approach field like text classification ir text understanding sentiment classification classify correctly wa wondering anybody ha worked area could give suggestion suggestion appreciated thank update actually looking manual sentence paragraph field first need recognize sentence paragraph manual second need classify field recognize field based previous next sentence paragraph classify paragraph manual manual developed promising approach problem come recognize field according previous next sentence paragraph one know answer would sentence update use whole text group input big sometimes ten thousand word contain class machine learn properly lead drop accuracy sharply picture maybe help better understanding problem
Semantic analysis of natural language using convoluted neural network,"<p>I would like to analysis some documents and classify them, but there are a lot of negative, double negative in the sentence. What would be the best way to analyse such text</p>

<p>I have some documents as such</p>

<pre><code>""It is unlikely to be a xxx"" -&gt; 0
""The chance of xxx is low"" -&gt; 0
""xxx is not impossible"" -&gt; 1
""xxx cannot be ruled out"" -&gt; 1
""Chance that it is other than xxx is low"" -&gt; 1
</code></pre>

<p>Obviously ""bag of word"" approach will not work. Need to have some sort of method to understand spatial relationship. 
Would a convoluted neural network help with analyzing this sort of text? or is there a better way?</p>
",Text Classification / Sentiment Analysis,semantic analysis natural language using convoluted neural network would like analysis document classify lot negative double negative sentence would best way analyse text document obviously bag word approach work need sort method understand spatial relationship would convoluted neural network help analyzing sort text better way
how to build function to detect negation in sentiment analysis,"<p>I want to build negation detection for Malay text, it is to tackle a problem like 'not beautiful' detected as a positive word. so here is some coding that I modified but the result is not something that I wanted it to be.</p>

<p>The result is</p>

<pre><code>text= ""is not good, danish died,""
se=negate(self=None,text=text)
print(se)
['is', 'not', 'not_good', 'not_danish', 'not_died']

I wanted it to be
['is', 'not', 'not_good', 'danish', 'died']
</code></pre>

<p>only word after ""not"" is changed to ""not_"" form.
this is the function that I use, any advice to change and add in order to get the result as i wanted?</p>

<pre><code>def negate(self,text):

    negation = False
    result = []
    words = text.split()

    for word in words:
        # stripped = word.strip(delchars)
        stripped = word.strip(delims).lower()
        negated = ""not_"" + stripped if negation else stripped
        result.append(negated)

        if any(neg in word for neg in [""not"", ""n't"", ""no""]):
            negation = not negation

    return result
</code></pre>
",Text Classification / Sentiment Analysis,build function detect negation sentiment analysis want build negation detection malay text tackle problem like beautiful detected positive word coding modified result something wanted result word changed form function use advice change add order get result wanted
How to integrate all the classifiers for multilabel svm classification,"<p>I have 4 labelled groups which I want to classify using SVM.</p>

<pre><code>Class-A, Class-B, Class-C, Class-D
</code></pre>

<p>Now If I need to train my classifier to recognize I will copy all the text from A,B,C,D into a file ""<code>A-against-all</code>"". SImilarly for B,C &amp; D as </p>

<pre><code>""B-against-all"" CLass B :1 , Rest all :-1
""C-against-all"" CLass C :1 , Rest all :-1
""D-against-all"" CLass D :1 , Rest all :-1
</code></pre>

<p>Now if I run SVM on ""<code>A-against-all</code>"" then I get a classifier as output.
Similarly I get three more classifiers for B,C &amp; D.</p>

<p>Now my questions is this : - How do I integrate these 4 classifiers so as to operate in unison ?</p>
",Text Classification / Sentiment Analysis,integrate classifier multilabel svm classification labelled group want classify using svm need train classifier recognize copy text b c file similarly b c run svm get classifier output similarly get three classifier b c question integrate classifier operate unison
Can tf-idf values be added to find document similarity?,"<p>I am working with tf-idf and text classification to rank words in documents. I was wondering if adding tf-idf values for respective words can be used to predict the nearest match for a new document. What I mean by this is:</p>

<pre><code>Suppose I had the following tf-idf values for certain words (hypothetical):

    word   Category 1   Category 2   Category 3
    x      0.3          0.6          0.2
    y      0.8          0.4          0.1
    z      0.2          0.5          0.7
</code></pre>

<p>The categories in this case are very long documents consisting of all of the documents in a certain category combined, this reduces the corpus size from thousands to just 10 in my case. It's also worth noting that I am using sublinear TF in order to reduce the effects of very frequent terms.</p>

<p>If I had a new document that had the words ""x y"" in it, what I was thinking of is adding up the tf-idf values for those words in each category and whichever category has the greatest sum would be the nearest match to the new document. In this case, the sum for category 1 would be 1.1, 1.0 for category 2, and 0.3 for category 3, therefore the nearest match to the new document is category 1. I was also wondering if this ""algorithm"" already exists and has a name.</p>

<p>I tried this on some test data and it predicts it accurately 86% of the time. And it seems to make more sense than using a LogisticRegression. So, is this a valid algorithm?</p>
",Text Classification / Sentiment Analysis,tf idf value added find document similarity working tf idf text classification rank word document wa wondering adding tf idf value respective word used predict nearest match new document mean category case long document consisting document certain category combined reduces corpus size thousand case also worth noting using sublinear tf order reduce effect frequent term new document word x wa thinking adding tf idf value word category whichever category ha greatest sum would nearest match new document case sum category would category category therefore nearest match new document category wa also wondering algorithm already exists ha name tried test data predicts accurately time seems make sense using logisticregression valid algorithm
What kind of NLP is this?,"<p>I know that NLP Categorization is when we classify the whole text as (Health, Sports, Social, Business, etc.)</p>

<blockquote>
  <p>(LONDON) -- Rafael Nadal offered a pointed criticism of All-England
  Club's unique seeding rules on Saturday, two days before the start of
  Wimbledon.</p>
  
  <p>""I respect the Wimbledon rules,"" Nadal told reporters, ""...If I
  believe that is fair or not, that's another story. I really personally
  believe [it] is not.""</p>
  
  <p>Wimbledon uses a special formula to develop the seedings for the
  tournament, which sometimes depart from standard ATP rankings. The
  formula gives extra weight to a player's ATP record on grass courts,
  elevating or penalizing players who play less often or have less
  success on grass.</p>
  
  <p>This year, the Wimbledon rankings bumped Nadal down to the
  tournament's third seed, with Roger Federer hopping above him. That
  would require Nadal to beat both players seeded above him to win the
  title. It also sets up the possibility of a matchup with Nik Kyrgios
  in the second round. </p>
  
  <p>Kyrgios has defeated Nadal at this tournament before.</p>
  
  <p>""The system is the way it is,"" Federer said at his own press
  conference. ""At the end of the day, if you want to win the tournament,
  you got to go through all the players that are in front of you.""</p>
  
  <p>Copyright © 2019, ABC Radio. All rights reserved.</p>
</blockquote>

<p>This will be considered as ""Sports"" text</p>

<p>But this not what I want</p>

<p>I want to identify words or multiple words in the text</p>

<p>like in the text above if I want to Identify players</p>

<blockquote>
  <p>(LONDON) -- <strong><em>Rafael Nadal</em></strong> offered a pointed criticism of All-England
  Club's unique seeding rules on Saturday, two days before the start of
  Wimbledon.</p>
  
  <p>""I respect the Wimbledon rules,"" <strong><em>Nadal</em></strong> told reporters, ""...If I
  believe that is fair or not, that's another story. I really personally
  believe [it] is not.""</p>
  
  <p>Wimbledon uses a special formula to develop the seedings for the
  tournament, which sometimes depart from standard ATP rankings. The
  formula gives extra weight to a player's ATP record on grass courts,
  elevating or penalizing players who play less often or have less
  success on grass.</p>
  
  <p>This year, the Wimbledon rankings bumped <strong><em>Nadal</em></strong> down to the
  tournament's third seed, with <strong><em>Roger Federer</em></strong> hopping above him. That
  would require <strong><em>Nadal</em></strong> to beat both players seeded above him to win the
  title. It also sets up the possibility of a matchup with <strong><em>Nik Kyrgios</em></strong>
  in the second round. </p>
  
  <p><strong><em>Kyrgios</em></strong> has defeated <strong><em>Nadal</em></strong> at this tournament before.</p>
  
  <p>""The system is the way it is,"" <strong><em>Federer</em></strong> said at his own press
  conference. ""At the end of the day, if you want to win the tournament,
  you got to go through all the players that are in front of you.""</p>
  
  <p>Copyright © 2019, ABC Radio. All rights reserved.</p>
</blockquote>

<p>What is this method called and is there any python libraries specified for it?</p>
",Text Classification / Sentiment Analysis,kind nlp know nlp categorization classify whole text health sport social business etc london rafael nadal offered pointed criticism england club unique seeding rule saturday two day start wimbledon respect wimbledon rule nadal told reporter believe fair another story really personally believe wimbledon us special formula develop seedings tournament sometimes depart standard atp ranking formula give extra weight player atp record grass court elevating penalizing player play le often le success grass year wimbledon ranking bumped nadal tournament third seed roger federer hopping would require nadal beat player seeded win title also set possibility matchup nik kyrgios second round kyrgios ha defeated nadal tournament system way federer said press conference end day want win tournament got go player front copyright abc radio right reserved considered sport text want want identify word multiple word text like text want identify player london rafael nadal offered pointed criticism england club unique seeding rule saturday two day start wimbledon respect wimbledon rule nadal told reporter believe fair another story really personally believe wimbledon us special formula develop seedings tournament sometimes depart standard atp ranking formula give extra weight player atp record grass court elevating penalizing player play le often le success grass year wimbledon ranking bumped nadal tournament third seed roger federer hopping would require nadal beat player seeded win title also set possibility matchup nik kyrgios second round kyrgios ha defeated nadal tournament system way federer said press conference end day want win tournament got go player front copyright abc radio right reserved method called python library specified
Twitter sentiment analysis on a string,"<p>I've written a program that takes a twitter data that contains tweets and labels (<code>0</code> for neutral sentiment and <code>1</code> for negative sentiment) and predicts which category the tweet belongs to.
The program works well on the training and test Set. However I'm having problem in applying prediction function with  a string. I'm not sure how to do that.</p>

<p>I have tried cleaning the string the way I cleaned the dataset before calling the predict function but the values returned are in wrong shape.</p>

<pre><code>import numpy as np
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
import re

#Loading dataset
dataset = pd.read_csv('tweet.csv')

#List to hold cleaned tweets
clean_tweet = []

#Cleaning tweets
for i in range(len(dataset)):
    tweet = re.sub('[^a-zA-Z]', ' ', dataset['tweet'][i])
    tweet = re.sub('@[\w]*',' ',dataset['tweet'][i])
    tweet = tweet.lower()
    tweet = tweet.split()
    tweet = [ps.stem(token) for token in tweet if not token in set(stopwords.words('english'))]
    tweet = ' '.join(tweet)
    clean_tweet.append(tweet)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 3000)
X = cv.fit_transform(clean_tweet)
X =  X.toarray()
y = dataset.iloc[:, 1].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.naive_bayes import GaussianNB
n_b = GaussianNB()
n_b.fit(X_train, y_train)
y_pred  = n_b.predict(X_test) 

some_tweet = ""this is a mean tweet""  # How to apply predict function to this string
</code></pre>
",Text Classification / Sentiment Analysis,twitter sentiment analysis string written program take twitter data contains tweet label neutral sentiment negative sentiment predicts category tweet belongs program work well training test set however problem applying prediction function string sure tried cleaning string way cleaned dataset calling predict function value returned wrong shape
How to test a model trained using teacher forcing,"<p>I used keras to train a seq2seq model (keras.models.Model). The X and y to the model are [X_encoder, X_decoder] and y i.e. a list of encoder and decoder inputs and labels (<strong>Note</strong> that the decoder input, X_decoder is ‘y’ with one position ahead than the actual y. Basically, teacher forcing).</p>

<p>So my question is now after training, when it comes to actual prediction where I do not have any labels how do I provide ‘X_decoder’ to my input? Or do I train on something else?</p>

<p>This is a snippet of the model definition if at all that helps:)</p>

<pre><code># Encoder
encoder_inputs = Input(batch_shape=(batch_size, max_len,), dtype='int32')
encoder_embedding = embedding_layer(encoder_inputs)
encoder_LSTM = CuDNNLSTM(hidden_dim, return_state=True, stateful=True)
encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)

# Decoder
decoder_inputs = Input(shape=(max_len,), dtype='int32')
decoder_embedding = embedding_layer(decoder_inputs)
decoder_LSTM = CuDNNLSTM(hidden_dim, return_state=True, return_sequences=True)
decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])

# Output
outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder_outputs)
model = Model([encoder_inputs, decoder_inputs], outputs)

# model fitting:
model.fit([X_encoder, X_decoder], y, steps_per_epoch=int(number_of_train_samples/batch_size),
epochs=epochs)
</code></pre>
",Text Classification / Sentiment Analysis,test model trained using teacher forcing used kera train seq seq model kera model model x model x encoder x decoder e list encoder decoder input label note decoder input x decoder one position ahead actual basically teacher forcing question training come actual prediction label provide x decoder input train something else snippet model definition help
Recommendations for text classification models to work with almost 200K labels,"<p>So basically I want to classify a lot of labels (200K+).
Are there any recommended models I should try in order to have a relatively good accuracy and not take days to complete?</p>

<p>I have tried to use Sklearn's OneVsRestClassifier for LinearRegression, and I left it overnight and the fitting still didn't finish</p>

<p>I believe that there should be more efficient algorithms for multiclass classification for NLP</p>

<p>Thanks in advance</p>
",Text Classification / Sentiment Analysis,recommendation text classification model work almost k label basically want classify lot label k recommended model try order relatively good accuracy take day complete tried use sklearn onevsrestclassifier linearregression left overnight fitting still finish believe efficient algorithm multiclass classification nlp thanks advance
How to determine which words have high predictive power in Sentiment Analysis?,"<p>I am working on a classification problem with Tweeter data. User labeled tweets (relevant, not relevant) are used to train a machine learning classifier to predict if an unseen tweet is relevant or not to the user. </p>

<p>I use a simple preprocessing techniques like removal of stopwords, stemming etc and a sklearn Tfidfvectorizer to convert the words into numbers before feeding them into a classifier e.g. SVM, kernel SVM , Naïve Bayes. </p>

<p>I would like to determine which words (features) have the higher predictive power. What is the best way to do so?</p>

<p>I have tried wordcloud but it just shows the words with highest frequency in the sample.</p>

<p>UPDATE:</p>

<p>The following approach along with sklearns feature_selection seem to provide the best answer so far to my problem:</p>

<p><a href=""https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d"" rel=""nofollow noreferrer"">top features</a> Any other suggestions?</p>
",Text Classification / Sentiment Analysis,determine word high predictive power sentiment analysis working classification problem tweeter data user labeled tweet relevant relevant used train machine learning classifier predict unseen tweet relevant user use simple preprocessing technique like removal stopwords stemming etc sklearn tfidfvectorizer convert word number feeding classifier e g svm kernel svm na bayes would like determine word feature higher predictive power best way tried wordcloud show word highest frequency sample update following approach along sklearns feature selection seem provide best answer far problem top feature suggestion
How to handle test set labels which are not in training set in Multi Class Text Classification?,"<p>I'm working with a multi class text classification data set having train and test sets. There are around 470 unique labels in training set and around 250 unique labels in test set. (<strong>These 470+ 250 unique labels comes from a large set of labels of size 4 million.</strong> )</p>

<p>There are around 30 labels which are only in test set but not in training set. </p>

<p><strong>DO I need to encode each label into a one hot vector of size 4 million rather than 450 ?</strong>  so that I can handle those missing 30 labels also </p>
",Text Classification / Sentiment Analysis,handle test set label training set multi class text classification working multi class text classification data set train test set around unique label training set around unique label test set unique label come large set label size million around label test set training set need encode label one hot vector size million rather handle missing label also
How to set a fixed and proper Sequence Length in the Sentiment Analysis using LSTM?,"<p>I am working on a Sentiment Classification problem, and as many of you guys know that we have to do pre-processing of the text in order to feed it into word embedding layers. So, accordingly, in the first few pre-processing steps, I encounter that after doing some pre-processing to the data, I have to set a sequence-length that the data will lie within. If the review text line is somehow smaller than the sequence-length, then we will have to pad it and if greater than sequence-length, then we have to truncate it to sequence-length.
But what should be the optimal value for this sequence-length? In many posts it is 200, 100 and even if I put 50, it works. This is my code - </p>

<pre><code>def pad_features(reviews_int, sequence_length):

  features = np.zeros((len(reviews_int), sequence_length), dtype = int)

  for i, review in enumerate(reviews_int):

    reviews_len = len(review)

    if reviews_len &lt;= sequence_length:
      zeroes = list(np.zeros(sequence_length - reviews_len))
      new = zeroes + review

    elif reviews_len &gt; sequence_length:
      new = review[0:sequence_length]

    features[i, :] = np.array(new)

  return features
</code></pre>

<pre><code>sequence_length = 100

features = pad_features(reviews_int, sequence_length = sequence_length)

#assert len(features)==len(reviews_int)
#assert len(features[0])==sequence_length

print (features[:10,:10])
</code></pre>

<p>I am confused now. Can you please help in choosing a right and optimal  Sequence-length? Thanks in advance.</p>
",Text Classification / Sentiment Analysis,set fixed proper sequence length sentiment analysis using lstm working sentiment classification problem many guy know pre processing text order feed word embedding layer accordingly first pre processing step encounter pre processing data set sequence length data lie within review text line somehow smaller sequence length pad greater sequence length truncate sequence length optimal value sequence length many post even put work code confused please help choosing right optimal sequence length thanks advance
Splitting Bot Records from Chatter Records,"<p>I have raw chat bot transcripts, and before doing any sentiment analysis, I would like to separate Bot records from Chatter records.  </p>

<p>Data is already in a dataframe, and looks like the following:</p>

<pre><code>Conversation_ID | Transcript
abcdef | BOT: Some text. CHATTER: Some text. BOT: Some text. BOT: Some text. CHATTER: Some text. BOT: Some text. BOT: Some text.
</code></pre>

<p>The result should look like:</p>

<pre><code>Conversation_ID | Transcript_BOT | Transcript_CHATTER
abcdef | Some text. Some text. Some text. Some text. Some text. | Some text. Some text.
</code></pre>
",Text Classification / Sentiment Analysis,splitting bot record chatter record raw chat bot transcript sentiment analysis would like separate bot record chatter record data already dataframe look like following result look like
How to save a text classification model and test it later on a new unseen data,"<p>I am a newbie to python and working on a binary text classification problem. I have developed a text classification model. Now I want to save that trained model and reload it again to test it on a new test data file. </p>

<p>I tried pickle, and joblib for this task and some other suggested methods here at stack overflow but unable to do this. With one method, I successfully saved my model but couldn't test it on a new test data file. Any help shall be highly appreciated. Apologies if I couldn't explain the problem well as I am new to python.</p>

<pre><code>Dataset = pd.read_csv('trainingdata.csv')
my_types = ['Requirement','Non-Requirement']


X_train, X_test, y_train, y_test = model_selection.train_test_split(Dataset['description'],Dataset['types'],test_size=0.0, random_state=45)

tfidf_vect_ngram = TfidfVectorizer(analyzer='word', 
token_pattern=r'\w{1,}', ngram_range=(1,1), max_features=5000)
tfidf_vect_ngram.fit(Dataset['description'])
X_train_Tfidf =  tfidf_vect_ngram.transform(X_train)

logreg = LogisticRegression(n_jobs=1, C=1e5)
logreg.fit(X_train_Tfidf, y_train)

import pickle
filename = 'finalized_model.sav'
pickle.dump(logreg, open(filename, 'wb'))

loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score('testdata.csv')
print(result)    
</code></pre>

<p>I also tried this one.</p>

<pre><code>with open('text_classifier', 'wb') as picklefile:  
    pickle.dump(logreg,picklefile)

with open('text_classifier', 'rb') as training_model:  
    model = pickle.load(training_model)

result = model.predict('testdata.csv')
print(result)
</code></pre>

<p>One more solution I tried.</p>

<pre><code>from keras.models import load_model

logreg.save('my_model.h5') 
del logreg

model = load_model('my_model.h5')
result=model('projectay.csv')
print(result)
</code></pre>

<p>Despite trying multiple solutions, I couldn't get the required results. I may be doing some blunder due to my less expertise in machine learning and python. May someone please point out where I am doing mistake. Thanks in anticipation.</p>
",Text Classification / Sentiment Analysis,save text classification model test later new unseen data newbie python working binary text classification problem developed text classification model want save trained model reload test new test data file tried pickle joblib task suggested method stack overflow unable one method successfully saved model test new test data file help shall highly appreciated apology explain problem well new python also tried one one solution tried despite trying multiple solution get required result may blunder due le expertise machine learning python may someone please point mistake thanks anticipation
Why are word embeddings with linguistic features (e.g. Sense2Vec) not used?,"<p>Given that embedding systems such as Sense2Vec incorporate linguistic features such as part-of-speech, why are these embeddings not more commonly used?</p>

<p>Across popular work in NLP today, Word2Vec and GloVe are the most commonly used word embedding systems. Despite the fact that they only incorporate word information and does not have linguistic features of the words.</p>

<p>For example, in sentiment analysis, text classification or machine translation tasks, it makes logical sense that if the input incorporates linguistic features as well, performance could be improved. Particular when disambiguating words such as ""duck"" the verb and ""duck"" the noun.</p>

<p>Is this thinking flawed? Or is there some other practical reason why these embeddings are not more widely used.</p>
",Text Classification / Sentiment Analysis,word embeddings linguistic feature e g sense vec used given embedding system sense vec incorporate linguistic feature part speech embeddings commonly used across popular work nlp today word vec glove commonly used word embedding system despite fact incorporate word information doe linguistic feature word example sentiment analysis text classification machine translation task make logical sense input incorporates linguistic feature well performance could improved particular disambiguating word duck verb duck noun thinking flawed practical reason embeddings widely used
"How do i use TfidfVectorizer in 2 steps, incrementing the number of analyzed texts?","<p>I am working on a Text-Classification problem, in Python3, using sklearn. </p>

<p>I am doing the following steps:</p>

<ol>
<li>Clean up all Texts to train the Classifier</li>
<li>Extract the features of the training-texts and vectorize using TfidfVectorizer</li>
<li>Generate the Classifier (RandomForestClassifier)</li>
</ol>

<p>This works quite well, now when I get a new Text that I would like to classify, what is the best way to process it? I understand that Tfidf method also looks at the occurrence of the features in the other data-sets, thats's why I am now applying TfidfVectorizer to the old dataset+the new text.
But is there a way I can do it in a kind of incremental way?
So that once the Training-set it does not get touched anymore. 
Would that make sense?</p>

<p>Thank you in advance for your help!
Luca</p>
",Text Classification / Sentiment Analysis,use tfidfvectorizer step incrementing number analyzed text working text classification problem python using sklearn following step clean text train classifier extract feature training text vectorize using tfidfvectorizer generate classifier randomforestclassifier work quite well get new text would like classify best way process understand tfidf method also look occurrence feature data set thats applying tfidfvectorizer old dataset new text way kind incremental way training set doe get touched anymore would make sense thank advance help luca
Text classification: Raw dictionary input and text vectorization,"<p>I am working with some text processing using a series of sklearn classifiers. In an <a href=""http://blog.chapagain.com.np/machine-learning-sentiment-analysis-text-classification-using-python-nltk/"" rel=""nofollow noreferrer"">example</a> I found on the internet, I have noticed that the input of the classifier is a series of dictionary items:</p>

<p><code>({'my': True, 'first': True, 'visit': True, 'was': True, ...}, 'pos')</code></p>

<p><code>({'wowjust': True, 'wow': True, 'who': True, 'would': True,..}, 'pos')</code></p>

<p>These items are passed into a classification model (e.g., sklearn <code>LinearSVC</code>). I have found in the sklearn site that in text classification text data are transformed into a vector using some technique e.g., <code>HashingVectorizer</code> but I couldn't locate any documentation on how the aforementioned dictionary input is treated. Is it possible to provide some explanation of what procedure is followed in this input case?</p>
",Text Classification / Sentiment Analysis,text classification raw dictionary input text vectorization working text processing using series sklearn classifier example found internet noticed input classifier series dictionary item item passed classification model e g sklearn found sklearn site text classification text data transformed vector using technique e g locate documentation aforementioned dictionary input treated possible provide explanation procedure followed input case
How do I train model in Google NLP Sentiment Analysis correctly,"<p>I need to compare to sentiment models trained with different types of content. Google supplies you with a training dataset filled with tweets in a .csv file, As expected training with this went well however, when I decided to train a model using the Stanford NLP's dataset of IMDB reviews, I manage to upload the dataset without issue but when I train it the NLP, for some reason only predicts that the sentiment value is 2, regardless of what I write.  </p>

<p>I figured that the dataset was diluted since, while there were 800-2000 examples of sentiment 0,1,3 and 4, there were 6000 examples of sentiment 2. Although after removing 4000 of these examples, the problem persisted. </p>

<p>I'm expecting my confusion matrix to not simply only have 100% prediction on each sentiment value. It should be distributed over the matrix w</p>
",Text Classification / Sentiment Analysis,train model google nlp sentiment analysis correctly need compare sentiment model trained different type content google supply training dataset filled tweet csv file expected training went well however decided train model using stanford nlp dataset imdb review manage upload dataset without issue train nlp reason predicts sentiment value regardless write figured dataset wa diluted since example sentiment example sentiment although removing example problem persisted expecting confusion matrix simply prediction sentiment value distributed matrix w
How to choose dimensionality of the Dense layer in LSTM?,"<p>I have a task of multi-label text classification. My dataset has 1369 classes:</p>

<pre><code># data shape
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)
(54629, 500)
(23413, 500)
(54629, 1369)
(23413, 1369)
</code></pre>

<p>For this task, I've decided to use LSTM NN with the next parameters:</p>

<pre><code># define model
maxlen = 400
inp = Input(shape=(maxlen, ))
embed_size = 128
x = Embedding(max_features, embed_size)(inp)
x = LSTM(60, return_sequences=True,name='lstm_layer')(x)
x = GlobalMaxPool1D()(x)
x = Dropout(0.1)(x)
x = Dense(2000, activation=""relu"")(x)
x = Dropout(0.1)(x)
x = Dense(1369, activation=""sigmoid"")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy']
batch_size = 32
epochs = 2
model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
</code></pre>

<p><strong>Question</strong>: Are there any scientific methods for determining <code>Dense</code> and <code>LSTM</code> dimensionality (in my example, <code>LSTM dimension=60</code>, <code>I Dense dimension=2000</code>, and <code>II Dense dimension=1369</code>)? </p>

<p>If there are no scientific methods, maybe there are some heuristics or tips on how to do this with data with similar dimension.</p>

<p>I randomly chose these parameters. I would like to improve the accuracy of the model and correctly approach to solving similar problems.</p>
",Text Classification / Sentiment Analysis,choose dimensionality dense layer lstm task multi label text classification dataset ha class task decided use lstm nn next parameter question scientific method determining dimensionality example scientific method maybe heuristic tip data similar dimension randomly chose parameter would like improve accuracy model correctly approach solving similar problem
How to know the feature count of a specific class in text classification?,"<p>I'm trying to calculate the prediction probabilities for a certain text. There're problems with the classification of those texts. The text should be classified as class B but it's assigned class A. So I want to calculate the probability manually. But I need the feature count for Class A and B. All what I have is the total feature count which is 60343. </p>

<p>So how can I get the feature count for each class?</p>

<p>I'm doing dialect text classificaiton. and using scikit learn.</p>
",Text Classification / Sentiment Analysis,know feature count specific class text classification trying calculate prediction probability certain text problem classification text text classified class b assigned class want calculate probability manually need feature count class b total feature count get feature count class dialect text classificaiton using scikit learn
How to have multioutput in text classification?,"<p>I'm doing dialect text classification. The problem is some tweets, can be classified as both dialect A and B, how can I do that? I want to do it and then automatically calculate the accuracy, I don't want to do it manually. When I don't classify them as both A and B, it gives me many misclassified texts. </p>

<p>In the training though, they're not classified as both dialect A and B. but separately. </p>
",Text Classification / Sentiment Analysis,multioutput text classification dialect text classification problem tweet classified dialect b want automatically calculate accuracy want manually classify b give many misclassified text training though classified dialect b separately
Sentiment Analysis - polarity,"<p>If Polarity is 0.0 in TextBlob, whether the sentence is purely negative or it means no output.</p>

<pre class=""lang-py prettyprint-override""><code>wiki = TextBlob (""Python is a high-level, general-purpose programming language."")

wiki.sentiment
</code></pre>

<p>output: Sentiment(polarity=0.0, subjectivity=0.0)</p>

<p>Please explain what the output is actually telling to us?</p>
",Text Classification / Sentiment Analysis,sentiment analysis polarity polarity textblob whether sentence purely negative mean output output sentiment polarity subjectivity please explain output actually telling u
How to reduce the number of features in text classification?,"<p>I'm doing dialect text classification and I'm using countVectorizer with naive bayes. The number of features are too many, I have collected 20k tweets with 4 dialects. every dialect have 5000 tweets. And the total number of features are 43K. I was thinking maybe that's why I could be having overfitting. Because the accuracy has dropped a lot when I tested on new data. So how can I fix the number of features to avoid overfitting the data?</p>
",Text Classification / Sentiment Analysis,reduce number feature text classification dialect text classification using countvectorizer naive bayes number feature many collected k tweet dialect every dialect tweet total number feature k wa thinking maybe could overfitting accuracy ha dropped lot tested new data fix number feature avoid overfitting data
Am I having an overfitting problem with my text classification?,"<p>I'm doing dialect text classification with 4 dialects. I split the dataset(of size 20K) into 75% training and 25% testing. When I trained them with naive bayes, and tested on the test dataset I got 90% accuracy. But when I did validation with a new dataset of size 400 tweets, I got accuracy of 63%. Do you think that drop is caused by overfitting? If there's any other information you need please tell me. </p>

<p>This is my code:</p>

<pre><code>from sklearn.pipeline import Pipeline
text_clf = Pipeline([
    ('vect', CountVectorizer()),
    ('clf', MultinomialNB())])

text_clf.fit(X_train, y_train)

pred=text_clf.predict(validate['tweets'])

accuracy_score(validate['dialect'],pred)

0.63
</code></pre>

<p>Those are my setting for for naive bayes and countVectorizer. I didn't add any hyperparameters for naive bayes. I'm not sure what to add or it it's gonna make any noticeable difference. </p>

<p>this is the confusion matrix:</p>

<pre><code>array([[150,   4,  44,   1],
       [  7,  67, 105,   3],
       [ 12,  10, 110,   0],
       [  0,   0,   0,   0]], dtype=int64)
</code></pre>

<h3>Reports</h3>

<p><strong>for training</strong></p>

<pre><code>                precision    recall  f1-score   support

       Egypt       0.96      0.98      0.97      4039
        Gulf       0.99      0.97      0.98      4456
      Hijazi       0.95      0.97      0.96      4905
    Maghribi       1.00      0.97      0.98      3014

   micro avg       0.97      0.97      0.97     16414
   macro avg       0.97      0.97      0.97     16414
weighted avg       0.97      0.97      0.97     16414

accuracy: 0.97
</code></pre>

<p><strong>for testing</strong></p>

<pre><code>                 precision    recall  f1-score   support

       Egypt       0.90      0.92      0.91      1321
        Gulf       0.92      0.88      0.90      1533
      Hijazi       0.84      0.92      0.88      1603
    Maghribi       0.98      0.87      0.92      1015

   micro avg       0.90      0.90      0.90      5472
   macro avg       0.91      0.90      0.90      5472
weighted avg       0.90      0.90      0.90      5472

accuracy: 0.89
</code></pre>

<p><strong>for new data</strong></p>

<pre><code>                 precision    recall  f1-score   support

       Egypt       0.89      0.75      0.82       199
        Gulf       0.83      0.37      0.51       182
      Hijazi       0.42      0.83      0.56       132
    Maghribi       0.00      0.00      0.00         0

   micro avg       0.64      0.64      0.64       513
   macro avg       0.53      0.49      0.47       513
weighted avg       0.75      0.64      0.64       513

accuracy: 0.63
</code></pre>

<p><strong>Important notes</strong>:</p>

<ol>
<li>the samples that my model fails on are new tweets from twitter.</li>
<li>The egypt, gulf and maghribi dataset were mannualy annotated.</li>
<li>The hijazi dataset were fetched from twitter using twitter API by the following conditions: geolocation, user_location and keywords(specific to hijazi dialect). I did that to reduce the noise as much as possible because those tweets aren't manually labeled. </li>
</ol>
",Text Classification / Sentiment Analysis,overfitting problem text classification dialect text classification dialect split dataset size k training testing trained naive bayes tested test dataset got accuracy validation new dataset size tweet got accuracy think drop caused overfitting information need please tell code setting naive bayes countvectorizer add hyperparameters naive bayes sure add gon na make noticeable difference confusion matrix report training testing new data important note sample model fails new tweet twitter egypt gulf maghribi dataset mannualy annotated hijazi dataset fetched twitter using twitter api following condition geolocation user location keywords specific hijazi dialect reduce noise much possible tweet manually labeled
How to gain insights in text classifications with error analysis?,"<p>I'm doing text classification for dialect. And I'm using naive bayes classifier with countVectorizer. I'm having many misclassified texts. Is there a method to analyse those errors to figure out where the classification is going wrong? For example, if I could know what words are being used to misclassify text as A, while it should be classified as B and then I can take those out words from the corpus of A.  </p>

<p>Should switch to unsupervised learning using clustering? or neural networks and deep learning? if naive bayes classifier isn't working. </p>

<p>Also, how can I know how countVectorizer classified the documents? And which words he used to classify a document.</p>
",Text Classification / Sentiment Analysis,gain insight text classification error analysis text classification dialect using naive bayes classifier countvectorizer many misclassified text method analyse error figure classification going wrong example could know word used misclassify text classified b take word corpus switch unsupervised learning using clustering neural network deep learning naive bayes classifier working also know countvectorizer classified document word used classify document
Read text files with paragraphs as one string using VCorpus from tm package in r,"<p>I have a list of text files in my directory, all of which are documents with multiple paragraphs. I want to read those documents and do sentiment analysis.</p>

<p>For example, I have one text document <code>data/hello.txt</code> with text like below:</p>

<pre><code>""Hello world.  
 This is an apple.

 That is an orange""
</code></pre>

<p>I read the document in like below (there can also be multiple documents):</p>

<pre><code>docs &lt;- VCorpus(DirSource('./data/hello.txt'))
</code></pre>

<p>When I look at the document content  <code>docs[[1]]$content</code> It seems like it is character vector.</p>

<pre><code>[1] ""hello  world""        ""this is apple.""      """"                   
[4] ""That is an orange. "" """"  
</code></pre>

<p>My question is how I can read in those documents so that in each document, paragraphs are concatenated into one single character string so that I can use it for sentiment analysis. (VCorpus from tm package)</p>

<p>Thanks a lot.</p>
",Text Classification / Sentiment Analysis,read text file paragraph one string using vcorpus tm package r list text file directory document multiple paragraph want read document sentiment analysis example one text document text like read document like also multiple document look document content seems like character vector question read document document paragraph concatenated one single character string use sentiment analysis vcorpus tm package thanks lot
Interactive learning,"<p>I'm new in NLP and text mining and I'm trying to build a documents classifier.
Once the model is trained, we test it on new documents (they, test-data, don't have labels). It is expected that the model is not 100% accurate; so for misclassified documents, we want interact with a user to correct these bad predictions.</p>

<p>I've two ideas:</p>

<ul>
<li><p>Retrain the model where: traindata = old_traindata + data corrected by the user.</p></li>
<li><p>After each user's rectification, update model parameters.</p></li>
</ul>

<p>Does this sound correct? in the second case, which kind of algorithms should I use? How efficiently can we solve this problem?</p>
",Text Classification / Sentiment Analysis,interactive learning new nlp text mining trying build document classifier model trained test new document test data label expected model accurate misclassified document want interact user correct bad prediction two idea retrain model traindata old traindata data corrected user user rectification update model parameter doe sound correct second case kind algorithm use efficiently solve problem
How to use bigrams + trigrams + word-marks vocabulary in countVectorizer?,"<p>I'm using text classification with naive Bayes and countVectorizer to classify dialects. I read a research paper that the author has used a combination of :</p>

<pre><code>bigrams + trigrams + word-marks vocabulary 
</code></pre>

<p>He means by word-marks here, the words that are specific to a certain dialect.</p>

<p>How can I tweak those parameters in countVectorizer? </p>

<h3>word marks</h3>

<p>So those are examples of word marks, but it isn't what I have, because mine are arabic. So I translated them.</p>

<pre><code>word_marks=['love', 'funny', 'happy', 'amazing']
</code></pre>

<p>Those are used to classify a text.</p>

<p>Also, in the this post:
<a href=""https://stackoverflow.com/questions/24005762/understanding-the-ngram-range-argument-in-a-countvectorizer-in-sklearn"">Understanding the `ngram_range` argument in a CountVectorizer in sklearn</a></p>

<p>There was this answer :</p>

<pre><code>&gt;&gt;&gt; v = CountVectorizer(ngram_range=(1, 2), vocabulary={""keeps"", ""keeps the""})
&gt;&gt;&gt; v.fit_transform([""an apple a day keeps the doctor away""]).toarray()
array([[1, 1]])  # unigram and bigram found
</code></pre>

<p>I couldn't understand the output, what does [1,1] mean here? and how was he able to use ngram with vocabulary? aren't both of them mutually exclusive? </p>
",Text Classification / Sentiment Analysis,use bigram trigram word mark vocabulary countvectorizer using text classification naive bayes countvectorizer classify dialect read research paper author ha used combination mean word mark word specific certain dialect tweak parameter countvectorizer word mark example word mark mine arabic translated used classify text also post href argument countvectorizer sklearn wa answer understand output doe mean wa able use ngram vocabulary mutually exclusive
Should I convert classification output to integer and how?,"<p>I'm using a neural network to classify text, and the label of the training data is 0 or 1(i.e. binary classification). It works well in the training and evaluating process, but the prediction output is float values rather than integer 0 or 1. How could I always get integer results? Do i need to manually convert them or change network parameters?</p>

<pre><code>model = Sequential()
e = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], 
input_length=max_length, trainable=False)
model.add(e)
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

# compile
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

# fit
model.fit(padded_docs, labels, epochs=5, verbose=2)

# eval
loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)
print('Accuracy: %f' % (accuracy*100))

# predict
result = model.predict(padded_docs_test, verbose=2)
</code></pre>
",Text Classification / Sentiment Analysis,convert classification output integer using neural network classify text label training data e binary classification work well training evaluating process prediction output float value rather integer could always get integer result need manually convert change network parameter
Is there a way to search for extracted features using python?,"<p>I have used td-idf vectorisation to extract features in a text classification problem and now I want to search for specific extracted features. Is there a way to develop a search functionality using python or any other tool for those extracted features? I have used facebook fasttext for out of vocabulary support and td-idf for feature extraction. then feature union is used in the pipeline. Now I want to search for these features(specific to one vector or in whole vector. so is there an option to do so? Here is the code I have used for feature extraction. Thanks</p>

<pre><code>
from nltk.tokenize import word_tokenize

alltext = pd.concat([training_set.scrape_text, test_set.scrape_text])
alltext.shape

rows = []
for row in alltext.values:
    rows.append(word_tokenize(row))

model = gensim.models.FastText(rows, size=100, window=4, min_count=2, iter=10)
#NLP pipeline
import re

from nltk.stem import SnowballStemmer

from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict


class TfidfEmbeddingVectorizer(object):

    def __preprocess(self, row):
        #print(""TfidfEmbeddingVectorizer  __preprocess begin"")
        cleanr = re.compile('&lt;.*?&gt;')
        cleantext = re.sub(cleanr, ' ', str(row))


        cleaned = cleantext.replace(""\n"","" "").strip()

        alpha_sent = """"

        for word in cleaned.split():
            alpha_word = re.sub('[^a-z A-Z]+', ' ', word)
            alpha_sent += self.stemmer.stem(alpha_word)
            #alpha_sent += lemmatizer.lemmatize(alpha_word)
            alpha_sent += "" ""
        alpha_sent = alpha_sent.strip()
        return alpha_sent



    def __tokenize(self, row):
        return word_tokenize(row)

    def __init__(self, model):

        self.model = model
        self.model2weight = None
        self.dim = model.vector_size
        self.stemmer = SnowballStemmer(""english"")


    def fit(self, X, y):
        print(""TfidfEmbeddingVectorizer  fit begin"")
        tfidf = TfidfVectorizer(analyzer=lambda x: x)

        preprocessed_X = []
        for row in X:
            preprocessed_X.append(self.__tokenize(self.__preprocess(row)))

        preprocessed_X = np.array(preprocessed_X)


        tfidf.fit(preprocessed_X)

        max_idf = max(tfidf.idf_)
        self.model2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])
        print(""TfidfEmbeddingVectorizer  fit end"")

        return self

    def transform(self, X):


        preprocessed_X = []
        for row in X:
            preprocessed_X.append(self.__tokenize(self.__preprocess(row)))

        preprocessed_X = np.array(preprocessed_X)

        return np.array([
                np.mean([self.model[w] * self.model2weight[w]
                         for w in words if w in self.model], axis=0)
                for words in preprocessed_X
            ])


from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline, FeatureUnion

from xgboost import XGBClassifier

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier


pipeline = Pipeline([
    ('feats', FeatureUnion([

        ('vec', TfidfVectorizer(preprocessor=preprocessor, 
                                tokenizer=word_tokenize,
                                analyzer='word', 
                                lowercase=True,
                                strip_accents='unicode',
                                stop_words='english',
                                ngram_range=(1,2)
                               ))

    ])),

    ('voting', VotingClassifier(estimators=[(""rf1"", RandomForestClassifier(n_jobs=1)),
                                        (""gb"", GradientBoostingClassifier()),
                                         (""et"", ExtraTreesClassifier(n_jobs=1)),
                                         (""xgb"",XGBClassifier(nthread= 4, learning_rate=0.08, n_estimators=1000, max_depth=10))
                                       ], 
                                voting='soft',

])
</code></pre>
",Text Classification / Sentiment Analysis,way search extracted feature using python used td idf vectorisation extract feature text classification problem want search specific extracted feature way develop search functionality using python tool extracted feature used facebook fasttext vocabulary support td idf feature extraction feature union used pipeline want search feature specific one vector whole vector option code used feature extraction thanks
How to classify a very large amount of text in python 3?,"<h1>I have to classify very large amounts of text in over 10,000 categories. I will need expert advice because I am still a student.</h1>
<p>My data is descriptions and titles of commercial products sorted into categories. For exemple, a keyboard with title &quot;large and good keyboard&quot; is in category office &gt; computer &gt; keyboard.</p>
<p>For now, i use &quot;from sklearn.feature_extraction.text import TfidfVectorizer&quot; for represent my text data. But the matrix is too big in memory.
Do you have any tips for representing a large amount of data?</p>
<p>I was thinking of using word-2-vec to represent the data followed by a neural network for classification.</p>
<h2>But I need your advice to get on the right path !!</h2>
<h2>Thanks</h2>
",Text Classification / Sentiment Analysis,classify large amount text python classify large amount text category need expert advice still student data description title commercial product sorted category exemple keyboard title large good keyboard category office computer keyboard use sklearn feature extraction text import tfidfvectorizer represent text data matrix big memory tip representing large amount data wa thinking using word vec represent data followed neural network classification need advice get right path thanks
How to deserialize OpenNLP trained model?,"<p>I'm using OpenNLP with Java to classify text. I have used <code>DoccatModel</code> object. Then I save it with the <code>serialize</code> method. I would like to know how to create the model from the <code>.bin</code> saved file.</p>

<p>Whitout that, I need to train the model each time that I launch the program.</p>

<pre class=""lang-java prettyprint-override""><code>DoccatModel model = trainedModel(trainingDatasetPath);
serializeModel(model);
</code></pre>

<p>The method <code>trainedModel</code> returns a <code>DoccatModel</code> object trained by the file located at <code>trainingDatasetPath</code>.</p>

<p>Does anyone have any idea ?</p>
",Text Classification / Sentiment Analysis,deserialize opennlp trained model using opennlp java classify text used object save method would like know create model saved file whitout need train model time launch program method return object trained file located doe anyone idea
How to classify text data with hundreds of classes and less amount of samples in each class,"<p>I have a dataset that contains around 10000 small paragraphs and the paragraphs belong to classes. There are around 80 - 100 classes. The paragraphs can be organized in hierarchies. I want to build a classifier model that will predict the class of an unseen paragraph.</p>

<p>Currently what I have done is, I have implemented a two step classification using FastText. First I classify the unseen text to a top level class and then using another classifier I classify it to a subclass of the identified top level class. This helped me to increase the accuracy.</p>

<p>Is there a better way to do this? Is there any good hierarchical classifier like <a href=""https://github.com/globality-corp/sklearn-hierarchical-classification"" rel=""nofollow noreferrer"">https://github.com/globality-corp/sklearn-hierarchical-classification</a> out there for text classification? Or can this be improved using FastText itself in some way?</p>
",Text Classification / Sentiment Analysis,classify text data hundred class le amount sample class dataset contains around small paragraph paragraph belong class around class paragraph organized hierarchy want build classifier model predict class unseen paragraph currently done implemented two step classification using fasttext first classify unseen text top level class using another classifier classify subclass identified top level class helped increase accuracy better way good hierarchical classifier like text classification improved using fasttext way
How to test new word set against my NLP Naive bayes classifier,"<p><strong>i build a NLP classifier based on Naive base  using python scikit-learn</strong> </p>

<p><em>the point is that , I want my classifier to classify a new text "" that is not belongs to any of my training or testing data set""</em> </p>

<p>in another model""like regression"" , I can extract the Theta's values so that i can predict any new value. </p>

<p>however i know that,naive based is working by calculation the probability of each word to against every class . </p>

<p><strong>for example</strong></p>

<p>my data set include (1000 record of some text) as 
"" it was so good "" 
"" i like it "" 
"" i don't like this movie "" 
etc .. </p>

<p>and each text is classified as either +ev or -ev </p>

<p>i do separation to my data set into training and testing set. every thing is ok .</p>

<p>now i want to classify a brand new text like "" Oh, I like this movie and the sound track was perfect"" </p>

<p>how to make my model predict this text !</p>

<p><strong>here is the code</strong> </p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=850)

X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, 1].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict()

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
</code></pre>

<p><strong>now iam expecting to do some kind new text like ""good movie and nice sound track"" and ""acting was so bad"". and let my classifier predict was it good or bad !</strong> </p>

<p>Xnew = [[""good movie and nice sound track""], [""acting was so bad""]]
ynew = classifier.predict(Xnew)</p>

<pre><code>but i get a super error 

 jointi = np.log(self.class_prior_[i])
    436             n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))
--&gt; 437             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /
    438                                  (self.sigma_[i, :]), 1)
    439             joint_log_likelihood.append(jointi + n_ij)

TypeError: ufunc 'subtract' did not contain a loop with signature matching types dtype('&lt;U32') dtype('&lt;U32') dtype('&lt;U32')
</code></pre>

<p><em>also I wonder if i can get all the probability for each word in my NLP Bag of my corpus.</em></p>

<p>thank's in advance</p>
",Text Classification / Sentiment Analysis,test new word set nlp naive bayes classifier build nlp classifier based naive base using python scikit learn point want classifier classify new text belongs training testing data set another model like regression extract theta value predict new value however know naive based working calculation probability word every class example data set include record text wa good like like movie etc text classified either ev ev separation data set training testing set every thing ok want classify brand new text like oh like movie sound track wa perfect make model predict text code iam expecting kind new text like good movie nice sound track acting wa bad let classifier predict wa good bad xnew good movie nice sound track acting wa bad ynew classifier predict xnew also wonder get probability word nlp bag corpus thank advance
Text Classification Approach,"<p>I have data with 2 important columns, Product Name and Product Category. I wanted to classify a search term into a category. The approach (in Python using Sklearn &amp; DaskML) to create a classifier was:</p>

<ol>
<li>Clean Product Name column for stopwords, numbers, etc.</li>
<li>Create 90% 10% train-test split</li>
<li>Convert text to vector using OneHotEncoder</li>
<li>Create classifier (Naive Bayes) on the training data</li>
<li>Test the classifier</li>
</ol>

<p>I realized the OneHotEncoder (or any encoder) converts the text to numbers by creating a matrix keeping into account where and how many times a word occurs. </p>

<p>Q1. Do I need to convert from Word to Vectors before train-test split or after train-test split?</p>

<p>Q2. When I will search for new words (which may not be in the text already), how will I classify it because if I encode the search term, it will be irrelevant to the encoder used for the training data. Can anybody help me with the approach so that I can classify a search term into a category if the term doesn't exist in the training data?</p>
",Text Classification / Sentiment Analysis,text classification approach data important column product name product category wanted classify search term category approach python using sklearn daskml create classifier wa clean product name column stopwords number etc create train test split convert text vector using onehotencoder create classifier naive bayes training data test classifier realized onehotencoder encoder convert text number creating matrix keeping account many time word occurs q need convert word vector train test split train test split q search new word may text already classify encode search term irrelevant encoder used training data anybody help approach classify search term category term exist training data
how can I speed up my sentiment analysis?,"<p>I'm doing sentiment analysis for several languages. my code runs successfully but it's extremely slow (10mn for just 11K records). Here is my code:</p>

<pre><code># Spanish Classifier - from https://github.com/aylliote/senti-py
clf = SentimentClassifier()

# Italian Classifier - Also for Russian
from polyglot.text import Text as T

# Germany Classifier
from textblob_de import TextBlobDE as TextBlob_d

# English
from textblob import TextBlob

# French
from textblob_fr import PatternTagger, PatternAnalyzer

def Flag(row):
    try:
        if row['lang'] == 'es':
            txt=clf.predict(row['rev'])
            return txt
        elif row['lang'] == 'it':
            txt=T(row['rev'])
            return txt.polarity
        elif row['lang'] == 'de':
            txt=TextBlob_d(row['rev'])
            return txt.sentiment
        elif row['lang'] == 'en':
            txt=TextBlob(row['rev'])
            return txt.sentiment.polarity
        elif row['lang'] == 'fr':
            txt=TextBlob(row['rev'], pos_tagger=PatternTagger(), 
            analyzer=PatternAnalyzer())
            return txt.sentiment[0]
        elif row['lang'] == 'ru':
            txt=T(row['rev'])
            return txt.polarity
        else:
            return """"
    except:
        return """"

df['sent']=df.apply(Flag,axis=1)
</code></pre>

<p>I've checked other posts about textblob.sentiments import NaiveBayesAnalyzer being very slow but I don't think it's the same situation i'm facing here?</p>

<p>Thank you</p>
",Text Classification / Sentiment Analysis,speed sentiment analysis sentiment analysis several language code run successfully extremely slow mn k record code checked post textblob sentiment import naivebayesanalyzer slow think situation facing thank
Is there a rule for deciding dictionary size for sentiment analysis with massive datasets?,"<p>I will be performing sentiment analysis on fiction. I'll be working with around 300 books of 350 pages. Can I limit the dictionary size by ignoring less frequent words? If so, what is the rule for defining the size?</p>
",Text Classification / Sentiment Analysis,rule deciding dictionary size sentiment analysis massive datasets performing sentiment analysis fiction working around book page limit dictionary size ignoring le frequent word rule defining size
How to deal with length variations for text classification using CNN (Keras),"<p>It has been proved that CNN (convolutional neural network) is quite useful for text/document classification. I wonder how to deal with the length differences as the lengths of articles are different in most cases. Are there any examples in Keras?  Thanks!! </p>
",Text Classification / Sentiment Analysis,deal length variation text classification using cnn kera ha proved cnn convolutional neural network quite useful text document classification wonder deal length difference length article different case example kera thanks
How to answer questions from big documents?,"<p>About two weeks ago, due to the low demand in my project, I have been assigned a completely unusual request: to automatically extract answers from documents based on machine learning. I've never read anything about ML, AI or NLP before, so I've been basically doing just that for the past two weeks.</p>

<p>When it comes to ML, most book recommendations and tutorials I've found so far use the Python language and tools, so I took the first week to learn about Python, NumPy, Scikit, Panda, Matplotlib and so on. Then, this week I started reading about NLP itself, after spending a few days reading about generic ML algorithms.</p>

<p>So far, I've basically read about Bag of Words, using TF-IDF (or simply terms count) to convert the words to numeric representations and a few methods such as the gaussian and multinomial naive bayes methods to train and predict values. The methods also mention the importance of using the usual pre-processing methods such as lemmatization and alikes. However, basically all examples assume that a given text can be classified in one of the categorized topics, like the sentiment analysis use case. I'm afraid this doesn't represent my use case, so I'd like to describe it here so that you could help me identifying which methods I should be looking for.</p>

<p>We have a system with thousands of transactions/deals inputted manually by an specialized team. Each deal has a set of documents (a dozen per deal typically) and some documents could have hundreds of pages. The inputing team has to extract about a thousand fields from those documents for any particular deal. So, in our database we have all their data and we typically also know the document specific snippets associated to each field value.</p>

<p>So, my task is to, given a new document and deal, and based on the previous answers, fill in as many fields as I could by automatically finding the corresponding snippets in the new documents. I'm not sure how I should approach this problem.</p>

<p>For example, I could consider each sentence of the document as a separate document to be analyzed and compared to the snippets I already have for the matching data. However, I can't be sure whether some of those sentences would actually answer the question. For example, maybe there are 6 occurrences in the documents that would answer a particular question/field, but maybe the inputters only identified 2 or 3 of them.</p>

<p>Also, for any given sentence, it could tell that the answer for a given field is A or B, or it could be that there's absolutely no association between the sentence and the field/question, as it would be the case for most sentences. I know that Scikit provides the predict_proba method, so that I could try to only consider the sentence as relevant if the probabilities of answering the question would be above 80%, for example, but based on a few quick tests I've made with a few sentences and words, I suspect this won't work very well. Also, it could be quite slow to treat each sentence of a 500-hundreds of pages documents as a separate document to be analyzed, so I'm not sure if there are better methods to handle this use case.</p>

<p>Some of the fields are free-text ones, like company and firm names, for example, and I suspect those would be the hardest to answer, so I'm trying to start with the multiple-choice ones, with a finite set of classification.</p>

<p>How would you advise me to look at this problem? Are there any algorithms you'd recommend me to study for solving this particular problem?</p>

<p>Please forgive me for any imprecise/incorrect terms or understanding on this topic as this is all very new to me. Any help is very appreciated, thanks.</p>

<p>Update: I've been asked for more details, so here you are.</p>

<p>One of the fields is called ""Deal Structure"" and it could have the following values: ""Asset Purchase"", ""Stock or Equity Purchase"" or ""Public Target Merger"" (there are a few others, but this gives you an idea).</p>

<p>So, here are some sentences highlighted for Public Target Merger deals (those documents come from Edgar Filings public database which are freely available for US deals):</p>

<p>deal 1 / doc 1:
""AGREEMENT AND PLAN OF MERGER, dated as of March 14, 2018 (this “Agreement”), by and among HarborOne Bancorp, Inc., a Massachusetts corporation (“Buyer”), Massachusetts Acquisitions, LLC, a Maryland limited liability company of which Buyer is the sole member (“Merger LLC”), and Coastway Bancorp, Inc., a Maryland corporation (the “Company”).""</p>

<p>""WHEREAS, Buyer, Merger LLC, and the Company intend to effect a merger (the “Merger”) of Merger LLC with and into the Company in accordance with this Agreement and the Maryland General Corporation Law (the “MGCL”) and the Maryland Limited Liability Company Act, as amended (the “MLLCA”), with the Company to be the surviving entity in the Merger. The Merger will be followed immediately by a merger of the Company with and into Buyer (the “Upstream Merger”), with the Buyer to be the surviving entity in the Upstream Merger. It is intended that the Merger be mutually interdependent with and a condition precedent to the Upstream Merger and that the Upstream Merger shall, through the binding commitment evidenced by this Agreement, be effected immediately following the Effective Time (as defined below) without further approval, authorization or direction from or by any of the parties hereto; and""</p>

<p>deal 2 / doc 1:</p>

<p>""WHEREAS, it is also proposed that, as soon as practicable following the consummation of the Offer, the Parties wish to effect the acquisition of the Company by Parent through the merger of Purchaser with and into the Company, with the Company being the surviving entity (the “Merger”);""</p>

<p>Now, for Asset Purchase deals:</p>

<p>deal 3 / doc 1:</p>

<p>""Subject to the terms and conditions of this Agreement, Sellers are willing to sell to Buyer, and Buyer is willing to purchase from Sellers, all of their assets relating to the Businesses as set forth herein.""</p>

<p>deal 4 / doc 1:</p>

<p>""WHEREAS, Seller wishes to sell and assign to Buyer, and Buyer wishes to purchase and assume from Seller, the rights and obligations of Seller to the Purchased Assets (as defined herein), subject to the terms and conditions set forth herein.""</p>

<p>I hope those examples could help on defining the problem, please let me know in the comments and responses if you need more details.</p>
",Text Classification / Sentiment Analysis,answer question big document two week ago due low demand project assigned completely unusual request automatically extract answer document based machine learning never read anything ml ai nlp basically past two week come ml book recommendation tutorial found far use python language tool took first week learn python numpy scikit panda matplotlib week started reading nlp spending day reading generic ml algorithm far basically read bag word using tf idf simply term count convert word numeric representation method gaussian multinomial naive bayes method train predict value method also mention importance using usual pre processing method lemmatization alikes however basically example assume given text classified one categorized topic like sentiment analysis use case afraid represent use case like describe could help identifying method looking system thousand transaction deal inputted manually specialized team deal ha set document dozen per deal typically document could hundred page inputing team ha extract thousand field document particular deal database data typically also know document specific snippet associated field value task given new document deal based previous answer fill many field could automatically finding corresponding snippet new document sure approach problem example could consider sentence document separate document analyzed compared snippet already matching data however sure whether sentence would actually answer question example maybe occurrence document would answer particular question field maybe inputters identified also given sentence could tell answer given field b could absolutely association sentence field question would case sentence know scikit provides predict proba method could try consider sentence relevant probability answering question would example based quick test made sentence word suspect work well also could quite slow treat sentence hundred page document separate document analyzed sure better method handle use case field free text one like company firm name example suspect would hardest answer trying start multiple choice one finite set classification would advise look problem algorithm recommend study solving particular problem please forgive incorrect term understanding topic new help appreciated thanks update asked detail one field called deal structure could following value asset purchase stock equity purchase public target merger others give idea sentence highlighted public target merger deal document come edgar filing public database freely available u deal deal doc agreement plan merger dated march agreement among harborone bancorp inc massachusetts corporation buyer massachusetts llc maryland limited liability company buyer sole member merger llc coastway bancorp inc maryland corporation company whereas buyer merger llc company intend effect merger merger merger llc company accordance agreement maryland general corporation law mgcl maryland limited liability company act amended mllca company surviving entity merger merger followed immediately merger company buyer upstream merger buyer surviving entity upstream merger intended merger mutually interdependent condition precedent upstream merger upstream merger shall binding commitment evidenced agreement effected immediately following effective time defined without approval authorization direction party hereto deal doc whereas also proposed soon practicable following consummation offer party wish effect company parent merger purchaser company company surviving entity merger asset purchase deal deal doc subject term condition agreement seller willing sell buyer buyer willing purchase seller asset relating business set forth herein deal doc whereas seller wish sell assign buyer buyer wish purchase assume seller right obligation seller purchased asset defined herein subject term condition set forth herein hope example could help defining problem please let know comment response need detail
NLP - which technique to use to classify labels of a paragraph?,"<p>I'm fairly new to NLP and trying to learn the techniques that can help me get my job done. </p>

<p>Here is my task: I have to classify stages of a drilling process based on text memos. </p>

<p><a href=""https://i.sstatic.net/j6BUS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j6BUS.png"" alt=""enter image description here""></a></p>

<p>I have to classify labels for ""Activity"", ""Activity Detail"", ""Operation"" based on what's written in ""Com"" column. </p>

<p>I've been reading a lot of articles online and all the different kinds of techniques that I've read really confuses me. </p>

<p>The buzz words that I'm trying to understand are</p>

<ol>
<li>Skip-gram (prediction based method, Word2Vec)</li>
<li>TF-IDF (frequency based method)</li>
<li>Co-Occurrence Matrix (frequency based method)</li>
</ol>

<p>I am given about ~40,000 rows of data (pretty small, I know), and I came across an article that says neural-net based models like Skip-gram might not be a good choice if I have small number of training data. So I was also looking into frequency based methods too. Overall, I am unsure which technique is the best for me.</p>

<p>Here's what I understand:</p>

<ol>
<li>Skip-gram: technique used to represent words in a vector space. But I don't understand what to do next once I vectorized my corpus</li>
<li>TF-IDF: tells how important each word is in each sentence. But I still don't know how it can be applied on my problem</li>
<li>Co-Occurence Matrix: I don'y really understand what it is.</li>
<li>All the three techniques are to numerically represent texts. But I am unsure what step I should take next to actually classify labels.</li>
</ol>

<p>What approach &amp; sequence of techniques should I use to tackle my problem? If there's any open source Jupyter notebook project, or link to an article (hopefully with codes) that did the similar job done, please share it here.</p>
",Text Classification / Sentiment Analysis,nlp technique use classify label paragraph fairly new nlp trying learn technique help get job done task classify stage drilling process based text memo classify label activity activity detail operation based written com column reading lot article online different kind technique read really confuses buzz word trying understand skip gram prediction based method word vec tf idf frequency based method co occurrence matrix frequency based method given row data pretty small know came across article say neural net based model like skip gram might good choice small number training data wa also looking frequency based method overall unsure technique best understand skip gram technique used represent word vector space understand next vectorized corpus tf idf tell important word sentence still know applied problem co occurence matrix really understand three technique numerically represent text unsure step take next actually classify label approach sequence technique use tackle problem open source jupyter notebook project link article hopefully code similar job done please share
precision and recall in fastText?,"<p>I implement the fastText for text classification, link <a href=""https://github.com/facebookresearch/fastText/blob/master/tutorials/supervised-learning.md"" rel=""noreferrer"">https://github.com/facebookresearch/fastText/blob/master/tutorials/supervised-learning.md</a>
I was wondering what's the precision@1, or P@5 means? I did a binary classification, but I tested different number, I don't understand results:</p>

<pre><code>haos-mbp:fastText hao$ ./fasttext test trainmodel.bin train.valid 2
N   312
P@2 0.5
R@2 1
Number of examples: 312
haos-mbp:fastText hao$ ./fasttext test trainmodel.bin train.valid 1
N   312
P@1 0.712
R@1 0.712
Number of examples: 312
haos-mbp:fastText hao$ ./fasttext test trainmodel.bin train.valid 3
N   312
P@3 0.333
R@3 1
Number of examples: 312
</code></pre>
",Text Classification / Sentiment Analysis,precision recall fasttext implement fasttext text classification link wa wondering precision p mean binary classification tested different number understand result
NLP data preparation and sorting for text-classification task,"<p>I read a lot of tutorials on the web and topics on stackoverflow but one question is still foggy for me. If consider just the stage of collecting data for multi-label training, what way (see below) are better and whether are both of them acceptable and effective? </p>

<ol>
<li>Try to find 'pure' one-labeled examples at any cost.</li>
<li>Every example can be multi labeled.</li>
</ol>

<p>For instance, I have articles about war, politics, economics, culture. Usually, politics tied to economics, war connected to politics, economics issues may appear in culture articles etc. I can assign strictly one main theme for each example and drop uncertain works or assign 2, 3 topics. </p>

<p>I'm going to train data using Spacy, volume of data will be about 5-10 thousand examples per topic.</p>

<p>I'd be grateful for any explanation and/or a link to some relevant discussion.</p>
",Text Classification / Sentiment Analysis,nlp data preparation sorting text classification task read lot tutorial web topic stackoverflow one question still foggy consider stage collecting data multi label training way see better whether acceptable effective try find pure one labeled example cost every example multi labeled instance article war politics economics culture usually politics tied economics war connected politics economics issue may appear culture article etc assign strictly one main theme example drop uncertain work assign topic going train data using spacy volume data thousand example per topic grateful explanation link relevant discussion
How to extract most significant verb from text strings data in R,"<p>I have a text classification task for which I am trying to extract most significant verb from the text corpus.
For eg:  </p>

<ol>
<li>Text=""Mailing the meeting notes"" : Significant verb = Mail  </li>
<li>Text=""Call to set up meeting."" : Significant verb Call.
How do I figure which is the most important verb?</li>
</ol>
",Text Classification / Sentiment Analysis,extract significant verb text string data r text classification task trying extract significant verb text corpus eg text mailing meeting note significant verb mail text call set meeting significant verb call figure important verb
Different accuracy for the same code in text classification in keras,"<p>I'm training a recurrent neural network based on LSTM for text classification and I have a strange behaviour. With the same code and same training set I obtain very different level of accuracy. </p>

<p>I know it's normal to have different value but sometimes i get value of 40% accuracy and others 90%. How is it possible?</p>

<p>Moreover sometimes I get ""stuck"" on the accuracy level, I mean that the loss and the accuracy doesn't change during the different epochs so both values remain the same. which is the  explanation for this?</p>

<p>Another aspect that I'm not sure that i truly understand is the fact of the padding for my input text (I'm using training in batch). I think, since I'm using a RNN, should be better use a left padding. Do you know how to pad the input? Is better left or right padding?</p>

<p>Last question is how to choose the number of layer and the number of node. I know that for someone the best approach is by experiment but some suggestion could be useful.</p>

<p>This is the implementation of my RNN:</p>

<pre><code>sentence_indices = Input(shape=input_shape, dtype=np.int32)
embedding_layer =  pretrained_embedding_layer(word_to_vec_map, word_to_index, emb_dim)

embeddings = embedding_layer(sentence_indices)   
X = LSTM(128, return_sequences=True)(embeddings)
X = Dropout(0.5)(X)
X = LSTM(128)(X)
X = Dropout(0.5)(X)
X = Dense(num_activation, activation='softmax')(X)
X =  Activation('softmax')(X)
model = Model(sentence_indices, X)
</code></pre>

<ul>
<li>the embeddings layer came from GloVe, a model pre-trained. </li>
<li>I'm using 5 categories and 300 samples more or less</li>
<li>My training set is not uniform</li>
</ul>

<p>I'll appreciate any hint of the question above and also some suggestion to improve my neural network.</p>
",Text Classification / Sentiment Analysis,different accuracy code text classification kera training recurrent neural network based lstm text classification strange behaviour code training set obtain different level accuracy know normal different value sometimes get value accuracy others possible moreover sometimes get stuck accuracy level mean loss accuracy change different epoch value remain explanation another aspect sure truly understand fact padding input text using training batch think since using rnn better use left padding know pad input better left right padding last question choose number layer number node know someone best approach experiment suggestion could useful implementation rnn embeddings layer came glove model pre trained using category sample le training set uniform appreciate hint question also suggestion improve neural network
r text mining extract keywords,"<p>I am new to R and working on a machine learning problem, I understand that machine learning requires labelled data to make accurate prediction.</p>

<p>I am working with text data where the reviews given by users for a particular mobile app in text format. My primary task is to first extract the main keywords(features) </p>

<p>The text data is as follows in the CSV file in the 'review' column</p>

<pre><code>review1  - ""the gps does not work"",
review2  - ""tracking of phone is inconsistent"",
review3  - ""the battery is draining fast"",
review4  - ""the tracks disappear after some time"",
review5  - ""the app consumes the battery lot because of gps""
</code></pre>

<p>now I want to extract the feature mentioned in the each review such as
""gps"", ""tracking"", ""battery"", ""tracks"",""battery gps"" and add it as a label next to it respectively in the CSV file; so there would be one more column created in the CSV file as 'Feature'. 
So my CSV will have 2 columns, one review and one feature column that will highlight the feature mentioned in the review.The snapshot of the data in the CSV will be as follows <a href=""https://i.sstatic.net/H1VGs.jpg"" rel=""nofollow noreferrer"">new csv file data</a></p>

<p>I have written a sample code mentioned below for the same but since I need to deal with thousands of reviews I need to get the Feature column in my csv file which will act as a label for feature prediction</p>

<pre><code>#Feature Prediction
library(tm)
library(e1071)
texts &lt;- c(""the gps does not work"",
           ""tracking of phone is inconsistent"",
           ""the battery is draining fast"",
           ""the tracks disappear after some time"",
           ""the app consumes the battery a lot"")

features &lt;- c(""gps"", ""tracking"", ""battery"", ""tracks"",""battery"")
docs &lt;- VCorpus(VectorSource(texts))
# Clean corpus
docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeNumbers)
docs &lt;- tm_map(docs, removeWords, stopwords(""english""))
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, stripWhitespace)
dtm &lt;- DocumentTermMatrix(docs)
# Transform dtm to matrix to data frame - df is easier to work with
mat.df &lt;- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df &lt;- cbind(mat.df, features)
View(mat.df)
# Split data by rownumber into two equal portions (Train and Test Data)
train &lt;- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test &lt;- (1:nrow(mat.df))[- train]
# Isolate classifier
cl &lt;- mat.df[, ""features""]
# Create model data and remove ""features""
modeldata &lt;- mat.df[,!colnames(mat.df) %in% ""features""]
feature_pred &lt;- naiveBayes(modeldata[train,], cl[train])
naiv_pred &lt;- predict(feature_pred, modeldata[test,])
conf.mat &lt;- table(""Predictions"" = naiv_pred, Actual = cl[test])
conf.mat
(accuracy &lt;- sum(diag(conf.mat))/length(test) * 100)
</code></pre>
",Text Classification / Sentiment Analysis,r text mining extract keywords new r working machine learning problem understand machine learning requires labelled data make accurate prediction working text data review given user particular mobile app text format primary task first extract main keywords feature text data follows csv file review column want extract feature mentioned review gps tracking battery track battery gps add label next respectively csv file would one column created csv file feature csv column one review one feature column highlight feature mentioned review snapshot data csv follows new csv file data written sample code mentioned since need deal thousand review need get feature column csv file act label feature prediction
Web/browser-oriented open source machine learning projects?,"<p>Applying machine learning techniques, more specifically text mining techniques, in browser environment (mainly Javascript) or as a web application is not a very widely discussed topic.</p>

<p>I want to build my own web application / browser extension that can accomplish certain level of text classification / visualization techniques. I would like to know, if there is any open source projects that apply text mining techniques in web application or even better as browser extensions? </p>

<p>So far, these are the projects/discussions I gathered with days of random searching:</p>

<p><strong>For text mining in web application:</strong></p>

<ul>
<li><a href=""http://text-processing.com/"" rel=""nofollow noreferrer"">http://text-processing.com/</a> with <a href=""http://text-processing.com/demo/"" rel=""nofollow noreferrer"">demo</a> (Close source, with limited api)</li>
<li><a href=""http://www.uclassify.com/Default.aspx"" rel=""nofollow noreferrer"">uClassify</a> (close source, no info about library base)<br> </li>
</ul>

<p><strong>For machine learning in Javascript:</strong></p>

<ul>
<li><a href=""http://news.ycombinator.com/item?id=1704648"" rel=""nofollow noreferrer"">Discussion</a> on the possibility about Machine learning in
JavaScript. (mainly about saying Node.js is going to change the landscape)</li>
<li><a href=""http://harthur.github.com/brain/"" rel=""nofollow noreferrer"">brain - javascript supervised machine learning</a></li>
<li>A <a href=""http://www.dusbabek.org/~garyd/bayes/"" rel=""nofollow noreferrer"">demo project</a> with Naive Bayes implemented in Javascript</li>
</ul>

<p>For web application text mining, the architect that I can think of:</p>

<ul>
<li>Python libraries (e.g. NLTK or scikit-learn) + Django</li>
<li>Java libraries (a lot) + Play! framework</li>
<li>Even R based + <a href=""http://rapache.net/"" rel=""nofollow noreferrer"">rApache</a></li>
</ul>
",Text Classification / Sentiment Analysis,web browser oriented open source machine learning project applying machine learning technique specifically text mining technique browser environment mainly javascript web application widely discussed topic want build web application browser extension accomplish certain level text classification visualization technique would like know open source project apply text mining technique web application even better browser extension far project discussion gathered day random searching text mining web application demo close source limited api uclassify close source info library base machine learning javascript discussion possibility machine learning javascript mainly saying node j going change landscape brain javascript supervised machine learning demo project naive bayes implemented javascript web application text mining architect think python library e g nltk scikit learn django java library lot play framework even r based rapache
Multiple input parameters during text classification - Scikit learn,"<p>I'm new to machine learning. I'm trying to do some text classification. 'CleanDesc' has the text sentence. And 'output' has the corresponding output. Initially i tried using one input parameter which is the string of texts(newMerged.cleanDesc) and one output parameter(newMerged.output)</p>

<pre><code>finaldata = newMerged[['id','CleanDesc','type','output']]

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(newMerged.CleanDesc)

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

clf = MultinomialNB().fit(X_train_tfidf, newMerged.output)    
testdata = newMerged.ix[1:200]
X_test_counts = count_vect.transform(testdata.CleanDesc)
X_test_tfidf = tfidf_transformer.transform(X_test_counts)

predicted = clf.predict(X_new_tfidf)
</code></pre>

<p>This works fine. But the accuracy is very low. I wanted to include one more parameter(newMerged.type) as the input, along with the text to try improving it. Can I do that? How do I do it. newMerged.type is not a text. It just a two character string like ""HT"". I tried doing it as follows, but it failed,</p>

<pre><code>finaldata = newMerged[['id','CleanDesc','type','output']]

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(newMerged.CleanDesc)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

clf = MultinomialNB().fit([[X_train_tfidf,newMerged.type]], 
newMerged.output)    
testdata = newMerged.ix[1:200]
X_test_counts = count_vect.transform(testdata.CleanDesc)
X_test_tfidf = tfidf_transformer.transform(X_test_counts)

predicted = clf.predict([[X_new_tfidf, testdata.type]])
</code></pre>
",Text Classification / Sentiment Analysis,multiple input parameter text classification scikit learn new machine learning trying text classification cleandesc ha text sentence output ha corresponding output initially tried using one input parameter string text newmerged cleandesc one output parameter newmerged output work fine accuracy low wanted include one parameter newmerged type input along text try improving newmerged type text two character string like ht tried follows failed
Sentiment analysis for sentences with overall positive sentiment but have negative words,"<p>I am trying to work on a sentiment analysis tool for reviews data. I came across a few edge-cases where the overall sentiment of a sentence may be positive but contain a negative word.  <br>
For example : </p>

<blockquote>
  <p>I want this iPad so bad. <br>
  F*ck yes, it looks good!</p>
</blockquote>

<p>I tried 2-3 sentiment analyzing libraries and they infer such sentences with negative sentiment. I have not found any work which is looking into such solutions. 
Is there any known solution to handle sentiment in such contextual cases?</p>

<p>I tried mostly lexicon based approaches. I used NLTK, SPACY, <a href=""https://www.ibm.com/watson/services/tone-analyzer/"" rel=""nofollow noreferrer"">IBM tone analyzer</a>, TextBlob, VADER. Currently I am averaging the summation of results from all of them. </p>
",Text Classification / Sentiment Analysis,sentiment analysis sentence overall positive sentiment negative word trying work sentiment analysis tool review data came across edge case overall sentiment sentence may positive contain negative word example want ipad bad f ck yes look good tried sentiment analyzing library infer sentence negative sentiment found work looking solution known solution handle sentiment contextual case tried mostly lexicon based approach used nltk spacy ibm tone analyzer textblob vader currently averaging summation result
How to optimize a model in a text classification task?,"<p>I am training a text classifier, using TextCNN, as a result, the <code>precision</code> is low(0.6), while the <code>recall</code> is high (0.9), as a fresher in deep-learning, I can not find the optimizing methods to raise the <code>recall</code>, I know I should observe the training data, check if the model is feeded with the related data.
but is there others reasons? is there some guide for model optimization？</p>
",Text Classification / Sentiment Analysis,optimize model text classification task training text classifier using textcnn result low high fresher deep learning find optimizing method raise know observe training data check model feeded related data others reason guide model optimization
treetagger module returns empty list,"<p>I made a sentiment analysis program with treetagger. It worked fine two weeks ago but now it doesn't works properly. </p>

<p>After that I used treetagger in a very simple program which returns ""hello world""'s tagging. It doesn't work properly again. I coded like this :</p>

<pre><code>from treetagger import TreeTagger

tt = TreeTagger(path_to_treetagger = ""./treetagger/"") # treetagger file is in the same directory

tt.tag(""hello world"")
</code></pre>

<p>It returns this =>  [['']]</p>

<p>I fulfilled the direcions from ""<a href=""http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"" rel=""nofollow noreferrer"">http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/</a>"" to install treetagger. And I am using treetagger.py as a module from <a href=""https://github.com/miotto/treetagger-python"" rel=""nofollow noreferrer"">https://github.com/miotto/treetagger-python</a> . All of the files are in the same directory .</p>

<p>I am using Python 3.6.7 and GCC version is 7.3.0. My OS is Kali Linux with dual boot Windows 10 on a Laptop. </p>

<p>I formatted and reinstalled Kali to my laptop because of corruption in pip. So treetagger was working before this operation. I think that is the case but I am not sure. </p>

<p>The question is briefly: ""How to use treetagger properly?"" 
What do you think?</p>

<p>Thanks in advance...</p>
",Text Classification / Sentiment Analysis,treetagger module return empty list made sentiment analysis program treetagger worked fine two week ago work properly used treetagger simple program return hello world tagging work properly coded like return fulfilled direcions install treetagger using treetagger py module file directory using python gcc version kali linux dual boot window laptop formatted reinstalled kali laptop corruption pip treetagger wa working operation think case sure question briefly use treetagger properly think thanks advance
How to add emoji to the Keras Tokenizer API?,"<p>I am doing a Twitter sentiment analysis project. It has been demonstrated from some literature that the use of information from emoji and emoticon could improve the performance of a sentiment classifier on Twitter data(such as a work done by IBM <a href=""https://arxiv.org/ftp/arxiv/papers/1511/1511.02556.pdf"" rel=""nofollow noreferrer"">Sentiment Expression via Emoticons on Social Media</a> in 2015). Moreover, the emoji2vec project <a href=""https://github.com/uclmr/emoji2vec"" rel=""nofollow noreferrer"">emoji2vec</a> which could create the representation of each emoji based on emoji descriptions <a href=""https://raw.githubusercontent.com/uclmr/emoji2vec/master/data/raw_training_data/emoji_joined.txt"" rel=""nofollow noreferrer"">emoji description</a> is really helpful for Twitter sentiment analysis.</p>

<p>Now, I am using Keras to construct the sequential model to do this sentiment classification. But my question is since before constructing all the sequential models, you should pass your text data to the Tokenizer API first:</p>

<pre><code>tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(df['Phrase'])
sequences = tokenizer.texts_to_sequences(df['Phrase'])
data = pad_sequences(sequences, maxlen=50)
</code></pre>

<p>where <code>df</code> is my pandas dataframe. Hence, is it possible to add emoji to the Tokenizer(since the Tokenizer API first select the top <code>vocabulary size</code> most frequent words and construct the word-index pair)? The emojis are apparently less frequent than the words and they are quite significant features in sentiment classification. Hence, I want to add emojis to the keras Tokenizer API and create emojis' emoji-index pair.</p>

<p>When it comes to model, I am constructing a BiLSTM model with pre-trained embedding(such as trained by FastText). How could I combine the emoji representation and the word representation in this task? The following code shows my BiLSTM model:</p>

<pre><code># BiLSTM model with Conv1D and fasttext word embedding
def get_bi_lstm_model(embedding_matrix):
    model = Sequential()
    model.add(Embedding(input_dim=vocabulary_size, output_dim=dim, input_length=input_length, 
                        weights=[embedding_matrix], trainable=False, name='embedding_1'))
    model.add(Dropout(0.2, name='dropout_1'))
    model.add(Conv1D(64, 5, activation='relu', name='conv1d_1'))
    model.add(MaxPooling1D(pool_size=4, name='maxpooling_1'))
    model.add(Bidirectional(LSTM(lstm_output_dim, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat', 
              name='bidirectional_1'))
    model.add(Flatten(name = 'flatten_1'))
    model.add(Dense(3, activation='softmax', name='dense_1'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', f1_score])
    return model
</code></pre>

<p>Any help and insights would be appreciated! Thanks! Merry Christmas!</p>
",Text Classification / Sentiment Analysis,add emoji kera tokenizer api twitter sentiment analysis project ha demonstrated literature use information emoji emoticon could improve performance sentiment classifier twitter data work done ibm sentiment expression via emoticon social medium moreover emoji vec project emoji vec could create representation emoji based emoji description emoji description really helpful twitter sentiment analysis using kera construct sequential model sentiment classification question since constructing sequential model pas text data tokenizer api first panda dataframe hence possible add emoji tokenizer since tokenizer api first select top frequent word construct word index pair emojis apparently le frequent word quite significant feature sentiment classification hence want add emojis kera tokenizer api create emojis emoji index pair come model constructing bilstm model pre trained embedding trained fasttext could combine emoji representation word representation task following code show bilstm model help insight would appreciated thanks merry christmas
Which architecture of Neural Network gives better accuracy for text classification?,"<p>I have tried and searched, found that RNNs give better results. Which to use: LSTMs or GRU or traditional RNN or CNN?</p>
",Text Classification / Sentiment Analysis,architecture neural network give better accuracy text classification tried searched found rnns give better result use lstms gru traditional rnn cnn
Why is this TF-IDF sentiment analysis classifier performing so well?,"<p><a href=""https://github.com/DenJev/NLPAirline/blob/master/AirlinesNotebook.ipynb"" rel=""nofollow noreferrer"">Jupter Notebook</a></p>

<p>The last confusion matrix is for the test set. Is this a case of overfitting with logistic regression? Because even when not pre-processing the text much (including emoticons, punctuation) the accuracy is still very good. Good anyone give some help/advice?</p>
",Text Classification / Sentiment Analysis,tf idf sentiment analysis classifier performing well jupter notebook last confusion matrix test set case overfitting logistic regression even pre processing text much including emoticon punctuation accuracy still good good anyone give help advice
What do I need to know on NLP to be able to use and train Stanford NLP for intent analysis?,"<p>Any books, tutorials, course reccommedations would be much appreciated.</p>

<p>I need to know at what level I need to be regarding NLP to be able to comprehend the Stanford NLP and train it to customize it for my app of commercial sentiment analysis.</p>

<p>My goal is not a career in NLP or become an expert in NLP but only to be as much proficient to be able to understand and use the open source NLP frameworks properly and train them for my application.</p>

<p>For this level, what NLP study/training would be needed?</p>

<p>I'm learning c# and .net as well.</p>
",Text Classification / Sentiment Analysis,need know nlp able use train stanford nlp intent analysis book tutorial course reccommedations would much appreciated need know level need regarding nlp able comprehend stanford nlp train customize app commercial sentiment analysis goal career nlp become expert nlp much proficient able understand use open source nlp framework properly train application level nlp study training would needed learning c net well
Using pretrained Word2Vec model for sentiment analysis,"<p>I am using a pretrained Word2Vec model for tweets to create vectors for each word. <a href=""https://www.fredericgodin.com/software/"" rel=""nofollow noreferrer"">https://www.fredericgodin.com/software/</a>.  I will then compute the average of this and use a classifier to determine sentiment. </p>

<p>My training data is very large and the pretrained Word2Vec model has been trained on millions of tweets, with dimensionality = 400. My problem is that it is taking too long to give vectors to the words in my training data. Is there a way to reduce the time taken to build the word vectors? </p>

<p>Cheers. </p>
",Text Classification / Sentiment Analysis,using pretrained word vec model sentiment analysis using pretrained word vec model tweet create vector word compute average use classifier determine sentiment training data large pretrained word vec model ha trained million tweet dimensionality problem taking long give vector word training data way reduce time taken build word vector cheer
How to do text classification with DeepPavlov,"<p>I am interested in doing text classification with <a href=""http://deeppavlov.ai"" rel=""nofollow noreferrer"">DeepPavlov</a> chatbot framework. </p>

<p>The problem is I don't have enough training data. Ideally, I would like to do text classification with just few samples for each class.</p>
",Text Classification / Sentiment Analysis,text classification deeppavlov interested text classification deeppavlov chatbot framework problem enough training data ideally would like text classification sample class
nltk bags of words showing emotions,"<p>i am working on NLP using python and nltk. </p>

<p>I was wondering whether is there any dataset which have bags of words which shows keywords relating to emotions such as happy, joy, anger, sadness and etc</p>

<p>from what i dug up in the nltk corpus, i see there are some sentiment analysis corpus which contain positive and negative review which doesn't exactly related to keywords showing emotions.</p>

<p>Is there anyway which i could build my own dictionary containing words which shows emotion for this purpose? is so, how do i do it and is there any collection of such words?</p>

<p>Any help would be greatly appreciated </p>
",Text Classification / Sentiment Analysis,nltk bag word showing emotion working nlp using python nltk wa wondering whether dataset bag word show keywords relating emotion happy joy anger sadness etc dug nltk corpus see sentiment analysis corpus contain positive negative review exactly related keywords showing emotion anyway could build dictionary containing word show emotion purpose collection word help would greatly appreciated
Text classification by pattern,"<p>Could you recomend me best way how to do it: i have a list phrases, for example [""free flower delivery"",""flower delivery Moscow"",""color + home delivery"",""flower delivery + delivery"",""order flowers + with delivery"",""color delivery""] and pattern - ""flower delivery"". I need to get list with phrases as close as possible to pattern. </p>

<p>Could you give some advice to how to do it? </p>
",Text Classification / Sentiment Analysis,text classification pattern could recomend best way list phrase example free flower delivery flower delivery moscow color home delivery flower delivery delivery order flower delivery color delivery pattern flower delivery need get list phrase close possible pattern could give advice
Text classification on CNN model,"<p>The problem is i need to load the file which i have in h5 format as below</p>
<pre><code>from keras.models import load_model
model = load_model('my_model.h5')
model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])
classes = model.predict_classes(&quot;How is the weather today&quot;)
print classes
</code></pre>
<p>And also i need that percentage value of the prediction to be printed</p>
<p><a href=""https://github.com/jatana-research/Text-Classification/blob/master/CNN.ipynb"" rel=""nofollow noreferrer"">Here is the link that i refered to while generating this model and saving the file</a></p>
",Text Classification / Sentiment Analysis,text classification cnn model problem need load file h format also need percentage value prediction printed link refered generating model saving file
NLP Categorizing Details with Confidence Values,"<h2>Background</h2>

<p>I'm writing a Swift application that requires the classification of user events by <code>categories</code>. These categories can be things like:</p>

<ul>
<li><code>Athletics</code></li>
<li><code>Cinema</code></li>
<li><code>Food</code></li>
<li><code>Work</code></li>
</ul>

<p>However, I have a <strong>set list</strong> of these categories, and do not wish to make any more than the minimal amount I believe is needed to be able to classify any type of event.</p>

<h2>Question</h2>

<p>Is there a machine learning (nlp) procedure that does the following?</p>

<ol>
<li>Takes a block of text (in my case, a description of an event).</li>
<li>Creates a ""percentage match"" to each possible classification.</li>
</ol>

<p>For instance, suppose the description of an event is as follows:</p>

<blockquote>
  <p>Fun, energetic bike ride for people of all ages.</p>
</blockquote>

<p>The algorithm in which this description would be passed in would return an object that looks something like this:</p>

<pre><code>{
    athletics: 0.8,
    cinema: 0.1,
    food: 0.06,
    work: 0.04
}
</code></pre>

<p>where the values of each key in the object is a <strong>confidence</strong>.</p>

<p>If anyone can guide me in the right direction (or even send some general resources or solutions specific to iOS dev), I'd be super appreciative!</p>
",Text Classification / Sentiment Analysis,nlp categorizing detail confidence value background writing swift application requires classification user event category thing like however set list category wish make minimal amount believe needed able classify type event question machine learning nlp procedure doe following take block text case description event creates percentage match possible classification instance suppose description event follows fun energetic bike ride people age algorithm description would passed would return object look something like value key object confidence anyone guide right direction even send general resource solution specific io dev super appreciative
How can a machine learning model handle unseen data and unseen label?,"<p>I am trying to solve a text classification problem. I have a limited number of labels that capture the category of my text data. If the incoming text data doesn't fit any label, it is tagged as 'Other'. In the below example, I built a text classifier to classify text data as 'breakfast' or 'italian'. In the test scenario, I included couple of text data that do not fit into the labels that I used for training. This is the challenge that I'm facing. Ideally, I want the model to say - 'Other' for 'i like hiking' and 'everyone should understand maths'. How can I do this? </p>

<pre><code>import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer

X_train = np.array([""coffee is my favorite drink"",
                    ""i like to have tea in the morning"",
                    ""i like to eat italian food for dinner"",
                    ""i had pasta at this restaurant and it was amazing"",
                    ""pizza at this restaurant is the best in nyc"",
                    ""people like italian food these days"",
                    ""i like to have bagels for breakfast"",
                    ""olive oil is commonly used in italian cooking"",
                    ""sometimes simple bread and butter works for breakfast"",
                    ""i liked spaghetti pasta at this italian restaurant""])

y_train_text = [""breakfast"",""breakfast"",""italian"",""italian"",""italian"",
                ""italian"",""breakfast"",""italian"",""breakfast"",""italian""]

X_test = np.array(['this is an amazing italian place. i can go there every day',
                   'i like this place. i get great coffee and tea in the morning',
                   'bagels are great here',
                   'i like hiking',
                   'everyone should understand maths'])

classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', MultinomialNB())])

classifier.fit(X_train, y_train_text)
predicted = classifier.predict(X_test)
proba = classifier.predict_proba(X_test)
print(predicted)
print(proba)

['italian' 'breakfast' 'breakfast' 'italian' 'italian']
[[0.25099411 0.74900589]
 [0.52943091 0.47056909]
 [0.52669142 0.47330858]
 [0.42787443 0.57212557]
 [0.4        0.6       ]]
</code></pre>

<p>I consider the 'Other' category as noise and I cannot model this category. </p>
",Text Classification / Sentiment Analysis,machine learning model handle unseen data unseen label trying solve text classification problem limited number label capture category text data incoming text data fit label tagged example built text classifier classify text data breakfast italian test scenario included couple text data fit label used training challenge facing ideally want model say like hiking everyone understand math consider category noise model category
Text classification + Bag of words + Python : Bag of words doesn&#39;t show document index,"<p>I have written the following code to produce bag of words:</p>

<pre><code>count_vect = CountVectorizer()
final_counts = count_vect.fit_transform(data['description'].values.astype('U'))
vocab = count_vect.get_feature_names()
print(type(final_counts)) #final_counts is a sparse matrix
print(""--------------------------------------------------------------"")
print(final_counts.shape)
print(""--------------------------------------------------------------"")
print(final_counts.toarray())
print(""--------------------------------------------------------------"")
print(final_counts[769].shape)
print(""--------------------------------------------------------------"")
print(final_counts[769])
print(""--------------------------------------------------------------"")
print(final_counts[769].toarray())
print(""--------------------------------------------------------------"")
print(len(vocab))
print(""--------------------------------------------------------------"")
</code></pre>

<p>I am getting following output:</p>

<pre><code>&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
--------------------------------------------------------------
(770, 10252)
--------------------------------------------------------------
[[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
--------------------------------------------------------------
(1, 10252)
--------------------------------------------------------------
  (0, 4819) 1
  (0, 2758) 1
  (0, 3854) 2
  (0, 3987) 1
  (0, 1188) 1
  (0, 3233) 1
  (0, 981)  1
  (0, 10065)    1
  (0, 9811) 1
  (0, 8932) 1
  (0, 9599) 1
  (0, 10150)    1
  (0, 7716) 1
  (0, 10045)    1
  (0, 5783) 1
  (0, 5500) 1
  (0, 5455) 1
  (0, 3234) 1
  (0, 7107) 1
  (0, 6504) 1
  (0, 3235) 1
  (0, 1625) 1
  (0, 3591) 1
  (0, 6525) 1
  (0, 365)  1
  : :
  (0, 5527) 1
  (0, 9972) 1
  (0, 4526) 3
  (0, 3592) 4
  (0, 10214)    1
  (0, 895)  1
  (0, 10062)    2
  (0, 10210)    1
  (0, 1246) 1
  (0, 9224) 2
  (0, 4924) 1
  (0, 6336) 2
  (0, 9180) 8
  (0, 6366) 2
  (0, 414)  12
  (0, 1307) 1
  (0, 9309) 1
  (0, 9177) 1
  (0, 3166) 1
  (0, 396)  1
  (0, 9303) 7
  (0, 320)  5
  (0, 4782) 2
  (0, 10088)    3
  (0, 4481) 3
--------------------------------------------------------------
[[0 0 0 ... 0 0 0]]
--------------------------------------------------------------
10252
--------------------------------------------------------------
</code></pre>

<p>It's clear that there are 770 documents and 10,252 unique words in the corpus. My confusion is why is this line <code>print(final_counts[769])</code> in my code printing this:</p>

<pre><code>(0, 4819) 1
  (0, 2758) 1
  (0, 3854) 2
  (0, 3987) 1
  (0, 1188) 1
  (0, 3233) 1
  (0, 981)  1
  (0, 10065)    1
  (0, 9811) 1
  (0, 8932) 1
  (0, 9599) 1
  (0, 10150)    1
  (0, 7716) 1
  (0, 10045)    1
  (0, 5783) 1
  (0, 5500) 1
  (0, 5455) 1
  (0, 3234) 1
  (0, 7107) 1
  (0, 6504) 1
  (0, 3235) 1
  (0, 1625) 1
  (0, 3591) 1
  (0, 6525) 1
  (0, 365)  1
  : :
  (0, 5527) 1
  (0, 9972) 1
  (0, 4526) 3
  (0, 3592) 4
  (0, 10214)    1
  (0, 895)  1
  (0, 10062)    2
  (0, 10210)    1
  (0, 1246) 1
  (0, 9224) 2
  (0, 4924) 1
  (0, 6336) 2
  (0, 9180) 8
  (0, 6366) 2
  (0, 414)  12
  (0, 1307) 1
  (0, 9309) 1
  (0, 9177) 1
  (0, 3166) 1
  (0, 396)  1
  (0, 9303) 7
  (0, 320)  5
  (0, 4782) 2
  (0, 10088)    3
  (0, 4481) 3
</code></pre>

<p>The first index is the document index. I am printing the vector of 769th document (started from 0). So the first index should have been 769 instead of 0, like, <code>(769, 4819)    1</code> . Why isn't it so?</p>
",Text Classification / Sentiment Analysis,text classification bag word python bag word show document index written following code produce bag word getting following output clear document unique word corpus confusion line code printing first index document index printing vector th document started first index instead like
R - How to apply terms from training document-term-matrix (dtm) to test dtm (both unigrams and bigrams)?,"<p>I am training a simple text classification method on 1,000 training examples and would like to make predictions on unseen test data (about 500,000 observations).</p>

<p>The script is working fine, when I work only with unigrams. However, I am not sure how to use <code>control = list(dictionary=Terms(dtm_train_unigram))</code> when working with unigrams and bigrams as I have two separate document-term-matrices (one for unigrams, one for bigrams, see below):</p>

<pre><code>  UnigramTokenizer &lt;- function(x) unlist(lapply(NLP::ngrams(words(x), 1), paste, collapse = "" ""), use.names = FALSE)
  dtm_train_unigram &lt;- DocumentTermMatrix(processed_dataset, control = list(tokenize = UnigramTokenizer, wordLengths=c(3,20), bounds = list(global = c(4,Inf))))

  BigramTokenizer &lt;- function(x) unlist(lapply(NLP::ngrams(words(x), 2), paste, collapse = "" ""), use.names = FALSE)
  dtm_train_bigram &lt;- DocumentTermMatrix(processed_dataset, control = list(tokenize = BigramTokenizer, wordLengths=c(6,20), bounds = list(global = c(7,Inf))))
</code></pre>

<p>To ensure that the test set has the same terms as the training set, I use the following function:</p>

<pre><code>corpus_test &lt;- VCorpus(VectorSource(test_set))
dtm_test &lt;- DocumentTermMatrix(corpus_test, control = list(dictionary=Terms(dtm_train_unigram), wordLengths = c(3,20)))
</code></pre>

<p>How do I feed the terms of both the <code>dtm_train_unigram</code> and the <code>dtm_train_bigram</code> to the dtm_test?</p>

<ol>
<li>Can I combine <code>dtm_train_unigram</code> and <code>dtm_train_bigram</code> to a single dtm after creating them separately (as currently done)?  </li>
<li>Can I simplify my two-step Tokenizer function so I only create a single
dtm with unigrams and bigrams in the first place?</li>
</ol>

<p>Thank you!</p>
",Text Classification / Sentiment Analysis,r apply term training document term matrix dtm test dtm unigrams bigram training simple text classification method training example would like make prediction unseen test data observation script working fine work unigrams however sure use working unigrams bigram two separate document term matrix one unigrams one bigram see ensure test set ha term training set use following function feed term dtm test combine single dtm creating separately currently done simplify two step tokenizer function create single dtm unigrams bigram first place thank
How to classify the text which are not belong to the classes which I have to &quot;Unknown&quot; in text classifcation?,"<p>I am working on a NLP problem to classify the text to four classes. 
1. Sports
2. Entertainment
3. Astrology
4. Unknown</p>

<p>I have created a training dataset for Sports, Entertainment, Astrology. But How to create a training dataset for ""Unknown"" category or how to classify the text which are not belong to first three category to the last category i.e ""Unknown category""</p>
",Text Classification / Sentiment Analysis,classify text belong class unknown text classifcation working nlp problem classify text four class sport entertainment astrology unknown created training dataset sport entertainment astrology create training dataset unknown category classify text belong first three category last category e unknown category
I want to classify some sentences on the basis of their semantic meaning.How can I use Doc2Vec in this? Or is there a better approach than this?,"<p>I want to implement doc2vec on various reviews which we extracted from a source.And I want to classify these reviews into different classes defined by the user. How can I do this?</p>
",Text Classification / Sentiment Analysis,want classify sentence basis semantic meaning use doc vec better approach want implement doc vec various review extracted source want classify review different class defined user
How to use AWS SKD for .Net in VBA?,"<p>I am trying to add AWS Comprehend NLP to my VBA project but there seems to be no support for VBA. AWS does provide a .NET SDK. What is the best way to make classes from this SDK available to my VBA code?</p>

<p>Here is a mockup of a VBA module to pass a string of text to Comprehend and get the sentiment analysis back. Obviously, it doesn't actually work.</p>

<p>This module could be very helpful to anyone working in VBA and wanting to add AWS services to their toolkit. Can you help?</p>

<pre><code>Function putAWSComprehendJSON(link As String, apiKey As String, apiSecret As String, text2comp As String) As Dictionary
'link=https://Comprehend.us-east-1.amazonaws.com
Dim endpoint as String
endpoint = ""/api/endpointX""
'VBA-JSON module installed from github to handle Json packing and unpacking
Dim jsonResponse As Dictionary
Dim jsonRequest As Dictionary
Set jsonRequest = New Dictionary
jsonRequest.Add key:=""action"", item:=""detect-sentiment""
jsonRequest.Add key:=""content"", item:=text2comp
jsonRequest.Add key:=""content-usage-code"", item:=""en""
jsonRequest.Add key:=""region"", item:=""usl-east-1""
jsonRequestBody = JsonConverter.ConvertToJson(jsonRequest)
Dim web As msxml2.XMLHTTP60
Set web = New msxml2.XMLHTTP60
With web
    .Open ""POST"", link &amp; endpoint, False, apiKey, apiSecret
    .setRequestHeader ""cache-control"", ""private""
    .setRequestHeader ""Cache-Control"", ""max-age=0""
    .setRequestHeader ""Content-type"", ""application/json""
    .setRequestHeader ""Authorization"", ""Basic "" &amp; EncodeBase64
    .send jsonRequestBody
End With
While .readyState 4
DoEvents
Wend
Set putAWSComprehendJSON = JsonConverter.ParseJson(web.responseText)
End Function              
</code></pre>
",Text Classification / Sentiment Analysis,use aws skd net vba trying add aws comprehend nlp vba project seems support vba aws doe provide net sdk best way make class sdk available vba code mockup vba module pas string text comprehend get sentiment analysis back obviously actually work module could helpful anyone working vba wanting add aws service toolkit help
What is an appropriate training set size for text classification (Sentiment analysis),"<p>I just wanted to understand (from your experience), that if I have to create a sentiment analysis classification model (using NLTK), what would be a good training data size. For instance if my training data is going to contain tweets, and I intend to classify them as positive,negative and neutral, how many tweets each should I ideally have per category to get a reasonable model working?</p>

<p>I understand that there are many parameters like quality of data, but if one has to get started what might be  a good number.</p>
",Text Classification / Sentiment Analysis,appropriate training set size text classification sentiment analysis wanted understand experience create sentiment analysis classification model using nltk would good training data size instance training data going contain tweet intend classify positive negative neutral many tweet ideally per category get reasonable model working understand many parameter like quality data one ha get started might good number
sentiment analysis of Non-English tweets in python,"<p>Objective: To classify each tweet as positive or negative and write it to an output file which will contain the username, original tweet and the sentiment of the tweet.</p>

<p>Code: </p>

<pre><code>import re,math
input_file=""raw_data.csv""
fileout=open(""Output.txt"",""w"")
wordFile=open(""words.txt"",""w"")
expression=r""(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)""

fileAFINN = 'AFINN-111.txt'
afinn = dict(map(lambda (w, s): (w, int(s)), [ws.strip().split('\t') for ws in open(fileAFINN)]))

pattern=re.compile(r'\w+')
pattern_split = re.compile(r""\W+"")
words = pattern_split.split(input_file.lower())
print ""File processing started""
with open(input_file,'r') as myfile:
for line in myfile:
    line = line.lower()

    line=re.sub(expression,"" "",line)
    words = pattern_split.split(line.lower())
    sentiments = map(lambda word: afinn.get(word, 0), words)
    #print sentiments
    # How should you weight the individual word sentiments?
    # You could do N, sqrt(N) or 1 for example. Here I use sqrt(N)
    """"""
    Returns a float for sentiment strength based on the input text.
    Positive values are positive valence, negative value are negative valence.
    """"""
    if sentiments:
        sentiment = float(sum(sentiments))/math.sqrt(len(sentiments))
        #wordFile.write(sentiments)
    else:
        sentiment = 0
    wordFile.write(line+','+str(sentiment)+'\n')
fileout.write(line+'\n')
print ""File processing completed""

fileout.close()
myfile.close()
wordFile.close()
</code></pre>

<p>Issue: Apparently the output.txt file is </p>

<pre><code>abc some tweet text 0
bcd some more tweets 1
efg some more tweet 0
</code></pre>

<p>Question 1: How do I add a comma between the userid tweet-text sentiment? The output should be like;</p>

<pre><code> abc,some tweet text,0
 bcd,some other tweet,1
 efg,more tweets,0
</code></pre>

<p>Question 2: The tweets are in Bahasa Melayu (BM) and the AFINN dictionary that I am using is of English words. So the classification is wrong. Do you know any BM dictionary that I can use?</p>

<p>Question 3: How do I pack this code in a JAR file?</p>

<p>Thank you.</p>
",Text Classification / Sentiment Analysis,sentiment analysis non english tweet python objective classify tweet positive negative write output file contain username original tweet sentiment tweet code issue apparently output txt file question add comma userid tweet text sentiment output like question tweet bahasa melayu bm afinn dictionary using english word classification wrong know bm dictionary use question pack code jar file thank
How to count number of citations/references in wikipedia raw text?,"<p>I'm building a model to classify raw Wikipedia text by article quality (Wikipedia has a dataset of ~30,000 hand-graded articles and their corresponding quality grades.). Nonetheless, I am trying to figure out a way to algorithmically count the number of citations that appear on the page. </p>

<p>As a quick example: here is an excerpt from a raw Wiki page:</p>

<p><code>'[[Image:GD-FR-Paris-Louvre-Sculptures034.JPG|320px|thumb|Tomb of Philippe Pot, governor of [[Burgundy (region)|Burgundy]]  under [[Louis XI]]|alt=A large sculpture of six life-sized black-cloaked men, their faces obscured by their hoods, carrying a slab upon which lies the supine effigy of a knight, with hands folded together in prayer. His head rests on a pillow, and his feet on a small reclining lion.]]\n[[File:Sejong tomb 1.jpg|thumb|320px|Korean tomb mound of King [[Sejong the Great]], d. 1450]]\n[[Image:Istanbul - Süleymaniye camii - Türbe di Roxellana - Foto G. Dall\'Orto 28-5-2006.jpg|thumb|320px|[[Türbe]] of [[Roxelana]] (d. 1558), [[Süleymaniye Mosque]], [[Istanbul]]]]\n\'\'\'Funerary art\'\'\' is any work of [[art]] forming, or placed in, a repository for the remains of the [[death|dead]]. [[Tomb]] is a general term for the repository, while [[grave goods]] are objects—other than the primary human remains—which have been placed inside.&lt;ref&gt;Hammond, 58–9 characterizes [[Dismemberment|disarticulated]] human skeletal remains packed in body bags and incorporated into [[Formative stage|Pre-Classic]] [[Mesoamerica]]n [[mass burial]]s (along with a set of primary remains) at Cuello, [[Belize]] as ""human grave goods"".&lt;/ref&gt;</code></p>

<p>So far, I've concluded that I can find the number of images by counting the number of <code>[[Image:</code> occurrences. I was hoping I could do something similar for references. In fact, after comparing raw Wiki pages and their corresponding live pages, <strong><em>I think</em></strong> I was able to determine that <code>&lt;/ref&gt;</code> corresponds to the end notation of a reference on a Wiki page. --> <strong>For example</strong>: Here, you can see that the author makes a statement at the end of the paragraph and references <strong>Hammond, 58–9</strong> within <code>&lt;ref&gt;</code> {text} <code>&lt;/ref&gt;</code></p>

<p>If somebody is familiar with Wiki's raw data and can shed some light on this, please let me know! Also, if you know a better way to do this, please tell me that, too! </p>

<p>Many thanks in advance! </p>

<hr>
",Text Classification / Sentiment Analysis,count number citation reference wikipedia raw text building model classify raw wikipedia text article quality wikipedia ha dataset hand graded article corresponding quality grade nonetheless trying figure way algorithmically count number citation appear page quick example excerpt raw wiki page far concluded find number image counting number occurrence wa hoping could something similar reference fact comparing raw wiki page corresponding live page think wa able determine corresponds end notation reference wiki page example see author make statement end paragraph reference hammond within text somebody familiar wiki raw data shed light please let know also know better way please tell many thanks advance
ValueError: operands could not be broadcast together with shapes in Naive bayes classifier,"<p>Getting straight to the point:<br></p>

<p><strong>1)</strong> My goal was to apply NLP and Machine learning algorithm to classify a dataset containing sentences into 5 different types of categories(numeric). For e.g. ""I want to know details of my order -> 1"".<br></p>

<p><strong>Code:</strong><br></p>

<pre><code>import numpy as np
import pandas as pd

dataset = pd.read_csv('Ecom.tsv', delimiter = '\t', quoting = 3)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpus = []
for i in range(0, len(dataset)):
    review = re.sub('[^a-zA-Z]', ' ', dataset['User'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

# # Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, 1].values

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Fitting Naive Bayes to the Training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
</code></pre>

<p>Everything works fine here, the model is trained well and predicts correct results for test data.</p>

<p><strong>2)</strong> Now i wanted to use this trained model to predict a category for a new sentence. So i pre-processed the text in the same way i did for my dataset.</p>

<p><strong>Code:</strong></p>

<pre><code>#Pre processing the new input
new_text = ""Please tell me the details of this order""
new_text = new_text.split()
ps = PorterStemmer()
processed_text = [ps.stem(word) for word in new_text if not word in set(stopwords.words('english'))]

vect = CountVectorizer()
Z = vect.fit_transform(processed_text).toarray()
classifier.predict(Z)
</code></pre>

<p><strong>ValueError: operands could not be broadcast together with shapes (4,4) (33,)</strong></p>

<p>The only thing i am able to understand is that when i transformed my <strong>corpus</strong> the first time i trained my model, the shape of the numpy array is (18, 33). Second time when i am trying to predict for a new input, when i transformed my <strong>processed_text</strong> using <strong>fit_transform()</strong>, the numpy array shape is (4, 4).</p>

<p>I am not able to figure out is there any process here that i applied incorrectly? What can be the resolution. Thanks in advance! :)</p>
",Text Classification / Sentiment Analysis,valueerror operand could broadcast together shape naive bayes classifier getting straight point goal wa apply nlp machine learning algorithm classify dataset containing sentence different type category numeric e g want know detail order code everything work fine model trained well predicts correct result test data wanted use trained model predict category new sentence pre processed text way dataset code valueerror operand could broadcast together shape thing able understand transformed corpus first time trained model shape numpy array second time trying predict new input transformed processed text using fit transform numpy array shape able figure process applied incorrectly resolution thanks advance
Text classification with Naive Bayes,"<p>I am leaning NLP and noticed that TextBlob classification based in Naive Bayes (textblob is Build on top of NLTK) <a href=""https://textblob.readthedocs.io/en/dev/classifiers.html"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/classifiers.html</a> works fine when training data is list of sentences and does not work at all when training data are individual words (where each word and assigned classification).</p>

<p>Why?</p>
",Text Classification / Sentiment Analysis,text classification naive bayes leaning nlp noticed textblob classification based naive bayes textblob build top nltk work fine training data list sentence doe work training data individual word word assigned classification
FastText using pre-trained word vector for text classification,"<p>I am working on a text classification problem, that is, given some text, I need to assign to it certain given labels.</p>

<p>I have tried using fast-text library by Facebook, which has two utilities of interest to me:</p>

<p>A) Word Vectors with pre-trained models</p>

<p>B) Text Classification utilities</p>

<p>However, it seems that these are completely independent tools as I have been unable to find any tutorials that merge these two utilities.</p>

<p>What I want is to be able to classify some text, by taking advantage of the pre-trained models of the Word-Vectors. Is there any way to do this?</p>
",Text Classification / Sentiment Analysis,fasttext using pre trained word vector text classification working text classification problem given text need assign certain given label tried using fast text library facebook ha two utility interest word vector pre trained model b text classification utility however seems completely independent tool unable find tutorial merge two utility want able classify text taking advantage pre trained model word vector way
Does similarity based algorithms outperform SVM/Tree algorithms in text classification ?,"<p>What performs better for text classification similarity (cosine distance) based algorithms or regular classification methods like SVM or decision trees in terms accuracy and performance? 
Does SVM resolves in finite time with 4GB-8GB RAM systems, while training with large text data ?</p>
",Text Classification / Sentiment Analysis,doe similarity based algorithm outperform svm tree algorithm text classification performs better text classification similarity cosine distance based algorithm regular classification method like svm decision tree term accuracy performance doe svm resolve finite time gb gb ram system training large text data
Formatting Training Data for Spacy Text Classification,"<p>I have been using fasttext for a personal project (for text classification) and have training data formatted as:</p>

<p><em>label</em> [text....]</p>

<p>This is on each line for my existing training data. As a disclaimer, I am relatively new to NLP and training data formats. I was wondering if anyone could offer advice on how to properly format my existing training data to be used in spacy text categorizer. I understand from reading their documentation that the data format accepted is JSON format. Is it possible that the GoldParse tool intended to be used for this task?</p>

<p>edit: found some help on this topic here:  <a href=""https://stackoverflow.com/questions/48834832/how-do-i-create-gold-data-for-textcategorizer-training?noredirect=1&amp;lq=1"">How do I create gold data for TextCategorizer training?</a></p>
",Text Classification / Sentiment Analysis,formatting training data spacy text classification using fasttext personal project text classification training data formatted label text line existing training data disclaimer relatively new nlp training data format wa wondering anyone could offer advice properly format existing training data used spacy text categorizer understand reading documentation data format accepted json format possible goldparse tool intended used task edit found help topic href create gold data textcategorizer training
Removing irrelevant information of online articles,"<p>I am doing text classification to detect political leaning in online news article. The problem is that the articles are very noisy with media attributes  such as media tag line, articles copyright, information of publication, author/reporter names, related-article links, etc, and there is no separator between the main information and the noise (html tags were already removed).
I've read several papers about how to clean irrelevant information from crawled online articles; however, all of them was doing the cleaning process in collection stage by using HTML tag. My research is purely NLP so it's out of my project's scope.</p>

<p>I've studied about removing stopwords based on IDF and Information gain, also by using outlier detection techniques (distance-based, clustering based) . But I don't think they can work in my case.
Any suggestions how to automatically remove those irrelevant contents of news articles?
Thank you for any comments and answers.</p>
",Text Classification / Sentiment Analysis,removing irrelevant information online article text classification detect political leaning online news article problem article noisy medium attribute medium tag line article copyright information publication author reporter name related article link etc separator main information noise html tag already removed read several paper clean irrelevant information crawled online article however wa cleaning process collection stage using html tag research purely nlp project scope studied removing stopwords based idf information gain also using outlier detection technique distance based clustering based think work case suggestion automatically remove irrelevant content news article thank comment answer
Why did NLTK NaiveBayes classifier misclassify one record?,"<p>This is the first time I am building a sentiment analysis machine learning model using the nltk NaiveBayesClassifier in Python. I know it is too simple of a model, but it is just a first step for me and I will try tokenized sentences next time. </p>

<p>The real issue I have with my current model is: I have clearly labeled the word 'bad' as negative in the training data set (as you can see from the 'negative_vocab' variable). However, when I ran the NaiveBayesClassifier on each sentence (lower case) in the list ['awesome movie', ' i like it', ' it is so bad'], the classifier mistakenly labeled 'it is so bad' as positive.   </p>

<p><strong>INPUT:</strong></p>

<pre><code>import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import names

positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]
negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]
neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not','it','so','really' ]

def word_feats(words):
    return dict([(word, True) for word in words])

positive_features_1 = [(word_feats(positive_vocab), 'pos')]
negative_features_1 = [(word_feats(negative_vocab), 'neg')]
neutral_features_1 = [(word_feats(neutral_vocab), 'neu')]

train_set = negative_features_1 + positive_features_1 + neutral_features_1

classifier = NaiveBayesClassifier.train(train_set) 

# Predict
neg = 0
pos = 0
sentence = ""Awesome movie. I like it. It is so bad""
sentence = sentence.lower()
words = sentence.split('.')

def word_feat(word):
    return dict([(word,True)])
#NOTE THAT THE FUNCTION 'word_feat(word)' I WROTE HERE IS DIFFERENT FROM THE 'word_feat(words)' FUNCTION I DEFINED EARLIER. THIS FUNCTION IS USED TO ITERATE OVER EACH OF THE THREE ELEMENTS IN THE LIST ['awesome movie', ' i like it', ' it is so bad'].

for word in words:
    classResult = classifier.classify(word_feat(word))
    if classResult == 'neg':
        neg = neg + 1
    if classResult == 'pos':
        pos = pos + 1
    print(str(word) + ' is ' + str(classResult))
    print() 
</code></pre>

<p><strong>OUTPUT:</strong></p>

<pre><code>awesome movie is pos

i like it is pos

it is so bad is pos
</code></pre>

<p>To make sure the function 'word_feat(word)' iterates over each sentences instead of each word or letter, I did some diagnostic codes to see what is each element in 'word_feat(word)':</p>

<pre><code>for word in words:
    print(word_feat(word))
</code></pre>

<p>And it printed out:</p>

<pre><code>{'awesome movie': True}
{' i like it': True}
{' it is so bad': True} 
</code></pre>

<p>So it seems like the function 'word_feat(word)' is correct?</p>

<p>Does anyone know why the classifier classified 'It is so bad' as positive? As mentioned before, I had clearly labeled the word 'bad' as negative in my training data. </p>
",Text Classification / Sentiment Analysis,nltk naivebayes classifier misclassify one record first time building sentiment analysis machine learning model using nltk naivebayesclassifier python know simple model first step try tokenized sentence next time real issue current model clearly labeled word bad negative training data set see negative vocab variable however ran naivebayesclassifier sentence lower case list awesome movie like bad classifier mistakenly labeled bad positive input output make sure function word feat word iterates sentence instead word letter diagnostic code see element word feat word printed seems like function word feat word correct doe anyone know classifier classified bad positive mentioned clearly labeled word bad negative training data
How do i build a classifier out of two already trained classifiers?,"<p>I want to classify text as either positive, negative or neutral. Therefore i build two different SVMs. The first one classifies between negative and positive/neutral and the other one between positive an negative/neutral. If both classifiers disagree the input is neutral. Now i want two combine these two classifiers into a single one which gives an output of wether the text is positive negative or neutral. I heard of the Voting classifier but this doesn't help because it has to be trained afterwards. Is there any way to make a single classifier out of these two?</p>
",Text Classification / Sentiment Analysis,build classifier two already trained classifier want classify text either positive negative neutral therefore build two different svms first one classifies negative positive neutral one positive negative neutral classifier disagree input neutral want two combine two classifier single one give output wether text positive negative neutral heard voting classifier help ha trained afterwards way make single classifier two
Text classification on feature vector X with multiple vs. merged columns,"<p>I am working on text classification problem where I have around 95 data points and the data looks like this (only two dummy entries shown):</p>

<pre><code>                           |   ID    | Location |  Emails     |
                           ------------------------------------
                           |  AZ12   |    UK    | Lorem Ipsum |
                           |  MR34   |    USA   | Lorem Ipsum |
</code></pre>

<p>In my current approach, I have merged the data in .csv, space delimited (shown below) and I am using only one column to do text-classification.</p>

<pre><code>                                |     Merged_columns    |
                                -------------------------
                                |  AZ12 UK Lorem Ipsum  |
                                |  MR34 USA Lorem Ipsum |
</code></pre>

<p>This approach seems to work for me and I am getting around 70% of accuracy with test data.</p>

<p>Now I am thinking of performing text classification on multiple columns of my feature vector (X) instead of merging all columns of X vector into one column (like by performing feature engineering on individual columns of X vector and then concatenating the transformed vectors. The approach has been mention in this article as well : <a href=""https://towardsdatascience.com/natural-language-processing-on-multiple-columns-in-python-554043e05308"" rel=""nofollow noreferrer"">https://towardsdatascience.com/natural-language-processing-on-multiple-columns-in-python-554043e05308</a>). </p>

<p>Now my question is : When it comes to NLP, are both approaches equivalent theoretically. Should the later approach yield any better/worst results than my former approach.
Thanks. </p>
",Text Classification / Sentiment Analysis,text classification feature vector x multiple v merged column working text classification problem around data point data look like two dummy entry shown current approach merged data csv space delimited shown using one column text classification approach seems work getting around accuracy test data thinking performing text classification multiple column feature vector x instead merging column x vector one column like performing feature engineering individual column x vector concatenating transformed vector approach ha mention article well question come nlp approach equivalent theoretically later approach yield better worst result former approach thanks
Categorizing data using sentiment analysis,"<p>I have a text file which has a data as below. There are many such lines but not of the same pattern</p>

<pre><code>Celina is acting as assistant to John, M.D.
</code></pre>

<p>I want to categorize Celina as assistant and John as M.D.</p>

<p>I need guidance on what would be the approach to handle such problems?
Is it Sentiment Analysis?</p>
",Text Classification / Sentiment Analysis,categorizing data using sentiment analysis text file ha data many line pattern want categorize celina assistant john need guidance would approach handle problem sentiment analysis
Categorizing data using sentiment analysis,"<p>I have a text file which has a data as below. There are many such lines but not of the same pattern</p>

<pre><code>Celina is acting as assistant to John, M.D.
</code></pre>

<p>I want to categorize Celina as assistant and John as M.D.</p>

<p>I need guidance on what would be the approach to handle such problems?
Is it Sentiment Analysis?</p>
",Text Classification / Sentiment Analysis,categorizing data using sentiment analysis text file ha data many line pattern want categorize celina assistant john need guidance would approach handle problem sentiment analysis
PoS Implementation with Naive Bayes Sentiment Analysis,"<p>I am trying to apply Sentiment Analysis (predicting negative and positive tweets) on a relatively large Dataset (10000 rows). So far, I achieved only ~73% accuracy using Naive Bayes and my method called ""final"" shown below to extract features. I want to add PoS to help with the classification, but am completely unsure how to implement it. I tried writing a simple function called ""pos"" (which I posted below) and attempted using the tags on my cleaned dataset as features, but only got around 52% accuracy this way.. Can anyone lead me in the right direction to implement PoS for my model? Thank you.</p>

<pre><code>def pos(word):
 return [t for w, t in nltk.pos_tag(word)]


def final(text):

   """"""
   I have code here to remove URLs,hashtags, 
   stopwords,usernames,numerals, and punctuation.
   """"""

   #lemmatization
   finished = []
   for x in clean:
      finished.append(lem.lemmatize(x))

   return finished
</code></pre>
",Text Classification / Sentiment Analysis,po implementation naive bayes sentiment analysis trying apply sentiment analysis predicting negative positive tweet relatively large dataset row far achieved accuracy using naive bayes method called final shown extract feature want add po help classification completely unsure implement tried writing simple function called po posted attempted using tag cleaned dataset feature got around accuracy way anyone lead right direction implement po model thank
Combining Text and Numerical Columns for ML Algorithm,"<p>Here I'm working with a Sentiment Classification problem, where I have to predict whether the tweets are positive, negative or neutral. Here's a glimpse of my dataset:</p>

<pre><code>tweet_id   airline_sentiment_confidence   negativereason   negativereason_confidence   airline   name   retweet_count   text   tweet_created   tweet location   user_timezone   airline_sentiment
Tr_tweet_1   1.000                          NaN             NaN                        Virgin America    0             tweets   date               Location        Time             Positive
Tr_tweet_2   0.3846                         NaN             0.7033                     Virgin America    0             tweets   date               Location        Time             Negative
Tr_tweet_3   0.6837                         Bad flight      0.3342                     Virgin America    0             tweets   date               Location        Time             Negative
Tr_tweet_4   1.000                          Can't tell      1.000                      Virgin America    0             tweets   date               Location        Time             Neutral   
Tr_tweet_5   1.000                          NaN             NaN                        Virgin America    0             tweets   date               Location        Time             Neutral   
</code></pre>

<p>However <code>text</code> is the column which I'm fitting in my <code>TfIdf_Vectorizer</code> and using <code>logreg</code> to predict the sentiment. However I'm getting a very low accuracy of <code>~68%</code>, which turns out to be a pure NLP problem. However the other features will surely increase my accuracy if I can somehow use them.</p>

<p>I'm interested in knowing how can I combine the other numerical as well as textual columns like <code>negativereason</code> as features with my <code>text</code> column, to increase my accuracy. </p>

<p>Or is there any method of stacking that can be done here? Like combining the predictions of <code>Tfidf</code> and then once again doing prediction with rest numerical columns?</p>

<p><strong>TL;DR</strong> How to deal with numerical as well as textual columns as features to make a good prediction?</p>
",Text Classification / Sentiment Analysis,combining text numerical column ml algorithm working sentiment classification problem predict whether tweet positive negative neutral glimpse dataset however column fitting using predict sentiment however getting low accuracy turn pure nlp problem however feature surely increase accuracy somehow use interested knowing combine numerical well textual column like feature column increase accuracy method stacking done like combining prediction prediction rest numerical column tl dr deal numerical well textual column feature make good prediction
Using Keras for text classification,"<p>I am struggling to approach the bag of words / vocabulary method for representing my input data as one hot vectors for my neural net model in keras. </p>

<p>I would like to build a simple 3 layer network but I need help in understanding and developing an approach to transform my labelled data in the form of text,sentinment which is has 7 labels, in the range of 0 - 1 in steps of 0.2. </p>

<p>I have tried to use scikit's vectorisers but they are too rigid i.e they either tokenise words or characters, whereas I need a sentence to be compared to the vocabulary which includes words, characters, punctuation and emojis. When i use tfid on a test sentence it only counts the words and ignores everything else. I also need guidance on taking this one hot approach and how it will be implemented in keras.</p>
",Text Classification / Sentiment Analysis,using kera text classification struggling approach bag word vocabulary method representing input data one hot vector neural net model kera would like build simple layer network need help understanding developing approach transform labelled data form text sentinment ha label range step tried use scikit vectorisers rigid e either tokenise word character whereas need sentence compared vocabulary includes word character punctuation emojis use tfid test sentence count word ignores everything else also need guidance taking one hot approach implemented kera
How to reread the news on website using newspaper3k,"<p>I'm trying to create a dataset to do sentiment analysis on news articles. I'm using Newspaper3k to scrape articles from the website. I scraped a few websites but didn't store the articles properly and hence I can't use them. When I try scraping the same websites again it only scrapes the new articles and not the ones it already scraped. Is there a way for me to scrape the articles I already scraped again??</p>
",Text Classification / Sentiment Analysis,reread news website using newspaper k trying create dataset sentiment analysis news article using newspaper k scrape article website scraped website store article properly hence use try scraping website scrape new article one already scraped way scrape article already scraped
Naive Bayes for classifying columns,"<p>Hello StackExchange Community,</p>

<p>I have a unique use case for Naive Bayes where I'm trying to train my model to identify output column names based on previous data of input column names.</p>

<p>Basically, my data looks like this:</p>

<pre><code> 1. Output Column  |   Input Columns  
 2. Col1        |       ABC DEF GHI 
 3. Col2        |       JKL MNO QPR
 4. Col3        |       STU VWX
</code></pre>

<p>This is historical data which indicates that</p>

<p>we received a data in a column named 'ABC' and we mapped the data into 'Col1'
we received a data in a column named 'DEF' and we mapped the data into 'Col1'
we received a data in a column named 'JKL' and we mapped the data into 'Col2'
and so on...</p>

<p>I rearranged my data as below and trained my model using Naive Bayes' text classification (similar to an email spam classifier).</p>

<pre><code>1. Output Column   |   Input Columns
 2. Col1           |         ABC
 3. Col2           |         JKL
 4. Col3           |         STU
 5. Col1           |         DEF
 6. Col2           |         MNO
 7. Col3           |         STU
</code></pre>

<p>and so on...</p>

<p>For predictions, I will be providing input columns to the trained model and asking which output column class the input should belong to, along with a probability score.</p>

<p>Do you think my approach to solving this problem is correct? With about 100 rows of data, I've been getting accurate predictions but low probability score (~40%).</p>

<p>Any feedback would be appreciated here, as this ML model can help us a lot in automating tasks.</p>

<p>Code:</p>

<pre><code>vectorizer = CountVectorizer()
counts = vectorizer.fit_transform(training_data['Input Column'].values)
classifier = MultinomialNB()
targets = training_data['Output Column'].values

classifier.fit(counts, targets)

for index, row in test_data.iterrows():
    panel=row['SourcePanel']
    panel=[panel]
    example_counts = vectorizer.transform(panel)
    predictions = classifier.predict(example_counts)
    probability = classifier.predict_proba(example_counts)
    df=pd.DataFrame(classifier.predict_proba(example_counts), columns=classifier.classes_)
    print(""The source panel attribute "", panel, "" maps to Target entity Attribute"", df[predictions], ""probability"")
</code></pre>
",Text Classification / Sentiment Analysis,naive bayes classifying column hello stackexchange community unique use case naive bayes trying train model identify output column name based previous data input column name basically data look like historical data indicates received data column named abc mapped data col received data column named def mapped data col received data column named jkl mapped data col rearranged data trained model using naive bayes text classification similar email spam classifier prediction providing input column trained model asking output column class input belong along probability score think approach solving problem correct row data getting accurate prediction low probability score feedback would appreciated ml model help u lot automating task code
Data sets for emotion detection in text,"<p>I'm implementing a system that could detect the human emotion in text. Are there any manually annotated data sets available for supervised learning and testing? </p>

<p>Here are some interesting datasets:
<a href=""https://dataturks.com/projects/trending"" rel=""noreferrer"">https://dataturks.com/projects/trending</a></p>
",Text Classification / Sentiment Analysis,data set emotion detection text implementing system could detect human emotion text manually annotated data set available supervised learning testing interesting datasets
NLP and Machine learning for sentiment analysis,"<p>I'm trying to write a program that takes text(article) as input and outputs the polarity of this text, weather its a positive or a negative sentiment. I've read extensively about different approaches but i am still confused. I read about many techniques like classifiers and machine learning. I would like direction and clear instructions on where to start. For example, i have a classifier which requires a dataset but how do i convert the text(article) into a dataset for the classifier. If anyone can tell me the logical sequence to approach this problem that would be greet. Thanks in advance!
PS: please mention any related algorithms or open-source implementation</p>

<p>Regards,
Mike</p>
",Text Classification / Sentiment Analysis,nlp machine learning sentiment analysis trying write program take text article input output polarity text weather positive negative sentiment read extensively different approach still confused read many technique like classifier machine learning would like direction clear instruction start example classifier requires dataset convert text article dataset classifier anyone tell logical sequence approach problem would greet thanks advance p please mention related algorithm open source implementation regard mike
Classify text using NaiveBayesClassifier,"<p>I have a text file with a sentence on each line:
eg """"Have you registered your email ID with your Bank Account?""</p>

<p>I want to classify it into interrogative or not. FYI these are sentences from bank websites.
I've seen <a href=""https://datascience.stackexchange.com/questions/26427/how-to-extract-question-s-from-document-with-nltk"">this answer</a>
with this nltk code block:</p>

<pre><code>import nltk
nltk.download('nps_chat')
posts = nltk.corpus.nps_chat.xml_posts()[:10000]


def dialogue_act_features(post):
    features = {}
    for word in nltk.word_tokenize(post):
        features['contains({})'.format(word.lower())] = True
    return features

featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]
size = int(len(featuresets) * 0.1)
train_set, test_set = featuresets[size:], featuresets[:size]
classifier = nltk.NaiveBayesClassifier.train(train_set)
print(nltk.classify.accuracy(classifier, test_set))
</code></pre>

<p>So I did some preprocessing to my text file i.e. stemming words, removing stop words etc, to make each sentence into a bag of words. From the code above, I have a trained classifier. How do I implement it on my text file of sentences (either raw or preprocessed)?</p>

<p>Update: <a href=""https://drive.google.com/file/d/1q-bcyoDHckGgK7aRy-tiLUtfPPfBDpZB/view?usp=sharing"" rel=""nofollow noreferrer"">here</a> is an example of my text file.  </p>
",Text Classification / Sentiment Analysis,classify text using naivebayesclassifier text file sentence line eg registered email id bank account want classify interrogative fyi sentence bank website seen example text file
Using pretrained word embeddings to classify &quot;pools&quot; of words,"<p>I have seen many papers explaining the use of pretrained word embeddings (such as Word2Vec or Fasttext) on sentence sentiment classification using CNNs (like Yoon Kim's paper). However, these classifiers also account for order that the words appear in.</p>

<p>My application of word embeddings is to predict the class of ""pools"" of words. For example, in the following list of lists</p>

<pre><code>example = [[""red"", ""blue"", ""green"", ""orange""], [""bear"", ""horse"", ""cow""], [""brown"", ""pink""]]
</code></pre>

<p>The order of the words doesn't matter, but I want to classify the sublists into either class of color or animal.</p>

<p>Are there any prebuilt Keras implementations of this, or any papers you could point me to which address this type of classification problem based on pretrained word embeddings?</p>

<p>I am sorry if this is off-topic in this forum. If so, please let me know where would be a better place to post it.</p>
",Text Classification / Sentiment Analysis,using pretrained word embeddings classify pool word seen many paper explaining use pretrained word embeddings word vec fasttext sentence sentiment classification using cnns like yoon kim paper however classifier also account order word appear application word embeddings predict class pool word example following list list order word matter want classify sublists either class color animal prebuilt kera implementation paper could point address type classification problem based pretrained word embeddings sorry topic forum please let know would better place post
Sentiment analysis in R not recognizing modifying words,"<p>I am having a problem with SentimentAnalysis in R. I would like to find a way to do sentiment analysis in r that is not just based on the positivity or negativity of a single word. For example </p>

<pre><code>library(SentimentAnalysis) 
sentiment_pos &lt;- analyzeSentiment(""This presentation is excellent and Informative "")
convertToDirection(sentiment_pos$SentimentQDAP)
sentiment_neg &lt;- analyzeSentiment(""This presentation is not excellent"")
convertToDirection(sentiment_neg$SentimentQDAP)
</code></pre>

<p>this package scores both sentences as positive (i think because it is dictionary based and does not take into account that ""not"" is a negating modifier). I am trying to imitate the functionality of the python package textblob but using R. Does anyone have any idea for packages that have this functionality? </p>
",Text Classification / Sentiment Analysis,sentiment analysis r recognizing modifying word problem sentimentanalysis r would like find way sentiment analysis r based positivity negativity single word example package score sentence positive think dictionary based doe take account negating modifier trying imitate functionality python package textblob using r doe anyone idea package functionality
Use string as input in Keras IMDB example,"<p>I was looking at the <a href=""https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"" rel=""nofollow noreferrer"">Keras IMDB Movie reviews sentiment classification example</a> (<a href=""https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"" rel=""nofollow noreferrer"">and the corresponding model on github</a>), which learns to decide whether a review is positive or negative.</p>

<p>The data has been preprocessed such that each review is encoded as a sequence of integers, e.g. the review ""This movie is awesome!"" would be <code>[11, 17, 6, 1187]</code> and for this input the model gives the output 'positive'.</p>

<p>The dataset also makes available the word index used for encoding the sequences, i.e. I know the map</p>

<pre><code>This: 11
movie: 17
is: 6
awesome: 1187
...
</code></pre>

<p>Can I somehow include this knowledge into the model such that its input is a string, i.e. it gives a prediction based on the input ""This movie is awesome!""?</p>
",Text Classification / Sentiment Analysis,use string input kera imdb example wa looking kera imdb movie review sentiment classification example corresponding model github learns decide whether review positive negative data ha preprocessed review encoded sequence integer e g review movie awesome would input model give output positive dataset also make available word index used encoding sequence e know map somehow include knowledge model input string e give prediction based input movie awesome
I am trying to get the key of a particular word from a Word2Vec Vocabulary,"<h2>Word2Vec</h2>

<p>Currently I am trying to perform text classification on a text corpus. In order to do so, I have decided to perform <code>word2vec</code> with the help of <code>gensim</code>. In order to do so, I have the code below: </p>

<pre><code>sentences = MySentences(""./corpus_samples"") # a memory-friendly iterator
model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>My sentences is basically a class that handles the File <em>I/O</em></p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>Now we can get the vocabulary of the model that has been created through these lines: </p>

<pre><code>print(model.wv.vocab)
</code></pre>

<p>The output of which is below(sample): </p>

<pre><code>t at 0x106f19438&gt;, 'raining.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19470&gt;, 'fly': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194a8&gt;, 'rain.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194e0&gt;, 'So…': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19518&gt;, 'Ohhh,': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19550&gt;, 'weird.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;}
</code></pre>

<p>As of now, the dictionary that is the vocabulary, contains the word string and a <code>&lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;</code> object or such. I want to be able to query an index of a particular word. In order to make my training data like: </p>

<pre><code>w91874 w2300 w6 w25363 w6332 w11 w767 w297441 w12480 w256 w23270 w13482 w22236 w259 w11 w26959 w25 w1613 w25363 w111 __label__4531492575592394249
w17314 w5521 w7729 w767 w10147 w111 __label__1315009618498473661
w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492
w30877 w72 w11 w2828 w141417 w77033 w10147 w111 __label__4970306416006110305
w3332 w1107 w4809 w1009 w327 w84792 w6 w922 w11 w2182 w79887 w1099 w111 __label__-3645735357732416904
w471 w14752 w1637 w12348 w72 w31330 w930 w11569 w863 w25 w1439 w72 w111 __label__-5932391056759866388
w8081 w5324 w91048 w875 w13449 w1733 w111 __label__3812457715228923422
</code></pre>

<p>Where the <code>wxxxx</code> represents the index of the word within the vocabulary and the label represents the class. </p>

<hr>

<h2>Corpora</h2>

<p>Some of the solutions that I have been experimenting with, is the <code>corpora</code> utility of <code>gensim</code>:  </p>

<pre><code>corpora = gensim.corpora.dictionary.Dictionary(sentences, prune_at=2000000)
print(corpora)
print(getKey(corpora,'am'))
</code></pre>

<p>This gives me a nice dictionary of the words, but this corpora vocabulary is not the same as the one created by the <code>word2vec</code> function mentioned above. </p>
",Text Classification / Sentiment Analysis,trying get key particular word word vec vocabulary word vec currently trying perform text classification text corpus order decided perform help order code sentence basically class handle file get vocabulary model ha created line output sample dictionary vocabulary contains word string object want able query index particular word order make training data like represents index word within vocabulary label represents class corpus solution experimenting utility give nice dictionary word corpus vocabulary one created function mentioned
I am trying to get the key of a particular word from a Word2Vec Vocabulary,"<h2>Word2Vec</h2>

<p>Currently I am trying to perform text classification on a text corpus. In order to do so, I have decided to perform <code>word2vec</code> with the help of <code>gensim</code>. In order to do so, I have the code below: </p>

<pre><code>sentences = MySentences(""./corpus_samples"") # a memory-friendly iterator
model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>My sentences is basically a class that handles the File <em>I/O</em></p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>Now we can get the vocabulary of the model that has been created through these lines: </p>

<pre><code>print(model.wv.vocab)
</code></pre>

<p>The output of which is below(sample): </p>

<pre><code>t at 0x106f19438&gt;, 'raining.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19470&gt;, 'fly': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194a8&gt;, 'rain.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f194e0&gt;, 'So…': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19518&gt;, 'Ohhh,': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19550&gt;, 'weird.': &lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;}
</code></pre>

<p>As of now, the dictionary that is the vocabulary, contains the word string and a <code>&lt;gensim.models.keyedvectors.Vocab object at 0x106f19588&gt;</code> object or such. I want to be able to query an index of a particular word. In order to make my training data like: </p>

<pre><code>w91874 w2300 w6 w25363 w6332 w11 w767 w297441 w12480 w256 w23270 w13482 w22236 w259 w11 w26959 w25 w1613 w25363 w111 __label__4531492575592394249
w17314 w5521 w7729 w767 w10147 w111 __label__1315009618498473661
w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492
w30877 w72 w11 w2828 w141417 w77033 w10147 w111 __label__4970306416006110305
w3332 w1107 w4809 w1009 w327 w84792 w6 w922 w11 w2182 w79887 w1099 w111 __label__-3645735357732416904
w471 w14752 w1637 w12348 w72 w31330 w930 w11569 w863 w25 w1439 w72 w111 __label__-5932391056759866388
w8081 w5324 w91048 w875 w13449 w1733 w111 __label__3812457715228923422
</code></pre>

<p>Where the <code>wxxxx</code> represents the index of the word within the vocabulary and the label represents the class. </p>

<hr>

<h2>Corpora</h2>

<p>Some of the solutions that I have been experimenting with, is the <code>corpora</code> utility of <code>gensim</code>:  </p>

<pre><code>corpora = gensim.corpora.dictionary.Dictionary(sentences, prune_at=2000000)
print(corpora)
print(getKey(corpora,'am'))
</code></pre>

<p>This gives me a nice dictionary of the words, but this corpora vocabulary is not the same as the one created by the <code>word2vec</code> function mentioned above. </p>
",Text Classification / Sentiment Analysis,trying get key particular word word vec vocabulary word vec currently trying perform text classification text corpus order decided perform help order code sentence basically class handle file get vocabulary model ha created line output sample dictionary vocabulary contains word string object want able query index particular word order make training data like represents index word within vocabulary label represents class corpus solution experimenting utility give nice dictionary word corpus vocabulary one created function mentioned
What Text Classification algorithms I can use to classify customer chat messages?,"<p>I am working on classifying customer chat messages into 5 categories. The example of categories are - Login, SSL etc. For Instance, if customer is having login issues, the message may read something like this - I am having a login issue or my login is not working... We have to take into account misspelling, mentioning multiple classified keywords (eg: I just upgraded my SSL but now I am having issue with login) etc.</p>

<p>Are there models/apis out there that I can use to solve this problem?</p>
",Text Classification / Sentiment Analysis,text classification algorithm use classify customer chat message working classifying customer chat message category example category login ssl etc instance customer login issue message may read something like login issue login working take account misspelling mentioning multiple classified keywords eg upgraded ssl issue login etc model apis use solve problem
Sentiment Analysis of Entity (Entity-level Sentiment Analysis),"<p>I've been working on document level sentiment analysis since past 1 year. <em>Document level sentiment analysis</em> provides the sentiment of the complete document. For example - The text ""<em>Nokia is good but vodafone sucks big time</em>"" would have a negative polarity associated with it as it would be agnostic to the entities Nokia and Vodafone. <em>How would it be possible to get entity level sentiment, like positive for Nokia but negative for Vodafone</em> ? Are there any research papers providing a solution to such problems ?</p>
",Text Classification / Sentiment Analysis,sentiment analysis entity entity level sentiment analysis working document level sentiment analysis since past year document level sentiment analysis provides sentiment complete document example text nokia good vodafone suck big time would negative polarity associated would agnostic entity nokia vodafone would possible get entity level sentiment like positive nokia negative vodafone research paper providing solution problem
Sentiment Analysis with Imbalanced Dataset in LightGBM,"<p>I am trying to perform sentiment analysis on a dataset of 2 classes (Binary Classification). Dataset is heavily imbalanced about <code>70% - 30%</code>. I am using <code>LightGBM</code> and <code>Python 3.6</code> for making the model and predicting the output.</p>

<p>I think imbalance in dataset effect performance of my model. I get about <code>90%</code> accuracy but it doesn't increase further even though I have performed fine-tuning of the parameters. I don't think this the maximum possible accuracy as there are others who scored better than this.</p>

<p>I have cleaned the dataset with <code>Textacy</code> and <code>nltk</code>. I am using <code>CountVectorizer</code> for encoding the text. </p>

<p>I have tried <code>up-sampling</code> the dataset but it resulted in poor model (I haven't tuned that model) </p>

<p>I have tried using the <code>is_unbalance</code> parameter of <code>LightGBM</code>, but it doesn't give me a better model.</p>

<p>Are there any approaches to follow to handle this type of datasets that are so imbalanced.? How can I further improve my model.? Should I try down-sampling.? Or is it the maximum possible accuracy.? How can I be sure of it.?</p>
",Text Classification / Sentiment Analysis,sentiment analysis imbalanced dataset lightgbm trying perform sentiment analysis dataset class binary classification dataset heavily imbalanced using making model predicting output think imbalance dataset effect performance model get accuracy increase even though performed fine tuning parameter think maximum possible accuracy others scored better cleaned dataset using encoding text tried dataset resulted poor model tuned model tried using parameter give better model approach follow handle type datasets imbalanced improve model try sampling maximum possible accuracy sure
Java text classification problem,"<p>I have a set of Books objects, classs <strong>Book</strong> is defined as following :</p>

<pre><code>Class Book{

String title;
ArrayList&lt;tags&gt; taglist;

}
</code></pre>

<p>Where <strong>title</strong> is the title of the book, example : <em>Javascript for dummies</em>.</p>

<p>and <strong>taglist</strong> is a list of tags for our example : <em>Javascript, jquery, ""web dev"", ..</em></p>

<p>As I said a have a set of books talking about different things : IT, BIOLOGY, HISTORY, ...
Each book has a title and a set of tags describing it..</p>

<p>I have to classify automaticaly those books into separated sets by topic, example :</p>

<p>IT BOOKS :</p>

<ul>
<li>Java for dummies</li>
<li>Javascript for dummies</li>
<li>Learn flash in 30 days</li>
<li>C++ programming</li>
</ul>

<p>HISTORY BOOKS :</p>

<ul>
<li>World wars</li>
<li>America in 1960</li>
<li>Martin luther king's life</li>
</ul>

<p>BIOLOGY BOOKS :</p>

<ul>
<li>....</li>
</ul>

<p>Do you guys know a classification algorithm/method to apply for that kind of problems ?</p>

<p>A solution is to use an external API to define the category of the text, but the problem here is that books are in different languages : french, spanish, english ..</p>
",Text Classification / Sentiment Analysis,java text classification problem set book object class book defined following title title book example javascript dummy taglist list tag example javascript jquery web dev said set book talking different thing biology history book ha title set tag describing classify automaticaly book separated set topic example book java dummy javascript dummy learn flash day c programming history book world war america martin luther king life biology book guy know classification algorithm method apply kind problem solution use external api define category text problem book different language french spanish english
Why Mallet text classification output the same value 1.0 for all test files?,"<p>I am learning Mallet text classification command lines. The output values for estimating differrent classes are all the same 1.0. I do not know where I am incorrect. Can you help?</p>

<p>mallet version: E:\Mallet\mallet-2.0.8RC3</p>

<pre><code>//there is a txt file about cat breed (catmaterial.txt) in cat dir.
//command 1
C:\Users\toshiba&gt;mallet import-dir --input E:\Mallet\testmaterial\cat --output E
:\Mallet\testmaterial\cat.mallet --remove-stopwords

//command 1 output
Labels =
   E:\Mallet\testmaterial\cat

//command 2, save classifier as catClass.classifier
C:\Users\toshiba&gt;mallet train-classifier --input E:\Mallet\testmaterial\cat.mall
et --trainer NaiveBayes --output-classifier E:\Mallet\testmaterial\catClass.clas
sifier

//command 2 output
Training portion = 1.0
Unlabeled training sub-portion = 0.0
Validation portion = 0.0
Testing portion = 0.0

-------------------- Trial 0  --------------------

Trial 0 Training NaiveBayesTrainer with 1 instances
Trial 0 Training NaiveBayesTrainer finished
No examples with predicted label !
No examples with true label !
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer training data accuracy = 1.0
Trial 0 Trainer NaiveBayesTrainer Test Data Confusion Matrix
No examples with predicted label !
Trial 0 Trainer NaiveBayesTrainer test data precision() = 1.0
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data recall() = 1.0
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data F1() = 1.0
Trial 0 Trainer NaiveBayesTrainer test data accuracy = NaN

NaiveBayesTrainer
Summary. train accuracy mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test accuracy mean = NaN stddev = NaN stderr = NaN
Summary. test precision() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test recall() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test f1() mean = 1.0 stddev = 0.0 stderr = 0.0

//command 3, estimate classes of the three files about cat, deer and dog. The cat file is the same as the one for cat.mallet
C:\Users\toshiba&gt;mallet classify-dir --input E:\Mallet\testmaterial\test_cat_dir
 --output - --classifier E:\Mallet\testmaterial\catClass.classifier


//command 3 output
file:/E:/Mallet/testmaterial/test_cat_dir/catmaterial.txt               1.0
file:/E:/Mallet/testmaterial/test_cat_dir/deertext.txt          1.0
file:/E:/Mallet/testmaterial/test_cat_dir/dogmaterial.txt               1.0

// why the three classes are all 1.0 ?

C:\Users\toshiba&gt;
</code></pre>

<p>Can you help? 
Thanks.</p>

<p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p>

<p><strong>Update:</strong> </p>

<p>Thank you for answer, but still output 1.0 for all files.</p>

<p>My idea was that I put some dog files in dog dir and treated these dog files as instances, trained model, then tested some files in test_dir to see the result.</p>

<p>I tried according to my understanding of your suggestion but still output all same 1.0.</p>

<p>Will you help me with my commandlines below?</p>

<p>In E:\Mallet\train_dir\dog,  there are  4 dog txt files(dog 2.txt, dog4.txt,dog5.txt, dogmaterial.txt).</p>

<p>In E:\Mallet\test_dir, there are 9 txt files (cat2.txt, catmaterial.txt, deermaterial.txt, dog3.txt, dog6.txt, dog 2.txt, dog4.txt, dog5.txt, dogmaterial.txt).</p>

<hr>

<pre><code>C:\Users\toshiba&gt;mallet import-dir --input E:\Mallet\train_dir\dog --output E:\M
allet\classifier_dir\3animal.mallet --remove-stopwords
Labels =
   E:\Mallet\train_dir\dog


C:\Users\toshiba&gt;mallet train-classifier --input E:\Mallet\classifier_dir\3anima
l.mallet --trainer NaiveBayes --output-classifier E:\Mallet\classifier_dir\3anim
alClass.classifier
Training portion = 1.0
Unlabeled training sub-portion = 0.0
Validation portion = 0.0
Testing portion = 0.0                          
-------------------- Trial 0  --------------------

Trial 0 Training NaiveBayesTrainer with 4 instances
Trial 0 Training NaiveBayesTrainer finished
No examples with predicted label !
No examples with true label !
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer training data accuracy = 1.0
Trial 0 Trainer NaiveBayesTrainer Test Data Confusion Matrix
No examples with predicted label !
Trial 0 Trainer NaiveBayesTrainer test data precision() = 1.0
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data recall() = 1.0
No examples with predicted label !
No examples with true label !
Trial 0 Trainer NaiveBayesTrainer test data F1() = 1.0
Trial 0 Trainer NaiveBayesTrainer test data accuracy = NaN

NaiveBayesTrainer
Summary. train accuracy mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test accuracy mean = NaN stddev = NaN stderr = NaN
Summary. test precision() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test recall() mean = 1.0 stddev = 0.0 stderr = 0.0
Summary. test f1() mean = 1.0 stddev = 0.0 stderr = 0.0


C:\Users\toshiba&gt;mallet classify-dir --input E:\Mallet\test_dir --output - --cla
ssifier E:\Mallet\classifier_dir\3animalClass.classifier

file:/E:/Mallet/test_dir/cat2.txt               1.0
file:/E:/Mallet/test_dir/catmaterial.txt                1.0
file:/E:/Mallet/test_dir/deertext.txt           1.0
file:/E:/Mallet/test_dir/dog%202.txt            1.0
file:/E:/Mallet/test_dir/dog3.txt               1.0
file:/E:/Mallet/test_dir/dog4.txt               1.0
file:/E:/Mallet/test_dir/dog5.txt               1.0
file:/E:/Mallet/test_dir/dog6.txt               1.0
file:/E:/Mallet/test_dir/dogmaterial.txt                1.0
C:\Users\toshiba&gt;
</code></pre>

<hr>

<p>Thank you.</p>
",Text Classification / Sentiment Analysis,mallet text classification output value test file learning mallet text classification command line output value estimating differrent class know incorrect help mallet version e mallet mallet rc help thanks update thank answer still output file idea wa put dog file dog dir treated dog file instance trained model tested file test dir see result tried according understanding suggestion still output help commandlines e mallet train dir dog dog txt file dog txt dog txt dog txt dogmaterial txt e mallet test dir txt file cat txt catmaterial txt deermaterial txt dog txt dog txt dog txt dog txt dog txt dogmaterial txt thank
How to find Most frequently used words used on data using Python?,"<p>I am doing a sentiment analysis project in Python (using Natural Language Processing). I already collected the data from twitter and saved it as a CSV file. The file contains tweets, which are mostly about cryptocurrency. I cleaned the data and applied sentiment analysis using classification algorithms.</p>

<p>Since the data is clean, I want to find the most frequently used words. Here's the code that I used to import the libraries and the csv file:</p>

<pre><code># importing Libraries
from pandas import DataFrame, read_csv
import chardet
import matplotlib.pyplot as plt; plt.rcdefaults()
from matplotlib import rc
%matplotlib inline
import pandas as pd
plt.style.use('ggplot')
import numpy as np
import re
import warnings

#Visualisation
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from IPython.display import display
from mpl_toolkits.basemap import Basemap
from wordcloud import WordCloud, STOPWORDS

#nltk
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
from nltk import tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.snowball import SnowballStemmer


matplotlib.style.use('ggplot')
pd.options.mode.chained_assignment = None
warnings.filterwarnings(""ignore"")

## Reading CSV File and naming the object called crime
ltweet=pd.read_csv(""C:\\Users\\name\\Documents\\python assignment\\bitcoin1.csv"",index_col = None, skipinitialspace = True)
print(btweet)
</code></pre>

<p>There is no need for me to post the other codes because they are very long.
For data cleaning, I got rid of hyperlinks, RT(Retweeted), URL, Punctuation's, put text in lowercase, etc.</p>

<p>Here's the output for the list of positive tweets for example</p>

<pre><code>In [35]: btweet[btweet.sentiment_type == 'POSITIVE'].Tweets.reset_index(drop = True)[0:5]

Out[35]:
0    anizameddine more than just bitcoin blockchain...
1    bitcoinmagazine icymi wyoming house unanimousl...
2    bitracetoken bitrace published the smart contr...
3    unusual and quite promising ico banca banca_of...
4    airdrop coinstocks link it is a exchange so ge...
Name: Tweets, dtype: object
</code></pre>

<p>Is there a way to find the most frequently used words in the data? Can anyone help me write the code for it?</p>
",Text Classification / Sentiment Analysis,find frequently used word used data using python sentiment analysis project python using natural language processing already collected data twitter saved csv file file contains tweet mostly cryptocurrency cleaned data applied sentiment analysis using classification algorithm since data clean want find frequently used word code used import library csv file need post code long data cleaning got rid hyperlink rt retweeted url punctuation put text lowercase etc output list positive tweet example way find frequently used word data anyone help write code
"Text Classification into Predefined Labels, Unsupervised and Continuous learning","<p>I am trying to work on something, I want to classify customer calls into some n predefined categories. The frequency of words in my the calls cannot decide the labels. however, the category word will be present in the call.  And I do not have training data.</p>

<p>Also, I can classify the incoming text into labels, and i want my machine during the same time to learn from the new classified data. </p>

<p>eg: A Call comes - the system must be able to classify it into the n Categories and then i will label the call manually too, and then system should be able to adjust accordingly, weightage given to manual classification. </p>
",Text Classification / Sentiment Analysis,text classification predefined label unsupervised continuous learning trying work something want classify customer call n predefined category frequency word call decide label however category word present call training data also classify incoming text label want machine time learn new classified data eg call come system must able classify n category label call manually system able adjust accordingly weightage given manual classification
what are the best methods to classify the user gender based on names?,"<p>If you check my <a href=""https://github.com/raady07"" rel=""nofollow noreferrer"">github</a>, I have successfully implemented CNN, KNN for classifying signal faults. For that, I have taken the signal with little preprocessing for dimensionality reduction and provided it to the network, using its class information I trained the network, later the trained network is tested with testing samples to determine the class and computed the accuracy.</p>
<p>My question here how do I input the text information to CNN or any other network. For inputs, I took the Twitter database from kaggle, I have selected 2 columns which have names and gender information. I have gone through some algorithms which classify gender based on their blog data. I wasn't clear how I implement to my data (In my case, if I just want to classify using only names alone).</p>
<p>In some examples, which I understood I saw computing sparse matrix for the text, but for 20,000 samples the sparse matrix is huge to give as input. I have no problem in implementing the CNN architectures(I want to implement because no features are required) or any other network.  I am stuck here, how to input data to the network. What kind of conversations can I make so that I take the names and gender information can be considered to train the network?</p>
<p>If my method of thinking is wrong please provide me suggestion which algorithm is the best way. Deep learning or any other methods are ok!</p>
",Text Classification / Sentiment Analysis,best method classify user gender based name check github successfully implemented cnn knn classifying signal fault taken signal little preprocessing dimensionality reduction provided network using class information trained network later trained network tested testing sample determine class computed accuracy question input text information cnn network input took twitter database kaggle selected column name gender information gone algorithm classify gender based blog data clear implement data case want classify using name alone example understood saw computing sparse matrix text sample sparse matrix huge give input problem implementing cnn architecture want implement feature required network stuck input data network kind conversation make take name gender information considered train network method thinking wrong please provide suggestion algorithm best way deep learning method ok
Lemmatization of slang words in Python,"<p>I'm trying to train an SVM for Twitter text classification. Obviously, tweets tend to contain a lot of words that are slang or misspelled relative to formal documents that are more common to NLP. Here's an example of a tweet I'm trying to classify:</p>

<pre><code>Word I'm bout to holla at her via twitter RT @iamJay_Fresh : #trushit - im tryna fucc nicki minaj lol
</code></pre>

<p>I want to know if it's possible to stem/lemmatize this text so the slang words are corrected and it looks something like this:</p>

<pre><code>Word I'm about to holler at her via Twitter RT @iamJay_Fresh : #trushit - I'm trying to fuck Nicki Minaj lol
</code></pre>

<p>Note: I'm not too concerned about expanding common acronyms like ""lol"" to ""laughing out loud"". The reason I want to lemmatize is to reduce data sparsity: if the SVM has seen ""holler"" many times in a negative context but ""holla"" only a few times because people use it less, it would benefit from using the connotation of ""holler"" to classify a tweet with ""holla"". OTOH ""lol"" is commonly used in positive tweets so if the SVM sees another ""lol"" it'll know about its polarity even though it's not formal English.</p>
",Text Classification / Sentiment Analysis,lemmatization slang word python trying train svm twitter text classification obviously tweet tend contain lot word slang misspelled relative formal document common nlp example tweet trying classify want know possible stem lemmatize text slang word corrected look something like note concerned expanding common acronym like lol laughing loud reason want lemmatize reduce data sparsity svm ha seen holler many time negative context holla time people use le would benefit using connotation holler classify tweet holla otoh lol commonly used positive tweet svm see another lol know polarity even though formal english
Text classification,"<p>I have a trivial understanding of NLP so please keep things basic.</p>

<p>I would like to run some PDFs at work through a keyword extractor/classifier and build a taxonomy - in the hope of delivering some business intelligence.</p>

<p>For example, given a few thousand PDFs to mine I would like to determine the markets they apply to (we serve about 5 major industries with each one having several minor industries. Each industry and sub-industry has a specific market and in most cases those deal with OEMs, which in turn deal models, which further sub divide into component parts, etc.</p>

<p>I would love to crunch these PDFs into a semi-structured (more a graph actually) output like:</p>

<ul>
<li>Aerospace

<ul>
<li>Manufacturing

<ul>
<li>Repair

<ul>
<li>PT Support

<ul>
<li>M250</li>
<li>C20</li>
<li>C18</li>
</ul></li>
</ul></li>
</ul></li>
<li>Distribution</li>
</ul></li>
</ul>

<p>Can text classifiers do that? Is this too specific? How do you train a system like this that <em>C18</em> is a ""model"" of ""manufacturer""  Rolls Royce of the M250 series and ""PT SUPPORT"" is a sub-component?</p>

<p>I could build this data manually but would take forever...</p>

<p>Is there a way I could use a text classifier framework and build something more efficiently than regex and python? </p>

<p>Just looking for ideas at this point... Watched a few tutorials on R and python libs but they didn't sound quite like what I am looking for.</p>
",Text Classification / Sentiment Analysis,text classification trivial understanding nlp please keep thing basic would like run pdfs work keyword extractor classifier build taxonomy hope delivering business intelligence example given thousand pdfs mine would like determine market apply serve major industry one several minor industry industry sub industry ha specific market case deal oems turn deal model sub divide component part etc would love crunch pdfs semi structured graph actually output like aerospace manufacturing repair pt support c c distribution text classifier specific train system like c model manufacturer roll royce series pt support sub component could build data manually would take forever way could use text classifier framework build something efficiently regex python looking idea point watched tutorial r python libs sound quite like looking
Is the average of individual sentiment analysis of 5000 comments the same as sentiment analysis of concatenation of 5000 comments?,"<p>I'm trying to do a sentiment analysis on a reddit thread. The issue I'm facing is that some of the free tiers of cloud NLP APIs (Google Natural Language, Azure Text Analytics etc.) only allow 5000 calls per month in the free tier. I'm trying to see if I can concatenate some of the comments up to the max limit of characters per call to get more of the comments analyzed in the free tier. </p>

<ul>
<li>Is this a flawed approach?</li>
<li>Will doing a sentiment analysis on a concatenated string of comments lead to wrong sentiment score?</li>
<li>Should I be doing sentiment analysis on individual comments and then average all the individual scores to get the overall thread score?</li>
</ul>
",Text Classification / Sentiment Analysis,average individual sentiment analysis comment sentiment analysis concatenation comment trying sentiment analysis reddit thread issue facing free tier cloud nlp apis google natural language azure text analytics etc allow call per month free tier trying see concatenate comment max limit character per call get comment analyzed free tier flawed approach sentiment analysis concatenated string comment lead wrong sentiment score sentiment analysis individual comment average individual score get overall thread score
Getting true class labels after saving model for text-classification,"<p>I am doing text-classification. I have used <code>Conv1D</code> layer on top of Keras <code>Embedding</code> layer. I am getting a validation accuracy of 0.68.This is the <a href=""https://www.dropbox.com/s/bxe63rkqkaqji1x/emotion_merged_dataset.csv?dl=0"" rel=""nofollow noreferrer"">dataset</a> I am using. This is the code I am using:</p>

<pre><code>   #Processing
import pandas as pd
import pickle

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
from sklearn.preprocessing import LabelEncoder

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
from sklearn.preprocessing import LabelEncoder
from keras.layers import Embedding,Flatten,Dense,Conv1D,MaxPooling1D,GlobalMaxPooling1D
from keras.models import load_model

# df = pd.read_csv('text_emotion.csv')
#
# df.drop(['tweet_id', 'author'], axis=1, inplace=True)

# df = df[~df['sentiment'].isin(['empty', 'enthusiasm', 'boredom', 'anger'])]

# df = df.sample(frac=1).reset_index(drop=True)
df=pd.read_csv('emotion_merged_dataset.csv')
labels = df['sentiment']
# texts = df['content']
texts=df['text']
print (texts.shape)
#############################################
tokenizer = Tokenizer(3000)
tokenizer.fit_on_texts(texts)

sequences = tokenizer.texts_to_sequences(texts)
# print(sequences)

word_index = tokenizer.word_index
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# with open('word_index.pickle', 'rb') as handle:
#     word_index_new = pickle.load(handle)

# print (word_index == word_index_new)

# print('Word index: '+str(word_index))
# print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=37)

encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)

from keras.utils import np_utils

labels = np_utils.to_categorical(encoded_Y)
print ('Labels: '+str(labels))

print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)
print('data: '+str(data))

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
# print ('data:'+str(data[0]))
labels = labels[indices]
print(labels.shape)

model = Sequential()
model.add(Embedding(3000, 300, input_length=37))
# model.add(Flatten())

model.add(Conv1D(32,7,activation='relu'))
model.add(MaxPooling1D(3))
model.add(Conv1D(32,7,activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(labels.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(data, labels, validation_split=0.2, epochs=10, batch_size=100)
model.save('model_keras_embedding_cnn.h5')
print (model.summary())
</code></pre>

<p>I am pickling the tokenizer and saving the model. Then I am using the tokenizer to pre-process a sample input sentence to check my model.Here is the code below for testing:</p>

<pre><code>from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences
import pickle
model = load_model('model_keras_embedding_cnn.h5')
texts='I am really sad'
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer_new = pickle.load(handle)
tokenizer_new.fit_on_texts(texts)
sequences = tokenizer_new.texts_to_sequences(texts)
data = pad_sequences(sequences, maxlen=37)
print (model.predict_classes(data,verbose=10))
</code></pre>

<p>I am getting an output like:</p>

<pre><code>[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5].
</code></pre>

<p>How can I get true class labels(like fear,anger,etc)?
Is my approach correct i.e. saving the tokenizer and using it again? Have I messed up somewhere conceptually? 
[Edit] I used inverse_transform at the recommendation of JARS:</p>

<pre><code>print (encoder_new.inverse_transform(pred))
</code></pre>

<p>The output was this:</p>

<pre><code>['neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'
 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'
 'neutral']
</code></pre>

<p>Can someone explain the output?</p>
",Text Classification / Sentiment Analysis,getting true class label saving model text classification text classification used layer top kera layer getting validation accuracy dataset using code using pickling tokenizer saving model using tokenizer pre process sample input sentence check model code testing getting output like get true class label like fear anger etc approach correct e saving tokenizer using messed somewhere conceptually edit used inverse transform recommendation jar output wa someone explain output
Bag of words in VADER NLTK,"<p>I've been reading up on VADER as an alternative to Naive Bayes for sentiment analysis. I understand how to retrieve a bag of words using NB model;</p>

<pre><code>from nltk import FreqDist
f = FreqDist(movie_reviews.words())
f.most_common()[:X]
</code></pre>

<p>Is there a way that I could achieve the same output using VADER instead?</p>
",Text Classification / Sentiment Analysis,bag word vader nltk reading vader alternative naive bayes sentiment analysis understand retrieve bag word using nb model way could achieve output using vader instead
Pipeline : add another feature to text classification in Python (FeatureUnion),"<p>I am attempting to implement a text classification solution using scikit learn.</p>

<p>I have been able to get results for simple classification of text. Now I want to add another feature (non-text) into the prediction process - to improve accuracy.</p>

<p><strong>My data-set is as follows :</strong></p>

<ul>
<li>label : the target value i.e, 'sandwich', 'greeting' or 'goodbye'</li>
<li>message : the text</li>
<li>number_feature : randomly assigned integer. To test FeatureUnion, I have assigned same number to each category. For example, all 'sandwich' instances are given number 2</li>
</ul>

<p>The code:</p>

<pre><code>import pandas as pd
import sklearn 
from sklearn.pipeline import Pipeline, FeatureUnion 
from sklearn.feature_extraction.text import TfidfTransformer
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import FunctionTransformer
from sklearn.svm import LinearSVC


path = 'sunny_day.xlsx'                         
sms = pd.read_excel(path,header = None, names = ['label', 'message','number_feature'])   


#convert labels to a numeric value using a map and give it new column 'label_num'
sms['label_num'] = sms.label.map({'greeting' : 0, 'Goodbye' : 1, 'Sandwich' : 2})


X = sms.message
y = sms.label_num
z = sms.number_feature

# train test split
X_train = np.array(X[0:9])
X_test = np.array(X[9:])
y_train = np.array(y[0:9])
y_test = np.array(y[9:])
z_train = np.array(z[0:9])
z_test = np.array(z[9:])


def get_z(x):
    if np.array_equal(x, np.array(X_train)):
        return np.array(z_train).reshape(-1,1)
    else:
        return np.array(z_test).reshape(-1,1)


classifier = Pipeline([
    ('features', FeatureUnion([
        ('text',Pipeline([
            ('vectorizer', CountVectorizer()),
        ])),
        ('length', Pipeline([
            ('count', FunctionTransformer(get_z, validate = False)),
        ]))
    ])),
    ('clf',OneVsRestClassifier(LinearSVC()))])

classifier.fit(X_train, y_train)
y_pred_class = classifier.predict(X_test)
y_pred_class
</code></pre>

<p>As mentioned in various posts, I have made use of FeatureUnion to accomplish this. 
However the accuracy that I get - before applying the 'rigged' number_feature feature and even after it - is 66.67%.</p>

<p>Why doesn't the accuracy seem to improve when given a biased feature? </p>

<p>The dataset :</p>

<p>Label        | Message              | feature_number</p>

<pre><code>greeting   How are you?             5
greeting   How is your day?         5
greeting   Good day                 5
greeting   How is it going today?   5
Goodbye    Have a nice day          4
Goodbye    See you later            4
Goodbye    Have a nice day          4
Goodbye    Talk to you soon         4
Sandwich   Make me a sandwich.      2
Sandwich    Can you make a sandwich 2
Sandwich   Having a sandwich today? 2
Sandwich    what’s for lunch        2
</code></pre>
",Text Classification / Sentiment Analysis,pipeline add another feature text classification python featureunion attempting implement text classification solution using scikit learn able get result simple classification text want add another feature non text prediction process improve accuracy data set follows label target value e sandwich greeting message text number feature randomly assigned integer test featureunion assigned number category example sandwich instance given number code mentioned various post made use featureunion accomplish however accuracy get applying rigged number feature feature even accuracy seem improve given biased feature dataset label message feature number
Text file import to dataframe with pandas: Data in Blocks,"<p>I'm kinda new at using pandas, since I've done most of my data analysis usually with R.</p>

<p>Data format can be seen from this link where I got my data from:
<a href=""https://snap.stanford.edu/data/web-Movies.html"" rel=""nofollow noreferrer"">https://snap.stanford.edu/data/web-Movies.html</a></p>

<p>There are also HTML tags in the review/text attribute as well. So I'm thinking I should get rid of those too when I perform text sentiment analysis on it.</p>

<p><a href=""https://i.sstatic.net/3YRvo.png"" rel=""nofollow noreferrer"">Data snippet of a record</a></p>
",Text Classification / Sentiment Analysis,text file import dataframe panda data block kinda new using panda since done data analysis usually r data format seen link got data also html tag review text attribute well thinking get rid perform text sentiment analysis data snippet record
Trying to find the name of a specific location from tweets,"<p>I am trying to find the name of a specific location from tweets and performing sentiment analysis on the hits I get from the search. The problem I am facing is that, I am looking for a location whose name is suppose ""Sammy's Tap and Grill"", searching which I get no hits. I need to search something like ""Sammys"" or ""Sammy's"" to get some hits. Alternatively, when I search for ""Empire State Building"", I cannot search for ""Empire"" alone, it gives weird tweets including Mayan and Chola empires. So here I have to search with ""Empire State Building"" or ""Empire State"". So is there an NLP trick where I can do something and search for the best possible term from the full name of the location that gets most relevent hits? I was just able to make a solution where I was checking if the hits I get were nouns, because some places have names like ""Excellent"" and ""Fantastic"" and I didnt want adjectives to pop up. So is there some NLP way to solve my problem about searching a locationname  from a tweet?</p>
",Text Classification / Sentiment Analysis,trying find name specific location tweet trying find name specific location tweet performing sentiment analysis hit get search problem facing looking location whose name suppose sammy tap grill searching get hit need search something like sammys sammy get hit alternatively search empire state building search empire alone give weird tweet including mayan chola empire search empire state building empire state nlp trick something search best possible term full name location get relevent hit wa able make solution wa checking hit get noun place name like excellent fantastic didnt want adjective pop nlp way solve problem searching locationname tweet
Classification of Categories in Text Data,"<p>This may be an abstract question, but I always face difficulties with this kind of problem and it keeps on coming to me.</p>

<p>I crawled data (example: news articles about Tata Steel) extracted the content, manually read the content of each link and classified them as Finance, Operation, Sustainability and so on.</p>

<p>Then I made tf-idf data frame to be the features for classifier model. </p>

<p>I want to train the model to classify these articles. I am only left with either SVM or Logistic using the tf-idf features.</p>

<p>Is there a better approach to clssify text data? Can there be better approach rather then making tf-idf as we may loose information (contextual meaning of sentence) when breaking them into words and use as features.</p>

<p>Any algorithm which can help me to improve classification on text data? </p>
",Text Classification / Sentiment Analysis,classification category text data may abstract question always face difficulty kind problem keep coming crawled data example news article tata steel extracted content manually read content link classified finance operation sustainability made tf idf data frame feature classifier model want train model classify article left either svm logistic using tf idf feature better approach clssify text data better approach rather making tf idf may loose information contextual meaning sentence breaking word use feature algorithm help improve classification text data
Amazon Machine Learning for sentiment analysis,"<p>How flexible or supportive is the Amazon Machine Learning platform for sentiment analysis and text analytics?</p>
",Text Classification / Sentiment Analysis,amazon machine learning sentiment analysis flexible supportive amazon machine learning platform sentiment analysis text analytics
How Can I Use Confusion Matrix With Naive Bayes in Python?,"<p>I want to classify many sentences with Naive Bayes classifier with 5 categories and I can do, but I can not create a confusion matrix. I researched much but I could not find. These are my code, please teach me how to create a confusion matrix over these code:</p>

<pre><code>nb_classifier = NaiveBayesClassifier.train(trainData)
print(accuracy(nb_classifier, testData))
#Output is 0.8152876648699048

print(nb_classifier.labels())
#Output is ['cat1', 'cat2', 'cat3', 'cat4', 'cat5']
</code></pre>
",Text Classification / Sentiment Analysis,use confusion matrix naive bayes python want classify many sentence naive bayes classifier category create confusion matrix researched much could find code please teach create confusion matrix code
Number of training samples for text classification tas,"<p>Suppose you have a set of transcribed customer service calls between customers and human agents, where on average each call's length is 7 minutes. Customers will mostly call because of issues they have with the product. Let's assume that a human can assign one label per axis per call:</p>

<ul>
<li>Axis 1: What was the problem from the customer's perspective?</li>
<li>Axis 2: What was the problem from the agent's perspective?</li>
<li>Axis 3: Could the agent resolve the customer's issue?</li>
</ul>

<p>Based on the manually labeled texts you want to train a text classifier that shall predict a label for each call for each of the three axes. But the labeling of recordings takes time and costs money. On the other hand you need a certain amount of training data to get good prediction results.</p>

<p>Given the above assumptions, how many manually labeled training texts would you start with? And how do you know that you need more labeled training texts?</p>

<p>Maybe you've worked on a similar task before and can give some advice.</p>

<p>UPDATE (2018-01-19): There's no right or wrong answer to my question. Ok, ideally, somebody worked on exactly the same task, but that's very unlikely. I'll leave the question open for one more week and then accept the best answer.</p>
",Text Classification / Sentiment Analysis,number training sample text classification ta suppose set transcribed customer service call customer human agent average call length minute customer mostly call issue product let assume human assign one label per axis per call axis wa problem customer perspective axis wa problem agent perspective axis could agent resolve customer issue based manually labeled text want train text classifier shall predict label call three ax labeling recording take time cost money hand need certain amount training data get good prediction result given assumption many manually labeled training text would start know need labeled training text maybe worked similar task give advice update right wrong answer question ok ideally somebody worked exactly task unlikely leave question open one week accept best answer
Change sentiment of a single word,"<p>I've been working with NLTK in Python for a few days for sentiment analysis and it's a wonderful tool. My only concern is the sentiment it has for the word 'Quick'. Most of the data that I am dealing with has comments about a certain service and MOST refer to the service as being 'Quick' which clearly has Positive sentiments to it. However, NLTK refers to it as being Neutral. I want to know if it's even possible to retrain NLTK to now refer to the Quick adjective as having positive annotations?</p>
",Text Classification / Sentiment Analysis,change sentiment single word working nltk python day sentiment analysis wonderful tool concern sentiment ha word quick data dealing ha comment certain service refer service quick clearly ha positive sentiment however nltk refers neutral want know even possible retrain nltk refer quick adjective positive annotation
Google Natural Language Sentiment Analysis Aggregate Scores,"<p>In <a href=""https://cloud.google.com/natural-language/docs/basics#entity_sentiment_analysis_responses"" rel=""nofollow noreferrer"">this part</a> of the documentation of the Google Cloud Platform Natural Language API, it is described that</p>

<blockquote>
  <p><em>The overall score and magnitude values for an entity are an aggregate of the specific score and magnitude values for each mention of the entity.</em></p>
</blockquote>

<p>I can't figure out how this aggregation works. In the example provided in the documentation, Marvin Gaye has two mentions. One of the mentions has a sentiment of 0.4 and a magnitude of 0.4, the other mention has a score of -0.2 and a magnitude 0.2. The aggregate sentiment for Marvin Gaye is score 0.1 and magnitude 0.6.</p>

<p>I have tried other texts myself and can't figure out how the aggregation is made. Does anyone know?</p>
",Text Classification / Sentiment Analysis,google natural language sentiment analysis aggregate score part documentation google cloud platform natural language api described overall score magnitude value entity aggregate specific score magnitude value mention entity figure aggregation work example provided documentation marvin gaye ha two mention one mention ha sentiment magnitude mention ha score magnitude aggregate sentiment marvin gaye score magnitude tried text figure aggregation made doe anyone know
Bag of Words - Add feature manually,"<p>I am trying to improve my model on text classification. </p>

<pre><code>text = df['text']
count_vect = CountVectorizer(min_df=1,ngram_range=(1, 2), 
stop_words=""english"", max_features=200)
count_vect.fit(text)
counts = count_vect.transform(text)
</code></pre>

<p>Here is the sample output:</p>

<pre><code>[(u'spring', 386), (u'https', 341), (u'com', 317), (u'pr', 313), (u'for 
the', 285), (u'the pr', 208), (u'need', 196), (u'session', 164), 
(u'jp', 158), (u'png', 156), (u'updated', 154), (u'please', 152), 
(u'see', 145)]
</code></pre>

<p>I want to add word/frequencies manually which I thought could be a good indicator.
Any help on this?</p>
",Text Classification / Sentiment Analysis,bag word add feature manually trying improve model text classification sample output want add word frequency manually thought could good indicator help
"Can we combine base line Naive Bayes, Multinomial Naive Bayes and Semi-supervised NB?","<p>I am working on sentiment analysis on twitter data. I have tried with couple of Naive Bayes models like Baseline Naive Bayes, Multinomial NB, Bernoulli NB, Semi-supervised NB. My question here is to understand if there is a way we can combine the models in below two way to improve perforrmance?</p>

<ol>
<li>NB+MNB+SSNB</li>
<li>NB+BNB+SSNB</li>
</ol>

<p>Thanks</p>
",Text Classification / Sentiment Analysis,combine base line naive bayes multinomial naive bayes semi supervised nb working sentiment analysis twitter data tried couple naive bayes model like baseline naive bayes multinomial nb bernoulli nb semi supervised nb question understand way combine model two way improve perforrmance nb mnb ssnb nb bnb ssnb thanks
Is it possible to make SVM probabiility predictions without tm and RTextTools using e1071 in R?,"<p>I am trying to create a topic classifier from an employee satisfaction survey. The survey contains several commentary fields, and therefore want to produce an effective way of classifying what a single comment is about, and later also whether it is positive or negative (pretty standard sentiment analysis).
I already have a sample data from last years survey, where comments have been given a category manually. </p>

<p>The data is structured in a CSV file with three rows:</p>

<p>The document (or comment) - The topic - The sentiment</p>

<p>One example could be:</p>

<p>Document: I am afraid of violence from our customers, since my position does not have sufficient sercurity</p>

<p>Topic: Violence</p>

<p>Sentiment: Negative</p>

<p><em>(Very crude example, but bear with me)</em></p>

<p>My tool for making this classifier is RStudio, but I only have access to a limited number of packages. I do not have access to tm or RTextTools, which are the packages I usually use when I am doing projects outside of work. I pretty much only have access to e1071, and that is why I figured a support vector machine might do the trick. I have bad experiences with NaiveBayes when dealing with text analytics, but I am of course open to any advice. Is it possible at all to do text mining without tm or RTextTools? I have access to the NLP and tau packages</p>
",Text Classification / Sentiment Analysis,possible make svm probabiility prediction without tm rtexttools using e r trying create topic classifier employee satisfaction survey survey contains several commentary field therefore want produce effective way classifying single comment later also whether positive negative pretty standard sentiment analysis already sample data last year survey comment given category manually data structured csv file three row document comment topic sentiment one example could document afraid violence customer since position doe sufficient sercurity topic violence sentiment negative crude example bear tool making classifier rstudio access limited number package access tm rtexttools package usually use project outside work pretty much access e figured support vector machine might trick bad experience naivebayes dealing text analytics course open advice possible text mining without tm rtexttools access nlp tau package
How do I approach this machine learning/NLP context aware text classification project? See the description below,"<p>I am a newbie in machine learning and NLP. I need help for my college project. It's’ actually a subtask of a major project. Description is as follow:</p>

<p>It is a classification problem. I will be given an item and I have to predict the shop type from where the item can be bought.<br>
Examples:<br>
    <strong>Item       Class-label (shop-type)</strong><br>
    Pencil -> book store<br>
    Beer -> bar<br>
    Cash -> ATM<br>
    Tube Light -> electronics store<br>
    Medicine -> pharmacy<br>
I am given many different class labels like <em>stationary_shop,book_store,bakery,pharmacy</em> etc.(around 50 labels) </p>

<p><strong>Problems faced</strong>-<br>
1. I don’t have much bigger dataset. I built a small dataset all by myself.<br>
2. I don’t know much about machine learning and NLP techniques like how to approach this problem.<br>
3. How to make correct prediction? For example if I have <em>(pencil,book_store)</em> in my dataset and it is given <em>sharpener</em> as input it must predict the label as <em>book_store</em> since <em>sharpener</em> is closely related to <em>pencil</em>.</p>

<p><strong>My approaches</strong>:<br>
I started with a small dataset and then used <a href=""http://www.datamuse.com/api/"" rel=""nofollow noreferrer"">datamuse</a> api for extending it by finding related words for a given word. For eg. I extracted all words related to <em>pencil</em> from the API and tagged them with the same label <em>book_store</em>. Then I used <a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">fastText</a> for generating a prediction model but  I am not getting expected results.</p>

<p><strong>Small Dataset</strong><br>
Dataset format : (example,class_label)</p>

<p>soap,department_store<br>
pencil,book_store<br>
pen,book_store<br>
tea,department_store<br>
coffee,department_store<br>
bulb,electronics_store<br>
battery,electronics_store<br>
tubelight,electronics_store<br>
medicine,pharmacy<br>
book,book_store<br>
money,bank<br>
cash,atm<br>
flowers,florist<br>
fruits,grocery_or_supermarket<br>
cake,bakery<br>
clothes,clothing_store<br>
paper,book_store<br>
jewellery,jewelry_store<br>
shampoo,department_store<br>
oil,department_store<br>
sugar,department_store<br>
beer,bar<br>
whisky,bar<br>
alcohol,bar<br>
haircut,beauty_salon<br>
coffee,cafe<br>
sandwich,cafe<br>
pastry,bakery<br>
suit,clothing_store<br>
shoes,shoe_store<br>
sofa,furniture_store<br>
chair,furniture_store<br>
bed,furniture_store<br>
petrol,gas_station<br>
diesel,gas_station<br>
tools,hardware_store<br>
pipe,hardware_store<br>
tank,hardware_store<br>
washing,laundry<br>
drycleaning,laundry<br>
necklace,jewelry_store<br>
ring,jewelry_store<br>
ornament,jewelry_store<br>
dinner,restaurant<br>
lunch,restaurant<br>
pet,veterinary_care<br>
chips,department_store  </p>
",Text Classification / Sentiment Analysis,approach machine learning nlp context aware text classification project see description newbie machine learning nlp need help college project actually subtask major project description follow classification problem given item predict shop type item bought example item class label shop type pencil book store beer bar cash atm tube light electronics store medicine pharmacy given many different class label like stationary shop book store bakery pharmacy etc around label problem faced much bigger dataset built small dataset know much machine learning nlp technique like approach problem make correct prediction example pencil book store dataset given sharpener input must predict label book store since sharpener closely related pencil approach started small dataset used datamuse api extending finding related word given word eg extracted word related pencil api tagged label book store used fasttext generating prediction model getting expected result small dataset dataset format example class label soap department store pencil book store pen book store tea department store coffee department store bulb electronics store battery electronics store tubelight electronics store medicine pharmacy book book store money bank cash atm flower florist fruit grocery supermarket cake bakery clothes clothing store paper book store jewellery jewelry store shampoo department store oil department store sugar department store beer bar whisky bar alcohol bar haircut beauty salon coffee cafe sandwich cafe pastry bakery suit clothing store shoe shoe store sofa furniture store chair furniture store bed furniture store petrol gas station diesel gas station tool hardware store pipe hardware store tank hardware store washing laundry drycleaning laundry necklace jewelry store ring jewelry store ornament jewelry store dinner restaurant lunch restaurant pet veterinary care chip department store
Python assign labels to the text data using positive and negative text files for sentiment analysis(Text analytics/mining)?,"<p>I am new to the text mining. I have data set of user comments on the job done by the technician.</p>

<p>Below is the dataset rows.</p>

<pre><code>id    comments
1     That's not good job as it was not completed on mentioned time.
2     nice job
3    good job but not satisfied
</code></pre>

<p>Now here i do not have any labels and I want to do the sentiment analysis (find the polarity score and classify the comments into positive,negative and neutral class).</p>

<p>What I have done till now is that i took the text file having positive words and negative words from google. Then I looked into each comment using these text files and if word is present then assigned the respective label. But i am not getting expected values. All are getting the label as positive.</p>

<p>And one more problem is that in first comment there is word called <strong>""not good""</strong> which should be negative but in my positive text file if it is find word as <strong>good</strong> then it assigns the label as positive which is wrong.</p>

<p>Below is my code:</p>

<pre><code>pos_words_list = [w.lower() for w in pos_words_list]
neg_words_list = [w.lower() for w in neg_words_list]

def assign_comments_labels(x):
    try:
        if any(w in x for w in pos_words_list):
            return 'positive'
        elif any(w in x for w in neg_words_list):
            return 'negative'
        else:
            return 'neutral'
    except:
        return 'neutral'



df['COMMENTS'] = df['COMMENTS'].str.lower()

df['labels'] = df['COMMENTS'].apply(lambda x: assign_comments_labels(x))

df[['COMMENTS','labels']].head()
</code></pre>

<p>output:</p>

<pre><code>  id    comments                                                    labels
    1     That not good job was not completed on mentioned time.      positive
    2     nice job                                                    negative
    3     good job but not satisfied                                  neutral
</code></pre>

<p>Could anyone please tell me how do I achieve this. What is the proper way to assign the labels and perform the sentiment analysis. What else could I do for more exploration to get meaningful insights from the text data ?</p>
",Text Classification / Sentiment Analysis,python assign label text data using positive negative text file sentiment analysis text analytics mining new text mining data set user comment job done technician dataset row label want sentiment analysis find polarity score classify comment positive negative neutral class done till took text file positive word negative word google looked comment using text file word present assigned respective label getting expected value getting label positive one problem first comment word called good negative positive text file find word good assigns label positive wrong code output could anyone please tell achieve proper way assign label perform sentiment analysis else could exploration get meaningful insight text data
variable size of input for CNN model in text classification?,"<p>I implemented the CNN model for text classification based on this <a href=""http://www.aclweb.org/anthology/D14-1181"" rel=""nofollow noreferrer"">paper</a>. Since the CNN can only deal with the sentences that have fixed size, so I set the size of input as max length of sentence in my dataset and zero padding the short sentence. But for my understanding, no matter how long the input sentence is, the max pooling strategy will always extract only one value for each filter map. So it doesn't matter the size of input sentence is long or short, because after filter convoluted/pooled, the output will be the same size. In this case, why should I zero padding all the short sentence into the fixed size?</p>

<p>For example, my code for feeding data into the CNN model is <code>self.input_data = tf.placeholder(tf.int32,[None,max_len],name=""input_data"")</code>, can I do not specify <code>max_len</code>, and using the <code>None value</code> which is based on the length of current training sentence?</p>

<p>In addition, I was wondering is there any other new approach that can solve the variable input for CNN model. I also found the other <a href=""http://www.aclweb.org/anthology/P14-1062"" rel=""nofollow noreferrer"">paper</a> that can solve this problem, but for my understanding, it only used k values for max-pooling instead of 1 value of max-pooling, which can deal with variable sentence? How?</p>
",Text Classification / Sentiment Analysis,variable size input cnn model text classification implemented cnn model text classification based paper since cnn deal sentence fixed size set size input max length sentence dataset zero padding short sentence understanding matter long input sentence max pooling strategy always extract one value filter map matter size input sentence long short filter convoluted pooled output size case zero padding short sentence fixed size example code feeding data cnn model specify using based length current training sentence addition wa wondering new approach solve variable input cnn model also found paper solve problem understanding used k value max pooling instead value max pooling deal variable sentence
"Difference usage of document level, sentence level and aspect level in sentiment analysis","<p>I'm studying about Sentiment analysis. And I wonder for what purpose we should use document level, for what we use sentence level and for what we should use aspect level.
Thanks.</p>
",Text Classification / Sentiment Analysis,difference usage document level sentence level aspect level sentiment analysis studying sentiment analysis wonder purpose use document level use sentence level use aspect level thanks
Task(text classification) Based Embedding Overfit occurs with few hidden units,"<p>
Dataset: Ted dataset (provided as part of <a href=""https://github.com/oxford-cs-deepnlp-2017/practical-2"" rel=""nofollow noreferrer"">assignment-2 of Oxford NLP 2017</a>).</p>

<p>Task: Learn word embedding while performing text classification using Tensorflow.</p>

<p>Issue: Facing Overfitting even with below 5 hidden units whereas the data size is batch of (50 sample (textual content) and each sample is of 500x50 (sentence_length x Embed_dim). 
The number of batches = 33
sentence_length = 500</p>

<p>Note: Unseen words (in glove dict) and padding(if max_len &lt; 500) are Zero Embedded.</p>

<p>Explain why it overfits though the complexity of model is lesser when compared to the task ? </p>

<p>Method:
Preprocess the text by keeping only alphabets and removed unknown words as well as mostly and least commonly used words. 
Used corresponding word’s Glove vectors as word embed which are trainable. 
The Data was trained to classify the texts into 8 (mutually exclusive) classes. 
Note: Overfitting occurs even after performing Dropout.  </p>

<p>Model:</p>

<pre class=""lang-py prettyprint-override""><code>x = embedding(text) #Glove Vectors used (trainable) [Glove.6B][2]
h = tanh(Wx + b)
u = Vh + c
p = softmax(u)
if testing:
    prediction = arg maxy’ py’ 
else: # training, with y as the given gold label
    loss = -log(py) # cross entropy criterion
</code></pre>
",Text Classification / Sentiment Analysis,task text classification based embedding overfit occurs hidden unit dataset ted dataset provided part assignment oxford nlp task learn word embedding performing text classification using tensorflow issue facing overfitting even hidden unit whereas data size batch sample textual content sample x sentence length x embed dim number batch sentence length note unseen word glove dict padding max len zero embedded explain overfits though complexity model lesser compared task method preprocess text keeping alphabet removed unknown word well mostly least commonly used word used corresponding word glove vector word embed trainable data wa trained classify text mutually exclusive class note overfitting occurs even performing dropout model
What are the best Pre-Processing techniques for Sentiment Analysis.?,"<p>I am trying to classify a dataset of reviews in to two classes say class A and class B. I am using <code>LightGBM</code> to classify.</p>

<p>I have changed the parameters for the classifier many times but I can't get a huge difference in the results.</p>

<p>I think the problem is with the pre-processing step. I defined a function as shown below to take care of pre-processing. I used <code>Stemming</code> and removed <code>stopwords</code>. I don't know what I am missing. I have tried <code>LancasterStemmer</code> and <code>PorterStemmer</code></p>

<pre><code>stops = set(stopwords.words(""english""))
def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemm = False):
    txt = str(text)
    txt = re.sub(r'[^A-Za-z0-9\s]',r'',txt)
    txt = re.sub(r'\n',r' ',txt)

    if lowercase:
        txt = "" "".join([w.lower() for w in txt.split()])

    if remove_stops:
        txt = "" "".join([w for w in txt.split() if w not in stops])

    if stemming:
        st = PorterStemmer()
        txt = "" "".join([st.stem(w) for w in txt.split()])

    if lemm:
        wordnet_lemmatizer = WordNetLemmatizer()
        txt = "" "".join([wordnet_lemmatizer.lemmatize(w) for w in txt.split()])
    return txt
</code></pre>

<p><strong>Are there any more pre-processing steps to be done to get a better accuracy.?</strong></p>

<p>URL for the dataset : <a href=""https://he-s3.s3.amazonaws.com/media/hackathon/predict-the-happiness/predict-the-happiness/f2c2f440-8-dataset_he.zip"" rel=""nofollow noreferrer"">Dataset</a></p>

<p><strong>EDIT :</strong></p>

<p>Parameters that I used are as mentioned below.</p>

<pre><code>params = {'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': 0.01, 
    'max_depth': 22, 
    'num_leaves': 78,
    'feature_fraction': 0.1, 
    'bagging_fraction': 0.4, 
    'bagging_freq': 1}
</code></pre>

<p>I have altered the <code>depth</code> and <code>num_leaves</code> parameters along with others. But the accuracy is kind of stuck at a certain level..</p>
",Text Classification / Sentiment Analysis,best pre processing technique sentiment analysis trying classify dataset review two class say class class b using classify changed parameter classifier many time get huge difference result think problem pre processing step defined function shown take care pre processing used removed know missing tried pre processing step done get better accuracy url dataset dataset edit parameter used mentioned altered parameter along others accuracy kind stuck certain level
Adding location to Twitter sentiment analysis,"<p>I am trying to build a twitter sentiment analysis tool and want to add a geolocation - that looks for tweets within 10 miles of NYC. How do I do this? I I tried to add the location to the end of the url but it did not work.</p>

<p>Here is the code that I have so far:</p>

<pre><code>import oauth2 as oauth
import urllib2 as urllib

# See assignment1.html instructions or README for how to get these credentials

api_key = ''
api_secret = ''
access_token_key = '
access_token_secret = ''

_debug = 0

oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)
oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)

signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()

http_method = ""GET""


http_handler  = urllib.HTTPHandler(debuglevel=_debug)
https_handler = urllib.HTTPSHandler(debuglevel=_debug)

'''
Construct, sign, and open a twitter request
using the hard-coded credentials above.
'''
def twitterreq(url, method, parameters):
  req = oauth.Request.from_consumer_and_token(oauth_consumer,
                                             token=oauth_token,
                                             http_method=http_method,
                                             http_url=url, 
                                             parameters=parameters)

  req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)

  headers = req.to_header()

  if http_method == ""POST"":
    encoded_post_data = req.to_postdata()
  else:
    encoded_post_data = None
    url = req.to_url()

  opener = urllib.OpenerDirector()
  opener.add_handler(http_handler)
  opener.add_handler(https_handler)

  response = opener.open(url, encoded_post_data)

  return response

def fetchsamples():

  url = ""https://stream.twitter.com/1.1/statuses/filter.json?
  track=money&amp;locations""
  parameters = []
  response = twitterreq(url, ""GET"", parameters)
  for line in response:
    print(line.strip())

if __name__ == '__main__':
  fetchsamples()
</code></pre>
",Text Classification / Sentiment Analysis,adding location twitter sentiment analysis trying build twitter sentiment analysis tool want add geolocation look tweet within mile nyc tried add location end url work code far
"For text mining in R, how do I combine DocumentTermMatrix with original Data Frame?","<p>What I am looking to do is create code that will allow me to classify tweets. So in the example below I would want to take tweets talking about a credit card and determine if they are related to the issue of travel.</p>

<p>Here is the initial dataset: </p>

<pre><code>id&lt;- c(123,124,125,126,127) 
text&lt;- c(""Since I love to travel, this is what I rely on every time."", 
        ""I got this card for the no international transaction fee"", 
        ""I got this card mainly for the flight perks"",
        ""Very good card, easy application process"",
        ""The customer service is outstanding!"") 
travel_cat&lt;- c(1,0,1,0,0) 
df_all&lt;- data.frame(id,text,travel) 
</code></pre>

<p>Output 1:</p>

<pre><code>id  text                                                        travel_cat
123 Since I love to travel, this is what I rely on every time.  1
124 I got this card for the no international transaction fee    0
125 I got this card mainly for the flight perks                 1
126 Very good card, easy application process                    0
127 The customer service is outstanding!                        0
</code></pre>

<p>I am then creating a data frame with only the text field and then doing the text analytics:</p>

<pre><code>myvars&lt;- c(""text"")
df&lt;- df_all[myvars]

library(tm)
corpus&lt;- Corpus(DataframeSource(df))
corpus&lt;- tm_map(corpus, content_transformer(tolower))
corpus&lt;- tm_map(corpus, removePunctuation)
corpus&lt;- tm_map(corpus, removeWords, stopwords(""english""))
corpus&lt;- tm_map(corpus, stripWhitespace)
dtm&lt;- as.matrix(DocumentTermMatrix(corpus))
</code></pre>

<p>Output 2 (dtm):</p>

<pre><code>Docs    application card    customer    easy    every ... etc.
1       0           0       0           1       0
2       0           1       0           0       1
3       0           1       0           0       0
4       1           1       0           0       0
5       0           0       1           0       0
</code></pre>

<p>How do I then tie this back to the original data so that it contains the fields from the original dataset and the matrix (Output 1 + Output 2):
    id,text,travel_cat + application,card,customer,easy,every...</p>
",Text Classification / Sentiment Analysis,text mining r combine documenttermmatrix original data frame looking create code allow classify tweet example would want take tweet talking credit card determine related issue travel initial dataset output creating data frame text field text analytics output dtm tie back original data contains field original dataset matrix output output id text travel cat application card customer easy every
Dataset for sentiment analysis of diary entries,"<p>I was wondering if there is a dataset of sentiment-labelled diary entries? What I am looking for is a table of diary entries and a label indicating at least whether the entry is ""positive"" or ""negative"" (or even classified into more categories). </p>

<p>Example (completely arbitrary):</p>

<ul>
<li>""Today the floor was icy, I slipped and fell. I hate ice."" <em>=> label:
""negative""</em> </li>
<li>""I love my friends! They organised a surprise party for
me!"" <em>=> label: ""positive""</em></li>
</ul>
",Text Classification / Sentiment Analysis,dataset sentiment analysis diary entry wa wondering dataset sentiment labelled diary entry looking table diary entry label indicating least whether entry positive negative even classified category example completely arbitrary today floor wa icy slipped fell hate ice label negative love friend organised surprise party label positive
Categories Busineesses with Text analytics in Python,"<p>I'm a new-bee to AI and want to perform the below exercise. Can you please suggest the way to achieve it using python:</p>

<p>Scenario -
I have list of businesses of some companies as below like:</p>

<pre><code> 1. AI
 2. Artificial Intelligence
 3. VR
 4. Virtual reality
 5. Mobile application
 6. Desktop softwares
</code></pre>

<p>and want to categorize them as below:</p>

<pre><code> Technology                 ---&gt; Category
 1. AI                      ---&gt; Category Artificial Intelligence
 2. Artificial Intelligence ---&gt; Category Artificial Intelligence
 3. VR                      ---&gt; Category Virtual Reality
 4. Virtual reality         ---&gt; Category Virtual Reality
 5. Mobile application      ---&gt; Category Application
 6. Desktop softwares       ---&gt; Category Application
</code></pre>

<p>i.e when I receive a text like <strong>AI</strong> or <strong>Artificial Intelligence</strong>, then it must identify AI &amp; Artificial Intelligence as one and the same and put both keywords under Artificial Intelligence category.</p>

<p>The current approach I follow is using the lookup a table but, I want to apply TEXT CLASSIFICATION on the technologies/business for the above input using python where I can segregate the technologies instead of using the lookup table.</p>

<p>Please suggest me any relevant approach.</p>
",Text Classification / Sentiment Analysis,category busineesses text analytics python new bee ai want perform exercise please suggest way achieve using python scenario list business company like want categorize e receive text like ai artificial intelligence must identify ai artificial intelligence one put keywords artificial intelligence category current approach follow using lookup table want apply text classification technology business input using python segregate technology instead using lookup table please suggest relevant approach
"sentiment analysis - wordNet , sentiWordNet lexicon","<p>I need a list of positive and negative words with the <strong>weights</strong> assigned to words according to how strong and week they are. I have got :</p>

<p>1.) WordNet - It gives a + or - score for every word.  </p>

<p>2.) SentiWordNet - Giving positive and negative values in the range [0,1].</p>

<p>I checked these on few words,</p>

<p>love - wordNet is giving 0.0 for both noun and verb, I dont know why i think it should be positive by at least some factor.  </p>

<p>repress - wordNet gives -9.93<br>
        - SentiWordNet gives - 0.0 for both pos and neg. (should be negative)  </p>

<p>repose  - wordNet - 2.488<br>
        - SentiWordNet - { pos - 0.125, neg - 0.5 } (should be positive)  </p>

<p>I need some help to decide which one to use. </p>

<p>Thanks. </p>
",Text Classification / Sentiment Analysis,sentiment analysis wordnet sentiwordnet lexicon need list positive negative word weight assigned word according strong week got wordnet give score every word sentiwordnet giving positive negative value range checked word love wordnet giving noun verb dont know think positive least factor repress wordnet give sentiwordnet give po neg negative repose wordnet sentiwordnet po neg positive need help decide one use thanks
Are there examples of using reinforcement learning for text classification?,"<p>Imagine a binary classification problem like sentiment analysis. Since we have the labels, cant we use the gap between actual - predicted as reward for RL ?</p>

<p>I wish to try Reinforcement Learning for Classification Problems</p>
",Text Classification / Sentiment Analysis,example using reinforcement learning text classification imagine binary classification problem like sentiment analysis since label cant use gap actual predicted reward rl wish try reinforcement learning classification problem
Sentiment analysis in PHP?,"<p>Since both OpenNlp and StanfordCoreLibraries are java implementation, is it possible to integrate with PHP developement?
if so can u guys provide me with some resource links or small examples.
Thank you</p>
",Text Classification / Sentiment Analysis,sentiment analysis php since opennlp stanfordcorelibraries java implementation possible integrate php developement u guy provide resource link small example thank
Classify text based on location/time from an establishing mention,"<p>Given a longer English text (> a few paragraphs), is there a rule-based NLP approach to classifying a set of text to be occurring at a place or time, from an establishing phrase? For example:</p>

<blockquote>
  <p>Alice went to London. She met Bob at his hotel, and they went out to
  dinner. They discussed old times and made a new friend, Cassandra.
  Over the next week, ...</p>
  
  <p>[more paragraphs of activity, implicitly in London]</p>
  
  <p>Alice then left London and went back to Madrid. She...</p>
</blockquote>

<p>As readers, we can say that Alice, Bob, and Cassandra all were in London and performed various actions there. But I can't find how to do automate this, how to match NLP techniques that I have read about with this more specific task.</p>

<p>Are there current tools or research that deal with this problem? I am interested in unstructured, likely narrative, text. Even a name for what this is would be helpful, something more specific than ""text mining"" or ""text classification"".</p>

<p>I understand that there will not be 100% accuracy with a rule-based approach. I also understand a sufficiently trained algorithm would perform this task better, but I am interested in something quick and dirty right now. Are there out-of-the-box tools that do similar work, rule-based or trained? (I am enjoying StanfordNLP, and the OpenIE module is pretty good, but doesn't quite get to this specificity, afaik.)</p>
",Text Classification / Sentiment Analysis,classify text based location time establishing mention given longer english text paragraph rule based nlp approach classifying set text occurring place time establishing phrase example alice went london met bob hotel went dinner discussed old time made new friend cassandra next week paragraph activity implicitly london alice left london went back madrid reader say alice bob cassandra london performed various action find automate match nlp technique read specific task current tool research deal problem interested unstructured likely narrative text even name would helpful something specific text mining text classification understand accuracy rule based approach also understand sufficiently trained algorithm would perform task better interested something quick dirty right box tool similar work rule based trained enjoying stanfordnlp openie module pretty good quite get specificity afaik
Grouping texts into Buckets in R,"<p>We are working on a survey where we have a few open-ended answers part from the numeric/categorical responses.Till now we used to categorize these texts into 10-15 buckets manually so that the marketing team can take actions on it.For example, if the respondent is asked what other features he wants in a particular tablet which he is using, we will group his/hers responses into buckets like '<strong>Better security features</strong>', '<strong>Better support</strong>' etc.</p>

<p>Instead of doing it manually I am automating this by creating individual logistic regression/CART/Random Forest Equations for each bucket. For example for bucket one 1 use the code</p>

<pre><code>model1=glm(Better.support~.,data=verbatimSparse,family=binomial)
verbatim$predict1=predict(model1,type=""response"")
</code></pre>

<p>I am building 12 other models like this and each response will be grouped into the bucket where the predicted probability is the highest.This is somewhat serving my purpose, but with the accuracy is only around 80%.Is there any other method to better classify the text.</p>
",Text Classification / Sentiment Analysis,grouping text bucket r working survey open ended answer part numeric categorical response till used categorize text bucket manually marketing team take action example respondent asked feature want particular tablet using group response bucket like better security feature better support etc instead manually automating creating individual logistic regression cart random forest equation bucket example bucket one use code building model like response grouped bucket predicted probability highest somewhat serving purpose accuracy around method better classify text
Convert string data into PTB format to train the Stanford Sentiment Analysis tool,"<p>How convert string data, like a tweet, into PTB format to train the Stanford Sentiment Analysis tool?</p>
",Text Classification / Sentiment Analysis,convert string data ptb format train stanford sentiment analysis tool convert string data like tweet ptb format train stanford sentiment analysis tool
Difference between TaggedDocument and TaggedLineDocument in gensim? and How to work with files in a directory?,"<p>I am new to doc2vec and I wish to classify set of texts using it.</p>

<p>I am confused about TaggedDocument and TaggedLineDocument.</p>

<p>1) What is the difference between two? Is it that TaggedLineDocument is collection of TaggedDocuments?</p>

<p>2) If I have a directory containing all the files, How to generate feature vectors for them? Should I create a new file where each line contains text from the file from the directory?</p>
",Text Classification / Sentiment Analysis,difference taggeddocument taggedlinedocument gensim work file directory new doc vec wish classify set text using confused taggeddocument taggedlinedocument difference two taggedlinedocument collection taggeddocuments directory containing file generate feature vector create new file line contains text file directory
Identifying multiple categories and associated sentiment within text,"<p>If you have a corpus of text, how can you identify all the categories (from a list of pre-defined categories) and the associated sentiment (positive/negative writing) with it?</p>

<p>I will be doing this in Python but at this stage I am not necessarily looking for a language specific solution.</p>

<hr>

<p>Let's look at this question with an example to try and clarify what I am asking.</p>

<p>If I have a whole corpus of reviews for products e.g.:</p>

<blockquote>
  <p>Microsoft's Xbox One offers impressive graphics and a solid list of exclusive 2015 titles. The Microsoft console currently edges ahead of the PS4 with a better selection of media apps. The console's fall-2015 dashboard update is a noticeable improvement. The console has backward compatibility with around 100 Xbox 360 titles, and that list is poised to grow. The Xbox One's new interface is still more convoluted than the PS4's. In general, the PS4 delivers slightly better installation times, graphics and performance on cross-platform games. The Xbox One also lags behind the PS4 in its selection of indie games. The Kinect's legacy is still a blemish. While the PS4 remains our overall preferred choice in the game console race, the Xbox One's significant course corrections and solid exclusives make it a compelling alternative.</p>
</blockquote>

<p>And I have a list of pre-defined categories e.g. :</p>

<ul>
<li>Graphics</li>
<li>Game Play</li>
<li>Game Selection</li>
<li>Apps</li>
<li>Performance</li>
<li>Irrelevant/Other</li>
</ul>

<p>I could take my big corpus of reviews and break them down by sentence.  For each sentence in my training data I can hand tag them with the appropriate categories.  The problem is that there could be various categories in 1 sentence.</p>

<p>If it was 1 category per sentence then any classification algorithm from scikit-learn would do the trick.  When working with multi-classes I could use something like multi-label classification. </p>

<p>Adding in the sentiment is the trickier part.  Identifying sentiment in a sentence is a fairly simple task but if there is a mix of sentiment on different labels that becomes different.  </p>

<p>The example sentence ""The Xbox One has a good selection of games but the performance is worse than the PS4"".  We can identify two of our pre-defined categories (game selection, performance) but we have positive sentiment towards game selection and a negative sentiment towards performance.  </p>

<p>What would be a way to identify all categories in text (from our pre-defined list) with their associated sentiment?</p>
",Text Classification / Sentiment Analysis,identifying multiple category associated sentiment within text corpus text identify category list pre defined category associated sentiment positive negative writing python stage necessarily looking language specific solution let look question example try clarify asking whole corpus review product e g microsoft xbox one offer impressive graphic solid list exclusive title microsoft console currently edge ahead p better selection medium apps console fall dashboard update noticeable improvement console ha backward compatibility around xbox title list poised grow xbox one new interface still convoluted p general p delivers slightly better installation time graphic performance cross platform game xbox one also lag behind p selection indie game kinect legacy still blemish p remains overall preferred choice game console race xbox one significant course correction solid exclusive make compelling alternative list pre defined category e g graphic game play game selection apps performance irrelevant could take big corpus review break sentence sentence training data hand tag appropriate category problem could various category sentence wa category per sentence classification algorithm scikit learn would trick working multi class could use something like multi label classification adding sentiment trickier part identifying sentiment sentence fairly simple task mix sentiment different label becomes different example sentence xbox one ha good selection game performance worse p identify two pre defined category game selection performance positive sentiment towards game selection negative sentiment towards performance would way identify category text pre defined list associated sentiment
Multiple Feature Sets,"<p>I am studying NLP and learning to use NLTK and scikit-learn for text classification. So I have implemented Unigram presence as a feature on the movie_review corpus of NLTK. Now I am trying to implement a research paper where they have used multiple features :</p>

<pre><code>1. N-Gram Features : Unigram : Presence and Count, Bigram : Presence and Count
2. Stylistic Features : POS_Noun, POS_Adverb, POS_Adjective : Ratios No. of spelling errors : Real-valued feature Length of text : Real-valued feature
</code></pre>

<p>Since I have only implemented one feature function thus far, I am having problems now. I can implement feature functions for all of the above separately like :</p>

<p>def find_POS_Noun_feature(document) : for using POS_Noun as a feature to classify the text def find_Length_of_text_feature(document) : For using #characters as my feature etc. etc. Then I can train multiple classifiers separately.</p>

<p>But what I require is how to integrate all of these features into a single classifier?</p>

<p>Please help, I searched the internet but couldn't find proper explanations.</p>
",Text Classification / Sentiment Analysis,multiple feature set studying nlp learning use nltk scikit learn text classification implemented unigram presence feature movie review corpus nltk trying implement research paper used multiple feature since implemented one feature function thus far problem implement feature function separately like def find po noun feature document using po noun feature classify text def find length text feature document using character feature etc etc train multiple classifier separately require integrate feature single classifier please help searched internet find proper explanation
derive ngrams that have independent meaning to feed into visualization part,"<p>Having some customer reviews, I derived important themes in the corpus[cost, side effect, time to take effect].
Now for each theme, I want to do sentiment analysis in this way:
derive n grams that have independent meaning and are related to the theme. For instance as you see, in the picture there are 3 themes. in the theme ""time to take effect"",phrases like ""a while"" or ""not immediate"" are related ngrams.</p>

<p>What I've done in Python: for each theme I manually, divided the corpus into 2 strata, </p>

<ol>
<li>sentences with the words related to theme</li>
<li>sentences with no word related to the theme</li>
</ol>

<p>Then for each word, I calculated the overindexing with the hope that the words that we want to be overindexed in the sentence. (eg. ['It', 0.001] ['is', 0.001], ['n't', 0.002], ['immediate', 0.042], ['but', 0.002], ['it', 0.001], ['does',0.002], ['work', 0.002])
but this didn't work because some other words unrelated to the theme have very high overindexing values.</p>

<p>Is there any other option that I could derive n-grams related to the theme?
my final goal is to produce something like this:</p>

<p><a href=""https://i.sstatic.net/CpdIV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CpdIV.png"" alt=""enter image description here""></a></p>
",Text Classification / Sentiment Analysis,derive ngrams independent meaning feed visualization part customer review derived important theme corpus cost side effect time take effect theme want sentiment analysis way derive n gram independent meaning related theme instance see picture theme theme time take effect phrase like immediate related ngrams done python theme manually divided corpus stratum sentence word related theme sentence word related theme word calculated overindexing hope word want overindexed sentence eg n immediate doe work work word unrelated theme high overindexing value option could derive n gram related theme final goal produce something like
"interpret sentiment analysis result (naive baye, svm , maxent)","<p>i have done the sentiment analysis for some airline reviews using code from <a href=""https://datascienceplus.com/sentiment-analysis-with-machine-learning-in-r/"" rel=""nofollow noreferrer"">https://datascienceplus.com/sentiment-analysis-with-machine-learning-in-r/</a> ... but i am facing a problem of interpreting the results of naive Bayes, SVM and the others . so my question is how do i interpret these results? (how do i make statistical sense out of it for my final year research for B.sc Statistics? ) Thank You!</p>
",Text Classification / Sentiment Analysis,interpret sentiment analysis result naive baye svm maxent done sentiment analysis airline review using code facing problem interpreting result naive bayes svm others question interpret result make statistical sense final year research b sc statistic thank
Has anyone used CoreNLP from stanford for sentiment analysis in Spark? It does not work as desired for me,"<p>Has anyone used CoreNLP from stanford for sentiment analysis in Spark? 
It is not working as desired or may be I need to do some work which I am not aware of. </p>

<p>Following is the example. </p>

<p>1). I look forward to interacting with kids of states governed by the congress. - POSITIVE 
2). I look forward to interacting with CM of states governed by the congress. - NEGETIVE (CM is chief minister) </p>

<p>Please note the change in one word here. kids -> CM 
Statement 2 is not negetive but coreNLP tagged it as negetive. 
is there anything I need to do to make it work as desired? Any alteration required? Please let me know if I need to plug-in any custom code. 
Whoever has knowledge on this, please suggest something. 
Also, suggest if there is any other better alternate to coreNLP. </p>

<p>Thanks. 
Gaurav</p>
",Text Classification / Sentiment Analysis,ha anyone used corenlp stanford sentiment analysis spark doe work desired ha anyone used corenlp stanford sentiment analysis spark working desired may need work aware following example look forward interacting kid state governed congress positive look forward interacting cm state governed congress negetive cm chief minister please note change one word kid cm statement negetive corenlp tagged negetive anything need make work desired alteration required please let know need plug custom code whoever ha knowledge please suggest something also suggest better alternate corenlp thanks gaurav
Logistic regression overfits even using cross validation in sklearn?,"<p>I am implementing a logistic regression model using sklearn, for a text classification competition on Kaggle.</p>

<ol>
<li><p>When I use unigram, there are 23,617 features. The best <code>mean_test_score</code> Cross validation search (sklearn's <code>GridSearchCV</code>) gives me is similar to the score I got from Kaggle, using the best model.</p></li>
<li><p>There are 1,046,524 features if I use bigram. <code>GridSearchCV</code> gives me a better <code>mean_test_score</code> compared to unigram, but using this new model I got a much much lower score on Kaggle.</p></li>
</ol>

<p>I guess the reason might be overfitting, since I have too many features. I have tried to set the <code>GridSearchCV</code> using 5-fold, or even 2-fold, but the scores are still inconsistent. </p>

<p>Does it really indicate my second model is overfitting, even in the validation stage? If so, how can I tune the regularization term for my logistic model using sklearn? Any suggestions are appreciated!</p>
",Text Classification / Sentiment Analysis,logistic regression overfits even using cross validation sklearn implementing logistic regression model using sklearn text classification competition kaggle use unigram feature best cross validation search sklearn give similar score got kaggle using best model feature use bigram give better compared unigram using new model got much much lower score kaggle guess reason might overfitting since many feature tried set using fold even fold score still inconsistent doe really indicate second model overfitting even validation stage tune regularization term logistic model using sklearn suggestion appreciated
Which deep learning model can classify categories which are not mutually exclusive,"<p>Examples : I have a sentence in job description : ""Java senior engineer in UK "".</p>

<p>I want to use a deep learning model to predict it as 2 categories : <code>English</code>  and <code>IT jobs</code>. If i use traditional classification model, it only can predict 1 label with <code>softmax</code> function at last layer . Thus, i can use 2 model neural networks to predict ""Yes""/""No"" with both categories, but if we have more categories, it is too expensive . So do we have any deeplearning or machine learning model to predict 2 or more categories at same time ?</p>

<p>""Edit"" : With 3 labels by traditional approach , it will be encoded by [1,0,0] but in my case, it will be encoded by [1,1,0] or [1,1,1]</p>

<p>Example : if we have 3 labels, and a sentence may be fit with all of these labels. So if output from softmax function is [0.45 , 0.35 , 0.2 ] we should classify it into 3 labels or 2 labels , or may be one ?</p>

<p>The main problem when we do it is : what is good threshold to classify into 1, or 2 , or 3 labels ?</p>
",Text Classification / Sentiment Analysis,deep learning model classify category mutually exclusive example sentence job description java senior engineer uk want use deep learning model predict category use traditional classification model predict label function last layer thus use model neural network predict yes category category expensive deeplearning machine learning model predict category time edit label traditional approach encoded case encoded example label sentence may fit label output softmax function classify label label may one main problem good threshold classify label
Which deep learning model can classify categories which are not mutually exclusive,"<p>Examples : I have a sentence in job description : ""Java senior engineer in UK "".</p>

<p>I want to use a deep learning model to predict it as 2 categories : <code>English</code>  and <code>IT jobs</code>. If i use traditional classification model, it only can predict 1 label with <code>softmax</code> function at last layer . Thus, i can use 2 model neural networks to predict ""Yes""/""No"" with both categories, but if we have more categories, it is too expensive . So do we have any deeplearning or machine learning model to predict 2 or more categories at same time ?</p>

<p>""Edit"" : With 3 labels by traditional approach , it will be encoded by [1,0,0] but in my case, it will be encoded by [1,1,0] or [1,1,1]</p>

<p>Example : if we have 3 labels, and a sentence may be fit with all of these labels. So if output from softmax function is [0.45 , 0.35 , 0.2 ] we should classify it into 3 labels or 2 labels , or may be one ?</p>

<p>The main problem when we do it is : what is good threshold to classify into 1, or 2 , or 3 labels ?</p>
",Text Classification / Sentiment Analysis,deep learning model classify category mutually exclusive example sentence job description java senior engineer uk want use deep learning model predict category use traditional classification model predict label function last layer thus use model neural network predict yes category category expensive deeplearning machine learning model predict category time edit label traditional approach encoded case encoded example label sentence may fit label output softmax function classify label label may one main problem good threshold classify label
Text classification using Keras: How to add custom features?,"<p>I'm writing a program to classify texts into a few classes. Right now, the program loads the train and test samples of word indices, applies an embedding layer and a convolutional layer, and classifies them into the classes. I'm trying to add handcrafted features for experimentation, as in the following code. The <code>features</code> is a list of two elements, where the first element consists of features for the training data, and the second consists of features for the test data. Each training/test sample will have a corresponding feature vector (i.e. the features are not word features).</p>

<pre><code>model = Sequential()
model.add(Embedding(params.nb_words,
                    params.embedding_dims,
                    weights=[embedding_matrix],
                    input_length=params.maxlen,
                    trainable=params.trainable))
model.add(Convolution1D(nb_filter=params.nb_filter,
                        filter_length=params.filter_length,
                        border_mode='valid',
                        activation='relu'))
model.add(Dropout(params.dropout_rate))
model.add(GlobalMaxPooling1D())

# Adding hand-picked features
model_features = Sequential()
nb_features = len(features[0][0])

model_features.add(Dense(1,
                         input_shape=(nb_features,),
                         init='uniform',
                         activation='relu'))

model_final = Sequential()
model_final.add(Merge([model, model_features], mode='concat'))

model_final.add(Dense(len(citfunc.funcs), activation='softmax'))
model_final.compile(loss='categorical_crossentropy',
                    optimizer='adam',
                    metrics=['accuracy'])

print model_final.summary()
model_final.fit([x_train, features[0]], y_train,
                nb_epoch=params.nb_epoch,
                batch_size=params.batch_size,
                class_weight=data.get_class_weights(x_train, y_train))

y_pred = model_final.predict([x_test, features[1]])
</code></pre>

<p>My question is, is this code correct? Is there any conventional way of adding features to each of the text sequences?</p>
",Text Classification / Sentiment Analysis,text classification using kera add custom feature writing program classify text class right program load train test sample word index applies embedding layer convolutional layer classifies class trying add handcrafted feature experimentation following code list two element first element consists feature training data second consists feature test data training test sample corresponding feature vector e feature word feature question code correct conventional way adding feature text sequence
Extracting text between html tags and labelling it with the tag in R,"<p>I am trying to learn how to classify sentences in R.</p>

<p>I have a text file containing sentences in the following format:</p>

<pre><code>&lt;happy&gt;
  This did the trick : the boys now have a more distant friendship and David is much happier . 
&lt;\happy&gt;
&lt;happy&gt;
  When Anna left Inspector Aziz , she was much happier . 
&lt;\happy&gt;
</code></pre>

<p>I do intent to tag the sentences in the following way:</p>

<pre><code>dataset$text = When Anna left Inspector Aziz , she was much happier
dataset$label = happy
</code></pre>

<p>I want to extract the sentence and label them with the emotion. How should I approach this? I know that I should use grouping in regex but I don't know how to do this in R. I am  new to it and learning.</p>

<pre><code>rl &lt;- readLines('sentences.txt')
</code></pre>
",Text Classification / Sentiment Analysis,extracting text html tag labelling tag r trying learn classify sentence r text file containing sentence following format intent tag sentence following way want extract sentence label emotion approach know use grouping regex know r new learning
Comparison one entity with other #tweeter data,"<p>I am exploring NLTK and other text analysis as i am a aspiring student in data analytics.
I am breaking my head to figure out how this can be done in Text analysis
Question 1)I have crawled tweeter data of #entity1 and #entity2
I want to compare what issues or how people are comparing entity 1 with entity 2
ex: i ate in #entity1 their service was good but not as good as we get in #entity 2(something on this lines). 
I have tried some tweeter data from kaggle to do sentiment analysis and stuff. But when this scenario comes i am stuck as to how to move ahead. 
Any starting suggestions are welcome.</p>
",Text Classification / Sentiment Analysis,comparison one entity tweeter data exploring nltk text analysis aspiring student data analytics breaking head figure done text analysis question crawled tweeter data entity entity want compare issue people comparing entity entity ex ate entity service wa good good get entity something line tried tweeter data kaggle sentiment analysis stuff scenario come stuck move ahead starting suggestion welcome
Why did the Keras Sequential model give a different result compared to Model model?,"<p>I've tried a simple lstm model in keras to do a simple sentiment analysis using imdb dataset using both Sequential model and Model model, and turns out the latter gives a worse result. Here's my code :</p>

<pre><code>model = Sequential()
model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))
model.add(LSTM(100))
model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
</code></pre>

<p>It gives a result around 0.6 of accuracy in the first epoch, while the other code that use Model :</p>

<pre><code>_input = Input(shape=[max_review_length], dtype='int32')
embedded = Embedding(
        input_dim=top_words,
        output_dim=embedding_size,
        input_length=max_review_length,
        trainable=False,
        mask_zero=False
    )(_input)
lstm = LSTM(100, return_sequences=True)(embedded)
probabilities = Dense(2, activation='softmax')(lstm)
model = Model(_input, probabilities)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
</code></pre>

<p>and it gives 0.5 accuracy as a result of the first epoch and never change afterwards. </p>

<p>Any reason for that, or am i doing something wrong? Thanks in advance</p>
",Text Classification / Sentiment Analysis,kera sequential model give different result compared model model tried simple lstm model kera simple sentiment analysis using imdb dataset using sequential model model model turn latter give worse result code give result around accuracy first epoch code use model give accuracy result first epoch never change afterwards reason something wrong thanks advance
How many classes can CNN classify the short text?,"<p>I know that CNN(conv-neural-network) could classify more than 10 thousands of images of the ImageNet.</p>

<p>I find that CNN could only classify 10-20 text classes as <a href=""https://arxiv.org/pdf/1502.01710.pdf"" rel=""nofollow noreferrer"">this paper</a> write.</p>

<p>How many classes can CNN classify the short text? <strong>What is the high limit of the classes number?</strong> </p>
",Text Classification / Sentiment Analysis,many class cnn classify short text know cnn conv neural network could classify thousand image imagenet find cnn could classify text class paper write many class cnn classify short text high limit class number
Text classification: Na&#239;ve Bayes classifier with skewed data distribution,"<p>I have a question about Naïve Bayes classifier with skewed data distribution for training and test data. </p>

<ul>
<li>training data has 90% spam and 10% non-spam</li>
<li>test data has 80% non- spam and 20% spam</li>
</ul>

<p>Would it be better to use MLE(max. likelihood) than MAP(standard max. posterior probability) for decision function for training data or not?</p>

<p>My understanding is as the distribution of training data and that of test data is different, if we use max. posterior probabilities then test results will be biased towards spam class, So MLE is better. Is my understanding correct? </p>
",Text Classification / Sentiment Analysis,text classification na bayes classifier skewed data distribution question na bayes classifier skewed data distribution training test data training data ha spam non spam test data ha non spam spam would better use mle max likelihood map standard max posterior probability decision function training data understanding distribution training data test data different use max posterior probability test result biased towards spam class mle better understanding correct
What type of NLP method to choose?,"<p>so I'm going to build a prototype of a Social Web application that:
- Incorporates Facebook data of users (working hours, house and work office)
to create a web app so that friends and friends of friends that have similar routes can drive/bike with each other.</p>

<p>However, in order for this app to be useful it should be able to extract keywords (e.g. working hours, or if someone has to work later (and he/she posts this on Facebook). Now I'm reading a lot of methods but I don't know which one to choose:
 - Sentiment analysis
 - Lexical analysis 
 - Syntactic parsing</p>

<p>Thanks in advance.</p>
",Text Classification / Sentiment Analysis,type nlp method choose going build prototype social web application incorporates facebook data user working hour house work office create web app friend friend friend similar route drive bike however order app useful able extract keywords e g working hour someone ha work later post facebook reading lot method know one choose sentiment analysis lexical analysis syntactic parsing thanks advance
Text Classification - Label Pre Process,"<p>I have a data set of 1M+ observations of customer interactions with a call center. The text is free text written by the representative taking the call. The text is not well formatted nor is it close to being grammatically correct (a lot of short hand). None of the free text has a label on the data as I do not know what labels to provide. </p>

<p>Given the size of the data, would a random sample of the data  (to give a high level of confidence) be reasonable first step in determining what labels to create? Is it possible not to have to manually label 400+ random observations from the data, or is there no other method to pre-process the data in order to determine the a good set of labels to use for classification?</p>

<p>Appreciate any help on the issue.</p>
",Text Classification / Sentiment Analysis,text classification label pre process data set observation customer interaction call center text free text written representative taking call text well formatted close grammatically correct lot short hand none free text ha label data know label provide given size data would random sample data give high level confidence reasonable first step determining label create possible manually label random observation data method pre process data order determine good set label use classification appreciate help issue
Sorting sentences into paragraphs,"<p>I want to arrange a set of sentences into paragraphs automatically, at the moment Im reading from a file the sentences and measuring the string distance between them. I was thinking that the next logical step would be to find something else with which to classify the sentences, and then use this two attributes to divide to plot the sentences in a graph and then apply a KMeans algorithm to them that would help me devise which sentences are similar to each other and therefor which ones are going into the same paragraphs. This has proven to be harder than I thought, I would therefore appreciate any input in either, the second attribute I can use to measure the tweets, a different approach or a tool that does this for me. Below is the code I'm currently using </p>

<pre><code>import re
import math
from collections import Counter
import itertools


#first understadn this code so that we can manipulate it.
WORD = re.compile(r'\w+')

def get_cosine(vec1, vec2):
    intersection =  set(vec1.keys()) &amp; set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x]**2 for x in vec1.keys()])
    sum2 = sum([vec2[x]**2 for x in vec2.keys()])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator

def text_to_vector(text):
    words = WORD.findall(text)
    return Counter(words)

#count the number of tweets set it to a variable and then set it as the length of this or  what ever
#This is where the text comes from
with open(""positive copy.txt"", ""r"") as pt:
    lines = pt.readlines()
    # Count how many lines we have
    count = len(lines)
    # Create a count * count size matrix
    Matrix = [[1 for x in range(count)] for y in range(count)] 
    # Loop through lines assigning x as the number of line we're on and lineA as it's text
    for x, lineA in enumerate(lines):
        vectorA = text_to_vector(lineA)
        for y, lineB in enumerate(itertools.islice(lines, count - x)):
            vectorB = text_to_vector(lineB)
            cosine = get_cosine(vectorA, vectorB)
            print lineA, lineB, ""\n Cosine:"", cosine, ""\n""
            Matrix[y][x]=get_cosine(vectorA, vectorB)
            Matrix[x][y]=get_cosine(vectorA, vectorB)
    print Matrix
</code></pre>

<p>and here is the sample data I'm using to run my tests</p>

<pre><code>Hello my name is Jeff
Hello everyone I’m named Jeff
this has absolutely nothing to do
everyone Im a doctor
hello I don’t even know whats happening
whats  happening is that you not know
</code></pre>

<p>This are my results as of now: </p>

<pre><code>[[0.9999999999999998, 0.33806170189140655, 0.0, 0.0, 0.0, 0.16903085094570328], [0.33806170189140655, 0.9999999999999999, 0.0, 0.1889822365046136, 0.13363062095621217, 1], [0.0, 0.0, 1.0000000000000002, 0.0, 1, 1], [0.0, 0.1889822365046136, 0.0, 1, 1, 1], [0.0, 0.13363062095621217, 1, 1, 1, 1], [0.16903085094570328, 1, 1, 1, 1, 1]]
</code></pre>

<p>While these are the results expected for the code above, I want to be able to output a set of values that I can plot into a graph. Or a way to derive into which paragraph I should assign each paragraph, this is more of a knowledge question than a code one, although any helpful code is more than welcomed</p>
",Text Classification / Sentiment Analysis,sorting sentence paragraph want arrange set sentence paragraph automatically moment im reading file sentence measuring string distance wa thinking next logical step would find something else classify sentence use two attribute divide plot sentence graph apply kmeans algorithm would help devise sentence similar therefor one going paragraph ha proven harder thought would therefore appreciate input either second attribute use measure tweet different approach tool doe code currently using sample data using run test result result expected code want able output set value plot graph way derive paragraph assign paragraph knowledge question code one although helpful code welcomed
svm file format in weka,"<p>I want to classify texts using svm (smo) in weka. The file I have, contains some sentences (Persian) and a word in front of each sentence which shows its class. The question is: should I change these sentences to a binary vector and give these vectors to weka as input or is it enough if I just turn the sentences to vector by choosing ""string to word vector"" in weka itself?</p>

<p>sample file:</p>

<p><a href=""https://www.dropbox.com/s/ohpyortve8jbwhe/shoor.arff?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/ohpyortve8jbwhe/shoor.arff?dl=0</a></p>
",Text Classification / Sentiment Analysis,svm file format weka want classify text using svm smo weka file contains sentence persian word front sentence show class question change sentence binary vector give vector weka input enough turn sentence vector choosing string word vector weka sample file
What are the segmentation patterns for sentiment analysis,"<p>I am trying to implement sentiment analysis for customer reviews.I found some patterns like POS tagging, bigrams models.I need to know more ways which i can use to segment sentence to find it is negative or positive.</p>
",Text Classification / Sentiment Analysis,segmentation pattern sentiment analysis trying implement sentiment analysis customer review found pattern like po tagging bigram model need know way use segment sentence find negative positive
Why negative sentences have the same polarity as a positive sentences in Sentiment analysis with TextBlob?,"<p>I'm using TextBlob for Sentiment analysis and observed this --</p>

<p>The positive sentence is: 
<strong>She is that good at this.</strong></p>

<p>Its sentiment analysis is:
<strong>polarity=0.7</strong>, subjectivity=0.6000000000000001</p>

<p>The negative sentence is:
<strong>She isn't that good at this.</strong></p>

<p>Its sentiment analysis is:
<strong>polarity=0.7</strong>, subjectivity=0.6000000000000001</p>

<p>I understand Subjectivity could be same(or almost same) but how could polarity be the same? Isn't the second sentence negative?</p>

<p>Here's the code as requested:</p>

<pre><code>analysis = TextBlob(""She isn't that good at this"")
print (analysis.sentiment)
</code></pre>
",Text Classification / Sentiment Analysis,negative sentence polarity positive sentence sentiment analysis textblob using textblob sentiment analysis observed positive sentence good sentiment analysis polarity subjectivity negative sentence good sentiment analysis polarity subjectivity understand subjectivity could almost could polarity second sentence negative code requested
NLP-steps or approch to classify text?,"<p>I'm working on a project to classify restaurant reviews on sentiment(positive or negative) basis. Also I want to classify that if these comments belongs to food, service, value-for-money, etc category. I am unable to link the steps or the methodology provided on the internet. can anyone provide detailed method or steps to get to the solution.</p>
",Text Classification / Sentiment Analysis,nlp step approch classify text working project classify restaurant review sentiment positive negative basis also want classify comment belongs food service value money etc category unable link step methodology provided internet anyone provide detailed method step get solution
Should sentiment analysis training data be evenly distributed?,"<p>If I am training a sentiment classifier off of a tagged dataset where most documents are negative, say ~95%, should the classifier be trained with the same distribution of negative comments? If not, what would be other options to ""normalize"" the data set? </p>
",Text Classification / Sentiment Analysis,sentiment analysis training data evenly distributed training sentiment classifier tagged dataset document negative say classifier trained distribution negative comment would option normalize data set
"classify new document - Random Forest, Bag of Words","<p>This is my first attempt of document classification with ML and Python.</p>

<ol>
<li>I first query my database to extract 5000 articles related to money laundering and convert them to pandas df</li>
<li>Then I extract 500 articles not related to money laundering and also convert them to pandas df</li>
<li>I concatenate both dfs and label them either 'money-laundering' or 'other'</li>
<li>I do preprocessing (removing punctuation and stopwords, lower case etc)</li>
<li><p>and then feed the model based on bag of words principle as below:</p>

<pre><code>vectorizer = CountVectorizer(analyzer = ""word"",   
                     tokenizer = None,    
                     preprocessor = None, 
                     stop_words = None,   
                     max_features = 5000) 

text_features = vectorizer.fit_transform(full_df[""processed full text""])    
text_features = text_features.toarray()    
labels = np.array(full_df['category'])
X_train, X_test, y_train, y_test = train_test_split(text_features, labels, test_size=0.33)    
forest = RandomForestClassifier(n_estimators = 100)     
forest = forest.fit(X_train, y_train)    
y_pred = forest.predict(X_test)    
accuracy_score(y_pred=y_pred, y_true=y_test)
</code></pre></li>
</ol>

<p>It works fine until now (even though gives me too high accuracy 99%). But I would like to test it on a completely new text document now. If I vectorize it and do              <code>forest.predict(test)</code> it obviously says:</p>

<pre><code>ValueError: Number of features of the model must  match the input. Model n_features is 5000 and  input n_features is 45 
</code></pre>

<p>I am not sure how to overcome this to be able to classify totally new article.</p>
",Text Classification / Sentiment Analysis,classify new document random forest bag word first attempt document classification ml python first query database extract article related money laundering convert panda df extract article related money laundering also convert panda df concatenate dfs label either money laundering preprocessing removing punctuation stopwords lower case etc feed model based bag word principle work fine even though give high accuracy would like test completely new text document vectorize obviously say sure overcome able classify totally new article
classifying text by sentence structure in python,"<p>I want to be able to tell if a sentence actually makes sense in python. For example  </p>

<pre><code>Hello, how are you today? --&gt; Correct
Hello, are today are how --&gt; Incorrect
</code></pre>

<p>So far I have this code which does the following:  </p>

<p>1 - Gets 20 000 sentences from the nltk.browns corpus.<br>
2 - Mixes up the words of the last 10 000 sentences (for incorrect data)<br>
3 - Tags all sentences using the Stanford POS Tagger  </p>

<pre><code>from nltk.corpus import brown
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.classify import accuracy
from nltk.tag import StanfordPOSTagger
from nltk import pos_tag
import string
import random

PUNC = [x for x in string.punctuation] + [""''"", ""``""]

def download_data(target_path):
    brown_data = brown.sents()
    i = 0; n = 0
    with open(target_path, 'w') as data_file:
        while n &lt; 20000:
            for sent in sent_tokenize(' '.join(brown_data[i])):
                updated_list = [x for x in word_tokenize(sent) if x not in PUNC]
                if n &gt; 10000:
                    random.shuffle(updated_list)
                sent = ' '.join(updated_list) + '\n'
                if sent != '\n':
                    data_file.write(sent)
                    n += 1
                i += 1

def get_data(data_path):
    with open(data_path, 'r') as data_file:
        return [x for x in data_file.readlines()]

def parse_data(data, tagger):
    parsed_data = []
    for i in range(len(data)):
        if i &gt; 10000:
            parsed_data.append((tagger.tag(word_tokenize(data[i].replace('\n', ''))), False))
        else:
            parsed_data.append((tagger.tag(word_tokenize(data[i].replace('\n', ''))), True))
    random.shuffle(parsed_data)
    return parsed_data[:15000], parsed_data[15000:]

if __name__ == '__main__':
    data_path = 'data.txt'

    stanford_jar_path = 'stanford-postagger.jar'
    stanford_model_path = 'models/english-bidirectional-distsim.tagger'

    tagger = StanfordPOSTagger(model_filename=stanford_model_path,
                               path_to_jar=stanford_jar_path)

    #download_data(data_path)
    all_data = get_data(data_path)
    train_data, test_data = parse_data(all_data, tagger)
</code></pre>

<p>So how can I train a classifier for example, to classify new texts as either correct or incorrect based on the data that I have provided?  </p>

<p>If anybody has any better suggestions I am also open.</p>
",Text Classification / Sentiment Analysis,classifying text sentence structure python want able tell sentence actually make sense python example far code doe following get sentence nltk brown corpus mix word last sentence incorrect data tag sentence using stanford po tagger train classifier example classify new text either correct incorrect based data provided anybody ha better suggestion also open
How to classify a sentence based on a few templates?,"<p>I have a question about how chatbot platforms such as api.ai and wit.ai work. You only provide them with a few templates for each intent/class and then their system is able to classify the new user queries to one of those classes.
I am trying to do the same and have tested algorithms such as SVM, logistic regression, naive bayes and a few other ones. For the feature extraction part I have tried most of the features that people usually use such as n-grams, vector representation, bow etc.
What would be the best algorithm(s) in such case when there is a few templates per class?</p>

<p>Thanks
Amir :)</p>
",Text Classification / Sentiment Analysis,classify sentence based template question chatbot platform api ai wit ai work provide template intent class system able classify new user query one class trying tested algorithm svm logistic regression naive bayes one feature extraction part tried feature people usually use n gram vector representation bow etc would best algorithm case template per class thanks amir
Can I extract y-values (data labels) from inside a cross-validation pipline in scikit-learn?,"<p>My text classification pipeline has these steps: </p>

<ol>
<li><strong>Chunking</strong>, with a custom transformer, with a few parameters (input: an XML text file; output: a bunch of documents and labels for those documents) </li>
<li><strong>Vectorizing</strong>, with TfidfVectorizer (input: a list of documents; output: an DxF matrix where D is the number of docs and F is the number of features)</li>
<li><strong>Sparse-to-Dense Matrix Transformer</strong> (input: a sparse matrix; output: a dense matrix) </li>
<li><strong>Dimensionality reduction</strong>, with PCA or similar technique (input: DxF matrix, output: DxN matrix, where N is a param: the number of desired components) </li>
<li><strong>Prediction with GaussianMixture</strong> (input: a DxN matrix, output: cluster assignments, i.e. groupings of documents) </li>
</ol>

<p>There are so many parameters for each of these steps that it's inefficient to look through all the possible param combinations manually, so I've been trying to do a cross-validataion grid search with <code>CVGridSearch()</code>. That can use a scorer to compare the output groupings with the original groupings (labels). (The scorer I'm using is <code>metrics.adjusted_rand_index()</code>.) </p>

<p>If I cut out step 1, the chunker, I can feed the data and the labels into a pipeline starting with step 2, and then run a grid search over all parameters of steps 2-4 to find the best params. But the problem is, the chunks generated by step 1 are also parameters that need to be tweaked, so I'd like to keep step 1 in. But I can't get the labels until after I finish step 1, and the grid search needs the labels to do its scoring. </p>

<p>So what I'd like to know is: is there a way to have CVGridSearch get the labels it needs from the first step, instead of being supplied labels ahead of time? </p>

<p>Edit: <a href=""https://github.com/JonathanReeve/character-attribution/blob/master/waves/waves-grid-search.ipynb"" rel=""nofollow noreferrer"">here's a link to a notebook that illustrates the kind of thing I've been trying. (Non-working first step grid search is commented out.)</a></p>
",Text Classification / Sentiment Analysis,extract value data label inside cross validation pipline scikit learn text classification pipeline ha step chunking custom transformer parameter input xml text file output bunch document label document vectorizing tfidfvectorizer input list document output dxf matrix number doc f number feature sparse dense matrix transformer input sparse matrix output dense matrix dimensionality reduction pca similar technique input dxf matrix output dxn matrix n param number desired component prediction gaussianmixture input dxn matrix output cluster assignment e grouping document many parameter step inefficient look possible param combination manually trying cross validataion grid search use scorer compare output grouping original grouping label scorer using cut step chunker feed data label pipeline starting step run grid search parameter step find best params problem chunk generated step also parameter need like keep step get label finish step grid search need label scoring like know way cvgridsearch get label need first step instead supplied label ahead time edit link notebook illustrates kind thing trying non working first step grid search commented
Features for sentiment analysis of twitter data related to music,"<p>Need some guidance related to sentiment analysis on tweets related to music on spark.</p>

<p>I was trying to perform sentiment analysis on twitter data for tweets related to music. After a lot of searching around the net, I have understood how to fetch the tweets using 'tweepy' python api and also realized that I can use 'Naive Bayes classifier' to finally classify the tweets. Now I am confused regarding how to define features for this classification, I am supposed to define at least 500 features. So here are my questions. I do not want to use any already available API like 'textblob' to find the sentiment of a tweet.</p>

<p>1) Can anyone give some examples of features that we can use for classifying music related tweets ?
[ can we use tweets with a happy smiley as positive training set ? if so are the words in those tweets features for my classifier ?]</p>

<p>2) How do we generate the training set for this classifier?</p>

<p>3) If I want to filter the tweets for music related tweets, can I use Bloom Filter to achieve it ?</p>

<p>4) What is the size of data I can get through tweepy api ?</p>

<p>Please correct me if  there is something wrong with my understanding.</p>
",Text Classification / Sentiment Analysis,feature sentiment analysis twitter data related music need guidance related sentiment analysis tweet related music spark wa trying perform sentiment analysis twitter data tweet related music lot searching around net understood fetch tweet using tweepy python api also realized use naive bayes classifier finally classify tweet confused regarding define feature classification supposed define least feature question want use already available api like textblob find sentiment tweet anyone give example feature use classifying music related tweet use tweet happy smiley positive training set word tweet feature classifier generate training set classifier want filter tweet music related tweet use bloom filter achieve size data get tweepy api please correct something wrong understanding
Simple Binary Text Classification,"<p>I seek the most effective and simple way to classify 800k+ scholarly articles as either relevant (1) or irrelevant (0) in relation to a defined conceptual space (here: <a href=""https://docs.google.com/spreadsheets/d/1GW1ooVc0BpVFVKrgKBPdWMsCpXDRebSBH2z-b0YAjec/"" rel=""nofollow noreferrer"">learning as it relates to work</a>).</p>

<p>Data is: title &amp; abstract (mean=1300 characters)</p>

<p>Any approaches may be used or even combined, including supervised machine learning and/or by establishing features that give rise to some threshold values for inclusion, among other.</p>

<p>Approaches could draw on the <a href=""https://docs.google.com/spreadsheets/d/1GW1ooVc0BpVFVKrgKBPdWMsCpXDRebSBH2z-b0YAjec/"" rel=""nofollow noreferrer"">key terms that describe the conceptual space</a>, though simple frequency count alone is too unreliable. Potential avenues might involve latent semantic analysis, n-grams, .. </p>

<p>Generating training data may be realistic for up to 1% of the corpus, though this already means manually coding 8,000 articles (1=relevant, 0=irrelevant), would that be enough?</p>

<p>Specific ideas and some brief reasoning are much appreciated so I can make an informed decision on how to proceed. Many thanks!</p>
",Text Classification / Sentiment Analysis,simple binary text classification seek effective simple way classify k scholarly article either relevant irrelevant relation defined conceptual space learning relates work data title abstract mean character approach may used even combined including supervised machine learning establishing feature give rise threshold value inclusion among approach could draw key term describe conceptual space though simple frequency count alone unreliable potential avenue might involve latent semantic analysis n gram generating training data may realistic corpus though already mean manually coding article relevant irrelevant would enough specific idea brief reasoning much appreciated make informed decision proceed many thanks
Looking for information on a Sentiment Analysis algorithm / tool for .NET,"<p>I need to use/purchase some sort of tool for doing Sentiment Analysis to determine positive or negative connotation in text content. There are some terrific threads on this topic which I have read and listed below:</p>

<p><a href=""https://stackoverflow.com/questions/293000/algorithm-to-determine-how-positive-or-negative-a-statement-text-is"">Algorithm to determine how positive or negative a statement/text is</a><br>
<a href=""https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence"">NLP: Qualitatively &quot;positive&quot; vs &quot;negative&quot; sentence</a><br>
<a href=""https://stackoverflow.com/questions/1326171/algorithm-to-determine-how-positive-or-negative-a-statement-text-is"">Algorithm to determine how positive or negative a statement/text is</a> (same name different thread)</p>

<p>The problem is each one gives a great description of the algorithm, but alludes to the complexity of doing the job from scratch and offers links to the algorithm explanations.</p>

<p>I need a <strong>.NET</strong> (VB.NET or C#) solution either in the form of a toolkit, API, .dll, etc. I have seen links to JAVA solutions but none really in my searches for .NET solutions.</p>

<p>My fallback plan is to create dictionaries of words with weights and go that route, but I would prefer something a bit more robust.</p>

<p>Does anyone have any information on a Sentiment Analysis solution specifically for .NET?</p>

<p>Thanks!</p>
",Text Classification / Sentiment Analysis,looking information sentiment analysis algorithm tool net need use purchase sort tool sentiment analysis determine positive negative connotation text content terrific thread topic read listed href determine positive negative statement text name different thread problem one give great description algorithm alludes complexity job scratch offer link algorithm explanation need net vb net c solution either form toolkit api dll etc seen link java solution none really search net solution fallback plan create dictionary word weight go route would prefer something bit robust doe anyone information sentiment analysis solution specifically net thanks
scikit-learn: classifying texts using custom labels,"<p>I have a large training set of words labeled <code>pos</code> and <code>neg</code> to classify texts. I used TextBlob (according to <a href=""http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/"" rel=""nofollow noreferrer"">this tutorial</a>) to classify texts. While it works fairly well, it can be very slow for a large training set (e.g. 8k words).</p>

<p>I would like to try doing this with <code>scikit-learn</code> but I'm not sure where to start. What would the above tutorial look like in <code>scikit-learn</code>? I'd also like the training set to include weights for certain words. Some that should pretty much guarantee that a particular text is classed as ""positive"" while others guarantee that it's classed as ""negative"". And lastly, is there a way to imply that certain parts of the analyzed text are more valuable than others?</p>

<p>Any pointers to existing tutorials or docs appreciated!</p>
",Text Classification / Sentiment Analysis,scikit learn classifying text using custom label large training set word labeled classify text used textblob according tutorial classify text work fairly well slow large training set e g k word would like try sure start would tutorial look like also like training set include weight certain word pretty much guarantee particular text classed positive others guarantee classed negative lastly way imply certain part analyzed text valuable others pointer existing tutorial doc appreciated
Analysis on Twitter Handle using R,"<p>I want to find tweets of one particular handle using R package. </p>

<p>Let's say for example I want to analyse tweets of David Beckham. </p>

<p>Parameters for Analysis :</p>

<ol>
<li>No. of tweets</li>
<li>No. of retweets.</li>
<li>No. of tweets which had direct or indirect link with Victoria Beckham.</li>
<li>No. of tweets on one particular day.</li>
<li>Sentiment analysis.</li>
</ol>

<p>Please suggest any R package having functionality to accomplish above task.</p>
",Text Classification / Sentiment Analysis,analysis twitter handle using r want find tweet one particular handle using r package let say example want analyse tweet david beckham parameter analysis tweet retweets tweet direct indirect link victoria beckham tweet one particular day sentiment analysis please suggest r package functionality accomplish task
Working of Machine learning algorithms for sentiment analysis,"<p>I found a good example of a <strong>Naive Bayes Classifier</strong> from <a href=""http://www.nltk.org/howto/classify.html"" rel=""nofollow"">here</a> . I am unable to understand the steps.</p>

<pre><code>from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
train_data = [({""a"": 4, ""b"": 1, ""c"": 0}, ""ham""),
      ({""a"": 5, ""b"": 2, ""c"": 1}, ""ham""),
      ({""a"": 0, ""b"": 3, ""c"": 4}, ""spam""),
          ({""a"": 5, ""b"": 1, ""c"": 1}, ""ham""),
          ({""a"": 1, ""b"": 4, ""c"": 3}, ""spam"")]
classif = SklearnClassifier(BernoulliNB()).train(train_data)
test_data = [{""a"": 3, ""b"": 2, ""c"": 1},
             {""a"": 0, ""b"": 3, ""c"": 7}]
classif.classify_many(test_data)
['ham', 'spam']
classif = SklearnClassifier(SVC(), sparse=False).train(train_data)
classif.classify_many(test_data)
['ham', 'spam']
</code></pre>

<p>What are :</p>

<ol>
<li>Features in the code above?</li>
<li>Actual Data for sentiment?</li>
<li>""a"": 4, ""b"": 1, ""c"": 0 ?</li>
<li>ham, spam?</li>
</ol>

<p>The basic purpose is to understand that how the ML Algorithm works. 
I am newbie in Sentiment Analysis. I hope someone will help.</p>
",Text Classification / Sentiment Analysis,working machine learning algorithm sentiment analysis found good example naive bayes classifier unable understand step feature code actual data sentiment b c ham spam basic purpose understand ml algorithm work newbie sentiment analysis hope someone help
Internal structure of SentiWordNet 3,"<p>I am working on a project of Sentiment analysis that uses <code>SentiwordNet3</code> lexicon. The problem is, I am unable to understand the structure of lexicon. One line of this Lexicon is:</p>

<blockquote>
  <p>a  00002730    0   0   acroscopic#1    facing or on the side toward the apex</p>
</blockquote>

<p><strong>1.</strong> <code>a</code>=? Purpose of <code>a</code>?</p>

<p><strong>2.</strong> <code>POS-ID=00002730</code>. What is the purpose of <code>POS-ID</code>?</p>

<p><strong>3.</strong> <code>SynsetTerms Gloss</code> = <code>acroscopic#1 facing or on the side toward the apex</code></p>

<ol>
<li>What is <code>acroscopic</code> ? Is it word that we want the score of?</li>
<li>What is <code>#1</code>? Purpose?</li>
<li>Why is the sentence after <code>#1</code>?  <code>facing or on the side toward the apex</code>.</li>
</ol>

<p>As far as the accessing of score is concerned, we accessed it as</p>

<pre><code>`list(SWN.senti_synsets(Tagged[i][0],'a')`
</code></pre>

<p><code>a</code> is <code>Adjective</code> whereas <code>v</code>,<code>r</code> and <code>n</code> are also in the lexicon.</p>
",Text Classification / Sentiment Analysis,internal structure sentiwordnet working project sentiment analysis us lexicon problem unable understand structure lexicon one line lexicon acroscopic facing side toward apex purpose purpose word want score purpose sentence far accessing score concerned accessed whereas also lexicon
NLP Algorithm for calculating urgency_intensity for a text fragment,"<p>I want to calculate score of urgency for a text fragment, Like SentiWordnet provides scores for words for sentiment polarity. I want to tag text as High, Medium or Low on the basis of how urgent the request is. It seems classification will not serve the purpose as it is not something Black or White. I want some algorithm that will give a score for urgency and then i may use threshold for reaching different categories.</p>

<p>I investigated that i can make use of adjective grading. Is there any already built theasure for it, like SentiWordNet for sentiment. </p>

<p>Can i use SentiWordNet's polarity score value for all adverbs and use them to calculate score.</p>

<p>Thanks in advance.</p>
",Text Classification / Sentiment Analysis,nlp algorithm calculating urgency intensity text fragment want calculate score urgency text fragment like sentiwordnet provides score word sentiment polarity want tag text high medium low basis urgent request seems classification serve purpose something black white want algorithm give score urgency may use threshold reaching different category investigated make use adjective already built theasure like sentiwordnet sentiment use sentiwordnet polarity score value adverb use calculate score thanks advance
How to train the Stanford NLP Sentiment Analysis tool,"<p>Hell everyone! I'm using the Stanford Core NLP package and my goal is to perform sentiment analysis on a live-stream of tweets. </p>

<p>Using the sentiment analysis tool as is returns a very poor analysis of text's 'attitude' .. many positives are labeled neutral, many negatives rated positive. I've gone ahead an acquired well over a million tweets in a text file, but I haven't a clue how to actually <em>train</em> the tool and create my own model.</p>

<p><a href=""http://nlp.stanford.edu/sentiment/code.html"">Link to Stanford Sentiment Analysis page</a></p>

<p>""Models can be retrained using the following command using the PTB format dataset:""</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath     dev.txt -train -model model.ser.gz
</code></pre>

<p>Sample from dev.txt (The leading 4 represents polarity out of 5 ... 4/5 positive)</p>

<pre><code>(4 (4 (2 A) (4 (3 (3 warm) (2 ,)) (3 funny))) (3 (2 ,) (3 (4 (4 engaging) (2 film)) (2 .))))
</code></pre>

<p>Sample from test.txt</p>

<pre><code>(3 (3 (2 If) (3 (2 you) (3 (2 sometimes) (2 (2 like) (3 (2 to) (3 (3 (2 go) (2 (2 to) (2 (2 the) (2 movies)))) (3 (2 to) (3 (2 have) (4 fun))))))))) (2 (2 ,) (2 (2 Wasabi) (3 (3 (2 is) (2 (2 a) (2 (3 good) (2 (2 place) (2 (2 to) (2 start)))))) (2 .)))))
</code></pre>

<p>Sample from train.txt</p>

<pre><code>(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's)) (2 (3 new) (2 (2 ``) (2 Conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger)) (2 ,)) (2 (2 Jean-Claud) (2 (2 Van) (2 Damme)))) (2 or)) (2 (2 Steven) (2 Segal))))))))))))) (2 .)))
</code></pre>

<p>I have two questions going forward.</p>

<p>What is the significance and difference between each file? Train.txt/Dev.txt/Test.txt ?</p>

<p>How would I train my own model with a raw, unparsed text file full of tweets?</p>

<p>I'm very new to NLP so if I am missing any required information or anything at all please critique! Thank you!</p>
",Text Classification / Sentiment Analysis,train stanford nlp sentiment analysis tool hell everyone using stanford core nlp package goal perform sentiment analysis live stream tweet using sentiment analysis tool return poor analysis text attitude many positive labeled neutral many negative rated positive gone ahead acquired well million tweet text file clue actually train tool create model sample dev txt leading represents polarity positive sample test txt sample train txt two question going forward significance difference file train txt dev txt test txt would train model raw unparsed text file full tweet new nlp missing required information anything please critique thank
Automatic html data extraction using deeplearning,"<p>We are dealing with web pages, the objective is to let web crawler to extract data items/fields from them and put the data in a database table, automatically, without manually configure every html page to achieve that. We have enough training samples, we are trying to use deeplearning, there are several ways we come up with:</p>

<ol>
<li>end-to-end mapping from web page to structured data in database, I want to use question-answer or summation paradigm, but current papers on these subjects are using a paragraph of text as input, not html page.  Is there a deep learning kind of model fit to html situation?</li>
<li>break down the problem (to that deep learning model can handle): deal with the <code>&lt;td&gt;&lt;/td&gt;</code> tags separately, classify each tag into items/fields of database using some cnn or rnn text-classification model. Problem is there are possibly many tags contain the same class of information(company name, time, etc.), we can't know which one we want. Maybe we can combine some ""position"" features of the html, still it's not clearly how to define these features and how to merge these into the classification model to get a somehow end-to-end framework.</li>
<li>Is there some better way?</li>
</ol>
",Text Classification / Sentiment Analysis,automatic html data extraction using deeplearning dealing web page objective let web crawler extract data item field put data database table automatically without manually configure every html page achieve enough training sample trying use deeplearning several way come end end mapping web page structured data database want use question answer summation paradigm current paper subject using paragraph text input html page deep learning kind model fit html situation break problem deep learning model handle deal tag separately classify tag item field database using cnn rnn text classification model problem possibly many tag contain class information company name time etc know one want maybe combine position feature html still clearly define feature merge classification model get somehow end end framework better way
Sentiments Analysis Vs emotion Analysis,"<p>What is difference between sentiments (positive and negative) and emotions in text mining (NLP)? For example Anger is negative emotion as well as negative sentiment both seems the same.
 Vijay Nadadur, Creator of SentiRank, an algorithm which ranks sentiment in a text, Suggest Bio
Sentiments can be expressed mostly in binary format (+ve &amp; -ve) in a simplistic way. To add further, you may varying degrees of +ve and -ve sentiments, and perhaps neutral. However, emotions have multi-dimensions. Anger for sure is -ve sentiment, and so is sadness, but they aren't really the same. </p>

<p>To talk more specifically about text mining (NLP based), it's much simpler to do sentiment analysis but very hard to carry out emotional analysis. The next level of sentiment analysis is the field of intent analysis where few researchers have been working on mining out intent from the chunk of text, which seems of very high business value.</p>
",Text Classification / Sentiment Analysis,sentiment analysis v emotion analysis difference sentiment positive negative emotion text mining nlp example anger negative emotion well negative sentiment seems vijay nadadur creator sentirank algorithm rank sentiment text suggest bio sentiment expressed mostly binary format simplistic way add may varying degree sentiment perhaps neutral however emotion multi dimension anger sure sentiment sadness really talk specifically text mining nlp based much simpler sentiment analysis hard carry emotional analysis next level sentiment analysis field intent analysis researcher working mining intent chunk text seems high business value
Algorithms behind the Alchemy API for concept and keywords extraction,"<p>I've started using the alchemy API but I would like to know if
there is any scientific publication that explains the models used for extracting the keywords and the concepts from the text? </p>

<p>Also according to this answer <a href=""https://stackoverflow.com/questions/30954978/is-there-way-to-influence-alchemyapi-sentiment-analysis?rq=1"">Is there way to influence AlchemyAPI sentiment analysis</a>
the models used for  the alchemy Api were trained on billions of web pages. My question is on which type of data the algorithms were trained on
(only news content for example?).</p>

<p>Thank you in advance for the answers.</p>
",Text Classification / Sentiment Analysis,algorithm behind alchemy api concept keywords extraction started using alchemy api would like know scientific publication explains model used extracting keywords concept text also according answer href way influence alchemyapi sentiment analysis model used alchemy api trained billion web page question type data algorithm trained news content example thank advance answer
How can I compare Stanford Core NLP with an algorithm without the trained dataset for the movies?,"<p>I am looking to use the Stanford Core NLP which is trained for movie reviews. I want to compare it with a regular sentiment analysis algorithm which is untrained for movie reviews. Is there a way to use the Stanford Core NLP <a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">Link Here</a> without the trained dataset for the movie reviews or is there any other source I can use to compare directly? </p>
",Text Classification / Sentiment Analysis,compare stanford core nlp algorithm without trained dataset movie looking use stanford core nlp trained movie review want compare regular sentiment analysis algorithm untrained movie review way use stanford core nlp link without trained dataset movie review source use compare directly
Train classifier (natural - NLP) on node.js for unexpected sentences,"<p>Some context: Node.js, Bot, <a href=""https://github.com/NaturalNode/natural"" rel=""nofollow noreferrer"">natural module</a>.</p>
<p>I would like to build a Bot and I am using the natural module in order to parse and overall classify the user input.</p>
<pre><code>var classifier = new natural.BayesClassifier();
classifier.addDocument('Hi', 'welcome');
classifier.addDocument('Hello', 'welcome');
classifier.addDocument('Hey', 'welcome');
classifier.addDocument('Good', 'welcome');
...
//back to home
classifier.addDocument('go back to home', 'back2home');
classifier.addDocument('go back home', 'back2home');
classifier.addDocument('return',  'back2home');
classifier.addDocument('return to home', 'back2home');
...
classifier.train();
...
classifier.classify(text);
</code></pre>
<p>Those tests work fine:</p>
<pre><code>  &quot;I would like to go back home&quot; =&gt; back2home
  &quot;Hi&quot; =&gt; welcome
</code></pre>
<p>All good, but what if the user text contains something such as: &quot;bla bla bla&quot;, I want to get a way to know that the that the text not fits enough in any of the above cases. &quot;bla bla bla&quot; returns me =&gt; welcome, but actually i would like it return something such &quot;unknown&quot;/not understood.</p>
<p>It is a way to &quot;train&quot; the classifier in such a way?
Thanks.</p>
",Text Classification / Sentiment Analysis,train classifier natural nlp node j unexpected sentence context node j bot natural module would like build bot using natural module order parse overall classify user input test work fine good user text contains something bla bla bla want get way know text fit enough case bla bla bla return welcome actually would like return something unknown understood way train classifier way thanks
How to get data within data,"<p>I don't know if I'm gonna be able to explain this properly but here goes. I have a data frame called <code>ZCP</code> that includes tokenized tweets (gonna use them for sentiment analysis) and related metadata. The structure looks like this:</p>

<pre><code>head(ZAD)
num_tokens unique_tokens
1         12            12
2         11            10
3         11            10
4         12            12
5         22            20
6         11            10
text
1 rt, caradelevingne, fam, a, lam, glastonbury, https, t, co, h, ew, oux
2 rt, caradelevingne, home, sweet, home, glastonbury, https, t, co, zolld, ltvt
3 rt, caradelevingne, home, sweet, home, glastonbury, https, t, co, zolld, ltvt
4 rt, caradelevingne, fam, a, lam, glastonbury, https, t, co, h, ew, oux
5 rt, yahoocelebuk, adele, set, to, dominate, the, uk, albums, chart, as, heads, back, to, number, post, glastonbury, https, t, co, cndkufsgo, https
6 rt, caradelevingne, home, sweet, home, glastonbury, https, t, co, zolld, ltvt

favoriteCount                 id retweetCount isRetweet
1             0 747942553010397184          593      TRUE
2             0 747942530340118529          729      TRUE
3             0 747941795988905986          729      TRUE
4             0 747941781820542976          593      TRUE
5             0 747940287847161856            3      TRUE
6             0 747940084603838464          729      TRUE
</code></pre>

<p>Basically I'm only interested in the data in the text column for now. That data looks like this:</p>

<pre><code>head(ZCP$text) 
$to_return [1] ""saw"" ""viola"" ""beach"" ""support"" ""courteeners"" ""in""
[7] ""w"" ""ton"" ""to"" ""see"" ""coldplay"" ""do""
[13] ""the"" ""tribute"" ""to"" ""them"" ""at"" ""glastonbury"" [19] ""was"" ""amazing"" ""so"" ""well"" ""thought"" ""out""
[[2]] [1] ""glastonbury"" ""coldplay"" ""elo"" ""break"" ""viewing"" ""records""
[7] ""muse"" ""s"" ""audience"" ""doubles"" ""https"" ""t""
[13] ""co"" ""eocvqnoeen"" ""coldplay"" ""muse"" ""https"" ""t""
[19] ""co"" ""yd"" ""ie"" ""xr"" ""n""
[[3]] [1] ""another"" ""cheeky"" ""glastonbury"" ""pic"" ""coldplay"" ""pyramidstage""[7] ""https"" ""t"" ""co"" ""qttz"" ""xgjpx"" ""https""
[13] ""t"" ""co"" ""rm"" ""y"" ""pbvml""
[[4]] [1] ""i"" ""m"" ""having"" ""my"" ""very"" ""own""
[7] ""glastonbury"" ""tonight"" ""coldplay"" ""adele""
[[5]] [1] ""that"" ""was"" ""awesome"" ""coldplay"" ""glastonbury"" ""glasto""
[7] ""https"" ""t"" ""co"" ""fz"" ""ly"" ""cvx""
[[6]] [1] ""beegees"" ""barry"" ""gibb"" ""stayin"" ""alive"" ""and""
[7] ""coldplay"" ""en"" ""glastonbury"" ""https"" ""t"" ""co""
[13] ""hoj""
</code></pre>

<p>What operator should i use to get the individual tokens? I'm planning on writing a for loop but I can't the right operator to reach data within data. <code>ZCP$text[1]</code> gives me the below result:</p>

<pre><code>ZCP$text[1] $to_return [1] ""saw"" ""viola"" ""beach"" ""support"" ""courteeners"" ""in""
[7] ""w"" ""ton"" ""to"" ""see"" ""coldplay"" ""do""
[13] ""the"" ""tribute"" ""to"" ""them"" ""at"" ""glastonbury"" [19] ""was"" ""amazing"" ""so"" ""well"" ""thought"" ""out""
</code></pre>

<p>How can I get the first element of this object? I can't find the right operator to do this for some reason. Any help is appreciated. Thanks.</p>

<p>edit: @Sotos asked for a dput for this. Not sure if this is what he wanted (I'm a noob at R and never used dput before) but here it is for <code>head(ZCP)</code>:</p>

<pre><code>structure(list(num_tokens = structure(list(to_return = 24L, 23L, 
17L, 10L, 12L, 16L), .Names = c(""to_return"", """", """", """", 
"""", """")), unique_tokens = structure(list(to_return = 23L, 18L, 
14L, 10L, 12L, 16L), .Names = c(""to_return"", """", """", """", 
"""", """")), text = structure(list(to_return = c(""saw"", ""viola"", 
""beach"", ""support"", ""courteeners"", ""in"", ""w"", ""ton"", ""to"", ""see"", 
""coldplay"", ""do"", ""the"", ""tribute"", ""to"", ""them"", ""at"", ""glastonbury"", 
""was"", ""amazing"", ""so"", ""well"", ""thought"", ""out""), c(""glastonbury"", 
""coldplay"", ""elo"", ""break"", ""viewing"", ""records"", ""muse"", ""s"", 
""audience"", ""doubles"", ""https"", ""t"", ""co"", ""eocvqnoeen"", ""coldplay"", 
""muse"", ""https"", ""t"", ""co"", ""yd"", ""ie"", ""xr"", ""n""), c(""another"", 
""cheeky"", ""glastonbury"", ""pic"", ""coldplay"", ""pyramidstage"", ""https"", 
""t"", ""co"", ""qttz"", ""xgjpx"", ""https"", ""t"", ""co"", ""rm"", ""y"", ""pbvml""
), c(""i"", ""m"", ""having"", ""my"", ""very"", ""own"", ""glastonbury"", 
""tonight"", ""coldplay"", ""adele""), c(""that"", ""was"", ""awesome"", 
""coldplay"", ""glastonbury"", ""glasto"", ""https"", ""t"", ""co"", ""fz"", 
""ly"", ""cvx""), c(""beegees"", ""barry"", ""gibb"", ""stayin"", ""alive"", 
""and"", ""coldplay"", ""en"", ""glastonbury"", ""https"", ""t"", ""co"", ""hoj"", 
""u"", ""j"", ""yz"")), .Names = c(""to_return"", """", """", """", """", """")),
favoriteCount = structure(list(to_return = 2, 1, 0, 0, 0, 
1), .Names = c(""to_return"", """", """", """", """", """")), id = structure(list(
to_return = ""747938975621521408"", ""747938533290049537"", 
""747934687696420864"", ""747934531756384256"", ""747931753373892608"", 
""747928260835696640""), .Names = c(""to_return"", """", """", 
"""", """", """")), retweetCount = structure(list(to_return = 1, 
0, 0, 0, 0, 0), .Names = c(""to_return"", """", """", """", """", 
"""")), isRetweet = structure(list(to_return = FALSE, FALSE, 
FALSE, FALSE, FALSE, FALSE), .Names = c(""to_return"", 
"""", """", """", """", """"))), .Names = c(""num_tokens"", ""unique_tokens"", 
""text"", ""favoriteCount"", ""id"", ""retweetCount"", ""isRetweet""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>
",Text Classification / Sentiment Analysis,get data within data know gon na able explain properly go data frame called includes tokenized tweet gon na use sentiment analysis related metadata structure look like basically interested data text column data look like operator use get individual token planning writing loop right operator reach data within data give result get first element object find right operator reason help appreciated thanks edit sotos asked dput sure wanted noob r never used dput
nltk naivebayes classifier for text classification,"<p>In following code, I know that my naivebayes classifier is working correctly because it is working correctly on trainset1 but why is it not working on trainset2? I even tried it on two classifiers, one from TextBlob and other directly from nltk.</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob
from nltk.tokenize import word_tokenize
import nltk

trainset1 = [('I love this sandwich.', 'pos'),
('This is an amazing place!', 'pos'),
('I feel very good about these beers.', 'pos'),
('This is my best work.', 'pos'),
(""What an awesome view"", 'pos'),
('I do not like this restaurant', 'neg'),
('I am tired of this stuff.', 'neg'),
(""I can't deal with this"", 'neg'),
('He is my sworn enemy!', 'neg'),
('My boss is horrible.', 'neg')]

trainset2 = [('hide all brazil and everything plan limps to anniversary inflation plan initiallyis limping its first anniversary amid soaring prices', 'class1'),
         ('hello i was there and no one came', 'class2'),
         ('all negative terms like sad angry etc', 'class2')]

def nltk_naivebayes(trainset, test_sentence):
    all_words = set(word.lower() for passage in trainset for word in word_tokenize(passage[0]))
    t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in trainset]
    classifier = nltk.NaiveBayesClassifier.train(t)
    test_sent_features = {word.lower(): (word in word_tokenize(test_sentence.lower())) for word in all_words}
    return classifier.classify(test_sent_features)

def textblob_naivebayes(trainset, test_sentence):
    cl = NaiveBayesClassifier(trainset)
    blob = TextBlob(test_sentence,classifier=cl)
    return blob.classify() 

test_sentence1 = ""he is my horrible enemy""
test_sentence2 = ""inflation soaring limps to anniversary""

print nltk_naivebayes(trainset1, test_sentence1)
print nltk_naivebayes(trainset2, test_sentence2)
print textblob_naivebayes(trainset1, test_sentence1)
print textblob_naivebayes(trainset2, test_sentence2)
</code></pre>

<p>Output:</p>

<pre><code>neg
class2
neg
class2
</code></pre>

<p>Although test_sentence2 clearly belongs to class1.</p>
",Text Classification / Sentiment Analysis,nltk naivebayes classifier text classification following code know naivebayes classifier working correctly working correctly trainset working trainset even tried two classifier one textblob directly nltk output although test sentence clearly belongs class
Can I do Sentiment Analysis (POS Tagging etc) on Android?,"<p>I am working on an <code>Android</code> Application. I want to do sentiment analysis on client side. I tried to do it using <code>StanfordCoreNLP</code> but it does not work in Android but working fine in <code>Java</code> . Is there any method to do it? Will <code>NDK</code> work? Or some other guidelines? Any help will be appreciative. Thanks in advance.</p>
",Text Classification / Sentiment Analysis,sentiment analysis po tagging etc android working application want sentiment analysis client side tried using doe work android working fine method work guideline help appreciative thanks advance
Is it possible to use Feature Learning for Binary Text Classification?,"<p>I'm currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment (SLI) from those who are typically developing (TD).</p>

<p>In my readings I noticed that there really isn't a convincing set of features to distinguish the two that have been discovered yet, so I came upon the crazy idea of trying to create a feature learning algorithm that could potentially make better ones.</p>

<p>Is this possible? If so how do you suggest I approach this? From the reading I have done, most feature learning is done on image processing. Another problem is the dataset I have is potentially too small to make it work (in the 100's) unless I find a way to get more transcripts from children.</p>
",Text Classification / Sentiment Analysis,possible use feature learning binary text classification currently working childes corpus trying create classifier distinguishes child suffer specific language impairment sli typically developing td reading noticed really convincing set feature distinguish two discovered yet came upon crazy idea trying create feature learning algorithm could potentially make better one possible suggest approach reading done feature learning done image processing another problem dataset potentially small make work unless find way get transcript child
Evaluating the result of stanford nlp for sentiment analysis,"<p>I want to test few sentence using stanford NLP package and want to get sentiment result with it's score.</p>

<p>I tried in couple of ways. In few test I got partial result, like polarity of the text I gave. But not the sentiment score.</p>

<p>This is the command I executed: <code>H:\Drive E\Stanford\stanfor-corenlp-full-2013~&gt;java -cp ""*"" -mx1g  edu.stanford. nlp.sentiment.Evaluate edu/stanford/nlp/models/sentiment/sentiment.ser.gz test.txt</code></p>

<p>Gives result:</p>

<pre><code>EVALUATION SUMMARY
Tested 0 labels
  0 correct
  0 incorrect
  ? accuracy
Tested 0 roots
  0 correct
  0 incorrect
  ? accuracy
Label confusion matrix: rows are gold label, columns predicted label
         0         0         0         0         0
         0         0         0         0         0
         0         0         0         0         0
         0         0         0         0         0
         0         0         0         0         0
Root label confusion matrix: rows are gold label, columns predicted label
         0         0         0         0         0
         0         0         0         0         0
         0         0         0         0         0
         0         0         0         0         0
         0         0         0         0         0
Approximate negative label accuracy: ?
Approximate positive label accuracy: ?
Combined approximate label accuracy: ?
Approximate negative root label accuracy: ?
Approximate positive root label accuracy: ?
Combined approximate root label accuracy: ?
</code></pre>

<p><code>text.txt</code> contains </p>

<p>This movie doesn't care about cleverness, wit or any other kind of intelligent humor.
Those who find ugly meanings in beautiful things are corrupt without being charming.
There are slow and repetitive parts, but it has just enough spice to keep it interesting.</p>
",Text Classification / Sentiment Analysis,evaluating result stanford nlp sentiment analysis want test sentence using stanford nlp package want get sentiment result score tried couple way test got partial result like polarity text gave sentiment score command executed give result contains movie care cleverness wit kind intelligent humor find ugly meaning beautiful thing corrupt without charming slow repetitive part ha enough spice keep interesting
convolutional neural network text classification additional features,"<p>I am trying to implement CNN on text classification task. I understand that CNN can extract and abstract features from pure text. </p>

<p>What if I have some additional very useful features that not in the text? How should I add those features into the CNN?</p>

<p>Currently, what I am doing is concatenating the convolution layer results with an additional feature vector. And then feed them to the hidden layers. Is this a right way to do it?</p>

<p>Thanks!</p>
",Text Classification / Sentiment Analysis,convolutional neural network text classification additional feature trying implement cnn text classification task understand cnn extract abstract feature pure text additional useful feature text add feature cnn currently concatenating convolution layer result additional feature vector feed hidden layer right way thanks
Financial Slang and NLP for Sentiment Analysis,"<p>I am working on a Sentiment-Analysis/Opinion-Mining of Tweets, focused on Finance related tweets.</p>
<p>One of the biggest issue I'm facing is the unability of my algorithm to detect equivalent entities (Definition in B.Liu 2012: <a href=""https://www.cs.uic.edu/%7Eliub/FBS/SentimentAnalysis-and-OpinionMining.pdf"" rel=""nofollow noreferrer"" title=""Bing Liu 2012"">Page 18-19</a>) when Financial slang is used. For example, for those familiar with it I would like the following entities to be detected as equivalent after lemmatization :</p>
<ul>
<li>Government-Bonds = Govies = Sovereign-Debt</li>
<li>Cash = Monetary</li>
<li>Stocks = Equities</li>
<li>FX = Forex = Currency-exchange = Foreign-Exchange</li>
<li>Bund = German-Bonds = Bundesbank 10y</li>
<li>T-Notes = US10 = Treasury-Notes = US-Govies = American-Sovereign-Debt</li>
<li>Etc...</li>
</ul>
<p>Here are my two questions :</p>
<ol>
<li>I was thinking about using some supervised learning (Naive-Bayesian-Classification) for such task, but can't find any classified set of data for training. Do you know if such dataset exists?</li>
<li>Do you have any alternative idea regarding how to perform such task?</li>
</ol>
<p>Thanks.</p>
",Text Classification / Sentiment Analysis,financial slang nlp sentiment analysis working sentiment analysis opinion mining tweet focused finance related tweet one biggest issue facing unability algorithm detect equivalent entity definition b liu page financial slang used example familiar would like following entity detected equivalent lemmatization government bond govies sovereign debt cash monetary stock equity fx forex currency exchange foreign exchange bund german bond bundesbank note u treasury note u govies american sovereign debt etc two question wa thinking using supervised learning naive bayesian classification task find classified set data training know dataset exists alternative idea regarding perform task thanks
Is there any situation the TF-IDF is worse that using term-frequency vectors?,"<p>I am doing text classification now. Is there any situation the TF-IDF is worse that using term-frequency vectors? How to explain it?
Thanks </p>
",Text Classification / Sentiment Analysis,situation tf idf worse using term frequency vector text classification situation tf idf worse using term frequency vector explain thanks
Sentiment Analysis for local languages (Nepali),"<p>I would like to do sentiment analysis on document level. But I am try to do sentiment analysis Nepali. So, I dont have any resources. I can't do Naive Bayes Classifier as I don't have any labelled data and I can't do vai wordnet as no nepali wordnet exist. Papers I read generally had labelled data or senti-wordnet for other languages. </p>

<p>I would like know these things:</p>

<ul>
<li>Which approach should I use in above case for sentiment analysis?</li>
<li>Is there any method for me to dynamically generate labels for data?</li>
</ul>
",Text Classification / Sentiment Analysis,sentiment analysis local language nepali would like sentiment analysis document level try sentiment analysis nepali dont resource naive bayes classifier labelled data vai wordnet nepali wordnet exist paper read generally labelled data sent wordnet language would like know thing approach use case sentiment analysis method dynamically generate label data
Defining vocabulary size in text classification,"<p>I have a question regarding the defining of vocabulary set needed for feature extraction in text classification.
 In an experiment, there are two approaches I can think of:</p>

<p>1.Define vocabulary size using both training data and test data, so that no word from the test data would be treated as being 'unknown' during the testing.</p>

<p>2.Define vocabulary size according to data only from the training data, and treat every word in the testing data that does not also appear in the training data as 'unknown'.</p>

<p>At first glance the more scientific way is the second one. However it is worth noticing that although there is no way we can know about the true size of vocabulary in a practical system, there seems to be no problem to set the vocabulary size a little bit larger than the size appeared in the training data in order to cover potentially larger problems. This is helpful in that it actually treats different unknown words as being different, instead of summing them up as 'unknown'. Is there any reason why this is not practical?</p>

<p>New to machine learning. Help much appreciated. </p>
",Text Classification / Sentiment Analysis,defining vocabulary size text classification question regarding defining vocabulary set needed feature extraction text classification experiment two approach think define vocabulary size using training data test data word test data would treated unknown testing define vocabulary size according data training data treat every word testing data doe also appear training data unknown first glance scientific way second one however worth noticing although way know true size vocabulary practical system seems problem set vocabulary size little bit larger size appeared training data order cover potentially larger problem helpful actually treat different unknown word different instead summing unknown reason practical new machine learning help much appreciated
Trainable Classifiers in NLTK and Sklearn for Sentiment Analysis,"<p>I have followed a tutorial to make a basic trainable sentiment analysis program using mostly nltk and some sklearn, however, I am only using 5 classifiers (and then using a voting system): LogisticRegression, BernoulliNB, MultinomialNB, LinearSVC, and NuSVC. I would like to expand upon this program, however, I don't know where to go. <strong>If I tune these 5 classifiers, can that create a good sentiment analysis program? If not, what other trainable classifiers are there (I've tried training MaxEnt but I don't think that it will give me the results I am looking for and it takes forever to train)?</strong></p>
",Text Classification / Sentiment Analysis,trainable classifier nltk sklearn sentiment analysis followed tutorial make basic trainable sentiment analysis program using mostly nltk sklearn however using classifier using voting system logisticregression bernoullinb multinomialnb linearsvc nusvc would like expand upon program however know go tune classifier create good sentiment analysis program trainable classifier tried training maxent think give result looking take forever train
Multiple negative constraints do not work in JAPE,"<p>I am trying to develop jape rules to classify text based on the previously created tags on it.</p>

<p>Then I create these rules:</p>

<pre><code>//Prediction A
Rule: A_Predictor
(
    {RECORD contains {Indicator.rule == A}}
): predict_A
--&gt;
:predict_A.Prediction = {prediction = A}


//Prediction B
Rule: B_Predictor
(
    {RECORD contains {Indicator.rule == B}, !RECORD contains {Indicator.rule == A}}
): predict_B
--&gt;
:predict_B.Prediction = {prediction = B}

//Prediction C
Rule: C_Predictor
(
    {RECORD contains {Indicator.rule == C}, !RECORD contains {Indicator.rule == A}, !RECORD contains {Indicator.rule == B}}
): predict_C
--&gt;
:predict_C.Prediction = {prediction = C}
</code></pre>

<p>As you can see that I have multiple conditions to match for each <code>prediction</code>, especially to predict B and C. But, these rules still give prediction <code>B</code> even though there are <code>Indicator.rule == a</code> in the <code>RECORD</code>, which I assume that I have already negate it in the rule above.</p>

<p>What's wrong with my code?</p>

<p>Any helps will be appreciated.</p>

<p>Thanks :)</p>
",Text Classification / Sentiment Analysis,multiple negative constraint work jape trying develop jape rule classify text based previously created tag create rule see multiple condition match especially predict b c rule still give prediction even though assume already negate rule wrong code help appreciated thanks
naive bayes classifier: does the size of corpus for each category have to be the same?,"<p>I am building a Naive Bayes classifier for two categories, pos and neg. I want the classifier to classify a sentence as pos if it contains certain words and as neg if it doesn't contain those words.</p>

<p>My corpus for pos are 518 sentences that contain those words. My question: How big does the corpus for neg have to be?</p>

<p>In the movie_reviews corpus of NLTK, both categories contain the same amount of text files. However, that classifier also learns the negative words, doesn't it? I really only care that the classifier recognizes certain words belongig to the pos category, I don't care about the words in the neg category.</p>

<p>So, is it important that both corpora contain 518 text files?</p>
",Text Classification / Sentiment Analysis,naive bayes classifier doe size corpus category building naive bayes classifier two category po neg want classifier classify sentence po contains certain word neg contain word corpus po sentence contain word question big doe corpus neg movie review corpus nltk category contain amount text file however classifier also learns negative word really care classifier recognizes certain word belongig po category care word neg category important corpus contain text file
Google Prediction API for FAQ/Recommendation system,"<p>I want to build automated FAQ system where user can ask some questions and based on the questions and their answers from the training data, the application would suggest set of answers. </p>

<p>Can this be achieved via Prediction API?
If yes, how should I create my training data?</p>

<p>I have tested Prediction API for sentiment analysis. But having doubts and confusion on using it as FAQ/Recommendation system.</p>

<p>My training data has following structure:<br><br>
""Question"":""How to create email account?""<br>
""Answer"":""Step1: xxxxxxxx Step2: xxxxxxxxxxxxx Step3: xxxxx xxx xxxxx""<br>
""Question"":""Who can view my contact list?""<br>
""Answer"":""xxxxxx xxxx xxxxxxxxxxxx x  xxxxx xxx""</p>
",Text Classification / Sentiment Analysis,google prediction api faq recommendation system want build automated faq system user ask question based question answer training data application would suggest set answer achieved via prediction api yes create training data tested prediction api sentiment analysis doubt confusion using faq recommendation system training data ha following structure question create email account answer step xxxxxxxx step xxxxxxxxxxxxx step xxxxx xxx xxxxx question view contact list answer xxxxxx xxxx xxxxxxxxxxxx x xxxxx xxx
Using Brown corpus for text classification NLTK,"<p>I am trying to use brown corpus genres as a task of classification but I am obtaining very low accuracy scores. I trying different features for examples the frequency of stopwords. Could you check if I am doing it right or is there a problem with my code? Any suggestion is appreciated.</p>

<pre><code>from collections import defaultdict
from nltk.corpus import brown,stopwords
import random
import nltk

dataset = [] # 500 samples

for category in brown.categories():
    for fileid in brown.fileids(category):
        dataset.append((brown.words(fileids = fileid),category))

dataset = [([w.lower() for w in text],category) for text,category in dataset]

def feature_extractor(text,bag):
    # bag -&gt; bag of words
    frec = defaultdict(int)
    for word in text:
        if word in bag:
            frec[word] += 1

    return frec

# training &amp; test 90%-10% naivebayes nltk

def train_and_test(featureset,n=90):

    random.shuffle(featureset)
    split = int((len(featureset)*n)/100)
    train,test = featureset[:split],featureset[split:]
    classifier = nltk.NaiveBayesClassifier.train(train)
    accuracy= nltk.classify.accuracy(classifier, test)
    return accuracy

# Stopwords as features
stopwords = stopwords.words(""english"") # 153 words

featureset = [(feature_extractor(text,stopwords),category)for text,category in dataset]

print(""Accuracy: "",train_and_test(featureset)) # around 0.25
</code></pre>
",Text Classification / Sentiment Analysis,using brown corpus text classification nltk trying use brown corpus genre task classification obtaining low accuracy score trying different feature example frequency stopwords could check right problem code suggestion appreciated
How to use Stanford Sentiment Analysis Dataset,"<p>I am trying to use Stanford Sentiment Analysis Dataset to do some sentiment analysis research.  I download the dataset <a href=""http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip"" rel=""nofollow"">enter link description here</a>
from <a href=""http://nlp.stanford.edu/sentiment/index.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/index.html</a> . After reading the readme file, I still have some confusion. </p>

<p>First question, In ""50446"" line of dictionary.txt file, it shows the  ""<strong>phrase ids</strong>"" of this sentence is ""<strong>No.226166</strong>"", So when I search in sentiment_lable.txt file, I find in the ""226168"" line, the ""<strong>sentiment values</strong>"" of phrase ""<strong>No.226166</strong>"" is <strong>0.69444</strong>. But In ""50445"" line of <strong>dictionary.txt file</strong>, this sentence is equal to the sentence in  ""50446"" line. But this sentence has different ""sentiment values"" in the sentiment_lable.txt file, why?!!!</p>

<p>Second question, In some sentiment analysis paper, they not only use full-length sentence in training sentence to train model, but also use  <strong>labelled phrases that occur as subparts of the training sentences</strong> to train model. But I find some phrase in  <strong>dictionary.txt</strong> file such as line 2 and 3 which are useless, should I use these useless phrases to train my model?</p>
",Text Classification / Sentiment Analysis,use stanford sentiment analysis dataset trying use stanford sentiment analysis dataset sentiment analysis research download dataset enter link description reading readme file still confusion first question line dictionary txt file show phrase id sentence search sentiment lable txt file find line sentiment value phrase line dictionary txt file sentence equal sentence line sentence ha different sentiment value sentiment lable txt file second question sentiment analysis paper use full length sentence training sentence train model also use labelled phrase occur subpart training sentence train model find phrase dictionary txt file line useless use useless phrase train model
Looking for sentiment model for twitter using Stanford NLP,"<p>Is there any sentiment model for twitter ?  I am using Stanford NLP for sentiment analysis, but it always give negative result if there are any ungrammatical issue on sentence. It might caused by the default trained model which is based on movie reviews. So I need a well trained sentiment model for unstructured or ungrammatical sentence like those in twitter, to which I can set the ""sentiment.model'like this:</p>

<pre><code> Properties props = new Properties();
 props.setProperty(""annotators"", ""tokenize, ssplit, parse,sentiment"");       
 props.put(""sentiment.model"", ""sentiment.ser.gz"");       
 pipeline = new StanfordCoreNLP(props);
</code></pre>
",Text Classification / Sentiment Analysis,looking sentiment model twitter using stanford nlp sentiment model twitter using stanford nlp sentiment analysis always give negative result ungrammatical issue sentence might caused default trained model based movie review need well trained sentiment model unstructured ungrammatical sentence like twitter set sentiment model like
Text Categorization In R for single paragraph,"<p>I have been searching for a solution/library or any function that performs text categorization of a single paragraph without any training involved in R. I need to categorize/classify contact center call data individually. The calls need to be categorized according to the terms used by the agent or caller. The terms may not be consecutive, and so it doesn't follow bigram. </p>

<p>For example, the following sample text should be categorized something like ""Router Internet issues""</p>

<p>""Hello thank you for calling XYZ solutions. This is Mark. How can I help you?
Hi, I have been facing issues in connecting to internet. There seems to be some issue with my router. ""</p>

<p>I have tried OpenNLP, RTextTools libraries in R, but could not figure out how to process a single paragraph. Does anyone have any ideas? Any help is appreciated.</p>

<p><strong>Edited</strong>
<em>As I am a beginner in R so would much appreciate a thorough solution if possible</em></p>
",Text Classification / Sentiment Analysis,text categorization r single paragraph searching solution library function performs text categorization single paragraph without training involved r need categorize classify contact center call data individually call need categorized according term used agent caller term may consecutive follow bigram example following sample text categorized something like router internet issue hello thank calling xyz solution mark help hi facing issue connecting internet seems issue router tried opennlp rtexttools library r could figure process single paragraph doe anyone idea help appreciated edited beginner r would much appreciate thorough solution possible
Apache Spark: Detect buying intent in a sentence,"<p>I have an Apache Spark application, written in scala, which does some basic processing of the input data received. Input data are sentences from a text file. I need to classify the ones with <strong>Buying</strong> intention. For example, if the sentence goes like : ""<em>Where can I buy a blue shoe?</em>"", then this counts as one.</p>

<p>What classifier/ ML program can I use to build this? A scala/java tool would be suitable. I am new to NLP/ML. </p>

<p>Any help appreciated.</p>
",Text Classification / Sentiment Analysis,apache spark detect buying intent sentence apache spark application written scala doe basic processing input data received input data sentence text file need classify one buying intention example sentence go like buy blue shoe count one classifier ml program use build scala java tool would suitable new nlp ml help appreciated
Classifying to three classes using a single output,"<p>I'm using LSTM for sentiment classification and I have three optional classes - negative/positive/neutral.</p>

<p>I wonder if there's a way to do this classification using a single output that will be in the range of -1:1 while -1 is the neutral class, 0 is the negative one and 1 is the positive class.</p>

<p>I know that <em>sigmoid</em> function goes from 0 to 1 and <em>tanh</em> from -1 to 1 so working with <em>tanh</em> is probably a good lead, but still does it make any sense using single output to classify to three different classes?</p>
",Text Classification / Sentiment Analysis,classifying three class using single output using lstm sentiment classification three optional class negative positive neutral wonder way classification using single output range neutral class negative one positive class know sigmoid function go tanh working tanh probably good lead still doe make sense using single output classify three different class
Does the input for LSTM in NLP need to be the same length?,"<p>What is the input for the problem of text sentiment classification using LSTM?  </p>

<p>Does each sentence need to be the same lenght? Or no?</p>

<p>Can anyone explain it step by step using two sentence example?</p>

<p>Example:</p>

<p>a. I like this book.
b. I do not like this book.</p>
",Text Classification / Sentiment Analysis,doe input lstm nlp need length input problem text sentiment classification using lstm doe sentence need lenght anyone explain step step using two sentence example example like book b like book
Which type of neural network is good for text classification(extractive summary),"<p>Currently i am working on text summarization.I am planning to use neural network to generate extractive summary of given text. Which type of neural network is good for text classification.  </p>
",Text Classification / Sentiment Analysis,type neural network good text classification extractive summary currently working text summarization planning use neural network generate extractive summary given text type neural network good text classification
How to prepare feature vectors for text classification when the words in the text is not frequently repeating?,"<p>I need to perform the text classification on set of emails. But all the words in my text are thinly sparse i.e frequency of each word with respect to all the documents are very less. words are not that much frequently repeating. Since to train the classifiers I think document term matrix with frequency as weightage is not suitable. Can you please suggest me what kind of other methods I need to use . </p>

<p>Thanks</p>
",Text Classification / Sentiment Analysis,prepare feature vector text classification word text frequently repeating need perform text classification set email word text thinly sparse e frequency word respect document le word much frequently repeating since train classifier think document term matrix frequency weightage suitable please suggest kind method need use thanks
What is an appropriate training set size for sentiment analysis?,"<p>I'm looking to use some tweets about measles/ the mmr vaccine to see how sentiment about vaccination changes over time.  I plan on creating the training set from the corpus of data I currently have (unless someone has a recommendation on where I can get similar data).</p>

<p>I would like to classify a tweet as either: Pro-vaccine, Anti-Vaccine, or Neither (these would be factual tweets about outbreaks).</p>

<p>So the question is: How big is big enough?  I want to avoid problems of overfitting (so I'll do a test train split) but as I include more and more tweets, the number of features needing to be learned increases dramatically.</p>

<p>I was thinking 1000 tweets (333 of each).  Any input is appreciated here, and if you could recommend some resources, that would be great too.</p>
",Text Classification / Sentiment Analysis,appropriate training set size sentiment analysis looking use tweet measles mmr vaccine see sentiment vaccination change time plan creating training set corpus data currently unless someone ha recommendation get similar data would like classify tweet either pro vaccine anti vaccine neither would factual tweet outbreak question big big enough want avoid problem overfitting test train split include tweet number feature needing learned increase dramatically wa thinking tweet input appreciated could recommend resource would great
Text classification precision and recall,"<p>Basically I had to do an application for classifying documents based on the part of the speech of the vocabulary of the words. The algorithm that was used for learning the classification problem was ready made and handed over for me. 
Based on the examples I got, I need to interpret these results( precision, recall, accuracy). Can someone say his opinion if these results are good or not?</p>

<pre><code>accuracy = 0.91 (true positive + true negative)/all
f-measure = 0.34
precision = 0.45
recall = 0.33
negative rate = 0.92
</code></pre>
",Text Classification / Sentiment Analysis,text classification precision recall basically application classifying document based part speech vocabulary word algorithm wa used learning classification problem wa ready made handed based example got need interpret result precision recall accuracy someone say opinion result good
Best Algorithmic Approach to Sentiment Analysis,"<p>My requirement is taking in news articles and determining if they are positive or negative about a subject.  I am taking the approach outlined below, but I keep reading NLP may be of use here.  All that I have read has pointed at NLP detecting opinion from fact, which I don't think would matter much in my case. I'm wondering two things:</p>

<p>1)  Why wouldn't my algorithm work and/or how can I improve it? ( I know sarcasm would probably be a pitfall, but again I don't see that occurring much in the type of news we will be getting)</p>

<p>2)  How would NLP help, why should I use it?</p>

<p>My algorithmic approach (I have dictionaries of positive, negative, and negation words):</p>

<p>1) Count number of positive and negative words in article</p>

<p>2) If a negation word is found with 2 or 3 words of the positive or negative word, (ie: NOT the best) negate the score.</p>

<p>3) Multiply the scores by weights that have been manually assigned to each word. (1.0 to start)</p>

<p>4) Add up the totals for positive and negative to get the sentiment score.</p>
",Text Classification / Sentiment Analysis,best algorithmic approach sentiment analysis requirement taking news article determining positive negative subject taking approach outlined keep reading nlp may use read ha pointed nlp detecting opinion fact think would matter much case wondering two thing algorithm work improve know sarcasm would probably pitfall see occurring much type news getting would nlp help use algorithmic approach dictionary positive negative negation word count number positive negative word article negation word found word positive negative word ie best negate score multiply score weight manually assigned word start add total positive negative get sentiment score
Supervised Learning Approach for Aspect Extraction,"<p>I'm developing an aspect-level sentiment analysis project for online travel reviews of travel domain.</p>

<p>I have a human annotated dataset that has labelled aspect terms, and aspect categories along with their sentiment polarity.</p>

<p>For example;</p>

<pre><code>Sentence:

This beach was a wonderful time for a day party  it had a fun crowd and has a big bar with a great atmosphere. The food was delicious too.
</code></pre>

<p>The above sentence has the following <strong>aspect terms</strong> labelled;</p>

<pre><code>{party#positive C} {crowd#positive C} {bar#positive C} {food#positive C}
</code></pre>

<p>And the following <strong>aspect categories</strong>;</p>

<pre><code>{entertainment#positive C} {accommodation#positive C}
</code></pre>

<p>I want to try a <code>supervised learning</code> approach to train a model to classify aspect terms from sentences. </p>

<p>I'm using <code>Stanford CORENLP</code> library. But confused as to how the training data format should be? and what is the best approach to take.</p>

<p>I have seen people using <code>IOB notation</code> to format training data to train <code>NER</code> systems. Can I use a similar method to get this done? As in, how do I format my training data file to get aspect terms as mentioned above from an input sentence?</p>

<p>If someone can point me in the right direction, I would appreciate that a lot.</p>
",Text Classification / Sentiment Analysis,supervised learning approach aspect extraction developing aspect level sentiment analysis project online travel review travel domain human annotated dataset ha labelled aspect term aspect category along sentiment polarity example sentence ha following aspect term labelled following aspect category want try approach train model classify aspect term sentence using library confused training data format best approach take seen people using format training data train system use similar method get done format training data file get aspect term mentioned input sentence someone point right direction would appreciate lot
Is this a use case for nlp?,"<p>I have a list of problems and resolutions. The current search for existing problems is just a keyword search. To improve searching for existing solutions to problems : 
Classify documents based on their semantic meaning using nlp. User enters a search term and documents that closely match this search are displayed alongside possible solutions.</p>

<p>The search will be based on semantic meaning, this is use case for nlp ?</p>
",Text Classification / Sentiment Analysis,use case nlp list problem resolution current search existing problem keyword search improve searching existing solution problem classify document based semantic meaning using nlp user enters search term document closely match search displayed alongside possible solution search based semantic meaning use case nlp
What classification algorithm select for text categorisation,"<p>I have 5000 articles on the subject I need they all categorised and grabbed as plain text.</p>

<p>I need to train a model so it was classify to relate it to my topic or not.</p>

<p>If I got example form scikit-learn form 20newsgroups.
In example i have Pipeline with SGDClassifier:</p>

<pre><code>text_clf = Pipeline([('vect', CountVectorizer()),
                 ('tfidf', TfidfTransformer()),
                 ('clf', SGDClassifier(loss='hinge', penalty='l2',
                                       alpha=1e-3, n_iter=5,  random_state=42)),])
</code></pre>

<p>and next i train my classifier send to it array of articles and array of categories but I have only one category and if i send array [1,1,1,1,1,1......1] it's have no effect.</p>

<p>What way to learn I should select in this case?
What classifier and how I need learn it and did I need other text normalisation or something else before train model </p>
",Text Classification / Sentiment Analysis,classification algorithm select text categorisation article subject need categorised grabbed plain text need train model wa classify relate topic got example form scikit learn form newsgroups example pipeline sgdclassifier next train classifier send array article array category one category send array effect way learn select case classifier need learn need text normalisation something else train model
Graphlab: How to avoid manually duplicating functions that has only a different string variable?,"<p>I imported my dataset with SFrame:</p>

<pre><code>products = graphlab.SFrame('amazon_baby.gl')
products['word_count'] = graphlab.text_analytics.count_words(products['review'])
</code></pre>

<p>I would like to do sentiment analysis on a set of words shown below:</p>

<pre><code>selected_words = ['awesome', 'great', 'fantastic', 'amazing', 'love', 'horrible', 'bad', 'terrible', 'awful', 'wow', 'hate']
</code></pre>

<p>Then I would like to create a new column for each of the selected words in the products matrix and the entry is the number of times such word occurs, so I created a function for the word ""awesome"":</p>

<pre><code>def awesome_count(word_count):
    if 'awesome' in product:
        return product['awesome']
    else:
        return 0;

products['awesome'] = products['word_count'].apply(awesome_count)
</code></pre>

<p>so far so good, but I need to manually create other functions for each of the selected words in this way, e.g., great_count, etc. How to avoid this manual effort and write cleaner code?</p>
",Text Classification / Sentiment Analysis,graphlab avoid manually duplicating function ha different string variable imported dataset sframe would like sentiment analysis set word shown would like create new column selected word product matrix entry number time word occurs created function word awesome far good need manually create function selected word way e g great count etc avoid manual effort write cleaner code
Sentiment Analysis - What does annotating dataset mean?,"<p>I'm currently working on my final year research project, which is an application which analyzes travel reviews found online, and give out a sentiment score for particular tourist attractions as a result, by conducting aspect level sentiment analysis.</p>

<p>I have a newly scraped dataset from a famous travel website which does not allow to use their API for research/academic purposes. (bummer)</p>

<p>My supervisor said that I might need to get this dataset annotated before using it for the aforementioned purpose. I am kind of confused as to what data annotation means in this context. Could someone please explain what exactly is happening when a dataset is annotated and how it helps in getting sentiment analysis done?</p>

<p>I was told that I might have to get two/three human annotators and get the data annotated to make it less biased. I'm on a tight schedule and I was wondering if there are any tools that can get it done for me? If so, what will be the impact of using such tools over human annotators? I would also like suggestions for such tools that you would recommend.</p>

<p>I would really appreciate a detailed explanation to my questions, as I am stuck with my project progressing to the next step because of this.</p>

<p>Thank you in advance.</p>
",Text Classification / Sentiment Analysis,sentiment analysis doe annotating dataset mean currently working final year research project application analyzes travel review found online give sentiment score particular tourist attraction result conducting aspect level sentiment analysis newly scraped dataset famous travel website doe allow use api research academic purpose bummer supervisor said might need get dataset annotated using aforementioned purpose kind confused data annotation mean context could someone please explain exactly happening dataset annotated help getting sentiment analysis done wa told might get two three human annotator get data annotated make le biased tight schedule wa wondering tool get done impact using tool human annotator would also like suggestion tool would recommend would really appreciate detailed explanation question stuck project progressing next step thank advance
t-SNE High Dimension Data Visualisation,"<p>I have a twitter corpus which I am using to build sentiment analysis application. The corpus has 5k tweets which have been hand labelled as - negative, neutral or positive</p>

<p>To represent the text - I am using gensim word2vec pretrained vectors. Each word is mapped to 300 dimensions. For a tweet, I add all the word vectors to get a single 300 dim vectors. Thus every tweet is mapped to a single vector of 300 dimension. </p>

<p>I am visualizing my data using t-SNE (tsne python package). See attached image <a href=""https://i.sstatic.net/3E0Qf.jpg"" rel=""nofollow noreferrer"">1</a> - Red points = negative tweets, Blue points = neutral tweets and Green points = Positive tweets</p>

<p><a href=""https://i.sstatic.net/3E0Qf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3E0Qf.jpg"" alt=""tweets represented using word2vec""></a>   </p>

<p><strong>Question:</strong>
In the plot there no clear separation (boundary) among the data points. Can I assume this will also be the case with the original points in 300 Dimensions ?</p>

<p>i.e if points overlap in t-SNE graph then they also overlap in original space and vice-versa ?</p>
",Text Classification / Sentiment Analysis,sne high dimension data visualisation twitter corpus using build sentiment analysis application corpus ha k tweet hand labelled negative neutral positive represent text using gensim word vec pretrained vector word mapped dimension tweet add word vector get single dim vector thus every tweet mapped single vector dimension visualizing data using sne tsne python package see attached image red point negative tweet blue point neutral tweet green point positive tweet question plot clear separation boundary among data point assume also case original point dimension e point overlap sne graph also overlap original space vice versa
how to combine and feed different features to an algorithm for text classification,"<p>Ive got some 120k text files, and 12 categories in which I want to classify these documents into.
Im using simple bag of words model and feeding it to NaiveBayes. But I was told that using a mixture of features would ""help"" OR rather I should atleast try. For instance :-</p>

<pre><code>1.] POS tags + Bigrams, 
2.] Bag-of-NER + POS tags 
</code></pre>

<p>But the problem is how do I combine these two /three different features as a single feature for each of the document ?
Secondly which ""feature-mixture"" is the best to help in document classification?</p>
",Text Classification / Sentiment Analysis,combine feed different feature algorithm text classification ive got k text file category want classify document im using simple bag word model feeding naivebayes wa told using mixture feature would help rather atleast try instance problem combine two three different feature single feature document secondly feature mixture best help document classification
improving prediction in sklearn,"<p>I am looking for some guidance. I am new to NLP. I can find my way round python ok and got some feature extractors coded. What I want to do is being able to predict emotion type such as happy, sad etc using sklearn. To this end I created a feature extractor that extracts several features like starting POS, ending POS, number of punctuations, number of WH__ words etc. It creates an array of these features and I am trying to find a way to use this data for supervised learning. I already went over all the 800+ sentenses and have targets to answer the sentences. Then I am giving the first 750 arrays and the answer array to sklearn and then try to predict on the last 50. sklearn predicts but the predictions are really poor. </p>

<p>Sure it is my problem than the sklearn. I am looking for some guidance to help me through in terms of perhaps right feature suggestions, suggestions re: normalizing the data, and in general any guidance really. I will give examples below.</p>

<p>Text file contains one sentence per line, feature extractor loads each sentence and gives out a feature list as an array.
At this point data looks like </p>

<p>features array of arrays :</p>

<pre><code>    setx=[[1, 0, 5, 12, 5, 13, .... -1, 0, 0, -1, 0, -1, 0, -1, 17, 11, 0],..... many more arrays like this]
</code></pre>

<p>targets array:
        sety=[0, 0, 0, 0, 0, 0, 0, ..... 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]</p>

<p>Then I am using :</p>

<pre><code>    clf=svm.SVC(gamma=0.001,C=10)
    clf.fit(setx,sety)
</code></pre>

<p>The tstx and tsty below are the last 50 of the total feature arrays and targets array</p>

<pre><code>    count=0
    for n in tstx:
            print clf.predict(n),',',tsty[count]
            count=count+1
            print ""-----""
</code></pre>

<p>Any suggestions on improving the predictions! I am pretty sure I am missing holes the size of Texas somewhere :-)</p>

<p>Thanks a bunch</p>
",Text Classification / Sentiment Analysis,improving prediction sklearn looking guidance new nlp find way round python ok got feature extractor coded want able predict emotion type happy sad etc using sklearn end created feature extractor extract several feature like starting po ending po number punctuation number wh word etc creates array feature trying find way use data supervised learning already went sentenses target answer sentence giving first array answer array sklearn try predict last sklearn predicts prediction really poor sure problem sklearn looking guidance help term perhaps right feature suggestion suggestion normalizing data general guidance really give example text file contains one sentence per line feature extractor load sentence give feature list array point data look like feature array array target array sety using tstx tsty last total feature array target array suggestion improving prediction pretty sure missing hole size texas somewhere thanks bunch
perceptron classifying,"<p>I have a folder that contains 7 sub folder and each sub folder contains 8 files. Generally I have 56 files for the train set. For the test set, I have a folder that contains 7 sub folders and each sub folder contains 2 files (generally 14 files for the test set). I have another file which contains 1000 most common words of the train set. I have to check if these 1000 words are in the train set or no. If they exist there, it should return +1, else it should return -1 to make a vector.Then I have to classify the texts with bipolar perceptron (neural network). The threshold is, 0.1 and the learning rate is, 0.5. The part after assigning the weights doesn't work well. how can I change the code?</p>

<pre><code>import os
file=""c:/python34/1000CommonWords.txt""
folder_path=""c:/python34/train""
def vector (folder_path, file):
    t=[]   
    vector=[]
    vector2=[]
    m=[]

    for folder in sorted(os.listdir(folder_path)):
        folder_path1 = os.path.join(folder_path, folder)
        for folder1 in sorted(os.listdir(folder_path1)):
            file=os.path.join(folder_path1, folder1)
            tex = open(file,encoding=""utf-8"") 
            tex=tex.read().split()
            t=tex+t






        with open (file, encoding=""utf-8"") as f1:
            f1=f1.read().split()


            for c in t:      # to make the [1, -1] vector
               for i in c:
                    for j in f1:
                        if j in i:
                            m.append (+1)
                        else:
                            m.append (-1)
                    vector.append(m)
                    vector2.append(vector)
                    #return vector2

                    w=[[0 for row in range(len(vector2[0][0] ))] for clmn in range(7)]   # weights
                    b=[0 for wb in range(7)]   # bias
                    l=0
                    while l&lt;=1000:
                        w_old=w[:]
                        b_old=b[:]
                        for Class in vector2:
                            for text in Class:
                                node=0
                                while node&lt;7:
                                    i=0
                                    y_in=0
                                    while i&lt;len(text):
                                        y_in= text[i]*w[node][i]
                                        i+=1
                                        y_in=b[node]+y_in
                                        if y_in&lt;=-0.1:  # activatin function
                                            y=-1
                                        elif (y_in &lt;=0.1 and y_in&gt;=-0.1):
                                            y=0
                                        else:
                                            y=1

                                        if node==vector2.index(Class):
                                            target=1  # assign target
                                        else:
                                            target=-1

                                        if target!=y:
                                            for j in range(0,len(w[0])): # update weights
                                                w[node][j]=w[nod][j]+0.5*text[j]*target
                                                b[node]=b[node]+0.5*target
                                              #  print(len(w)) 
                                               # print(len(w[0]))
                                                node+=1
                                                l+=1
                                                print (w)
                                                print(b)
</code></pre>

<p>the folders name:(the language is persian)</p>

<p>['اجتماعی', 'اديان', 'اقتصادی', 'سیاسی', 'فناوري', 'مسائل راهبردي ايران', 'ورزشی']</p>

<p>Each folder contains these files:</p>

<p>['13810320-txt-0132830_utf.txt', '13810821-txt-0172902_utf.txt', '13830627-txt-0431835_utf.txt', '13850502-txt-0751465_utf.txt', '13850506-txt-0754145_utf.txt', '13850723-txt-0802407_utf.txt', '13860630-txt-1002033_utf.txt', '13870730-txt-1219770_utf.txt']
['13860431-txt-0963964_utf.txt', '13860616-txt-0992811_utf.txt', '13860625-txt-0997674_utf.txt', '13860722-txt-1013944_utf.txt', '13860802-txt-1021550_utf.txt', '13870329-txt-1149735_utf.txt', '13870903-txt-1240455_utf.txt', '13871001-txt-1256894_utf.txt']
['13860321-txt-0940314_utf.txt', '13860930-txt-1055987_utf.txt', '13870504-txt-1169324_utf.txt', '13880223-txt-1337283_utf.txt', '13890626-txt-1614537_utf.txt', '13891005-txt-1681151_utf.txt', '13891025-txt-1694816_utf.txt', '13891224-txt-1732745_utf.txt']
['13821109-txt-0342352_utf.txt', '13840501-txt-0558076_utf.txt', '13840725-txt-0599073_utf.txt', '13850728-txt-0809843_utf.txt', '13850910-txt-0834263_utf.txt', '13871015-txt-1264594_utf.txt', '13880304-txt-1345179_utf.txt', '13890531-txt-1596470_utf.txt']
['13850816-txt-0807093_utf.txt', '13850903-txt-0830601_utf.txt', '13851012-txt-0853818_utf.txt', '13870605-txt-1185666_utf.txt', '13890301-txt-1542795_utf.txt', '13890626-txt-1614287_utf.txt', '13890716-txt-1628932_utf.txt', '13900115-txt-1740412_utf.txt']
['13870521-txt-1177039_utf.txt', '13870706-txt-1196885_utf.txt', '13870911-txt-1220118_utf.txt', '13871029-txt-1273519_utf.txt', '13880118-txt-1312303_utf.txt', '13880303-txt-1202027_utf.txt', '13880330-txt-1132374_utf.txt', '13880406-txt-1360964_utf.txt']
['13840803-txt-0602704_utf.txt', '13841026-txt-0651073_utf.txt', '13880123-txt-1315587_utf.txt', '13880205-txt-1324336_utf.txt', '13880319-txt-1353520_utf.txt', '13880621-txt-1401062_utf.txt', '13890318-txt-1553380_utf.txt', '13890909-txt-1665470_utf.txt']</p>
",Text Classification / Sentiment Analysis,perceptron classifying folder contains sub folder sub folder contains file generally file train set test set folder contains sub folder sub folder contains file generally file test set another file contains common word train set check word train set exist return else return make vector classify text bipolar perceptron neural network threshold learning rate part assigning weight work well change code folder name language persian folder contains file txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt txt utf txt
How to improve classification of small texts,"<p>The data that I've got are mostly tweets or small comments (300-400 chars). I used a Bag-Of-Word model and used NaiveBayes classification. Now I'm having a lot of misclassified cases which are of the type mentioned below :-</p>

<pre><code>1.] He sucked on a lemon early morning to get rid of hangover.
2.] That movie sucked big time.
</code></pre>

<p>Now the problem is that during sentiment classification both are getting ""Negative"" just because of the word ""sucked""</p>

<pre><code>Sentiment Classification : 1.] Negative 2.] Negative
</code></pre>

<p>Similarly during document classification both are getting classified into ""movies"" due to the presence of word ""sucked"".</p>

<pre><code>Document classification  : 1.] Movie    2.] Movie
</code></pre>

<p>This is just one of such instances, I'm facing a huge number of wrong classifications and don't have any idea on how to improve the accuracy. </p>
",Text Classification / Sentiment Analysis,improve classification small text data got mostly tweet small comment char used bag word model used naivebayes classification lot misclassified case type mentioned problem sentiment classification getting negative word sucked similarly document classification getting classified movie due presence word sucked one instance facing huge number wrong classification idea improve accuracy
Any NLP API or Utility for Hadoop?,"<p>I am working on large scare text based analysis. More precisely I am doing Sentiment analysis on Twitter data for particular products.
I am using Flume to pull Twitter data in HDFS. 
Is there any NLP API or Utility I can apply on these twitts to get correct and meaningful sentiment out of it?</p>

<p>I am looking for NLP API or utility that i can use in Hadoop system.</p>
",Text Classification / Sentiment Analysis,nlp api utility hadoop working large scare text based analysis precisely sentiment analysis twitter data particular product using flume pull twitter data hdfs nlp api utility apply twitts get correct meaningful sentiment looking nlp api utility use hadoop system
explicit POS tagged input provided and getting sentiment stanfordnlp,"<p>I am trying the code mentioned in question 11 from the <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#f"" rel=""nofollow"">URL</a>.</p>

<p>I want to first give POS tagged input and second get sentiment analysis. First one I able to successfully get done. I able to print the tree and it looks fine. However second one returns me <code>-1</code> (it should return me <code>4=very positive</code>).</p>

<p>Please provide inputs/suggestions.</p>

<pre><code>public static String test(){
    try{

        String grammer=""/Users/lenin/jar/stanfordparser-master/stanford-parser/models/englishPCFG.ser.gz"";

        // set up grammar and options as appropriate
        LexicalizedParser lp = LexicalizedParser.loadModel(grammer);
        String[] sent3 = { ""movie"", ""was"",""very"", ""good"",""."" };
        // Parser gets tag of second ""can"" wrong without help                    
        String[] tag3 = { ""PRP"", ""VBD"", ""RB"", ""JJ"",""."" };                             
        List sentence3 = new ArrayList();
        for (int i = 0; i &lt; sent3.length; i++) {
          sentence3.add(new TaggedWord(sent3[i], tag3[i]));
        }
        Tree parse = lp.parse(sentence3);
        parse.pennPrint();

        int sentiment_score = RNNCoreAnnotations.getPredictedClass(parse);
        System.out.println(""score: ""+sentiment_score);


    }
    catch(Exception e){
        e.printStackTrace();
    }
    return """";
}
</code></pre>
",Text Classification / Sentiment Analysis,explicit po tagged input provided getting sentiment stanfordnlp trying code mentioned question url want first give po tagged input second get sentiment analysis first one able successfully get done able print tree look fine however second one return return please provide input suggestion
On StanfordCoreNLP usage on sentiment analysis,"<p>This a question on StanfordCoreNLP usage on sentiment analysis. I am not sure, based on my exploration  ""SentimentCoreAnnotations.AnnotatedTree.class"" has been changed to ""SentimentCoreAnnotations.SentimentAnnotatedTree.class"". Because I get ""SentimentCoreAnnotations.AnnotatedTree cannot be resolved to a type"". However when i changed to ""SentimentCoreAnnotations.SentimentAnnotatedTree.class"" I get NULL.   Can someone please clarify? Thank you!
I am using below code available in internet. I found most implementations are similar. I encounter same problem in almost all implementations i tried.</p>

<pre><code>package crawler;
import java.util.Properties;
import org.ejml.simple.SimpleMatrix;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;
import java.util.List;
import java.util.Properties;
import edu.stanford.*;


public class NLP {  
    public static void main(String[] args) {
        findSentiment(""life is good."");
    }

    public static void findSentiment(String line) {

        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        int mainSentiment = 0;

        if (line != null &amp;&amp; line.length() &gt; 0) {
            System.out.println(""line:""+line);
            int longest = 0;
            Annotation annotation = pipeline.process(line);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence
                        .get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        if (mainSentiment == 2 || mainSentiment &gt; 4 || mainSentiment &lt; 0) {
            System.out.println(""Neutral "" + line);
        }
        else{
        }
        /*
         * TweetWithSentiment tweetWithSentiment = new TweetWithSentiment(line,
         * toCss(mainSentiment)); return tweetWithSentiment;
         */

    }
}   
</code></pre>
",Text Classification / Sentiment Analysis,stanfordcorenlp usage sentiment analysis question stanfordcorenlp usage sentiment analysis sure based exploration sentimentcoreannotations annotatedtree class ha changed sentimentcoreannotations sentimentannotatedtree class get sentimentcoreannotations annotatedtree resolved type however changed sentimentcoreannotations sentimentannotatedtree class get null someone please clarify thank using code available internet found implementation similar encounter problem almost implementation tried
Compare documents by sequence vector,"<p>I'm trying to classify documents by sequence vector. Basically, I have a vocabulary (more than 5000 words). Each document is converted to a vector of integer numbers so that each element in the vector corresponds the position of the word in the vocabulary. 
<br/><br/>For example, if the vocab is [hello, how, are, you, today] and the document is ""hello you"" then I'll have the vector: <code>[1 4]</code>.<br/> Another document of ""how are you"" will result in <code>[2 3 4]</code>.
<br/><br/> Now what I want is to assess the similarity between the first and the second vector. Here you can see these vectors don't have the same length. Furthermore, comparing directly them may not make sense because they represent sequence of words. This case is different from binary (bag-of-word) vector, which considers the appearance of a word in the document (1 if appear, otherwise 0), and also frequency (word count) vector, which considers frequency of a word in the document with the given vocabulary.
<Br>Can you give me a suggestion?</p>
",Text Classification / Sentiment Analysis,compare document sequence vector trying classify document sequence vector basically vocabulary word document converted vector integer number element vector corresponds position word vocabulary example vocab hello today document hello vector another document result want ass similarity first second vector see vector length furthermore comparing directly may make sense represent sequence word case different binary bag word vector considers appearance word document appear otherwise also frequency word count vector considers frequency word document given vocabulary give suggestion
Python Naive Bayes Classifier trained on Movie Review Corpus to test on Tweets,"<pre><code>import nltk.classify.util
import csv
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews

def word_feats(words):
    return dict([(word, True) for word in words])

negids = movie_reviews.fileids('neg')
posids = movie_reviews.fileids('pos')

negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]

negcutoff = len(negfeats)*3/4
poscutoff = len(posfeats)*3/4

trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]
print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))

classifier = NaiveBayesClassifier.train(trainfeats)
print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
classifier.show_most_informative_features()
</code></pre>

<p>I am very new to Python and am trying to perform sentiment analysis on Tweets. I am using the Naive Bayes Classifier that is built into the NLTK package. I am testing it on the movie review corpus and want to test in on tweets I have stored into a .txt or .csv file using Tweepy. Can anyone help figure out how to test this classifier off the tweets in my output file? Thanks!</p>
",Text Classification / Sentiment Analysis,python naive bayes classifier trained movie review corpus test tweet new python trying perform sentiment analysis tweet using naive bayes classifier built nltk package testing movie review corpus want test tweet stored txt csv file using tweepy anyone help figure test classifier tweet output file thanks
python textblob and text classification,"<p>I'm trying do build a text classification model with python and <a href=""https://textblob.readthedocs.org/en/dev/index.html"" rel=""nofollow noreferrer"">textblob</a>, the script is runing on my server and in the future the idea is that users will be able to submit their text and it will be classified.
i'm loading the training set from csv :</p>
<pre><code># -*- coding: utf-8 -*-
import sys
import codecs
sys.stdout = open('yyyyyyyyy.txt',&quot;w&quot;);
from nltk.tokenize import word_tokenize
from textblob.classifiers import NaiveBayesClassifier
with open('file.csv', 'r', encoding='latin-1') as fp:
    cl = NaiveBayesClassifier(fp, format=&quot;csv&quot;)  

print(cl.classify(&quot;some text&quot;))
</code></pre>
<p>csv is about 500 lines long (with string between 10 and 100 chars), and NaiveBayesclassifier needs about 2 minutes for training and then be able to classify my text(not sure if is normal that it need so much time, maybe is my server slow with only 512mb ram).</p>
<p>example of csv line :</p>
<pre><code>&quot;Oggi alla Camera con la Fondazione Italia-Usa abbiamo consegnato a 140 studenti laureati con 110 e 110 lode i diplomi del Master in Marketing Comunicazione e Made in Italy.&quot;,FI-PDL
</code></pre>
<p>what is not clear to me, and i cant find an answer on textblob documentation, is if there is a way to 'save' my trained classifier (so save a lot of time), because by now everytime i run the script it will train again the classifier.
I'm new to text classification and machine learing so my apologize if it is a dumb question.</p>
<p>Thanks in advance.</p>
",Text Classification / Sentiment Analysis,python textblob text classification trying build text classification model python textblob script runing server future idea user able submit text classified loading training set csv csv line long string char naivebayesclassifier need minute training able classify text sure normal need much time maybe server slow mb ram example csv line clear cant find answer textblob documentation way save trained classifier save lot time everytime run script train classifier new text classification machine learing apologize dumb question thanks advance
Features for sentiment analysis using Maxent model,"<p>I want to implement my own sentiment analysis using maximum entropy model. without using any Api. what could be the best features f(c,d) for my maximum entropy model. I have three classes positive, negative and neutral</p>
",Text Classification / Sentiment Analysis,feature sentiment analysis using maxent model want implement sentiment analysis using maximum entropy model without using api could best feature f c maximum entropy model three class positive negative neutral
Stanford NLP sentiment score returns -1 always in c#,"<p>I am trying to implement Stanford NLP sentiment analysis code in c# using code borrowed from this site and on the main Stanford website. The following code works but Score is always -1. Score should be between 0 to 4. Any help?</p>

<pre><code>    // We should change current directory, so StanfordCoreNLP could find all the model files automatically
    var curDir = Environment.CurrentDirectory;
    Directory.SetCurrentDirectory(jarRoot);
    var pipeline = new StanfordCoreNLP(props);
    Directory.SetCurrentDirectory(curDir);

    foreach(String text in texts) {   
        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

        // these are all the sentences in this document
        // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
        var sentences = document.get(new CoreAnnotations.SentencesAnnotation().getClass()) as ArrayList;

        String[] sentimentText = { ""Very Negative"",""Negative"", ""Neutral"", ""Positive"", ""Very Positive""};

        foreach(CoreMap sentence in sentences) {
            Tree tree = (Tree)sentence.get(typeof(edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation));
            int score = RNNCoreAnnotations.getPredictedClass(tree);
            Console.WriteLine(sentimentText[score]);  // prints sentiment for each sentence in the doc

        }
</code></pre>
",Text Classification / Sentiment Analysis,stanford nlp sentiment score return always c trying implement stanford nlp sentiment analysis code c using code borrowed site main stanford website following code work score always score help
Missing values in sentiment classification,"<p>I am trying to build a sentiment analysis engine using python's sklearn package.
the problem is analyzing Rotten Tomatoes reviews on this Kaggle Competition </p>

<p><a href=""https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"" rel=""nofollow"">https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews</a></p>

<p>the sentiments can take 5 possible values</p>

<p>I am using the following classifiers</p>

<ol>
<li>Multinomial Naive Bayes</li>
<li>Logistic Regression</li>
<li>Stochastic Gradient Descent</li>
</ol>

<p>Since these are all linear classifiers suited for binary classification, here are the steps that i have to take</p>

<ol>
<li><p>Break up the training and test set into 5 parts, one part per sentiment.
Lets say the possible values for the sentiment are a,b,c,d,e. So in part one of my data, i will have all the reviews, but the reviews that have sentiment 'a' will be marked as positive and all of the others will be marked as negative. Similarly i create other parts for the other sentiment values.</p></li>
<li><p>Clean up the data in all 5 parts</p></li>
<li><p>Create a pipeline and feed all the test set parts to my classifier, one after the other. I will store one result per part. So result of classifying part one is partOneRes and so on. Anything which is marked as positive in partOneRes belongs to sentiment 'a'. Similarly for other parts.</p></li>
<li><p>Finally i would like to combine the results for all 5 parts. I will look at partOneRes. Anything that is marked positive will be changed to Sentiment 'a'. I will do similarly for all the other parts. Then i simply merge the results.</p></li>
<li><p>It would have been ideal if i got no overlaps or duplicates. But i get a small number of duplicates, which is fine. I can add some logic to handle that.</p></li>
<li><p>I would do this for all three classifiers and finally i want to find out which classifier give me the best results.</p></li>
</ol>

<p>My problem is that I can see that there are many reviews which my classifier was not able to put in any category! Why would this happen? Could it be due to the small size of the dataset?</p>
",Text Classification / Sentiment Analysis,missing value sentiment classification trying build sentiment analysis engine using python sklearn package problem analyzing rotten tomato review kaggle competition sentiment take possible value using following classifier multinomial naive bayes logistic regression stochastic gradient descent since linear classifier suited binary classification step take break training test set part one part per sentiment let say possible value sentiment b c e part one data review review sentiment marked positive others marked negative similarly create part sentiment value clean data part create pipeline feed test set part classifier one store one result per part result classifying part one partoneres anything marked positive partoneres belongs sentiment similarly part finally would like combine result part look partoneres anything marked positive changed sentiment similarly part simply merge result would ideal got overlap duplicate get small number duplicate fine add logic handle would three classifier finally want find classifier give best result problem see many review classifier wa able put category would happen could due small size dataset
Training dataset for sentiment analysis of restaurant reviews,"<p>I am trying to analyze the customer reviews for restaurants and was wondering if there was any particular dataset available for this domain?</p>

<p>If not, what would be the best training dataset for this category?</p>

<p>Thanks!</p>
",Text Classification / Sentiment Analysis,training dataset sentiment analysis restaurant review trying analyze customer review restaurant wa wondering wa particular dataset available domain would best training dataset category thanks
Subjectivity and objectivity detection,"<p>I am trying to separate subjective and objective sentences from eachother, I tried to find some good research papers in this area but all of the work is in sentiment analysis and I could not find any good paper just focusing on subjectivity and objectivity of the text... So now my question is how should I interpret interpret subjective and objective text? Is it possible to have a subjective sentence which is neutral in terms of sentiment? or when I say ""I went to school"", is it subjective or objective(I assume it is subjective since it is not about general fact)?  </p>
",Text Classification / Sentiment Analysis,subjectivity objectivity detection trying separate subjective objective sentence eachother tried find good research paper area work sentiment analysis could find good paper focusing subjectivity objectivity text question interpret interpret subjective objective text possible subjective sentence neutral term sentiment say went school subjective objective assume subjective since general fact
Best Text Document Classification Algorithm,"<p>I would like to know the best available algorithms for text Classification. I want to classify the document based on Sports, Bank, technology etc.Please suggest good algorithms to get highest accuracy. </p>
",Text Classification / Sentiment Analysis,best text document classification algorithm would like know best available algorithm text classification want classify document based sport bank technology etc please suggest good algorithm get highest accuracy
Stanford sentiment - Cannot replicate same experiments with same accuracy - I get 79% instead of 85%,"<p>I am using the Stanford NLP for Sentiment Analysis,</p>

<p>but after training the model for more or less 24 hours, the session ended for maximum training time exceeded.</p>

<p>After running the evaluation of the created models, I have found out that the results in accuracy are far less performing than the ones from the Stanford paper.</p>

<blockquote>
  <p><strong>These are the results of the Evaluation</strong>:</p>
  
  <p>Tested 82600 labels</p>
  
  <p>65166 correct</p>
  
  <p>17434 incorrect</p>
  
  <p>0.788935 accuracy</p>
  
  <p>Tested 2210 roots</p>
  
  <p>828 correct</p>
  
  <p>1382 incorrect</p>
  
  <p>0,374661 accuracy</p>
  
  <p>Approximate Negative label accuracy: 0,595578</p>
  
  <p>Approximate Positive label accuracy: 0,663263</p>
  
  <p>Combined approximate label accuracy: 0,634001</p>
  
  <p>Approximate Negative root label accuracy: 0,665570</p>
  
  <p>Approximate Positive root label accuracy: 0,601760</p>
  
  <p>Combined approximate root label accuracy: 0,633718</p>
</blockquote>

<p>I decided to retrain the model and set a MaximumTrainTimeSeconds to 3 days, hoping to get better accuracy performance.</p>

<p>Has anyone encountered the same issue?</p>

<p>Do you think that retraining the algorithm for a longer period would make me achieve the expected accuracy?</p>

<p>Moreover, I am not entirely sure of how the score described in the model (e.g. 79,30 in the model in the picture) relates to the accuracy-best performance of the model.</p>

<p>I'm very new to NLP so if I am missing any required information or anything at all please let me know! Thank you!</p>
",Text Classification / Sentiment Analysis,stanford sentiment replicate experiment accuracy get instead using stanford nlp sentiment analysis training model le hour session ended maximum training time exceeded running evaluation created model found result accuracy far le performing one stanford paper result evaluation tested label correct incorrect accuracy tested root correct incorrect accuracy approximate negative label accuracy approximate positive label accuracy combined approximate label accuracy approximate negative root label accuracy approximate positive root label accuracy combined approximate root label accuracy decided retrain model set maximumtraintimeseconds day hoping get better accuracy performance ha anyone encountered issue think retraining algorithm longer period would make achieve expected accuracy moreover entirely sure score described model e g model picture relates accuracy best performance model new nlp missing required information anything please let know thank
Sentiment analysis: more than 3 sentiments,"<p>My app needs sentiment analysis functionality. I've found plenty of services and libraries which can help with this task. But most of them have ""three-dimensional"" output: the text may be classified as ""positive"", ""negative"" or ""neutral. </p>

<p>But what if I need larger variety of options? For example: ""confident/doubtful"", ""calm/alerted"", ""kind/aggressive"" or something like that.</p>

<p>Is it even possible to perform such classification? May be there are already some services/frameworks/libraries available?</p>
",Text Classification / Sentiment Analysis,sentiment analysis sentiment app need sentiment analysis functionality found plenty service library help task three dimensional output text may classified positive negative neutral need larger variety option example confident doubtful calm alerted kind aggressive something like even possible perform classification may already service framework library available
"NLP with Python - how to build a corpus, which classifier to use?","<p>I’m trying to figure out which direction to take my Python NLP project in, and I’d be very grateful to the SO community for any advice.</p>

<p><strong>Problem:</strong></p>

<p>Let’s say I have 100 .txt files that contain the minutes of 100 meetings held by a decisionmaking body. I also have 100 .txt files of corresponding meeting outcomes, which contain the resolutions passed by this body. The outcomes fall into one of seven categories – 1 – take no action, 2 – take soft action, 3 – take stronger action, 4 – take strongest action, 5 – cancel soft action previously taken, 6 – cancel stronger action previously taken, 7 – cancel strongest action previously taken. Alternatively, this can be presented on a scale from -3 to +3, with 0 signifying no action, +1 signifying soft action, -1 signifying cancellation of soft action previously taken, and so on. </p>

<p><strong>Based on the text of the inputs, I’m interested in predicting which of these seven outcomes will occur.</strong> </p>

<p>I’m thinking of treating this as a form of sentiment analysis, since the decision to take a certain kind of action is basically a sentiment. However, all the sentiment analysis examples I’ve found have focused on positive/negative dichotomies, sometimes adding in neutral sentiment as a category. I haven’t found any examples with more than 3 possible classifications of outcomes – not sure whether this is because I haven’t looked in the right places, because it just isn’t really an approach of interest for whatever reason, or because this approach is a silly idea for some reason of which I’m not yet quite sure. </p>

<p><strong>Question 1.</strong> Should be I approaching this as a form of sentiment analysis, or is there some other approach that would work better? Should I instead treat this as a kind of categorization matter, similar to classifying news articles by topic and training the model to recognize the ""topic"" (outcome)?  </p>

<p><strong>Corpus:</strong></p>

<p>I understand that I will need to build a corpus for training/test data, and it looks like I have two immediately evident options: </p>

<p>1 – hand-code a CSV file for training data that would contain some key phrases from each input text and list the value of the corresponding outcome on a 7-point scale, similar to what’s been done here: <a href=""http://help.sentiment140.com/for-students"" rel=""nofollow"">http://help.sentiment140.com/for-students</a></p>

<p>2 – use the approach Pang and Lee used (<a href=""http://www.cs.cornell.edu/people/pabo/movie-review-data/"" rel=""nofollow"">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a>) and put each of my .txt files of inputs into one of seven folders based on outcomes, since the outcomes (what kind of action was taken) are known based on historical data.</p>

<p>The downside to the first option is that it would be very subjective – I would determine which keywords/phrases I think are the most important to include, and I may not necessarily be the best arbiter. The downside to the second option is that it might have less predictive power because the texts are pretty long, contain lots of extraneous words/phrases, and are often stylistically similar (policy speeches tend to use policy words). I looked at Pang and Lee’s data, though, and it seems like that may not be a huge problem, since the reviews they’re using are also not very varied in terms of style. I’m leaning towards the Pang and Lee approach, but I’m not sure if it would even work with more than two types of outcomes.</p>

<p><strong>Question 2.</strong> Am I correct in assuming that these are my two general options for building the corpus? Am I missing some other (better) option? </p>

<p><strong>Question 3.</strong> Given all of the above, which classifier should I be using? I’m thinking maximum entropy would work best; I’ve also looked into random forests, but I have no experience with the latter and really have no idea what I’m doing (yet) when it comes to them. </p>

<p>Thank you very much in advance :)</p>
",Text Classification / Sentiment Analysis,nlp python build corpus classifier use trying figure direction take python nlp project grateful community advice problem let say txt file contain minute meeting held decisionmaking body also txt file corresponding meeting outcome contain resolution passed body outcome fall one seven category take action take soft action take stronger action take strongest action cancel soft action previously taken cancel stronger action previously taken cancel strongest action previously taken alternatively presented scale signifying action signifying soft action signifying cancellation soft action previously taken based text input interested predicting seven outcome occur thinking treating form sentiment analysis since decision take certain kind action basically sentiment however sentiment analysis example found focused positive negative dichotomy sometimes adding neutral sentiment category found example possible classification outcome sure whether looked right place really approach interest whatever reason approach silly idea reason yet quite sure question approaching form sentiment analysis approach would work better instead treat kind categorization matter similar classifying news article topic training model recognize topic outcome corpus understand need build corpus training test data look like two immediately evident option hand code csv file training data would contain key phrase input text list value corresponding outcome point scale similar done use approach pang lee used put txt file input one seven folder based outcome since outcome kind action wa taken known based historical data downside first option would subjective would determine keywords phrase think important include may necessarily best arbiter downside second option might le predictive power text pretty long contain lot extraneous word phrase often stylistically similar policy speech tend use policy word looked pang lee data though seems like may huge problem since review using also varied term style leaning towards pang lee approach sure would even work two type outcome question correct assuming two general option building corpus missing better option question given classifier using thinking maximum entropy would work best also looked random forest experience latter really idea yet come thank much advance
NLP: how do you determine the polarity of a certain word?,"<p>Sorry for the vague nature of this question, but I don't even know the proper NLP terminology to search constructively. </p>

<p>Basically what I'm attempting to do is to determine whether a word (usually an adjective or a noun) is ""confirmed"" by its context, or ""negated."" (Apologies for potential misuse of terminology) </p>

<p>The application is one step of a larger process that parses through a large body of user reviews, and uses keyword frequency to determine certain aspects of a product. However, certain keywords may either apply or not apply depending on the context.</p>

<p>As an example, when looking for a keyword ""flimsy"", two possible contexts are:</p>

<p>A) ""The product felt flimsy in my hands and fell apart within days"" --> keyword confirmed</p>

<p>B) ""The build quality was very solid and not at all flimsy like its competitors"" --> keyword negated</p>

<p>Note that this is different from the sentiment analysis of the phrase; the purpose is not to determine whether or not the opinion is positive or negative, but rather if a word is applicable in context or not.</p>

<p>Is there any standard methodology or technique to achieve this?</p>
",Text Classification / Sentiment Analysis,nlp determine polarity certain word sorry vague nature question even know proper nlp terminology search constructively basically attempting determine whether word usually adjective noun confirmed context negated apology potential misuse terminology application one step larger process par large body user review us keyword frequency determine certain aspect product however certain keywords may either apply apply depending context example looking keyword flimsy two possible context product felt flimsy hand fell apart within day keyword confirmed b build quality wa solid flimsy like keyword negated note different sentiment analysis phrase purpose determine whether opinion positive negative rather word applicable context standard methodology technique achieve
Filtering twitter data,"<p>I have implemented an unsupervised algorithm for sentiment analysis on data from social media sites, specifically from Twitter.<br>
However I intend to gauge what people are saying about a specific topic, say 'traffic in certain state' for example.<br>
I could gather data using keywords such as 'traffic', 'congestion', pedestrians' and so on.<br>
Some of this acquired data has different contexts and has nothing to do with traffic.<br><br>
My questions are -<br><br>
1. How do i filter out data based on topics?<br>
2. Do i need to perform topic extraction or use spam filter?</p>
",Text Classification / Sentiment Analysis,filtering twitter data implemented unsupervised algorithm sentiment analysis data social medium site specifically twitter however intend gauge people saying specific topic say traffic certain state example could gather data using keywords traffic congestion pedestrian acquired data ha different context ha nothing traffic question filter data based topic need perform topic extraction use spam filter
Do I need to standardize data when doing text classification in Scikit,"<p>I am developing a spam filter using <code>Scikit.</code> 
Here are the steps I follow:</p>

<p>Xdata = <code>[""This is spam"" , ""This is Ham"" , ""This is spam again""]</code></p>

<ol>
<li><p><code>Matrix</code> = <code>Countvectorizer (XData)</code> . Matrix will contains count of each word in all documents. So Matrix[i][j] will give me counts of word <code>j</code> in document <code>i</code></p></li>
<li><p><code>Matrix_idfX</code> = <code>TFIDFVectorizer(Matrix)</code>. It will normalize score. </p></li>
<li><p><code>Matrix_idfX_Select</code> = <code>SelectKBest( Matrix_IdfX , 500)</code> . It will reduce matrix to  500 best score columns</p></li>
<li><p><code>Multinomial.train( Matrix_Idfx_Select)</code>  </p></li>
</ol>

<p>Now my question Do I need to perform <strong>normalization or standardization</strong> in any of the above four steps ? If yes then after which step and why?</p>

<p>Thanks</p>
",Text Classification / Sentiment Analysis,need standardize data text classification scikit developing spam filter using step follow xdata matrix contains count word document matrix j give count word document normalize score reduce matrix best score column question need perform normalization standardization four step yes step thanks
Understanding Naive Bayes for text classification,"<p>I'm familiar with machine learning and Naive Bayes, but I am having some trouble understanding how to implement it for document classification where my feature vector is a bag of words.</p>

<p>In particular, how do you compute the class-conditional feature likelihood <code>Pr(word | class)</code>? In many texts I see the following terminology:</p>

<p><img src=""https://i.sstatic.net/AL3UF.png"" alt=""enter image description here""></p>

<p>How is the right-hand side implemented? Is it the count of documents of class <code>c</code> in which the feature <code>f</code> occurs divided by the count of documents of class <code>c</code>?</p>

<p>For example, suppose you have 10 documents, where 7 are class C1 and 3 are class C2. The word ""amazing"" occurs in some of them:</p>

<pre><code>C1: ...
C1: ... amazing ...
C1: ...
C1: ... amazing ...
C1: ... amazing ...
C1: ...
C1: ...
C2: ...
C2: ... amazing ...
C2: ...
</code></pre>

<p>It looks like:</p>

<ul>
<li>count(amazing, C1) = 3 </li>
<li>count(amazing, C2) = 1</li>
<li>count(C1) = 7</li>
<li>count(C2) = 3</li>
</ul>

<p>Would Pr(amazing|C1) = 3/7 and Pr(amazing|C2) = 1/3?</p>

<hr>

<p>Edit 5/7/2015</p>

<p>I came across a discussion of Naive Bayes for text classification in ""<a href=""http://nlp.stanford.edu/IR-book/"" rel=""nofollow noreferrer"">Introduction to Information Retrieval</a>"" book, Chapter 13 (<a href=""http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf"" rel=""nofollow noreferrer"">PDF</a>). There is a different formulation for the class-conditional feature probability:</p>

<p><img src=""https://i.sstatic.net/p17dJ.png"" alt=""enter image description here""></p>

<p>So, here it looks like count(word, class) is the total number of occurrences of the <strong>words</strong> in documents in the class rather then the number of documents in the class.</p>

<p>Likewise, count(class) is the total number of <strong>words</strong> in documents in the class, not the number of documents in the class.</p>

<p>Which formulation of P(feature|class) is preferred?</p>
",Text Classification / Sentiment Analysis,understanding naive bayes text classification familiar machine learning naive bayes trouble understanding implement document classification feature vector bag word particular compute class conditional feature likelihood many text see following terminology right hand side implemented count document class feature occurs divided count document class example suppose document class c class c word amazing occurs look like count amazing c count amazing c count c count c would pr amazing c pr amazing c edit came across discussion naive bayes text classification introduction information retrieval book chapter pdf different formulation class conditional feature probability look like count word class total number occurrence word document class rather number document class likewise count class total number word document class number document class formulation p feature class preferred
Christopher Potts&#39; 39 features in his sentiment analyzer,"<p>I'm relatively new to sentiment analysis and have been reading the excellent online tutorial by Stanford professor Christopher Potts. </p>

<p>In <a href=""http://sentiment.christopherpotts.net/classifiers.html#numeric"" rel=""nofollow"">his tutorial under the section on machine learning classifiers</a>, he provides a brief discussion of a ""lightweight, accurate classifier"" and suggests the following steps:</p>

<blockquote>
  <ol>
  <li>Begin with a set of N fixed sentiment lexicons L. For my experiments, I used the fixed polarity lexicons, the IMDB scores, the
  Experience Project O/E vectors, and the sentiment-rich classes from
  the Harvard General Inquirer and LIWC. <strong>The total number of
  predictors was 39</strong>, all of them numeric.</li>
  <li>...</li>
  <li>For a given text, the feature function simply sums up all the words' scores for each of the <strong>39 predictors</strong> and then normalizes
  them by the length of the text. Thus, each text is modeled as a vector
  of 39 numbers.</li>
  <li>...</li>
  </ol>
</blockquote>

<p>I am wondering where he got those 39 features from? From my understanding, the lexicons have thousands of entries, resulting in thousands of features using a bag-of-words model. How did he go from thousands of features to 39 features? He must have obviously done some feature selection, but I cannot infer his methodology or selection criteria. </p>
",Text Classification / Sentiment Analysis,christopher potts feature sentiment analyzer relatively new sentiment analysis reading excellent online tutorial stanford professor christopher potts tutorial section machine learning classifier provides brief discussion lightweight accurate classifier suggests following step begin set n fixed sentiment lexicon l experiment used fixed polarity lexicon imdb score experience project e vector sentiment rich class harvard general inquirer liwc total number predictor wa numeric given text feature function simply sum word score predictor normalizes length text thus text modeled vector number wondering got feature understanding lexicon thousand entry resulting thousand feature using bag word model go thousand feature feature must obviously done feature selection infer methodology selection criterion
"transfer documents to vector space representation, how to generate the dictionary?","<p>I have large amount of unstructured text documents, for each document, I want a vector space representation, so that it is easy for me to classify the documents into clusters and do semantic nature analysis. Many way to transfer documents to vector space, like bag-of-words (BOW) model,  Latent Semantic Analysis (LSA), n gram model,etc. But I think all of them need a Dictionary for the keywords.(not sure) But if there is no query, how to generate the Dictionary for a large amount of documents?(1 million) How to determine important words in a document?</p>
",Text Classification / Sentiment Analysis,transfer document vector space representation generate dictionary large amount unstructured text document document want vector space representation easy classify document cluster semantic nature analysis many way transfer document vector space like bag word bow model latent semantic analysis lsa n gram model etc think need dictionary keywords sure query generate dictionary large amount document million determine important word document
Fitting the training dataset for text classification in Java,"<p>I'm building a system that does text classification. I'm building the system in Java. As features I'm using the bag-of-words model. However one problem with such a model is that the number of features is really high, which makes it impossible to fit the data in memory. </p>

<p>However, I came across this <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">tutorial</a> from Scikit-learn which uses specific data structures to solve the issue.</p>

<p>My questions:</p>

<p>1 - How do people solve such an issue using Java in general?</p>

<p>2- Is there a solution similar to the solution given in scikit-learn?</p>

<p>Edit: the only solution I've found so far is to personally write a Sparse Vector implementation using HashTables.</p>
",Text Classification / Sentiment Analysis,fitting training dataset text classification java building system doe text classification building system java feature using bag word model however one problem model number feature really high make impossible fit data memory however came across tutorial scikit learn us specific data structure solve issue question people solve issue using java general solution similar solution given scikit learn edit solution found far personally write sparse vector implementation using hashtables
Sentence classification subjectivity/objectivity,"<p>I want to classify sentences as objective or subjective.</p>

<p>For example : </p>

<p>Objective: Taipei 101 is the world's tallest building.</p>

<p>Subjective: Venus Williams is the greatest athlete of this decade.</p>

<p>I was thinking of using Naive Bayes as my classifier.</p>

<p>What data should I use for training?
I was thinking of news articles and wiki articles for objective sentences. 
For subjectivity, I was thinking of using a lot of books (since books tend to be more subjective).</p>

<p>Is this likely to work or am I thinking about this wrong? What accuracy can I expect? Which data will work best for training objective and subjective?</p>
",Text Classification / Sentiment Analysis,sentence classification subjectivity objectivity want classify sentence objective subjective example objective taipei world tallest building subjective venus williams greatest athlete decade wa thinking using naive bayes classifier data use training wa thinking news article wiki article objective sentence subjectivity wa thinking using lot book since book tend subjective likely work thinking wrong accuracy expect data work best training objective subjective
Good training data for text classification by LDA?,"<p>I'm classifying content based on LDA into generic topics such as Music, Technology, Arts, Science</p>

<p>This is the process i'm using,</p>

<p>9 topics ->  <strong>Music</strong>, <strong>Technology</strong>, <strong>Arts</strong>, <strong>Science</strong> <strong>etc etc</strong>.</p>

<p>9 documents -> <strong>Music.txt</strong>, <strong>Technology.txt</strong>, <strong>Arts.txt</strong>, <strong>Science.txt</strong> etc etc.</p>

<p>I've filled in each document(.txt file) with about 10,000 lines of content of what i think is ""pure"" categorical content</p>

<p>I then classify a test document, to see how well the classifier is trained</p>

<p>My Question is, </p>

<p>a.) Is this an efficient way to classify text (using the above steps)?</p>

<p>b.) Where should i be looking for ""pure"" topical content to fill each of these files? Sources which are not too large (text data > 1GB)</p>

<p>classification is only on ""generic"" topics such as the above</p>
",Text Classification / Sentiment Analysis,good training data text classification lda classifying content based lda generic topic music technology art science process using topic music technology art science etc etc document music txt technology txt art txt science txt etc etc filled document txt file line content think pure categorical content classify test document see well classifier trained question efficient way classify text using step b looking pure topical content fill file source large text data gb classification generic topic
implementing a perceptron classifier,"<p>Hi I'm pretty new to Python and to NLP. I need to implement a perceptron classifier. I searched through some websites but didn't find enough information. For now I have a number of documents which I grouped according to category(sports, entertainment etc). I also have a list of the most used words in these documents along with their frequencies. On a particular website there was stated that I must have some sort of a decision function accepting arguments x and w. x apparently is some sort of vector ( i dont know what w is). But I dont know how to use the information I have to build the perceptron algorithm and how to use it to classify my documents. Have you got any ideas? Thanks :)</p>
",Text Classification / Sentiment Analysis,implementing perceptron classifier hi pretty new python nlp need implement perceptron classifier searched website find enough information number document grouped according category sport entertainment etc also list used word document along frequency particular website wa stated must sort decision function accepting argument x w x apparently sort vector dont know w dont know use information build perceptron algorithm use classify document got idea thanks
Classification of single sentence,"<p>I have 4 different categories and I also have around 3000 words which belong to each of these categories. Now if a new sentence comes, I am able to break the sentence into words and get more words related to it. So say for each new sentence I can get 20-30 words generated from the sentence.
Now what is the best way to classify this sentence in above mentioned category? I know  bag of words works well.
I also looked at LDA, but it works with documents, where as I have a list of words as a training corpus. In LDA it looks at the position of word in document. So I could not get meaningful results from LDA.</p>
",Text Classification / Sentiment Analysis,classification single sentence different category also around word belong category new sentence come able break sentence word get word related say new sentence get word generated sentence best way classify sentence mentioned category know bag word work well also looked lda work document list word training corpus lda look position word document could get meaningful result lda
Sentiment Analysis(SentiWordNet) - Judging the context of a sentence,"<p>I am trying to find whether a <strong>sentence</strong> is Positive or Negative in the following steps:</p>

<p>1.) Retrieving the <em>Parts of speech</em>(verbs, nouns, adjectives etc) from the sentence using the Stanford NLP parser.</p>

<p>2.) Using the <em>SentiWordNet</em> to find the Positive and Negative values related to each Part of Speech.</p>

<p>3.) Summing up the Positive and Negative values obtained to calculate a <em>Net Positive</em> and <em>Net Negative</em> value related to a sentence.</p>

<p>But the problem is that, the SentiWordNet return a list of Positive/Negative values based on different senses/contexts. Is it possible to pass a particular sentence along with the part of speech to the SentiWordNet parser, so that it can judge the sense/context automatically and returns <strong>only one</strong> pair of Positive and Negative value?</p>

<p>Or is there any other alternate solution to this problem?</p>

<p>Thanks.</p>
",Text Classification / Sentiment Analysis,sentiment analysis sentiwordnet judging context sentence trying find whether sentence positive negative following step retrieving part speech verb noun adjective etc sentence using stanford nlp parser using sentiwordnet find positive negative value related part speech summing positive negative value obtained calculate net positive net negative value related sentence problem sentiwordnet return list positive negative value based different sens context possible pas particular sentence along part speech sentiwordnet parser judge sense context automatically return one pair positive negative value alternate solution problem thanks
How to increase the presicion of text classification with the RBM?,"<p>I am learning about text classification and I classify with my own corpus with linnear regression as follows:</p>

<pre><code>from sklearn.linear_model.logistic import LogisticRegression
classifier = LogisticRegression(penalty='l2', C=7)
classifier.fit(training_matrix, y_train)
prediction = classifier.predict(testing_matrix)
</code></pre>

<p>I would like to increase the classification report with a Restricted Boltzman Machine that scikit-learn provide, from the <a href=""http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html"" rel=""nofollow"">documentation</a> I read that this could be use to increase the classification recall, f1-score, accuracy, etc. Could anybody help me to increase this is what I tried so far, thanks in advance:</p>

<pre><code>vectorizer = TfidfVectorizer(max_df=0.5,
                             max_features=None,
                             ngram_range=(1, 1),
                             norm='l2',
                             use_idf=True)


X_train = vectorizer.fit_transform(X_train_r)
X_test = vectorizer.transform(X_test_r)


from sklearn.pipeline import Pipeline
from sklearn.neural_network import BernoulliRBM
logistic = LogisticRegression()
rbm= BernoulliRBM(random_state=0, verbose=True)
classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])
classifier.fit(X_train, y_train)
</code></pre>
",Text Classification / Sentiment Analysis,increase presicion text classification rbm learning text classification classify corpus linnear regression follows would like increase classification report restricted boltzman machine scikit learn provide documentation read could use increase classification recall f score accuracy etc could anybody help increase tried far thanks advance
What are some good tools/practises for aspect level sentiment analysis?,"<p>I am planning to get some review data from tripadvisor and I want to be able to extract hotel related aspects and assign polarity to them and classify them as negative or positive.</p>

<p>What tools can I use for this purpose and how and where do I start? I know there are some tools like GATE, Stanford NLP, Open NLP etc, but would I be able to perform the above specific tasks? If so, please let me know an approach to go forward. I am planning to use Java as the choice of programming language and would like to use some APIs</p>

<p>Also, should I go ahead with a rule based approach or a ML approach that uses a trained corpus of reviews, so some other approach completely?</p>

<p>P.S : I am new to NLP and I need some help to go forward.</p>
",Text Classification / Sentiment Analysis,good tool practises aspect level sentiment analysis planning get review data tripadvisor want able extract hotel related aspect assign polarity classify negative positive tool use purpose start know tool like gate stanford nlp open nlp etc would able perform specific task please let know approach go forward planning use java choice programming language would like use apis also go ahead rule based approach ml approach us trained corpus review approach completely p new nlp need help go forward
Python NLTK: How to retrieve percentage confidence in classifier prediction,"<p>I am currently training an NLTK classifier to recognize motion commands.  These commands can include ""move left"", ""please move forward"", ""halt!"", ""move towards the right"", etc.</p>

<p>I am currently using the classifier based on a few key features (such as the existence of ""halt"" and ""left"") to classify information, and it works fine.</p>

<p>However, let us presume the following text, ""move to the left right"", is given.  In this case, both keywords are conflicting each other, and presumably, the classifier should have a low confidence level when offering its resultant prediction.</p>

<p>As such, is there any way to retrieve the confidence of its predicted ""direction"", after using <code>&lt;CLASSIFIER&gt;.classify()</code> ?</p>

<p>NOTE: I have tried to use <code>nltk.classify.accuracy()</code> but it is only for use on a test data-set, not a single query.</p>
",Text Classification / Sentiment Analysis,python nltk retrieve percentage confidence classifier prediction currently training nltk classifier recognize motion command command include move left please move forward halt move towards right etc currently using classifier based key feature existence halt left classify information work fine however let u presume following text move left right given case keywords conflicting presumably classifier low confidence level offering resultant prediction way retrieve confidence predicted direction using note tried use use test data set single query
what is the best way to understand the user&#39;s query?,"<p>In my java project, I need to classify the users natural language query under three categories: </p>

<ol>
<li>Add some tuples to the database. </li>
<li>Delete some tuples from the database</li>
<li>Create new table</li>
</ol>

<p>The user query will be something like: ""Please Add the user Jhon who was born on 1999"" if we supposed that this is the only needed information to be added.</p>

<p>What is best way to understand the semantic of such sentence hence perform the proper action.</p>

<p>I know I can deal with it within my code by creating an array for the words that means ""ADD"" or ""DELETE"" then send the query to the class that deals with addition or deletion.</p>

<p>But I don't want to solve it this way, I thought of finding the synonym of the word from wordnet but that was not helpful as one of the synonym of delete is edit which needs a different reaction. </p>

<p>I want something more like NLP and Semantic processing.</p>

<p>Any ideas?</p>

<p>Thanks in Advance</p>
",Text Classification / Sentiment Analysis,best way understand user query java project need classify user natural language query three category add tuples database delete tuples database create new table user query something like please add user jhon wa born supposed needed information added best way understand semantic sentence hence perform proper action know deal within code creating array word mean add delete send query class deal addition deletion want solve way thought finding synonym word wordnet wa helpful one synonym delete edit need different reaction want something like nlp semantic processing idea thanks advance
Understanding accuracy_score with scikit-learn with my own corpus?,"<p>Suppose that i all ready do some text classification with scikit learn with <code>SVC</code>. First i vectorized the corpus, i split the data into test and train sets and then i set up the labels into the train set. Now i would like to obtain the accuracy of the classification.</p>

<p>From the <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score"" rel=""nofollow"">documentation</a> i read the following:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import accuracy_score
&gt;&gt;&gt; y_pred = [0, 2, 1, 3]
&gt;&gt;&gt; y_true = [0, 1, 2, 3]
&gt;&gt;&gt; accuracy_score(y_true, y_pred)
0.5
&gt;&gt;&gt; accuracy_score(y_true, y_pred, normalize=False)
2
</code></pre>

<p>The problem is i dont understand what are: <code>y_pred = [0, 2, 1, 3]</code> and <code>y_true = [0, 1, 2, 3]</code> and how can i ""reach"" or obtain these values once i Classified test set of my own corpus. Could anybody help me with this issue?.</p>

<p>Let's say as an example the following:</p>

<p>trainingdata:</p>

<pre><code>Pošto je EULEX obećao da će obaviti istragu o prošlosedmičnom izbijanju nasilja na sjeveru Kosova, taj incident predstavlja još jedan ispit kapaciteta misije da doprinese jačanju vladavine prava.
De todas as provações que teve de suplantar ao longo da vida, qual foi a mais difícil? O início. Qualquer começo apresenta dificuldades que parecem intransponíveis. Mas tive sempre a minha mãe do meu lado. Foi ela quem me ajudou a encontrar forças para enfrentar as situações mais decepcionantes, negativas, as que me punham mesmo furiosa.
Al parecer, Andrea Guasch pone que una relación a distancia es muy difícil de llevar como excusa. Algo con lo que, por lo visto, Alex Lequio no está nada de acuerdo. ¿O es que más bien ya ha conseguido la fama que andaba buscando?
Vo väčšine golfových rezortov ide o veľký komplex niekoľkých ihrísk blízko pri sebe spojených s hotelmi a ďalšími možnosťami trávenia voľného času – nie vždy sú manželky či deti nadšenými golfistami, a tak potrebujú iný druh vyžitia. Zaujímavé kombinácie ponúkajú aj rakúske, švajčiarske či talianske Alpy, kde sa dá v zime lyžovať a v lete hrať golf pod vysokými alpskými končiarmi.
</code></pre>

<p>testdata:</p>

<pre><code>Por ello, ha insistido en que Europa tiene que darle un toque de atención porque Portugal esta incumpliendo la directiva del establecimiento del peaje
Estima-se que o mercado homossexual só na Cidade do México movimente cerca de oito mil milhões de dólares, aproximadamente seis mil milhões de euros


import codecs, re, time
from itertools import chain

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

trainfile = 'train.txt'
testfile = 'test.txt'

# Vectorizing data.
train = []
word_vectorizer = CountVectorizer(analyzer='word')
trainset = word_vectorizer.fit_transform(codecs.open(trainfile,'r','utf8'))
tags = ['bs','pt','es','sr']

# Training NB
mnb = MultinomialNB()
mnb.fit(trainset, tags)

# Tagging the documents
codecs.open(testfile,'r','utf8')
testset = word_vectorizer.transform(codecs.open(testfile,'r','utf8'))
results = mnb.predict(testset)

print results
</code></pre>
",Text Classification / Sentiment Analysis,understanding accuracy score scikit learn corpus suppose ready text classification scikit learn first vectorized corpus split data test train set set label train set would like obtain accuracy classification documentation read following problem dont understand reach obtain value classified test set corpus could anybody help issue let say example following trainingdata testdata
How to use labels to classify text with scikit-learn?,"<p>I have a NLP task (text clasification). I extracted some bigrams like this:</p>

<pre><code>training_data = [[('this', 'is'), ('is', 'a'), ('a', 'text')],
        [('and', 'one'), ('one', 'more')]]
</code></pre>

<p>Then i could use some vectorizer like this:</p>

<pre><code>from sklearn.feature_extraction import FeatureHasher

fh = FeatureHasher(input_type='string')

X = fh.transform(((' '.join(x) for x in sample)
                  for sample in training_data))
print X.toarray()

[[ 0.  0.  0. ...,  0.  0.  0.]
 [ 0.  0.  0. ...,  0.  0.  0.]]
</code></pre>

<p>This is how svm algorithm can be used to classify:</p>

<pre><code>from sklearn import svm
s = svm.SVC()
lables = [HAM, SPAM]    
s.fit(training_data, labels)
</code></pre>

<p>How can i use labels in the above brigam (i.e. <code>training_data</code>) in order to classify?, for example: </p>

<pre><code>data = [[('this', 'is'), ('is', 'a'), ('a', 'text'), 'SPAM'], 
[('and', 'one'), ('one', 'more'), 'HAM']]
</code></pre>
",Text Classification / Sentiment Analysis,use label classify text scikit learn nlp task text clasification extracted bigram like could use vectorizer like svm algorithm used classify use label brigam e order classify example
What is the standard way in scikit-learn to arrange textual data for text classification?,"<p>I have a <strong>NLP task which basically is supervised text classification</strong>. I tagged a corpus with it's POS-tags, then i use the diferent vectorizers that scikit-learn provide in order to feed some classification algorithm that scikit-learn provide as well. I also have the labels (categories) of the corpus which previously i obtained in an unsupervised way. </p>

<p>First I POS-tagged the corpus, then I obtained some differents bigrams, they have the following structure: </p>

<pre><code>bigram = [[('word','word'),...,('word','word')]]
</code></pre>

<p>Apparently it seems that i have everything to classify (i all ready classify with some little examples but not with all the corpus). </p>

<p>I would like to use the bigrams as features in order to present them to a classification algorithm(Multinomial naive bayes, SVM, etc).</p>

<p>What could be a standard (pythonic) way to arrange all the text data to classify and show the results of the classified corpus?. I was thinking about using <a href=""http://weka.wikispaces.com/ARFF"" rel=""nofollow"">arff files</a> and use numpy arrays, but I guess it could complicate the task unnecessarily. By the other hand i was thinking about spliting the data into <strong>train and test folders</strong> but i dont visualize how to set up the labels in the train folder.</p>
",Text Classification / Sentiment Analysis,standard way scikit learn arrange textual data text classification nlp task basically supervised text classification tagged corpus po tag use diferent vectorizers scikit learn provide order feed classification algorithm scikit learn provide well also label category corpus previously obtained unsupervised way first po tagged corpus obtained differents bigram following structure apparently seems everything classify ready classify little example corpus would like use bigram feature order present classification algorithm multinomial naive bayes svm etc could standard pythonic way arrange text data classify show result classified corpus wa thinking using arff file use numpy array guess could complicate task unnecessarily hand wa thinking spliting data train test folder dont visualize set label train folder
How to classify text with scikit&#39;s SVM?,"<p>I have a text classification task. By now i only tagged a corpus and extracted some features in a bigram format (i.e <code>bigram = [('word', 'word'),...,('word', 'word')]</code>. I would like to classify some text, as i understand SVM algorithm only can receive vectors in orther to classify, so i use some vectorizer in scikit as follows:</p>

<pre><code>bigram = [ [('load', 'superior')
             ('point', 'medium'), ('color', 'white'),
             ('the load', 'tower')]]

fh = FeatureHasher(input_type='string')

X = fh.transform(((' '.join(x) for x in sample)
                  for sample in bigram))
print X
</code></pre>

<p>the output is a sparse matrix:</p>

<pre><code>  (0, 226456)   -1.0
  (0, 607603)   -1.0
  (0, 668514)   1.0
  (0, 715910)   -1.0
</code></pre>

<p>How can i use the previous sparse matrix  <code>X</code> to classify with <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"" rel=""nofollow"">SVC</a>?, assuming that i have 2 classes and a train and test sets.</p>
",Text Classification / Sentiment Analysis,classify text scikit svm text classification task tagged corpus extracted feature bigram format e would like classify text understand svm algorithm receive vector orther classify use vectorizer scikit follows output sparse matrix use previous sparse matrix classify svc assuming class train test set
Getting sentiment analysis result using stanford core nlp java code,"<p>When we test it on Stanford demo page:  <a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/sentiment/rntnDemo.html</a></p>

<p>it gives the tree with the sentiment score of each node as below:</p>

<p><img src=""https://i.sstatic.net/hkduA.png"" alt=""enter image description here""></p>

<p>I am trying to test it on my local system using command:</p>

<pre><code>H:\Drive E\Stanford\stanfor-corenlp-full-2013~&gt;java -cp ""*"" edu.stanford.nlp.sen
timent.Evaluate edu/stanford/nlp/models/sentiment/sentiment.ser.gz test.txt
</code></pre>

<p><code>text.txt</code> has </p>

<p><code>This movie doesn't care about cleverness, wit or any other kind of intelligent humor.
Those who find ugly meanings in beautiful things are corrupt without being charming.
</code></p>

<p>which yields result:</p>

<p><img src=""https://i.sstatic.net/4Zj1p.png"" alt=""Result""></p>

<p>Can anyone please tell me why it is null? Or maybe I'm making any mistake in execution? My purpose is to analyze the text and get the sentiment result with the score.</p>
",Text Classification / Sentiment Analysis,getting sentiment analysis result using stanford core nlp java code test stanford demo page give tree sentiment score node trying test local system using command ha yield result anyone please tell null maybe making mistake execution purpose analyze text get sentiment result score
How to extract syntactic features with python using corenlp?,"<p>I want to extract syntactic information from a sentence. What I have tried is </p>

<pre><code>corenlp_dir = ""/home/corenlp-python/stanford-corenlp-full-2013-11-12/""
parser = corenlp.StanfordCoreNLP(corenlp_path=corenlp_dir)

result_json = json.loads(parser.parse(""I am Alice.""))
#pprint.pprint(result_json)
for sentence in result_json[""sentences""]:
    pprint.pprint(sentence[""parsetree""]) 
    print type(sentence[""parsetree""])
</code></pre>

<p>I only get </p>

<pre><code>--&gt;# u'(ROOT (S (NP (PRP I)) (VP (VBP am) (NP (NNP Alice))) (. .)))'
&lt;type 'unicode'&gt;
</code></pre>

<p>and the result is a string, not a parsertree.
However, what I really want is like a syntactic feature that can be used to do text classification. How to extract particular syntactic features from the result above?
I have no ideas about how to do it. Could anyone help me or give me some suggestions?</p>
",Text Classification / Sentiment Analysis,extract syntactic feature python using corenlp want extract syntactic information sentence tried get result string parsertree however really want like syntactic feature used text classification extract particular syntactic feature result idea could anyone help give suggestion
Using pre-defined topics in Mallet,"<p>I'm looking to use Mallet to classify different documents by topics that I have defined. I know that Mallet will first determine the topics, then classify the documents but I want to skip the first step because I already have a list of topics with words associated with them. Is there any way to use pre-defined topic lists that I have created to classify documents with Mallet? </p>

<p>Any guidance is appreciated. Thanks!</p>
",Text Classification / Sentiment Analysis,using pre defined topic mallet looking use mallet classify different document topic defined know mallet first determine topic classify document want skip first step already list topic word associated way use pre defined topic list created classify document mallet guidance appreciated thanks
how to construct training vectors of word n-gram using TF-IDF,"<p>My task is to do the text classification with svm, using word n-gram as features. 
Before using TF-IDF, my code is:</p>

<pre><code>word_dic = ngram.wordNgrams(text, n)
freq_term_vector = [word_dic[gram] if gram in word_dic else 0 for gram in global_vector]
X.append(freq_term_vector)
</code></pre>

<p>And It works well. However, when I tried TF-IDF, the code is below:</p>

<pre><code>freq_term_vector = [word_dic[gram] if gram in word_dic else 0 for gram in global_vector]
tfidf = TfidfTransformer(norm=""l2"")
tfidf.fit(freq_term_vector)
X.append(tfidf.transform(freq_term_vector).toarray())
</code></pre>

<p>The training part can be done, but when the program ran to the predict part, it said</p>

<pre><code> clf.predict(X_test)
  File ""/usr/lib/python2.7/dist-packages/sklearn/linear_model/base.py"", line 223, in predict
    scores = self.decision_function(X)
  File ""/usr/lib/python2.7/dist-packages/sklearn/linear_model/base.py"", line 207, in decision_function
    dense_output=True) + self.intercept_
  File ""/usr/lib/python2.7/dist-packages/sklearn/utils/extmath.py"", line 83, in safe_sparse_dot
    return np.dot(a, b)
ValueError: shapes (1100,1,38) and (1,11) not aligned: 38 (dim 2) != 1 (dim 0)
</code></pre>

<p>The training method and predict method are the same. How can I solve this align problem? Could anyone help me check my code above or give me some idea?</p>
",Text Classification / Sentiment Analysis,construct training vector word n gram using tf idf task text classification svm using word n gram feature using tf idf code work well however tried tf idf code training part done program ran predict part said training method predict method solve align problem could anyone help check code give idea
How to make POS n-grams more effective?,"<p>I am doing text classification with SVM, using POS n-grams as features. But it take me 2 hours to complete only POS unigram. I have 5000 texts, in each text there are 300 words. Here is my code:</p>

<pre><code>def posNgrams(s,n):
    '''Calculate POS n-grams and return a dictionary'''
    text = nltk.word_tokenize(s)
    text_tags = nltk.pos_tag(text)
    taglist = []
    output = {}
    for item in text_tags: 
        taglist.append(item[1])
    for i in xrange(len(taglist)-n+1):
        g = ' '.join(taglist[i:i+n])
        output.setdefault(g,0)
        output[g] += 1
    return output
</code></pre>

<p>I tried the same method to do character n-grams and it only took me several minutes. Could you give me some idea about how to make my POS n-grams faster? </p>
",Text Classification / Sentiment Analysis,make po n gram effective text classification svm using po n gram feature take hour complete po unigram text text word code tried method character n gram took several minute could give idea make po n gram faster
Choice of distance metric in sklearn.feature_extraction.text - feature engineering,"<p>I am following a tutorial about building machine learning systems in Python, and I am modifiying it as I go and trying to classify a new post as belonging to one of 7 different categories.</p>

<pre><code>english_stemmer = nltk.stem.SnowballStemmer('english')
class StemmedTfidfVectorizer(TfidfVectorizer):
    def build_analyzer(self):
        analyzer = super(TfidfVectorizer, self).build_analyzer()
        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))
</code></pre>

<p>My vectorizer looks like the one below. Among other things, I am trying to test the sensitivity to n_grams of size 4; but I am not sure if that's an optimal parameter or not.</p>

<pre><code>vectorizer = StemmedTfidfVectorizer(min_df = 1, stop_words = 'english', decode_error ='ignore', ngram_range=(1, 4))
</code></pre>

<p>My 'new post' to classify gets transformed into a vector, which is then compared to the other vectors that represent the categories where I want to compare my 'new post' vector. Although the classifier is doing a good job for some tags, for some other tags the best category that describes the post is the 2nd highest score, not the first.</p>

<p>I suspect that my problem is the distance metric that I am using to compare vectors, which is a simple Euclidean distance.</p>

<pre><code>def dist_norm(v1, v2):
    v1_normalized = v1/sp.linalg.norm(v1.toarray())
    v2_normalized = v2/sp.linalg.norm(v2.toarray())
    delta = v1_normalized - v2_normalized
    return sp.linalg.norm(delta.toarray())
</code></pre>

<p>My questions are:
1) Are there other distance metrics that can be used? 
2) How can I modify dist_norm to accommodate other distance metrics?
3) For the ML experts out there, is my problem a feature engineering problem or a distance metric problem? I currently have 7 large samples with over 1MM features (using ngram size 4 might be an overkill)
4) Are there any ipython notebook or classic tutorials to follow for text classification into several categories? (For example, a topic that can be classified as both ""politics"" and ""people"", or some ""fuzzy metric"" to choice 2 tags instead of one.</p>

<p>Thanks</p>
",Text Classification / Sentiment Analysis,choice distance metric sklearn feature extraction text feature engineering following tutorial building machine learning system python modifiying go trying classify new post belonging one different category vectorizer look like one among thing trying test sensitivity n gram size sure optimal parameter new post classify get transformed vector compared vector represent category want compare new post vector although classifier good job tag tag best category describes post nd highest score first suspect problem distance metric using compare vector simple euclidean distance question distance metric used modify dist norm accommodate distance metric ml expert problem feature engineering problem distance metric problem currently large sample mm feature using ngram size might overkill ipython notebook classic tutorial follow text classification several category example topic classified politics people fuzzy metric choice tag instead one thanks
"Python Sentiment Analysis (When comparing words, the repeated word in the text is not counted)","<p>I have this code that that is supposed to compare a positive corpus of words to a subject text. It was doing fine until I discovered that the repeated text is not factored.</p>

<p><strong>Text:</strong> this is a very good movie, it is so good</p>

<p><strong>Positive List:</strong> good, better etc..</p>

<p>The script only counted ""good"" once in the following implementation:</p>

<pre><code> readFile = open('test.txt','r').read()
    readFileList = readFile.split('\n')

    counter = 0


    for eachNeg in negWords:
            if eachNeg in readFile:
                    counter -= 1
                    print eachNeg
    print counter


    for eachPos in posWords:
            if eachPos in readFile:
                    counter +=1
                    print eachPos
    print counter
</code></pre>
",Text Classification / Sentiment Analysis,python sentiment analysis comparing word repeated word text counted code supposed compare positive corpus word subject text wa fine discovered repeated text factored text good movie good positive list good better etc script counted good following implementation
How much text can handle scikit-learn?,"<p>I have a sentiment analysis task and i need to specify how much data (in my case text) does scikit can handle. I have a corpus of 2500 opinions all ready tagged. I now that it´s a small corpus but my thesis advisor is asking me to specifically argue how many data does scikit learn can handle. My advisor has his doubts about python/scikit and she wants facts about how many text parameters, featues and stuff related can handle scikit-learn.</p>
",Text Classification / Sentiment Analysis,much text handle scikit learn sentiment analysis task need specify much data case text doe scikit handle corpus opinion ready tagged small corpus thesis advisor asking specifically argue many data doe scikit learn handle advisor ha doubt python scikit want fact many text parameter featues stuff related handle scikit learn
How much text can Weka handle?,"<p>I have a sentiment analysis task and I need to specify how much data (in my case text) weka can handle. I have a corpus of 2500 opinions already tagged. I know that it´s a small corpus but my thesis advisor is asking me to specifically argue on how much data can Weka handle.</p>
",Text Classification / Sentiment Analysis,much text weka handle sentiment analysis task need specify much data case text weka handle corpus opinion already tagged know small corpus thesis advisor asking specifically argue much data weka handle
Any advice for crawling data from TripAdvisor,"<p>We are currently working on an NLP project and in need of a corpus that is intended to be extracted from tripadvisor.com. We are expecting the output as a couple of types: comment and the rating of that comment. My question is:</p>

<ul>
<li><p>Is there any crawling tool best for this purpose? It must be easy to use and python is preferred. Beautiful Soup is what I found but I wanted to ask it here for any other recommendations.</p></li>
<li><p>Is there any complete tool just for this purpose? I mean a program that is written for tripadvisor.com?  </p></li>
<li><p>Any other recommendation regarding data(comment/rating) crawling from giant web-sites will be appreciated. </p></li>
</ul>

<p>The corpus will be used in sentiment analysis for university research and we need to crawl it as soon as possible.</p>
",Text Classification / Sentiment Analysis,advice crawling data tripadvisor currently working nlp project need corpus intended extracted tripadvisor com expecting output couple type comment rating comment question crawling tool best purpose must easy use python preferred beautiful soup found wanted ask recommendation complete tool purpose mean program written tripadvisor com recommendation regarding data comment rating crawling giant web site appreciated corpus used sentiment analysis university research need crawl soon possible
weka 3.7 explorer cannot classify text,"<p>I am trying to do text classification using weka 3.7 explorer. I converted 2 text files( separated into two dir class1 and class2) into arff using text loader. Before doing so, I standardized the case to lower. Now when I load the file into weka and apply filter stringtowordvector (such as stopwords,usewordcount, usestoplist, stemmer - snowballstemmer) I do not see any change in my list of variables . All the variables (words ) are given as 1 or 0 against each class. </p>

<p>Please help me.</p>

<p>Here is my filter command</p>

<p>weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 1000 -prune-rate -1.0 -C -N 0 -S -stemmer weka.core.stemmers.SnowballStemmer -M 1 -tokenizer ""weka.core.tokenizers.WordTokenizer -delimiters \"" \r\n\t.,;:\\'\\""()?!\""""</p>
",Text Classification / Sentiment Analysis,weka explorer classify text trying text classification using weka explorer converted text file separated two dir class class arff using text loader standardized case lower load file weka apply filter stringtowordvector stopwords usewordcount usestoplist stemmer snowballstemmer see change list variable variable word given class please help filter command weka filter unsupervised attribute stringtowordvector r first last w prune rate c n stemmer weka core stemmer snowballstemmer tokenizer weka core tokenizers wordtokenizer delimiters r n
Unsupervised feature learning from raw text as a previous step for clasification?,"<p>I have a corpus of 2500 opinions, is it posible to use scikit´s restricted boltzmann machine implementation to extract a feature vector as a previous step to a classification task?. What aproach do i need to follow in order to use a restricted boltzman machine for text classification?, do i need to label my data to classify?</p>
",Text Classification / Sentiment Analysis,unsupervised feature learning raw text previous step clasification corpus opinion posible use scikit restricted boltzmann machine implementation extract feature vector previous step classification task aproach need follow order use restricted boltzman machine text classification need label data classify
NLTK classification and WordNet with Text Blob,"<p>I have the following two sets. The idea is to be able to classify news articles based on a few meta tags I am provided with. For example when I get an article that has ""Judge"" ""5 Years"" then it should be classified as a crime story</p>

<pre><code>train = [
             ('Honda', 'cars'),
             ('Ford', 'cars'),
             ('Volkswagen', 'cars'),
             ('Courthouse', 'crime'),
             ('Police', 'crime'),
             ('Taurus', 'cars'),
             ('Chevrolet', 'cars'),
             ('Sonic', 'cars'),
             ('Judge', 'crime'),
             ('Jail', 'crime')
             ]
    test = [
            ('Porsche', 'cars'),
            ('Toyota', 'cars'),
            ('Arrest', 'crime'),
            ('Prison', 'crime')
            ]

    cl = NaiveBayesClassifier(train)
</code></pre>

<p>The problem is that when I run this:</p>

<pre><code>for a, b in test:
        print a, cl.classify(a)
</code></pre>

<p>It classifies everything as ""cars""</p>

<p>I am sure I am missing the semantic similarity comparision here. I tried using WordNet through text blob.</p>

<p>I ran</p>

<pre><code>word = Word(""Volkswagen"")
for each in word.definitions:
    print each
</code></pre>

<p>but It does not give me any results.</p>

<p>The question now is:</p>

<p>How do I get WordNet to say that Volkswagen is a car, integrate that into the classifier so that it will realize that Hyndai is also a car and classify it correctly?</p>
",Text Classification / Sentiment Analysis,nltk classification wordnet text blob following two set idea able classify news article based meta tag provided example get article ha judge year classified crime story problem run classifies everything car sure missing semantic similarity comparision tried using wordnet text blob ran doe give result question get wordnet say volkswagen car integrate classifier realize hyndai also car classify correctly
rapidminer and sentiment analysis,"<p>Is anyone out there used Rapidminer for sentiment analysis... Is this a right combination???</p>

<p>If not how do I get started with a simple sentiment analysis application??</p>
",Text Classification / Sentiment Analysis,rapidminer sentiment analysis anyone used rapidminer sentiment analysis right combination get started simple sentiment analysis application
Twitter Sentiments Analysis useful features,"<p>I'm trying to implement Sentiments Analysis functionality and looking for useful features which can be extracted from tweet messages.The features which I have in my mind for now are:</p>

<ol>
<li>Sentiment words</li>
<li>Emotion icons</li>
<li>Exclamation marks</li>
<li>Negation words</li>
<li>Intensity words(very,really etc)</li>
</ol>

<p>Is there any other useful features for this task?
My goal is not only detect that tweet is positive or negative but also I need to detect level of positivity or negativity(let say in a scale from 0 to 100).
Any inputs or references to printed papers are very welcome.</p>

<p>Thanks.</p>
",Text Classification / Sentiment Analysis,twitter sentiment analysis useful feature trying implement sentiment analysis functionality looking useful feature extracted tweet message feature mind sentiment word emotion icon exclamation mark negation word intensity word really etc useful feature task goal detect tweet positive negative also need detect level positivity negativity let say scale input reference printed paper welcome thanks
Feature Construction for Text Classification using Autoencoders,"<p>Autoencoders can be used to reduce dimensionallity in feature vectors - as far as I understand. In text classification a feature vector is normally constructed via a dictionary - which tends to be extremely large. I have no experience in using autoencoders, so my questions are:</p>

<ol>
<li>Could autoencoders be used to reduce dimensionallity in text classification? (Why? / Why not?)</li>
<li>Has anyone already done this? A source would be nice, if so.</li>
</ol>
",Text Classification / Sentiment Analysis,feature construction text classification using autoencoders autoencoders used reduce dimensionallity feature vector far understand text classification feature vector normally constructed via dictionary tends extremely large experience using autoencoders question could autoencoders used reduce dimensionallity text classification ha anyone already done source would nice
Simple statistical yes/no classifier in WEKA,"<p>In order for me to compare my results of my research in labeled text classification, I need to have a baseline to compare with. One of my colleagues told me one solution would be to make the most easiest and dumbest classifier possible. The classifier makes a decision based on the frequency of a particular label. 
This means that, when in my dataset I have a total of 100 samples and when it knows 80% of these samples have the label A, it will classify a sample as 'A' in 80% of the time. Since my entire research is using the Weka API, I have looked into the documentation but unfortunatly haven't found anything about this.</p>

<p>So my question is, is it possible in Weka to implement such a classifier and yes, could someone point out how this is possible? This question is pure informative since I looked into this thing but did not find anything, here is where I hope to find an answer.</p>
",Text Classification / Sentiment Analysis,simple statistical yes classifier weka order compare result research labeled text classification need baseline compare one colleague told one solution would make easiest dumbest classifier possible classifier make decision based frequency particular label mean dataset total sample know sample label classify sample time since entire research using weka api looked documentation unfortunatly found anything question possible weka implement classifier yes could someone point possible question pure informative since looked thing find anything hope find answer
Text classification vs. Sentence classification,"<p>What's the difference between the two? Articles seem to treat them differently... that is, a paper would show research on either text classification <strong>or</strong> on sentence classification.</p>

<p>I wonder - if one applied sentence classification on a whole text, and then classified the paragraph according to what most of its sentences were classified to - would that count as proper text classification? or does text classification have a different 'catch'? </p>
",Text Classification / Sentiment Analysis,text classification v sentence classification difference two article seem treat differently paper would show research either text classification sentence classification wonder one applied sentence classification whole text classified paragraph according sentence classified would count proper text classification doe text classification different catch
Features in SVM based Sentiment Analysis,"<p>I am unable to convert semantic and lexical information into feature vectors.</p>

<p>I know the following information </p>

<ul>
<li>Part of Speech tag - output of POS tagger ex Adjective,verb</li>
<li>Word Sense- output of Word Sense Disambiguation ex Bank - financial institution,heap</li>
<li>Ontological information - ex mammal,Location</li>
<li>n-gram - ex good-boy</li>
<li>Head word - ex act root word of acting</li>
</ul>

<p>My question is how to represent them as real values.Should I just just choose the occurrence of each of the feature(POS,sense,etc..) i.e. boolean vector but then the semantic information will be lost in case of n-grams(ex very good boy and good boy have different semantic orientation in case of sentiment analysis). </p>
",Text Classification / Sentiment Analysis,feature svm based sentiment analysis unable convert semantic lexical information feature vector know following information part speech tag output po tagger ex adjective verb word sense output word sense disambiguation ex bank financial institution heap ontological information ex mammal location n gram ex good boy head word ex act root word acting question represent real value choose occurrence feature po sense etc e boolean vector semantic information lost case n gram ex good boy good boy different semantic orientation case sentiment analysis
Text classification using Java,"<p>I need to categorize a text or word to a particular category. For example, the text 'Pink Floyd' should be categorized as 'music' or 'Wikimedia' as 'technology' or 'Einstein' as 'science'.  </p>

<p>How can this be done? Is there a way I can use the <a href=""http://dbpedia.org/"" rel=""nofollow"">DBpedia</a> for the same? If not, the database has to be trained from time to time, right?</p>
",Text Classification / Sentiment Analysis,text classification using java need categorize text word particular category example text pink floyd categorized music wikimedia technology einstein science done way use dbpedia database ha trained time time right
How to asses sentiment on &quot;double negative&quot; sentences,"<p>This might be very trivial question. But, I am not able to find the answer I am looking for by doing a google or SO search.</p>

<p>I want to build a simple sentiment analysis logic.</p>

<p>There are two list of negative and positive words/phrases with some score how much negative or positive that word/phrases are.</p>

<p>Negative phrase/words:</p>

<pre><code>really hate         -2
hate         -1
dislike      -1
</code></pre>

<p>Positive phrase/words:</p>

<pre><code>like           1   
very much like 2
</code></pre>

<p>Following would be corresponding sentiment scores</p>

<pre><code>""I like to eat Apple""          =&gt; Sentiment Score = 1
""I really hate the college president""  =&gt; Sentiment Score = -2
</code></pre>

<p>It is fine for above cases. But, I wonder how to solve following instances:</p>

<pre><code>""Though I am an Android user, I hate to say I like iPhone 5s""
</code></pre>

<p>You see, ""hate"" and ""like"" both will make sentiment score=0
But, I expect it to be +1 because it has positive sentiment about iPhone 5s.</p>
",Text Classification / Sentiment Analysis,ass sentiment double negative sentence might trivial question able find answer looking google search want build simple sentiment analysis logic two list negative positive word phrase score much negative positive word phrase negative phrase word positive phrase word following would corresponding sentiment score fine case wonder solve following instance see hate like make sentiment score expect ha positive sentiment iphone
Sentiment Analysis using perceptron,"<p>I am trying to implement Sentiment analysis using perceptron to get a better accuracy in python. I am lost in  the maths that sorounds it and need easy explanation on how to port it to be used for sentiment analysis. There is already  a paper published on the same : <a href=""http://aclweb.org/anthology/P/P11/P11-1015.pdf"" rel=""nofollow"">http://aclweb.org/anthology/P/P11/P11-1015.pdf</a></p>

<p>Would anyone here be able to explain in detail and clarity ? I have a training datatset and test dataset of 5000 reviews each and am getting an accuracy of 78 percent with bag of words. I have been told perceptron will give me an accuracy of 88% and am curious to implement it. </p>
",Text Classification / Sentiment Analysis,sentiment analysis using perceptron trying implement sentiment analysis using perceptron get better accuracy python lost math sorounds need easy explanation port used sentiment analysis already paper published would anyone able explain detail clarity training datatset test dataset review getting accuracy percent bag word told perceptron give accuracy curious implement
Naive bayes calculation in sql,"<p>I want to use naive bayes to classify documents into a relatively large number of classes. I'm looking to confirm whether an mention of an entity name in an article really is that entity, on the basis of whether that article is similar to articles where that entity has been correctly verified.</p>

<p>Say, we find the text ""General Motors"" in an article. We have a set of data that contains articles and the correct entities mentioned within in. So, if we have found ""General Motors"" mentioned in a new article, should it fall into that class of articles in the prior data that contained a known genuine mention ""General Motors"" vs. the class of articles which did not mention that entity?</p>

<p>(I'm not creating a class for every entity and trying to classify every new article into every possible class. I already have a heuristic method for finding plausible mentions of entity names, and I just want to verify the plausibility of the limited number of entity name mentions per article that the method already detects.)</p>

<p>Given that the number of potential classes and articles was quite large and naive bayes is relatively simple, I wanted to do the whole thing in sql, but I'm having trouble with the scoring query...</p>

<p>Here's what I have so far:</p>

<pre><code>CREATE TABLE `each_entity_word` (
  `word` varchar(20) NOT NULL,
  `entity_id` int(10) unsigned NOT NULL,
  `word_count` mediumint(8) unsigned NOT NULL,
  PRIMARY KEY (`word`, `entity_id`)
);

CREATE TABLE `each_entity_sum` (
  `entity_id` int(10) unsigned NOT NULL DEFAULT '0',
  `word_count_sum` int(10) unsigned DEFAULT NULL,
  `doc_count` mediumint(8) unsigned NOT NULL,
  PRIMARY KEY (`entity_id`)
);

CREATE TABLE `total_entity_word` (
  `word` varchar(20) NOT NULL,
  `word_count` int(10) unsigned NOT NULL,
  PRIMARY KEY (`word`)
);

CREATE TABLE `total_entity_sum` (
  `word_count_sum` bigint(20) unsigned NOT NULL,
  `doc_count` int(10) unsigned NOT NULL,
  `pkey` enum('singleton') NOT NULL DEFAULT 'singleton',
  PRIMARY KEY (`pkey`)
);
</code></pre>

<p>Each article in the marked data is split into distinct words, and for each article for each entity every word is added to <code>each_entity_word</code> and/or its <code>word_count</code> is incremented and <code>doc_count</code> is incremented in <code>entity_word_sum</code>, both with respect to an <code>entity_id</code>. This is repeated for each entity known to be mentioned in that article.</p>

<p>For each article regardless of the entities contained within for each word <code>total_entity_word</code> <code>total_entity_word_sum</code> are similarly incremented.</p>

<ul>
<li>P(word|any document) should equal the
<code>word_count</code> in <code>total_entity_word</code> for that word over
<code>doc_count</code> in <code>total_entity_sum</code></li>
<li>P(word|document mentions entity <em>x</em>)
should equal <code>word_count</code> in
<code>each_entity_word</code> for that word for <code>entity_id</code> <em>x</em> over <code>doc_count</code> in
<code>each_entity_sum</code> for <code>entity_id</code> <em>x</em></li>
<li>P(word|document does <em>not</em> mention entity <em>x</em>) should equal (the <code>word_count</code> in <code>total_entity_word</code> minus its <code>word_count</code> in <code>each_entity_word</code> for that word for that entity) over (the <code>doc_count</code> in <code>total_entity_sum</code> minus <code>doc_count</code> for that entity in <code>each_entity_sum</code>)</li>
<li>P(document mentions entity <em>x</em>) should equal <code>doc_count</code> in <code>each_entity_sum</code> for that entity id over <code>doc_count</code> in <code>total_entity_word</code></li>
<li>P(document does not mention entity <em>x</em>) should equal 1 minus (<code>doc_count</code> in <code>each_entity_sum</code> for <em>x</em>'s entity id over <code>doc_count</code> in <code>total_entity_word</code>).</li>
</ul>

<p>For a new article that comes in, split it into words and just select where word in ('I', 'want', 'to', 'use'...) against either <code>each_entity_word</code> or <code>total_entity_word</code>. In the db platform I'm working with (mysql) IN clauses are relatively well optimized.</p>

<p>Also there is no product() aggregate function in sql, so of course you can just do sum(log(x)) or exp(sum(log(x))) to get the equivalent of product(x).</p>

<p>So, if I get a new article in, split it up into distinct words and put those words into a big IN() clause and a potential entity id to test, how can I get the naive bayesian probability that the article falls into that entity id's class in sql?</p>

<p>EDIT:</p>

<p>Try #1:</p>

<pre><code>set @entity_id = 1;

select @entity_doc_count = doc_count from each_entity_sum where entity_id=@entity_id;

select @total_doc_count = doc_count from total_entity_sum;

select 
            exp(

                log(@entity_doc_count / @total_doc_count) + 

                (
                    sum(log((ifnull(ew.word_count,0) + 1) / @entity_doc_count)) / 
                    sum(log(((aew.word_count + 1) - ifnull(ew.word_count, 0)) / (@total_doc_count - @entity_doc_count)))
                )

            ) as likelihood,
        from total_entity_word aew 
        left outer join each_entity_word ew on ew.word=aew.word and ew.entity_id=@entity_id

        where aew.word in ('I', 'want', 'to', 'use'...);
</code></pre>
",Text Classification / Sentiment Analysis,naive bayes calculation sql want use naive bayes classify document relatively large number class looking confirm whether mention entity name article really entity basis whether article similar article entity ha correctly verified say find text general motor article set data contains article correct entity mentioned within found general motor mentioned new article fall class article prior data contained known genuine mention general motor v class article mention entity creating class every entity trying classify every new article every possible class already heuristic method finding plausible mention entity name want verify plausibility limited number entity name mention per article method already detects given number potential class article wa quite large naive bayes relatively simple wanted whole thing sql trouble scoring query far article marked data split distinct word article entity every word added incremented incremented respect repeated entity known mentioned article article regardless entity contained within word similarly incremented p word document equal word p word document mention entity x equal word x x p word document doe mention entity x equal minus word entity minus entity p document mention entity x equal entity id p document doe mention entity x equal minus x entity id new article come split word select word want use either db platform working mysql clause relatively well optimized also product aggregate function sql course sum log x exp sum log x get equivalent product x get new article split distinct word put word big clause potential entity id test get naive bayesian probability article fall entity id class sql edit try
Is TF-IDF necessary when using SVM?,"<p>I'm using Support Vector Machines to classify phrases. Before using the SVM, I understand I should do some kind of normalization on the phrase-vectors. One popular method is TF-IDF.</p>

<p>The terms with the highest TF-IDF score are often the terms that best characterize the topic of the document.</p>

<p>But isn't that exactly what SVM does anyway? Giving the highest weight to the terms that best characterize the document?</p>

<p>Thanks in advance :-)</p>
",Text Classification / Sentiment Analysis,tf idf necessary using svm using support vector machine classify phrase using svm understand kind normalization phrase vector one popular method tf idf term highest tf idf score often term best characterize topic document exactly svm doe anyway giving highest weight term best characterize document thanks advance
&quot;Consensus&quot; Among Maximum Entropy Classifications,"<p>Imagine we have three classes: A, B, and C, and we classify a document 'd' using a standard MaxEnt classifier, and come up with the following probabilities:</p>

<pre><code>P(d, A) = 0.50
P(d, B) = 0.25
P(d, C) = 0.25
</code></pre>

<p>I feel like that is very different, in a way, from this set of probabilities:</p>

<pre><code>P(d, A) = 0.50
P(d, B) = 0.49
P(d, C) = 0.01
</code></pre>

<p>Is there a way to score the difference between these two?</p>
",Text Classification / Sentiment Analysis,consensus among maximum entropy classification imagine three class b c classify document using standard maxent classifier come following probability feel like different way set probability way score difference two
Text Classification - using stemmer degrades results?,"<p>There's <a href=""http://www.cs.indiana.edu/~mkorayem/paper/survey_Arabic.pdf"" rel=""nofollow"">this</a> article about sentiment analysis of Arabic. </p>

<p>In the beginning of page 5 it says that:</p>

<blockquote>
  <p>""Experiments also show that stemming words before feature extraction and classification nearly always degrades the results"".</p>
</blockquote>

<p>Later on in the same page, they state that:</p>

<blockquote>
  <p>""...and an Arabic light stemmer is used for stemming the words""</p>
</blockquote>

<p>Um I thought that a stemmer/lemmatizer was <em>always</em> used before text classifications, why does he say that it degrades the results?</p>

<p>Thanks :)</p>
",Text Classification / Sentiment Analysis,text classification using stemmer degrades result article sentiment analysis arabic beginning page say experiment also show stemming word feature extraction classification nearly always degrades result later page state arabic light stemmer used stemming word um thought stemmer lemmatizer wa always used text classification doe say degrades result thanks
NLP to classify/label the content of a sentence (Ruby binding necesarry),"<p>I am analysing a few million emails. My aim is to be able to classify then into groups. Groups could be e.g.:</p>

<ul>
<li><strong>Delivery problems</strong> (slow delivery, slow handling before dispatch, incorrect availability information, etc.)</li>
<li><strong>Customer service problems</strong> (slow email response time, impolite response, etc.)</li>
<li><strong>Return issues</strong> (slow handling of return request, lack of helpfulness from the customer service, etc.)</li>
<li><strong>Pricing complaint</strong> (hidden fee's discovered, etc.)</li>
</ul>

<p>In order to perform this classification, I need a NLP that can recognize the combination of word groups like:</p>

<ul>
<li><em>""[they|the company|the firm|the website|the merchant]""</em></li>
<li><em>""[did not|didn't|no]""</em></li>
<li><em>""[response|respond|answer|reply]""</em></li>
<li><em>""[before the next day|fast enough|at all]""</em></li>
<li>etc.</li>
</ul>

<p>A few of these exemplified groups in combination should then match sentences like:</p>

<ul>
<li>""They didn't respond""</li>
<li>""They didn't respond at all""</li>
<li>""There was no response at all""</li>
<li>""I received no response from the website""</li>
</ul>

<p>And then classify the sentence as <strong>Customer service problems</strong>.</p>

<p>Which NLP would be able to handle such a task? From what I read these are the most relevant:</p>

<ul>
<li>Stanford CoreNLP</li>
<li>OpenNLP</li>
</ul>

<p>Check also <a href=""https://stackoverflow.com/questions/999410/natural-language-processing-in-ruby/10056667#10056667"">these suggested NLP's</a>.</p>
",Text Classification / Sentiment Analysis,nlp classify label content sentence ruby binding necesarry analysing million email aim able classify group group could e g delivery problem slow delivery slow handling dispatch incorrect availability information etc customer service problem slow email response time impolite response etc return issue slow handling return request lack helpfulness customer service etc pricing complaint hidden fee discovered etc order perform classification need nlp recognize combination word group like company firm website merchant response respond answer reply next day fast enough etc exemplified group combination match sentence like respond respond wa response received response website classify sentence customer service problem nlp would able handle task read relevant stanford corenlp opennlp check also href suggested nlp
Text Classification - how to find the features that most affected the decision,"<p>When using <code>SVMlight</code> or <code>LIBSVM</code> in order to classify phrases as positive or negative (Sentiment Analysis), is there a way to determine which are the most influential words that affected the algorithms decision? For example, finding that the word <code>""good""</code> helped determine a phrase as positive, etc. </p>
",Text Classification / Sentiment Analysis,text classification find feature affected decision using order classify phrase positive negative sentiment analysis way determine influential word affected algorithm decision example finding word helped determine phrase positive etc
Apache Stanbol sentiment analysis and sentence detection not working,"<p>I am using Apache Stanbol. It works for enhancing the text, however when I tried sentiment analysis and sentence detection, it doesn't work.</p>

<p>I tried this code</p>

<pre><code>curl -v -X POST -H ""Accept: text/plain"" -H ""Content-type: text/plain; \
charset=UTF-8"" --data ""Some text for analysis"" \
""http://localhost:8081/enhancer/engine/sentiment-wordclassifier""
</code></pre>

<p>But it gives blank <code>{ }</code> output, I tried changing the header attributes but no luck.</p>

<p>am I missing something? Do I need to do some configuration first? </p>

<p>I even tried adding analyzer in the enhancer chain but the same blank output, also tried REST API for <code>opennlp-sentence</code>, but it didn't work.</p>
",Text Classification / Sentiment Analysis,apache stanbol sentiment analysis sentence detection working using apache stanbol work enhancing text however tried sentiment analysis sentence detection work tried code give blank output tried changing header attribute luck missing something need configuration first even tried adding analyzer enhancer chain blank output also tried rest api work
Ruby Text/Sentiment Analysis,"<p>I have two strings -</p>

<p><code>""I like running around the track.</code></p>

<p><code>I like swimming in the pool, but only in the morning.</code></p>

<p>I need to pull out what people ""like"" from the above two comments (<code>running around the track</code> and <code>swimming in the pool</code>.</p>

<p>Does anyone have a recommendation for a text analytics gem or other method of pulling in that kind of information? I don't necessarily need word counts or n-grams, I just want to know what words are seen in relation to the word ""<code>like</code>"".</p>
",Text Classification / Sentiment Analysis,ruby text sentiment analysis two string need pull people like two comment doe anyone recommendation text analytics gem method pulling kind information necessarily need word count n gram want know word seen relation word
How to determine topic of given document (text)?,"<p>I know how to classify texts through Weka, I can insert a folder of texts in Weka GUI and trying different algorithms it can show me if one of the texts is positive/negative to some topic. </p>

<p>Now I need something different, I want to build an application that when the user inserts a single-text file, to tell the topic of the text. </p>

<p>Is there any possibility to do this by Weka? If anyone would give me a hint I will be very pleased. </p>
",Text Classification / Sentiment Analysis,determine topic given document text know classify text weka insert folder text weka gui trying different algorithm show one text positive negative topic need something different want build application user insert single text file tell topic text possibility weka anyone would give hint
How to generate pertinent text?,"<p>What I want to do is, get a text training set (natural language) and increase this set with automatically created text that tries to mimic the text content. I'm using a bag-of-words assumption, sequence doesn't matter, syntax doesn't matter, I just want to create text that contains words that is pertinent with the general topic of the base.</p>

<p>Right now I'm using <strong>Latent Dirichlet Allocation</strong> to classify my documents in topics distributions, average the topic distribution of my set, and generate documents from these topic distribution.</p>

<p>I want to know two things:</p>

<blockquote>
  <p>1- Is there a better way to do that?</p>
  
  <p>2- Can I train LDA with texts that are not of the domain of my set,
  without tainting my topics: Eg. The set that I want to increase has
  texts about politics. Can I train my model with any kind of text
  (cars, fashion, musics) and classificates my base of politics text get its topics distributions and generates similar text from this distribution.</p>
</blockquote>

<p>I'm using python 2.7 and gensim.</p>
",Text Classification / Sentiment Analysis,generate pertinent text want get text training set natural language increase set automatically created text try mimic text content using bag word assumption sequence matter syntax matter want create text contains word pertinent general topic base right using latent dirichlet allocation classify document topic distribution average topic distribution set generate document topic distribution want know two thing better way train lda text domain set without tainting topic eg set want increase ha text politics train model kind text car fashion music classificates base politics text get topic distribution generates similar text distribution using python gensim
Is it possible to supplement Naive Bayes text classification algorithm with author information?,"<p>I am working on a text classification project where I am trying to assign topic classifications to speeches from the Congressional Record. </p>

<p>Using topic codes from the Congressional Bills Project (<a href=""http://congressionalbills.org/"" rel=""nofollow"">http://congressionalbills.org/</a>), I've tagged speeches that mention a specific bill as belonging to the topic of the bill. I'm using this as my ""training set"" for the model.</p>

<p>I have a ""vanilla"" Naive Bayes classifier working well-enough, but I keep feeling like I could get better accuracy out of the algorithm by incorporating information about the member of Congress who is making the speech (e.g. certain members are much more likely to talk about Foreign Policy than others).</p>

<p>One possibility would be to replace the prior in the NB classifier (usually defined as the proportion of documents in the training set that have the given classification) with speaker's observed prior speeches.</p>

<p>Is this worth pursuing? Are there existing approaches that have followed this same kind of logic? I'm a little bit familiar with the ""author-topic models"" that come out of Latent Dirichlet Allocation models, but I like the simplicity of the NB model.</p>
",Text Classification / Sentiment Analysis,possible supplement naive bayes text classification algorithm author information working text classification project trying assign topic classification speech congressional record using topic code congressional bill project tagged speech mention specific bill belonging topic bill using training set model vanilla naive bayes classifier working well enough keep feeling like could get better accuracy algorithm incorporating information member congress making speech e g certain member much likely talk foreign policy others one possibility would replace prior nb classifier usually defined proportion document training set given classification speaker observed prior speech worth existing approach followed kind logic little bit familiar author topic model come latent dirichlet allocation model like simplicity nb model
Predicting Classifications with Naive Bayes and dealing with Features/Words not in the training set,"<p>Consider the text classification problem of spam or not spam with the Naive Bayes algorithm.</p>

<p>The question is the following:</p>

<p>how do you make predictions about a document W =  if in that set of words you see a new word wordX that was not seen at all by your model (so you do not even have a laplace smoothing probabilty estimated for it)?</p>

<p>Is the usual thing to do is just ignore that wordX eventhough it was seen in the current text because it has no probability associated with? I.e. I know sometimes the laplace smoothing is used to try to solve this problem, but what if that word is definitively new?</p>

<p>Some of the solutions that I've thought of:</p>

<p>1) Just ignore that words in estimating a classification (most simple, but sometimes wrong...?, however, if the training set is large enough, this is probably the best thing to do, as I think its reasonable to assume your features and stuff were selected well enough if you have say 1M or 20M data).</p>

<p>2) Add that word to your model and change your model completely, because the vocabulary changed so probabilities have to change everywhere (this does have a problem though since it could mean that you have to update the model frequently, specially if your analysis 1M documents, say)</p>

<p>I've done some research on this, read some of the Dan Jurafsky NLP and NB slides and watched some videos on coursera and looked through some research papers but I was not able to find something I found useful. It feels to me this problem is not new at all and there should be something (a heuristic..?) out there. If there isn't, it would be awesome to know that too!</p>

<p>Hope this is a useful post for the community and Thanks in advance.</p>

<p>PS: to make the issue a little more explicit with one of the solutions I've seen is, say that we see an unknown new word wordX in a spam, then for that word we can do 1/ count(spams) + |Vocabulary + 1|, the issue I have with doing something like that is that, then, does that mean we change the size of the vocabulary and now, every new document we classify, has a new feature and vocabulary word? This video seems to attempt to solve that issue but I'm not sure if either, thats a good thing to do or 2, maybe I have misunderstood it:</p>

<p><a href=""https://class.coursera.org/nlp/lecture/26"" rel=""nofollow"">https://class.coursera.org/nlp/lecture/26</a></p>
",Text Classification / Sentiment Analysis,predicting classification naive bayes dealing feature word training set consider text classification problem spam spam naive bayes algorithm question following make prediction document w set word see new word wordx wa seen model even laplace smoothing probabilty estimated usual thing ignore wordx eventhough wa seen current text ha probability associated e know sometimes laplace smoothing used try solve problem word definitively new solution thought ignore word estimating classification simple sometimes wrong however training set large enough probably best thing think reasonable assume feature stuff selected well enough say data add word model change model completely vocabulary changed probability change everywhere doe problem though since could mean update model frequently specially analysis document say done research read dan jurafsky nlp nb slide watched video coursera looked research paper wa able find something found useful feel problem new something heuristic would awesome know hope useful post community thanks advance p make issue little explicit one solution seen say see unknown new word wordx spam word count spam vocabulary issue something like doe mean change size vocabulary every new document classify ha new feature vocabulary word video seems attempt solve issue sure either thats good thing maybe misunderstood
train nltk classifier for just one label,"<p>I am just starting out with nltk, and I am following the book. Chapter six is about text classification, and i am a bit confused about something. In the examples (the names, and movie reviews) the classifier is trained to select between two well-defined labels (male-female, and pos-neg). But how to train if you have only one label. </p>

<p>Say I have a bunch of movie plot outlines, and I am only interested in fishing out movies from the sci-fi genre. Can I train a classifier to only recognize sci-fi plots, en say f.i. if classification confidence is > 80%, then put it in the sci-fi group, otherwise, just ignore it.</p>

<p>Hope somebody can clarify, thank you,</p>
",Text Classification / Sentiment Analysis,train nltk classifier one label starting nltk following book chapter six text classification bit confused something example name movie review classifier trained select two well defined label male female po neg train one label say bunch movie plot outline interested fishing movie sci fi genre train classifier recognize sci fi plot en say f classification confidence put sci fi group otherwise ignore hope somebody clarify thank
Short text classification,"<p>I am about to start a project where my final goal is to classify short texts into classes: ""may be interested in visiting place X"" : ""not interested or neutral"". Place is described by set of keywords (e.g. meals or types of miles like ""chinese food""). So ideally I need some approach to model desire of user based on short text analysis - and then classify based on a desire score or desire probability - is there any state-of-the-art in this field ? Thank you  </p>
",Text Classification / Sentiment Analysis,short text classification start project final goal classify short text class may interested visiting place x interested neutral place described set keywords e g meal type mile like chinese food ideally need approach model desire user based short text analysis classify based desire score desire probability state art field thank
Natural Language Processing - Features for Text Classification,"<p>So I'm trying to classify texts using Weka SVM. So far, my feature vectors used for training the SVM are composed of TF-IDF statistics for unigrams and bigrams that appear in the training texts. But, the results I get from testing the trained SVM model haven't been accurate at all, so can someone give me feedback on my procedure? I am following these steps to classify texts:</p>

<ol>
<li>Construct a dictionary made up of extracted unigrams and bigrams from the training texts</li>
<li>Count how many times each unigram/bigram appears in each training text, as well as how many training texts the unigram/bigram appears in</li>
<li>Use the data from step 2 to calcuate the TF-IDF for each unigram/bigram </li>
<li>For each document, construct a feature vector that is the length of the dictionary, and store the corresponding TF-IDF statistic in each element of the vector (so for example, the first element in the feature vector for document one would correspond to the TF-IDF for the first word in the dictionary relative to document one)</li>
<li>Append class label to each feature vector to distinguish which text belongs to which author</li>
<li>Train SVM using these feature vectors</li>
<li>Feature vectors for the testing texts are constructed in the same way as the training texts, and are classified by the SVM</li>
</ol>

<p>Also, could it be that I need to train the SVM with more features? If so, what features are most effective in this case? Any help would be greatly appreciated, thanks.</p>
",Text Classification / Sentiment Analysis,natural language processing feature text classification trying classify text using weka svm far feature vector used training svm composed tf idf statistic unigrams bigram appear training text result get testing trained svm model accurate someone give feedback procedure following step classify text construct dictionary made extracted unigrams bigram training text count many time unigram bigram appears training text well many training text unigram bigram appears use data step calcuate tf idf unigram bigram document construct feature vector length dictionary store corresponding tf idf statistic element vector example first element feature vector document one would correspond tf idf first word dictionary relative document one append class label feature vector distinguish text belongs author train svm using feature vector feature vector testing text constructed way training text classified svm also could need train svm feature feature effective case help would greatly appreciated thanks
Naive Bayes Text Classifier - determining when a document should be labelled &#39;unclassified&#39;,"<p>I have designed and implemented a Naive Bayes Text Classifier (in Java). I am primarily using it to classify tweets into 20 classes. To determine the probability that a document belongs to a class I use </p>

<pre><code>foreach(class)
{
   Probability = (P(bag of words occurring for class) * P(class)) / P(bag of words occurring globally)
}
</code></pre>

<p>What is the best way to determine if a bag of words really shouldn't belong to any class? I'm aware I could just sent a minimum threshold for P(bag of words occurring for class) and if all the classes are under that threshold then to class the document as unclassifed, however I'm realising this prevents this classifier from being sensitive.</p>

<p>Would an option be to create an Unclassified class and train that with document I deem to be unclassifiable? </p>

<p>Thanks,</p>

<p>Mark</p>

<p>--Edit---</p>

<p>I just had thought - I could set a maximum threshold for P(bag of words occurring globally)*(number of words in document) . This would mean that any documents which mainly consisted of common words (typically the tweets I want to filter out) eg. ""Yes I agree with you"". Would be filtered out. - Your thoughts on this would be appreciated also.</p>

<p>Or perhaps I should find the standard deviation and if it is low determine it should be unclassified?</p>
",Text Classification / Sentiment Analysis,naive bayes text classifier determining document labelled unclassified designed implemented naive bayes text classifier java primarily using classify tweet class determine probability document belongs class use best way determine bag word really belong class aware could sent minimum threshold p bag word occurring class class threshold class document unclassifed however realising prevents classifier sensitive would option create unclassified class train document deem unclassifiable thanks mark edit thought could set maximum threshold p bag word occurring globally number word document would mean document mainly consisted common word typically tweet want filter eg yes agree would filtered thought would appreciated also perhaps find standard deviation low determine unclassified
From where do I get the featx.py module?,"<p>Good day,
I'm trying some examples to classify text from the ""NLTK cookbook"" and the book references to a featx.py module which contains several feature extractors.
The problem is I cannot find that featx.py module anywhere.</p>

<p>Thank you for your help. </p>
",Text Classification / Sentiment Analysis,get featx py module good day trying example classify text nltk cookbook book reference featx py module contains several feature extractor problem find featx py module anywhere thank help
word sense disambiguation in sentiwordnet python,"<p>I'm currently doing research for sentiment analysis in twitter.
i want to combine predefined lexicon resource like sentiwordnet polarity score. and then proceed it with machine learning.
the problem is in getting the correct score of sentiwordnet, previous work always simply choose by the total score of negative and positive polarity of the word meaning.
i mean for example the word ""mad"" can appear 3 times as negative and 2 times as positive words.
most of previous work will automatically average of each polarity.
so i want to disambiguate the words before getting the score so we can really use the sentiwordnet as it should be.
i was thinking by comparing the similarity of target sentence and gloss sentence..
is there any method to compare it? do you think it will works?
if not please share your idea..</p>

<p>i'm completely new to this field and novice python programmer, so i really need advice from you..
thank you..</p>
",Text Classification / Sentiment Analysis,word sense disambiguation sentiwordnet python currently research sentiment analysis twitter want combine predefined lexicon resource like sentiwordnet polarity score proceed machine learning problem getting correct score sentiwordnet previous work always simply choose total score negative positive polarity word meaning mean example word mad appear time negative time positive word previous work automatically average polarity want disambiguate word getting score really use sentiwordnet wa thinking comparing similarity target sentence gloss sentence method compare think work please share idea completely new field novice python programmer really need advice thank
extract data from twitter,"<p>I want to extract tweets at real time, I'm using RapidmMiner to do the sentiment analysis and in order to gather data I prefer to use a tool to extract them automatically from twitter, I think that groovy with json using weka can accomplish this task but I didn't find a tuto to do that  , is there other simple tools to ?</p>
",Text Classification / Sentiment Analysis,extract data twitter want extract tweet real time using rapidmminer sentiment analysis order gather data prefer use tool extract automatically twitter think groovy json using weka accomplish task find tuto simple tool
How to use context-sensitive grammar in sentiment analysis?,"<p>Is it possible to use context-sensitive grammar in sentiment analysis? If yes, then how?
Basically, I want to do some phrase-level analysis.</p>
",Text Classification / Sentiment Analysis,use context sensitive grammar sentiment analysis possible use context sensitive grammar sentiment analysis yes basically want phrase level analysis
How to implement Bag of words feature hashing in python?,"<p>I'm trying to classify a few thousand documents, with a few lines each.  I've used regular bag of words before, but want to use the hashing trick this time, and I'm having trouble understanding the implementation.  There are around 8000 unique words in my data, so I figure 128*128 should be enough</p>

<p>I'm using mostly these sources:</p>

<p><a href=""http://blog.someben.com/2013/01/hashing-lang/"" rel=""nofollow"">http://blog.someben.com/2013/01/hashing-lang/</a>
<a href=""http://www.hpl.hp.com/techreports/2008/HPL-2008-91R1.pdf"" rel=""nofollow"">http://www.hpl.hp.com/techreports/2008/HPL-2008-91R1.pdf</a></p>

<p>Here is my function to generatve feature vectors for each document:</p>

<pre><code>import mmh3

def add_doc(text):
    text = str.split(text)
    d_input = dict()
    for word in text:
        hashed_token = mmh3.hash(word) % 127
        d_input[hashed_token] = d_input.setdefault(hashed_token, 0) + 1
    return(d_input)
</code></pre>

<p>Now I must be doing something wrong, or not understanding something somewhere, because there seem to be a huge amount of collisions.  Any help would be appreciated</p>
",Text Classification / Sentiment Analysis,implement bag word feature hashing python trying classify thousand document line used regular bag word want use hashing trick time trouble understanding implementation around unique word data figure enough using mostly source function generatve feature vector document must something wrong understanding something somewhere seem huge amount collision help would appreciated
Comparison of binary vs tfidf Ngram features in sentiment analysis / classification tasks?,"<p>Simple question again: Is it better to use Ngrams (unigram/ bigrams etc) as simple binary features  or rather use their Tfidf scores in ML models such as Support Vectory Machines for performing NLP tasks such as sentiment analysis or text categorization/classification?</p>
",Text Classification / Sentiment Analysis,comparison binary v tfidf ngram feature sentiment analysis classification task simple question better use ngrams unigram bigram etc simple binary feature rather use tfidf score ml model support vectory machine performing nlp task sentiment analysis text categorization classification
which is better... GATE or RapidMiner,"<p>I've started to write a simple sentiment analysis tool.</p>

<p>Currently I am looking at <a href=""http://gate.ac.uk"" rel=""noreferrer"">GATE</a> and <a href=""http://rapid-i.com/"" rel=""noreferrer"">RapidMiner</a> but being a beginner not able to concentrate on both.</p>

<p>Could someone please tell me which one will be better in terms of usage, learning curve, licensing etc?</p>
",Text Classification / Sentiment Analysis,better gate rapidminer started write simple sentiment analysis tool currently looking gate rapidminer beginner able concentrate could someone please tell one better term usage learning curve licensing etc
Find &quot;near duplicates&quot; strings in R,"<p>I am using R to build a sentiment analysis tool and I am having some problems with duplicates. The main source of data is Twitter, and it looks like many are bypassing twitter own spam filter by adding some random text at the end of each tweet. For example</p>

<pre><code>Click xxxxx to buy the amazing xxxxx for FREE ugjh
</code></pre>

<p>I get tons of those exact tweets with a different random string at the end. They are either from the same user or from different.</p>

<p>Is there any function like <code>duplicated</code> or <code>unique</code> which returns how close 2 strings are and if they are above a certain % dismiss them?</p>

<p>I know doing that will eventually delete real tweets from people saying exactly the same, like</p>

<pre><code>I love xxxx !
</code></pre>

<p>but I will deal with that in the future.</p>

<p>Any tip in the right direction will be much appreciated!</p>
",Text Classification / Sentiment Analysis,find near duplicate string r using r build sentiment analysis tool problem duplicate main source data twitter look like many bypassing twitter spam filter adding random text end tweet example get ton exact tweet different random string end either user different function like return close string certain dismiss know eventually delete real tweet people saying exactly like deal future tip right direction much appreciated
Timeline Detection,"<p>I am trying to do a timeline detection problem using text classification. As a newbie I am confused as to how I can go about with this. Is this  a classification problem? i.e, Can I use the years(timelines) as outcomes and solve this as a classification problem?</p>
",Text Classification / Sentiment Analysis,timeline detection trying timeline detection problem using text classification newbie confused go classification problem e use year timeline outcome solve classification problem
Storing a DynamicLMClassifier (Lingpipe),"<p>I'm using lingpipe for sentiment analysis (following <a href=""http://alias-i.com/lingpipe/demos/tutorial/sentiment/src/PolarityBasic.java"" rel=""nofollow"">this</a> code) and I am trying to store the classifier after I train it. The problem is that after storing it I can't load it:</p>

<p><code>java.lang.ClassCastException: com.aliasi.classify.LMClassifier cannot be cast to com.aliasi.classify.DynamicLMClassifier</code></p>

<p>For storing the classifier I use:</p>

<pre><code>AbstractExternalizable.compileTo(mClassifier, classifierFile);
</code></pre>

<p>and for loading it:</p>

<pre><code>mClassifier = (DynamicLMClassifier&lt;NGramProcessLM&gt;) AbstractExternalizable.readObject(classifierFile);
</code></pre>

<p>Edit:
I should have read the lingpipe's javadoc before. To store the classifier and load it afterwards the <code>mClassifier</code>'s class should be <code>LMClassifier&lt;NGramProcessLM, MultivariateEstimator&gt;</code>. Thus it can be initialized to be a <a href=""http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/DynamicLMClassifier.html#compileTo%28java.io.ObjectOutput%29"" rel=""nofollow""><code>DynamicLMClassifier&lt;NGramProcessLM&gt;</code></a> and during the training phase it is necessary to cast it to <code>DynamicLMClassifier&lt;NGramProcessLM&gt;</code>. After this it can be loaded and used to classify new objects. As @mjv said this makes the classifier no longer trainable.</p>
",Text Classification / Sentiment Analysis,storing dynamiclmclassifier lingpipe using lingpipe sentiment analysis following code trying store classifier train problem storing load storing classifier use loading edit read lingpipe javadoc store classifier load afterwards class thus initialized training phase necessary cast loaded used classify new object mjv said make classifier longer trainable
Classify or keyword match a natural language string or phrase,"<p>This is my first post on StackOverflow, so apologies if it's lacking the right information.</p>
<p>Scenario.</p>
<p>I'm in the process of moving away from the Google Weather API to BOM (Australia) weather service. I've managed to get the weather data from BOM just fine using streamreaders etc, but what I'm stuck on is the image icon that matches the daily forecast.</p>
<p>What I did with the old Google Weather API was quite brutal yet did the trick. The Google Weather API only gave off a couple of different type of forecasts that I could jam together into a string that i could in turn use in an imageURL.</p>
<p>Example of what I did with the Google Weather API...</p>
<p><strong>imageDay1.ImageUrl = &quot;images/weather/&quot; + lbWeatherDay1Cond.Text.Replace(&quot; &quot;, string.Empty) + &quot;.png&quot;;</strong></p>
<blockquote>
<p>&quot;Mostly sunny&quot; = mostlysunny.png</p>
<p>&quot;Sunny&quot; = sunny.png</p>
<p>&quot;Chance of Rain&quot; = chanceofrain.png</p>
<p>&quot;Showers&quot; = showers.png</p>
<p>&quot;Partly cloudy&quot; = partlycloudy.png</p>
</blockquote>
<p>There was on say 15 different possible options for the daily forecast.</p>
<p>The problems I have now and with BOM (Australia Weather Service) is this...</p>
<blockquote>
<p>Possible morning shower</p>
<p>Shower or two, clearing later</p>
<p>So many thousands more.... there is no standard.</p>
</blockquote>
<p>What I'm hoping is that it is possible is some of the great minds on here to create a string from a keyword within this string? Something like &quot;Showers&quot; for &quot;Showers.png&quot; or something a little more complex to recognise &quot;Chance of Showers&quot; as &quot;Chanceshowers.jpg&quot; while keeping &quot;Shower or two&quot; as &quot;Showers.png&quot;.</p>
<p>I'm easy to any ideas or solutions (hopefully in c#). As long as it's very lightweight (the process has to be repeated for the 5 day forecast) and can capture almost any scenario...</p>
<p>At this point of time, I'm carrying on with String.Replace, after String.Replace, after String.Replace option.... It will do for now, but I can't roll it into production like this.</p>
<p>Cheers all!</p>
<p>Trent</p>
",Text Classification / Sentiment Analysis,classify keyword match natural language string phrase first post stackoverflow apology lacking right information scenario process moving away google weather api bom australia weather service managed get weather data bom fine using streamreaders etc stuck image icon match daily forecast old google weather api wa quite brutal yet trick google weather api gave couple different type forecast could jam together string could turn use imageurl example google weather api imageday imageurl image weather lbweatherday cond text replace string empty png mostly sunny mostlysunny png sunny sunny png chance rain chanceofrain png shower shower png partly cloudy partlycloudy png wa say different possible option daily forecast problem bom australia weather service possible morning shower shower two clearing later many thousand standard hoping possible great mind create string keyword within string something like shower shower png something little complex recognise chance shower chanceshowers jpg keeping shower two shower png easy idea solution hopefully c long lightweight process ha repeated day forecast capture almost scenario point time carrying string replace string replace string replace option roll production like cheer trent
Identifying the entity in sentiment analysis using Lingpipe,"<p>I have implemented sentiment analysis using the sentiment analysis module of Lingpipe. I know that they use a Dynamic LR model for this. It just tells me if the test string is a positive sentiment or negative sentiment. What ideas could I use to determine the object for which the sentiment has been expressed? </p>

<p>If the text is categorized as positive sentiment, I would like to get the object for which the sentiment has been expressed - this could be a movie name, product name or others.</p>
",Text Classification / Sentiment Analysis,identifying entity sentiment analysis using lingpipe implemented sentiment analysis using sentiment analysis module lingpipe know use dynamic lr model tell test string positive sentiment negative sentiment idea could use determine object sentiment ha expressed text categorized positive sentiment would like get object sentiment ha expressed could movie name product name others
Why should we perform cosine normalization for SVM feature vectors?,"<p>I was recently playing around with the well known movie review dataset used in binary sentiment analysis. It consists of 1,000 positive and 1,000 negative reviews. While exploring various feature-encodings with unigram features, I noticed that all previous research publications normalize the vectors by their Euclidean norm in order to scale them to unit-length.</p>

<p>In my experiments using Liblinear, however, I found that such length-normalization decreases the classification accuracy significantly. I studied the vectors, and I think this is the reason: the dimension of the vector space is, say, 10,000. As a result, the Euclidean norm of the vectors is very high compared to the individual projections. Therefore, after normalization, all the vectors get very small numbers on each axis (i.e., the projection on an axis).</p>

<p>This surprised me, because all publications in this field claim that they perform cosine normalization, whereas I found that NOT normalizing yields better classification.</p>

<p>Thus my question: is there any specific disadvantage if we don't perform cosine normalization for SVM feature vectors? (Basically, I am seeking a mathematical explanation for this need for normalization).</p>
",Text Classification / Sentiment Analysis,perform cosine normalization svm feature vector wa recently playing around well known movie review dataset used binary sentiment analysis consists positive negative review exploring various feature encoding unigram feature noticed previous research publication normalize vector euclidean norm order scale unit length experiment using liblinear however found length normalization decrease classification accuracy significantly studied vector think reason dimension vector space say result euclidean norm vector high compared individual projection therefore normalization vector get small number axis e projection axis surprised publication field claim perform cosine normalization whereas found normalizing yield better classification thus question specific disadvantage perform cosine normalization svm feature vector basically seeking mathematical explanation need normalization
How can I find if there is comparison between two objects in a sentence or not?,"<p>Good day.
First I would like to apologies for not being very specific in the title, please if anyone knows the correct term for what I need, do let me know.
Here is my question. There is a person X and a person Y and I'm doing sentiment analysis on each person. Here are the two case that are baffling me (have in mind that the cases here are simplified) . </p>

<ul>
<li>Case 1: ""X is outperforming Y""</li>
<li>Case 2: ""X's performance is outstanding while Y's performance is poor""</li>
</ul>

<p>When doing sentiment analysis in Case 2 you can just split the sentence in two and do the analysis separately for the part for X and the part for Y while this would not work for Case 1. There is a relationship between X and Y in #1 which means that in sentences in that type if X is good than Y is bad. 
So here is my question: Is there any way to recognize sentence structures like in case 2. I was thinking of POS tagging but since I'm not a native speaker (and my grammar is bad) it's a bit hard for me to see how this would work.</p>

<p>Thank you very much for your help.</p>
",Text Classification / Sentiment Analysis,find comparison two object sentence good day first would like apology specific title please anyone know correct term need let know question person x person sentiment analysis person two case baffling mind case simplified case x outperforming case x performance outstanding performance poor sentiment analysis case split sentence two analysis separately part x part would work case relationship x mean sentence type x good bad question way recognize sentence structure like case wa thinking po tagging since native speaker grammar bad bit hard see would work thank much help
Sentence Classification (Categorization),"<p>I have been reading about text classification and found several Java tools which are available for classification, but I am still wondering: Is text classification the same as sentence classification!</p>

<p>Is there any tool which focuses on sentence classification?</p>
",Text Classification / Sentiment Analysis,sentence classification categorization reading text classification found several java tool available classification still wondering text classification sentence classification tool focus sentence classification
run a java program with lingpipe classes on a website?,"<p>I am working on a text classification project for a class and I am having some difficulties setting it up correctly. My classification code is in Java and uses methods from the Lingpipe toolkit, but I have to run the program from a website. I've been attempting to put together a servlet for this purpose, and so far have downloaded and set up a container (Tomcat), but I'm finding the process of setting up all the necessary files in the right directories to be complicated. Does anyone out there have any advice as to how to run such a Java program from a website, either using a servlet or not?
Thanks!!!</p>
",Text Classification / Sentiment Analysis,run java program lingpipe class website working text classification project class difficulty setting correctly classification code java us method lingpipe toolkit run program website attempting put together servlet purpose far downloaded set container tomcat finding process setting necessary file right directory complicated doe anyone advice run java program website either using servlet thanks
simple sentiment analysis with java,"<p>I am very new to Sentiment analysis. How can I judge if a given word or sentence is positive or negative. I have to implement it with java. I tried to read something like lingpipe, rapidminer tutorial, but I do not understand. In their examples they use a lot of data. In my case I do not have much data. All I have is a word or a sentence, lets say. I tried to read the questions from stackoverflow too. But they do not help me much.
Thanks in advance.</p>
",Text Classification / Sentiment Analysis,simple sentiment analysis java new sentiment analysis judge given word sentence positive negative implement java tried read something like lingpipe rapidminer tutorial understand example use lot data case much data word sentence let say tried read question stackoverflow help much thanks advance
How to prepare text data for orange SVM train?,"<p>I used NLTK classifiers 2 years ago. Now I want to learn to use orange SVM for text classification. The example for SVM in orange tutorial is iris.tab: </p>

<pre><code>sepal length    sepal width petal length    petal width iris
c   c   c   c   d
                class
5.1 3.5 1.4 0.2 Iris-setosa
4.9 3.0 1.4 0.2 Iris-setosa
</code></pre>

<p>If I want to classify text, how to prepare data. Is it like the below?</p>

<pre><code>token     frequency     tokenlength

the        23             3
for        21             3
at         10             2
</code></pre>

<p>Please give me examples of different ways of preparing data. Can token be seen as label in SVM, if not, how to do it?</p>

<p>Thanks very much in advance.  </p>
",Text Classification / Sentiment Analysis,prepare text data orange svm train used nltk classifier year ago want learn use orange svm text classification example svm orange tutorial iris tab want classify text prepare data like please give example different way preparing data token seen label svm thanks much advance
Collocations in text classification,"<p>Suppose i have trained my classifier and i want to find the right sense of a word in a sentence. One feature people use is called collocation where you consider words to the left/right of the confusing word and position is important . I am curious why this approach works? What information does considering collocations give us that helps us in text classification? Moreover, why is the position important</p>
",Text Classification / Sentiment Analysis,collocation text classification suppose trained classifier want find right sense word sentence one feature people use called collocation consider word left right confusing word position important curious approach work information doe considering collocation give u help u text classification moreover position important
How do you find the subject of a sentence?,"<p>I am new to NLP and was doing research about what language toolkit I should be using to do the following. I would like to do one of the two things which accomplishes the same thing:</p>

<ol>
<li><p>I basically would like to classify a text, usually one sentence that contains 15 words. Would like to classify if the sentence is talking about a specific subject.  </p></li>
<li><p>Is there a tool that given a sentence, it finds out the subject of a sentence.  </p></li>
</ol>

<p>I am using PHP and Java but the tool can be anything that runs on Linux command line</p>

<p>Thank you very much.</p>
",Text Classification / Sentiment Analysis,find subject sentence new nlp wa research language toolkit using following would like one two thing accomplishes thing basically would like classify text usually one sentence contains word would like classify sentence talking specific subject tool given sentence find subject sentence using php java tool anything run linux command line thank much
Dynamic text-pattern detection algorithm?,"<p>I was wondering if such algorithm exists. I have a bunch of text documents and would like to find a pattern among all these documents, if a pattern exists. Please note im NOT trying to classify the documents all i want to do is find a pattern if it exists among some documents. Thanks!</p>
",Text Classification / Sentiment Analysis,dynamic text pattern detection algorithm wa wondering algorithm exists bunch text document would like find pattern among document pattern exists please note im trying classify document want find pattern exists among document thanks
Term Extraction and Sentiment Analysis Open Source Project,"<p>I want to extract important terms from a text and create a domain specific term set. Then I want to learn how these words are used in text, positively or negatively. </p>

<p>Do you know any open source project which will help me to accomplish this tasks?</p>

<p>Edit: </p>

<p>Example Text:</p>

<pre><code>""Although car is not comfortable, I like the design of it.""
</code></pre>

<p>From this text, I want to extract something like these:</p>

<pre><code>design:        positive
comfort(able): negative
</code></pre>
",Text Classification / Sentiment Analysis,term extraction sentiment analysis open source project want extract important term text create domain specific term set want learn word used text positively negatively know open source project help accomplish task edit example text text want extract something like
Doing a hierarchical sentiment analysis with LingPipe,"<p>This is in the context of doing sentiment analysis using LingPipe machine learning tool. I have to classify if a sentence in a big paragraph has a positive/negative sentiment. I know of the following approach in LingPipe</p>

<ol>
<li><p>Classify if the complete paragraph based on its polarity - negative or positive.</p>

<p>Here, I yet don't know the polarity at the sentence level. We are still at the paragraph level. How do I determine the polarity at the sentence level of a paragraph, of whether a sentence in a paragraph is a positive/negative sentence? I know that LingPipe is capable of classifying if a sentence is subjective/objective. So using this approach,,,,</p>

<p>,,,, should I </p></li>
<li><p>First train LingPipe on a large set of sentences that are subjective/objective. </p></li>
<li>Use the trained model to extract all subjective sentences out of a test paragraph.</li>
<li>Train a LingPipe classifier based on the extracted subjective sentences for polarity by manually labeling them as positive/negative.</li>
<li><p>Now used the trained polarity model and feed a test subjective sentence (that is done by passing a sentence through the trained subjective/objective) model, and then determine if the statement is positive/negative?</p>

<p>Does the above approach work? In the above proposed approach, we know that LingPipe is capable of accepting a large textual content (paragraph) for polarity classification. Will it do a good job if we just pass a single subjective sentence for polarity classification? I am confused!</p></li>
</ol>
",Text Classification / Sentiment Analysis,hierarchical sentiment analysis lingpipe context sentiment analysis using lingpipe machine learning tool classify sentence big paragraph ha positive negative sentiment know following approach lingpipe classify complete paragraph based polarity negative positive yet know polarity sentence level still paragraph level determine polarity sentence level paragraph whether sentence paragraph positive negative sentence know lingpipe capable classifying sentence subjective objective using approach first train lingpipe large set sentence subjective objective use trained model extract subjective sentence test paragraph train lingpipe classifier based extracted subjective sentence polarity manually labeling positive negative used trained polarity model feed test subjective sentence done passing sentence trained subjective objective model determine statement positive negative doe approach work proposed approach know lingpipe capable accepting large textual content paragraph polarity classification good job pas single subjective sentence polarity classification confused
Simple Sentiment Analysis,"<p>It appears that the simplest, naivest way to do basic sentiment analysis is with a Bayesian classifier (confirmed by what I'm finding here on SO). Any counter-arguments or other suggestions?</p>
",Text Classification / Sentiment Analysis,simple sentiment analysis appears simplest naivest way basic sentiment analysis bayesian classifier confirmed finding counter argument suggestion
Perl or Java Sentiment Analysis,"<p>I was wondering if anybody knew of any good Perl modules and/or Java classes for sentiment analysis.  I have read about LingPipe, but the program would eventually need to be used for commercial use so something open-source would be better.  I also looked into GATE, but their documentation on sentiment analysis is sparse at best.</p>
",Text Classification / Sentiment Analysis,perl java sentiment analysis wa wondering anybody knew good perl module java class sentiment analysis read lingpipe program would eventually need used commercial use something open source would better also looked gate documentation sentiment analysis sparse best
How to classify text when pre defined categories are not available,"<p>I have a problem and not getting idea which algorithm have to apply. 
I am thinking to apply clustering  in case two but no idea on case one: </p>

<p>I have .5 million credit card activity documents. Each document is well defined and contains 1 transaction per line. The date, the amount, the retailer name, and a short 5-20 word description of the retailer. 
Sample:
2004-11-47,$500,Amazon,An online retailer providing goods and services including books, hardware, music, etc.
Questions:
1. How would classify each entry given no pre defined categories.
2. How would do this if you were given pre defined categories such as ""restaurant"", ""entertainment"", etc.</p>
",Text Classification / Sentiment Analysis,classify text pre defined category available problem getting idea algorithm apply thinking apply clustering case two idea case one million credit card activity document document well defined contains transaction per line date amount retailer name short word description retailer sample amazon online retailer providing good service including book hardware music etc question would classify entry given pre defined category would given pre defined category restaurant entertainment etc
getting into sentiment analysis,"<p>I've got a requirement of determining whether the entered sentence is positive or negative.... First I thought it is something to do with Social Network analysis and later I realised that it is Sentiment analysis. My first question is what is the difference between these two? I think SNA itself uses SA... plz correct me if i am wrong...</p>

<p>Regarding this I got a very good discussion by Alexander @ <a href=""https://stackoverflow.com/questions/122595/nlp-qualitatively-positive-vs-negative-sentence"">NLP: Qualitatively &quot;positive&quot; vs &quot;negative&quot; sentence</a>...</p>

<p>I want to get into this field with the power of open source and preferably with Java (but I am open to others too). Can anyone pls guide me on how to get started and move ahead</p>

<p>Thanks in advance 
Siva</p>
",Text Classification / Sentiment Analysis,getting sentiment analysis got requirement determining whether entered sentence positive negative first thought something social network analysis later realised sentiment analysis first question difference two think sna us sa plz correct wrong regarding got good discussion alexander href quot positive quot v quot negative quot sentence want get field power open source preferably java open others anyone pls guide get started move ahead thanks advance siva
Final Semester project about semantic analysis/information retrieval,"<p>I'm moving to my final year at college Engineering Computer Science  department and i wanted to have my graduation project in a topic related to information retrieval &amp; semantic analysis.</p>

<p>I've had my internship in those topics and i'm very interested to continue in them 
so if you could please give me some examples of good Real projects to do and practice in order to be good in those fields </p>

<p>I want something beyond simple recommendation systems in websites </p>

<p>my related backGround : 
i worked before on making classifiers using wikipedia data 
i have a fair knowledge working with Huge datasources like dbpedia , probase , freebase ..etc 
i know fair knowledge about , NLP , classifiers , semantics , RDF , sentiment analysis 
i have a good web development knowledge
i have a good knowledge about scrapping data from facebook blogs and other websites </p>
",Text Classification / Sentiment Analysis,final semester project semantic analysis information retrieval moving final year college engineering computer science department wanted graduation project topic related information retrieval semantic analysis internship topic interested continue could please give example good real project practice order good field want something beyond simple recommendation system website related background worked making classifier using wikipedia data fair knowledge working huge datasources like dbpedia probase freebase etc know fair knowledge nlp classifier semantics rdf sentiment analysis good web development knowledge good knowledge scrapping data facebook blog website
There is a entity recognizer classifier algorithm that doesn&#39;t needs entire texts for training data?,"<p>I want to recognize some entities on texts that I have and I found a lot of algorithms (NaiveBayes, Hidden Markov Models, Conditional Random Field, etc.), but seems that almost all needs a huge training data to classify the entities.</p>

<p>I want to know if there is some algorithm that can recognize without having texts in training data, but maybe only words representing the data I want to recognize, or maybe some String Patterns, or another way. </p>

<p>The only thing I want to avoid is the necessity of having huge text as training data.</p>
",Text Classification / Sentiment Analysis,entity recognizer classifier algorithm need entire text training data want recognize entity text found lot algorithm naivebayes hidden markov model conditional random field etc seems almost need huge training data classify entity want know algorithm recognize without text training data maybe word representing data want recognize maybe string pattern another way thing want avoid necessity huge text training data
How to use reuters-21578 dataset with svm.net for text classification?,"<p>I've just started an application for text classification and I've read lots of papers about this topic, but till now I don't know how to start, I feel like I've not got the whole image. I've got the training dataset and read its description and got a great implementation for SVM algorithm (SVM.Net) but I don't know how to use that dataset with this implementation. I know that I should extract features from the dataset's texts and use these features as input to the SVM so could any body please tell me about a detailed tutorial about how to extract text's features and use them as input to the SVM algorithm, and then use this algorithm to classify a new text?
And if there is a full example about using SVM for text classification, that's would be great.</p>

<p>Any help would be appreciated.
Thanks in advance.</p>
",Text Classification / Sentiment Analysis,use reuters dataset svm net text classification started application text classification read lot paper topic till know start feel like got whole image got training dataset read description got great implementation svm algorithm svm net know use dataset implementation know extract feature dataset text use feature input svm could body please tell detailed tutorial extract text feature use input svm algorithm use algorithm classify new text full example using svm text classification would great help would appreciated thanks advance
How to classify words to their correspoding categories?,"<p>I've to implement text classification for a long list of words. I've some categories defined e.g. If the word ""UK"" is in the list, it will come under ""Regions"". If the word is ""Pizza"", it will come under category ""food"".</p>

<p>How can I classify the words under different categories? Is there any open source tool available to do that? </p>
",Text Classification / Sentiment Analysis,classify word correspoding category implement text classification long list word category defined e g word uk list come region word pizza come category food classify word different category open source tool available
Sentiment analysis with NLTK python for sentences using sample data or webservice?,"<p>I am embarking upon a NLP project for sentiment analysis.</p>

<p>I have successfully installed NLTK for python (seems like a great piece of software for this). However,I am having trouble understanding how it can be used to accomplish my task.</p>

<p>Here is my task:</p>

<ol>
<li>I start with one long piece of data (lets say several hundred tweets on the subject of the UK election from their webservice)</li>
<li>I would like to break this up into sentences (or info no longer than 100 or so chars) (I guess i can just do this in python??)</li>
<li>Then to search through all the sentences for specific instances within that sentence e.g. ""David Cameron""</li>
<li>Then I would like to check for positive/negative sentiment in each sentence and count them accordingly</li>
</ol>

<p>NB: I am not really worried too much about accuracy because my data sets are large and also not worried too much about sarcasm. </p>

<p>Here are the troubles I am having:</p>

<ol>
<li><p>All the data sets I can find e.g. the corpus movie review data that comes with NLTK arent in webservice format. It looks like this has had some processing done already. As far as I can see the processing (by stanford) was done with WEKA. Is it not possible for NLTK to do all this on its own? Here all the data sets have already been organised into positive/negative already e.g. polarity dataset <a href=""http://www.cs.cornell.edu/People/pabo/movie-review-data/"" rel=""noreferrer"">http://www.cs.cornell.edu/People/pabo/movie-review-data/</a> How is this done? (to organise the sentences by sentiment, is it definitely WEKA? or something else?)</p></li>
<li><p>I am not sure I understand why WEKA and NLTK would be used together. Seems like they do much the same thing. If im processing the data with WEKA first to find sentiment why would I need NLTK? Is it possible to explain why this might be necessary?</p></li>
</ol>

<p>I have found a few scripts that get somewhat near this task, but all are using the same pre-processed data. Is it not possible to process this data myself to find sentiment in sentences rather than using the data samples given in the link?</p>

<p>Any help is much appreciated and will save me much hair!</p>

<p>Cheers Ke</p>
",Text Classification / Sentiment Analysis,sentiment analysis nltk python sentence using sample data webservice embarking upon nlp project sentiment analysis successfully installed nltk python seems like great piece software however trouble understanding used accomplish task task start one long piece data let say several hundred tweet subject uk election webservice would like break sentence info longer char guess python search sentence specific instance within sentence e g david cameron would like check positive negative sentiment sentence count accordingly nb really worried much accuracy data set large also worried much sarcasm trouble data set find e g corpus movie review data come nltk arent webservice format look like ha processing done already far see processing stanford wa done weka possible nltk data set already organised positive negative already e g polarity dataset done organise sentence sentiment definitely weka something else sure understand weka nltk would used together seems like much thing im processing data weka first find sentiment would need nltk possible explain might necessary found script get somewhat near task using pre processed data possible process data find sentiment sentence rather using data sample given link help much appreciated save much hair cheer ke
Algorithm to determine how positive or negative a statement/text is,"<p>I need to implement sentiment analysis.  Can anyone point me to examples/reference implementations?</p>
",Text Classification / Sentiment Analysis,algorithm determine positive negative statement text need implement sentiment analysis anyone point example reference implementation
NLP classify sentences/paragraph as funny,"<p>Is there a way to classify a particular sentence/paragraph as funny. There are very few pointers as to where one should go further on this.</p>
",Text Classification / Sentiment Analysis,nlp classify sentence paragraph funny way classify particular sentence paragraph funny pointer one go
