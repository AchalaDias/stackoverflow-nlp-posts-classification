Title,Description,category,combined_text
When is entity replacement necessary for relation extraction?,"<p>In this <a href=""https://www.microsoft.com/developerblog/2016/09/13/training-a-classifier-for-relation-extraction-from-medical-literature/#Reuse"" rel=""nofollow noreferrer"">tutorial</a> for ""Training a Machine Learning Classifier for Relation Extraction from Medical Literature"" the author does Entity replacement because that ""we don’t want the model to learn according to a specific entity name, but we want it to learn according to the structure of the text"".</p>

<p>Is this generally true or does it depend on the dataset or the used models?</p>
",Coreference & Relation Extraction,entity replacement necessary relation extraction tutorial training machine learning classifier relation extraction medical literature author doe entity replacement want model learn according specific entity name want learn according structure text generally true doe depend dataset used model
Using Stanford CoreNLP,"<p>I am trying to get around using the Stanford CoreNLP. I used some code from the web to understand what is going on with the coreference tool. I tried running the project in Eclipse but keep encountering an out of memory exception. I tried increasing the heap size but there isnt any difference.</p>
<p>Why this keeps happening? Is this a code specific problem?</p>
<p>Here is my code:</p>
<pre><code>import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;


import java.util.Iterator;
import java.util.Map;
import java.util.Properties;


public class testmain {

    public static void main(String[] args) {

        String text = &quot;Viki is a smart boy. He knows a lot of things.&quot;;
        Annotation document = new Annotation(text);
        Properties props = new Properties();
        props.put(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, parse, dcoref&quot;);
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        pipeline.annotate(document);


        Map&lt;Integer, CorefChain&gt; graph = document.get(CorefCoreAnnotations.CorefChainAnnotation.class);



        Iterator&lt;Integer&gt; itr = graph.keySet().iterator();
    
        while (itr.hasNext()) {
        
             String key = itr.next().toString();
        
             String value = graph.get(key).toString();
        
             System.out.println(key + &quot; &quot; + value);      
        }

   }
}
</code></pre>
",Coreference & Relation Extraction,using stanford corenlp trying get around using stanford corenlp used code web understand going coreference tool tried running project eclipse keep encountering memory exception tried increasing heap size isnt difference keep happening code specific problem code
Relation Extraction Model returns only one entity instead of entity pairs,"<p>I'm working on a relation extraction model task using a transformer-based model. the `pipeline is expected to extract entity pairs along with their labelled relation labels. When I run the evaluation after training the model on labelled data, it works fine. However, when I run the model pipeline, the model sometimes returns only one entity instead of a complete entity pair, along with relation label. I don't quite understand how the model is making a prediction for relation label just by considering one entity.</p>
<pre><code>def entity_extract(text):
    doc_=MODEL_NER(text)
    for name, proc in MODEL_REL.pipeline:
            doc_ = proc(doc_)
    return doc_
</code></pre>
<pre><code>def relation_extraction(doc):
    relation_extraction_output = {}
    unique_entities = set()

    for span, rel_dict in doc._.rel.items():
        # Extract the relation with the highest confidence score
        most_probable_relation = max(rel_dict, key=rel_dict.get)
        score = rel_dict[most_probable_relation]
        
        # Skip if the score is below the cutoff (0.5 50%  in your case)
        if score &lt;= 0.5:
            continue

        start, end = span
        relation_span = doc[start:end]
        # Extract entities involved in the relation
        entities = [ent for ent in relation_span.ents if ent.text not in unique_entities]
        # Store the information if entities exist
        if entities:
            unique_entities.update(ent.text for ent in entities)  # Efficient update
            # Create a key for the entity pair
            entity_pair = tuple(sorted([ent.text for ent in entities]))
            if entity_pair not in relation_extraction_output:
                relation_extraction_output[entity_pair] = {
                    'relations': [],
                    'scores': []
                }
            # Append the relation and score
            relation_extraction_output[entity_pair]['relations'].append(most_probable_relation)
            relation_extraction_output[entity_pair]['scores'].append(score)
    # Convert the output to a list of dictionaries
    final_output = [
        {
            'entities': key,
            'relations': value['relations'],
            'scores': value['scores']
        }
        for key, value in relation_extraction_output.items()
    ]
    return final_output
</code></pre>
<pre><code>def extract_relation(file):
    filename = os.path.splitext(os.path.basename(file))[0]
    print(&quot;Starting to process RelationExtract part for file:&quot;, filename)
    chunk_size=10000
    # Define a function to process each row in a chunk
    def process_row(row):
        try:
            text = entity_extract(row[&quot;sent&quot;])
            return relation_extraction(text)
        except Exception as e:
            print(f&quot;Error processing row: {e}&quot;)
            return None
</code></pre>
<p>Expected output: entity pair: [entity1, entity2], relation: [relation_label]
Actual output: [entity1, ], relation: [relation_label]</p>
",Coreference & Relation Extraction,relation extraction model return one entity instead entity pair working relation extraction model task using transformer based model pipeline expected extract entity pair along labelled relation label run evaluation training model labelled data work fine however run model pipeline model sometimes return one entity instead complete entity pair along relation label quite understand model making prediction relation label considering one entity expected output entity pair entity entity relation relation label actual output entity relation relation label
What does surface form mean in relation extraction?,"<p>The term &quot;entity surface form&quot; is repeatedly mentioned in most of relation extraction papers. What does it mean?</p>
<p>For example, in the REBEL paper, the author mentions that &quot;a relation is considered correct only if the head and tail entity surface forms are correctly extracted.&quot;</p>
<p>What is meant by &quot;head&quot; and &quot;tail&quot;?</p>
",Coreference & Relation Extraction,doe surface form mean relation extraction term entity surface form repeatedly mentioned relation extraction paper doe mean example rebel paper author mention relation considered correct head tail entity surface form correctly extracted meant head tail
Is there a method to extract quotes and their related speakers in the French language?,"<p>Is there a method to extract quote and their related speaker with the gestion of coreference?</p>
<p>I want in output to get a dict with [{&quot;speaker&quot; : , &quot;quotes&quot;: }] and if we don’t find the speaker, we put None to speaker and add &quot;Potential Speaker&quot; : coreference</p>
",Coreference & Relation Extraction,method extract quote related speaker french language method extract quote related speaker gestion coreference want output get dict speaker quote find speaker put none speaker add potential speaker coreference
Spacy CoReference resolution in version 3.7,"<p>While following some documentation from spaCy to use the experimental Coreference Resolver (<a href=""https://spacy.io/api/coref"" rel=""nofollow noreferrer"">https://spacy.io/api/coref</a>), I could make it work with the code below in spaCy 3.4.0.</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_coreference_web_trf&quot;)
text = &quot;Philip plays the bass because he loves it&quot;
doc = nlp(text)
print(doc.spans)
</code></pre>
<p>Unfortunately the same code, does not work in version 3.7.6 (latest as in 2024 Sept 26). For many reasons, the first is that there is no compatible model with 3.7.*. And I could not find other way to use it.</p>
<p>Any hints in how to use Coref resolution with the latest spaCy version?</p>
",Coreference & Relation Extraction,spacy coreference resolution version following documentation spacy use experimental coreference resolver could make work code spacy unfortunately code doe work version latest sept many reason first compatible model could find way use hint use coref resolution latest spacy version
Coreference resolution when there are no explicit references,"<p>To improve a chatbot's performance I need to modify the user's question to include contextual information before passing it to the chatbot. For this task I planned on using a neural coreference resolution model such as spaCy's &quot;en-coreference-web-trf&quot; <a href=""https://github.com/explosion/spaCy/discussions/11585#discussioncomment-3970887"" rel=""nofollow noreferrer"">model</a>. I can just use GPT for this but it is heavy weight and not suitable for my application, but spaCy's model is just fine for this issue.</p>
<p>Now I came across another problem, that is, when there are no explicit references available to resolve in the user's question. For example, consider the following conversation:</p>
<pre><code>Q: How to treat for blast pest?  
A: To treat for blast pest in rice, you can treat the seeds with Pseudomonas fluorescens 
Q: What about brinjal?
</code></pre>
<p>I need the question &quot;What about brinjal&quot; to include contextual information and be modified to &quot;What about blast pest in brinjal&quot;. The coreference in the user's question here is implicit.</p>
<p>If there is an explicit coreference term such as &quot;it&quot;, it can be resolved/replaced by &quot;blast pest&quot; by the coreference resolution model.</p>
<pre><code>Q: What about it in brinjal?
</code></pre>
<p>Any suggestions on how these cases can be handled? I don't think a neural coreference resolution model can deal with this. I know we can solve this through the use of large LLMs but I need something lightweight with fast inference time.</p>
<p>How to resolve implicit coreferences such as in the above examples?</p>
",Coreference & Relation Extraction,coreference resolution explicit reference improve chatbot performance need modify user question include contextual information passing chatbot task planned using neural coreference resolution model spacy en coreference web trf model use gpt heavy weight suitable application spacy model fine issue came across another problem explicit reference available resolve user question example consider following conversation need question brinjal include contextual information modified blast pest brinjal coreference user question implicit explicit coreference term resolved replaced blast pest coreference resolution model suggestion case handled think neural coreference resolution model deal know solve use large llm need something lightweight fast inference time resolve implicit coreference example
How can I tune neuralcoref to get the better coreference results?,"<p>I'm using neuralcoref - a coreference resolution module based on the spaCy parser. GIT <a href=""https://github.com/huggingface/neuralcoref"" rel=""nofollow noreferrer"">https://github.com/huggingface/neuralcoref</a></p>

<p>However, the results I'm getting could be improved. The online visualizer provided by huggingface (developer of neuralcoref) gives me more accurate results.</p>

<p>The text I'm analyzing:
""London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, it has been a major settlement for two millennia.""</p>

<p>I get this result:</p>

<pre><code>doc._.coref_resolved 
</code></pre>

<blockquote>
  <blockquote>
    <blockquote>
      <p>London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, the River Thames has been a major settlement for two millennia.</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>So it's mistakingly linking London with River Thames. (it -> River Thames)</p>

<p>The neuralcoref online visualizer returns the correct link (it -> London)</p>

<p><a href=""https://huggingface.co/coref/?text=London%20is%20the%20capital%20and%20most%20populous%20city%20of%20England%20and%20the%20United%20Kingdom.%20Standing%20on%20the%20River%20Thames%20in%20the%20south%20east%20of%20the%20island%20of%20Great%20Britain%2C%20it%20has%20been%20a%20major%20settlement%20for%20two%20millennia.%20It%20was%20founded%20by%20the%20Romans%2C%20who%20named%20it%20Londinium"" rel=""nofollow noreferrer"">https://huggingface.co/coref/?text=London%20is%20the%20capital%20and%20most%20populous%20city%20of%20England%20and%20the%20United%20Kingdom.%20Standing%20on%20the%20River%20Thames%20in%20the%20south%20east%20of%20the%20island%20of%20Great%20Britain%2C%20it%20has%20been%20a%20major%20settlement%20for%20two%20millennia.%20It%20was%20founded%20by%20the%20Romans%2C%20who%20named%20it%20Londinium</a>.</p>

<p>I have already tried tuning parameters such as greedyness, max_dist mentioned on the project's git page <a href=""https://github.com/huggingface/neuralcoref"" rel=""nofollow noreferrer"">https://github.com/huggingface/neuralcoref</a></p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

import neuralcoref
neuralcoref.add_to_pipe(nlp,greedyness=0.5,store_scores=True)

text = ""London is the capital and most populous city of England and   the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, it has been a major settlement for two millennia.""# It was founded by the Romans, who named it Londinium.""

doc = nlp(text)
print(doc._.coref_resolved)
doc._.coref_scores
</code></pre>

<p>Is there a way to tune it to get results similar to those from the visualizer?</p>

<p>Thank you!</p>
",Coreference & Relation Extraction,tune neuralcoref get better coreference result using neuralcoref coreference resolution module based spacy parser git however result getting could improved online visualizer provided huggingface developer neuralcoref give accurate result text analyzing london capital populous city england united kingdom standing river thames south east island great britain ha major settlement two millennium get result london capital populous city england united kingdom standing river thames south east island great britain river thames ha major settlement two millennium mistakingly linking london river thames river thames neuralcoref online visualizer return correct link london already tried tuning parameter greedyness max dist mentioned project git page way tune get result similar visualizer thank
What is the best way to handle Coreference Resolution in NLP,"<p>For example,</p>
<p>Context: I am Yash and he is Jay who is my close friend.
Generated Question: Who is Jay?
Generated Answer: my close friend.</p>
<p>But the Answer should be something like following:</p>
<ul>
<li>Yash's close friend.</li>
<li>Jay is a cose friend of Yash.</li>
</ul>
<p>I have tried out AllenNLP and other Standford-NLP solutions but didn't work out at all.</p>
",Coreference & Relation Extraction,best way handle coreference resolution nlp example context yash jay close friend generated question jay generated answer close friend answer something like following yash close friend jay cose friend yash tried allennlp standford nlp solution work
is there a method to extract adj nous pairs for french (with conjunction),"<p>i want to extract all NOUN - ADJ pairs (with conjunction and coreference) from a texts using spacy ? is there any method or solution ?</p>
",Coreference & Relation Extraction,method extract adj nous pair french conjunction want extract noun adj pair conjunction coreference text using spacy method solution
Identifying Cataphora and Anaphora using the Stanford Parser,"<p>Can the Stanford Parser find instances of cataphora and anaphora in a given set of sentences?</p>
<p>Are there any alternative open-source (or proprietary) software packages that are capable of coreference resolution?</p>
",Coreference & Relation Extraction,identifying cataphora anaphora using stanford parser stanford parser find instance cataphora anaphora given set sentence alternative open source proprietary software package capable coreference resolution
Allennlp Does not generate model.tar.gz after training,"<p>I am training a coreference resolution model with Alennlp. However after I execute the command <code>train_data_path=&lt;train path&gt; validation_data_path=&lt;dev path&gt; test_data_path=&lt;testpath&gt; dataset_reader=&lt;dataset reader&gt; allennlp train &lt;jsonnet file&gt; -s &lt;output dir&gt; --include-package &lt;training module&gt;</code>
it does not produce a <code>model.tar.gz</code> file in the <code>&lt;output dir&gt;</code>.</p>
<p>Any idea why?</p>
<p>edit: I've been running this via polyaxon and somehow the error logs does not appear after it fails. Rightfully once training completes the model.tar.gz file should appear. I think it should be an error that appears during training that fails it but I am unable to inspect it</p>
",Coreference & Relation Extraction,allennlp doe generate model tar gz training training coreference resolution model alennlp however execute command doe produce file idea edit running via polyaxon somehow error log doe appear fails rightfully training completes model tar gz file appear think error appears training fails unable inspect
What&#39;s the meaning of the following parameters of the coreference in Corenlp?,"<p>What's the meaning of the following parameters of the coreference in Corenlp: &quot;type&quot;, &quot;number&quot;, &quot;headIndex&quot;, &quot;position&quot;?
Here is an example of the output when using the coreference in corenlp:
{'id': 67, 'text': 'a whole input sentence', 'type': 'NOMINAL', 'number': 'SINGULAR', 'gender': 'UNKNOWN', 'animacy': 'INANIMATE', 'startIndex': 23, 'endIndex': 27, 'headIndex': 26, 'sentNum': 9, 'position': [9, 5], 'isRepresentativeMention': True}</p>
",Coreference & Relation Extraction,meaning following parameter coreference corenlp meaning following parameter coreference corenlp type number headindex position example output using coreference corenlp id text whole input sentence type nominal number singular gender unknown animacy inanimate startindex endindex headindex sentnum position isrepresentativemention true
Parsing HTML into sentences - how to handle tables/lists/headings/etc?,"<p>How do you go about parsing an HTML page with free text, lists, tables, headings, etc., into sentences?</p>

<p>Take <a href=""http://en.wikipedia.org/wiki/Neurotransmitter"" rel=""noreferrer"">this wikipedia page</a> for example. There is/are:</p>

<ul>
<li>free text: <a href=""http://en.wikipedia.org/wiki/Neurotransmitter#Discovery"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Neurotransmitter#Discovery</a></li>
<li>lists: <a href=""http://en.wikipedia.org/wiki/Neurotransmitter#Actions"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Neurotransmitter#Actions</a></li>
<li>tables: <a href=""http://en.wikipedia.org/wiki/Neurotransmitter#Common_neurotransmitters"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Neurotransmitter#Common_neurotransmitters</a></li>
</ul>

<p>After messing around with the python <a href=""http://nltk.org/"" rel=""noreferrer"">NLTK</a>, I want to test out all of these different corpus annotation methods (from <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch11.html#deciding-which-layers-of-annotation-to-include"" rel=""noreferrer"">http://nltk.googlecode.com/svn/trunk/doc/book/ch11.html#deciding-which-layers-of-annotation-to-include</a>):</p>

<ul>
<li><strong>Word Tokenization</strong>: The orthographic form of text does not unambiguously identify its tokens. A tokenized and normalized version, in addition to the conventional orthographic version, may be a very convenient resource.</li>
<li><strong>Sentence Segmentation</strong>: As we saw in Chapter 3, sentence segmentation can be more difficult than it seems. Some corpora therefore use explicit annotations to mark sentence segmentation.</li>
<li><strong>Paragraph Segmentation</strong>: Paragraphs and other structural elements (headings, chapters, etc.) may be explicitly annotated.</li>
<li><strong>Part of Speech</strong>: The syntactic category of each word in a document.</li>
<li><strong>Syntactic Structure</strong>: A tree structure showing the constituent structure of a sentence.</li>
<li><strong>Shallow Semantics</strong>: Named entity and coreference annotations, semantic role labels.</li>
<li><strong>Dialogue and Discourse</strong>: dialogue act tags, rhetorical structure</li>
</ul>

<p>Once you break a document into sentences it seems pretty straightforward. But how do you go about breaking down something like the HTML from that Wikipedia page? I am very familiar with using HTML/XML parsers and traversing the tree, and I have tried just stripping the HTML tags to get the plain text, but because punctuation is missing after HTML is removed, NLTK doesn't parse things like table cells, or even lists, correctly.</p>

<p>Is there some best-practice or strategy for parsing that stuff with NLP? Or do you just have to manually write a parser specific to that individual page?</p>

<p>Just looking for some pointers in the right direction, really want to try this NLTK out!</p>
",Coreference & Relation Extraction,parsing html sentence handle table list heading etc go parsing html page free text list table heading etc sentence take wikipedia page example free text list table messing around python nltk want test different corpus annotation method word tokenization orthographic form text doe unambiguously identify token tokenized normalized version addition conventional orthographic version may convenient resource sentence segmentation saw chapter sentence segmentation difficult seems corpus therefore use explicit annotation mark sentence segmentation paragraph segmentation paragraph structural element heading chapter etc may explicitly annotated part speech syntactic category word document syntactic structure tree structure showing constituent structure sentence shallow semantics named entity coreference annotation semantic role label dialogue discourse dialogue act tag rhetorical structure break document sentence seems pretty straightforward go breaking something like html wikipedia page familiar using html xml parser traversing tree tried stripping html tag get plain text punctuation missing html removed nltk parse thing like table cell even list correctly best practice strategy parsing stuff nlp manually write parser specific individual page looking pointer right direction really want try nltk
Reference resolution in python,"<p>Im trying to perform reference resolution on a short story (a few sentences).</p>
<p>I understand some of the theory behind reference resolution, I'm lost on how to start. I'm not asking for the answer I just need some sort of tip.</p>
<ul>
<li><a href=""https://i.sstatic.net/pRc2W.png"" rel=""nofollow noreferrer"">Input example (multiple sentences)</a></li>
<li><a href=""https://i.sstatic.net/yHG1C.png"" rel=""nofollow noreferrer"">Output</a></li>
<li><a href=""https://i.sstatic.net/kw4GS.png"" rel=""nofollow noreferrer"">Formal method</a></li>
</ul>
<p>We can't use any prebuilt modules besides the <code>TRIPS</code> ontology and <code>numpy</code></p>
",Coreference & Relation Extraction,reference resolution python im trying perform reference resolution short story sentence understand theory behind reference resolution lost start asking answer need sort tip input example multiple sentence output formal method use prebuilt module besides ontology
Understanding the Winograd Schema,"<p>What is the Winograd Schema?
A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is resolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its resolution.</p>
<p>Is this a good summary?</p>
<p>I do not understand why it is difficult to use NLP to resolve coreference in a winograd schema?</p>
",Coreference & Relation Extraction,understanding winograd schema winograd schema winograd schema pair sentence differ one two word contain ambiguity resolved opposite way two sentence requires use world knowledge reasoning resolution good summary understand difficult use nlp resolve coreference winograd schema
Is there a code to perform coreference resolution in AllenNLP in Python?,"<p>Is there a code to perform coreference resolution in AllenNLP in Python?</p>
<p>The existing code samples on the internet don't seem to work unfortunately.</p>
",Coreference & Relation Extraction,code perform coreference resolution allennlp python code perform coreference resolution allennlp python existing code sample internet seem work unfortunately
Is there a way to change the tokenizer in AllenNLP&#39;s coreference resolution model?,"<p>Does anyone know how to change the tokenizer in <a href=""https://demo.allennlp.org/coreference-resolution"" rel=""nofollow noreferrer"">AllenNLP's coreference resolution</a>? By default, it uses SpaCy and I would like to use a white space tokenizer so as to tokenize only words, not punctuation.</p>
<p>This is what I have tried so far but it does not seem to work:</p>
<pre><code>review = &quot;&quot;&quot;Judging from previous posts this used to be a good place, but not any longer.
        We, there were four of us, arrived at noon - the place was empty - 
        and the staff acted like we were imposing on them and they were very rude. 
        They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.
        The food was lousy - too sweet or too salty and the portions tiny.
        After all that, they complained to me about the small tip.
        Avoid this place!&quot;&quot;&quot;

from allennlp.data.tokenizers.whitespace_tokenizer import WhitespaceTokenizer
from allennlp.predictors.predictor import Predictor

predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz&quot;)
predictor._tokenizer = WhitespaceTokenizer()

pred = predictor.predict(document=review)

# expected output: 'Judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place,', 'but', 'not', 'any', 'longer.'
print(pred['document'])
</code></pre>
<p>I found the documentation on tokenizers <a href=""http://docs.allennlp.org/main/api/data/tokenizers/whitespace_tokenizer/"" rel=""nofollow noreferrer"">here</a>, but I don't know if it is possible to use them on other models like on coreference resolution.</p>
",Coreference & Relation Extraction,way change tokenizer allennlp coreference resolution model doe anyone know change tokenizer allennlp coreference resolution default us spacy would like use white space tokenizer tokenize word punctuation tried far doe seem work found documentation tokenizers know possible use model like coreference resolution
is there a method to extract noun-adjectives pair from sentence in french?,"<p>i have this bit of code to extract noun-adj pairs using spacy , but this code work perfectly for english and not french because in french we have difficulty to extract pair of noun-adj :</p>
<p>1-  la voiture est belle,grande et jolie. (CCONJ = &quot;et&quot; when we have many adjectives)
2-  le tableau qui est juste en dessous est grand et beau. (so we have a coreference here , we should associate grand et beau to &quot;tableau&quot;</p>
<p>i know that dependencymatcher in spacy are robust but in  my case , sometimes I have texts which are not cleaned since it is about the opinions of people... so we need to do this manually...</p>
<p>in the output we should have something like this :
{&quot;voiture&quot;:[&quot;belle&quot;,&quot;grande&quot;,&quot;jolie&quot;], &quot;tableau&quot;:[&quot;beau&quot;,&quot;grand&quot;]}</p>
<pre><code>import spacy

nlp = spacy.load(&quot;fr_core_news_sm&quot;)
doc = nlp('la voiture est belle et jolie. le tableau qui est juste en dessous est grand ') 
noun_adj_pairs = {}
for chunk in doc.noun_chunks:
    adj = []
    noun = &quot;&quot;
    for tok in chunk:
        if tok.pos_ == &quot;NOUN&quot;:
           noun = tok.text
        if tok.pos_ == &quot;ADJ&quot; or tok.pos_ == &quot;CCONJ&quot;:
           adj.append(tok.text)
    if noun:
        noun_adj_pairs.update({noun:&quot; &quot;.join(adj)}) 
</code></pre>
",Coreference & Relation Extraction,method extract noun adjective pair sentence french bit code extract noun adj pair using spacy code work perfectly english french french difficulty extract pair noun adj la voiture est belle grande et jolie cconj et many adjective le tableau qui est juste en dessous est grand et beau coreference associate grand et beau tableau know dependencymatcher spacy robust case sometimes text cleaned since opinion people need manually output something like voiture belle grande jolie tableau beau grand
SpaCy - Split the neuralcoref results into sentences,"<p>I am using the <a href=""https://github.com/huggingface/neuralcoref"" rel=""nofollow noreferrer"">neuralcoref</a> library for coreference resolution. It works on top of the Spacy library. I get it working as by the documentation.</p>
<pre><code>import spacy
import neuralcoref

nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)
doc1 = nlp('My sister has a dog. She loves him.')
print(doc1._.coref_resolved) # My sister has a dog. My sister loves a dog.
</code></pre>
<p>What I want to do is to split the coref_resolved attribute into sentences and remove punctuations. I don't know how to do it without running it through another NLP annotation, like so:</p>
<pre><code> doc1 = nlp('My sister has a dog. She loves him.')
 doc2 = nlp(doc1._.coref_resolved)
 print(list(doc2.sents)) # [My sister has a dog., My sister loves a dog.]
</code></pre>
<p>Next, I would need to also remove punctuations. How can I do this without running a single sentence through two different NLP pipelines? I want my result to look like:</p>
<pre><code>[&quot;My sister has a dog&quot;, &quot;My sister loves a dog&quot;]
</code></pre>
<p>Thanks!</p>
",Coreference & Relation Extraction,spacy split neuralcoref result sentence using neuralcoref library coreference resolution work top spacy library get working documentation want split coref resolved attribute sentence remove punctuation know without running another nlp annotation like next would need also remove punctuation without running single sentence two different nlp pipeline want result look like thanks
How can I iterate token attributes with coreference results in CoreNLP?,"<p>I am looking for a way to extract and merge annotation results from CoreNLP. To specify,</p>
<pre><code>import stanza
import os
from stanza.server import CoreNLPClient
corenlp_dir = '/Users/fatih/stanford-corenlp-4.2.0/'
os.environ['CORENLP_HOME'] = corenlp_dir

client = CoreNLPClient(
    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'coref'], 
    memory='4G', 
    endpoint='http://localhost:9001',
    be_quiet=True)

text = &quot;Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.&quot;

doc = client.annotate(text)

for x in doc.corefChain:
    for y in x.mention:
        print(y.animacy)
        
ANIMATE
ANIMATE
ANIMATE
</code></pre>
<p>I'd like to merge these results with the ones from following code:</p>
<pre><code>for i, sent in enumerate(document.sentence):
    print(&quot;[Sentence {}]&quot;.format(i+1))
    for t in sent.token:
        print(&quot;{:12s}\t{:12s}\t{:6s}\t{}&quot;.format(t.word, t.lemma, t.pos, t.ner))
    print(&quot;&quot;)

Barack          Barack          NNP     PERSON
Obama           Obama           NNP     PERSON
was             be              VBD     O
born            bear            VBN     O
in              in              IN      O
Hawaii          Hawaii          NNP     STATE_OR_PROVINCE
.               .               .       O

[Sentence 2]
He              he              PRP     O
is              be              VBZ     O
the             the             DT      O
president       president       NN      TITLE
.               .               .       O

[Sentence 3]
Obama           Obama           NNP     PERSON
was             be              VBD     O
elected         elect           VBN     O
in              in              IN      O
2008            2008            CD      DATE
.               .               .       O
</code></pre>
<p>Since annotations are stored in different object, I cannot iterate over the two different object and get the results for related items.</p>
<p>Is there a way out?</p>
<p>Thanks.</p>
",Coreference & Relation Extraction,iterate token attribute coreference result corenlp looking way extract merge annotation result corenlp specify like merge result one following code since annotation stored different object iterate two different object get result related item way thanks
using NLTK to find the related verbs to a specific noun,"<p>Is there any way to find the related verbs to a specific noun by using NLTK. For example for the word ""University"" I'd like to have the verbs ""study"" and ""graduate"" as an output. I mainly need this feature for relation extraction among some given entities.</p>
",Coreference & Relation Extraction,using nltk find related verb specific noun way find related verb specific noun using nltk example word university like verb study graduate output mainly need feature relation extraction among given entity
Repetition of named entity in coreference resolution,"<p>I am trying to find coreferences for the following text, using neuralcoref:</p>
<p>Alan and Bruder are great friends with Chris and Donald. Alan and Bruder want to head to Lebanon while Chris and Donald wish to stay in United States. Chris and Donald have not made up their mind yet, but will get there soon. Alan and Bruder do not want to separate but there seems to be no choice.</p>
<p><strong>Code:</strong></p>
<pre><code>case_a_bi = 'Alan and Bruder are great friends with Chris and Donald. Alan 
and Bruder want to head to Lebanon while Chris and Donald wish to stay in 
United States. Chris and Donald have not made up their mind yet, but will 
get there soon. Alan and Bruder do not want to separate but there seems to 
be no choice.'
nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)
case_a_bi = unidecode(case_a_bi)
doc = nlp(case_a_bi)
doc._.coref_clusters
doc._.coref_resolved
</code></pre>
<p><strong>Output for coref clusters:</strong></p>
<pre><code>[Alan and Bruder: [Alan and Bruder, Alan, Alan and Bruder, Alan, Alan and Bruder],

Chris: [Chris, Chris, Chris],

Chris and Donald: [Chris and Donald, Chris and Donald, Chris and Donald, their],

Donald: [Donald, Donald, Donald]]
</code></pre>
<p><strong>Output for coref resolution:</strong></p>
<p>Alan and Bruder are great friends with Chris and Donald. Alan and Bruder want to head to Lebanon while Chris and Donald Donald wish to stay in United States. <strong>Chris and Donald Donald</strong> have not made up Chris and Donald mind yet, but will get there soon. Alan and Bruder do not want to separate but there seems to be no choice.</p>
<p>As per the outputs mentioned, I get correct clusters for Chris and Donald, but when I try to resolve these coreferences, Donald gets repeated twice in the result.</p>
<p>Can someone help me in understanding what might be going wrong here?</p>
",Coreference & Relation Extraction,repetition named entity coreference resolution trying find coreference following text using neuralcoref alan bruder great friend chris donald alan bruder want head lebanon chris donald wish stay united state chris donald made mind yet get soon alan bruder want separate seems choice code output coref cluster output coref resolution alan bruder great friend chris donald alan bruder want head lebanon chris donald donald wish stay united state chris donald donald made chris donald mind yet get soon alan bruder want separate seems choice per output mentioned get correct cluster chris donald try resolve coreference donald get repeated twice result someone help understanding might going wrong
Which coreference chains does each sentence relate to in NeuralCoref?,"<p>I am using <a href=""https://github.com/huggingface/neuralcoref"" rel=""nofollow noreferrer"">neuralcoref</a> for the task of coreference resolution in a text.</p>
<p>I want to know each sentence has mentions from which coreference clusters. For example, sentence1 has mentions from coreference clusters 1, and 4; sentence 2 has mentions from coreference clusters 10 , 14.</p>
<p>How can I do this?</p>
",Coreference & Relation Extraction,coreference chain doe sentence relate neuralcoref using neuralcoref task coreference resolution text want know sentence ha mention coreference cluster example sentence ha mention coreference cluster sentence ha mention coreference cluster
NLP - Determine whether a piece of text is talking about a given topic?,"<p>I have a Java application where I'm looking to determine in real time whether a given piece of text is talking about a topic supplied as a query.</p>

<p>Some techniques I've looked into for this are coreference detection with packages like open-nlp and Stanford-NLP coref detection, but these models take extremely long to load and don't seem practical in a production application environment. Is it possible to perform coreference analysis such that given a piece of text and a topic, I can get a boolean answer that the text is discussing the topic?</p>

<p>Other than document classification which requires a trained corpus, are there any other techniques that can help me achieve such a thing?</p>
",Coreference & Relation Extraction,nlp determine whether piece text talking given topic java application looking determine real time whether given piece text talking topic supplied query technique looked coreference detection package like open nlp stanford nlp coref detection model take extremely long load seem practical production application environment possible perform coreference analysis given piece text topic get boolean answer text discussing topic document classification requires trained corpus technique help achieve thing
NLP: Resolve coreference pronoun in blocks,"<p>I'm planning on executing my NLP pipeline on a corpus of books. Since resolving coreferences is an intensive process, I wouldn't be able to process an entire book or maybe even an entire chapter at a time. I was planning on splitting the text into sizeable chunks to resolve coreferences.</p>
<p>The issue I need help with is how would I resolve pronouns from <code>Group2</code> when the noun that they're referencing is located in <code>Group1</code>. Is there a way to seed the dependencies from <code>Group1</code> to the following groups? If not, how is this typically handled?</p>
<p>For what it's worth I'm using CoreNLP, but I'm open to other others.</p>
<blockquote>
<p>&quot;Group 1&quot;: George was born in New York. George is 10.</p>
<p>&quot;Group 2&quot;: He loves New York city.</p>
</blockquote>
",Coreference & Relation Extraction,nlp resolve coreference pronoun block planning executing nlp pipeline corpus book since resolving coreference intensive process able process entire book maybe even entire chapter time wa planning splitting text sizeable chunk resolve coreference issue need help would resolve pronoun noun referencing located way seed dependency following group typically handled worth using corenlp open others group george wa born new york george group love new york city
"Kernel keeps dying Jupyter, Anaconda. Trying to implement Coreference Resolution using neuralcoref","<p>I need to implement a solution which can recognize pronouns associated with the noun in a sentence. Say I have an paragraph about a person, I wanna count how many times the person has been referenced (name or any other pronoun). I want to implement this is Python.</p>
<p>After some research I came across neuralcoref and though it could be useful. After several attempts I'm still getting stuck because the kernel keeps dying.</p>
<p>It would be great if someone can help with this problem. I am also open to suggestions about other libraries/resources I could use to implement this.</p>
<p>Thanks!</p>
<p>This is the code I used:</p>
<pre><code>import neuralcoref
import spacy
nlp = spacy.load('en_core_web_md')
coref = neuralcoref.NeuralCoref(nlp.vocab)
nlp.add_pipe(coref, name = 'neuralcoref')
doc = nlp('My sister has a dog. She loves him')
</code></pre>
",Coreference & Relation Extraction,kernel keep dying jupyter anaconda trying implement coreference resolution using neuralcoref need implement solution recognize pronoun associated noun sentence say paragraph person wan na count many time person ha referenced name pronoun want implement python research came across neuralcoref though could useful several attempt still getting stuck kernel keep dying would great someone help problem also open suggestion library resource could use implement thanks code used
Anaphora resolution in stanford-nlp using python,"<p>I am trying to do anaphora resolution and for that below is my code.</p>

<p>first i navigate to the folder where i have downloaded the stanford module. Then i run the command in command prompt to initialize stanford nlp module</p>

<pre><code>java -mx4g -cp ""*;stanford-corenlp-full-2017-06-09/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
</code></pre>

<p>After that i execute below code in Python</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')
</code></pre>

<p>I want to change the sentence <code>Tom is a smart boy. He know a lot of thing.</code> into <code>Tom is a smart boy. Tom know a lot of thing.</code> and there is no tutorial or any help available in Python.</p>

<p>All i am able to do is annotate by below code in Python</p>

<p>coreference resolution</p>

<pre><code>output = nlp.annotate(sentence, properties={'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})
</code></pre>

<p>and by parsing for coref </p>

<pre><code>coreferences = output['corefs']
</code></pre>

<p>i get below JSON</p>

<pre><code>coreferences

{u'1': [{u'animacy': u'ANIMATE',
   u'endIndex': 2,
   u'gender': u'MALE',
   u'headIndex': 1,
   u'id': 1,
   u'isRepresentativeMention': True,
   u'number': u'SINGULAR',
   u'position': [1, 1],
   u'sentNum': 1,
   u'startIndex': 1,
   u'text': u'Tom',
   u'type': u'PROPER'},
  {u'animacy': u'ANIMATE',
   u'endIndex': 6,
   u'gender': u'MALE',
   u'headIndex': 5,
   u'id': 2,
   u'isRepresentativeMention': False,
   u'number': u'SINGULAR',
   u'position': [1, 2],
   u'sentNum': 1,
   u'startIndex': 3,
   u'text': u'a smart boy',
   u'type': u'NOMINAL'},
  {u'animacy': u'ANIMATE',
   u'endIndex': 2,
   u'gender': u'MALE',
   u'headIndex': 1,
   u'id': 3,
   u'isRepresentativeMention': False,
   u'number': u'SINGULAR',
   u'position': [2, 1],
   u'sentNum': 2,
   u'startIndex': 1,
   u'text': u'He',
   u'type': u'PRONOMINAL'}],
 u'4': [{u'animacy': u'INANIMATE',
   u'endIndex': 7,
   u'gender': u'NEUTRAL',
   u'headIndex': 4,
   u'id': 4,
   u'isRepresentativeMention': True,
   u'number': u'SINGULAR',
   u'position': [2, 2],
   u'sentNum': 2,
   u'startIndex': 3,
   u'text': u'a lot of thing',
   u'type': u'NOMINAL'}]}
</code></pre>

<p>Any help on this?</p>
",Coreference & Relation Extraction,anaphora resolution stanford nlp using python trying anaphora resolution code first navigate folder downloaded stanford module run command command prompt initialize stanford nlp module execute code python want change sentence tutorial help available python able annotate code python coreference resolution parsing coref get json help
What is the difference between machine learning and deep learning in building a chatbot?,"<p>To be more specific, The traditional chatbot framework consists of 3 components: </p>

<ol>
<li>NLU (1.intent classification 2. entity recognition) </li>
<li>Dialogue Management (1. DST 2. Dialogue Policy) </li>
<li>NLG. </li>
</ol>

<p>I am just confused that If I use a deep learning model(<code>seq2seq</code>, <code>lstm</code>, <code>transformer</code>, <code>attention</code>, <code>bert</code>…) to train a chatbot, Is it cover all those 3 components? If so, could you explain more specifically how it related to those 3 parts? If not, how can I combine them?</p>

<p>For example, I have built a closed-domain chatbot, but it is only task-oriented which cannot handle the other part like greeting… And it can’t handle the problem of Coreference Resolution (it seems doesn't have Dialogue Management).</p>
",Coreference & Relation Extraction,difference machine learning deep learning building chatbot specific traditional chatbot framework consists component nlu intent classification entity recognition dialogue management dst dialogue policy nlg confused use deep learning model train chatbot cover component could explain specifically related part combine example built closed domain chatbot task oriented handle part like greeting handle problem coreference resolution seems dialogue management
Epicene pronoun reference resolution ambiguity,"<p><strong>Context:</strong> I am developing resolution strategies of 2nd-person <em>you</em> sequences into 3rd-person epicene (gender-neutral, singular) <em>they</em> sequences. These are unique recombinable sequences, not unlike a chatbot. The majority, in this case, have been solvable through simple rule-based string replacement with perplexity scoring for objective (<em>them</em>) cases, and a distinct strategy for imperative and prohibitive statements. </p>

<p><strong>Problem:</strong> I am coming up against a seemingly hard limit, however, for sequences that contain both a reference to a plural <em>they</em> and the singular epicene <em>they</em>. Tokenization and dependency parsing as so far yield little to start with. SpaCy has been my primary software.  </p>

<p>(Note in the examples below that were the pronoun not the epicene use of <em>they</em> but gendered, the ambiguity would be solved. This is however not an option.)</p>

<p><strong>Examples:</strong></p>

<ol>
<li><p>""When someone plays games with you, they make the rules."" <em>becomes</em> ""When plays games with them, they make the rules.""</p></li>
<li><p>""There are no rules but the ones you make with each other."" <em>becomes</em> ""There are no rules but the ones they make with each other.""</p></li>
<li><p>""Frustration and delays will occur. They will ultimately teach you persistence."" <em>becomes</em> ""Frustration and delays will occur. They will ultimately teach them persistence.""     </p></li>
</ol>

<p>And so on.</p>

<p><strong>Questions:</strong></p>

<ol>
<li><p>Is there anything missing from my understanding of the problem? Am I articulating it adequately?</p></li>
<li><p>Would coreference modeling be able to sort out the difference between the anaphora? </p></li>
<li><p>What else should I attempt in order to flag these? I don't need to ""resolve"" them necessarily.</p></li>
</ol>
",Coreference & Relation Extraction,epicene pronoun reference resolution ambiguity context developing resolution strategy nd person sequence rd person epicene gender neutral singular sequence unique recombinable sequence unlike chatbot majority case solvable simple rule based string replacement perplexity scoring objective case distinct strategy imperative prohibitive statement problem coming seemingly hard limit however sequence contain reference plural singular epicene tokenization dependency parsing far yield little start spacy ha primary software note example pronoun epicene use gendered ambiguity would solved however option example someone play game make rule becomes play game make rule rule one make becomes rule one make frustration delay occur ultimately teach persistence becomes frustration delay occur ultimately teach persistence question anything missing understanding problem articulating adequately would coreference modeling able sort difference anaphora else attempt order flag need resolve necessarily
Microsoft LUIS: coreference,"<p>I'm looking for solution to be able to manage <a href=""https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30"" rel=""nofollow noreferrer"">coreference</a> into conversation with Microsoft LUIS and Microsoft Bot Framework SDK.</p>

<p>Is there any existing solution into Microsoft's environment ?</p>
",Coreference & Relation Extraction,microsoft luis coreference looking solution able manage coreference conversation microsoft luis microsoft bot framework sdk existing solution microsoft environment
replace coreference resolution words in sentence using stanford corenlp,"<p>I am using stanford corenlp to get the coreference resolution. This is the code iam using now.</p>

<pre><code>from stanfordnlp.server import CoreNLPClient

# set up the client
client = CoreNLPClient(properties={'annotators': 'coref', 'coref.algorithm' : 'statistical'},max_char_length=10000000, timeout=600000000, memory='16G')

# submit the request to the server
ann = client.annotate(text)    

mychains = list()
chains = ann.corefChain
for chain in chains:
    mychain = list()
    # Loop through every mention of this chain
    for mention in chain.mention:
        # Get the sentence in which this mention is located, and get the words which are part of this mention
        # (we can have more than one word, for example, a mention can be a pronoun like ""he"", but also a compound noun like ""His wife Michelle"")
        words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]
        #print(words_list)
        #build a string out of the words of this mention
        ment_word = ' '.join([x.word for x in words_list])
        print(ment_word)
        mychain.append(ment_word)
    mychains.append(mychain)

for chain in mychains:
    print(' &lt;-&gt; '.join(chain))
</code></pre>

<p>Iam getting outputs like this,</p>

<pre><code>Deepika &lt;-&gt; She &lt;-&gt; her
he &lt;-&gt; a dog &lt;-&gt; him &lt;-&gt; He
</code></pre>

<p>I want the replaced sentence. Anyone tell how to modify this code to get the replaced sentence. I am using latest Stanford Corenlp not the old one. </p>
",Coreference & Relation Extraction,replace coreference resolution word sentence using stanford corenlp using stanford corenlp get coreference resolution code iam using iam getting output like want replaced sentence anyone tell modify code get replaced sentence using latest stanford corenlp old one
NLP Coreference resolution,"<p>I am new in NLP domain and was going through this blog:
<a href=""https://blog.goodaudience.com/learn-natural-language-processing-from-scratch-7893314725ff"" rel=""nofollow noreferrer"">https://blog.goodaudience.com/learn-natural-language-processing-from-scratch-7893314725ff</a></p>

<p><a href=""https://i.sstatic.net/614BQ.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/614BQ.gif"" alt=""enter image description here""></a></p>

<blockquote>
  <p>London is the capital of and largest city in England and the United
  Kingdom. Standing on the River Thames in the south-east of England, at
  the head of its 50-mile (80 km) estuary leading to the North Sea,
  London has been a major settlement for two millennia. It was founded
  by the Romans.</p>
</blockquote>

<p>I have the experience in NER and POS tagging using spacy.
I would like to know that how i will link the <strong>london</strong> with <strong>it</strong> like:</p>

<p>London is the capital .....</p>

<p>It has been a major settlement..</p>

<p>It was founded by the Romans....</p>

<p>I have tried the Dependency parser but not able to produce the same result.
<a href=""https://explosion.ai/demos/displacy"" rel=""nofollow noreferrer"">https://explosion.ai/demos/displacy</a></p>

<p>I am open to use any other library, please suggest the right approach to achieve it </p>
",Coreference & Relation Extraction,nlp coreference resolution new nlp domain wa going blog london capital largest city england united kingdom standing river thames south east england head mile km estuary leading north sea london ha major settlement two millennium wa founded roman experience ner po tagging using spacy would like know link london like london capital ha major settlement wa founded roman tried dependency parser able produce result open use library please suggest right approach achieve
How can we implement coreference resolution in a given text using python?,"<p>Currently I have identified the noun phrases in a given text.Now I need to identify whether there are references among them. </p>

<p><strong>For an example:</strong> </p>

<p>There are two doors in a house,door_X and door_Y. 
If both doors are open the house is unsafe.</p>

<p>I need to map the ""doors"" in 2nd sentence with door_X and door_Y</p>
",Coreference & Relation Extraction,implement coreference resolution given text using python currently identified noun phrase given text need identify whether reference among example two door house door x door door open house unsafe need map door nd sentence door x door
Coreference resolution in python nltk using Stanford coreNLP,"<p>Stanford CoreNLP provides coreference resolution <a href=""http://nlp.stanford.edu/software/dcoref.shtml"" rel=""noreferrer"">as mentioned here</a>, also <a href=""https://stackoverflow.com/questions/30954649/coreference-resolution-using-stanford-corenlp"">this thread</a>, <a href=""https://stackoverflow.com/questions/30362691/stanford-corenlp-wrong-coreference-resolution"">this</a>,   provides some insights about its implementation in Java.</p>

<p>However, I am using python and NLTK and I am not sure how can I use Coreference resolution functionality of CoreNLP in my python code. I have been able to set up StanfordParser in NLTK, this is my code so far.</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
stanford_parser_dir = 'stanford-parser/'
eng_model_path = stanford_parser_dir  + ""stanford-parser-models/edu/stanford/nlp/models/lexparser/englishRNN.ser.gz""
my_path_to_models_jar = stanford_parser_dir  + ""stanford-parser-3.5.2-models.jar""
my_path_to_jar = stanford_parser_dir  + ""stanford-parser.jar""
</code></pre>

<p>How can I use coreference resolution of CoreNLP in python?</p>
",Coreference & Relation Extraction,coreference resolution python nltk using stanford corenlp stanford corenlp provides coreference resolution mentioned also however using python nltk sure use coreference resolution functionality corenlp python code able set stanfordparser nltk code far use coreference resolution corenlp python
Stanford CoreNLP Not Respecting Overrides,"<p>I've modified several of the files used by CoreNLP for Coreference Resolution. It doesn't appear to be using that list. Has anyone had success in overriding their base dcoref dictionaries? </p>

<p>Here is my code to get the CorefChain:</p>

<pre><code>public class CorefChains {

    StanfordCoreNLP pipeline;

    public CorefChains() {
        Properties props = new Properties();
        props.put(""dcoref.male"",""male.unigrams.txt"");
        props.put(""dcoref.female"",""female.unigrams.txt"");
        props.put(""dcoref.animate"",""animate.unigrams.txt"");
        props.put(""dcoref.inanimate"",""inanimate.unigrams.txt"");
        props.put(""dcoref.demonym"",""demonyms.txt"");
        props.put(""dcoref.neutral"",""neutral.unigrams.txt"");
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        this.pipeline = new StanfordCoreNLP(props);
    }

    public Map&lt;Integer, CorefChain&gt; getCoreferences(String str) {
        Annotation document = new Annotation(str);
        pipeline.annotate(document);
        Map&lt;Integer, CorefChain&gt; graph = document.get(CorefChainAnnotation.class);
        return graph;
    }
}
</code></pre>

<p>(I have also tried putting the dcoref properties after the pipeline properties.) </p>

<p>Here is what Eclipse's debugger is showing me (I removed Malta from inanimatite.unigrams.txt, neutral.unigrams.txt, and demonyms.txt. I added it to female.unigrams.txt and animate.unigrams.txt). </p>

<p><a href=""https://i.sstatic.net/Xm4vh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xm4vh.png"" alt=""Eclipse Debugger""></a></p>
",Coreference & Relation Extraction,stanford corenlp respecting override modified several file used corenlp coreference resolution appear using list ha anyone success overriding base dcoref dictionary code get corefchain also tried putting dcoref property pipeline property eclipse debugger showing removed malta inanimatite unigrams txt neutral unigrams txt demonyms txt added female unigrams txt animate unigrams txt
Reducing the execution time of Stanford corenlp?,"<p>I am using Stanford Core NLP in my project,
I am performing Parsing, Pos tagging, Ner, COREF resolution  of nearly 100 sentences.
But it is consuming too much of time.</p>

<p>How to use Threads to reduce the execution time of Stanford coreNlp?</p>
",Coreference & Relation Extraction,reducing execution time stanford corenlp using stanford core nlp project performing parsing po tagging ner coref resolution nearly sentence consuming much time use thread reduce execution time stanford corenlp
How to generate custom triples with OpenIEDemo.java provided by stanford-nlp,"<p>I have trained custom NER and Relation extraction model and I have checked generating triples with corenlp server but when I'm using <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/naturalli/OpenIEDemo.java"" rel=""nofollow noreferrer"">OpenIEDemo.java</a>
to generate triples it's generating triples having relations ""has"" and ""have"" only but not the relations on which I have trained my Relation Extraction model on.</p>

<p>I'm loading custom NER and Relation Extraction model while running the same script. Here is my OpenIEDemo.java file...</p>

<pre><code>package edu.stanford.nlp.naturalli;

import edu.stanford.nlp.ie.util.RelationTriple;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.PropertiesUtils;

import java.util.Collection;
import java.util.List;
import java.util.Properties;

/**
 * A demo illustrating how to call the OpenIE system programmatically.
 * You can call this code with:
 *
 * &lt;pre&gt;
 *   java -mx1g -cp stanford-openie.jar:stanford-openie-models.jar edu.stanford.nlp.naturalli.OpenIEDemo
 * &lt;/pre&gt;
 *
 */
public class OpenIEDemo {

  private OpenIEDemo() {} // static main

  public static void main(String[] args) throws Exception {

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, depparse, natlog, openie"");
    props.setProperty(""ner.model"", ""./ner/ner-model.ser.gz"");
    props.setProperty(""sup.relation.model"", ""./relation_extractor/relation_model_pipeline.ser.ser"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // Annotate an example document.
    String text;
    if (args.length &gt; 0) {
      text = args[0];
    } else {
      text = ""Obama was born in Hawaii. He is our president."";
    }
    Annotation doc = new Annotation(text);
    pipeline.annotate(doc);

    // Loop over sentences in the document
    int sentNo = 0;
    for (CoreMap sentence : doc.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""Sentence #"" + ++sentNo + "": "" + sentence.get(CoreAnnotations.TextAnnotation.class));

      // Print SemanticGraph
      System.out.println(sentence.get(SemanticGraphCoreAnnotations.EnhancedDependenciesAnnotation.class).toString(SemanticGraph.OutputFormat.LIST));

      // Get the OpenIE triples for the sentence
      Collection&lt;RelationTriple&gt; triples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);

      // Print the triples
      for (RelationTriple triple : triples) {
        System.out.println(triple.confidence + ""\t"" +
            triple.subjectLemmaGloss() + ""\t"" +
            triple.relationLemmaGloss() + ""\t"" +
            triple.objectLemmaGloss());
      }

      // Alternately, to only run e.g., the clause splitter:
      List&lt;SentenceFragment&gt; clauses = new OpenIE(props).clausesInSentence(sentence);
      for (SentenceFragment clause : clauses) {
        System.out.println(clause.parseTree.toString(SemanticGraph.OutputFormat.LIST));
      }
      System.out.println();
    }
  }
}
</code></pre>

<p>Thanks in advance.</p>
",Coreference & Relation Extraction,generate custom triple openiedemo java provided stanford nlp trained custom ner relation extraction model checked generating triple corenlp server using openiedemo java generate triple generating triple relation ha relation trained relation extraction model loading custom ner relation extraction model running script openiedemo java file thanks advance
How to replace a word by its most representative mention using Stanford CoreNLP Coreferences module,"<p>I am trying to figure out the way to rewrite sentences by ""resolving"" (replacing words with) their coreferences using Stanford Corenlp's Coreference module.</p>

<p>The idea is to rewrite a sentence like the following :</p>

<blockquote>
  <p>John drove to Judy’s house. He made her dinner.</p>
</blockquote>

<p>into </p>

<blockquote>
  <p>John drove to Judy’s house. John made Judy dinner.</p>
</blockquote>

<p>Here's the code I've been fooling around with : </p>

<pre><code>    private void doTest(String text){
    Annotation doc = new Annotation(text);
    pipeline.annotate(doc);


    Map&lt;Integer, CorefChain&gt; corefs = doc.get(CorefChainAnnotation.class);
    List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);


    List&lt;String&gt; resolved = new ArrayList&lt;String&gt;();

    for (CoreMap sentence : sentences) {

        List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);

        for (CoreLabel token : tokens) {

            Integer corefClustId= token.get(CorefCoreAnnotations.CorefClusterIdAnnotation.class);
            System.out.println(token.word() +  "" --&gt; corefClusterID = "" + corefClustId);


            CorefChain chain = corefs.get(corefClustId);
            System.out.println(""matched chain = "" + chain);


            if(chain==null){
                resolved.add(token.word());
            }else{

                int sentINdx = chain.getRepresentativeMention().sentNum -1;
                CoreMap corefSentence = sentences.get(sentINdx);
                List&lt;CoreLabel&gt; corefSentenceTokens = corefSentence.get(TokensAnnotation.class);

                String newwords = """";
                CorefMention reprMent = chain.getRepresentativeMention();
                System.out.println(reprMent);
                for(int i = reprMent.startIndex; i&lt;reprMent.endIndex; i++){
                    CoreLabel matchedLabel = corefSentenceTokens.get(i-1); //resolved.add(tokens.get(i).word());
                    resolved.add(matchedLabel.word());

                    newwords+=matchedLabel.word()+"" "";

                }




                System.out.println(""converting "" + token.word() + "" to "" + newwords);
            }


            System.out.println();
            System.out.println();
            System.out.println(""-----------------------------------------------------------------"");

        }

    }


    String resolvedStr ="""";
    System.out.println();
    for (String str : resolved) {
        resolvedStr+=str+"" "";
    }
    System.out.println(resolvedStr);


}
</code></pre>

<p>The best output I was able to achieve for now is </p>

<blockquote>
  <p>John drove to Judy 's 's Judy 's house . John made Judy 's her dinner . </p>
</blockquote>

<p>which is not very brilliant ... </p>

<p>I'm pretty sure there is a MUCH easier way to do what I am trying to achieve.</p>

<p>Ideally, I would like to reorganize the sentence as a list of CoreLabels, so that I could keep the other data they have attached to them. </p>

<p>Any help appreciated.</p>
",Coreference & Relation Extraction,replace word representative mention using stanford corenlp coreference module trying figure way rewrite sentence resolving replacing word coreference using stanford corenlp coreference module idea rewrite sentence like following john drove judy house made dinner john drove judy house john made judy dinner code fooling around best output wa able achieve john drove judy judy house john made judy dinner brilliant pretty sure much easier way trying achieve ideally would like reorganize sentence list corelabels could keep data attached help appreciated
NLP - Finding parenthetical sentence,"<p>I need help to find parenthetical information in a sentence.</p>

<p>E.g.,</p>

<ol>
<li>Some foods, <strong>sugar for example</strong>, are not good for us.</li>
<li>Timothy, <strong>who lives near Stonehenge</strong>, goes to church regularly.
Are there any libraries in NLP to find parenthetical information in a sentence?
Chunking, Co-reference resolution, Dependency parser in NLP will not give efficient results in finding parenthetical sentences. And we cannot go by the pattern alone. 
E.g.,</li>
<li>commas , .......,</li>
<li>round brackets (.......)</li>
<li>long dashes —.......—</li>
</ol>

<p>There are so many exceptions to the above patterns. </p>
",Coreference & Relation Extraction,nlp finding parenthetical sentence need help find parenthetical information sentence e g food sugar example good u timothy life near stonehenge go church regularly library nlp find parenthetical information sentence chunking co reference resolution dependency parser nlp give efficient result finding parenthetical sentence go pattern alone e g comma round bracket long dash many exception pattern
"Transfer learning from Relation extraction to hate detection, does it make sense?","<p>so I recently finished my Neural Relation Extraction (RE) model the results were quite notable. I wanted to use transfer learning  with RE being the source task and Hate Detection (HD) is the target task. Does that make sense ?</p>

<p>The thing is in RE I labelled each word in the sentence whether it represents a Entity or Relation or Nothing, so I used categorical cross entropy and treated it a labelling problem, however in HD I will process the whole sentence not each word individually </p>

<p>So do you think using Transfer learning makes sense or the source topic and target topic must be somehow related.</p>

<p>Thank you</p>
",Coreference & Relation Extraction,transfer learning relation extraction hate detection doe make sense recently finished neural relation extraction model result quite notable wanted use transfer learning source task hate detection hd target task doe make sense thing labelled word sentence whether represents entity relation nothing used categorical cross entropy treated labelling problem however hd process whole sentence word individually think using transfer learning make sense source topic target topic must somehow related thank
Is it possible to give an input set of named entities as well as a set of sentences to coreNLP for coreference resolution,"<p>I am trying to do co-reference resolution on a data-set however Stanford's named entity recogniser is unable to properly classify the named entities within my set of text. As such is it possible to give the Stanford co-reference module a set of named entities and the text from a different NER such as NLTK as from my research so far it seems like you cannot split the pipeline up when it does coref? Ideally I would be able to use stanfords NER and then update it using my named entities from another NER before passing it to the coref module. Any help would be greatly appreciated. </p>

<p>I am doing this all in Python currently so I have tested a variety of Python wrappers for stanfordcoreNLP all of which seem to only have the catch all option of annotation to do coref thus making it not possible to achieve what I need. I also looked through the coreNLP documentation and could not find a clear answer as to whether, even in Java or using the server, this would be possible.</p>
",Coreference & Relation Extraction,possible give input set named entity well set sentence corenlp coreference resolution trying co reference resolution data set however stanford named entity recogniser unable properly classify named entity within set text possible give stanford co reference module set named entity text different ner nltk research far seems like split pipeline doe coref ideally would able use stanford ner update using named entity another ner passing coref module help would greatly appreciated python currently tested variety python wrapper stanfordcorenlp seem catch option annotation coref thus making possible achieve need also looked corenlp documentation could find clear answer whether even java using server would possible
When we run coreference resolution program it will throw an error how can i solve?,"<p>I am new to coreference resolution when we run the program it will throw an error i m very confused to resolve please help me with it</p>

<pre><code>    Annotation document = new Annotation(""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);
    System.out.println(""---"");
    System.out.println(""coref chains"");
    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
      System.out.println(""\t"" + cc);
    }
    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
      System.out.println(""---"");
      System.out.println(""mentions"");
      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
        System.out.println(""\t"" + m);
       }
</code></pre>

<p>The error occured is mentioned below</p>

<pre><code>&gt; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.Arrays.copyOfRange(Unknown Source)
at java.lang.String.&lt;init&gt;(Unknown Source)
at edu.stanford.nlp.util.StringUtils.splitOnChar(StringUtils.java:537)
at edu.stanford.nlp.coref.data.Dictionaries.loadGenderNumber(Dictionaries.java:405)
at edu.stanford.nlp.coref.data.Dictionaries.&lt;init&gt;(Dictionaries.java:676)
at edu.stanford.nlp.coref.data.Dictionaries.&lt;init&gt;(Dictionaries.java:576)
at edu.stanford.nlp.coref.CorefSystem.&lt;init&gt;(CorefSystem.java:32)
at edu.stanford.nlp.pipeline.CorefAnnotator.&lt;init&gt;(CorefAnnotator.java:66)
at edu.stanford.nlp.pipeline.AnnotatorImplementations.coref(AnnotatorImplementations.java:196)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$53(StanfordCoreNLP.java:555)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$24/544724190.apply(Unknown Source)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$69(StanfordCoreNLP.java:625)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$37/1673605040.get(Unknown Source)
at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:201)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:194)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:181)
at test.test.main(test.java:17)
</code></pre>
",Coreference & Relation Extraction,run coreference resolution program throw error solve new coreference resolution run program throw error confused resolve please help error occured mentioned
Coreference Resolution using OpenNLP,"<p>I want to do <em>""coreference resolution""</em> using OpenNLP. Documentation from Apache (<a href=""http://incubator.apache.org/opennlp/documentation/manual/opennlp.html#tools.coref"" rel=""noreferrer"">Coreference Resolution</a>) doesn't cover how to do <em>""coreference resolution""</em>. Does anybody have any docs/tutorial how to do this?</p>
",Coreference & Relation Extraction,coreference resolution using opennlp want coreference resolution using opennlp documentation apache coreference resolution cover coreference resolution doe anybody doc tutorial
Coreference Resolution with CoreNLP,"<p>I am trying to get CoreNLP to access CorefChains. My intention is that words like ""he, she, ..."" will be substituted by their best mention, but I am not able to access the CorefChains (they are always null). </p>

<pre><code>    public static void main (String [] args) {
         Properties props = new Properties();
         props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,dcoref"");
         props.put(""dcoref.score"", true);
         StanfordCoreNLP corefPipeline = new StanfordCoreNLP(props);
         String text = ""Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008."";
         Annotation document = new Annotation(text);
         corefPipeline.annotate(document);
         // Chains is always null
         Map&lt;Integer, CorefChain&gt; chains = document.get(CorefCoreAnnotations.CorefChainAnnotation.class);
}
</code></pre>
",Coreference & Relation Extraction,coreference resolution corenlp trying get corenlp access corefchains intention word like substituted best mention able access corefchains always null
Coreferencing in Google NLP API,"<p>I was testing the Google NLP API and found out that it works only for a single sentence. I was wondering if it works for multiple sentences at once with coreference as that would help a lot. I know about Stanford Coreference resolution and other tools but want to know if Goggle  NLP API does it?
Or can someone recommend a good accurate tool for coreference resolution.</p>
",Coreference & Relation Extraction,coreferencing google nlp api wa testing google nlp api found work single sentence wa wondering work multiple sentence coreference would help lot know stanford coreference resolution tool want know goggle nlp api doe someone recommend good accurate tool coreference resolution
How to get entities with direction in relation extraction?,"<p>I have been working with relation extraction for a week. But what I need is direction between two entities, such as Company_x got bought by Company_y. So the model should predict the entities like Company_y->bought-> Company_X. Any models you guys think will be helpful for this?</p>
",Coreference & Relation Extraction,get entity direction relation extraction working relation extraction week need direction two entity company x got bought company model predict entity like company bought company x model guy think helpful
How to access data from a MMAX2 annotated XML corpus,"<p>I have an annotated corpus for the task of Coreference Resolution. Can you let me know how to extract the data from xml file. I did the following but not work.</p>

<pre><code>from lxml import objectify
import pandas as pd

    xml = objectify.parse(open('Dari_Coref_2_coref_level.xml'))
    root = xml.getroot()

    df = pd.DataFrame(columns='markable')

    for i in range(0, 2):
        obj = root.getchildren()[i].getchildren()
        row = dict(zip(['markable'], [obj[0].text]))
        row_s = pd.Series(row)
        row_s.name = i
        df = df.append(row_s)

 print(df)
</code></pre>

<p>And the structure of my xml file is like:</p>

<pre><code> &lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;!DOCTYPE markables SYSTEM ""markables.dtd""&gt;
&lt;markables xmlns=""www.eml.org/NameSpaces/coref""&gt;
&lt;markable id=""markable_1"" span=""word_1..word_4"" mentiontype=""ne""  
coref_class=""set_1""  mmax_level=""coref""  coreftype=""ident"" /&gt;
&lt;markable id=""markable_3"" span=""word_33..word_34"" mentiontype=""ne""  
coref_class=""set_2""  mmax_level=""coref""  coreftype=""ident"" /&gt;
&lt;markable id=""markable_2"" span=""word_5..word_9"" mentiontype=""np""  
coref_class=""set_1""  mmax_level=""coref""  coreftype=""ident"" /&gt;
&lt;markable id=""markable_5"" span=""word_89..word_90"" mentiontype=""np""  
coref_class=""set_3""  mmax_level=""coref""  coreftype=""ident"" /&gt;
&lt;markable id=""markable_4"" span=""word_35..word_44"" mentiontype=""np""  
coref_class=""set_2""  mmax_level=""coref""  coreftype=""ident"" /&gt;
&lt;markable id=""markable_7"" span=""word_124..word_126"" mentiontype=""ne""  
coref_class=""set_4""  mmax_level=""coref""  coreftype=""ident"" /&gt;
&lt;markable id=""markable_6"" span=""word_91..word_95"" mentiontype=""np""  
coref_class=""set_3""  mmax_level=""coref""  coreftype=""ident"" /&gt;
&lt;/markables&gt;
</code></pre>
",Coreference & Relation Extraction,access data mmax annotated xml corpus annotated corpus task coreference resolution let know extract data xml file following work structure xml file like
identity vs appositive coreference,"<p>what is the difference between identity coreference and appositive coreference?
In the following sentence for example:</p>

<p><code>Mohammad traveled to Washington last week. He was on leave of absence. The 30-year old man stayed in an hotel overlooking the National Mall.</code></p>

<p>As per what I understand, there is an identity coreference between <code>Mohammad</code> and <code>he</code>. Is there an appositive coreference between <code>he</code> and <code>the 30-year old man</code>? or <code>Mohammad</code> and <code>the 30-year old man'</code>? </p>
",Coreference & Relation Extraction,identity v appositive coreference difference identity coreference appositive coreference following sentence example per understand identity coreference appositive coreference
Corpus for Coreference Resolution Training,"<p>I have built a neural network for coreference resolution. I need a corpus for training the neural network. Since the OntoNotes corpus has become private is there an alternative? The corpus need not be large. Any alternative with few annotated documents is fine. I just need the coreference annotation other ones are not important to me.</p>

<p>PS: The language in context is ENGLISH.</p>
",Coreference & Relation Extraction,corpus coreference resolution training built neural network coreference resolution need corpus training neural network since ontonotes corpus ha become private alternative corpus need large alternative annotated document fine need coreference annotation one important p language context english
Subject Extraction of a paragraph/document using NLP,"<p>I am trying to build a subject extractor, simply put, read all the sentences of a paragraph and make a calculated guess to what the subject of the paragraph/article/document is. I <em>might</em> even upgrade it to a summerize depending on the progress I make.</p>

<p>There is a great deal of information on the internet. It is difficult to understand all of it and select a correct path, as I am not well versed with NLP.</p>

<p>I was hoping someone with some experience could point me in the right direction.</p>

<ol>
<li><p>I am NOT looking for a linguistic computation model, but rather an n-gram or neural network approach, something that has been done recently.</p></li>
<li><p>I am also looking into coreference resolution using n-grams, if anyone has any leads on that, it is much appreciated. Slightly familiar with the Stanford Coreferential Solver, but don't want to use it as is.</p></li>
</ol>

<p>Any information, ideas and opinions are welcome.</p>
",Coreference & Relation Extraction,subject extraction paragraph document using nlp trying build subject extractor simply put read sentence paragraph make calculated guess subject paragraph article document might even upgrade summerize depending progress make great deal information internet difficult understand select correct path well versed nlp wa hoping someone experience could point right direction looking linguistic computation model rather n gram neural network approach something ha done recently also looking coreference resolution using n gram anyone ha lead much appreciated slightly familiar stanford coreferential solver want use information idea opinion welcome
StanfordNLP custom relation extraction,"<p>I'm trying to make a custom relation extractor using Stanford NLP Core. I downloaded their file kill.corp. I trained this file.</p>

<p>Look at the example: <a href=""http://nlp.stanford.edu/software/relationExtractor.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/relationExtractor.shtml</a></p>

<p>Now, when I use the following command to identify the kill relation by loading my model (kill model).</p>

<p><code>java -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -sup.relation.model=tmp/roth_relation_model_pipeline.ser -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref,relation</code></p>

<p>Because I did not have input file the terminal allows me to write a sentence - ""Joe killed Jack"" and it doesn't identify any relations. However, when I write a sentence with Live_In already built in StanfordNLP ""Julia lives in California"" it works fine.</p>

<p>Extracted the following MachineReading relation mentions:</p>

<blockquote>
  <p>RelationMention [type=Located_In, start=0, end=4, {Located_In, 0.37758087211058006; _NR, 0.2433177585101524; OrgBased_In, 0.15485273181687478; Live_In, 0.14393618927198826; Work_For, 0.08031244829040446}
      EntityMention [type=PEOPLE, objectId=EntityMention-3, hstart=0, hend=1, estart=0, eend=1, headPosition=0, value=""Julia"", corefID=-1]
      EntityMention [type=O, objectId=EntityMention-4, hstart=3, hend=4, estart=3, eend=4, headPosition=3, value=""California"", corefID=-1]</p>
</blockquote>

<p>How can I add kill as a relation and get my custom relation model to identify it?</p>
",Coreference & Relation Extraction,stanfordnlp custom relation extraction trying make custom relation extractor using stanford nlp core downloaded file kill corp trained file look example use following command identify kill relation loading model kill model input file terminal allows write sentence joe killed jack identify relation however write sentence live already built stanfordnlp julia life california work fine extracted following machinereading relation mention relationmention type located start end located nr orgbased live work entitymention type people objectid entitymention hstart hend estart eend headposition value julia corefid entitymention type objectid entitymention hstart hend estart eend headposition value california corefid add kill relation get custom relation model identify
Is it possible to get singleton mentions by stanford-nlp software?,"<p>We want to detect the mentions in our data by using the stanford-nlp coref annotator. It seems that the coref annotator does not create singleton chains (i.e. chains with only one mention) by default. Is it possible to configure the annotator so that the resulting annotation contains the signleton chains in addition to the non-singleton ones.</p>

<p>Thank you.</p>
",Coreference & Relation Extraction,possible get singleton mention stanford nlp software want detect mention data using stanford nlp coref annotator seems coref annotator doe create singleton chain e chain one mention default possible configure annotator resulting annotation contains signleton chain addition non singleton one thank
Interpreting output of Stanford CoreNLP&#39;s Coreference Resolution,"<p>Is there any API or a way to get <strong>Stanford CoreNLP's Coreference Resolution output</strong> in the form of <strong>text</strong> and not xml, so that it can be interpreted easily?</p>
",Coreference & Relation Extraction,interpreting output stanford corenlp coreference resolution api way get stanford corenlp coreference resolution output form text xml interpreted easily
Dynamically add properties to StanfordCoreNLP Annotator or Pipeline,"<p>Below my situation.</p>

<p>I have a class TextProcessor that process a text. I need to find the coreferences in such a text and then extract the informations with the Stanford's tool OpenIE. I use this two pipelines:</p>

<blockquote>
  <p><strong>""tokenize,ssplit,pos,lemma,ner,parse,mention,coref""</strong> for coreferences.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p><strong>""tokenize,ssplit,pos,lemma,depparse,natlog,openie""</strong> for Information Extraction.</p>
</blockquote>

<p>It requires lot of time to use them separately for analyzing a single text, but for the moment I have to do so cause using them together requires a large amount of memory and the pipeline would exeed my memory's bounds. </p>

<pre><code>public class TextProcessor(){
    Properties props;
    StanfordCoreNLP pipeline;

    public TextProcessor() {
        props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,mention,coref"");
        pipeline = new StanfordCoreNLP(props);
    }


    // Performs NER and COREF 
     public void process(String text) {
         Annotation document = new Annotation(malware.getDescription());
         pipeline.annotate(document);

         // Process text (tokenization, pos, lemma, ner, coref)....
     }

     public void extractInformation(String document) {
         props = new Properties();
         props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
         pipeline = new StanfordCoreNLP(props);

         Annotation doc = new Annotation(document);
         pipeline.annotate(doc);

         // Extract informations from doc ...
    }
</code></pre>

<p>Is there a way to put together the two pipelines dynamically? I mean, something like this:</p>

<blockquote>
  <p>1) <strong>""tokenize,ssplit,pos,lemma,ner,depparse,mention,coref</strong>""</p>
  
  <p>2) ""tokenize,ssplit,pos,lemma,ner,depparse,mention,coref,<strong>natlog,openie</strong>"".</p>
</blockquote>

<p>I tried to return an Annotation object from the first method <code>process(String text)</code> and then add the other three properties to it in the method  <code>extractInformation(String text)</code>, like this:</p>

<pre><code>     public Annotation process(String text) {
         Annotation document = new Annotation(malware.getDescription());
         pipeline.annotate(document);

         // Process text (tokenization, pos, lemma, ner, coref)....
         return document;
     }

     public void extractInformation(Annotation document) {
         props.setProperty(""annotators"",""depparse,natlog,openie"");
         pipeline = new StanfordCoreNLP(props);
         pipeline.annotate(document);

         // Extract informations from doc ...
    }
</code></pre>

<p>But I get this error:</p>

<p><code>annotator ""depparse"" requires annotation ""TextAnnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos</code>.</p>

<p>I thought that adding the new three properties (depparse, natlog, openie) to an already annotated document (with tokenize,ssplit,pos) would work, but it didn't. </p>

<p>So, is there a way to add those properties to the oldest pipeline avoiding to perform again all the pipeline (plus the new properties) and avoid the memory to exceed its bounds?</p>

<p><br><br>
<strong>UPDATE</strong></p>

<p>All I needed to do was</p>

<pre><code>     public Annotation process(String text) {
         Annotation document = new Annotation(malware.getDescription());
         pipeline.annotate(document);

         // Process text (tokenization, pos, lemma, ner, coref)....
         StanfordCoreNLP.clearAnnotatorPool(); // &lt;-- Added: to get rid of the models and solve the memory issue
         return document;
     }

     public void extractInformation(Annotation document) {
         props.setProperty(""annotators"",""natlog,openie"");

         props.setProperty(""enforceRequirements"", ""false"") //&lt;-- Added

         pipeline = new StanfordCoreNLP(props);
         pipeline.annotate(document);

         // Extract informations from doc ...
    }
</code></pre>

<p>Alternatively, you can use: </p>

<pre><code>pipeline = new StanfordCoreNLP(props, false);
</code></pre>

<p>in extractInformation(Annotation document).</p>
",Coreference & Relation Extraction,dynamically add property stanfordcorenlp annotator pipeline situation class textprocessor process text need find coreference text extract information stanford tool openie use two pipeline tokenize ssplit po lemma ner parse mention coref coreference tokenize ssplit po lemma depparse natlog openie information extraction requires lot time use separately analyzing single text moment cause using together requires large amount memory pipeline would exeed memory bound way put together two pipeline dynamically mean something like tokenize ssplit po lemma ner depparse mention coref tokenize ssplit po lemma ner depparse mention coref natlog openie tried return annotation object first method add three property method like get error thought adding new three property depparse natlog openie already annotated document tokenize ssplit po would work way add property oldest pipeline avoiding perform pipeline plus new property avoid memory exceed bound update needed wa alternatively use extractinformation annotation document
How to build POS-tagged corpus with NLTK?,"<p>I try to build a POS-tagged corpus from external <code>.txt</code> files for chunking and entity and relation extraction. So far I have found a cumbersome multistep solution:</p>

<ol>
<li><p>Read files with into a plain text corpus:</p>

<pre><code>from nltk.corpus.reader import PlaintextCorpusReader
my_corp = PlaintextCorpusReader(""."", r"".*\.txt"")
</code></pre></li>
<li><p>Tag corpus with built-in Penn POS-tagger:</p>

<pre><code>my_tagged_corp= nltk.batch_pos_tag(my_corp.sents())
</code></pre></li>
</ol>

<p>(By the way, at this pont Python threw an error: <code>NameError: name 'batch' is not defined</code>)</p>

<ol start=""3"">
<li><p>Write out tagged sentences into file:</p>

<pre><code>taggedfile = open(""output.txt"" , ""w"")
for sent in dd_tagged:
    line = "" "".join( w+""/""+t for (w, t) in sent )
taggedfile.write(line + ""\n"")
taggedfile.close ()
</code></pre></li>
<li><p>And finally, read this output again as tagged corpus:</p>

<pre><code>from nltk.corpus.reader import TaggedCorpusReader
my_corpus2 = TaggedCorpusReader(""."",r""output.txt"")
</code></pre></li>
</ol>

<p>That is all very inconvenient for a quite common task (chunking always requests tagged corpus). My question is: is there a more compact and elegant way to implement this? A corpus reader that gets raw input files and a tagger at the same time for instance?</p>
",Coreference & Relation Extraction,build po tagged corpus nltk try build po tagged corpus external file chunking entity relation extraction far found cumbersome multistep solution read file plain text corpus tag corpus built penn po tagger way pont python threw error write tagged sentence file finally read output tagged corpus inconvenient quite common task chunking always request tagged corpus question compact elegant way implement corpus reader get raw input file tagger time instance
Custom relation extraction model using Stanford Core NLP doesn&#39;t find any relations,"<p>I trained a custom model for relation extraction using Stanford Core NLP's <a href=""http://nlp.stanford.edu/software/relationExtractor.html"" rel=""nofollow noreferrer"">example</a>. But when I run the model, it doesn't find any relations -- even when I use sentences directly from my training set. I used a verrrry small training set (20 examples) just to make sure I could get the model to train. Even though my training set is ridiculously small, I would still expect the model to work, just very poorly. Why isn't the model able to find any relations?</p>

<p>Also, I wanted to name my relation ""affordance"", but when I try to do so in my dataset, I get a NullPointerException when I try to train. If I change the name of my relation in the dataset to ""kill"" it suddenly works. I'm assuming that since ""kill"" is one of the examples relations Stanford gives, it's been added to some file. Does anyone know how I could rename my relation?</p>

<p>Thank you so much!</p>

<hr>

<p>Example Training Set:</p>

<pre><code>3   Peop    0   O   NNP Alice   O   O   O
3   O   1   O   VBD was O   O   O
3   O   2   O   VBG beginning   O   O   O
3   O   3   O   TO  to  O   O   O
3   O   4   O   VB  get O   O   O
3   O   5   O   RB  very    O   O   O
3   O   6   O   JJ  tired   O   O   O
3   O   7   O   IN  of  O   O   O
3   O   8   O   VBG sitting O   O   O
3   O   9   O   IN  by  O   O   O
3   O   10  O   PRP$    her O   O   O
3   O   11  O   NN  sister  O   O   O
3   O   12  O   IN  on  O   O   O
3   O   13  O   DT  the O   O   O
3   O   14  O   NN  bank    O   O   O
3   O   15  O   .   .   O   O   O

8   14  kill

4   O   0   O   RB  Once    O   O   O
4   O   1   O   CC  or  O   O   O
4   O   2   O   RB  twice   O   O   O
4   O   3   O   PRP she O   O   O
4   O   4   O   VBD had O   O   O
4   O   5   O   VBN peeped  O   O   O
4   O   6   O   IN  into    O   O   O
4   O   7   O   DT  the O   O   O
4   O   8   O   NN  book    O   O   O
4   O   9   O   PRP$    her O   O   O
4   O   10  O   NN  sister  O   O   O
4   O   11  O   VBD was O   O   O
4   O   12  O   VBG reading O   O   O
4   O   13  O   ,   ,   O   O   O
4   O   14  O   CC  but O   O   O
4   O   15  O   PRP it  O   O   O
4   O   16  O   VBD had O   O   O
4   O   17  O   DT  no  O   O   O
4   O   18  O   NNS pictures    O   O   O
4   O   19  O   CC  or  O   O   O
4   O   20  O   NN  conversation    O   O   O
4   O   21  O   .   .   O   O   O

12  8   kill

5   O   0   O   IN  So  O   O   O
5   O   1   O   PRP she O   O   O
5   O   2   O   VBD was O   O   O
5   O   3   O   VBG considering O   O   O
5   O   4   O   IN  in  O   O   O
5   O   5   O   PRP$    her O   O   O
5   O   6   O   JJ  own O   O   O
5   O   7   O   NN  mind    O   O   O
5   O   8   O   ,   ,   O   O   O
5   O   9   O   IN  whether O   O   O
5   O   10  O   DT  the O   O   O
5   O   11  O   NN  pleasure    O   O   O
5   O   12  O   IN  of  O   O   O
5   O   13  O   VBG making  O   O   O
5   O   14  O   DT  a   O   O   O
5   O   15  O   NN  daisy-chain O   O   O
5   O   16  O   MD  would   O   O   O
5   O   17  O   VB  be  O   O   O
5   O   18  O   JJ  worth   O   O   O
5   O   19  O   DT  the O   O   O
5   O   20  O   NN  trouble O   O   O
5   O   21  O   IN  of  O   O   O
5   O   22  O   VBG getting O   O   O
5   O   23  O   RB  up  O   O   O
5   O   24  O   CC  and O   O   O
5   O   25  O   VBG picking O   O   O
5   O   26  O   DT  the O   O   O
5   O   27  O   NNS daisies O   O   O
5   O   28  O   .   .   O   O   O

25  27  kill

6   Peop    0   O   NNP Alice   O   O   O
6   O   1   O   VBD opened  O   O   O
6   O   2   O   DT  the O   O   O
6   O   3   O   NN  door    O   O   O
6   O   4   O   CC  and O   O   O
6   O   5   O   VBD found   O   O   O
6   O   6   O   IN  that    O   O   O
6   O   7   O   PRP it  O   O   O
6   O   8   O   VBD led O   O   O
6   O   9   O   IN  into    O   O   O
6   O   10  O   DT  a   O   O   O
6   O   11  O   JJ  small   O   O   O
6   O   12  O   NN  passage O   O   O
6   O   13  O   ,   ,   O   O   O
6   O   14  O   RB  not O   O   O
6   O   15  O   RB  much    O   O   O
6   O   16  O   JJR larger  O   O   O
6   O   17  O   IN  than    O   O   O
6   O   18  O   DT  a   O   O   O
6   O   19  O   NN  rat-hole    O   O   O
6   O   20  O   .   .   O   O   O

1   3   kill
</code></pre>
",Coreference & Relation Extraction,custom relation extraction model using stanford core nlp find relation trained custom model relation extraction using stanford core nlp example run model find relation even use sentence directly training set used verrrry small training set example make sure could get model train even though training set ridiculously small would still expect model work poorly model able find relation also wanted name relation affordance try dataset get nullpointerexception try train change name relation dataset kill suddenly work assuming since kill one example relation stanford give added file doe anyone know could rename relation thank much example training set
NLP parsing multiple questions contained in one single query,"<p>If a single query from the user contains multiple questions belonging to different categories, how can they be identified, split and parsed?</p>

<p>Eg -   </p>

<pre><code>User - what is the weather now and tell me my next meeting  
Parser - {:weather =&gt; ""what is the weather"", :schedule =&gt; ""tell me my next meeting""}  
</code></pre>

<p><em>Parser identifies the parts of sentences where the question belongs to two different categories</em></p>

<pre><code>User - show me hotels in san francisco for tomorrow that are less than $300 but not less than $200 are pet friendly have a gym and a pool with 3 or 4 stars staying for 2 nights and dont include anything that doesnt have wifi  
Parser - {:hotels =&gt; [""show me hotels in san francisco"",  
          ""for tomorrow"", ""less than $300 but not less than $200"",
          ""pet friendly have a gym and a pool"",
          ""with 3 or 4 stars"", ""staying for 2 nights"", ""with wifi""]}
</code></pre>

<p><em>Parser identifies the question belonging to only one category but has additional steps for fine tuning the answer and created an array ordered according to the steps to take</em></p>

<p>From what I can understand this requires a <strong>sentence segmenter</strong>, <strong>multi-label classifier</strong> and <strong>co-reference resolution</strong></p>

<p>But the <em>sentence segementer</em> I have come across depend heavily on grammar, punctuations.   </p>

<p><em>Multi-label classifiers</em>, like a good trained naive bayes classifier works in most cases but since they are multi-label, most times output multiple categories for sentences which clearly belong to one class. Depending solely on the array outputs to check the labels present would fail.  </p>

<p>If used a multi-class classifier, that is also good to check the array output of probable categories but obviously they dont tell the different parts of the sentence much accurately, much less in what fashion to proceed with the next step.</p>

<p><strong>As a first step, how can I tune sentence segmenter to correctly split the sentence without any strict grammar rules.</strong> Good accuracy of this would help a lot in classification.</p>
",Coreference & Relation Extraction,nlp parsing multiple question contained one single query single query user contains multiple question belonging different category identified split parsed eg parser identifies part sentence question belongs two different category parser identifies question belonging one category ha additional step fine tuning answer created array ordered according step take understand requires sentence segmenter multi label classifier co reference resolution sentence segementer come across depend heavily grammar punctuation multi label classifier like good trained naive bayes classifier work case since multi label time output multiple category sentence clearly belong one class depending solely array output check label present would fail used multi class classifier also good check array output probable category obviously dont tell different part sentence much accurately much le fashion proceed next step first step tune sentence segmenter correctly split sentence without strict grammar rule good accuracy would help lot classification
Where do the negative examples come from in relation extraction via distant supervision?,"<p>In the <a href=""http://www.youtube.com/watch?v=wA-Wvclz8LQ#t=363"" rel=""nofollow noreferrer"">coursera nlp</a> videos Dan Jurafsky shows how you can start with seeds, search for them in a corpus, and extract the features for instances of those seeds (although he does not say how nlp practitioners extract the features: <a href=""https://stackoverflow.com/questions/23401313/in-semi-supervised-relation-extraction-how-do-you-create-features-from-seed-exa"">https://stackoverflow.com/questions/23401313/in-semi-supervised-relation-extraction-how-do-you-create-features-from-seed-exa</a>)</p>

<p>He then says that the instances of the seeds in the corpus count as positive examples for a supervised classifier. But a supervised classifier will need positive and negative examples. When NLP practitioners do this sort of relation extraction via distant supervision, where do the negative examples come from?</p>
",Coreference & Relation Extraction,negative example come relation extraction via distant supervision coursera nlp video dan jurafsky show start seed search corpus extract feature instance seed although doe say nlp practitioner extract feature href p say instance seed corpus count positive example supervised classifier supervised classifier need positive negative example nlp practitioner sort relation extraction via distant supervision negative example come
Stanford Core NLP - understanding coreference resolution,"<p>I'm having some trouble understanding the changes made to the coref resolver in the last version of the Stanford NLP tools.
As an example, below is a sentence and the corresponding CorefChainAnnotation:</p>

<pre><code>The atom is a basic unit of matter, it consists of a dense central nucleus surrounded by a cloud of negatively charged electrons.

{1=[1 1, 1 2], 5=[1 3], 7=[1 4], 9=[1 5]}
</code></pre>

<p>I am not sure I understand the meaning of these numbers. Looking at the source doesn't really help either.</p>

<p>Thank you</p>
",Coreference & Relation Extraction,stanford core nlp understanding coreference resolution trouble understanding change made coref resolver last version stanford nlp tool example sentence corresponding corefchainannotation sure understand meaning number looking source really help either thank
CoreNLP in Python for coreference resolution,"<p>Is there a way to use CoreNLP in Python for coreference resolution? I have tried using the Stanford server and <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow noreferrer"">this link</a>. But they don't work. </p>
",Coreference & Relation Extraction,corenlp python coreference resolution way use corenlp python coreference resolution tried using stanford server link work
How to combine the output of Stanford English Parser with that of Graphene,"<p>Graphene is an information extraction pipeline which extracts Knowledge Graphs from texts (n-ary relations and rhetorical structures extracted from complex factoid discourse). Given a sentence or a text, Graphene outputs a semantic representation of the text which is a labeled directed graph (a knowledge graph). This knowledge graph can be later used for addressing different AI tasks, such as building Question Answering systems, extracting structured data from text, supporting semantic inference, among other tasks. Differently from existing open relation extraction tools, which focus on the main relation expressed in a sentence, Graphene aims at maximizing the extraction of contextual relations. </p>

<p>Link for Graphene (<a href=""http://lambda3.org/Graphene/"" rel=""nofollow noreferrer"">http://lambda3.org/Graphene/</a> ).
Can the parser performance be improved by combining the outputs of Stanford English Parser with that of Graphene?If yes then how?</p>
",Coreference & Relation Extraction,combine output stanford english parser graphene graphene information extraction pipeline extract knowledge graph text n ary relation rhetorical structure extracted complex factoid discourse given sentence text graphene output semantic representation text labeled directed graph knowledge graph knowledge graph later used addressing different ai task building question answering system extracting structured data text supporting semantic inference among task differently existing open relation extraction tool focus main relation expressed sentence graphene aim maximizing extraction contextual relation link graphene parser performance improved combining output stanford english parser graphene yes
How to convert text file to conll 2012 format for co-reference resolution system,"<p>I am working on training co-reference resolution system(statistical coreference model) for a new domain . I need to convert my data to conll 2012 data format. Please help</p>

<p>I am using  following text format. I would like to know whether Stanford provides any functionalities to convert below data to conll 2012 format.<br>
To comprehend Bunuel 's works it is essential to understand that he was first a Spaniard and secondly a product of Spanish bourgeoisie .
Virginia Higgenbotham points out that <code>blasphemy is only a form of thinking for any intelligent Spaniard , '' and also that the</code> art of his country is rich in eroticism and profoundly preoccupied with death '' -LRB- 18 , 20 -RRB- .
Bunuel says : My infancy slipped by in an almost medieval atmosphere -LRB- like that of nearly all the Spanish provinces -RRB- between my native town and Zaragova .   </p>
",Coreference & Relation Extraction,convert text file conll format co reference resolution system working training co reference resolution system statistical coreference model new domain need convert data conll data format please help using following text format would like know whether stanford provides functionality convert data conll format comprehend bunuel work essential understand wa first spaniard secondly product spanish bourgeoisie virginia higgenbotham point art country rich eroticism profoundly preoccupied death lrb rrb bunuel say infancy slipped almost medieval atmosphere lrb like nearly spanish province rrb native town zaragova
Which database can be used to store processed data from NLP engine,"<p>I am looking at taking unstructured data in the form of files, processing it and storing it in a <code>database</code> for retrieval.
The data will be in natural language and the queries to get information will also be in natural language.
Ex: the data could be <strong>""Roses are red""</strong> and the query could be <strong>""What is the color of a rose?""</strong></p>

<p>I have looked at several <code>nlp</code> systems, focusing more on <code>open-source</code> information extraction and relation extraction system and the following seems apt and easy for quick start:
<a href=""https://www.npmjs.com/package/mitie"" rel=""noreferrer""><a href=""https://www.npmjs.com/package/mitie"" rel=""noreferrer"">https://www.npmjs.com/package/mitie</a></a></p>

<p>This can give data in the form of (word,type) pairs. It also gives a relation as result of running the the processing (check the site example).</p>

<p>I want to know if <code>sql</code> is good <code>database</code> to save this information. For retrieving the information, I will need to convert the natural language query also to some kind of (word, meaning) pairs
and for using <code>sql</code> I will have to write a layer that converts natural language to <code>sql</code> queries.  </p>

<p>Please suggest if there are any open source <code>database</code> that work well in this situation. I'm open to suggestions for databases that work with other <code>open-source</code> information extraction and relation extraction systems if not MITIE.</p>
",Coreference & Relation Extraction,database used store processed data nlp engine looking taking unstructured data form file processing storing retrieval data natural language query get information also natural language ex data could rose red query could color rose looked several system focusing information extraction relation extraction system following seems apt easy quick start give data form word type pair also give relation result running processing check site example want know good save information retrieving information need convert natural language query also kind word meaning pair using write layer convert natural language query please suggest open source work well situation open suggestion database work information extraction relation extraction system mitie
What tools can perform automatic relation extraction without any configuration or coding?,"<p>I have designed a system to do automatic relation extraction on a specific corpus, where the relations are not known in advance. I want to compare my system to another system that does <a href=""http://reverb.cs.washington.edu/"" rel=""nofollow"">automatic relation extraction</a> to see if my system performs any better (for an academic thesis). The only such automatic extraction system that I know about is reverb. </p>

<p>In the readme for the <a href=""http://reverb.cs.washington.edu/"" rel=""nofollow"">reverb</a> system they say that reverb is ""is designed for Web-scale information extraction, where the target relations cannot be specified in advance and speed is important."" My system is much slower than reverb (I think because it parses sentences and performs deeper analysis) so I am not sure if the comparison is meaningful. I extract more relations but the comparison is a little unfair (because my system is so much more computationally intensive). Are there other systems like reverb that can do automatic relation extraction? Maybe systems that are not ""designed for web-scale information extraction"" that are more fair competitors?</p>

<p>I know NLTK does relation extraction but you have to supply a specific regex glue to bind entities in a relation. </p>

<p>Are there other tools out there that NLP practitioners use to do automatic relation extraction?</p>
",Coreference & Relation Extraction,tool perform automatic relation extraction without configuration coding designed system automatic relation extraction specific corpus relation known advance want compare system another system doe automatic relation extraction see system performs better academic thesis automatic extraction system know reverb readme reverb system say reverb designed web scale information extraction target relation specified advance speed important system much slower reverb think par sentence performs deeper analysis sure comparison meaningful extract relation comparison little unfair system much computationally intensive system like reverb automatic relation extraction maybe system designed web scale information extraction fair know nltk doe relation extraction supply specific regex glue bind entity relation tool nlp practitioner use automatic relation extraction
Event extraction vs N-ary relation extraction,"<p>What is the difference between an event and an n-ary relation?</p>

<p>I am trying to differentiate between the two and eventually focus on the task of extraction. For example, from the given sentence:</p>

<pre><code>Peter completed B.Sc. in physics from Boston University.
</code></pre>

<p>Extracted n-ary relation:</p>

<pre><code>r(Peter, B.Sc., physics, Boston University)
</code></pre>

<p><em>(Assuming that the entities are already labeled)</em></p>

<p>For the problem of event extraction, we have datasets like <code>ACE 2005</code> event extraction corpus. However, I haven't come across any corpus for n-ary relation extraction. Is anyone aware of any such corpus which might facilitate n-ary relation extraction?</p>
",Coreference & Relation Extraction,event extraction v n ary relation extraction difference event n ary relation trying differentiate two eventually focus task extraction example given sentence extracted n ary relation assuming entity already labeled problem event extraction datasets like event extraction corpus however come across corpus n ary relation extraction anyone aware corpus might facilitate n ary relation extraction
Relationship Extraction using Stanford CoreNLP,"<p>I'm trying to extract information from natural language content using the Stanford CoreNLP library.</p>

<p>My goal is to extract ""subject-action-object"" pairs (simplified) from sentences.</p>

<p>As an example consider the following sentence:</p>

<blockquote>
  <p>John Smith only eats an apple and a banana for lunch. He's on a diet and his mother told him that it would be very healthy to eat less for lunch. John doesn't like it at all but since he's very serious with his diet, he doesn't want to stop.</p>
</blockquote>

<p>From this sentence I would like to get results as followed:</p>

<ul>
<li>John Smith - eats - only an apple and a banana for lunch</li>
<li>He - is - on a diet</li>
<li>His mother - told - him - that it would be very healthy to eat less for lunch</li>
<li>John - doesn't like - it (at all)</li>
<li>He - is - very serious with his diet</li>
</ul>

<p>How would one do this?</p>

<p>Or to be more specific:
How can I parse a dependency tree (or a better-suited tree?) to obtain results as specified above?</p>

<p>Any hint, resource or code snippet given this task would be highly appreciated.</p>

<p>Side note:
I managed to replace coreferences with their representative mention which would then change the <code>he</code> and <code>his</code> to the corresponding entity (John Smith in that case).</p>
",Coreference & Relation Extraction,relationship extraction using stanford corenlp trying extract information natural language content using stanford corenlp library goal extract subject action object pair simplified sentence example consider following sentence john smith eats apple banana lunch diet mother told would healthy eat le lunch john like since serious diet want stop sentence would like get result followed john smith eats apple banana lunch diet mother told would healthy eat le lunch john like serious diet would one specific parse dependency tree better suited tree obtain result specified hint resource code snippet given task would highly appreciated side note managed replace coreference representative mention would change corresponding entity john smith case
ValueError: invalid literal for int() with base 10: &#39;_&#39;,"<p>I am using the cort, a coreference resolution toolkit. I have installed cort using :</p>

<pre><code>pip install cort
</code></pre>

<p>with python version 2.7</p>

<p>However, while running the command as mentioned in the documentation:</p>

<pre><code>kenden@kenden-Lenovo-G500:~/deeshacodes$ cort-predict-conll -in test.conll \
           -model model.obj \
           -out output.conll \
           -extractor cort.coreference.approaches.mention_ranking.extract_substructures \
           -perceptron cort.coreference.approaches.mention_ranking.RankingPerceptron \
           -clusterer cort.coreference.clusterer.all_ante \
</code></pre>

<p>I get the folowing error:</p>

<pre><code> File ""/home/kenden/deeshacodes/deeshaenv/bin/cort-predict-conll"", line 185, in &lt;module&gt;
    codecs.open(args.input_filename, ""r"", ""utf-8""))
  File ""/home/kenden/deeshacodes/deeshaenv/local/lib/python2.7/site-packages/cort/core/corpora.py"", line 79, in from_file
    document_as_strings]))
  File ""/home/kenden/deeshacodes/deeshaenv/local/lib/python2.7/site-packages/cort/core/corpora.py"", line 14, in from_string
    return documents.CoNLLDocument(string)
  File ""/home/kenden/deeshacodes/deeshaenv/local/lib/python2.7/site-packages/cort/core/documents.py"", line 380, in __init__
    in_sentence_ids = [int(i) for i in self.__extract_from_column(2)]
ValueError: invalid literal for int() with base 10: '_'
</code></pre>

<p>How can I solve this error in the packages?</p>

<p>Here is a link to github repo <a href=""https://github.com/smartschat/cort"" rel=""nofollow noreferrer"">cort</a></p>

<p>This my conll file format:
test.conll</p>

<pre><code>1       Scores        _       NNS     NNS     _       4       nsubj        _       _
2       of            _       IN      IN      _       0       erased       _       _
3       properties    _       NNS     NNS     _       1       prep_of      _       _
4       are           _       VBP     VBP     _       0       root         _       _
5       under         _       IN      IN      _       0       erased       _       _
6       extreme       _       JJ      JJ      _       8       amod         _       _
7       fire          _       NN      NN      _       8       nn           _       _
8       threat        _       NN      NN      _       4       prep_under   _       _
9       as            _       IN      IN      _      13       mark         _       _
10      a             _       DT      DT      _      12       det          _       _
11      huge          _       JJ      JJ      _      12       amod         _       _
12      blaze         _       NN      NN      _      15       xsubj        _       _
13      continues     _       VBZ     VBZ     _       4       advcl        _       _
14      to            _       TO      TO      _      15       aux          _       _
15      advance       _       VB      VB      _      13       xcomp        _       _
16      through       _       IN      IN      _       0       erased       _       _
17      Sydney        _       NNP     NNP     _      20       poss         _       _
18      's            _       POS     POS     _       0       erased       _       _
19      north-western _       JJ      JJ      _      20       amod         _       _
20      suburbs       _       NNS     NNS     _      15       prep_through _       _
21      .             _       .       .       _       4       punct        _       _
</code></pre>
",Coreference & Relation Extraction,valueerror invalid literal int base using cort coreference resolution toolkit installed cort using python version however running command mentioned documentation get folowing error solve error package link github repo cort conll file format test conll
ValueError: invalid literal for int() with base 10: &#39;_&#39;,"<p>I am using the cort, a coreference resolution toolkit. I have installed cort using :</p>

<pre><code>pip install cort
</code></pre>

<p>with python version 2.7</p>

<p>However, while running the command as mentioned in the documentation:</p>

<pre><code>kenden@kenden-Lenovo-G500:~/deeshacodes$ cort-predict-conll -in test.conll \
           -model model.obj \
           -out output.conll \
           -extractor cort.coreference.approaches.mention_ranking.extract_substructures \
           -perceptron cort.coreference.approaches.mention_ranking.RankingPerceptron \
           -clusterer cort.coreference.clusterer.all_ante \
</code></pre>

<p>I get the folowing error:</p>

<pre><code> File ""/home/kenden/deeshacodes/deeshaenv/bin/cort-predict-conll"", line 185, in &lt;module&gt;
    codecs.open(args.input_filename, ""r"", ""utf-8""))
  File ""/home/kenden/deeshacodes/deeshaenv/local/lib/python2.7/site-packages/cort/core/corpora.py"", line 79, in from_file
    document_as_strings]))
  File ""/home/kenden/deeshacodes/deeshaenv/local/lib/python2.7/site-packages/cort/core/corpora.py"", line 14, in from_string
    return documents.CoNLLDocument(string)
  File ""/home/kenden/deeshacodes/deeshaenv/local/lib/python2.7/site-packages/cort/core/documents.py"", line 380, in __init__
    in_sentence_ids = [int(i) for i in self.__extract_from_column(2)]
ValueError: invalid literal for int() with base 10: '_'
</code></pre>

<p>How can I solve this error in the packages?</p>

<p>Here is a link to github repo <a href=""https://github.com/smartschat/cort"" rel=""nofollow noreferrer"">cort</a></p>

<p>This my conll file format:
test.conll</p>

<pre><code>1       Scores        _       NNS     NNS     _       4       nsubj        _       _
2       of            _       IN      IN      _       0       erased       _       _
3       properties    _       NNS     NNS     _       1       prep_of      _       _
4       are           _       VBP     VBP     _       0       root         _       _
5       under         _       IN      IN      _       0       erased       _       _
6       extreme       _       JJ      JJ      _       8       amod         _       _
7       fire          _       NN      NN      _       8       nn           _       _
8       threat        _       NN      NN      _       4       prep_under   _       _
9       as            _       IN      IN      _      13       mark         _       _
10      a             _       DT      DT      _      12       det          _       _
11      huge          _       JJ      JJ      _      12       amod         _       _
12      blaze         _       NN      NN      _      15       xsubj        _       _
13      continues     _       VBZ     VBZ     _       4       advcl        _       _
14      to            _       TO      TO      _      15       aux          _       _
15      advance       _       VB      VB      _      13       xcomp        _       _
16      through       _       IN      IN      _       0       erased       _       _
17      Sydney        _       NNP     NNP     _      20       poss         _       _
18      's            _       POS     POS     _       0       erased       _       _
19      north-western _       JJ      JJ      _      20       amod         _       _
20      suburbs       _       NNS     NNS     _      15       prep_through _       _
21      .             _       .       .       _       4       punct        _       _
</code></pre>
",Coreference & Relation Extraction,valueerror invalid literal int base using cort coreference resolution toolkit installed cort using python version however running command mentioned documentation get folowing error solve error package link github repo cort conll file format test conll
Stanford nlp-Coreference resolution - &quot;java.lang.OutOfMemoryError: Java heap space&quot;,"<p>I tried to train the statistical coreference resolution system with conll 2012 trial data(<a href=""http://conll.cemantix.org/2012/data.html"" rel=""nofollow noreferrer"">http://conll.cemantix.org/2012/data.html</a>).
I wanted to train it for medical data. But I started with conll 2012 trial data inorder to understand the statistical coreference pipeline. 
I took only two "".conll"" files of size less than 2MB(eng_0012.conll,eng_0014.conll). These two files contains total of 8 training docs. </p>

<p>I followed below link to build the model.<br>
<a href=""http://stanfordnlp.github.io/CoreNLP/coref.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/coref.html</a><br>
(java -Xmx60g -cp stanford-corenlp-3.7.0.jar:stanford-english-corenlp-models-3.7.0.jar:* edu.stanford.nlp.coref.statistical.StatisticalCorefTrainer -props )</p>

<p>Here the heap size is mentioned as 60g. I used 60g heap size and 15g swap memory and 16 core processor.</p>

<p>But I got ""java.lang.OutOfMemoryError: Java heap space"" exception while building the model.</p>

<pre><code>Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3181)
    at java.util.ArrayList.grow(ArrayList.java:261)
    at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235)
    at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227)
    at java.util.ArrayList.add(ArrayList.java:458)
    at      edu.stanford.nlp.coref.statistical.Clusterer.getFeatures(Clusterer.java:661)
    at edu.stanford.nlp.coref.statistical.Clusterer.access$700(Clusterer.java:27)
    at edu.stanford.nlp.coref.statistical.Clusterer$State.getActions(Clusterer.java:460)
    at edu.stanford.nlp.coref.statistical.Clusterer.runPolicy(Clusterer.java:225)
    at edu.stanford.nlp.coref.statistical.Clusterer.doTraining(Clusterer.java:167)
    at edu.stanford.nlp.coref.statistical.StatisticalCorefTrainer.doTraining(StatisticalCorefTrainer.java:127)
    at edu.stanford.nlp.coref.statistical.StatisticalCorefTrainer.main(StatisticalCorefTrainer.java:146)
</code></pre>

<p>When i reduced the training doc for  from 8 to 4 in ""doTraining"" method of ""edu/stanford/nlp/coref/statistical/Clusterer.java"" class, it ran successfully. </p>

<pre><code>int count = 1;
for (ClustererDoc trainDoc : trainDocs) {
    if (count == 5) {
        break;
    }
    count++;
    examples.add(runPolicy(trainDoc, Math.pow(EXPERT_DECAY(iteration +1))));
}
</code></pre>

<p>I don't understand why I am  getting this out of memory exception even after giving required configuration for a very small amount of data(less than 2 MB)</p>

<p>Is there any way to optimize the memory usage?</p>

<p>When I went through the source code I found some files like demonyms.txt, gender.data.gz, inanimate.unigrams.txt, state-abbrevations.txt etc. 
Do I need to create any files specifying medical entities for training the medical domain to get better accuracy ?</p>
",Coreference & Relation Extraction,stanford nlp coreference resolution java lang outofmemoryerror java heap space tried train statistical coreference resolution system conll trial data wanted train medical data started conll trial data inorder understand statistical coreference pipeline took two conll file size le mb eng conll eng conll two file contains total training doc followed link build model java xmx g cp stanford corenlp jar stanford english corenlp model jar edu stanford nlp coref statistical statisticalcoreftrainer prop heap size mentioned g used g heap size g swap memory core processor got java lang outofmemoryerror java heap space exception building model reduced training doc dotraining method edu stanford nlp coref statistical clusterer java class ran successfully understand getting memory exception even giving required configuration small amount data le mb way optimize memory usage went source code found file like demonyms txt gender data gz inanimate unigrams txt state abbrevations txt etc need create file specifying medical entity training medical domain get better accuracy
FileNotFoundException on tmp/roth_sentences.ser when training Stanford Relation Extractor model,"<p>I'm trying to train my own relation extraction model as described <a href=""http://nlp.stanford.edu/software/relationExtractor.html#training"" rel=""nofollow noreferrer"">here</a> but keep getting a strange error.</p>

<p>My properties file:</p>

<pre><code>#Below are some basic options. See edu.stanford.nlp.ie.machinereading.MachineReadingProperties class for more options.

# Pipeline options
annotators = pos, lemma, parse
parse.maxlen = 100

# MachineReading properties. You need one class to read the dataset into correct format. See edu.stanford.nlp.ie.machinereading.domains.ace.AceReader for another example.
datasetReaderClass = edu.stanford.nlp.ie.machinereading.domains.roth.RothCONLL04Reader

readerLogLevel = INFO
#Data directory for training. The datasetReaderClass reads data from this path and makes corresponding sentences and annotations.
trainPath = ../re-training-data.corp

#Whether to crossValidate, that is evaluate, or just train.
crossValidate = false
kfold = 10

#Change this to true if you want to use CoreNLP pipeline generated NER tags. The default model generated with the relation extractor release uses the CoreNLP pipeline provided tags (option set to true$
trainUsePipelineNER=true

# where to save training sentences. uses the file if it exists, otherwise creates it.
serializedTrainingSentencesPath = tmp/roth_sentences.ser

serializedEntityExtractorPath = tmp/roth_entity_model.ser

# where to store the output of the extractor (sentence objects with relations generated by the model). This is what you will use as the model when using 'relation' annotator in the CoreNLP pipeline.
serializedRelationExtractorPath = tmp/kpl-relation-model-pipeline.ser

# uncomment to load a serialized model instead of retraining
# loadModel = true

#relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter,edu.stanford.nlp.ie.machinereading.domains.roth.RothResultsByRelation. For printing output of the model.
relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter

#In this domain, this is trivial since all the entities are given (or set using CoreNLP NER tagger).
entityClassifier = edu.stanford.nlp.ie.machinereading.domains.roth.RothEntityExtractor

extractRelations = true
extractEvents = false

#We are setting the entities beforehand so the model does not learn how to extract entities etc.
extractEntities = false

#Opposite of crossValidate.
trainOnly=true

# The set chosen by feature selection using RothCONLL04:
relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path
</code></pre>

<p>Here's what I run in the terminal:</p>

<pre><code>sudo java -cp stanford-corenlp-3.7.0.jar:stanford-corenlp-3.7.0-models.jar edu.stanford.nlp.ie.machinereading.MachineReading --arguments kpl-re-model.properties
</code></pre>

<p>And the the result:</p>

<pre><code>PERCENTAGE OF TRAIN: 1.0
The reader log level is set to INFO
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec].
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from 
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading makeResultsPrinters
INFO: Making result printers from 
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: Parsing corpus sentences...
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: These sentences will be serialized to /home/ubuntu/stanford-corenlp-full-2016-10-31/tmp/roth_sentences.ser
Jan 17, 2017 4:55:06 PM edu.stanford.nlp.ie.machinereading.domains.roth.RothCONLL04Reader read
INFO: Reading file: ../re-training-data.corp
Jan 17, 2017 4:55:07 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader preProcessSentences
SEVERE: GenericDataSetReader: Started pre-processing the corpus...
Jan 17, 2017 4:55:07 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader preProcessSentences
INFO: Annotating dataset with edu.stanford.nlp.pipeline.StanfordCoreNLP@5f9d02cb
Jan 17, 2017 4:58:32 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader preProcessSentences
SEVERE: GenericDataSetReader: Pre-processing complete.
Jan 17, 2017 4:58:32 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader parse
SEVERE: Changing NER tags using the CoreNLP pipeline.
Replacing old annotator ""parse"" with signature [edu.stanford.nlp.pipeline.ParserAnnotator#parse.maxlen:100;#] with new annotator with signature [edu.stanford.nlp.pipeline.ParserAnnotator##]
Adding annotator pos
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Jan 17, 2017 4:58:45 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: Done. Parsed 1183 sentences.
Jan 17, 2017 4:58:45 PM edu.stanford.nlp.ie.machinereading.MachineReading loadOrMakeSerializedSentences
INFO: Serializing parsed sentences to /home/ubuntu/stanford-corenlp-full-2016-10-31/tmp/roth_sentences.ser...
Exception in thread ""main"" java.io.FileNotFoundException: tmp/roth_sentences.ser (No such file or directory)
    at java.io.FileOutputStream.open0(Native Method)
    at java.io.FileOutputStream.open(FileOutputStream.java:270)
    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)
    at edu.stanford.nlp.io.IOUtils.writeObjectToFile(IOUtils.java:77)
    at edu.stanford.nlp.io.IOUtils.writeObjectToFile(IOUtils.java:63)
    at edu.stanford.nlp.ie.machinereading.MachineReading.loadOrMakeSerializedSentences(MachineReading.java:914)
    at edu.stanford.nlp.ie.machinereading.MachineReading.run(MachineReading.java:270)
    at edu.stanford.nlp.ie.machinereading.MachineReading.main(MachineReading.java:111
</code></pre>

<p>The error states that it can't find 'tmp/roth_sentences.ser' but it doesn't make sense because it's supposed to <em>create</em> that file.</p>

<p>Any ideas?</p>

<p>Thanks!
Simon.</p>
",Coreference & Relation Extraction,filenotfoundexception tmp roth sentence ser training stanford relation extractor model trying train relation extraction model described keep getting strange error property file run terminal result error state find tmp roth sentence ser make sense supposed create file idea thanks simon
Stanford CoreNLP server&#39;s JSON response missing RelationExtractor annotations,"<p>I'm processing a simple sentence to test Stanford's <strong>RelationExtractor</strong>:</p>

<blockquote>
  <p>Microsoft is based in New York.</p>
</blockquote>

<p>(it's not)</p>

<p>When I'm annotating the sentence in Java, by directly using the CoreNLP jar files I get the wanted result - CoreNLP finds a <strong>OrgBased_In</strong> relation between <em>Microsoft</em> and <em>New York</em>.</p>

<pre><code>for (CoreMap sentence : sentences) {
    relationType = sentence.get(MachineReadingAnnotations.RelationMentionsAnnotation.class).get(0).type // =&gt; OrgBased_In
}
</code></pre>

<p>However, sending the same sentence into the <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">CoreNLP Server</a> like so:</p>

<pre><code>curl --data 'Microsoft is based in New York.' 'http://localhost:9000/?properties={%22annotators%22%3A%22tokenize%2Cssplit%2Cpos%2Clemma%2Cner%2Cparse%2Cdepparse%2Crelation%22%2C%22outputFormat%22%3A%22json%22}' -o -
</code></pre>

<p>Results in a json response that contains no data on relations whatsoever:</p>

<pre><code>{'sentences': [{'basicDependencies': [{'dep': 'ROOT',
                                   'dependent': 3,
                                   'dependentGloss': 'based',
                                   'governor': 0,
                                   'governorGloss': 'ROOT'},
                                  {'dep': 'nsubjpass',
                                   'dependent': 1,
                                   'dependentGloss': 'Microsoft',
                                   'governor': 3,
                                   'governorGloss': 'based'},
                                  {'dep': 'auxpass',
                                   'dependent': 2,
                                   'dependentGloss': 'is',
                                   'governor': 3,
                                   'governorGloss': 'based'},
                                  {'dep': 'case',
                                   'dependent': 4,
                                   'dependentGloss': 'in',
                                   'governor': 6,
                                   'governorGloss': 'York'},
                                  {'dep': 'compound',
                                   'dependent': 5,
                                   'dependentGloss': 'New',
                                   'governor': 6,
                                   'governorGloss': 'York'},
                                  {'dep': 'nmod',
                                   'dependent': 6,
                                   'dependentGloss': 'York',
                                   'governor': 3,
                                   'governorGloss': 'based'},
                                  {'dep': 'punct',
                                   'dependent': 7,
                                   'dependentGloss': '.',
                                   'governor': 3,
                                   'governorGloss': 'based'}],
            'enhancedDependencies': [{'dep': 'ROOT',
                                      'dependent': 3,
                                      'dependentGloss': 'based',
                                      'governor': 0,
                                      'governorGloss': 'ROOT'},
                                     {'dep': 'nsubjpass',
                                      'dependent': 1,
                                      'dependentGloss': 'Microsoft',
                                      'governor': 3,
                                      'governorGloss': 'based'},
                                     {'dep': 'auxpass',
                                      'dependent': 2,
                                      'dependentGloss': 'is',
                                      'governor': 3,
                                      'governorGloss': 'based'},
                                     {'dep': 'case',
                                      'dependent': 4,
                                      'dependentGloss': 'in',
                                      'governor': 6,
                                      'governorGloss': 'York'},
                                     {'dep': 'compound',
                                      'dependent': 5,
                                      'dependentGloss': 'New',
                                      'governor': 6,
                                      'governorGloss': 'York'},
                                     {'dep': 'nmod:in',
                                      'dependent': 6,
                                      'dependentGloss': 'York',
                                      'governor': 3,
                                      'governorGloss': 'based'},
                                     {'dep': 'punct',
                                      'dependent': 7,
                                      'dependentGloss': '.',
                                      'governor': 3,
                                      'governorGloss': 'based'}],
            'enhancedPlusPlusDependencies': [{'dep': 'ROOT',
                                              'dependent': 3,
                                              'dependentGloss': 'based',
                                              'governor': 0,
                                              'governorGloss': 'ROOT'},
                                             {'dep': 'nsubjpass',
                                              'dependent': 1,
                                              'dependentGloss': 'Microsoft',
                                              'governor': 3,
                                              'governorGloss': 'based'},
                                             {'dep': 'auxpass',
                                              'dependent': 2,
                                              'dependentGloss': 'is',
                                              'governor': 3,
                                              'governorGloss': 'based'},
                                             {'dep': 'case',
                                              'dependent': 4,
                                              'dependentGloss': 'in',
                                              'governor': 6,
                                              'governorGloss': 'York'},
                                             {'dep': 'compound',
                                              'dependent': 5,
                                              'dependentGloss': 'New',
                                              'governor': 6,
                                              'governorGloss': 'York'},
                                             {'dep': 'nmod:in',
                                              'dependent': 6,
                                              'dependentGloss': 'York',
                                              'governor': 3,
                                              'governorGloss': 'based'},
                                             {'dep': 'punct',
                                              'dependent': 7,
                                              'dependentGloss': '.',
                                              'governor': 3,
                                              'governorGloss': 'based'}],
            'index': 0,
            'parse': '(ROOT\n'
                     '  (S\n'
                     '    (NP (NNP Microsoft))\n'
                     '    (VP (VBZ is)\n'
                     '      (VP (VBN based)\n'
                     '        (PP (IN in)\n'
                     '          (NP (NNP New) (NNP York)))))\n'
                     '    (. .)))',
            'tokens': [{'after': ' ',
                        'before': '',
                        'characterOffsetBegin': 0,
                        'characterOffsetEnd': 9,
                        'index': 1,
                        'lemma': 'Microsoft',
                        'ner': 'ORGANIZATION',
                        'originalText': 'Microsoft',
                        'pos': 'NNP',
                        'word': 'Microsoft'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 10,
                        'characterOffsetEnd': 12,
                        'index': 2,
                        'lemma': 'be',
                        'ner': 'O',
                        'originalText': 'is',
                        'pos': 'VBZ',
                        'word': 'is'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 13,
                        'characterOffsetEnd': 18,
                        'index': 3,
                        'lemma': 'base',
                        'ner': 'O',
                        'originalText': 'based',
                        'pos': 'VBN',
                        'word': 'based'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 19,
                        'characterOffsetEnd': 21,
                        'index': 4,
                        'lemma': 'in',
                        'ner': 'O',
                        'originalText': 'in',
                        'pos': 'IN',
                        'word': 'in'},
                       {'after': ' ',
                        'before': ' ',
                        'characterOffsetBegin': 22,
                        'characterOffsetEnd': 25,
                        'index': 5,
                        'lemma': 'New',
                        'ner': 'LOCATION',
                        'originalText': 'New',
                        'pos': 'NNP',
                        'word': 'New'},
                       {'after': '',
                        'before': ' ',
                        'characterOffsetBegin': 26,
                        'characterOffsetEnd': 30,
                        'index': 6,
                        'lemma': 'York',
                        'ner': 'LOCATION',
                        'originalText': 'York',
                        'pos': 'NNP',
                        'word': 'York'},
                       {'after': '',
                        'before': '',
                        'characterOffsetBegin': 30,
                        'characterOffsetEnd': 31,
                        'index': 7,
                        'lemma': '.',
                        'ner': 'O',
                        'originalText': '.',
                        'pos': '.',
                        'word': '.'}]}]}
</code></pre>

<p>I can see on the CoreNLP server terminal that the relation extraction model <em>is</em> loaded.</p>

<pre><code>[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.RelationExtractorAnnotator - Loading relation model from edu/stanford/nlp/models/supervised_relation_extractor/roth_relation_model_pipelineNER.ser
</code></pre>

<p>What am I missing here?</p>

<p>Thanks!</p>
",Coreference & Relation Extraction,stanford corenlp server json response missing relationextractor annotation processing simple sentence test stanford relationextractor microsoft based new york annotating sentence java directly using corenlp jar file get wanted result corenlp find orgbased relation microsoft new york however sending sentence corenlp server like result json response contains data relation whatsoever see corenlp server terminal relation extraction model loaded missing thanks
Where to find a state of art relation extraction dataset,"<p>I am looking for a dataset which contains large quantities of relation tuples. For example, the search of ""people"" and ""location"" yields ""lives in"", ""worked in"", etc. University of Washington's OpenIE <a href=""http://OpenIE.cs.washington.edu"" rel=""nofollow"">http://OpenIE.cs.washington.edu</a> is a good tool but their dataset is only accessible through web. Where can I download a database or library like this?</p>
",Coreference & Relation Extraction,find state art relation extraction dataset looking dataset contains large quantity relation tuples example search people location yield life worked etc university washington openie good tool dataset accessible web download database library like
Extract recommendations/suggestions from text,"<p>My documents often include sentences like:</p>

<blockquote>
  <p>Had I known about this, I would have prevented this problem</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>If John was informed, this wouldn't happen</p>
</blockquote>

<p>or </p>

<blockquote>
  <p>this wouldn't be a problem if Jason was smart </p>
</blockquote>

<p>I'm interested in extracting these sort of information (not sure what they are called, linguistically). So I would like to extract either the whole sentence, or ideally, a summary like:</p>

<blockquote>
  <p>(inform John) (prevent)</p>
</blockquote>

<p>Most, if not all, the examples of relation extraction, and information extraction that I've come across, follow fairly standard flow:
do NER, then relation extraction looks for relations like ""in"" or ""at"", etc (ch7 of nltk book for example). </p>

<p>Do these type of sentences fall under a certain category in NLP? Are there any papers/tutorials on something like this?</p>
",Coreference & Relation Extraction,extract recommendation suggestion text document often include sentence like known would prevented problem john wa informed happen problem jason wa smart interested extracting sort information sure called linguistically would like extract either whole sentence ideally summary like inform john prevent example relation extraction information extraction come across follow fairly standard flow ner relation extraction look relation like etc ch nltk book example type sentence fall certain category nlp paper tutorial something like
Does an algorithm exist to identify different queries/questions in sentence?,"<p>I want to identifies different queries in sentences. </p>

<p>Like - <code>Who is Bill Gates and where he was born?</code> or <code>Who is Bill Gates, where he was born?</code> contains two queries </p>

<ol>
<li>Who is Bill Gates?</li>
<li>Where Bill Gates was born</li>
</ol>

<p>I worked on Coreference resolution, so I can identify that <code>he</code> points to <code>Bill Gates</code> so resolved sentence is ""Who is Bill Gates, where Bill Gates was born""</p>

<p>Like wise</p>

<pre><code>MGandhi is good guys, Where he was born?
single query
who is MGandhi and where was he born?
2 queries
who is MGandhi, where he was born and died?
3 queries
India won world cup against Australia, when?
1 query (when India won WC against Auz)
</code></pre>

<p>I can perform Coreference resolution but not getting how can I distinguish queries in it. 
How to do this? </p>

<p>I checked various sentence parser, but as this is pure nlp stuff, sentence parser does not identify it. </p>

<p>I tried to find ""Sentence disambiguation"" like ""word sense disambiguation"", but nothing exist like that.</p>

<p>Any help or suggestion would be much appreciable. </p>
",Coreference & Relation Extraction,doe algorithm exist identify different query question sentence want identifies different query sentence like contains two query bill gate bill gate wa born worked coreference resolution identify point resolved sentence bill gate bill gate wa born like wise perform coreference resolution getting distinguish query checked various sentence parser pure nlp stuff sentence parser doe identify tried find sentence disambiguation like word sense disambiguation nothing exist like help suggestion would much appreciable
Understanding conditions of NP-Pronoun non-coreference - Lappin &amp; Leass (1992),"<p>I am going through the Lappin and Leass ""An algorithm of pronominal anapora resolution"". I am particularly confused about some of the conditions for the syntactic filter.</p>

<p>Background: (Section 2.1.1 of the above paper)</p>

<blockquote>
  <p>The agreement features of an NP are its number, person, and gender
  features. We will say that a phrase P is in the argument domain of a
  phrase N iff P and N are both arguments of the same head.</p>
  
  <p>We will say that P is in the adjunct domain of N iff N is an argument of a head H, P is the object of a  preposition PREP, and PREP
  is an adjunct of H. </p>
  
  <p>P is in the NP domain of N iff N is the determiner of a noun Q and (i)
  P is an argument of Q, or (ii) P is the object of a preposition PREP
  and PREP is an adjunct of Q. </p>
  
  <p>A phrase P is contained in a phrase Q iff (i) P is either an argument
  or an adjunct of Q, i.e., P is immediately contained in Q, or (ii) P
  is immediately contained in some phrase R, and R is contained in Q.</p>
</blockquote>

<p>Then the go on to write 6 conditions for this purpose. These are the Conditions on NP-pronoun non-coreference.
I fail to understand that given a constituency tree how to apply all of this? Could anyone explain the examples to me? I strongly feel I have flawed understanding that is resulting in my code to wrongly process sentences.</p>

<blockquote>
  <ol start=""2"">
  <li>P is in the argument domain of N.</li>
  </ol>
</blockquote>

<p>Sentence: ""She likes her"" &amp; ""John seems to want to see him""
How does one differentiate between co referential valid sentences from the above?? The second sentence stumps me the most.</p>

<pre><code>(S (NP she)
   (VP likes
       (NP her))

(S (NP John)
   (VP seems
       (S (VP to
              (VP want
                  (S (VP to
                         (VP see
                             (NP him)))))))))
</code></pre>

<blockquote>
  <ol start=""3"">
  <li>P is in the adjunct domain of N.</li>
  </ol>
</blockquote>

<p>Sentence: ""She sat near her""
Here ""her"" is supposedly adjunct of Head H. How do I know this based on the tree?</p>

<pre><code>(S (NP she)
   (VP sat
       (PP near
           (NP her))))
</code></pre>

<blockquote>
  <ol start=""4"">
  <li>P is an argument of a head H, N is not a pronoun, and N is contained in H.</li>
  </ol>
</blockquote>

<p>Sentence1: He believes that the main is amusing.</p>

<p>Sentence2: This is the man he said John wrote about.</p>

<pre><code>(S (NP He)
   (VP believes
       (SBAR that
             (S (NP the man)
                (VP is
                    (NP amusing))))))

(S (NP This)
   (VP is
       (NP (NP the man)
           (SBAR (S (NP he)
                    (VP said
                        (SBAR (S (NP John)
                                 (VP wrote
                                     (PP about))))))))))
</code></pre>

<blockquote>
  <ol start=""5"">
  <li>P is in the NP domain of N.
  Sentence: John's portrait of him is interesting</li>
  </ol>
</blockquote>

<pre><code>(S (NP (NP (NP ohn 's)
       portrait)
   (PP of
       (NP him)))
</code></pre>

<p>(VP is
       (ADJP interesting)))</p>

<p>Thank You.</p>
",Coreference & Relation Extraction,understanding condition np pronoun non coreference lappin lea going lappin lea algorithm pronominal anapora resolution particularly confused condition syntactic filter background section paper agreement feature np number person gender feature say phrase p argument domain phrase n iff p n argument head say p adjunct domain n iff n argument head h p object preposition prep prep adjunct h p np domain n iff n determiner noun q p argument q ii p object preposition prep prep adjunct q phrase p contained phrase q iff p either argument adjunct q e p immediately contained q ii p immediately contained phrase r r contained q go write condition purpose condition np pronoun non coreference fail understand given constituency tree apply could anyone explain example strongly feel flawed understanding resulting code wrongly process sentence p argument domain n sentence like john seems want see doe one differentiate co referential valid sentence second sentence stump p adjunct domain n sentence sat near supposedly adjunct head h know based tree p argument head h n pronoun n contained h sentence belief main amusing sentence man said john wrote p np domain n sentence john portrait interesting vp adjp interesting thank
Linking multiple name finder entities using OpenNLP,"<p>First a little bit of context: I'm trying to identify street addresses in a corpus of documents and we decided that the obvious solution for this would be to use an NLP (Apache OpenNLP in this case) tool to achieve this and so far everything looks great although we still need to train the model with a lot of documents, but that's not really an issue. We improved the solution by adding a extra step for address validation by using the <a href=""https://github.com/datamade/usaddress"" rel=""nofollow"">USAddress</a> parser from Datamade. My biggest issue is the fact that the addresses by themselves are nothing without a location next to them, sometimes the location is specified in the text and we will assume that this happens quite often. </p>

<p>Here comes my question: Is there someway to use coreference to associate the entities in the text? Or better yet is there a way to annotate arbitrary words in the text and identify them as being one entity? </p>

<p>I've been looking at the Apache OpenNLP documentation but...it's pretty thin and I think it still needs some work. </p>
",Coreference & Relation Extraction,linking multiple name finder entity using opennlp first little bit context trying identify street address corpus document decided obvious solution would use nlp apache opennlp case tool achieve far everything look great although still need train model lot document really issue improved solution adding extra step address validation using usaddress parser datamade biggest issue fact address nothing without location next sometimes location specified text assume happens quite often come question someway use coreference associate entity text better yet way annotate arbitrary word text identify one entity looking apache opennlp documentation pretty thin think still need work
Natural Language Process using SharpNLP with a sample,"<p>I am really new to C# and want to do a NLP project using SharpNLP. I know Currently it provides the following NLP tools:<br><br>
sentence splitter<br>
tokenizer<br>
part-of-speech tagger<br>
chunker <br>
parser<br>
name finder<br>
coreference tool<br>
interface to the WordNet lexical database</p>

<p>I tried several examples (i have those .nbin models) but failed to integrate the SharNLP tools to VS 2015. Can anyone give some guidance or samples to use this sharpNLP tool with VS. 
Thanks  </p>
",Coreference & Relation Extraction,natural language process using sharpnlp sample really new c want nlp project using sharpnlp know currently provides following nlp tool sentence splitter tokenizer part speech tagger chunker parser name finder coreference tool interface wordnet lexical database tried several example nbin model failed integrate sharnlp tool v anyone give guidance sample use sharpnlp tool v thanks
TensorFlow dimension issues: incompatible shapes of prediction vs. label tensors,"<p>I am trying to run a CNN for relation extraction code (found <a href=""https://github.com/OleNet/CNN-RelationExtraction"" rel=""nofollow"">here</a>), but I am running into an incompatible shapes issue with the tensors for logits and labels. For a batch size = 10, I get:</p>

<pre><code>InvalidArgumentError: Incompatible shapes: [10,2] vs. [270,2]
</code></pre>

<p>The model initialization looks like this:</p>

<pre><code>def weight_variable(shape, name):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial, name=name)

@staticmethod
def bias_variable(shape, name):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial, name=name)

def max_pool_16x1(x):
    return tf.nn.max_pool(x, ksize=[1, 16, 1, 1],
                          strides=[1, 1, 1, 1], padding='VALID')

def conv2d_valid(x, W):
    # by choosing [1,1,1,1] and ""same"" the output dimension == input dimension
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')

def net_work_diy(self, input_shape, classes):
    # assume x : [ batch x 20 x 320 x c ]
    self.m, self.n, self.c = input_shape
    self.classes = classes

    self.sess = tf.InteractiveSession()

    # 4 dimensional  datasize x seqwidth x veclength x channels
    self.x = tf.placeholder(tf.float32,  [None, self.m, self.n, self.c], name=""x-input"")
    self.y_ = tf.placeholder(tf.float32, [None, len(self.classes)], name=""y-input"")

    W_conv1 = CNN.weight_variable([5, self.n, self.c, 150], name=""w_conv1"")
    b_conv1 = CNN.bias_variable([150], name=""b_conv1"")

    with tf.name_scope(""conv_1"") as scope:
        h_conv1 = tf.nn.relu(CNN.conv2d_valid(self.x, W_conv1) + b_conv1)
        h_relu1 = tf.nn.relu(h_conv1)
        h_pool1 = CNN.max_pool_16x1(h_relu1)

    with tf.name_scope(""fully_connected"") as scope:
        self.keep_prob = tf.placeholder(""float"")
        h_fc1_drop = tf.nn.dropout(h_pool1, self.keep_prob)
        h_fc1_drop_flat = tf.reshape(h_fc1_drop, [-1, 150])
        W_fc1 = CNN.weight_variable([150, len(self.classes)], name=""w_fc1"")
        b_fc1 = CNN.bias_variable([len(self.classes)], name=""b_fc1"")
        h_fc1 = tf.matmul(h_fc1_drop_flat, W_fc1) + b_fc1

        self.y_conv = tf.nn.softmax(h_fc1)

    # Add summary ops to collect data
    _ = tf.histogram_summary(""weights"", W_conv1)
    _ = tf.histogram_summary(""biases"", b_conv1)
    _ = tf.histogram_summary(""y"", self.y_conv)

    with tf.name_scope(""xent"") as scope:
        cross_entropy = -tf.reduce_sum(self.y_ * tf.log(self.y_conv))
        _ = tf.scalar_summary('cross entropy', cross_entropy)

    with tf.name_scope(""train"") as scope:
        self.train_step = tf.train.AdamOptimizer(1e-5).minimize(cross_entropy)

    with tf.name_scope(""test"") as scope:
        # self.predict = tf.argmax(self.y_conv, 1)
        self.correct_prediction = tf.equal(tf.argmax(self.y_conv, 1), tf.argmax(self.y_, 1))
        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))
</code></pre>

<p>I saw some answers around suggesting to change the stride parameter in the conv2d and max_pool, so I tried replacing <code>conv2d_valid</code> and <code>max_pool_16x1</code> with the following functions:</p>

<pre><code>def conv2d_same(x, W):
    # by choosing [1,1,1,1] and ""same"" the output dimension == input dimension
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                          strides=[1, 2, 2, 1], padding='SAME')
</code></pre>

<p>but it did not solve the problem, I got the following error:</p>

<pre><code>InvalidArgumentError: Incompatible shapes: [10,2] vs. [34500,2]
</code></pre>

<p>Would appreciate your help in understanding this issue!</p>
",Coreference & Relation Extraction,tensorflow dimension issue incompatible shape prediction v label tensor trying run cnn relation extraction code found running incompatible shape issue tensor logits label batch size get model initialization look like saw answer around suggesting change stride parameter conv max pool tried replacing following function solve problem got following error would appreciate help understanding issue
How to increase accuracy in Coreference resolution for a chat conversation?,"<p>I need to resolve coreferences but the input text is excerpted from a chat conversation so the accuracy of the tool(<em>Stanford Corenlp</em>), currently being used, is low owing to the dialogue between two persons.</p>
",Coreference & Relation Extraction,increase accuracy coreference resolution chat conversation need resolve coreference input text excerpted chat conversation accuracy tool stanford corenlp currently used low owing dialogue two person
NLTK Relation Extraction - custom corpus in relextract.extract_rels,"<p>I learnt that there is a built-in function in NLTK which could extract relations from NER-tagged sentences according the following:</p>

<pre><code>   import re

   IN = re.compile(r'.*\bin\b(?!\b.+ing\b)')

   for fileid in ieer.fileids():
       for doc in ieer.parsed_docs(fileid):
           for rel in relextract.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):
               print(relextract.rtuple(rel))
</code></pre>

<p>It seems me very promising for general purpose, but I understood that <code>relextract.extract_rels</code> accepts only <code>'ieer'</code> or <code>'conll2002'</code> for the parameter <code>corpus</code>. But in this case, its usage is restricted only to these two corpora, isn't it? How could one utilize it for his own corpus (presuming, of course, that it is NER-tagged).</p>
",Coreference & Relation Extraction,nltk relation extraction custom corpus relextract extract rels learnt built function nltk could extract relation ner tagged sentence according following seems promising general purpose understood accepts parameter case usage restricted two corpus could one utilize corpus presuming course ner tagged
How do I implement entity coreferencing for company name data?,"<p>I have data with a column vector of company names as such:</p>

<pre><code>Var1
AEP
American Electric Power
AEP - Ohio
Chase Bank
JP Morgan
Chase
</code></pre>

<p>I am trying to figure out how to resolve the coreferences in this dataset. I have been trying to figure out how to use Wikipedia miner to solve this problem, but I don't know how it works or how to use it. Does there exist a tool that can find instances of different names that relate to the same company?</p>

<p>In other words, I would like to merge similar names of the same company (e.g. AEP, American Electric Power, and AEP - Ohio are all one company and so is Chase Bank, Chase, and JP Morgan). The column has over 41,000 unique number of companies.</p>
",Coreference & Relation Extraction,implement entity coreferencing company name data data column vector company name trying figure resolve coreference dataset trying figure use wikipedia miner solve problem know work use doe exist tool find instance different name relate company word would like merge similar name company e g aep american electric power aep ohio one company chase bank chase jp morgan column ha unique number company
How to build features for relation extraction for SVM?,"<p>The field of interest: Information Extraction</p>

<p>This is very new to me and I am trying to understand how to engineer features (whether lexical or semantic) in order to learn relationship in information extraction.</p>

<p>From papers, one of the simple steps to take features for supervised learning in relations between two named entities is </p>

<pre><code>* The sequence of words between the two entities
* the part of speech tags of these words
* bag of words between the two words
</code></pre>

<p>Shouldn't the number of words between the two entities change depending on the training sentence you are looking at? Then, how to build feature vectors which are not consistent in their sizes?</p>

<p>For instance, see the following two sentences for learning person and organization with a relation: an owner of. </p>

<pre><code>1. Mike is the owner of the company called, Spark.
2. Denis owns the black building called Halo.
</code></pre>

<p>For the example 1, the number of words between Mike and Spark are [is, the, owner, of, the, company, called] which are 7 feature words, while for the example 2, we have [owns, the, black, building, called] 5 feature words.</p>

<p>Where am I misunderstanding this problem? 
Thank you!</p>
",Coreference & Relation Extraction,build feature relation extraction svm field interest information extraction new trying understand engineer feature whether lexical semantic order learn relationship information extraction paper one simple step take feature supervised learning relation two named entity number word two entity change depending training sentence looking build feature vector consistent size instance see following two sentence learning person organization relation owner example number word mike spark owner company called feature word example owns black building called feature word misunderstanding problem thank
How to classify extracted relations (NLP)?,"<p>There are some not labeled corpus. I extracted from it triples (OBJECT, RELATION, OBJECT). For relation extraction I use Stanford OpenIE. But I need only some of this triples. For example, I need relation ""<strong><em>funded</em></strong>"".</p>

<p><strong><em>Text</em></strong>:
<code>Every startup needs a steady diet of funding to keep it strong and growing. Datadog, a monitoring service that helps customers bring together data from across a variety of infrastructure and software is no exception. Today it announced a massive $94.5 million Series D Round. The company would not discuss valuation.</code></p>

<p>From this text i want to extract relation <code>(Datadog, announced, $94.5 million Round)</code></p>

<p>I have only one idea:</p>

<ol>
<li>Use StanfordCoreference to detect that 'Datadog' in the first sentence and 'it' in second sentence are the same entity  </li>
<li>Try to cluster relations, but i think it's won't work well</li>
</ol>

<p>May be there are better approach? May be I need labeled corpus(i haven't it)?</p>
",Coreference & Relation Extraction,classify extracted relation nlp labeled corpus extracted triple object relation object relation extraction use stanford openie need triple example need relation funded text text want extract relation one idea use stanfordcoreference detect datadog first sentence second sentence entity try cluster relation think work well may better approach may need labeled corpus
Using Stanford CoreNLP for CorefResolution,"<p>I am trying to use Stanford CoreNLP to perform Coref resolution. The version I use is stanford-corenlp-full-2015-12-09. Basically, I have wrote some classes:</p>

<pre><code>import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Properties;


public class CorefResolution {
    public static String corefResolute(String text, List&lt;String&gt; tokenToReplace) {
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        Annotation doc = new Annotation(text);
        pipeline.annotate(doc);

        Map&lt;Integer, CorefChain&gt; corefs = doc.get(CorefCoreAnnotations.CorefChainAnnotation.class);
        System.out.println(corefs);
        List&lt;CoreMap&gt; sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
        List&lt;String&gt; resolved = new ArrayList&lt;String&gt;();

        for (CoreMap sentence : sentences) {
            List&lt;CoreLabel&gt; tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);

            for (CoreLabel token : tokens) {

                Integer corefClustId = token.get(CorefCoreAnnotations.CorefClusterIdAnnotation.class);
                token.get(Coref)

                if (corefClustId == null) {
                    System.out.println(""NULL NULL NULL\n"");
                    resolved.add(token.word());
                    continue;
                }
                else {
                    System.out.println(""Exist Exist Exist\n"");
                }

                System.out.println(""coreClustId is ""+corefClustId.toString()+""\n"");
                CorefChain chain = corefs.get(corefClustId);

                if (chain == null || chain.getMentionsInTextualOrder().size() == 1) {
                    resolved.add(token.word());
                } else {
                    int sentINdx = chain.getRepresentativeMention().sentNum - 1;
                    CoreMap corefSentence = sentences.get(sentINdx);
                    List&lt;CoreLabel&gt; corefSentenceTokens = corefSentence.get(CoreAnnotations.TokensAnnotation.class);

                    CorefChain.CorefMention reprMent = chain.getRepresentativeMention();

                    if (tokenToReplace.contains(token.word())) {
                        for (int i = reprMent.startIndex; i &lt; reprMent.endIndex; i++) {
                            CoreLabel matchedLabel = corefSentenceTokens.get(i - 1);
                            resolved.add(matchedLabel.word());
                        }
                    } else {
                        resolved.add(token.word());
                    }
                }
            }
        }

        Detokenizer detokenizer = new Detokenizer();
        String resolvedStr = detokenizer.detokenize(resolved);

        return resolvedStr;
    }
}
</code></pre>

<p>Another class</p>

<pre><code>import java.util.Arrays;
import java.util.List;
import java.util.LinkedList;


public class Detokenizer {

    public String detokenize(List&lt;String&gt; tokens) {
        //Define list of punctuation characters that should NOT have spaces before or after
        List&lt;String&gt; noSpaceBefore = new LinkedList&lt;String&gt;(Arrays.asList("","", ""."","";"", "":"", "")"", ""}"", ""]"", ""'"", ""'s"", ""n't""));
        List&lt;String&gt; noSpaceAfter = new LinkedList&lt;String&gt;(Arrays.asList(""("", ""["",""{"", ""\"""",""""));

        StringBuilder sentence = new StringBuilder();

        tokens.add(0, """");  //Add an empty token at the beginning because loop checks as position-1 and """" is in noSpaceAfter
        for (int i = 1; i &lt; tokens.size(); i++) {
            if (noSpaceBefore.contains(tokens.get(i))
                    || noSpaceAfter.contains(tokens.get(i - 1))) {
                sentence.append(tokens.get(i));
            } else {
                sentence.append("" "" + tokens.get(i));
            }

            // Assumption that opening double quotes are always followed by matching closing double quotes
            // This block switches the "" to the other set after each occurrence
            // ie The first double quotes should have no space after, then the 2nd double quotes should have no space before
            if (""\"""".equals(tokens.get(i - 1))) {
                if (noSpaceAfter.contains(""\"""")) {
                    noSpaceAfter.remove(""\"""");
                    noSpaceBefore.add(""\"""");
                } else {
                    noSpaceAfter.add(""\"""");
                    noSpaceBefore.remove(""\"""");
                }
            }
        }
        return sentence.toString();
    }
}
</code></pre>

<p>Another class file</p>

<pre><code>import java.io.*;
import java.nio.charset.Charset;
import java.util.Arrays;
import java.util.List;


public class PlainTextCorefResolver {

    public static void resolveFile(File inputFile, File outputFile) {
        try {
            BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(inputFile), Charset.forName(""UTF-8"")));
            PrintWriter writer = new PrintWriter(outputFile, ""UTF-8"");


            if (inputFile.exists()) System.out.println(""input exist\n"");
            else System.out.println(""input not exist\n"");

            if (outputFile.exists()) System.out.println(""output exist\n"");
            else System.out.println(""output not exist\n"");

            while(true){
                String line = reader.readLine();
                //EOF
                if(line == null)
                    break;
                //Resolve line
                List&lt;String&gt; tokenToReplace = Arrays.asList(""He"", ""he"", ""She"", ""she"", ""It"", ""it"", ""They"", ""they""); //!!!
                String resolvedLine = CorefResolution.corefResolute(line, tokenToReplace);
                writer.println(resolvedLine);
            }
            reader.close();
            writer.close();

        } catch (Exception e){
            System.err.println(""Failed to open/resolve input file ["" +inputFile.getAbsoluteFile()+ ""] in loader"");
            e.printStackTrace();
            return;
        }

    }


    public static void main(String[] args) {
        String inputFileName = ""path/file.txt"";
        String outputFileName =  ""path/file.resolved.txt"";
        File inputFile = new File(inputFileName);
        File outputFile = new File(outputFileName);
        resolveFile(inputFile, outputFile);
    }

}
</code></pre>

<p>However, it doesn't give any useful result. The corefClusterId is always null, thus I always get a bunch of ""NULL NULL NULL"" outputs. </p>

<p>How can I correctly perform coreference resolution to replace such as ""He/he/She/she/It/it/The stadium/..."" with its most typical mention (person or organization's name)?</p>

<p>For example, given:
""Estadio El Madrigal is a stadium in Spain, used since 1923. It is currently mostly used for football matches.""
I want to get
""Estadio El Madrigal is a stadium in Spain, used since 1923. Estadio El Madrigal is currently mostly used for football matches.""</p>
",Coreference & Relation Extraction,using stanford corenlp corefresolution trying use stanford corenlp perform coref resolution version use stanford corenlp full basically wrote class another class another class file however give useful result corefclusterid always null thus always get bunch null null null output correctly perform coreference resolution replace stadium typical mention person organization name example given estadio el madrigal stadium spain used since currently mostly used football match want get estadio el madrigal stadium spain used since estadio el madrigal currently mostly used football match
relation matching after relation extraction,"<p>I have the following concepts. </p>

<p>chief accepted proposal,<br>
chief rejected proposal,<br>
proposal was deferred<br>
...<br>
proposal accepted consensus,<br>
no consensus proposal rejected.  </p>

<p>I want to match instances in other sentences which share the same concept/meaning. For instance, other sentences where the ""proposal was accepted"".</p>

<p>I have researched on the topic relation extraction. However, i would like to use these retrieved relations to match relations in other sentences. </p>

<p>I am using string matching at the moment but it not giving desired result. </p>

<p>Meaningful capture:<br>
""the revised proposal was accepted by chief at seminar.""</p>

<p>Inaccurate captures:<br>
But also inaccurate captures happen which don't mean the same thing : same words also exist in these sentences and its not same meaning.</p>

<p>""Chief, i 'm particularly interested in hearing your opinion about the following three issues : should this proposal be accepted at all . .""</p>

<p>""Chief said, i will retract the proposal, because is too late for me.is the proposal accepted ? .""</p>

<p>""hi chief, should this proposal be accepted at all . .""</p>

<p>I was thinking of using relationship extraction and then match it to the relationships in these sentences. Is there a way to do relationship matching after relation extraction?  I cant find any reference to relationship matching on the web.</p>
",Coreference & Relation Extraction,relation matching relation extraction following concept chief accepted proposal chief rejected proposal proposal wa deferred proposal accepted consensus consensus proposal rejected want match instance sentence share concept meaning instance sentence proposal wa accepted researched topic relation extraction however would like use retrieved relation match relation sentence using string matching moment giving desired result meaningful capture revised proposal wa accepted chief seminar inaccurate capture also inaccurate capture happen mean thing word also exist sentence meaning chief particularly interested hearing opinion following three issue proposal accepted chief said retract proposal late proposal accepted hi chief proposal accepted wa thinking using relationship extraction match relationship sentence way relationship matching relation extraction cant find reference relationship matching web
How to identify Coreference set and representative mentions in Stanford CoreNLP Coreferences?,"<p>I am using Stanford CoreNLP. I need to detect and identify the ""Coreference set""s and ""representative mention""s for each CorefChain in my input text:</p>

<p>For example:
Input: 
<strong>Obama was elected to the Illinois state senate in 1996 and served there for eight years. In 2004, he was elected by a record majority to the U.S. Senate from Illinois and, in February 2007, announced his candidacy for President.</strong></p>

<p>Output: With ""Pretty Print"" I can get the output below:</p>

<pre><code>**Coreference set:
(2,4,[4,5]) -&gt; (1,1,[1,2]), that is: ""he"" -&gt; ""Obama""

(2,24,[24,25]) -&gt; (1,1,[1,2]), that is: ""his"" -&gt; ""Obama""

(3,22,[22,23]) -&gt; (1,1,[1,2]), that is: ""Obama"" -&gt; ""Obama""**
</code></pre>

<p>However, I need to programmatically identify and detect the output above, which is called the ""Coreference set"". (I mean I need to identify all the pairs like: ""he"" -> ""Obama"") </p>

<p><strong>Note: My base code is the one below (it is from <a href=""http://stanfordnlp.github.io/CoreNLP/coref.html"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/coref.html</a>):</strong></p>

<pre><code>import edu.stanford.nlp.hcoref.CorefCoreAnnotations;
import edu.stanford.nlp.hcoref.data.CorefChain;
import edu.stanford.nlp.hcoref.data.Mention;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;
import java.util.Properties;
public class CorefExample {

public static void main(String[] args) throws Exception {

Annotation document = new Annotation(""Obama was elected to the Illinois state senate in 1996 and served there for eight years. In 2004, he was elected by a record majority to the U.S. Senate from Illinois and, in February 2007, announced his candidacy for President."");
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,mention,coref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(document);
System.out.println(""---"");
System.out.println(""coref chains"");
for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {
  System.out.println(""\t""+cc);
}
for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
  System.out.println(""---"");
  System.out.println(""mentions"");
  for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {
    System.out.println(""\t""+m);
     }
   }
  }
 }

 ///// Any Idea? THANK YOU in ADVANCE
</code></pre>
",Coreference & Relation Extraction,identify coreference set representative mention stanford corenlp coreference using stanford corenlp need detect identify coreference set representative mention corefchain input text example input obama wa elected illinois state senate served eight year wa elected record majority u senate illinois february candidacy president output pretty print get output however need programmatically identify detect output called coreference set mean need identify pair like obama note base code one
pronoun resolution backwards,"<p>The usual coreference resolution works in the following way:</p>

<p>Provided</p>

<pre><code>The man likes math. He really does.
</code></pre>

<p>it figures out that </p>

<pre><code>he 
</code></pre>

<p>refers to </p>

<pre><code>the man.
</code></pre>

<p>There are plenty of tools to do this.</p>

<p>However, is there a way to do it backwards? </p>

<p>For example,</p>

<p>given</p>

<pre><code>The man likes math. The man really does.
</code></pre>

<p>I want to do the pronoun resolution ""backwards,""</p>

<p>so that I get an output like</p>

<pre><code>The man likes math. He really does.
</code></pre>

<p>My input text will mostly be 3~10 sentences, and I'm working with python.</p>
",Coreference & Relation Extraction,pronoun resolution backwards usual coreference resolution work following way provided figure refers plenty tool however way backwards example given want pronoun resolution backwards get output like input text mostly sentence working python
Auto analyse data (natural language processing),"<p>I am an intern in a hospital maintenance company. I am asked to categorize and analyse the log which records all the problems encountered and raised by users from the helpdesk. Since these incidents (at least 2000-3000) are typed in natural language, is there a way to automate the categorizing process to build a web application which can answer users' questions(NLP?)? (This could be something using IBM Watson or alternative ways).</p>

<p>I am quite blur about how to use Watson to do it (writing code to the services it provide, like Q&amp;A and Relation Extraction). Alternative ideas are appreciated as well. </p>
",Coreference & Relation Extraction,auto analyse data natural language processing intern hospital maintenance company asked categorize analyse log record problem encountered raised user helpdesk since incident least typed natural language way automate categorizing process build web application answer user question nlp could something using ibm watson alternative way quite blur use watson writing code service provide like q relation extraction alternative idea appreciated well
How to encode dependency path as a feature for classification?,"<p>I am trying to implement relation extraction between verb pairs. I want to use dependency path from one verb to the other as a feature for my classifier (predicts if relation X exists or not). But I am not sure how to encode the dependency path as a feature. Following are some example dependency paths, as space separated relation annotations from StanfordCoreNLP Collapsed Dependencies:</p>

<pre><code>nsubj acl nmod:from acl nmod:by conj:and
nsubj nmod:into
nsubj acl:relcl advmod nmod:of
</code></pre>

<p>It is important to keep in mind that these path are of <strong>variable length</strong> and a relation could <strong>reappear</strong> without any restriction. </p>

<p>Two compromising ways of encoding this feature that come to my mind are: </p>

<p>1) Ignore the sequence, and just have one feature for each relation with its value being the number of times it appears in the path</p>

<p>2) Have a sliding window of length n, and have one feature for each possible pair of relations with the value being the number of times those two relations appeared consecutively. I suppose this is how one encodes n-grams. However, the number of possible relations is 50, which means I cannot really go with this approach. </p>

<p>Any suggestions are welcomed.</p>
",Coreference & Relation Extraction,encode dependency path feature classification trying implement relation extraction verb pair want use dependency path one verb feature classifier predicts relation x exists sure encode dependency path feature following example dependency path space separated relation annotation stanfordcorenlp collapsed dependency important keep mind path variable length relation could reappear without restriction two compromising way encoding feature come mind ignore sequence one feature relation value number time appears path sliding window length n one feature possible pair relation value number time two relation appeared consecutively suppose one encodes n gram however number possible relation mean really go approach suggestion welcomed
How to replicate the pronouns (pronominal) with their antecedents,"<p>Given the following Text:</p>

<p>""Patient:   Fine, and the movie was fascinating. It was called First Angels, and it was about Lincoln -- it was based on a book, I guess. But it was about Lincoln when he was eight years old. The whole movie  w -- took place then. And in that span of time -- I mean he must have been like eight or nine, his mother died of some disease, and, um, he had a cousin, and the cousin's parents died. So the cousin was living with him. And the cousin's voice i -- the cousin tells the story. You very seldom hear Lincoln speak. And, um, you learn about the relationship with his father, and -- and his father leaves them to find a wife, and comes back with a wife and several children.""</p>

<p>I need to replicate the mentions of ""his,her,my,their...."" with their actual antecedents. When I used Stanford coref. resolution, it gives me lots of mentions which are not useful like ""his father"" to ""his father"" or even ""his father"" to ""his"". I need the text have ""Lincoln's father"" or ""Lincoln's mother"".  I have list of mentions : 
[619, 625]=[647, 653], [565, 575]=[588, 598], [565, 575]=[588, 591].....
I can solve this problem in an ugly way of programming but I wonder if there is any method or way to clean extra mentions or a method helping for putting actual names in place of their pronouns?</p>
",Coreference & Relation Extraction,replicate pronoun pronominal antecedent given following text patient fine movie wa fascinating wa called first angel wa lincoln wa based book guess wa lincoln wa eight year old whole movie w took place span time mean must like eight nine mother died disease um cousin cousin parent died cousin wa living cousin voice cousin tell story seldom hear lincoln speak um learn relationship father father leaf find wife come back wife several child need replicate mention actual antecedent used stanford coref resolution give lot mention useful like father father even father need text lincoln father lincoln mother list mention solve problem ugly way programming wonder method way clean extra mention method helping putting actual name place pronoun
How to implement a good Pronoun Resolver algorithm in OpenNLP?,"<p>I use OpenNLP's coreference package for anaphora resolution. So basically I have this input string:</p>

<blockquote>
  <p>""Harry writes a letter to his brother. He told him that he met Mary in London. They had a lunch together."";</p>
</blockquote>

<p>The set of mentions output are as below:</p>

<blockquote>
  <p>Harry, his, He, him, he, They</p>
</blockquote>

<p>I need to replace the pronouns with its proper nouns. I wrote a simple algorithm for this by adding each mention into a list, then iterate the list while replacing each pronouns with the 1st mention (""Harry""). My problem is ""his"" will be ""Harry"" not ""Harry's"".</p>

<p>There aren't a lot of examples/tutorials on pronoun resolver. I have looked through the OpenNLP API. Either I'm looking in the wrong direction or there is one in the API but I don't know how to use it. Can someone please guide me in the right direction to pronoun resolving or give an example of how to do this? Maybe there's a better way that I didn't know of.</p>
",Coreference & Relation Extraction,implement good pronoun resolver algorithm opennlp use opennlp coreference package anaphora resolution basically input string harry writes letter brother told met mary london lunch together set mention output harry need replace pronoun proper noun wrote simple algorithm adding mention list iterate list replacing pronoun st mention harry problem harry harry lot example tutorial pronoun resolver looked opennlp api either looking wrong direction one api know use someone please guide right direction pronoun resolving give example maybe better way know
CLIPS command line prompt for Mac,"<p>I have downloaded the dmg file for CLIPS, the rule based language system due to NASA. But I am not able to find any executable that lets me run the command prompt version of CLIPS on mac. I have thoroughly checked all the folders and have googled to no end.</p>

<p>Also, is there any other more modern rule engine out there that is worth looking at? We are trying to write language rules for phrase chunking and entity relation extraction in incomplete phrase like sentences.</p>
",Coreference & Relation Extraction,clip command line prompt mac downloaded dmg file clip rule based language system due nasa able find executable let run command prompt version clip mac thoroughly checked folder googled end also modern rule engine worth looking trying write language rule phrase chunking entity relation extraction incomplete phrase like sentence
Accessing Stanford Core NLP Coreference Chain output in Ruby,"<p>I am trying to use the Stanford Core NLP <code>:coref_chain</code> to get a list of entity mentions out of some text. When I run this code:</p>

<pre><code>text = 'Angela Merkel met Nicolas Sarkozy on January 25th in ' +
   'Berlin to discuss a new austerity package. Sarkozy ' +
   'looked pleased, but Merkel was dismayed.'

pipeline =  StanfordCoreNLP.load(:tokenize, :ssplit, :pos, :lemma, :parse, :ner, :dcoref)
text = StanfordCoreNLP::Annotation.new(text)
pipeline.annotate(text)

  puts text.get(:coref_chain)
</code></pre>

<p>I get this output:</p>

<pre><code>{1=CHAIN1-[""Angela Merkel"" in sentence 1, ""Merkel"" in sentence 2], 3=CHAIN3-[""January 25th"" in sentence 1], 4=CHAIN4-[""Berlin"" in sentence 1], 5=CHAIN5-[""Nicolas Sarkozy on January 25th"" in sentence 1, ""Sarkozy"" in sentence 2], 6=CHAIN6-[""a new austerity package"" in sentence 1]}
</code></pre>

<p>Is this a hash? According to the documentation on Stanford site, I should be able to access these values through the attribute names but no combination has worked for me. In fact, adding anything other than <code>to_s</code> yields a ""no method found"" error.</p>

<p>Does any one know how I would get the names out of this? Just ""Angela Merkel"" for example? In a best case scenario, I would get the start, end and heads as well.</p>
",Coreference & Relation Extraction,accessing stanford core nlp coreference chain output ruby trying use stanford core nlp get list entity mention text run code get output hash according documentation stanford site able access value attribute name combination ha worked fact adding anything yield method found error doe one know would get name angela merkel example best case scenario would get start end head well
Modeling features of Relation Extraction in the SVMlight input format,"<p>I am currently working on a project that focuses on relation extraction from a corpus of Wikipedia text, and I plan to use an SVM to extract these relations. To model this, I plan to use Word features, POS Tag features, Entity features, Mention features and so on as mentioned in the following paper - <a href=""https://gate.ac.uk/sale/eswc06/eswc06-relation.pdf"" rel=""nofollow"">https://gate.ac.uk/sale/eswc06/eswc06-relation.pdf</a> (Page 6 onwards)</p>

<p>Now, I have set up the pipeline for feature extraction and got the corpus annotated and I wish to use a package like SVM-Light for the purpose of the project. According to the input file format of the SVM-Light package, this is the requisite format - 
 .=.  : : ... : #</p>

<p>Example (from the SVM-Light webpage) - </p>

<p>In classification mode, the target value denotes the class of the example. +1 as the target value marks a positive example, -1 a negative example respectively. So, for example, the line</p>

<p>-1 1:0.43 3:0.12 9284:0.2 # abcdef</p>

<p>specifies a negative example for which feature number 1 has the value 0.43, feature number 3 has the value 0.12, feature number 9284 has the value 0.2, and all the other features have value 0. In addition, the string abcdef is stored with the vector, which can serve as a way of providing additional information for user defined kernels. </p>

<p>Now, I wish to know how do we model the features that I am using whose values include words, POS Tags and entity types and subtypes into the feature vector accepted by the SVM-Light package, where each feature has a real number value associated with it. How is the mapping from my choice of features to these real values done? </p>

<p>It would be of great help if someone who has worked at a similar problem before could just prod me in the right direction. </p>

<p>Thanks.</p>
",Coreference & Relation Extraction,modeling feature relation extraction svmlight input format currently working project focus relation extraction corpus wikipedia text plan use svm extract relation model plan use word feature po tag feature entity feature mention feature mentioned following paper page onwards set pipeline feature extraction got corpus annotated wish use package like svm light purpose project according input file format svm light package requisite format example svm light webpage classification mode target value denotes class example target value mark positive example negative example respectively example line abcdef specifies negative example feature number ha value feature number ha value feature number ha value feature value addition string abcdef stored vector serve way providing additional information user defined kernel wish know model feature using whose value include word po tag entity type subtypes feature vector accepted svm light package feature ha real number value associated mapping choice feature real value done would great help someone ha worked similar problem could prod right direction thanks
Stanford coreNLP : can a word in a sentence be part of multiple Coreference chains,"<p>The question is in the title. Using Stanford's NLP coref module, I am wondering if a given word can be part of multiple coreference chains. Or can it only be part of one chain. 
Could you give me examples of when this might occur. </p>

<p>Similarly, can a word be part of multiple coreference mentions, or only one.</p>

<p>Thank you.</p>
",Coreference & Relation Extraction,stanford corenlp word sentence part multiple coreference chain question title using stanford nlp coref module wondering given word part multiple coreference chain part one chain could give example might occur similarly word part multiple coreference mention one thank
Stanford CoreNLP wrong coreference resolution,"<p>I am still playing with Stanford's CoreNLP and I am encountering strange results on a very trivial test of Coreference resolution. </p>

<p>Given the two sentences :  </p>

<blockquote>
  <p>The hotel had a big bathroom. It was very clean.</p>
</blockquote>

<p>I would expect ""It"" in sentence 2 to be coreferenced by ""bathroom"" or at least ""a big bathroom"" of sentence 1. </p>

<p>Unfortunately it point to ""The hotel"" which in my opinion is wrong.</p>

<p>Is there a way to solve this problem ? Do I need to train anything or is it supposed to work out of the box ?</p>

<pre><code>    Annotation a = getPipeline().getAnnotation(""The hotel had a big bathroom. It was very clean."");

    System.out.println(a.get(CorefChainAnnotation.class));
</code></pre>

<p>output : </p>

<blockquote>
  <p>{1=CHAIN1-[""The hotel"" in sentence 1, ""It"" in sentence 2], 2=CHAIN2-[""a big bathroom"" in sentence 1]}</p>
</blockquote>

<p>Many thanks for your help.</p>
",Coreference & Relation Extraction,stanford corenlp wrong coreference resolution still playing stanford corenlp encountering strange result trivial test coreference resolution given two sentence hotel big bathroom wa clean would expect sentence coreferenced bathroom least big bathroom sentence unfortunately point hotel opinion wrong way solve problem need train anything supposed work box output chain hotel sentence sentence chain big bathroom sentence many thanks help
How to Identify mentions in a text?,"<p>I am looking for rule-based methods or any other methods to identify all mentions in a text. I have found several libraries that give coreferences but no exact options for only mentions. What I want is something like below: </p>

<p>Input text: </p>

<blockquote>
  <p>[This painter]'s indulgence of visual fantasy, and appreciation of different historic architectural styles can be seen in his 1840 Architect's Dream. After a series of paintings on The Last of the Mohicans, [he] made a three year trip to Europe in 1829, but [he] is better known for a trip four years earlier in which [he] journeyed up the Hudson River to the Catskill Mountains. FTP, name [this painter of The Oxbow] and The Voyage of Life series.</p>
</blockquote>

<p>*The square brackets highlight mentions. </p>

<p>How do I find mentions? Also, how is it different from coreferences? It would be really helpful if someone could post links to the concerned papers. </p>
",Coreference & Relation Extraction,identify mention text looking rule based method method identify mention text found several library give coreference exact option mention want something like input text painter indulgence visual fantasy appreciation different historic architectural style seen architect dream series painting last mohican made three year trip europe better known trip four year earlier journeyed hudson river catskill mountain ftp name painter oxbow voyage life series square bracket highlight mention find mention also different coreference would really helpful someone could post link concerned paper
Stanford NLP - Using Parsed or Tagged text to generate Full XML,"<p>I'm trying to extract data from the PennTreeBank, Wall Street Journal corpus. Most of it already has the parse trees, but some of the data is only tagged.
i.e. wsj_DDXX.mrg and wsj_DDXX.pos files.</p>

<p>I would like to use the already parsed trees and tagged data in these files so as not to use the parser and taggers within CoreNLP, but I still want the output file format that CoreNLP gives; namely, the XML file that contains the dependencies, entity coreference, and the parse tree and tagged data.</p>

<p>I've read many of the java docs but I cannot figure out how to get it the way I described.</p>

<p>For POS, I tried using the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.2/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html"" rel=""nofollow"">LexicalizedParser</a> and it allows me to use the tags, but I can only generate an XML file with the some of the information I want; there is no option for coreference or generating the parse trees. To get it to correctly generate the sub-optimal XML files here, I had to write a script to get rid of all of the brackets within the files. This is the command I use:</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat typedDependenciesCollapsed,wordsAndTags -outputFilesExtension xml -outputFormatOptions xml -writeOutputFiles -outputFilesDirectory my\dir -tokenized -tagSeparator / -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz my\wsj\files\dir</p>
</blockquote>

<p>I also can't generate the data I would like to have for the WSJ data that already has the trees. I tried using what is said <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#s"" rel=""nofollow"">here</a> and I looked at the corresponding Javadocs. I used the command similar to what is described. But I had to write a python program to retrieve the stdout data resulting from analyzing each file and wrote it into a new file. This resulting data is only a text file with the dependencies and is not in the desired XML notation.</p>

<p>To summarize, I would like to use the POS and tree data from these PTB files in order to generate a CoreNLP parse corresponding to what would occur if I used CoreNLP on a regular text file. The pseudo command would be like this:</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.pipeline.CoreNLP -useTreeFile wsj_DDXX.mrg</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.pipeline.CoreNLP -usePOSFile wsj_DDXX.pos</p>
</blockquote>

<p>Edit: fixed a link.</p>
",Coreference & Relation Extraction,stanford nlp using parsed tagged text generate full xml trying extract data penntreebank wall street journal corpus already ha parse tree data tagged e wsj ddxx mrg wsj ddxx po file would like use already parsed tree tagged data file use parser tagger within corenlp still want output file format corenlp give namely xml file contains dependency entity coreference parse tree tagged data read many java doc figure get way described po tried using lexicalizedparser allows use tag generate xml file information want option coreference generating parse tree get correctly generate sub optimal xml file write script get rid bracket within file command use java cp edu stanford nlp parser lexparser lexicalizedparser outputformat typeddependenciescollapsed wordsandtags outputfilesextension xml outputformatoptions xml writeoutputfiles outputfilesdirectory dir tokenized tagseparator tokenizerfactory edu stanford nlp process whitespacetokenizer tokenizermethod newcorelabeltokenizerfactory edu stanford nlp model lexparser englishpcfg ser gz wsj file dir also generate data would like wsj data already ha tree tried using said looked corresponding javadocs used command similar described write python program retrieve stdout data resulting analyzing file wrote new file resulting data text file dependency desired xml notation summarize would like use po tree data ptb file order generate corenlp parse corresponding would occur used corenlp regular text file pseudo command would like java cp edu stanford nlp pipeline corenlp usetreefile wsj ddxx mrg java cp edu stanford nlp pipeline corenlp useposfile wsj ddxx po edit fixed link
Other coreference resolution alternatives,"<p>I'm working a project that requires use of coreference resolution for sentences. I tried out the Stanford CoreNLP's coreference resolution and it works fine, though it does noticeably and expectedly run slower for larger pieces of text I'm analyzing.</p>

<p>Could anyone suggest alternative coreference resolution tools that may run faster than Stanford's? (Preferably in Java or Python)</p>
",Coreference & Relation Extraction,coreference resolution alternative working project requires use coreference resolution sentence tried stanford corenlp coreference resolution work fine though doe noticeably expectedly run slower larger piece text analyzing could anyone suggest alternative coreference resolution tool may run faster stanford preferably java python
Executing stanford corenlp coreference resolution,"<p>I am trying to execute stanford corenlp package to get coreference resolution. </p>

<p>Here is the command given for coref execution:</p>

<pre><code>java -cp &lt;jars_in_corenlp&gt; -Xmx8g edu.stanford.nlp.dcoref.SieveCoreferenceSystem -props &lt;properties file&gt;
</code></pre>

<p>I executed like this -</p>

<pre><code>java - cp ""*"" -Xmx2g edu.stanford.nlp.dcoref.SieveCoreferenceSystem -props annotators = pos, lemma, ner, parse dcoref.postprocessing = true dcoref.maxdist = -1 -file input.txt

java - cp ""*"" -Xmx2g edu.stanford.nlp.dcoref.SieveCoreferenceSystem -props annotators = pos, lemma, ner, parse dcoref.postprocessing = true dcoref.maxdist = -1 input.txt
</code></pre>

<p>gives error -</p>

<pre><code>Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
</code></pre>

<p>This way it works, but it loads all <code>jar</code> files, which takes additional time, I want to execute with minimal execution time.</p>

<pre><code>java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</code></pre>
",Coreference & Relation Extraction,executing stanford corenlp coreference resolution trying execute stanford corenlp package get coreference resolution command given coref execution executed like give error way work load file take additional time want execute minimal execution time
customizing stanford corenlp package,"<p>I am using python version of stanford corenlp which uses jsonrpc client/server logic. - <a href=""https://bitbucket.org/karimkhanp/corenlp-python"" rel=""nofollow"">link</a></p>

<p>While using this package, I get following result (result is for coreference resolution)</p>

<pre><code>Sentence - Bill Gates is richest person, he is also good by nature
</code></pre>

<p>result : </p>

<pre><code>Coreference set:
    (1,5,[4,13]) -&gt; (1,2,[1,3]), that is: ""richest person , he is also good by nature"" -&gt; ""Bill Gates""
    (1,5,[4,6]) -&gt; (1,2,[1,3]), that is: ""richest person"" -&gt; ""Bill Gates""
    (1,7,[7,8]) -&gt; (1,2,[1,3]), that is: ""he"" -&gt; ""Bill Gates""
</code></pre>

<p>Initially I thought replacing referring entity by referred entity will give me resolved sentence. But looking at above result, I thought this idea will not always work. </p>

<p>So I thought if referring entity is pronoun from I, you, he, she, it, we, they. then only replace it. So above sentence would now result as</p>

<pre><code>Bill Gates is richest person, Bill Gates is also good by nature
</code></pre>

<p>I want to know, can I customize stanford package to do this and get the modified resultant sentence as a result for this package?</p>

<p>Please consider this note :</p>

<blockquote>
  <p>It runs the Stanford CoreNLP jar in a separate process, communicates
  with the java process using its command-line interface, and makes
  assumptions about the output of the parser in order to parse it into a
  Python dict object and transfer it using JSON. The parser will break
  if the output changes significantly, but it has been tested on Core
  NLP tools version 3.4.1 released 2014-08-27.</p>
</blockquote>
",Coreference & Relation Extraction,customizing stanford corenlp package using python version stanford corenlp us jsonrpc client server logic link using package get following result result coreference resolution result initially thought replacing referring entity referred entity give resolved sentence looking result thought idea always work thought referring entity pronoun replace sentence would result want know customize stanford package get modified resultant sentence result package please consider note run stanford corenlp jar separate process communicates java process using command line interface make assumption output parser order parse python dict object transfer using json parser break output change significantly ha tested core nlp tool version released
Textrunner algorithm: what is a dependency chain?,"<p>On page 3 of <a href=""http://turing.cs.washington.edu/papers/ijcai07.pdf"" rel=""nofollow"">Open information extraction from the web</a> Banko et al describe a self-supervised classifier for relation extraction that that looks at a potential relations between noun phrases. They say that if a sequence of words linking two noun phrases does not meet certain criteria the relation is labeled as ""negative."" If the sequence of words does meet all criteria it is labeled as positive. </p>

<p>One of the criteria that they list is that: </p>

<blockquote>
  <p>There exists a dependency chain between Ei and Ej that is no longer
  than a certain length.</p>
</blockquote>

<p>What are they talking about here? What is a dependency chain? How do you identify a dependency chain?</p>
",Coreference & Relation Extraction,textrunner algorithm dependency chain page open information extraction web banko et al describe self supervised classifier relation extraction look potential relation noun phrase say sequence word linking two noun phrase doe meet certain criterion relation labeled negative sequence word doe meet criterion labeled positive one criterion list exists dependency chain ei ej longer certain length talking dependency chain identify dependency chain
which parser is most suitable for [biomedical] relation extraction?,"<p>I have read about continuency parser and dependency parser. but confused which could be the best choice.</p>

<p>my task is to extract relationship from english wikipedia text(other source may also be included later). What I need is an semantic path(with only most important information) between the two entities interesting. for instance,</p>

<p>form text:
<em><strong>""In America, diabetes is, as everybody knows, a common disease.""</strong></p>

<p>I need the information: 
<strong>""diabetes is disease""</em></strong></p>

<p>which implementation of parser would you suggest? Stanford? Maltparser? or other?</p>

<p>any clue is appreciated.</p>
",Coreference & Relation Extraction,parser suitable biomedical relation extraction read continuency parser dependency parser confused could best choice task extract relationship english wikipedia text source may also included later need semantic path important information two entity interesting instance form text america diabetes everybody know common disease need information diabetes disease implementation parser would suggest stanford maltparser clue appreciated
Huge overhead to loading StanfordOpenNLP model in Java?,"<p>I'm trying to use StanfordNLP to do coreference resolution on chunks of text relating to a given topic, and while trying to load in the StanfordCoreNLP model, it at first completely ran out of memory while loading models, but now is still taking upwards of 15 minutes to load.</p>

<p>I have code like: </p>

<pre><code> public Map&lt;Integer, CorefChain&gt; getCoreferences(String text) {
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation document = new Annotation(text);

    pipeline.annotate(document);

    return document.get(CorefCoreAnnotations.CorefChainAnnotation.class);
}
</code></pre>

<p>Is this unavoidable by design? Will it even be possible to do coreference resolution like this in a production application where anything more than 10 seconds is unacceptable?</p>
",Coreference & Relation Extraction,huge overhead loading stanfordopennlp model java trying use stanfordnlp coreference resolution chunk text relating given topic trying load stanfordcorenlp model first completely ran memory loading model still taking upwards minute load code like unavoidable design even possible coreference resolution like production application anything second unacceptable
Internal Access to Coreference Resolution System (Stanford CoreNLP),"<p>I am trying to accomplish coreference resolution within text in Tweets. I would like to use a system built for the Twitter use case as much as possible, so my plan is to do my tokenization and POS tagging using the ark-tweet-nlp package by CMU. I then would like to integrate this with the coreference resolution system in Stanford NLP (my plan is to use the Penn Treebank style tagger from CMU for easier integration). </p>

<p>I am new to Stanford NLP, and although I have been able to implement it's coreference resolution functionality, I can only figure out how to use it for the black box case of it handling all of it's POS, Lemma, NER, and Parsing through the properties interface. I have seen examples online of how to access the parser with outside POS, and tokens through a Java interface:</p>

<pre><code>String[] sent3 = { ""It"", ""can"", ""can"", ""it"", ""."" };
// Parser gets second ""can"" wrong without help (parsing it as modal MD)
String[] tag3 = { ""PRP"", ""MD"", ""VB"", ""PRP"", "".""           };                                                 
List&lt;TaggedWord&gt; sentence3 = new ArrayList&lt;TaggedWord&gt;();
for (int i = 0; i &lt; sent3.length; i++) {
    sentence3.add(new TaggedWord(sent3[i], tag3[i]));
}

Tree parse = lp.parse(sentence3);
parse.pennPrint();
</code></pre>

<p>So I was wondering if anyone knew how to implement this case through a similar kind of interface for coreference. </p>
",Coreference & Relation Extraction,internal access coreference resolution system stanford corenlp trying accomplish coreference resolution within text tweet would like use system built twitter use case much possible plan tokenization po tagging using ark tweet nlp package cmu would like integrate coreference resolution system stanford nlp plan use penn treebank style tagger cmu easier integration new stanford nlp although able implement coreference resolution functionality figure use black box case handling po lemma ner parsing property interface seen example online access parser outside po token java interface wa wondering anyone knew implement case similar kind interface coreference
What is the meaning of this NLP notation?,"<p>I am learning NLP to try and do relation extraction on a corpus. I found <a href=""http://www.ryanmcd.com/courses/gslt2007/gslt2007.pdf"" rel=""nofollow noreferrer"">these</a> slides and am trying to parse the notation for a high-dimensional feature vector (shown below). </p>

<p><img src=""https://i.sstatic.net/G3GoG.png"" alt=""enter image description here""></p>

<p>where</p>

<p><img src=""https://i.sstatic.net/hMZwV.png"" alt=""enter image description here""></p>

<p>How do I turn the top most equation into an English sentence? For each input text unit, x ; for each possible feature, y -- the feature x is-a y can be represented by a feature vector? I am used to seeing cartesian product notation and I am used to seeing function notation and I am used to seeing set builder notation. But there are too many unfamiliar things going on in that line for me to understand what it says. What does the colon mean? What does the arrow mean?</p>
",Coreference & Relation Extraction,meaning nlp notation learning nlp try relation extraction corpus found slide trying parse notation high dimensional feature vector shown turn top equation english sentence input text unit x possible feature feature x represented feature vector used seeing cartesian product notation used seeing function notation used seeing set builder notation many unfamiliar thing going line understand say doe colon mean doe arrow mean
Resolve coreference using Stanford CoreNLP - unable to load parser model,"<p>I want to do a very simple job: given a string containing pronouns, I want to resolve them.</p>

<p>for example, I want to turn the sentence ""Mary has a little lamb. She is cute."" in ""Mary has a little lamb. Mary is cute."".</p>

<p>I have tried to use Stanford CoreNLP. However, I seem unable to get the parser to start. I have imported all the included jars in my project using Eclipse, and I have allocated 3GB to the JVM (-Xmx3g).</p>

<p>The error is very awkward:</p>

<blockquote>
  <p>Exception in thread ""main"" java.lang.NoSuchMethodError:
  edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(Ljava/lang/String;[Ljava/lang/String;)Ledu/stanford/nlp/parser/lexparser/LexicalizedParser;</p>
</blockquote>

<p>I don't understand where that L comes from, I think it is the root of my problem... This is rather weird. I have tried to get inside the source files, but there is no wrong reference there.</p>

<p>Code:</p>

<pre><code>import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefGraphAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.IntTuple;
import edu.stanford.nlp.util.Pair;
import edu.stanford.nlp.util.Timing;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import java.util.Properties;

public class Coref {

/**
 * @param args the command line arguments
 */
public static void main(String[] args) throws IOException, ClassNotFoundException {
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = ""Mary has a little lamb. She is very cute.""; // Add your text here!

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);       
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);
      System.out.println(tree);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = 
      document.get(CorefChainAnnotation.class);
    System.out.println(graph);
  }
}
</code></pre>

<p>Full stack trace:</p>

<blockquote>
  <p>Adding annotator tokenize
  Adding annotator ssplit
  Adding annotator pos
  Loading POS Model [edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger] ... Loading default properties from trained tagger edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger
  Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.1 sec].
  done [2.2 sec].
  Adding annotator lemma
  Adding annotator ner
  Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [4.0 sec].
  Loading classifier from edu/stanford/nlp/models/ner/english.muc.distsim.crf.ser.gz ... done [3.0 sec].
  Loading classifier from edu/stanford/nlp/models/ner/english.conll.distsim.crf.ser.gz ... done [3.3 sec].
  Adding annotator parse
  Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(Ljava/lang/String;[Ljava/lang/String;)Ledu/stanford/nlp/parser/lexparser/LexicalizedParser;
      at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:115)
      at edu.stanford.nlp.pipeline.ParserAnnotator.(ParserAnnotator.java:64)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:603)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP$12.create(StanfordCoreNLP.java:585)
      at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:329)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:196)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:186)
      at edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:178)
      at Coref.main(Coref.java:41)</p>
</blockquote>
",Coreference & Relation Extraction,resolve coreference using stanford corenlp unable load parser model want simple job given string containing pronoun want resolve example want turn sentence mary ha little lamb cute mary ha little lamb mary cute tried use stanford corenlp however seem unable get parser start imported included jar project using eclipse allocated gb jvm xmx g error awkward exception thread main java lang nosuchmethoderror edu stanford nlp parser lexparser lexicalizedparser loadmodel ljava lang string ljava lang string ledu stanford nlp parser lexparser lexicalizedparser understand l come think root problem rather weird tried get inside source file wrong reference code full stack trace adding annotator tokenize adding annotator ssplit adding annotator po loading po model edu stanford nlp model po tagger english left word english left word distsim tagger loading default property trained tagger edu stanford nlp model po tagger english left word english left word distsim tagger reading po tagger model edu stanford nlp model po tagger english left word english left word distsim tagger done sec done sec adding annotator lemma adding annotator ner loading classifier edu stanford nlp model ner english class distsim crf ser gz done sec loading classifier edu stanford nlp model ner english muc distsim crf ser gz done sec loading classifier edu stanford nlp model ner english conll distsim crf ser gz done sec adding annotator parse exception thread main java lang nosuchmethoderror edu stanford nlp parser lexparser lexicalizedparser loadmodel ljava lang string ljava lang string ledu stanford nlp parser lexparser lexicalizedparser edu stanford nlp pipeline parserannotator loadmodel parserannotator java edu stanford nlp pipeline parserannotator parserannotator java edu stanford nlp pipeline stanfordcorenlp create stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp create stanfordcorenlp java edu stanford nlp pipeline annotatorpool get annotatorpool java edu stanford nlp pipeline stanfordcorenlp construct stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java edu stanford nlp pipeline stanfordcorenlp stanfordcorenlp java coref main coref java
Getting corefrences with Standard corenlp package,"<p>I'm trying to get coreferences in a text. I'm new to the corenlp package. I tried the code below, which doesn't work, but I'm open to other methods as well.</p>

<pre><code>/*
 * To change this template, choose Tools | Templates
 * and open the template in the editor.
 */

package corenlp;
import edu.stanford.nlp.ling.CoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.CorefGraphAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.semgraph.SemanticGraph;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.IntTuple;
import edu.stanford.nlp.util.Pair;
import edu.stanford.nlp.util.Timing;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import java.util.Properties;
/**
 *
 * @author Karthi
 */
public class Main {


        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
    Properties props = new Properties();
    FileInputStream in = new FileInputStream(""Main.properties"");

    props.load(in);
    in.close();
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    String text = ""The doctor can consult with other doctors about this patient. If that is the case, the name of the doctor and the names of the consultants have to be maintained. Otherwise, only the name of the doctor is kept. ""; // Add your text here!

    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);
    System.out.println(document);
    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = (List&lt;CoreMap&gt;) document.get(SentencesAnnotation.class);
    System.out.println(sentences);
    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);
System.out.println(tree);
      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
      System.out.println(dependencies);
    }

    // this is the coreference link graph
    // each link stores an arc in the graph; the first element in the Pair is the source, the second is the target
    // each node is stored as &lt;sentence id, token id&gt;. Both offsets start at 1!
    List&lt;Pair&lt;IntTuple, IntTuple&gt;&gt; graph = document.get(CorefGraphAnnotation.class);
    System.out.println(graph);

    }

}
</code></pre>

<p>This is the error I get:</p>

<pre class=""lang-none prettyprint-override""><code>Loading POS Model [// For POS model] ... Loading default properties from trained tagger // For POS model
Error: No such trained tagger config file found.
java.io.FileNotFoundException: \\ For POS model (The specified path is invalid)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:106)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:66)
        at edu.stanford.nlp.tagger.maxent.TaggerConfig.getTaggerDataInputStream(TaggerConfig.java:741)
        at edu.stanford.nlp.tagger.maxent.TaggerConfig.&lt;init&gt;(TaggerConfig.java:178)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:228)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:57)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:44)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:441)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:434)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:309)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:347)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:337)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:329)
        at corenlp.Main.main(Main.java:66)
Exception in thread ""main"" java.lang.RuntimeException: java.io.FileNotFoundException: \\ For POS model (The specified path is invalid)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:443)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:434)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:62)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:309)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:347)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:337)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:329)
        at corenlp.Main.main(Main.java:66)
Caused by: java.io.FileNotFoundException: \\ For POS model (The specified path is invalid)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:106)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:66)
        at edu.stanford.nlp.tagger.maxent.TaggerConfig.getTaggerDataInputStream(TaggerConfig.java:741)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:643)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:268)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:228)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:57)
        at edu.stanford.nlp.pipeline.POSTaggerAnnotator.&lt;init&gt;(POSTaggerAnnotator.java:44)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$4.create(StanfordCoreNLP.java:441)
        ... 7 more
Java Result: 1
</code></pre>
",Coreference & Relation Extraction,getting corefrences standard corenlp package trying get coreference text new corenlp package tried code work open method well error get
How to get the finalized text after resolving co-references using StandfordNLP,"<p>Hi I just started learning NLP and chose Stanford api to do all my required tasks. I am able to do POS and NER tasks but I am stuck with co-reference resolution. I am even able to get the 'corefChaingraph' and able to print all the representative mention and corresponding mentions to console. But, I really would like to know how to get the finalized text after resolving the co-references. Can some one help me regarding this?</p>

<p>example:
Input sentence:
John Smith talks about the EU. He likes the family of nations.</p>

<p>Expected ouput:
John Smith talks about the EU. <strong>John Smith</strong> likes the family of nations.</p>
",Coreference & Relation Extraction,get finalized text resolving co reference using standfordnlp hi started learning nlp chose stanford api required task able po ner task stuck co reference resolution even able get corefchaingraph able print representative mention corresponding mention console really would like know get finalized text resolving co reference one help regarding example input sentence john smith talk eu like family nation expected ouput john smith talk eu john smith like family nation
Clean text coming from PDFs,"<p>this is more of an algorithmic question rather than a specific language question, so I am happy to receive an answer in any language - even pseudocode, even just an idea.</p>

<p>Here is my problem: I need to work on large dataset of papers that come from articles in PDF and that were brutally copied/pasted into .txt. I only have the result of this abomination, which is around 16k papers, for 3.5 GB or text (the corpus I am using is the ACL Antology Network, <a href=""http://clair.si.umich.edu/clair/aan/DatasetContents.html"">http://clair.si.umich.edu/clair/aan/DatasetContents.html</a> ).</p>

<p>The ""junk"" comes from things like formulae, images, tables, and so on. It just pops in the middle of the running text, so I can't use regular expressions to clean it, and I can't think of any way to use machine learning for it either. I already spent a week on it, and then I decided to move on with a quick&amp;dirty fix. I don't care about cleaning it completely anymore, I don't care about false negatives and positives as long as the majority of this areas of text is removed.</p>

<p>Some examples of the text: note that formulae contain junk characters, but tables and caption don't (but they still make my sentence very long, and thus unparsable). Junk in bold.</p>

<p>Easy one:</p>

<blockquote>
  <p>The experiments were repeated while inhibiting specialization of first the scheme with the most expansions, and then the two most expanded schemata.
  Measures of coverage and speedup are important 1 As long as we are interested in preserving the f-structure assigned to sentences, this notion of coverage is stricter than necessary.
  The same f-structure can in fact be assigned by more than one parse, so that in some cases a sentence is considered out of coverage even if the specialized grammar assigns to it the correct f-structure.
  <strong>2'VPv' and 'VPverb[main]' cover VPs headed by a main verb.
  'NPadj' covers NPs with adjectives attached.
  205 The original rule: l/Pperfp --+ ADVP* SE (t ADJUNCT) ($ ADV_TYPE) = t,padv ~/r { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl $) { AD VP+ SE ('~ ADJUNCT) ($ ADV_TYPE) = vpadv is replaced by the following: ADVP,[.E (~ ADJUNCT) (.l.
  ADV_TYPE) = vpadv l/'Pperfp --+ @PPadjunct @PPcase_obl {@M.Head_Pevfp [@M..Head_Passp} @( Anaph_Ctrl ~ ) V { @M_Head_Perfp I@M_Head_Passp } @( Anaph_Ctrl ~) Figure 1: The pruning of a rule from the actual French grammar.</strong>
  The ""*"" and the ""+"" signs have the usual interpretation as in regular expressions.
  A sub-expression enclosed in parenthesis is optional.
  Alternative sub-expressions are enclosed in curly brackets and separated by the ""["" sign.
  An ""@"" followed by an identifier is a macro expansion operator, and is eventually replaced by further functional descriptions.
  <strong>Corpus --..
  ,, 0.1[ Disambiguated Treebank treebank Human expert Grammar specialization Specialized grammar Figure 2: The setting for our experiments on grammar specialization.
  indicators of what can be achieved with this form of grammar pruning.</strong>
  However, they could potentially be misleading, since failure times for uncovered sentences might be considerably lower than their sentences times, had they not been out of coverage.</p>
</blockquote>

<p>Hard one:</p>

<blockquote>
  <p>Table 4 summarizes the precision results for both English and Romanian coreference.
  The results indicate that the English coreference is more indicate than the Romanian coreference, but SNIZZLE improves coreference resolution in both languages.
  There were 64% cases when the English coreference was resolved by a heuristic with higher priority than the corresponding heuristic for the Romanian counterpart.
  This result explains why there is better precision enhancement for 
  <strong>English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal Pronominal 73% 89% 66% 78% 76% 93% 71°/o 82% Table 4: Coreference precision Total 84% 72% 87% 76% English Romanian SWIZZLE on English SWIZZLE on Romanian Nominal 69% 63% 66% 61% Pronominal Total 89% 78% 83% 72% 87% 77% 80% 70% Table 5: Coreference recall</strong> the English coreference. Table 5 also illustrates the recall results.
  The advantage of the data-driven coreference resolution over other methods is based on its better recall performance.
  This is explained by the fact that this method captures a larger variety of coreference patterns.
  Even though other coreference resolution systems perform better for some specific forms of systems, their recall results are surpassed by the systems approach.
  Multilingual coreference in turn improves more the precision than the recall of the monolingual data-driven coreference systems.
  In addition, Table 5 shows that the English coref- erence results in better recall than Romanian coref- erence.
  However, the recall shows a decrease for both languages for SNIZZLE because imprecise coreference links are deleted.
  As is usually the case, deleting data lowers the recall.
  All results were obtained by using the automatic scorer program developed for the MUC evaluations.</p>
</blockquote>

<p>Note how the table does not contain strange characters and goes right in the middle of the sentence: ""This result explains why there is better precision enhancement for -TABLE HERE- the English coreference."" I can't know where the table will be in regard to the running text. It may occur before a sentence, after it or within it like in this case. Also note that the table shit does not end with a full stop (most captions in papers don't...) so I can't rely on punctuation to spot it. I am happy with non-accurate boundaries of course, but I still need to do something with these tables. Some of them contain words rather than numbers, and I don't have enough information in those cases: no junky characters, nothing. It is obvious to only humans :S</p>
",Coreference & Relation Extraction,clean text coming pdfs algorithmic question rather specific language question happy receive answer language even pseudocode even idea problem need work large dataset paper come article pdf brutally copied pasted txt result abomination around k paper gb text corpus using acl antology network example text note formula contain junk character table caption still make sentence long thus unparsable junk bold easy one experiment repeated inhibiting specialization first scheme expansion two expanded schema measure coverage speedup important long interested preserving f structure assigned sentence notion coverage stricter necessary f structure fact assigned one parse case sentence considered coverage even specialized grammar assigns correct f structure vpv vpverb main cover vps headed main verb npadj cover np adjective attached original rule l pperfp advp se adjunct adv type padv r head perfp head passp anaph ctrl ad vp se adjunct adv type vpadv replaced following advp e adjunct l adv type vpadv l pperfp ppadjunct ppcase obl head pevfp head passp anaph ctrl v head perfp head passp anaph ctrl figure pruning rule actual french grammar sign usual interpretation regular expression sub expression enclosed parenthesis optional alternative sub expression enclosed curly bracket separated sign followed identifier macro expansion operator eventually replaced functional description corpus disambiguated treebank treebank human expert grammar specialization specialized grammar figure setting experiment grammar specialization indicator achieved form grammar pruning however could potentially misleading since failure time uncovered sentence might considerably lower sentence time coverage hard one table summarizes precision result english romanian coreference result indicate english coreference indicate romanian coreference snizzle improves coreference resolution language case english coreference wa resolved heuristic higher priority corresponding heuristic romanian counterpart result explains better precision enhancement english romanian swizzle english swizzle romanian nominal pronominal table coreference precision total english romanian swizzle english swizzle romanian nominal pronominal total table coreference recall english coreference table also illustrates recall result advantage data driven coreference resolution method based better recall performance explained fact method capture larger variety coreference pattern even though coreference resolution system perform better specific form system recall result surpassed system approach multilingual coreference turn improves precision recall monolingual data driven coreference system addition table show english coref erence result better recall romanian coref erence however recall show decrease language snizzle coreference link deleted usually case deleting data lower recall result obtained using automatic scorer program developed muc evaluation note table doe contain strange character go right middle sentence result explains better precision enhancement table english coreference know table regard running text may occur sentence within like case also note table shit doe end full stop caption paper rely punctuation spot happy non accurate boundary course still need something table contain word rather number enough information case junky character nothing obvious human
Anaphora resolution using Stanford Coref,"<p>I have sentences <strong>(Text I)</strong>:</p>

<blockquote>
  <p><em>Tom is a smart boy. <strong>He</strong> know a lot of thing.</em></p>
</blockquote>

<p>I want to change <strong>He</strong> in the second sentence to <strong>Tom</strong>, so final sentences will become <strong>(Text II)</strong>:</p>

<blockquote>
  <p><em>Tom is a smart boy. <strong>Tom</strong> know a lot of thing.</em></p>
</blockquote>

<p>I've wrote some code, but my <em><strong>coref</strong></em> object always <strong><em>null</em></strong>.<br>
Besides I have no idea what to do next to get correct result.</p>

<pre><code>    String text = ""Tom is a smart boy. He know a lot of thing."";
    Annotation document = new Annotation(text);
    Properties props = new Properties();
    props.put(""annotators"", ""tokenize, ssplit, pos, parse, lemma, ner, dcoref"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(document);

    List&lt;Pair&lt;IntTuple, IntTuple&gt;&gt; coref = document.get(CorefGraphAnnotation.class);
</code></pre>

<p>I want to know if I'm doing it wrong and what I should do next to get <strong>Text II</strong> from <strong>Text I</strong>.<br>
PS: I'm using Stanford CoreNLP 1.3.0.</p>

<p>Thanks.</p>
",Coreference & Relation Extraction,anaphora resolution using stanford coref sentence text tom smart boy know lot thing want change second sentence tom final sentence become text ii tom smart boy tom know lot thing wrote code coref object always null besides idea next get correct result want know wrong next get text ii text p using stanford corenlp thanks
Stanford NLP tools and lots of text,"<p>I'm currently using the Stanford CoreNLP tools to extract triplets from text. I've been dealing with relatively small pieces of text, but I need to make it work with larger ones. Also, I need this work on the average machine with an average amount of memory. Any suggestions on how I can reduce the memory load? Perhaps split the text into smaller pieces (this would screw up coreference however...)?</p>

<p>Thank you</p>
",Coreference & Relation Extraction,stanford nlp tool lot text currently using stanford corenlp tool extract triplet text dealing relatively small piece text need make work larger one also need work average machine average amount memory suggestion reduce memory load perhaps split text smaller piece would screw coreference however thank
